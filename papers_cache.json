{
  "2511.21087": {
    "summary_zh": "<ul>\n    <li>\u6307\u4ee4\u5f15\u5bfc\u7684\u56fe\u50cf\u7f16\u8f91\u5141\u8bb8\u7528\u6237\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u76f4\u89c2\u7f16\u8f91\u3002</li>\n    <li>\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u7f16\u8f91\u6a21\u578b\u5728\u7406\u89e3\u590d\u6742\u7528\u6237\u6307\u4ee4\u65f6\u5e38\u5e38\u6548\u679c\u4e0d\u4f73\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MIRA\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u591a\u6a21\u6001\u63a8\u7406\u4ee3\u7406\uff0c\u901a\u8fc7\u8fed\u4ee3\u7684\u611f\u77e5-\u63a8\u7406-\u884c\u52a8\u5faa\u73af\u8fdb\u884c\u7f16\u8f91\u3002</li>\n    <li>MIRA\u9010\u6b65\u9884\u6d4b\u539f\u5b50\u7f16\u8f91\u6307\u4ee4\uff0c\u5e76\u4f7f\u7528\u89c6\u89c9\u53cd\u9988\u6765\u505a\u51fa\u51b3\u7b56\u3002</li>\n    <li>\u4e0e\u5f00\u6e90\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u7ed3\u5408\u65f6\uff0cMIRA\u663e\u8457\u63d0\u9ad8\u4e86\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u611f\u77e5\u8d28\u91cf\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li>MIRA is a new tool for image editing that helps users edit images using natural language instructions.</li>\n    <li>Current image editing models struggle with complex instructions, which can lead to incorrect edits.</li>\n    <li>MIRA uses a step-by-step process to make editing decisions based on visual feedback, similar to how humans interact with models.</li>\n    <li>It was trained on a large dataset and uses a specific training method to understand and perform complex edits better.</li>\n    <li>MIRA improves the quality and accuracy of image edits when used with popular editing models, matching or surpassing some commercial tools.</li>\n</ul>"
  },
  "2511.21662": {
    "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u591a\u6a21\u6001\u8bc4\u4f30\u7cfb\u7edf\u4e2d\u8d8a\u6765\u8d8a\u5e38\u88ab\u7528\u4f5c\u8bc4\u5224\u8005\uff0c\u56e0\u5176\u80fd\u591f\u6709\u6548\u8ddf\u968f\u6307\u4ee4\u5e76\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86Multi-Crit\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u80fd\u5426\u9075\u5faa\u591a\u79cd\u8bc4\u4f30\u6807\u51c6\u5e76\u4ea7\u751f\u53ef\u9760\u7684\u5224\u65ad\u3002</li>\n    <li>Multi-Crit\u5305\u62ec\u5f00\u653e\u5f0f\u751f\u6210\u548c\u53ef\u9a8c\u8bc1\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u4e25\u683c\u7684\u6570\u636e\u6536\u96c6\u6d41\u7a0b\u5efa\u7acb\uff0c\u6536\u96c6\u4e86\u5177\u6709\u591a\u6807\u51c6\u4eba\u5de5\u6807\u6ce8\u7684\u6311\u6218\u6027\u54cd\u5e94\u5bf9\u3002</li>\n    <li>\u5206\u6790\u663e\u793a\uff0c\u4e13\u6709\u6a21\u578b\u5728\u9075\u5faa\u591a\u5143\u6807\u51c6\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u5f00\u653e\u5f0f\u8bc4\u4f30\u4e2d\uff1b\u5f00\u6e90\u6a21\u578b\u5728\u7075\u6d3b\u8ddf\u968f\u591a\u6837\u6807\u51c6\u65b9\u9762\u8868\u73b0\u66f4\u5dee\u3002</li>\n    <li>Multi-Crit\u4e3a\u5efa\u7acb\u53ef\u9760\u4e14\u53ef\u8c03\u8282\u7684\u591a\u6a21\u6001AI\u8bc4\u4f30\u6253\u4e0b\u4e86\u57fa\u7840\uff0c\u662f\u4e00\u9879\u5f00\u521b\u6027\u7684\u7814\u7a76\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li>Large multimodal models (LMMs) are becoming popular for judging multimodal evaluation systems because they follow instructions well and match human preferences.</li>\n    <li>Multi-Crit is a new benchmark designed to test how well these models can adhere to multiple evaluation criteria and make reliable judgments.</li>\n    <li>This benchmark includes both creative tasks and reasoning tasks, and uses carefully curated data with human feedback on responses.</li>\n    <li>Analysis of 25 LMMs shows that proprietary models struggle with following diverse criteria, while open-source models do even worse.</li>\n    <li>The study suggests that improvements in fine-tuning for holistic judgments help with visual tasks but do not improve performance on varied criteria.</li>\n</ul>"
  },
  "2511.21678": {
    "summary_zh": "<ul>\n    <li>MLLMs\uff08\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff09\u5728\u72ec\u7acb\u67e5\u8be2\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b83\u4eec\u72ec\u7acb\u89e3\u51b3\u6bcf\u4e2a\u95ee\u9898\uff0c\u5e38\u5e38\u91cd\u590d\u9519\u8bef\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u589e\u5f3a\u4ee3\u7406\u4e3b\u8981\u5b58\u50a8\u8fc7\u53bb\u7684\u8f68\u8ff9\uff0c\u7136\u800c\u57fa\u4e8e\u8f68\u8ff9\u7684\u8bb0\u5fc6\u5bb9\u6613\u4e22\u5931\u91cd\u8981\u7684\u9886\u57df\u77e5\u8bc6\u3002</li>\n    <li>ViLoMem \u662f\u4e00\u79cd\u53cc\u6d41\u8bb0\u5fc6\u6846\u67b6\uff0c\u80fd\u591f\u5206\u522b\u7f16\u7801\u89c6\u89c9\u5e72\u6270\u6a21\u5f0f\u548c\u903b\u8f91\u63a8\u7406\u9519\u8bef\uff0c\u4ece\u800c\u5e2e\u52a9 MLLMs \u5b66\u4e60\u6210\u529f\u548c\u5931\u8d25\u7684\u7ecf\u9a8c\u3002</li>\n    <li>\u8be5\u7cfb\u7edf\u901a\u8fc7\u9010\u6b65\u79ef\u7d2f\u548c\u66f4\u65b0\u591a\u6a21\u6001\u8bed\u4e49\u77e5\u8bc6\uff0c\u4fdd\u6301\u7a33\u5b9a\u7684\u3001\u53ef\u63a8\u5e7f\u7684\u7b56\u7565\uff0c\u907f\u514d\u4e25\u91cd\u9057\u5fd8\u3002</li>\n    <li>\u5728\u516d\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cViLoMem \u4e00\u81f4\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u89c6\u89c9\u548c\u903b\u8f91\u9519\u8bef\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n  <li>MLLMs can solve problems well in isolation, but they don\u2019t learn from past mistakes effectively.</li>\n  <li>Current memory systems mostly keep track of past actions, but they can lose important knowledge over time.</li>\n  <li>These systems often fail to combine visual and logical reasoning, which is how humans think.</li>\n  <li>ViLoMem is a new memory system that improves learning by separately storing visual distractions and reasoning errors.</li>\n  <li>This method has been shown to enhance accuracy and reduce mistakes in multimodal tasks through better memory management.</li>\n</ul>"
  },
  "2511.20626": {
    "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u52a0\u5267\u4e86\u7b97\u6cd5\u4e0d\u7cbe\u786e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668ROOT\uff0c\u65e8\u5728\u901a\u8fc7\u53cc\u91cd\u7a33\u5065\u673a\u5236\u6765\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>ROOT\u91c7\u7528\u81ea\u9002\u5e94\u725b\u987f\u8fed\u4ee3\u7684\u7ef4\u5ea6\u7a33\u5065\u6b63\u4ea4\u5316\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4e0d\u540c\u67b6\u6784\u914d\u7f6e\u4e0b\u7684\u7cbe\u786e\u5ea6\u3002</li>\n    <li>ROOT\u8fd8\u901a\u8fc7\u90bb\u8fd1\u4f18\u5316\u6846\u67b6\u6291\u5236\u5f02\u5e38\u503c\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u610f\u4e49\u7684\u68af\u5ea6\u65b9\u5411\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cROOT\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e2d\u6bd4Muon\u548c\u57fa\u4e8eAdam\u7684\u4f18\u5316\u5668\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li>Large language models (LLMs) face challenges with training stability and precision as they grow in size.</li>\n    <li>Recent optimizers have improved efficiency but struggle with robustness against noise and dimensional changes.</li>\n    <li>ROOT is a new optimizer that strengthens training stability using two innovative methods: dimension-robust orthogonalization and noise suppression.</li>\n    <li>Experiments show ROOT outperforms other optimizers like Muon and Adam in tough conditions, achieving faster training and better results.</li>\n    <li>The ROOT code will be available online for others to use and build upon.</li>\n</ul>"
  },
  "2511.17592": {
    "summary_zh": "<ul>\n    <li>GigaEvo\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u65e8\u5728\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u4f7f\u7528LLM-\u8fdb\u5316\u65b9\u6cd5\u8fdb\u884c\u5b9e\u9a8c\u3002</li>\n    <li>\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u591a\u4e2a\u6a21\u5757\u5316\u7ec4\u4ef6\uff0c\u5305\u62ecMAP-Elites\u7b97\u6cd5\u548c\u591a\u5c9b\u8fdb\u5316\u7b56\u7565\u3002</li>\n    <li>GigaEvo\u7684\u8bbe\u8ba1\u5f3a\u8c03\u6a21\u5757\u5316\u548c\u5e76\u53d1\u6027\uff0c\u65b9\u4fbf\u5feb\u901f\u539f\u578b\u5f00\u53d1\u3002</li>\n    <li>\u6211\u4eec\u5728\u6311\u6218\u6027\u95ee\u9898\u4e0a\u8bc4\u4f30\u4e86GigaEvo\uff0c\u4ee5\u9a8c\u8bc1\u5176\u53ef\u91cd\u590d\u6027\u548c\u5b9e\u73b0\u6548\u679c\u3002</li>\n    <li>\u6240\u6709\u4ee3\u7801\u548c\u6846\u67b6\u53ef\u5728GitHub\u4e0a\u83b7\u53d6\uff0c\u652f\u6301\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u3002</li>\n</ul>"
  },
  "2511.15552": {
    "summary_simple": "<ul>\n    <li>Multimodal large language models (MLLMs) are rapidly advancing, but their intelligence and risks are not well understood.</li>\n    <li>Mera Multi is a new evaluation framework for multimodal models focused on the Russian language, which currently lacks such benchmarks.</li>\n    <li>The framework includes 18 new tasks that test various types of models, covering text, images, audio, and video.</li>\n    <li>Key contributions include a detailed classification of multimodal abilities, new datasets tailored to Russian culture, and baseline performance results.</li>\n    <li>The methodology developed can be used to create similar benchmarks for other languages, especially within the Slavic language group.</li>\n</ul>"
  },
  "2511.20639": {
    "summary_zh": "<ul>\n    <li>\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u901a\u8fc7\u76f4\u63a5\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u534f\u4f5c\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u6587\u672c\u57fa\u7840\u63a8\u7406\u548c\u4ea4\u6d41\u3002</li>\n    <li>LatentMAS\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u652f\u6301LLM\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u7eaf\u6f5c\u5728\u534f\u4f5c\u3002</li>\n    <li>\u6bcf\u4e2a\u667a\u80fd\u4f53\u901a\u8fc7\u6700\u540e\u4e00\u5c42\u7684\u9690\u85cf\u5d4c\u5165\u751f\u6210\u6f5c\u5728\u601d\u7ef4\uff0c\u5e76\u901a\u8fc7\u5171\u4eab\u7684\u6f5c\u5728\u5de5\u4f5c\u8bb0\u5fc6\u8fdb\u884c\u4fe1\u606f\u4ea4\u6362\u3002</li>\n    <li>\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cLatentMAS\u5728\u4fe1\u606f\u8868\u8fbe\u548c\u4fdd\u7559\u65b9\u9762\u7684\u590d\u6742\u6027\u66f4\u4f4e\uff0c\u4e14\u8868\u73b0\u66f4\u4f18\u3002</li>\n    <li>\u57289\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLatentMAS\u5728\u51c6\u786e\u6027\u4e0a\u63d0\u9ad8\u4e8614.6%\uff0c\u51cf\u5c11\u4e86\u8f93\u51fa\u4ee4\u724c\u4f7f\u7528\u91cf\uff0c\u5e76\u52a0\u5feb\u4e86\u63a8\u7406\u901f\u5ea6\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li>Multi-agent systems (MAS) improve collaboration among large language models (LLMs) beyond just single-model thinking.</li>\n    <li>LatentMAS allows LLM agents to work together directly in a shared space without relying on text communication.</li>\n    <li>It uses a shared memory to exchange information seamlessly while maintaining the quality of data.</li>\n    <li>LatentMAS shows better performance in various tasks, with up to 14.6% higher accuracy and significantly faster processing times.</li>\n    <li>The framework is open-source and does not require additional training to operate effectively.</li>\n</ul>"
  },
  "2511.19046": {
    "summary_zh": "<ul>\n    <li>\u533b\u5b66\u56fe\u50cf\u5206\u5272\u5bf9\u751f\u7269\u533b\u5b66\u7814\u7a76\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u901a\u7528\u6027\uff0c\u9700\u5927\u91cf\u624b\u52a8\u6807\u6ce8\u65b0\u4e34\u5e8a\u5e94\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MedSAM-3\uff0c\u4e00\u4e2a\u53ef\u901a\u8fc7\u6587\u672c\u63d0\u793a\u8fdb\u884c\u533b\u5b66\u56fe\u50cf\u548c\u89c6\u9891\u5206\u5272\u7684\u6a21\u578b\u3002</li>\n    <li>MedSAM-3\u53ef\u4ee5\u901a\u8fc7\u5f00\u653e\u8bcd\u6c47\u7684\u6587\u672c\u63cf\u8ff0\u7cbe\u786e\u5b9a\u4f4d\u89e3\u5256\u7ed3\u6784\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cMedSAM-3\u5728\u5404\u7c7b\u533b\u5b66\u6210\u50cf\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li>MedSAM-3 is a new model for segmenting medical images and videos, making it easier to identify important parts of the body.</li>\n    <li>It uses text descriptions instead of just geometric shapes, allowing for more precise targeting of anatomical structures.</li>\n    <li>The model is fine-tuned on medical images with labeled concepts, enhancing its ability to understand medical content.</li>\n    <li>MedSAM-3 works with a new system called the MedSAM-3 Agent that combines language models for better reasoning and improvement during use.</li>\n    <li>Tests show that MedSAM-3 performs better than existing models across various medical imaging types like X-ray, MRI, and CT.</li>\n</ul>"
  },
  "2511.19900": {
    "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\u5728\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u53d7\u9650\u4e8e\u4eba\u5de5\u6807\u6ce8\u7684\u76d1\u7763\u3002</li>\n    <li>\u8fd1\u671f\u7684\u81ea\u6211\u5956\u52b1\u65b9\u6cd5\u8bd5\u56fe\u901a\u8fc7\u8ba9\u6a21\u578b\u81ea\u6211\u8bc4\u4ef7\u6765\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u4f46\u7eaf\u6587\u672c\u8bc4\u4ef7\u96be\u4ee5\u9a8c\u8bc1\u590d\u6742\u89c6\u89c9\u63a8\u7406\u6b65\u9aa4\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Agent0-VL\uff0c\u8fd9\u662f\u4e00\u79cd\u81ea\u6211\u8fdb\u5316\u7684\u89c6\u89c9\u8bed\u8a00\u4ee3\u7406\uff0c\u5229\u7528\u5de5\u5177\u96c6\u6210\u63a8\u7406\u5b9e\u73b0\u6301\u7eed\u6539\u8fdb\u3002</li>\n    <li>Agent0-VL\u7ed3\u5408\u4e86\u63a8\u7406\u3001\u81ea\u6211\u8bc4\u4ef7\u548c\u81ea\u6211\u4fee\u6b63\uff0c\u901a\u8fc7\u57fa\u4e8e\u8bc1\u636e\u7684\u5206\u6790\u6765\u63d0\u9ad8\u5176\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cAgent0-VL\u5728\u51e0\u4f55\u95ee\u9898\u89e3\u51b3\u548c\u89c6\u89c9\u79d1\u5b66\u5206\u6790\u4e2d\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u9ad8\u4e8612.5%\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li>Vision-language agents have made great progress but still rely on human supervision.</li>\n    <li>Self-rewarding methods allow models to evaluate themselves, but they can struggle with complex visual tasks.</li>\n    <li>Agent0-VL is a new agent that improves itself by using tools for reasoning, evaluation, and correction.</li>\n    <li>It has two main roles: a Solver for reasoning and a Verifier for giving feedback and rewards.</li>\n    <li>Agent0-VL shows a 12.5% improvement in tasks without needing human help or external rewards.</li>\n</ul>"
  },
  "2511.20714": {
    "summary_zh": "<ul>\n    <li>\u4e16\u754c\u6a21\u578b\u662f\u4ee3\u7406 AI\u3001\u5177\u8eab AI \u548c\u6e38\u620f\u7b49\u9886\u57df\u7684\u6838\u5fc3\u6a21\u62df\u5668\uff0c\u80fd\u591f\u751f\u6210\u957f\u800c\u771f\u5b9e\u4e92\u52a8\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u6269\u5c55\u8fd9\u4e9b\u6a21\u578b\u53ef\u80fd\u4f1a\u89e3\u9501\u65b0\u7684\u89c6\u89c9\u611f\u77e5\u3001\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u63a8\u52a8\u8d85\u8d8a\u5f53\u524d LLM \u89c6\u89d2\u7684\u57fa\u7840\u6a21\u578b\u7684\u65b0\u8303\u5f0f\u3002</li>\n    <li>\u534a\u81ea\u56de\u5f52\uff08\u5757\u6269\u6563\uff09\u89e3\u7801\u65b9\u6cd5\u662f\u5173\u952e\u7a81\u7834\uff0c\u5b83\u7ed3\u5408\u4e86\u6269\u6563\u548c\u81ea\u56de\u5f52\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u751f\u6210\u66f4\u8fde\u8d2f\u548c\u7a33\u5b9a\u7684\u89c6\u9891\u5e8f\u5217\u3002</li>\n    <li>Inferix \u662f\u4e0b\u4e00\u4ee3\u63a8\u7406\u5f15\u64ce\uff0c\u4e13\u4e3a\u4f18\u5316\u534a\u81ea\u56de\u5f52\u89e3\u7801\u8fc7\u7a0b\u800c\u8bbe\u8ba1\uff0c\u80fd\u591f\u5b9e\u73b0\u6c89\u6d78\u5f0f\u7684\u4e16\u754c\u5408\u6210\u3002</li>\n    <li>\u5b83\u652f\u6301\u5b9e\u65f6\u4e92\u52a8\u548c\u73b0\u5b9e\u6a21\u62df\uff0c\u5e76\u901a\u8fc7 LV-Bench \u96c6\u6210\u9ad8\u6548\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63a8\u52a8\u4e16\u754c\u6a21\u578b\u7684\u63a2\u7d22\u3002 </li>\n</ul>",
    "summary_simple": "<ul>\n    <li>World models can create realistic and interactive videos, useful in AI, gaming, and simulation.</li>\n    <li>Scaling these models can improve visual perception and reasoning, moving beyond current methods focused on language models.</li>\n    <li>The new semi-autoregressive decoding combines diffusion and autoregressive techniques to create better video sequences.</li>\n    <li>Inferix is a new engine designed for efficient world simulation using this advanced decoding method.</li>\n    <li>It offers real-time video streaming and integrates a new evaluation benchmark for testing video generation quality.</li>\n</ul>"
  },
  "2511.19320": {
    "summary_zh": "<ul>\n    <li>\u4eba\u50cf\u52a8\u753b\u4e2d\uff0c\u4fdd\u6301\u7b2c\u4e00\u5e27\u7684\u8eab\u4efd\u548c\u7cbe\u786e\u7684\u8fd0\u52a8\u63a7\u5236\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u3002</li>\n    <li>\u73b0\u6709\u7684\u56fe\u50cf\u5230\u89c6\u9891\uff08R2V\uff09\u65b9\u6cd5\u5ffd\u89c6\u4e86\u5e38\u89c1\u7684\u65f6\u7a7a\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u5bfc\u81f4\u8eab\u4efd\u6f02\u79fb\u548c\u89c6\u89c9\u5931\u771f\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u7684SteadyDancer\u65b9\u6cd5\u80fd\u591f\u534f\u8c03\u52a8\u753b\u6548\u679c\uff0c\u5e76\u7a33\u56fa\u5730\u4fdd\u6301\u7b2c\u4e00\u5e27\u7684\u8eab\u4efd\u3002</li>\n    <li>\u8be5\u6846\u67b6\u4f7f\u7528\u6761\u4ef6\u8c03\u548c\u673a\u5236\u548c\u534f\u540c\u59ff\u6001\u8c03\u5236\u6a21\u5757\u6765\u751f\u6210\u4e0e\u53c2\u8003\u56fe\u50cf\u9ad8\u5ea6\u517c\u5bb9\u7684\u59ff\u6001\u8868\u793a\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSteadyDancer\u5728\u5916\u89c2\u4fdd\u771f\u5ea6\u548c\u8fd0\u52a8\u63a7\u5236\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u8bad\u7ec3\u8d44\u6e90\u9700\u6c42\u66f4\u5c11\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li>Human image animation faces challenges in keeping the identity consistent while controlling motion accurately.</li>\n    <li>The traditional method, Reference-to-Video (R2V), often leads to issues like identity drift and visual errors due to misalignments.</li>\n    <li>SteadyDancer is a new framework that focuses on Image-to-Video (I2V) and successfully preserves the identity from the first frame.</li>\n    <li>It includes a special mechanism to balance conflicting conditions for better control and visual quality.</li>\n    <li>SteadyDancer outperforms existing methods in quality and motion control while needing less training effort.</li>\n</ul>"
  },
  "2511.20635": {
    "summary_zh": "<ul>\n    <li>iMontage\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u5c06\u5f3a\u5927\u7684\u89c6\u9891\u6a21\u578b\u8f6c\u5316\u4e3a\u56fe\u50cf\u751f\u6210\u5668\u3002</li>\n    <li>\u8be5\u6846\u67b6\u53ef\u4ee5\u751f\u6210\u591a\u79cd\u957f\u5ea6\u7684\u56fe\u50cf\u96c6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4efb\u52a1\u3002</li>\n    <li>\u901a\u8fc7\u6ce8\u5165\u56fe\u50cf\u6570\u636e\u7684\u591a\u6837\u6027\uff0ciMontage\u63d0\u5347\u4e86\u751f\u6210\u56fe\u50cf\u7684\u81ea\u7136\u8fc7\u6e21\u548c\u52a8\u6001\u8303\u56f4\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4fdd\u6301\u4e86\u826f\u597d\u7684\u56fe\u50cf\u95f4\u4e00\u81f4\u6027\uff0c\u5e76\u80fd\u751f\u6210\u8d85\u51fa\u5e38\u89c4\u8303\u56f4\u7684\u52a8\u6001\u573a\u666f\u3002</li>\n    <li>\u8be5\u6280\u672f\u7684\u9002\u5e94\u7b56\u7565\u548c\u6570\u636e\u5904\u7406\u8fc7\u7a0b\u975e\u5e38\u4f18\u96c5\uff0c\u786e\u4fdd\u4e86\u6a21\u578b\u7684\u539f\u59cb\u8fd0\u52a8\u4f18\u5148\u7ea7\u4e0d\u88ab\u7834\u574f\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li>iMontage is a new framework that transforms a powerful video model into a versatile image generator.</li>\n    <li>It combines the strength of video models with diverse image data to create high-quality image sets with smooth transitions.</li>\n    <li>The framework can handle various image generation and editing tasks while maintaining the model's original motion quality.</li>\n    <li>iMontage is designed to work well with many different tasks, producing images with great detail and dynamic range.</li>\n    <li>The approach includes a careful training process and data selection to enhance image manipulation abilities.</li>\n</ul>"
  },
  "2511.20561": {
    "summary_zh": "<ul>\n    <li>\u6700\u8fd1\uff0c\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7406\u89e3\u4e0e\u751f\u6210\u4e4b\u95f4\u7684\u5173\u7cfb\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86UniSandbox\uff0c\u4e00\u4e2a\u89e3\u8026\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u914d\u5408\u53d7\u63a7\u7684\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u8be6\u7ec6\u5206\u6790\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\u7406\u89e3\u4e0e\u751f\u6210\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u63a8\u7406\u751f\u6210\u548c\u77e5\u8bc6\u8f6c\u79fb\u4e24\u4e2a\u65b9\u9762\u3002</li>\n    <li>\u63a8\u7406\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u5728\u7406\u89e3\u6a21\u5757\u4e2d\u80fd\u591f\u6709\u6548\u7f29\u5c0f\u8fd9\u4e00\u5dee\u8ddd\u3002</li>\n    <li>UniSandbox\u4e3a\u672a\u6765\u7edf\u4e00\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u521d\u6b65\u6d1e\u5bdf\uff0c\u65e8\u5728\u771f\u6b63\u5f25\u5408\u7406\u89e3\u4e0e\u751f\u6210\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li>New research introduces UniSandbox, a testing framework to study how understanding impacts generation in Unified Multimodal Models.</li>\n    <li>The study finds a significant gap between understanding and generating responses, particularly in reasoning tasks and knowledge transfer.</li>\n    <li>Using Chain-of-Thought (CoT) techniques helps improve reasoning tasks by making the understanding process clearer.</li>\n    <li>CoT also aids in knowledge transfer by retrieving newly learned information during generation.</li>\n    <li>UniSandbox offers insights for developing better models and training methods to connect understanding and generation more effectively.</li>\n</ul>"
  },
  "2511.18423": {
    "summary_zh": "<ul>\n    <li>\u5185\u5b58\u5bf9\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u9759\u6001\u5185\u5b58\u5bb9\u6613\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u79f0\u4e3a\u4e00\u822c\u4ee3\u7406\u5185\u5b58\uff08GAM\uff09\uff0c\u91c7\u7528\u201c\u53ca\u65f6\u7f16\u8bd1\u201d\uff08JIT\uff09\u539f\u5219\u3002</li>\n    <li>GAM\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u8bb0\u5fc6\u5668\u548c\u7814\u7a76\u8005\uff0c\u5206\u522b\u7528\u4e8e\u7ba1\u7406\u5386\u53f2\u4fe1\u606f\u548c\u5728\u7ebf\u68c0\u7d22\u6709\u7528\u4fe1\u606f\u3002</li>\n    <li>\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u6709\u6548\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6574\u4f53\u6027\u80fd\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cGAM\u5728\u591a\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684\u4efb\u52a1\u5b8c\u6210\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5185\u5b58\u7cfb\u7edf\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n  <li>Memory is very important for AI agents, but traditional static memory can lead to significant information loss.</li>\n  <li>We introduce a new system called general agentic memory (GAM) that creates memory when needed, rather than in advance.</li>\n  <li>GAM consists of two main parts: a Memorizer that keeps important past information, and a Researcher that finds useful information when needed.</li>\n  <li>This system helps AI agents use large language models more effectively and improves their overall performance.</li>\n  <li>Tests show that GAM performs much better in memory-related tasks compared to existing memory systems.</li>\n</ul>"
  },
  "2511.19304": {
    "summary_zh": "<ul>\n    <li>\u4eba\u7c7b\u80fd\u5feb\u901f\u9002\u5e94\u4e0d\u540c\u73af\u5883\uff0c\u901a\u8fc7\u5b66\u4e60\u4e0d\u540c\u4e16\u754c\u7684\u89c4\u5219\u3002</li>\n    <li>\u73b0\u6709\u7684\u667a\u80fd\u4f53\u901a\u5e38\u53ea\u5728\u5355\u4e00\u73af\u5883\u4e2d\u81ea\u6211\u8fdb\u5316\uff0c\u5047\u8bbe\u73af\u5883\u662f\u56fa\u5b9a\u7684\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86AutoEnv\uff0c\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u53ef\u4ee5\u4f4e\u6210\u672c\u751f\u6210\u591a\u6837\u5316\u7684\u73af\u5883\u3002</li>\n    <li>\u4f7f\u7528AutoEnv\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u5305\u542b36\u4e2a\u73af\u5883\u548c358\u4e2a\u9a8c\u8bc1\u5173\u5361\u7684\u6570\u636e\u96c6AutoEnv-36\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u56fa\u5b9a\u7684\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u6548\u679c\u4e0b\u964d\uff0c\u800c\u73af\u5883\u81ea\u9002\u5e94\u7684\u5b66\u4e60\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li>Humans can adapt to different environments by learning rules, but current agents mainly improve in one fixed environment.</li>\n    <li>There is a lack of standardized environments and methods for measuring learning across different settings.</li>\n    <li>AutoEnv is introduced as a new framework to easily create diverse environments for testing agents.</li>\n    <li>AutoEnv-36 is a dataset with 36 environments, where agents achieved only modest rewards, highlighting its challenges.</li>\n    <li>Learning methods need to adapt to different environments, but their effectiveness decreases as complexity increases, indicating limitations in current approaches.</li>\n</ul>"
  },
  "2511.19365": {
    "summary_zh": "<ul>\n    <li>\u50cf\u7d20\u6269\u6563\u76f4\u63a5\u5728\u50cf\u7d20\u7a7a\u95f4\u751f\u6210\u56fe\u50cf\uff0c\u907f\u514d\u4e86VAE\u7684\u9650\u5236\uff0c\u6a21\u578b\u80fd\u529b\u66f4\u5f3a\u3002</li>\n    <li>\u73b0\u6709\u7684\u50cf\u7d20\u6269\u6563\u6a21\u578b\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u8f83\u6162\uff0c\u56e0\u4e3a\u5b83\u4eec\u5728\u540c\u4e00\u4e2a\u6269\u6563\u53d8\u6362\u5668\u4e2d\u5904\u7406\u9ad8\u9891\u548c\u4f4e\u9891\u4fe1\u53f7\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u9891\u7387\u89e3\u8026\u50cf\u7d20\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u9ad8\u4f4e\u9891\u7ec4\u4ef6\u7684\u751f\u6210\uff0c\u63d0\u9ad8\u6548\u7387\u3002</li>\n    <li>\u5f15\u5165\u9891\u7387\u611f\u77e5\u6d41\u5339\u914d\u635f\u5931\uff0c\u5f3a\u8c03\u91cd\u8981\u9891\u7387\uff0c\u6291\u5236\u4e0d\u91cd\u8981\u7684\u9891\u7387\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cDeCo\u5728\u50cf\u7d20\u6269\u6563\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0cFID\u5f97\u5206\u5206\u522b\u4e3a1.62\u548c2.22\uff0c\u5e76\u5728\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e2d\u53d6\u5f97\u4e860.86\u7684\u9886\u5148\u5206\u6570\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li>Pixel diffusion creates images directly in pixel space, improving on traditional two-stage methods.</li>\n    <li>Current models are slow because they handle both high and low-frequency image details together.</li>\n    <li>The new framework, called frequency-DeCoupled pixel diffusion, separates the generation of high and low-frequency components for efficiency.</li>\n    <li>A lightweight decoder focuses on adding high-frequency details while a main model handles low-frequency information.</li>\n    <li>Experiments show this approach outperforms others, achieving impressive scores on image quality benchmarks.</li>\n</ul>"
  },
  "2511.19399": {
    "summary_zh": "<ul>\n    <li>\u6df1\u5ea6\u7814\u7a76\u6a21\u578b\u53ef\u4ee5\u8fdb\u884c\u591a\u6b65\u9aa4\u7684\u7814\u7a76\uff0c\u751f\u6210\u957f\u7bc7\u3001\u51c6\u786e\u7684\u7b54\u6848\u3002</li>\n    <li>\u5927\u591a\u6570\u5f00\u653e\u7684\u6df1\u5ea6\u7814\u7a76\u6a21\u578b\u53ea\u5728\u77ed\u671f\u95ee\u7b54\u4efb\u52a1\u4e0a\u8bad\u7ec3\uff0c\u65e0\u6cd5\u5904\u7406\u771f\u5b9e\u7684\u957f\u7bc7\u4efb\u52a1\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u79f0\u4e3a\u6f14\u53d8\u8bc4\u5206\u6807\u51c6\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLER\uff09\uff0c\u4f7f\u8bc4\u5206\u6807\u51c6\u4e0e\u8bad\u7ec3\u4e2d\u7684\u6a21\u578b\u5171\u540c\u6f14\u53d8\u3002</li>\n    <li>\u4f7f\u7528RLER\uff0c\u6211\u4eec\u5f00\u53d1\u4e86DR Tulu\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4e13\u95e8\u4e3a\u5f00\u653e\u5f0f\u957f\u7bc7\u6df1\u5ea6\u7814\u7a76\u8bad\u7ec3\u7684\u5f00\u653e\u6a21\u578b\u3002</li>\n    <li>DR Tulu\u5728\u79d1\u5b66\u3001\u533b\u7597\u548c\u4e00\u822c\u9886\u57df\u7684\u957f\u7bc7\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u6bd4\u73b0\u6709\u7684\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u66f4\u5c0f\u3001\u66f4\u4fbf\u5b9c\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li>Deep research models are designed to give detailed answers but struggle with long-form tasks.</li>\n    <li>Most existing models are trained on short questions and answers, which limits their ability for complex research.</li>\n    <li>The new method, Reinforcement Learning with Evolving Rubrics (RLER), helps improve the training process by adapting feedback as the model learns.</li>\n    <li>DR Tulu-8B is the first open model trained specifically for long-form research and performs better than other models in various fields.</li>\n    <li>All related data, models, and code are shared publicly to support future research efforts.</li>\n</ul>"
  },
  "2510.25741": {
    "summary_zh": "<ul>\n    <li>\u73b0\u4ee3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e3b\u8981\u901a\u8fc7\u751f\u6210\u6587\u672c\u6765\u8fdb\u884c\u63a8\u7406\uff0c\u8fd9\u79cd\u65b9\u6cd5\u5728\u8bad\u7ec3\u540e\u8fdb\u884c\u63a8\u7406\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u9884\u8bad\u7ec3\u6570\u636e\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u5e76\u5f00\u6e90\u4e86Ouro\uff0c\u8fd9\u662f\u4e00\u4e2a\u9884\u8bad\u7ec3\u7684\u5faa\u73af\u8bed\u8a00\u6a21\u578b\uff08LoopLM\uff09\uff0c\u5b83\u5c06\u63a8\u7406\u5185\u7f6e\u4e8e\u9884\u8bad\u7ec3\u9636\u6bb5\u3002</li>\n    <li>Ouro\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u8fed\u4ee3\u8ba1\u7b97\u3001\u71b5\u6b63\u5219\u5316\u76ee\u6807\u4ee5\u53ca\u6269\u5c55\u52307.7\u4e07\u4ebf\u4e2a\u6807\u8bb0\u6765\u5b9e\u73b0\u5176\u6027\u80fd\u3002</li>\n    <li>Ouro\u76841.4B\u548c2.6B\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u4e0e\u9ad8\u8fbe12B\u53c2\u6570\u7684\u6700\u65b0\u6a21\u578b\u76f8\u5f53\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0cLoopLM\u5728\u63a8\u7406\u65b9\u9762\u7684\u6027\u80fd\u4f18\u8d8a\uff0c\u53ef\u80fd\u662f\u63a8\u7406\u65f6\u4ee3\u7684\u4e00\u79cd\u65b0\u6269\u5c55\u65b9\u5411\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li>Ouro is a new type of language model that improves reasoning during its training process.</li>\n    <li>It uses innovative methods like iterative computation and a special training objective to enhance performance.</li>\n    <li>The models (1.4B and 2.6B parameters) perform as well as larger models (up to 12B parameters) on various tests.</li>\n    <li>Ouro's advantage comes from better knowledge manipulation, not just more information.</li>\n    <li>The reasoning process in Ouro is more aligned with the final outputs compared to traditional methods.</li>\n</ul>"
  },
  "2511.14993": {
    "summary_zh": "<ul>\n    <li>\u4ecb\u7ecdKandinsky 5.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c10\u79d2\u89c6\u9891\u5408\u6210\u7684\u5148\u8fdb\u57fa\u7840\u6a21\u578b\u7cfb\u5217\u3002</li>\n    <li>Kandinsky 5.0\u5305\u62ec\u4e09\u79cd\u6838\u5fc3\u6a21\u578b\uff1a6B\u53c2\u6570\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u30012B\u53c2\u6570\u7684\u5feb\u901f\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u548c19B\u53c2\u6570\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002</li>\n    <li>\u8be6\u7ec6\u8bf4\u660e\u4e86\u6570\u636e\u6574\u7406\u751f\u547d\u5468\u671f\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u5904\u7406\u3001\u8fc7\u6ee4\u548c\u805a\u7c7b\uff0c\u4ee5\u652f\u6301\u591a\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>\u5c55\u793a\u4e86\u65b0\u9896\u7684\u67b6\u6784\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u4f18\u5316\uff0c\u4f7fKandinsky 5.0\u5728\u751f\u6210\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002</li>\n    <li>\u5e0c\u671b\u901a\u8fc7\u53d1\u5e03\u5f00\u6e90\u4ee3\u7801\u548c\u8bad\u7ec3\u68c0\u67e5\u70b9\uff0c\u63a8\u52a8\u9ad8\u8d28\u91cf\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u548c\u4fbf\u5229\u6027\u3002 </li>\n</ul>",
    "summary_simple": "<ul>\n    <li>Kandinsky 5.0 is a new set of advanced models for creating high-quality images and short videos.</li>\n    <li>It includes three main models: Kandinsky 5.0 Image Lite for image generation, Kandinsky 5.0 Video Lite for quick video creation, and Kandinsky 5.0 Video Pro for top-quality video generation.</li>\n    <li>The report details the process of gathering and preparing data for training these models, including various techniques to improve quality.</li>\n    <li>Kandinsky 5.0 features improvements in its design and training methods, allowing it to generate high-quality content quickly.</li>\n    <li>The models are open-source, making them accessible for researchers and developers to use in different creative projects.</li>\n</ul>"
  },
  "2511.04570": {
    "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u201c\u89c6\u9891\u601d\u7ef4\u201d\u65b0\u8303\u5f0f\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u6587\u672c\u548c\u56fe\u50cf\u601d\u7ef4\u7684\u5c40\u9650\u6027\u3002</li>\n    <li>\u89c6\u9891\u601d\u7ef4\u5229\u7528\u89c6\u9891\u751f\u6210\u6a21\u578b\uff08\u5982Sora-2\uff09\u6765\u5b9e\u73b0\u89c6\u89c9\u548c\u6587\u672c\u63a8\u7406\u7684\u7edf\u4e00\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u89c6\u9891\u601d\u7ef4\u57fa\u51c6\uff08VideoThinkBench\uff09\uff0c\u5305\u542b\u89c6\u89c9\u548c\u6587\u672c\u4efb\u52a1\u4e24\u7c7b\u3002</li>\n    <li>Sora-2\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684VLMs\u76f8\u5f53\uff0c\u5e76\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u8d85\u8d8a\u5b83\u4eec\u3002</li>\n    <li>Sora-2\u5728\u6587\u672c\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u51c6\u786e\u7387\u8fbe\u523092%\u548c75.53%\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li> \"Thinking with Text\" and \"Thinking with Images\" improve reasoning in language and vision models but have limitations.</li>\n    <li> Images can only show single moments and don't capture ongoing changes; separating text and images limits understanding.</li>\n    <li> The new \"Thinking with Video\" approach uses video generation to connect visual and textual reasoning over time.</li>\n    <li> We created the Video Thinking Benchmark to test this approach with vision-centric and text-centric tasks.</li>\n    <li> The Sora-2 model performs well, matching or exceeding other models in many tasks, showing potential for unified multimodal reasoning.</li>\n</ul>"
  },
  "2511.08892": {
    "summary_zh": "<ul>\n    <li>Lumine\u662f\u7b2c\u4e00\u4e2a\u5f00\u653e\u7684\u901a\u7528\u667a\u80fd\u4f53\u5f00\u53d1\u914d\u65b9\uff0c\u80fd\u591f\u5728\u590d\u6742\u76843D\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u5b9e\u65f6\u5b8c\u6210\u957f\u8fbe\u6570\u5c0f\u65f6\u7684\u4efb\u52a1\u3002</li>\n    <li>\u5b83\u91c7\u7528\u7c7b\u4f3c\u4eba\u7c7b\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u5c06\u611f\u77e5\u3001\u63a8\u7406\u548c\u884c\u52a8\u7edf\u4e00\u5728\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u7cfb\u7edf\u4e2d\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u3002</li>\n    <li>Lumine\u5728\u300a\u539f\u795e\u300b\u4e2d\u8bad\u7ec3\uff0c\u80fd\u591f\u4e0e\u4eba\u7c7b\u6548\u7387\u76f8\u5f53\u5730\u5b8c\u6210\u4e94\u5c0f\u65f6\u7684\u4e3b\u7ebf\u4efb\u52a1\uff0c\u5e76\u6839\u636e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6267\u884c\u5404\u79cd\u4efb\u52a1\u3002</li>\n    <li>\u5b83\u4e0d\u4ec5\u5728\u7279\u5b9a\u6e38\u620f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fd8\u80fd\u591f\u5728\u6ca1\u6709\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5176\u4ed6\u6e38\u620f\u4e2d\u5b8c\u6210\u957f\u8fbe100\u5206\u949f\u7684\u4efb\u52a1\u3002</li>\n    <li>Lumine\u7684\u6210\u529f\u5c55\u793a\u4e86\u5728\u5f00\u653e\u73af\u5883\u4e2d\u5f00\u53d1\u901a\u7528\u667a\u80fd\u4f53\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li>Lumine is a new open recipe for creating generalist agents that can handle long, complex tasks in 3D open-world games.</li>\n    <li>It uses a human-like approach to combine sensing, thinking, and acting, using a vision-language model.</li>\n    <li>Lumine processes visual information quickly and efficiently to perform actions based on natural language commands.</li>\n    <li>It can complete the five-hour main storyline of Genshin Impact at a level similar to human players and follow various instructions across different tasks.</li>\n    <li>Lumine shows strong performance in other games without needing extra training, successfully completing missions in Wuthering Waves and Honkai: Star Rail.</li>\n</ul>"
  },
  "2511.11793": {
    "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86MiroThinker v1.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u7814\u7a76\u4ee3\u7406\uff0c\u65e8\u5728\u589e\u5f3a\u5de5\u5177\u8f85\u52a9\u63a8\u7406\u548c\u4fe1\u606f\u83b7\u53d6\u80fd\u529b\u3002</li>\n    <li>MiroThinker\u901a\u8fc7\u5728\u6a21\u578b\u5c42\u9762\u63a2\u7d22\u4ea4\u4e92\u6269\u5c55\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u4ee3\u7406\u4e0e\u73af\u5883\u4e4b\u95f4\u7684\u4e92\u52a8\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u7684\u4ea4\u4e92\u6269\u5c55\uff0c\u652f\u6301\u591a\u8fbe600\u6b21\u5de5\u5177\u8c03\u7528\uff0c\u9002\u5e94\u590d\u6742\u7684\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>MiroThinker\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8d85\u8fc7\u4e86\u4ee5\u5f80\u7684\u5f00\u6e90\u4ee3\u7406\uff0c\u63a5\u8fd1\u5546\u4e1a\u6a21\u578b\u7684\u6c34\u5e73\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u4ea4\u4e92\u6df1\u5ea6\u7684\u6269\u5c55\u4e0e\u6a21\u578b\u7684\u89c4\u6a21\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u5177\u6709\u76f8\u4f3c\u7684\u6269\u5c55\u7279\u6027\uff0c\u662f\u6784\u5efa\u4e0b\u4e00\u4ee3\u5f00\u653e\u7814\u7a76\u4ee3\u7406\u7684\u91cd\u8981\u7ef4\u5ea6\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li>MiroThinker v1.0 is an open-source research agent aimed at improving reasoning and information-seeking skills.</li>\n    <li>It focuses on enhancing how the model interacts with its environment, rather than just increasing model size or context length.</li>\n    <li>The model uses reinforcement learning to effectively manage many tool calls (up to 600) during tasks, supporting complex research activities.</li>\n    <li>MiroThinker outperforms previous open-source agents in various benchmarks, showing accuracy rates that approach those of advanced commercial models.</li>\n    <li>The research highlights that better performance comes from deeper and more frequent interactions with the environment, suggesting that this interaction is crucial for future research agent development.</li>\n</ul>"
  },
  "2511.13254": {
    "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u8017\u65f6\u4e14\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002</li>\n    <li>\u6a21\u578b\u201c\u6df7\u5408\u201d\uff08model souping\uff09\u901a\u8fc7\u5bf9\u591a\u4e2a\u76f8\u540c\u67b6\u6784\u7684\u6a21\u578b\u8fdb\u884c\u52a0\u6743\u5e73\u5747\uff0c\u80fd\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6027\u80fd\u3002</li>\n    <li>\u672c\u6587\u4ecb\u7ecd\u4e86\u201c\u7c7b\u522b\u4e13\u5bb6\u6df7\u5408\u201d\uff08SoCE\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u51c6\u7ec4\u5408\u6765\u9009\u62e9\u6700\u4f73\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u975e\u5747\u5300\u52a0\u6743\u5e73\u5747\u6765\u4f18\u5316\u6027\u80fd\u3002</li>\n    <li>SoCE\u65b9\u6cd5\u5229\u7528\u57fa\u51c6\u7c7b\u522b\u4e4b\u95f4\u7684\u4f4e\u76f8\u5173\u6027\u6765\u8bc6\u522b\u201c\u4e13\u5bb6\u201d\u6a21\u578b\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u4f18\u5316\u52a0\u6743\u5408\u5e76\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u80fd\u529b\u3001\u5de5\u5177\u8c03\u7528\u548c\u6570\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u4f2f\u514b\u5229\u51fd\u6570\u8c03\u7528\u6392\u884c\u699c\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u6210\u7ee9\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are powerful but require a lot of resources and time to train.</li>\n    <li>Model souping, or averaging weights from multiple models, can improve performance without needing extensive retraining.</li>\n    <li>This paper presents Soup Of Category Experts (SoCE), a new method for model souping that selects the best models based on benchmark categories.</li>\n    <li>SoCE uses weighted averaging to combine models, focusing on those that perform well in related areas instead of using equal weights for all models.</li>\n    <li>The method shows better performance and reliability in various tasks, achieving top results in certain benchmarks.</li>\n</ul>"
  },
  "2511.13612": {
    "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u79d1\u5b66\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u5c24\u5176\u662f\u5728\u89e3\u51b3\u7269\u7406\u95ee\u9898\u4e0a\u3002</li>\n    <li>P1\u662f\u4e00\u4e2a\u5f00\u6e90\u7269\u7406\u63a8\u7406\u6a21\u578b\u7cfb\u5217\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u800c\u6210\u3002</li>\n    <li>P1-235B-A22B\u662f\u9996\u4e2a\u5728\u56fd\u9645\u7269\u7406\u5965\u6797\u5339\u514b\u7ade\u8d5b\u4e2d\u83b7\u5f97\u91d1\u724c\u7684\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u57282024/2025\u5e74\u8d62\u5f97\u4e8612\u679a\u91d1\u724c\u3002</li>\n    <li>P1-30B-A3B\u5728\u56fd\u9645\u7269\u7406\u5965\u6797\u5339\u514b\u7ade\u8d5b\u4e2d\u83b7\u5f97\u94f6\u724c\uff0c\u8d85\u8d8a\u4e86\u51e0\u4e4e\u6240\u6709\u5176\u4ed6\u5f00\u6e90\u6a21\u578b\u3002</li>\n    <li>P1\u7cfb\u5217\u6a21\u578b\u5728\u6570\u5b66\u548c\u7f16\u7801\u7b49\u5176\u4ed6\u63a8\u7406\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u51fa\u8272\uff0c\u663e\u793a\u4e86\u5176\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li>Recent advancements in large language models (LLMs) have improved their ability to solve complex scientific problems, especially in physics.</li>\n    <li>The P1 models are a new series of open-source models designed specifically for physics reasoning, trained using reinforcement learning.</li>\n    <li>P1-235B-A22B is the first open-source model to achieve Gold-medal performance at the International Physics Olympiad (IPhO) 2025.</li>\n    <li>The P1 models also excel in other reasoning tasks such as math and coding, demonstrating their versatility.</li>\n    <li>P1-30B-A3B earned a silver medal at the IPhO 2025, showing strong performance among open-source models.</li>\n</ul>"
  },
  "2511.06221": {
    "summary_zh": "<ul>\n    <li>\u62a5\u544a\u4ecb\u7ecd\u4e86VibeThinker-1.5B\uff0c\u4e00\u4e2a\u5177\u670915\u4ebf\u53c2\u6570\u7684\u5c0f\u578b\u6a21\u578b\uff0c\u6311\u6218\u4e86\u5c0f\u6a21\u578b\u7f3a\u4e4f\u63a8\u7406\u80fd\u529b\u7684\u4f20\u7edf\u89c2\u70b9\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7Spectrum-to-Signal Principle (SSP)\u5f00\u53d1\uff0c\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u7684\u591a\u6837\u6027\u63a2\u7d22\u84b8\u998f\u548c\u6700\u5927\u71b5\u5f15\u5bfc\u7684\u7b56\u7565\u4f18\u5316\u3002</li>\n    <li>VibeThinker-1.5B\u7684\u8bad\u7ec3\u6210\u672c\u4ec5\u4e3a7800\u7f8e\u5143\uff0c\u63a8\u7406\u80fd\u529b\u4f18\u4e8e\u4e00\u4e9b\u95ed\u6e90\u6a21\u578b\uff0c\u5e76\u4e0e\u5f00\u6e90\u6a21\u578b\u76f8\u5f53\u3002</li>\n    <li>\u5728\u4e09\u9879\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVibeThinker-1.5B\u7684\u8868\u73b0\u8d85\u8fc7\u4e86400\u500d\u66f4\u5927\u7684DeepSeek R1\u3002</li>\n    <li>\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u5c0f\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u4e0e\u5927\u6a21\u578b\u76f8\u4f3c\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4ece\u800c\u964d\u4f4e\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\uff0c\u4fc3\u8fdb\u5148\u8fdbAI\u7814\u7a76\u7684\u666e\u53ca\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li>VibeThinker-1.5B is a new 1.5 billion parameter model that challenges the idea that smaller models can't reason well.</li>\n    <li>This model uses a unique approach called the Spectrum-to-Signal Principle (SSP) to improve its performance without needing a massive number of parameters.</li>\n    <li>It was trained at a low cost of $7,800 and shows better reasoning than some larger closed-source models.</li>\n    <li>VibeThinker-1.5B outperformed the much larger DeepSeek R1 on three math tests, achieving better scores.</li>\n    <li>This research suggests that smaller models can match the reasoning abilities of larger ones, making advanced AI more accessible and cost-effective.</li>\n</ul>"
  },
  "2511.21691": {
    "summary_zh": "<ul>\n    <li>\u73b0\u4ee3\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u548c\u591a\u6837\u5316\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u7684\u7528\u6237\u63a7\u5236\u65f6\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u753b\u5e03\u5230\u56fe\u50cf\u201d\u6846\u67b6\uff0c\u5c06\u591a\u79cd\u63a7\u5236\u65b9\u5f0f\u6574\u5408\u5230\u4e00\u4e2a\u753b\u5e03\u754c\u9762\u4e2d\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u751f\u6210\u66f4\u7b26\u5408\u610f\u56fe\u7684\u56fe\u50cf\u3002</li>\n    <li>\u8be5\u6846\u67b6\u901a\u8fc7\u7f16\u7801\u4e0d\u540c\u7684\u63a7\u5236\u4fe1\u53f7\u4e3a\u4e00\u4e2a\u590d\u5408\u753b\u5e03\u56fe\u50cf\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u7efc\u5408\u7684\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5236\u5b9a\u4e86\u591a\u4efb\u52a1\u6570\u636e\u96c6\u548c\u591a\u4efb\u52a1\u753b\u5e03\u8bad\u7ec3\u7b56\u7565\uff0c\u4f18\u5316\u6269\u6563\u6a21\u578b\u4ee5\u5171\u540c\u7406\u89e3\u548c\u6574\u5408\u4e0d\u540c\u7684\u63a7\u5236\u65b9\u5f0f\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u201c\u753b\u5e03\u5230\u56fe\u50cf\u201d\u5728\u8eab\u4efd\u4fdd\u6301\u548c\u63a7\u5236\u9075\u5faa\u65b9\u9762\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u591a\u4eba\u7269\u6784\u56fe\u548c\u59ff\u52bf\u63a7\u5236\u751f\u6210\u7b49\u590d\u6742\u573a\u666f\u4e2d\u3002</li>\n</ul>",
    "summary_simple": "<ul>\n    <li>Modern diffusion models are good at creating quality images but have trouble with detailed controls when users want specific features.</li>\n    <li>Canvas-to-Image is a new system that allows users to combine different controls in one easy-to-use interface to create images that match their needs.</li>\n    <li>The system uses a single composite canvas image to help the model understand and generate visuals based on multiple different inputs.</li>\n    <li>Canvas-to-Image includes a special training method that helps it learn to handle various controls together instead of needing separate approaches for each task.</li>\n    <li>Tests show that Canvas-to-Image works better than other methods in keeping identities and following user instructions in complex scenarios.</li>\n</ul>"
  }
}
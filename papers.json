[{"paper":{"id":"2511.21678","authors":[{"_id":"692918571f3fae537858b537","name":"Weihao Bo","hidden":false},{"_id":"692918571f3fae537858b538","name":"Shan Zhang","hidden":false},{"_id":"692918571f3fae537858b539","name":"Yanpeng Sun","hidden":false},{"_id":"692918571f3fae537858b53a","name":"Jingjing Wu","hidden":false},{"_id":"692918571f3fae537858b53b","name":"Qunyi Xie","hidden":false},{"_id":"692918571f3fae537858b53c","name":"Xiao Tan","hidden":false},{"_id":"692918571f3fae537858b53d","name":"Kunbin Chen","hidden":false},{"_id":"692918571f3fae537858b53e","name":"Wei He","hidden":false},{"_id":"692918571f3fae537858b53f","name":"Xiaofan Li","hidden":false},{"_id":"692918571f3fae537858b540","name":"Na Zhao","hidden":false},{"_id":"692918571f3fae537858b541","name":"Jingdong Wang","hidden":false},{"_id":"692918571f3fae537858b542","name":"Zechao Li","hidden":false}],"publishedAt":"2025-11-26T18:55:08.000Z","submittedOnDailyAt":"2025-11-28T01:10:12.370Z","title":"Agentic Learner with Grow-and-Refine Multimodal Semantic Memory","submittedOnDailyBy":{"_id":"64297212e5f33939cf3a3d9b","avatarUrl":"/avatars/bd21759ab5d7e526b99fcb7ed813ffb3.svg","isPro":false,"fullname":"yanpeng_sun","user":"syp115","type":"user"},"summary":"MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.","upvotes":0,"discussionId":"692918571f3fae537858b543","ai_summary":"ViLoMem, a dual-stream memory framework, enhances MLLMs by preserving multimodal semantic knowledge, reducing errors, and improving accuracy across benchmarks.","ai_keywords":["memory-augmented agents","trajectory-based memory","brevity bias","semantic memory","multimodal problem-solving","visual attention","logical reasoning","ViLoMem","dual-stream memory","schema-based memory","visual distraction patterns","logical reasoning errors","pass@1 accuracy","catastrophic forgetting","lifelong learning","cross-domain learning"]},"publishedAt":"2025-11-26T13:55:08.000Z","title":"Agentic Learner with Grow-and-Refine Multimodal Semantic Memory","summary":"MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21678.png","numComments":1,"submittedBy":{"_id":"64297212e5f33939cf3a3d9b","avatarUrl":"/avatars/bd21759ab5d7e526b99fcb7ed813ffb3.svg","fullname":"yanpeng_sun","name":"syp115","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"isAuthorParticipating":false},{"paper":{"id":"2511.21662","authors":[{"_id":"6927d4bd243b2216fb75cdb6","name":"Tianyi Xiong","hidden":false},{"_id":"6927d4bd243b2216fb75cdb7","name":"Yi Ge","hidden":false},{"_id":"6927d4bd243b2216fb75cdb8","name":"Ming Li","hidden":false},{"_id":"6927d4bd243b2216fb75cdb9","name":"Zuolong Zhang","hidden":false},{"_id":"6927d4bd243b2216fb75cdba","name":"Pranav Kulkarni","hidden":false},{"_id":"6927d4bd243b2216fb75cdbb","name":"Kaishen Wang","hidden":false},{"_id":"6927d4bd243b2216fb75cdbc","name":"Qi He","hidden":false},{"_id":"6927d4bd243b2216fb75cdbd","name":"Zeying Zhu","hidden":false},{"_id":"6927d4bd243b2216fb75cdbe","name":"Chenxi Liu","hidden":false},{"_id":"6927d4bd243b2216fb75cdbf","name":"Ruibo Chen","hidden":false},{"_id":"6927d4bd243b2216fb75cdc0","name":"Tong Zheng","hidden":false},{"_id":"6927d4bd243b2216fb75cdc1","name":"Yanshuo Chen","hidden":false},{"_id":"6927d4bd243b2216fb75cdc2","name":"Xiyao Wang","hidden":false},{"_id":"6927d4bd243b2216fb75cdc3","name":"Renrui Zhang","hidden":false},{"_id":"6927d4bd243b2216fb75cdc4","name":"Wenhu Chen","hidden":false},{"_id":"6927d4bd243b2216fb75cdc5","name":"Heng Huang","hidden":false}],"publishedAt":"2025-11-26T18:35:17.000Z","submittedOnDailyAt":"2025-11-28T00:23:35.734Z","title":"Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following","submittedOnDailyBy":{"_id":"6570977f87a92b76922c9950","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6570977f87a92b76922c9950/AQGto1w6ugBvH2yCV46YU.jpeg","isPro":false,"fullname":"Tianyi Xiong","user":"txiong23","type":"user"},"summary":"Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.","upvotes":2,"discussionId":"6927d4be243b2216fb75cdc6","projectPage":"https://multi-crit.github.io/","ai_summary":"Multi-Crit evaluates multimodal models on following diverse criteria with metrics for pluralistic adherence, criterion-switching flexibility, and recognizing preference conflicts, revealing gaps in model capabilities.","ai_keywords":["LMMs","multimodal evaluation systems","instruction following","human preferences","multi-criterion human annotations","pluralistic criteria","criterion-level judgments","open-ended generation","verifiable reasoning tasks","holistic judgment signals","visual grounding","reasoning fine-tuning","test-time scaling","boundary consistency"],"organization":{"_id":"68b3c3bbc375e05b059370b2","name":"UMCP","fullname":"University of Maryland College Park","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68b3c2c3a4ea236d1a97871a/bji3nI5ZWm2r4JX_-HLo0.png"}},"publishedAt":"2025-11-26T13:35:17.000Z","title":"Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following","summary":"Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21662.png","numComments":1,"submittedBy":{"_id":"6570977f87a92b76922c9950","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6570977f87a92b76922c9950/AQGto1w6ugBvH2yCV46YU.jpeg","fullname":"Tianyi Xiong","name":"txiong23","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":7},"organization":{"_id":"68b3c3bbc375e05b059370b2","name":"UMCP","fullname":"University of Maryland College Park","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68b3c2c3a4ea236d1a97871a/bji3nI5ZWm2r4JX_-HLo0.png"},"isAuthorParticipating":false},{"paper":{"id":"2511.21087","authors":[{"_id":"6928a2681f3fae537858b4a4","name":"Ziyun Zeng","hidden":false},{"_id":"6928a2681f3fae537858b4a5","name":"Hang Hua","hidden":false},{"_id":"6928a2681f3fae537858b4a6","name":"Jiebo Luo","hidden":false}],"publishedAt":"2025-11-26T06:13:32.000Z","submittedOnDailyAt":"2025-11-28T00:06:07.849Z","title":"MIRA: Multimodal Iterative Reasoning Agent for Image Editing","submittedOnDailyBy":{"_id":"639f8277beb95d698de007dd","avatarUrl":"/avatars/57f223ccd9d3cb03166ccf0e41361c58.svg","isPro":false,"fullname":"HangHua","user":"hhua2","type":"user"},"summary":"Instruction-guided image editing offers an intuitive way for users to edit images with natural language. However, diffusion-based editing models often struggle to accurately interpret complex user instructions, especially those involving compositional relationships, contextual cues, or referring expressions, leading to edits that drift semantically or fail to reflect the intended changes. We tackle this problem by proposing MIRA (Multimodal Iterative Reasoning Agent), a lightweight, plug-and-play multimodal reasoning agent that performs editing through an iterative perception-reasoning-action loop, effectively simulating multi-turn human-model interaction processes. Instead of issuing a single prompt or static plan, MIRA predicts atomic edit instructions step by step, using visual feedback to make its decisions. Our 150K multimodal tool-use dataset, MIRA-Editing, combined with a two-stage SFT + GRPO training pipeline, enables MIRA to perform reasoning and editing over complex editing instructions. When paired with open-source image editing models such as Flux.1-Kontext, Step1X-Edit, and Qwen-Image-Edit, MIRA significantly improves both semantic consistency and perceptual quality, achieving performance comparable to or exceeding proprietary systems such as GPT-Image and Nano-Banana.","upvotes":4,"discussionId":"6928a2691f3fae537858b4a7","ai_summary":"MIRA, a multimodal reasoning agent, enhances diffusion-based image editing by iteratively interpreting complex instructions, improving both semantic consistency and perceptual quality.","ai_keywords":["diffusion-based editing models","multimodal reasoning agent","iterative perception-reasoning-action loop","atomic edit instructions","visual feedback","multimodal tool-use dataset","MIRA-Editing","two-stage SFT + GRPO training pipeline","semantic consistency","perceptual quality","Flux.1-Kontext","Step1X-Edit","Qwen-Image-Edit","GPT-Image","Nano-Banana"]},"publishedAt":"2025-11-26T01:13:32.000Z","title":"MIRA: Multimodal Iterative Reasoning Agent for Image Editing","summary":"Instruction-guided image editing offers an intuitive way for users to edit images with natural language. However, diffusion-based editing models often struggle to accurately interpret complex user instructions, especially those involving compositional relationships, contextual cues, or referring expressions, leading to edits that drift semantically or fail to reflect the intended changes. We tackle this problem by proposing MIRA (Multimodal Iterative Reasoning Agent), a lightweight, plug-and-play multimodal reasoning agent that performs editing through an iterative perception-reasoning-action loop, effectively simulating multi-turn human-model interaction processes. Instead of issuing a single prompt or static plan, MIRA predicts atomic edit instructions step by step, using visual feedback to make its decisions. Our 150K multimodal tool-use dataset, MIRA-Editing, combined with a two-stage SFT + GRPO training pipeline, enables MIRA to perform reasoning and editing over complex editing instructions. When paired with open-source image editing models such as Flux.1-Kontext, Step1X-Edit, and Qwen-Image-Edit, MIRA significantly improves both semantic consistency and perceptual quality, achieving performance comparable to or exceeding proprietary systems such as GPT-Image and Nano-Banana.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21087.png","numComments":1,"submittedBy":{"_id":"639f8277beb95d698de007dd","avatarUrl":"/avatars/57f223ccd9d3cb03166ccf0e41361c58.svg","fullname":"HangHua","name":"hhua2","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"isAuthorParticipating":false},{"paper":{"id":"2511.21688","authors":[{"_id":"6927d285243b2216fb75cdaa","user":{"_id":"645940c8f1f5263c2ef334ab","avatarUrl":"/avatars/e908686d0e1b2708adb0e30e596a6f39.svg","isPro":false,"fullname":"Wenbo Hu","user":"gordonhu","type":"user"},"name":"Wenbo Hu","status":"claimed_verified","statusLastChangedAt":"2025-11-27T09:58:34.612Z","hidden":false},{"_id":"6927d285243b2216fb75cdab","name":"Jingli Lin","hidden":false},{"_id":"6927d285243b2216fb75cdac","name":"Yilin Long","hidden":false},{"_id":"6927d285243b2216fb75cdad","name":"Yunlong Ran","hidden":false},{"_id":"6927d285243b2216fb75cdae","name":"Lihan Jiang","hidden":false},{"_id":"6927d285243b2216fb75cdaf","name":"Yifan Wang","hidden":false},{"_id":"6927d285243b2216fb75cdb0","name":"Chenming Zhu","hidden":false},{"_id":"6927d285243b2216fb75cdb1","name":"Runsen Xu","hidden":false},{"_id":"6927d285243b2216fb75cdb2","name":"Tai Wang","hidden":false},{"_id":"6927d285243b2216fb75cdb3","name":"Jiangmiao Pang","hidden":false}],"publishedAt":"2025-11-26T18:59:39.000Z","submittedOnDailyAt":"2025-11-27T18:52:14.636Z","title":"G^2VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning","submittedOnDailyBy":{"_id":"645940c8f1f5263c2ef334ab","avatarUrl":"/avatars/e908686d0e1b2708adb0e30e596a6f39.svg","isPro":false,"fullname":"Wenbo Hu","user":"gordonhu","type":"user"},"summary":"Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G^2VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G^2VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G^2VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G^2VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.","upvotes":6,"discussionId":"6927d286243b2216fb75cdb4","projectPage":"https://gordonhu608.github.io/g2vlm.github.io/","githubRepo":"https://github.com/InternRobotics/G2VLM","ai_summary":"G$^2$VLM integrates 3D geometry learning with vision-language models to enhance spatial understanding and reasoning, outperforming existing models in these tasks.","ai_keywords":["geometry grounded vision-language model","spatial 3D reconstruction","in-context learning","interleaved reasoning","multi-view image","video data","3D visual priors","spatial understanding","spatial reasoning","feed-forward 3D reconstruction models"],"githubStars":51,"organization":{"_id":"6881c146ff13df8b65153273","name":"InternRobotics","fullname":"Intern Robotics","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65d9f09bbcd15bc5cb255fed/REfA3nEK1_Y-PTfGn_5H1.jpeg"}},"publishedAt":"2025-11-26T13:59:39.000Z","title":"G^2VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning","summary":"Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G^2VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G^2VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G^2VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G^2VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21688.png","numComments":2,"submittedBy":{"_id":"645940c8f1f5263c2ef334ab","avatarUrl":"/avatars/e908686d0e1b2708adb0e30e596a6f39.svg","fullname":"Wenbo Hu","name":"gordonhu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6},"organization":{"_id":"6881c146ff13df8b65153273","name":"InternRobotics","fullname":"Intern Robotics","avatar":"https://cdn-uploads.huggingface.co/production/uploads/65d9f09bbcd15bc5cb255fed/REfA3nEK1_Y-PTfGn_5H1.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2511.18452","authors":[{"_id":"6927339d243b2216fb75cca7","user":{"_id":"646b8882e96a751c5252bf18","avatarUrl":"/avatars/14bbe1d53ce00e530b29fa3c6cac7a6b.svg","isPro":false,"fullname":"Loick ","user":"LChambon","type":"user"},"name":"Loick Chambon","status":"claimed_verified","statusLastChangedAt":"2025-11-27T09:59:23.747Z","hidden":false},{"_id":"6927339d243b2216fb75cca8","name":"Paul Couairon","hidden":false},{"_id":"6927339d243b2216fb75cca9","name":"Eloi Zablocki","hidden":false},{"_id":"6927339d243b2216fb75ccaa","name":"Alexandre Boulch","hidden":false},{"_id":"6927339d243b2216fb75ccab","name":"Nicolas Thome","hidden":false},{"_id":"6927339d243b2216fb75ccac","name":"Matthieu Cord","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/646b8882e96a751c5252bf18/qI0Bc9jvexA_xyCP-opqd.mp4"],"publishedAt":"2025-11-23T13:43:52.000Z","submittedOnDailyAt":"2025-11-27T18:19:03.334Z","title":"NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering","submittedOnDailyBy":{"_id":"646b8882e96a751c5252bf18","avatarUrl":"/avatars/14bbe1d53ce00e530b29fa3c6cac7a6b.svg","isPro":false,"fullname":"Loick ","user":"LChambon","type":"user"},"summary":"Vision Foundation Models (VFMs) extract spatially downsampled representations, posing challenges for pixel-level tasks. Existing upsampling approaches face a fundamental trade-off: classical filters are fast and broadly applicable but rely on fixed forms, while modern upsamplers achieve superior accuracy through learnable, VFM-specific forms at the cost of retraining for each VFM. We introduce Neighborhood Attention Filtering (NAF), which bridges this gap by learning adaptive spatial-and-content weights through Cross-Scale Neighborhood Attention and Rotary Position Embeddings (RoPE), guided solely by the high-resolution input image. NAF operates zero-shot: it upsamples features from any VFM without retraining, making it the first VFM-agnostic architecture to outperform VFM-specific upsamplers and achieve state-of-the-art performance across multiple downstream tasks. It maintains high efficiency, scaling to 2K feature maps and reconstructing intermediate-resolution maps at 18 FPS. Beyond feature upsampling, NAF demonstrates strong performance on image restoration, highlighting its versatility. Code and checkpoints are available at https://github.com/valeoai/NAF.","upvotes":1,"discussionId":"6927339d243b2216fb75ccad","ai_summary":"Neighborhood Attention Filtering (NAF) upsamples features from Vision Foundation Models without retraining, achieving state-of-the-art performance across tasks with high efficiency.","ai_keywords":["Neighborhood Attention Filtering (NAF)","Vision Foundation Models (VFMs)","spatially downsampled representations","upsampling approaches","classical filters","learnable upsamplers","Cross-Scale Neighborhood Attention","Rotary Position Embeddings (RoPE)","zero-shot upsampling","feature upsampling","image restoration"]},"publishedAt":"2025-11-23T08:43:52.000Z","title":"NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering","summary":"Vision Foundation Models (VFMs) extract spatially downsampled representations, posing challenges for pixel-level tasks. Existing upsampling approaches face a fundamental trade-off: classical filters are fast and broadly applicable but rely on fixed forms, while modern upsamplers achieve superior accuracy through learnable, VFM-specific forms at the cost of retraining for each VFM. We introduce Neighborhood Attention Filtering (NAF), which bridges this gap by learning adaptive spatial-and-content weights through Cross-Scale Neighborhood Attention and Rotary Position Embeddings (RoPE), guided solely by the high-resolution input image. NAF operates zero-shot: it upsamples features from any VFM without retraining, making it the first VFM-agnostic architecture to outperform VFM-specific upsamplers and achieve state-of-the-art performance across multiple downstream tasks. It maintains high efficiency, scaling to 2K feature maps and reconstructing intermediate-resolution maps at 18 FPS. Beyond feature upsampling, NAF demonstrates strong performance on image restoration, highlighting its versatility. Code and checkpoints are available at https://github.com/valeoai/NAF.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/646b8882e96a751c5252bf18/qI0Bc9jvexA_xyCP-opqd.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18452.png","numComments":3,"submittedBy":{"_id":"646b8882e96a751c5252bf18","avatarUrl":"/avatars/14bbe1d53ce00e530b29fa3c6cac7a6b.svg","fullname":"Loick ","name":"LChambon","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"isAuthorParticipating":true},{"paper":{"id":"2511.19504","authors":[{"_id":"69286865805584b280405112","name":"Subramanyam Sahoo","hidden":false},{"_id":"69286865805584b280405113","user":{"_id":"63a4754927f1f64ed7238dac","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg","isPro":false,"fullname":"Aman Chadha","user":"amanchadha","type":"user"},"name":"Aman Chadha","status":"claimed_verified","statusLastChangedAt":"2025-11-27T20:39:36.790Z","hidden":false},{"_id":"69286865805584b280405114","name":"Vinija Jain","hidden":false},{"_id":"69286865805584b280405115","name":"Divya Chaudhary","hidden":false}],"publishedAt":"2025-11-23T20:23:23.000Z","submittedOnDailyAt":"2025-11-27T13:18:43.979Z","title":"Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma","submittedOnDailyBy":{"_id":"63a4754927f1f64ed7238dac","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg","isPro":false,"fullname":"Aman Chadha","user":"amanchadha","type":"user"},"summary":"Reinforcement Learning from Human Feedback (RLHF) is widely used for aligning large language models, yet practitioners face a persistent puzzle: improving safety often reduces fairness, scaling to diverse populations becomes computationally intractable, and making systems robust often amplifies majority biases. We formalize this tension as the Alignment Trilemma: no RLHF system can simultaneously achieve (i) epsilon-representativeness across diverse human values, (ii) polynomial tractability in sample and compute complexity, and (iii) delta-robustness against adversarial perturbations and distribution shift. Through a complexity-theoretic analysis integrating statistical learning theory and robust optimization, we prove that achieving both representativeness (epsilon <= 0.01) and robustness (delta <= 0.001) for global-scale populations requires Omega(2^{d_context}) operations, which is super-polynomial in the context dimensionality. We show that current RLHF implementations resolve this trilemma by sacrificing representativeness: they collect only 10^3--10^4 samples from homogeneous annotator pools while 10^7--10^8 samples are needed for true global representation. Our framework provides a unified explanation for documented RLHF pathologies including preference collapse, sycophancy, and systematic bias amplification. We conclude with concrete directions for navigating these fundamental trade-offs through strategic relaxations of alignment requirements.","upvotes":1,"discussionId":"69286865805584b280405116","ai_summary":"The Alignment Trilemma in RLHF shows that achieving representativeness, tractability, and robustness is computationally infeasible, leading to trade-offs in current implementations.","ai_keywords":["Reinforcement Learning from Human Feedback (RLHF)","Alignment Trilemma","epsilon-representativeness","polynomial tractability","delta-robustness","complexity-theoretic analysis","statistical learning theory","robust optimization","preference collapse","sycophancy","systematic bias amplification"]},"publishedAt":"2025-11-23T15:23:23.000Z","title":"Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma","summary":"Reinforcement Learning from Human Feedback (RLHF) is widely used for aligning large language models, yet practitioners face a persistent puzzle: improving safety often reduces fairness, scaling to diverse populations becomes computationally intractable, and making systems robust often amplifies majority biases. We formalize this tension as the Alignment Trilemma: no RLHF system can simultaneously achieve (i) epsilon-representativeness across diverse human values, (ii) polynomial tractability in sample and compute complexity, and (iii) delta-robustness against adversarial perturbations and distribution shift. Through a complexity-theoretic analysis integrating statistical learning theory and robust optimization, we prove that achieving both representativeness (epsilon <= 0.01) and robustness (delta <= 0.001) for global-scale populations requires Omega(2^{d_context}) operations, which is super-polynomial in the context dimensionality. We show that current RLHF implementations resolve this trilemma by sacrificing representativeness: they collect only 10^3--10^4 samples from homogeneous annotator pools while 10^7--10^8 samples are needed for true global representation. Our framework provides a unified explanation for documented RLHF pathologies including preference collapse, sycophancy, and systematic bias amplification. We conclude with concrete directions for navigating these fundamental trade-offs through strategic relaxations of alignment requirements.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19504.png","numComments":2,"submittedBy":{"_id":"63a4754927f1f64ed7238dac","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg","fullname":"Aman Chadha","name":"amanchadha","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":10},"isAuthorParticipating":true},{"paper":{"id":"2511.20633","authors":[{"_id":"69286c60ab13296a93f862fa","user":{"_id":"640d8a26b03f4cd29f52acdd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1678608917790-noauth.png","isPro":false,"fullname":"Jiahui Zhang","user":"jasonzhango","type":"user"},"name":"Jiahui Zhang","status":"claimed_verified","statusLastChangedAt":"2025-11-27T20:39:30.552Z","hidden":false},{"_id":"69286c60ab13296a93f862fb","name":"Ze Huang","hidden":false},{"_id":"69286c60ab13296a93f862fc","name":"Chun Gu","hidden":false},{"_id":"69286c60ab13296a93f862fd","name":"Zipei Ma","hidden":false},{"_id":"69286c60ab13296a93f862fe","name":"Li Zhang","hidden":false}],"publishedAt":"2025-11-25T18:52:56.000Z","submittedOnDailyAt":"2025-11-27T12:53:33.787Z","title":"Reinforcing Action Policies by Prophesying","submittedOnDailyBy":{"_id":"640d8a26b03f4cd29f52acdd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1678608917790-noauth.png","isPro":false,"fullname":"Jiahui Zhang","user":"jasonzhango","type":"user"},"summary":"Vision-Language-Action (VLA) policies excel in aligning language, perception, and robot control. However, most VLAs are trained purely by imitation, which overfits to demonstrations, and is brittle under distribution shift. Reinforcement learning (RL) directly optimizes task reward and thus addresses this misalignment, but real-robot interaction is expensive and conventional simulators are hard to engineer and transfer. We address both data efficiency and optimization stability in VLA post-training via a learned world model and an RL procedure tailored to flow-based action heads. Specifically, we introduce Prophet, a unified action-to-video robot actuation pretrained across large-scale, heterogeneous robot data to learn reusable action-outcome dynamics. It is able to few-shot adapt to new robots, objects, and environments, yielding a rollout-ready simulator. Upon Prophet, we reinforce action policies with Flow-action-GRPO (FA-GRPO), which adapts Flow-GRPO to operate on VLA actions, and with FlowScale, a stepwise reweighting that rescales per-step gradients in the flow head. Together, Prophet, FA-GRPO, and FlowScale constitute ProphRL, a practical, data- and compute-efficient path to VLA post-training. Experiments show 5-17% success gains on public benchmarks and 24-30% gains on real robots across different VLA variants.","upvotes":2,"discussionId":"69286c61ab13296a93f862ff","projectPage":"https://logosroboticsgroup.github.io/ProphRL/","githubRepo":"https://github.com/LogosRoboticsGroup/ProphRL","ai_summary":"ProphRL enhances Vision-Language-Action policies through a learned world model and reinforcement learning tailored to flow-based action heads, improving data efficiency and optimization stability.","ai_keywords":["Vision-Language-Action","imitation","reinforcement learning","world model","Flow-action-GRPO","FlowScale","Prophet","rollout-ready simulator","ProphRL"],"githubStars":11,"organization":{"_id":"68fceb6d4be572720b6900d4","name":"LogosRobotics","fullname":"LogosRobotics"}},"publishedAt":"2025-11-25T13:52:56.000Z","title":"Reinforcing Action Policies by Prophesying","summary":"Vision-Language-Action (VLA) policies excel in aligning language, perception, and robot control. However, most VLAs are trained purely by imitation, which overfits to demonstrations, and is brittle under distribution shift. Reinforcement learning (RL) directly optimizes task reward and thus addresses this misalignment, but real-robot interaction is expensive and conventional simulators are hard to engineer and transfer. We address both data efficiency and optimization stability in VLA post-training via a learned world model and an RL procedure tailored to flow-based action heads. Specifically, we introduce Prophet, a unified action-to-video robot actuation pretrained across large-scale, heterogeneous robot data to learn reusable action-outcome dynamics. It is able to few-shot adapt to new robots, objects, and environments, yielding a rollout-ready simulator. Upon Prophet, we reinforce action policies with Flow-action-GRPO (FA-GRPO), which adapts Flow-GRPO to operate on VLA actions, and with FlowScale, a stepwise reweighting that rescales per-step gradients in the flow head. Together, Prophet, FA-GRPO, and FlowScale constitute ProphRL, a practical, data- and compute-efficient path to VLA post-training. Experiments show 5-17% success gains on public benchmarks and 24-30% gains on real robots across different VLA variants.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20633.png","numComments":2,"submittedBy":{"_id":"640d8a26b03f4cd29f52acdd","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1678608917790-noauth.png","fullname":"Jiahui Zhang","name":"jasonzhango","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1},"organization":{"_id":"68fceb6d4be572720b6900d4","name":"LogosRobotics","fullname":"LogosRobotics"},"isAuthorParticipating":true},{"paper":{"id":"2511.19413","authors":[{"_id":"69284dd2805584b2804050de","user":{"_id":"6887a18b7253a2e54262cfb5","avatarUrl":"/avatars/a1c2df1ac85fa6e940a34f216745645f.svg","isPro":false,"fullname":"zhaolong su","user":"rollingsu","type":"user"},"name":"Zhaolong Su","status":"claimed_verified","statusLastChangedAt":"2025-11-27T20:39:40.284Z","hidden":false},{"_id":"69284dd2805584b2804050df","name":"Wang Lu","hidden":false},{"_id":"69284dd2805584b2804050e0","name":"Hao Chen","hidden":false},{"_id":"69284dd2805584b2804050e1","name":"Sharon Li","hidden":false},{"_id":"69284dd2805584b2804050e2","name":"Jindong Wang","hidden":false}],"publishedAt":"2025-11-24T18:50:01.000Z","submittedOnDailyAt":"2025-11-27T10:41:07.823Z","title":"UniGame: Turning a Unified Multimodal Model Into Its Own Adversary","submittedOnDailyBy":{"_id":"6204cc0d522e40b4a18d86e2","avatarUrl":"/avatars/18daf2de5671e711dc745388dd60569d.svg","isPro":false,"fullname":"Jindong Wang","user":"jindongwang","type":"user"},"summary":"Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame","upvotes":9,"discussionId":"69284dd2805584b2804050e3","githubRepo":"https://github.com/AIFrontierLab/UniGame","ai_summary":"UniGame, a self-adversarial post-training framework, improves consistency, understanding, generation, and robustness in unified multimodal models by introducing a lightweight perturber at the shared token interface.","ai_keywords":["Unified Multimodal Models","self-adversarial","post-training framework","compact embeddings","reconstruction-rich representations","decision boundaries","cross-modal coherence","distributional shifts","adversarial shifts","adversarial self-play","multimodal foundation models"],"githubStars":5},"publishedAt":"2025-11-24T13:50:01.000Z","title":"UniGame: Turning a Unified Multimodal Model Into Its Own Adversary","summary":"Unified Multimodal Models (UMMs) have shown impressive performance in both understanding and generation with a single architecture. However, UMMs still exhibit a fundamental inconsistency: understanding favors compact embeddings, whereas generation favors reconstruction-rich representations. This structural trade-off produces misaligned decision boundaries, degraded cross-modal coherence, and heightened vulnerability under distributional and adversarial shifts. In this paper, we present UniGame, a self-adversarial post-training framework that directly targets the inconsistencies. By applying a lightweight perturber at the shared token interface, UniGame enables the generation branch to actively seek and challenge fragile understanding, turning the model itself into its own adversary. Experiments demonstrate that UniGame significantly improves the consistency (+4.6%). Moreover, it also achieves substantial improvements in understanding (+3.6%), generation (+0.02), out-of-distribution and adversarial robustness (+4.8% and +6.2% on NaturalBench and AdVQA). The framework is architecture-agnostic, introduces less than 1% additional parameters, and is complementary to existing post-training methods. These results position adversarial self-play as a general and effective principle for enhancing the coherence, stability, and unified competence of future multimodal foundation models. The official code is available at: https://github.com/AIFrontierLab/UniGame","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19413.png","numComments":2,"submittedBy":{"_id":"6204cc0d522e40b4a18d86e2","avatarUrl":"/avatars/18daf2de5671e711dc745388dd60569d.svg","fullname":"Jindong Wang","name":"jindongwang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3},"isAuthorParticipating":false},{"paper":{"id":"2511.15552","authors":[{"_id":"6920b3b2b5612535ed9554f9","name":"Artem Chervyakov","hidden":false},{"_id":"6920b3b2b5612535ed9554fa","user":{"_id":"6207bbdb2f97dabc41b3b32b","avatarUrl":"/avatars/6e568045b0ebaa05ac1d469d941d4c96.svg","isPro":false,"fullname":"Ulyana","user":"ulyanaisaeva","type":"user"},"name":"Ulyana Isaeva","status":"claimed_verified","statusLastChangedAt":"2025-11-27T20:40:04.347Z","hidden":false},{"_id":"6920b3b2b5612535ed9554fb","name":"Anton Emelyanov","hidden":false},{"_id":"6920b3b2b5612535ed9554fc","name":"Artem Safin","hidden":false},{"_id":"6920b3b2b5612535ed9554fd","name":"Maria Tikhonova","hidden":false},{"_id":"6920b3b2b5612535ed9554fe","name":"Alexander Kharitonov","hidden":false},{"_id":"6920b3b2b5612535ed9554ff","name":"Yulia Lyakh","hidden":false},{"_id":"6920b3b2b5612535ed955500","name":"Petr Surovtsev","hidden":false},{"_id":"6920b3b2b5612535ed955501","name":"Denis Shevelev","hidden":false},{"_id":"6920b3b2b5612535ed955502","name":"Vildan Saburov","hidden":false},{"_id":"6920b3b2b5612535ed955503","user":{"_id":"622b1f6b9f6139daa8e998ce","avatarUrl":"/avatars/842719c100a5969be75d04da97333675.svg","isPro":false,"fullname":"Vasily Konovalov","user":"Vasily","type":"user"},"name":"Vasily Konovalov","status":"claimed_verified","statusLastChangedAt":"2025-11-27T20:40:06.412Z","hidden":false},{"_id":"6920b3b2b5612535ed955504","user":{"_id":"636b867ecde3707d10999b96","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1667991162387-noauth.png","isPro":false,"fullname":"Rykov Elisei","user":"lmeribal","type":"user"},"name":"Elisei Rykov","status":"claimed_verified","statusLastChangedAt":"2025-11-27T20:40:09.480Z","hidden":false},{"_id":"6920b3b2b5612535ed955505","user":{"_id":"61dedb1b2066746d68b63adb","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/61dedb1b2066746d68b63adb/PLBHQxvbcay3qDJjY7HM3.jpeg","isPro":false,"fullname":"Ivan Sviridov","user":"univanxx","type":"user"},"name":"Ivan Sviridov","status":"claimed_verified","statusLastChangedAt":"2025-11-27T10:00:36.928Z","hidden":false},{"_id":"6920b3b2b5612535ed955506","name":"Amina Miftakhova","hidden":false},{"_id":"6920b3b2b5612535ed955507","name":"Ilseyar Alimova","hidden":false},{"_id":"6920b3b2b5612535ed955508","name":"Alexander Panchenko","hidden":false},{"_id":"6920b3b2b5612535ed955509","name":"Alexander Kapitanov","hidden":false},{"_id":"6920b3b2b5612535ed95550a","name":"Alena Fenogenova","hidden":false}],"publishedAt":"2025-11-19T15:43:53.000Z","submittedOnDailyAt":"2025-11-27T10:05:09.849Z","title":"Multimodal Evaluation of Russian-language Architectures","submittedOnDailyBy":{"_id":"61dedb1b2066746d68b63adb","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/61dedb1b2066746d68b63adb/PLBHQxvbcay3qDJjY7HM3.jpeg","isPro":false,"fullname":"Ivan Sviridov","user":"univanxx","type":"user"},"summary":"Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.","upvotes":73,"discussionId":"6920b3b2b5612535ed95550b","projectPage":"https://mera.a-ai.ru/en/multi","githubRepo":"https://github.com/MERA-Evaluation/MERA_MULTIMODAL/tree/main","ai_summary":"Mera Multi is an open multimodal evaluation framework for Russian-spoken architectures, addressing the lack of such benchmarks with 18 newly constructed tasks and a methodology to prevent benchmark leakage.","ai_keywords":["multimodal large language models","Mera Multi","multimodal evaluation framework","Russian language","multimodal abilities","benchmark leakage","watermarking"],"githubStars":11},"publishedAt":"2025-11-19T10:43:53.000Z","title":"Multimodal Evaluation of Russian-language Architectures","summary":"Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15552.png","numComments":3,"submittedBy":{"_id":"61dedb1b2066746d68b63adb","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/61dedb1b2066746d68b63adb/PLBHQxvbcay3qDJjY7HM3.jpeg","fullname":"Ivan Sviridov","name":"univanxx","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3},"isAuthorParticipating":true},{"paper":{"id":"2511.20410","authors":[{"_id":"6927f3c9243b2216fb75ce24","user":{"_id":"6927f08d48991e82e392a40b","avatarUrl":"/avatars/cfc69ddde26d0f32b6bcebff70060944.svg","isPro":false,"fullname":"bao tang","user":"Ttt82","type":"user"},"name":"Bao Tang","status":"claimed_verified","statusLastChangedAt":"2025-11-27T08:58:05.545Z","hidden":false},{"_id":"6927f3c9243b2216fb75ce25","name":"Shuai Zhang","hidden":false},{"_id":"6927f3c9243b2216fb75ce26","name":"Yueting Zhu","hidden":false},{"_id":"6927f3c9243b2216fb75ce27","name":"Jijun Xiang","hidden":false},{"_id":"6927f3c9243b2216fb75ce28","name":"Xin Yang","hidden":false},{"_id":"6927f3c9243b2216fb75ce29","name":"Li Yu","hidden":false},{"_id":"6927f3c9243b2216fb75ce2a","name":"Wenyu Liu","hidden":false},{"_id":"6927f3c9243b2216fb75ce2b","name":"Xinggang Wang","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6927f08d48991e82e392a40b/Y4-uv8I4XjOFDNZHjP6y4.png"],"publishedAt":"2025-11-25T15:36:20.000Z","submittedOnDailyAt":"2025-11-27T09:24:11.781Z","title":"Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs","submittedOnDailyBy":{"_id":"6927f08d48991e82e392a40b","avatarUrl":"/avatars/cfc69ddde26d0f32b6bcebff70060944.svg","isPro":false,"fullname":"bao tang","user":"Ttt82","type":"user"},"summary":"Timestep distillation is an effective approach for improving the generation efficiency of diffusion models. The Consistency Model (CM), as a trajectory-based framework, demonstrates significant potential due to its strong theoretical foundation and high-quality few-step generation. Nevertheless, current continuous-time consistency distillation methods still rely heavily on training data and computational resources, hindering their deployment in resource-constrained scenarios and limiting their scalability to diverse domains. To address this issue, we propose Trajectory-Backward Consistency Model (TBCM), which eliminates the dependence on external training data by extracting latent representations directly from the teacher model's generation trajectory. Unlike conventional methods that require VAE encoding and large-scale datasets, our self-contained distillation paradigm significantly improves both efficiency and simplicity. Moreover, the trajectory-extracted samples naturally bridge the distribution gap between training and inference, thereby enabling more effective knowledge transfer. Empirically, TBCM achieves 6.52 FID and 28.08 CLIP scores on MJHQ-30k under one-step generation, while reducing training time by approximately 40% compared to Sana-Sprint and saving a substantial amount of GPU memory, demonstrating superior efficiency without sacrificing quality. We further reveal the diffusion-generation space discrepancy in continuous-time consistency distillation and analyze how sampling strategies affect distillation performance, offering insights for future distillation research. GitHub Link: https://github.com/hustvl/TBCM.","upvotes":2,"discussionId":"6927f3c9243b2216fb75ce2c","githubRepo":"https://github.com/hustvl/TBCM","ai_summary":"TBCM, a self-contained trajectory-based distillation method, enhances diffusion model efficiency by eliminating external data dependency and improving knowledge transfer, achieving high-quality generation with reduced computational resources.","ai_keywords":["timestep distillation","diffusion models","Consistency Model","trajectory-based framework","continuous-time consistency distillation","latent representations","VAE encoding","FID","CLIP scores","one-step generation","diffusion-generation space discrepancy","sampling strategies"],"githubStars":13},"publishedAt":"2025-11-25T10:36:20.000Z","title":"Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs","summary":"Timestep distillation is an effective approach for improving the generation efficiency of diffusion models. The Consistency Model (CM), as a trajectory-based framework, demonstrates significant potential due to its strong theoretical foundation and high-quality few-step generation. Nevertheless, current continuous-time consistency distillation methods still rely heavily on training data and computational resources, hindering their deployment in resource-constrained scenarios and limiting their scalability to diverse domains. To address this issue, we propose Trajectory-Backward Consistency Model (TBCM), which eliminates the dependence on external training data by extracting latent representations directly from the teacher model's generation trajectory. Unlike conventional methods that require VAE encoding and large-scale datasets, our self-contained distillation paradigm significantly improves both efficiency and simplicity. Moreover, the trajectory-extracted samples naturally bridge the distribution gap between training and inference, thereby enabling more effective knowledge transfer. Empirically, TBCM achieves 6.52 FID and 28.08 CLIP scores on MJHQ-30k under one-step generation, while reducing training time by approximately 40% compared to Sana-Sprint and saving a substantial amount of GPU memory, demonstrating superior efficiency without sacrificing quality. We further reveal the diffusion-generation space discrepancy in continuous-time consistency distillation and analyze how sampling strategies affect distillation performance, offering insights for future distillation research. GitHub Link: https://github.com/hustvl/TBCM.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6927f08d48991e82e392a40b/Y4-uv8I4XjOFDNZHjP6y4.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20410.png","numComments":2,"submittedBy":{"_id":"6927f08d48991e82e392a40b","avatarUrl":"/avatars/cfc69ddde26d0f32b6bcebff70060944.svg","fullname":"bao tang","name":"Ttt82","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"isAuthorParticipating":true},{"paper":{"id":"2511.21579","authors":[{"_id":"69283a32805584b280405089","name":"Teng Hu","hidden":false},{"_id":"69283a32805584b28040508a","name":"Zhentao Yu","hidden":false},{"_id":"69283a32805584b28040508b","name":"Guozhen Zhang","hidden":false},{"_id":"69283a32805584b28040508c","name":"Zihan Su","hidden":false},{"_id":"69283a32805584b28040508d","name":"Zhengguang Zhou","hidden":false},{"_id":"69283a32805584b28040508e","name":"Youliang Zhang","hidden":false},{"_id":"69283a32805584b28040508f","name":"Yuan Zhou","hidden":false},{"_id":"69283a32805584b280405090","name":"Qinglin Lu","hidden":false},{"_id":"69283a32805584b280405091","name":"Ran Yi","hidden":false}],"publishedAt":"2025-11-26T16:53:05.000Z","submittedOnDailyAt":"2025-11-27T09:20:11.686Z","title":"Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy","submittedOnDailyBy":{"_id":"6484576450e57f3530ee6432","avatarUrl":"/avatars/16a780f94597e9f5a4d57a84879102d7.svg","isPro":false,"fullname":"huteng","user":"JTUplayer","type":"user"},"summary":"The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization.","upvotes":19,"discussionId":"69283a32805584b280405092","projectPage":"https://sjtuplayer.github.io/projects/Harmony/","githubRepo":"https://github.com/sjtuplayer/Harmony","ai_summary":"Harmony addresses audio-visual synchronization in generative AI by introducing a Cross-Task Synergy training paradigm, Global-Local Decoupled Interaction Module, and Synchronization-Enhanced CFG to improve alignment and fidelity.","ai_keywords":["joint diffusion process","Correspondence Drift","global attention mechanisms","temporal cues","intra-modal bias","Classifier-Free Guidance","Cross-Task Synergy","Global-Local Decoupled Interaction Module","Synchronization-Enhanced CFG"],"githubStars":20,"organization":{"_id":"6645f953c39288df638dbdd5","name":"Tencent-Hunyuan","fullname":"Tencent Hunyuan","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}},"publishedAt":"2025-11-26T11:53:05.000Z","title":"Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy","summary":"The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21579.png","numComments":2,"submittedBy":{"_id":"6484576450e57f3530ee6432","avatarUrl":"/avatars/16a780f94597e9f5a4d57a84879102d7.svg","fullname":"huteng","name":"JTUplayer","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"organization":{"_id":"6645f953c39288df638dbdd5","name":"Tencent-Hunyuan","fullname":"Tencent Hunyuan","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"},"isAuthorParticipating":false},{"paper":{"id":"2511.18005","authors":[{"_id":"69283863805584b28040507a","name":"Shengyuan Wang","hidden":false},{"_id":"69283863805584b28040507b","name":"Zhiheng Zheng","hidden":false},{"_id":"69283863805584b28040507c","name":"Yu Shang","hidden":false},{"_id":"69283863805584b28040507d","name":"Lixuan He","hidden":false},{"_id":"69283863805584b28040507e","name":"Yangcheng Yu","hidden":false},{"_id":"69283863805584b28040507f","name":"Fan Hangyu","hidden":false},{"_id":"69283863805584b280405080","user":{"_id":"6465d3bd63e7e09dd02e95c3","avatarUrl":"/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg","isPro":false,"fullname":"Jie Feng","user":"JJ-TMT","type":"user"},"name":"Jie Feng","status":"claimed_verified","statusLastChangedAt":"2025-11-27T20:39:44.514Z","hidden":false},{"_id":"69283863805584b280405081","name":"Qingmin Liao","hidden":false},{"_id":"69283863805584b280405082","name":"Yong Li","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/IU7BNgpDOnNDxRquLjD6N.jpeg"],"publishedAt":"2025-11-22T10:09:22.000Z","submittedOnDailyAt":"2025-11-27T09:13:59.930Z","title":"RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale","submittedOnDailyBy":{"_id":"6465d3bd63e7e09dd02e95c3","avatarUrl":"/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg","isPro":false,"fullname":"Jie Feng","user":"JJ-TMT","type":"user"},"summary":"City-scale 3D generation is of great importance for the development of embodied intelligence and world models. Existing methods, however, face significant challenges regarding quality, fidelity, and scalability in 3D world generation. Thus, we propose RAISECity, a Reality-Aligned Intelligent Synthesis Engine that creates detailed, City-scale 3D worlds. We introduce an agentic framework that leverages diverse multimodal foundation tools to acquire real-world knowledge, maintain robust intermediate representations, and construct complex 3D scenes. This agentic design, featuring dynamic data processing, iterative self-reflection and refinement, and the invocation of advanced multimodal tools, minimizes cumulative errors and enhances overall performance. Extensive quantitative experiments and qualitative analyses validate the superior performance of RAISECity in real-world alignment, shape precision, texture fidelity, and aesthetics level, achieving over a 90% win-rate against existing baselines for overall perceptual quality. This combination of 3D quality, reality alignment, scalability, and seamless compatibility with computer graphics pipelines makes RAISECity a promising foundation for applications in immersive media, embodied intelligence, and world models.","upvotes":1,"discussionId":"69283864805584b280405083","githubRepo":"https://github.com/tsinghua-fib-lab/RAISECity","ai_summary":"RAISECity generates high-quality, city-scale 3D worlds with real-world alignment, using an agentic framework with multimodal tools, iterative refinement, and advanced representations.","ai_keywords":["agentic framework","multimodal foundation tools","dynamic data processing","iterative self-reflection","reality-aligned","shape precision","texture fidelity","aesthetics level","perceptual quality","computer graphics pipelines"],"githubStars":3},"publishedAt":"2025-11-22T05:09:22.000Z","title":"RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale","summary":"City-scale 3D generation is of great importance for the development of embodied intelligence and world models. Existing methods, however, face significant challenges regarding quality, fidelity, and scalability in 3D world generation. Thus, we propose RAISECity, a Reality-Aligned Intelligent Synthesis Engine that creates detailed, City-scale 3D worlds. We introduce an agentic framework that leverages diverse multimodal foundation tools to acquire real-world knowledge, maintain robust intermediate representations, and construct complex 3D scenes. This agentic design, featuring dynamic data processing, iterative self-reflection and refinement, and the invocation of advanced multimodal tools, minimizes cumulative errors and enhances overall performance. Extensive quantitative experiments and qualitative analyses validate the superior performance of RAISECity in real-world alignment, shape precision, texture fidelity, and aesthetics level, achieving over a 90% win-rate against existing baselines for overall perceptual quality. This combination of 3D quality, reality alignment, scalability, and seamless compatibility with computer graphics pipelines makes RAISECity a promising foundation for applications in immersive media, embodied intelligence, and world models.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/IU7BNgpDOnNDxRquLjD6N.jpeg"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18005.png","numComments":2,"submittedBy":{"_id":"6465d3bd63e7e09dd02e95c3","avatarUrl":"/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg","fullname":"Jie Feng","name":"JJ-TMT","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1},"isAuthorParticipating":true},{"paper":{"id":"2511.20426","authors":[{"_id":"6928381b805584b280405072","name":"Hmrishav Bandyopadhyay","hidden":false},{"_id":"6928381b805584b280405073","name":"Nikhil Pinnaparaju","hidden":false},{"_id":"6928381b805584b280405074","name":"Rahim Entezari","hidden":false},{"_id":"6928381b805584b280405075","name":"Jim Scott","hidden":false},{"_id":"6928381b805584b280405076","name":"Yi-Zhe Song","hidden":false},{"_id":"6928381b805584b280405077","name":"Varun Jampani","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/638c81fa61eb51017518fa31/sJ5nwkavPQrYbJrbi3-mo.qt"],"publishedAt":"2025-11-25T15:52:58.000Z","submittedOnDailyAt":"2025-11-27T09:09:13.732Z","title":"Block Cascading: Training Free Acceleration of Block-Causal Video Models","submittedOnDailyBy":{"_id":"638c81fa61eb51017518fa31","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/f0eCrzBrxz7Y9n25WkZ2v.png","isPro":false,"fullname":"Hmrishav Bandyopadhyay","user":"Hmrishav","type":"user"},"summary":"Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: https://hmrishavbandy.github.io/block_cascading_page/","upvotes":4,"discussionId":"6928381b805584b280405078","projectPage":"https://hmrishavbandy.github.io/block_cascading_page/","ai_summary":"Block Cascading parallelizes video block generation, achieving significant speed improvements without compromising quality.","ai_keywords":["block-causal video generation","parallelization","denoised current blocks","sequential pipelines","parallel cascades","temporal parallelism","KV-recaching","context switches","interactive generation"],"organization":{"_id":"62e1573a6fb6e362b4a90690","name":"stabilityai","fullname":"Stability AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/643feeb67bc3fbde1385cc25/7vmYr2XwVcPtkLzac_jxQ.png"}},"publishedAt":"2025-11-25T10:52:58.000Z","title":"Block Cascading: Training Free Acceleration of Block-Causal Video Models","summary":"Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: https://hmrishavbandy.github.io/block_cascading_page/","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/638c81fa61eb51017518fa31/sJ5nwkavPQrYbJrbi3-mo.qt"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20426.png","numComments":3,"submittedBy":{"_id":"638c81fa61eb51017518fa31","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/f0eCrzBrxz7Y9n25WkZ2v.png","fullname":"Hmrishav Bandyopadhyay","name":"Hmrishav","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":11},"organization":{"_id":"62e1573a6fb6e362b4a90690","name":"stabilityai","fullname":"Stability AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/643feeb67bc3fbde1385cc25/7vmYr2XwVcPtkLzac_jxQ.png"},"isAuthorParticipating":false},{"paper":{"id":"2511.20814","authors":[{"_id":"69280993243b2216fb75ce47","user":{"_id":"620c1a977af55d45f5519914","avatarUrl":"/avatars/4d2fb7c1a5ad5dabdb8888fa2fe72e65.svg","isPro":true,"fullname":"Tanvirul Alam","user":"xashru","type":"user"},"name":"Md Tanvirul Alam","status":"claimed_verified","statusLastChangedAt":"2025-11-27T20:39:46.813Z","hidden":false},{"_id":"69280993243b2216fb75ce48","name":"Saksham Aggarwal","hidden":false},{"_id":"69280993243b2216fb75ce49","name":"Justin Yang Chae","hidden":false},{"_id":"69280993243b2216fb75ce4a","name":"Nidhi Rastogi","hidden":false}],"publishedAt":"2025-11-25T20:00:47.000Z","submittedOnDailyAt":"2025-11-27T06:03:14.739Z","title":"SPHINX: A Synthetic Environment for Visual Perception and Reasoning","submittedOnDailyBy":{"_id":"620c1a977af55d45f5519914","avatarUrl":"/avatars/4d2fb7c1a5ad5dabdb8888fa2fe72e65.svg","isPro":true,"fullname":"Tanvirul Alam","user":"xashru","type":"user"},"summary":"We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives. Sphinx procedurally generates puzzles using motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions, enabling both precise evaluation and large-scale dataset construction. The benchmark covers 25 task types spanning symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning.","upvotes":1,"discussionId":"69280994243b2216fb75ce4b","githubRepo":"https://github.com/xashru/sphinx","ai_summary":"Sphinx, a synthetic environment for visual perception and reasoning, evaluates large vision-language models and demonstrates that reinforcement learning with verifiable rewards improves model accuracy on diverse tasks.","ai_keywords":["synthetic environment","visual perception","reasoning","cognitive primitives","procedural generation","motifs","tiles","charts","icons","geometric primitives","ground-truth solutions","dataset construction","symmetry detection","geometric transformations","spatial reasoning","chart interpretation","sequence prediction","large vision-language models","GPT-5","reinforcement learning","verifiable rewards","multimodal reasoning"],"githubStars":0},"publishedAt":"2025-11-25T15:00:47.000Z","title":"SPHINX: A Synthetic Environment for Visual Perception and Reasoning","summary":"We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives. Sphinx procedurally generates puzzles using motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions, enabling both precise evaluation and large-scale dataset construction. The benchmark covers 25 task types spanning symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20814.png","numComments":2,"submittedBy":{"_id":"620c1a977af55d45f5519914","avatarUrl":"/avatars/4d2fb7c1a5ad5dabdb8888fa2fe72e65.svg","fullname":"Tanvirul Alam","name":"xashru","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false},"isAuthorParticipating":true},{"paper":{"id":"2511.21208","authors":[{"_id":"692808b8243b2216fb75ce41","user":{"_id":"63e24a0cf0740bec2bfd7f9e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63e24a0cf0740bec2bfd7f9e/VS786oOKIF9IqIm3xnniS.jpeg","isPro":false,"fullname":"Lucas Thil","user":"LucasThil","type":"user"},"name":"Lucas Thil","status":"claimed_verified","statusLastChangedAt":"2025-11-27T08:58:03.581Z","hidden":false},{"_id":"692808b8243b2216fb75ce42","name":"Jesse Read","hidden":false},{"_id":"692808b8243b2216fb75ce43","name":"Rim Kaddah","hidden":false},{"_id":"692808b8243b2216fb75ce44","name":"Guillaume Doquet","hidden":false}],"publishedAt":"2025-11-26T09:39:35.000Z","submittedOnDailyAt":"2025-11-27T05:47:11.194Z","title":"I-GLIDE: Input Groups for Latent Health Indicators in Degradation Estimation","submittedOnDailyBy":{"_id":"63e24a0cf0740bec2bfd7f9e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63e24a0cf0740bec2bfd7f9e/VS786oOKIF9IqIm3xnniS.jpeg","isPro":false,"fullname":"Lucas Thil","user":"LucasThil","type":"user"},"summary":"Accurate remaining useful life (RUL) prediction hinges on the quality of health indicators (HIs), yet existing methods often fail to disentangle complex degradation mechanisms in multi-sensor systems or quantify uncertainty in HI reliability. This paper introduces a novel framework for HI construction, advancing three key contributions. First, we adapt Reconstruction along Projected Pathways (RaPP) as a health indicator (HI) for RUL prediction for the first time, showing that it outperforms traditional reconstruction error metrics. Second, we show that augmenting RaPP-derived HIs with aleatoric and epistemic uncertainty quantification (UQ) via Monte Carlo dropout and probabilistic latent spaces- significantly improves RUL-prediction robustness. Third, and most critically, we propose indicator groups, a paradigm that isolates sensor subsets to model system-specific degradations, giving rise to our novel method, I-GLIDE which enables interpretable, mechanism-specific diagnostics. Evaluated on data sourced from aerospace and manufacturing systems, our approach achieves marked improvements in accuracy and generalizability compared to state-of-the-art HI methods while providing actionable insights into system failure pathways. This work bridges the gap between anomaly detection and prognostics, offering a principled framework for uncertainty-aware degradation modeling in complex systems.","upvotes":0,"discussionId":"692808b8243b2216fb75ce45","projectPage":"https://lucasandrei.com/pages/i_glide.html","githubRepo":"https://github.com/LucasStill/I-GLIDE","ai_summary":"A novel framework using RaPP and uncertainty quantification improves RUL prediction accuracy and interpretability in multi-sensor systems.","ai_keywords":["Reconstruction along Projected Pathways (RaPP)","aleatoric uncertainty","epistemic uncertainty","Monte Carlo dropout","probabilistic latent spaces","indicator groups","I-GLIDE","anomaly detection","prognostics"],"githubStars":3,"organization":{"_id":"691f39b236fe6b60136afe00","name":"orailix","fullname":"Orailix","avatar":"https://cdn-uploads.huggingface.co/production/uploads/654e1e137cac9d7b51418f83/EwhwHf5mYSV65VRf-5bXb.png"}},"publishedAt":"2025-11-26T04:39:35.000Z","title":"I-GLIDE: Input Groups for Latent Health Indicators in Degradation Estimation","summary":"Accurate remaining useful life (RUL) prediction hinges on the quality of health indicators (HIs), yet existing methods often fail to disentangle complex degradation mechanisms in multi-sensor systems or quantify uncertainty in HI reliability. This paper introduces a novel framework for HI construction, advancing three key contributions. First, we adapt Reconstruction along Projected Pathways (RaPP) as a health indicator (HI) for RUL prediction for the first time, showing that it outperforms traditional reconstruction error metrics. Second, we show that augmenting RaPP-derived HIs with aleatoric and epistemic uncertainty quantification (UQ) via Monte Carlo dropout and probabilistic latent spaces- significantly improves RUL-prediction robustness. Third, and most critically, we propose indicator groups, a paradigm that isolates sensor subsets to model system-specific degradations, giving rise to our novel method, I-GLIDE which enables interpretable, mechanism-specific diagnostics. Evaluated on data sourced from aerospace and manufacturing systems, our approach achieves marked improvements in accuracy and generalizability compared to state-of-the-art HI methods while providing actionable insights into system failure pathways. This work bridges the gap between anomaly detection and prognostics, offering a principled framework for uncertainty-aware degradation modeling in complex systems.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21208.png","numComments":2,"submittedBy":{"_id":"63e24a0cf0740bec2bfd7f9e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63e24a0cf0740bec2bfd7f9e/VS786oOKIF9IqIm3xnniS.jpeg","fullname":"Lucas Thil","name":"LucasThil","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1},"organization":{"_id":"691f39b236fe6b60136afe00","name":"orailix","fullname":"Orailix","avatar":"https://cdn-uploads.huggingface.co/production/uploads/654e1e137cac9d7b51418f83/EwhwHf5mYSV65VRf-5bXb.png"},"isAuthorParticipating":true},{"paper":{"id":"2511.17889","authors":[{"_id":"69259d62c97bfa495574df19","user":{"_id":"675aa0331b310ed01068857f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PtTMPhuHlfdui_CJOjgHV.png","isPro":false,"fullname":"HuangTing","user":"Believe0029","type":"user"},"name":"Ting Huang","status":"claimed_verified","statusLastChangedAt":"2025-11-27T10:00:30.581Z","hidden":false},{"_id":"69259d62c97bfa495574df1a","name":"Dongjian Li","hidden":false},{"_id":"69259d62c97bfa495574df1b","name":"Rui Yang","hidden":false},{"_id":"69259d62c97bfa495574df1c","user":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","isPro":false,"fullname":"Zeyu Zhang","user":"SteveZeyuZhang","type":"user"},"name":"Zeyu Zhang","status":"claimed_verified","statusLastChangedAt":"2025-11-27T10:00:26.747Z","hidden":true},{"_id":"69259d62c97bfa495574df1d","user":{"_id":"67e129f772dece12fa3e20d3","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ifxojVXbdpxd02IIVCkWX.png","isPro":false,"fullname":"Yangzida","user":"PKUYZD","type":"user"},"name":"Zida Yang","status":"claimed_verified","statusLastChangedAt":"2025-11-27T10:00:28.652Z","hidden":false},{"_id":"69259d62c97bfa495574df1e","name":"Hao Tang","hidden":false}],"publishedAt":"2025-11-22T02:34:10.000Z","submittedOnDailyAt":"2025-11-27T05:18:13.911Z","title":"MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots","submittedOnDailyBy":{"_id":"675aa0331b310ed01068857f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PtTMPhuHlfdui_CJOjgHV.png","isPro":false,"fullname":"HuangTing","user":"Believe0029","type":"user"},"summary":"Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.","upvotes":5,"discussionId":"69259d62c97bfa495574df1f","projectPage":"https://aigeeksgroup.github.io/MobileVLA-R1/","githubRepo":"https://github.com/AIGeeksGroup/MobileVLA-R1","ai_summary":"A unified vision-language-action framework, MobileVLA-R1, enhances reasoning and control for quadruped robots through supervised chain-of-thought alignment and GRPO reinforcement learning, achieving superior performance in complex environments.","ai_keywords":["vision-language-action framework","MobileVLA-R1","chain-of-thought (CoT)","GRPO reinforcement learning","VLN tasks","VLA tasks"],"githubStars":6},"publishedAt":"2025-11-21T21:34:10.000Z","title":"MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots","summary":"Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17889.png","numComments":2,"submittedBy":{"_id":"675aa0331b310ed01068857f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PtTMPhuHlfdui_CJOjgHV.png","fullname":"HuangTing","name":"Believe0029","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"isAuthorParticipating":true},{"paper":{"id":"2511.17918","authors":[{"_id":"692680db243b2216fb75cab2","user":{"_id":"68e34e1f349df589247872e4","avatarUrl":"/avatars/884f38e7b4370d0661aab7a93dd1cd5d.svg","isPro":false,"fullname":"Youngsik Yun","user":"bbangsik13","type":"user"},"name":"Youngsik Yun","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:27.289Z","hidden":false},{"_id":"692680db243b2216fb75cab3","name":"Dongjun Gu","hidden":false},{"_id":"692680db243b2216fb75cab4","name":"Youngjung Uh","hidden":false}],"publishedAt":"2025-11-22T05:04:46.000Z","submittedOnDailyAt":"2025-11-27T02:06:03.890Z","title":"Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization","submittedOnDailyBy":{"_id":"68e34e1f349df589247872e4","avatarUrl":"/avatars/884f38e7b4370d0661aab7a93dd1cd5d.svg","isPro":false,"fullname":"Youngsik Yun","user":"bbangsik13","type":"user"},"summary":"Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.","upvotes":0,"discussionId":"692680db243b2216fb75cab5","projectPage":"https://bbangsik13.github.io/FASR","githubRepo":"https://github.com/bbangsik13/FASR","ai_summary":"Frequency-Adaptive Sharpness Regularization (FASR) enhances 3D Gaussian Splatting's generalization to novel viewpoints by adaptively adjusting regularization based on local image frequency.","ai_keywords":["3D Gaussian Splatting","3DGS","Frequency-Adaptive Sharpness Regularization","FASR","Sharpness-Aware Minimization","SAM","generalization","novel view synthesis","sharpness regularization","high-frequency details","floater artifacts"],"githubStars":1},"publishedAt":"2025-11-22T00:04:46.000Z","title":"Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization","summary":"Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17918.png","numComments":2,"submittedBy":{"_id":"68e34e1f349df589247872e4","avatarUrl":"/avatars/884f38e7b4370d0661aab7a93dd1cd5d.svg","fullname":"Youngsik Yun","name":"bbangsik13","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"isAuthorParticipating":true},{"paper":{"id":"2511.21692","authors":[{"_id":"6927cf95243b2216fb75cd90","user":{"_id":"626d13b15f7327906f0514e9","avatarUrl":"/avatars/6fb9b22961b9a283700c60713bc5a61f.svg","isPro":false,"fullname":"Yeganeh Kordi","user":"Yeganeh","type":"user"},"name":"Yeganeh Kordi","status":"claimed_verified","statusLastChangedAt":"2025-11-27T09:58:38.604Z","hidden":false},{"_id":"6927cf95243b2216fb75cd91","name":"Nihal V. Nayak","hidden":false},{"_id":"6927cf95243b2216fb75cd92","name":"Max Zuo","hidden":false},{"_id":"6927cf95243b2216fb75cd93","name":"Ilana Nguyen","hidden":false},{"_id":"6927cf95243b2216fb75cd94","name":"Stephen H. Bach","hidden":false}],"publishedAt":"2025-11-26T18:59:57.000Z","submittedOnDailyAt":"2025-11-27T01:58:38.144Z","title":"Revisiting Generalization Across Difficulty Levels: It's Not So Easy","submittedOnDailyBy":{"_id":"64fbb457c7f04f7cee8624e0","avatarUrl":"/avatars/f0512561780625d9be43f00dfd5cd46d.svg","isPro":false,"fullname":"Max Zuo","user":"zuom","type":"user"},"summary":"We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.","upvotes":11,"discussionId":"6927cf95243b2216fb75cd95","githubRepo":"https://github.com/BatsResearch/Cross-Difficulty","ai_summary":"LLMs do not consistently generalize across different task difficulties, indicating the need for a broad range of difficulty levels in both training and evaluation datasets.","ai_keywords":["large language models","LLMs","generalization","task difficulties","data curation","evaluation","Item Response Theory","IRT","cross-difficulty generalization"],"githubStars":3,"organization":{"_id":"650d8bcd5085c0ce1f286c12","name":"BatsResearch","fullname":"Bats Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/608849cadf398c3b285ce95b/oFrkKbxGUKIW2W8FFGUje.png"}},"publishedAt":"2025-11-26T13:59:57.000Z","title":"Revisiting Generalization Across Difficulty Levels: It's Not So Easy","summary":"We investigate how well large language models (LLMs) generalize across different task difficulties, a key question for effective data curation and evaluation. Existing research is mixed regarding whether training on easier or harder data leads to better results, and whether those gains come on easier or harder test data. We address this question by conducting a systematic evaluation of LLMs' generalization across models, datasets, and fine-grained groups of example difficulty. We rank examples in six datasets using the outputs of thousands of different LLMs and Item Response Theory (IRT), a well-established difficulty metric in educational testing. Unlike prior work, our difficulty ratings are therefore determined solely by the abilities of many different LLMs, excluding human opinions of difficulty. With a more objective, larger-scale, and finer-grained analysis, we show that cross-difficulty generalization is often limited; training on either easy or hard data cannot achieve consistent improvements across the full range of difficulties. These results show the importance of having a range of difficulties in both training and evaluation data for LLMs, and that taking shortcuts with respect to difficulty is risky.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21692.png","numComments":2,"submittedBy":{"_id":"64fbb457c7f04f7cee8624e0","avatarUrl":"/avatars/f0512561780625d9be43f00dfd5cd46d.svg","fullname":"Max Zuo","name":"zuom","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3},"organization":{"_id":"650d8bcd5085c0ce1f286c12","name":"BatsResearch","fullname":"Bats Research","avatar":"https://cdn-uploads.huggingface.co/production/uploads/608849cadf398c3b285ce95b/oFrkKbxGUKIW2W8FFGUje.png"},"isAuthorParticipating":false},{"paper":{"id":"2511.20639","authors":[{"_id":"6927c504243b2216fb75cd62","user":{"_id":"65c288280aa2d53135734a42","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg","isPro":false,"fullname":"Jiaru Zou","user":"jiaruz2","type":"user"},"name":"Jiaru Zou","status":"claimed_verified","statusLastChangedAt":"2025-11-27T09:58:44.029Z","hidden":false},{"_id":"6927c504243b2216fb75cd63","name":"Xiyuan Yang","hidden":false},{"_id":"6927c504243b2216fb75cd64","name":"Ruizhong Qiu","hidden":false},{"_id":"6927c504243b2216fb75cd65","name":"Gaotang Li","hidden":false},{"_id":"6927c504243b2216fb75cd66","name":"Katherine Tieu","hidden":false},{"_id":"6927c504243b2216fb75cd67","user":{"_id":"60f5f68fa7fd83d025749234","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg","isPro":false,"fullname":"Pan Lu","user":"lupantech","type":"user"},"name":"Pan Lu","status":"claimed_verified","statusLastChangedAt":"2025-11-27T09:58:40.924Z","hidden":false},{"_id":"6927c504243b2216fb75cd68","name":"Ke Shen","hidden":false},{"_id":"6927c504243b2216fb75cd69","name":"Hanghang Tong","hidden":false},{"_id":"6927c504243b2216fb75cd6a","name":"Yejin Choi","hidden":false},{"_id":"6927c504243b2216fb75cd6b","name":"Jingrui He","hidden":false},{"_id":"6927c504243b2216fb75cd6c","name":"James Zou","hidden":false},{"_id":"6927c504243b2216fb75cd6d","name":"Mengdi Wang","hidden":false},{"_id":"6927c504243b2216fb75cd6e","name":"Ling Yang","hidden":false}],"publishedAt":"2025-11-25T18:56:57.000Z","submittedOnDailyAt":"2025-11-27T01:00:26.981Z","title":"Latent Collaboration in Multi-Agent Systems","submittedOnDailyBy":{"_id":"65c288280aa2d53135734a42","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg","isPro":false,"fullname":"Jiaru Zou","user":"jiaruz2","type":"user"},"summary":"Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.","upvotes":63,"discussionId":"6927c504243b2216fb75cd6f","githubRepo":"https://github.com/Gen-Verse/LatentMAS","ai_summary":"LatentMAS enables efficient and effective collaboration among LLM agents using latent space representations, enhancing reasoning quality and reducing computational costs.","ai_keywords":["multi-agent systems","large language models","latent space","LatentMAS","auto-regressive latent thoughts generation","last-layer hidden embeddings","shared latent working memory","expressiveness","information preservation","complexity","math and science reasoning","commonsense understanding","code generation","accuracy","output token usage","end-to-end inference"],"githubStars":79,"organization":{"_id":"67a21d7efeeacb7707bf40de","name":"Gen-Verse","fullname":"Princeton-AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64fde4e252e82dd432b74ce9/TAEScS71YX5NPRM4TXZc8.png"}},"publishedAt":"2025-11-25T13:56:57.000Z","title":"Latent Collaboration in Multi-Agent Systems","summary":"Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20639.png","numComments":5,"submittedBy":{"_id":"65c288280aa2d53135734a42","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg","fullname":"Jiaru Zou","name":"jiaruz2","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":9},"organization":{"_id":"67a21d7efeeacb7707bf40de","name":"Gen-Verse","fullname":"Princeton-AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64fde4e252e82dd432b74ce9/TAEScS71YX5NPRM4TXZc8.png"},"isAuthorParticipating":true},{"paper":{"id":"2511.21395","authors":[{"_id":"6927bed6243b2216fb75cd58","name":"Qixun Wang","hidden":false},{"_id":"6927bed6243b2216fb75cd59","user":{"_id":"673c7319d11b1c2e246ead9c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg","isPro":false,"fullname":"Yang Shi","user":"DogNeverSleep","type":"user"},"name":"Yang Shi","status":"claimed_verified","statusLastChangedAt":"2025-11-27T09:59:04.104Z","hidden":false},{"_id":"6927bed6243b2216fb75cd5a","name":"Yifei Wang","hidden":false},{"_id":"6927bed6243b2216fb75cd5b","name":"Yuanxing Zhang","hidden":false},{"_id":"6927bed6243b2216fb75cd5c","name":"Pengfei Wan","hidden":false},{"_id":"6927bed6243b2216fb75cd5d","name":"Kun Gai","hidden":false},{"_id":"6927bed6243b2216fb75cd5e","name":"Xianghua Ying","hidden":false},{"_id":"6927bed6243b2216fb75cd5f","name":"Yisen Wang","hidden":false}],"publishedAt":"2025-11-26T13:46:39.000Z","submittedOnDailyAt":"2025-11-27T00:31:32.938Z","title":"Monet: Reasoning in Latent Visual Space Beyond Images and Language","submittedOnDailyBy":{"_id":"673c7319d11b1c2e246ead9c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg","isPro":false,"fullname":"Yang Shi","user":"DogNeverSleep","type":"user"},"summary":"\"Thinking with images\" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts. We identify two core challenges in training MLLMs for latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with a three-stage distillation-based supervised fine-tuning (SFT) pipeline. We further reveal a limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO (Visual-latent Policy Optimization), a reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates. To support SFT, we construct Monet-SFT-125K, a high-quality text-image interleaved CoT dataset containing 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong out-of-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/Monet.","upvotes":9,"discussionId":"6927bed6243b2216fb75cd60","githubRepo":"https://github.com/NOVAglow646/Monet","ai_summary":"Monet, a training framework, enables MLLMs to reason in latent visual space using continuous embeddings, addressing challenges like computational cost and supervision, and outperforms on visual reasoning tasks.","ai_keywords":["multimodal large language models","MLLMs","latent visual space","continuous embeddings","intermediate visual thoughts","three-stage distillation-based supervised fine-tuning","SFT","GRPO","Visual-latent Policy Optimization","VLPO","reinforcement learning","policy gradient updates","CoT dataset","real-world perception","reasoning benchmarks","out-of-distribution generalization"],"githubStars":7},"publishedAt":"2025-11-26T08:46:39.000Z","title":"Monet: Reasoning in Latent Visual Space Beyond Images and Language","summary":"\"Thinking with images\" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts. We identify two core challenges in training MLLMs for latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with a three-stage distillation-based supervised fine-tuning (SFT) pipeline. We further reveal a limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO (Visual-latent Policy Optimization), a reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates. To support SFT, we construct Monet-SFT-125K, a high-quality text-image interleaved CoT dataset containing 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong out-of-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/Monet.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21395.png","numComments":2,"submittedBy":{"_id":"673c7319d11b1c2e246ead9c","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/673c7319d11b1c2e246ead9c/IjFIO--N7Hm_BOEafhEQv.jpeg","fullname":"Yang Shi","name":"DogNeverSleep","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":6},"isAuthorParticipating":true},{"paper":{"id":"2511.19797","authors":[{"_id":"6927bd2d243b2216fb75cd52","name":"Linqi Zhou","hidden":false},{"_id":"6927bd2d243b2216fb75cd53","name":"Mathias Parger","hidden":false},{"_id":"6927bd2d243b2216fb75cd54","name":"Ayaan Haque","hidden":false},{"_id":"6927bd2d243b2216fb75cd55","name":"Jiaming Song","hidden":false}],"publishedAt":"2025-11-24T23:55:45.000Z","submittedOnDailyAt":"2025-11-27T00:23:41.631Z","title":"Terminal Velocity Matching","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high-fidelity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the 2-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256x256, TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512, representing state-of-the-art performance for one/few-step models from scratch.","upvotes":10,"discussionId":"6927bd2d243b2216fb75cd56","ai_summary":"Terminal Velocity Matching (TVM) generalizes flow matching for high-fidelity generative modeling, achieving state-of-the-art performance on ImageNet with minimal computational steps.","ai_keywords":["Terminal Velocity Matching","flow matching","diffusion timesteps","2-Wasserstein distance","Lipschitz continuous","Diffusion Transformers","fused attention kernel","Jacobian-Vector Products","ImageNet-256x256","ImageNet-512x512","FID"]},"publishedAt":"2025-11-24T18:55:45.000Z","title":"Terminal Velocity Matching","summary":"We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high-fidelity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the 2-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256x256, TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512, representing state-of-the-art performance for one/few-step models from scratch.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19797.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":171},"isAuthorParticipating":false},{"paper":{"id":"2511.20714","authors":[{"_id":"6927bc37243b2216fb75cd3c","name":"Inferix Team","hidden":false},{"_id":"6927bc37243b2216fb75cd3d","name":"Tianyu Feng","hidden":false},{"_id":"6927bc37243b2216fb75cd3e","name":"Yizeng Han","hidden":false},{"_id":"6927bc37243b2216fb75cd3f","name":"Jiahao He","hidden":false},{"_id":"6927bc37243b2216fb75cd40","name":"Yuanyu He","hidden":false},{"_id":"6927bc37243b2216fb75cd41","name":"Xi Lin","hidden":false},{"_id":"6927bc37243b2216fb75cd42","name":"Teng Liu","hidden":false},{"_id":"6927bc37243b2216fb75cd43","name":"Hanfeng Lu","hidden":false},{"_id":"6927bc37243b2216fb75cd44","name":"Jiasheng Tang","hidden":false},{"_id":"6927bc37243b2216fb75cd45","name":"Wei Wang","hidden":false},{"_id":"6927bc37243b2216fb75cd46","name":"Zhiyuan Wang","hidden":false},{"_id":"6927bc37243b2216fb75cd47","name":"Jichao Wu","hidden":false},{"_id":"6927bc37243b2216fb75cd48","name":"Mingyang Yang","hidden":false},{"_id":"6927bc37243b2216fb75cd49","name":"Yinghao Yu","hidden":false},{"_id":"6927bc37243b2216fb75cd4a","user":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","isPro":false,"fullname":"Zeyu Zhang","user":"SteveZeyuZhang","type":"user"},"name":"Zeyu Zhang","status":"claimed_verified","statusLastChangedAt":"2025-11-27T09:59:06.036Z","hidden":false},{"_id":"6927bc37243b2216fb75cd4b","name":"Bohan Zhuang","hidden":false}],"publishedAt":"2025-11-25T01:45:04.000Z","submittedOnDailyAt":"2025-11-27T00:19:36.886Z","title":"Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.\n  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.","upvotes":37,"discussionId":"6927bc38243b2216fb75cd4c","githubRepo":"https://github.com/alibaba-damo-academy/Inferix","ai_summary":"Inferix is a next-generation inference engine designed for immersive world synthesis using semi-autoregressive decoding, combining diffusion and autoregressive methods for high-quality, real-time video generation and interaction.","ai_keywords":["world models","agentic AI","embodied AI","gaming","visual perception","understanding","reasoning","semi-autoregressive","block-diffusion","diffusion","autoregressive methods","video tokens","KV Cache management","Inferix","vLLM","SGLang","xDiTs","interactive video streaming","profiling","LV-Bench","minute-long video generation"],"githubStars":21},"publishedAt":"2025-11-24T20:45:04.000Z","title":"Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation","summary":"World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.\n  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20714.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":171},"isAuthorParticipating":false},{"paper":{"id":"2511.20478","authors":[{"_id":"6926d024243b2216fb75cbca","name":"Kateryna Chumachenko","hidden":false},{"_id":"6926d024243b2216fb75cbcb","name":"Amala Sanjay Deshmukh","hidden":false},{"_id":"6926d024243b2216fb75cbcc","name":"Jarno Seppanen","hidden":false},{"_id":"6926d024243b2216fb75cbcd","name":"Ilia Karmanov","hidden":false},{"_id":"6926d024243b2216fb75cbce","name":"Chia-Chih Chen","hidden":false},{"_id":"6926d024243b2216fb75cbcf","name":"Lukas Voegtle","hidden":false},{"_id":"6926d024243b2216fb75cbd0","name":"Philipp Fischer","hidden":false},{"_id":"6926d024243b2216fb75cbd1","name":"Marek Wawrzos","hidden":false},{"_id":"6926d024243b2216fb75cbd2","name":"Saeid Motiian","hidden":false},{"_id":"6926d024243b2216fb75cbd3","name":"Roman Ageev","hidden":false},{"_id":"6926d024243b2216fb75cbd4","name":"Kedi Wu","hidden":false},{"_id":"6926d024243b2216fb75cbd5","name":"Alexandre Milesi","hidden":false},{"_id":"6926d024243b2216fb75cbd6","name":"Maryam Moosaei","hidden":false},{"_id":"6926d024243b2216fb75cbd7","name":"Krzysztof Pawelec","hidden":false},{"_id":"6926d024243b2216fb75cbd8","name":"Padmavathy Subramanian","hidden":false},{"_id":"6926d024243b2216fb75cbd9","name":"Mehrzad Samadi","hidden":false},{"_id":"6926d024243b2216fb75cbda","name":"Xin Yu","hidden":false},{"_id":"6926d024243b2216fb75cbdb","name":"Celina Dear","hidden":false},{"_id":"6926d024243b2216fb75cbdc","name":"Sarah Stoddard","hidden":false},{"_id":"6926d024243b2216fb75cbdd","name":"Jenna Diamond","hidden":false},{"_id":"6926d024243b2216fb75cbde","name":"Jesse Oliver","hidden":false},{"_id":"6926d024243b2216fb75cbdf","name":"Leanna Chraghchian","hidden":false},{"_id":"6926d024243b2216fb75cbe0","name":"Patrick Skelly","hidden":false},{"_id":"6926d024243b2216fb75cbe1","name":"Tom Balough","hidden":false},{"_id":"6926d024243b2216fb75cbe2","name":"Yao Xu","hidden":false},{"_id":"6926d024243b2216fb75cbe3","name":"Jane Polak Scowcroft","hidden":false},{"_id":"6926d024243b2216fb75cbe4","name":"Daniel Korzekwa","hidden":false},{"_id":"6926d024243b2216fb75cbe5","name":"Darragh Hanley","hidden":false},{"_id":"6926d024243b2216fb75cbe6","name":"Sandip Bhaskar","hidden":false},{"_id":"6926d024243b2216fb75cbe7","name":"Timo Roman","hidden":false},{"_id":"6926d024243b2216fb75cbe8","name":"Karan Sapra","hidden":false},{"_id":"6926d024243b2216fb75cbe9","name":"Andrew Tao","hidden":false},{"_id":"6926d024243b2216fb75cbea","name":"Bryan Catanzaro","hidden":false}],"publishedAt":"2025-11-25T16:41:25.000Z","submittedOnDailyAt":"2025-11-27T00:15:47.055Z","title":"NVIDIA Nemotron Parse 1.1","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.","upvotes":11,"discussionId":"6926d024243b2216fb75cbeb","ai_summary":"Nemotron-Parse-1.1 is a lightweight OCR and document parsing model with improved capabilities in general OCR, markdown formatting, structured table parsing, and text extraction from images, using an encoder-decoder architecture.","ai_keywords":["OCR","document parsing","markdown formatting","structured table parsing","text extraction","encoder-decoder architecture","bounding boxes","semantic classes","Huggingface","NIM container","vision tokens"],"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}},"publishedAt":"2025-11-25T11:41:25.000Z","title":"NVIDIA Nemotron Parse 1.1","summary":"We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20478.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":171},"organization":{"_id":"60262b67268c201cdc8b7d43","name":"nvidia","fullname":"NVIDIA","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"},"isAuthorParticipating":false},{"paper":{"id":"2511.15703","authors":[{"_id":"692067018c38b39d6a482e02","name":"Beichen Zhang","hidden":false},{"_id":"692067018c38b39d6a482e03","user":{"_id":"63859cf3b2906edaf83af9f0","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/kajwuVzd4pDucSPlwghxo.png","isPro":true,"fullname":"Yuhang Zang","user":"yuhangzang","type":"user"},"name":"Yuhang Zang","status":"claimed_verified","statusLastChangedAt":"2025-11-24T07:58:12.891Z","hidden":false},{"_id":"692067018c38b39d6a482e04","name":"Xiaoyi Dong","hidden":false},{"_id":"692067018c38b39d6a482e05","name":"Yuhang Cao","hidden":false},{"_id":"692067018c38b39d6a482e06","name":"Haodong Duan","hidden":false},{"_id":"692067018c38b39d6a482e07","name":"Dahua Lin","hidden":false},{"_id":"692067018c38b39d6a482e08","name":"Jiaqi Wang","hidden":false}],"publishedAt":"2025-11-19T18:59:04.000Z","submittedOnDailyAt":"2025-11-26T23:35:40.091Z","title":"Think Visually, Reason Textually: Vision-Language Synergy in ARC","submittedOnDailyBy":{"_id":"64b93578ee257c3a4cfceed1","avatarUrl":"/avatars/e6188562254f75a09b4048b800860016.svg","isPro":false,"fullname":"Beichen Zhang","user":"BeichenZhang","type":"user"},"summary":"Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.","upvotes":8,"discussionId":"692067018c38b39d6a482e09","githubRepo":"https://github.com/InternLM/ARC-VL","ai_summary":"Combining visual and linguistic reasoning strategies improves performance on abstract reasoning tasks in the ARC-AGI dataset by leveraging the strengths of each modality.","ai_keywords":["abstract reasoning","GPT-5","Grok 4","Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI)","conceptual rule induction","visual abstraction","symbolic rule formulation","Vision-Language Synergy Reasoning (VLSR)","Modality-Switch Self-Correction (MSSC)","global pattern abstraction","intrinsic error correction"],"githubStars":11,"organization":{"_id":"64a2d5fa81252883206f24c9","name":"internlm","fullname":"Intern Large Models","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg"}},"publishedAt":"2025-11-19T13:59:04.000Z","title":"Think Visually, Reason Textually: Vision-Language Synergy in ARC","summary":"Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15703.png","numComments":2,"submittedBy":{"_id":"64b93578ee257c3a4cfceed1","avatarUrl":"/avatars/e6188562254f75a09b4048b800860016.svg","fullname":"Beichen Zhang","name":"BeichenZhang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":17},"organization":{"_id":"64a2d5fa81252883206f24c9","name":"internlm","fullname":"Intern Large Models","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2511.20626","authors":[{"_id":"6927ab26243b2216fb75cd1b","name":"Wei He","hidden":false},{"_id":"6927ab26243b2216fb75cd1c","user":{"_id":"65e52e7d27dc8aa470a640e3","avatarUrl":"/avatars/022a179d14de29b9ab9d96fcc85aa264.svg","isPro":false,"fullname":"hankai","user":"hankaixyz","type":"user"},"name":"Kai Han","status":"claimed_verified","statusLastChangedAt":"2025-11-27T09:59:11.052Z","hidden":false},{"_id":"6927ab26243b2216fb75cd1d","name":"Hang Zhou","hidden":false},{"_id":"6927ab26243b2216fb75cd1e","name":"Hanting Chen","hidden":false},{"_id":"6927ab26243b2216fb75cd1f","name":"Zhicheng Liu","hidden":false},{"_id":"6927ab26243b2216fb75cd20","name":"Xinghao Chen","hidden":false},{"_id":"6927ab26243b2216fb75cd21","name":"Yunhe Wang","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"],"publishedAt":"2025-11-25T18:48:05.000Z","submittedOnDailyAt":"2025-11-26T23:08:13.066Z","title":"ROOT: Robust Orthogonalized Optimizer for Neural Network Training","submittedOnDailyBy":{"_id":"65e52e7d27dc8aa470a640e3","avatarUrl":"/avatars/022a179d14de29b9ab9d96fcc85aa264.svg","isPro":false,"fullname":"hankai","user":"hankaixyz","type":"user"},"summary":"The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.","upvotes":138,"discussionId":"6927ab27243b2216fb75cd22","projectPage":"https://github.com/huawei-noah/noah-research/tree/master/ROOT","githubRepo":"https://github.com/huawei-noah/noah-research","ai_summary":"ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.","ai_keywords":["large language models","LLMs","momentum orthogonalization","dimensional fragility","outlier-induced noise","adaptive Newton iterations","proximal optimization","Muon","Adam-based optimizers","robust optimizer"],"githubStars":909,"organization":{"_id":"5f83c275f0801648bf88454a","name":"huawei-noah","fullname":"HUAWEI Noah's Ark Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}},"publishedAt":"2025-11-25T13:48:05.000Z","title":"ROOT: Robust Orthogonalized Optimizer for Neural Network Training","summary":"The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20626.png","numComments":2,"submittedBy":{"_id":"65e52e7d27dc8aa470a640e3","avatarUrl":"/avatars/022a179d14de29b9ab9d96fcc85aa264.svg","fullname":"hankai","name":"hankaixyz","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3},"organization":{"_id":"5f83c275f0801648bf88454a","name":"huawei-noah","fullname":"HUAWEI Noah's Ark Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"},"isAuthorParticipating":true},{"paper":{"id":"2511.18659","authors":[{"_id":"69260a8b45972f0c0eea1713","name":"Jie He","hidden":false},{"_id":"69260a8b45972f0c0eea1714","name":"Richard He Bai","hidden":false},{"_id":"69260a8b45972f0c0eea1715","name":"Sinead Williamson","hidden":false},{"_id":"69260a8b45972f0c0eea1716","name":"Jeff Z. Pan","hidden":false},{"_id":"69260a8b45972f0c0eea1717","name":"Navdeep Jaitly","hidden":false},{"_id":"69260a8b45972f0c0eea1718","name":"Yizhe Zhang","hidden":false}],"publishedAt":"2025-11-24T00:11:14.000Z","submittedOnDailyAt":"2025-11-26T21:35:55.354Z","title":"CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning","submittedOnDailyBy":{"_id":"65269d143b3b217bf80aef0a","avatarUrl":"/avatars/02d7291bd6b307f3e9bd5c87a09833a0.svg","isPro":true,"fullname":"jie he","user":"probejie","type":"user"},"summary":"Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrieval-generation optimization. In this work, we propose CLaRa (Continuous Latent Reasoning), a unified framework that performs embedding-based compression and joint optimization in a shared continuous space. To obtain semantically rich and retrievable compressed vectors, we introduce SCP, a key-preserving data synthesis framework using QA and paraphrase supervision. CLaRa then trains the reranker and generator end-to-end via a single language modeling loss, with gradients flowing through both modules using a differentiable top-k estimator. Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show that CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines.","upvotes":3,"discussionId":"69260a8b45972f0c0eea1719","githubRepo":"https://github.com/apple/ml-clara","ai_summary":"CLaRa enhances retrieval-augmented generation by introducing unified embedding-based compression and joint optimization, achieving state-of-the-art performance in QA benchmarks.","ai_keywords":["RAG","large language models","external knowledge","long contexts","disjoint retrieval-generation optimization","CLaRa","continuous latent reasoning","embedding-based compression","joint optimization","shared continuous space","SCP","key-preserving data synthesis","QA","paraphrase supervision","reranker","generator","differentiable top-k estimator"],"githubStars":14,"organization":{"_id":"628cbd99ef14f971b69948ab","name":"apple","fullname":"Apple","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"}},"publishedAt":"2025-11-23T19:11:14.000Z","title":"CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning","summary":"Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrieval-generation optimization. In this work, we propose CLaRa (Continuous Latent Reasoning), a unified framework that performs embedding-based compression and joint optimization in a shared continuous space. To obtain semantically rich and retrievable compressed vectors, we introduce SCP, a key-preserving data synthesis framework using QA and paraphrase supervision. CLaRa then trains the reranker and generator end-to-end via a single language modeling loss, with gradients flowing through both modules using a differentiable top-k estimator. Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show that CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18659.png","numComments":2,"submittedBy":{"_id":"65269d143b3b217bf80aef0a","avatarUrl":"/avatars/02d7291bd6b307f3e9bd5c87a09833a0.svg","fullname":"jie he","name":"probejie","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1},"organization":{"_id":"628cbd99ef14f971b69948ab","name":"apple","fullname":"Apple","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2511.20462","authors":[{"_id":"69274327243b2216fb75ccc3","name":"Jiatao Gu","hidden":false},{"_id":"69274327243b2216fb75ccc4","name":"Ying Shen","hidden":false},{"_id":"69274327243b2216fb75ccc5","name":"Tianrong Chen","hidden":false},{"_id":"69274327243b2216fb75ccc6","name":"Laurent Dinh","hidden":false},{"_id":"69274327243b2216fb75ccc7","name":"Yuyang Wang","hidden":false},{"_id":"69274327243b2216fb75ccc8","name":"Miguel Angel Bautista","hidden":false},{"_id":"69274327243b2216fb75ccc9","name":"David Berthelot","hidden":false},{"_id":"69274327243b2216fb75ccca","name":"Josh Susskind","hidden":false},{"_id":"69274327243b2216fb75cccb","name":"Shuangfei Zhai","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6164e72d73996c363c52e66d/hxaNrcznaH7r6C5HYU7N8.mp4"],"publishedAt":"2025-11-25T16:27:58.000Z","submittedOnDailyAt":"2025-11-26T16:13:59.019Z","title":"STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow","submittedOnDailyBy":{"_id":"6164e72d73996c363c52e66d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1634002684894-noauth.png","isPro":false,"fullname":"Jiatao Gu","user":"thomagram","type":"user"},"summary":"Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as a promising research direction for building world models. Code and generated samples are available at https://github.com/apple/ml-starflow.","upvotes":16,"discussionId":"69274328243b2216fb75cccc","projectPage":"https://starflow-v.github.io","githubRepo":"https://github.com/apple/ml-starflow","ai_summary":"STARFlow-V, a normalizing flow-based video generator, offers end-to-end learning, robust causal prediction, and high-quality video generation with practical sampling efficiency.","ai_keywords":["normalizing flows","NFs","likelihood-based generative models","video generation","spatiotemporal latent space","global-local architecture","causal denoiser","flow-score matching","Jacobi iteration scheme","autoregressive generation","text-to-video","image-to-video","video-to-video generation","world models"],"githubStars":38,"organization":{"_id":"628cbd99ef14f971b69948ab","name":"apple","fullname":"Apple","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"}},"publishedAt":"2025-11-25T11:27:58.000Z","title":"STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow","summary":"Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as a promising research direction for building world models. Code and generated samples are available at https://github.com/apple/ml-starflow.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6164e72d73996c363c52e66d/hxaNrcznaH7r6C5HYU7N8.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20462.png","numComments":2,"submittedBy":{"_id":"6164e72d73996c363c52e66d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1634002684894-noauth.png","fullname":"Jiatao Gu","name":"thomagram","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3},"organization":{"_id":"628cbd99ef14f971b69948ab","name":"apple","fullname":"Apple","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1653390727490-5dd96eb166059660ed1ee413.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2511.20250","authors":[{"_id":"6927290f243b2216fb75cc98","name":"Daniel Kienzle","hidden":false},{"_id":"6927290f243b2216fb75cc99","name":"Katja Ludwig","hidden":false},{"_id":"6927290f243b2216fb75cc9a","name":"Julian Lorenz","hidden":false},{"_id":"6927290f243b2216fb75cc9b","name":"Shin'ichi Satoh","hidden":false},{"_id":"6927290f243b2216fb75cc9c","name":"Rainer Lienhart","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6888b31e795863924e46d249/9GpDiys7y2tpt24fmBJc6.png"],"publishedAt":"2025-11-25T12:25:20.000Z","submittedOnDailyAt":"2025-11-26T13:55:02.217Z","title":"Uplifting Table Tennis: A Robust, Real-World Application for 3D Trajectory and Spin Estimation","submittedOnDailyBy":{"_id":"6888b31e795863924e46d249","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/jpCbNuOAKsXjlT5ZpoNIY.png","isPro":false,"fullname":"Daniel Kienzle","user":"KieDani","type":"user"},"summary":"Obtaining the precise 3D motion of a table tennis ball from standard monocular videos is a challenging problem, as existing methods trained on synthetic data struggle to generalize to the noisy, imperfect ball and table detections of the real world. This is primarily due to the inherent lack of 3D ground truth trajectories and spin annotations for real-world video. To overcome this, we propose a novel two-stage pipeline that divides the problem into a front-end perception task and a back-end 2D-to-3D uplifting task. This separation allows us to train the front-end components with abundant 2D supervision from our newly created TTHQ dataset, while the back-end uplifting network is trained exclusively on physically-correct synthetic data. We specifically re-engineer the uplifting model to be robust to common real-world artifacts, such as missing detections and varying frame rates. By integrating a ball detector and a table keypoint detector, our approach transforms a proof-of-concept uplifting method into a practical, robust, and high-performing end-to-end application for 3D table tennis trajectory and spin analysis.","upvotes":1,"discussionId":"69272910243b2216fb75cc9d","projectPage":"https://kiedani.github.io/WACV2026/index.html","ai_summary":"A two-stage pipeline with a front-end perception task and a back-end 2D-to-3D uplifting task is proposed for accurate 3D motion analysis of a table tennis ball using monocular video.","ai_keywords":["monocular videos","3D motion","2D-to-3D uplifting","front-end perception","TTHQ dataset","physically-correct synthetic data","ball detector","table keypoint detector"],"organization":{"_id":"68a43e693f883647998a0a57","name":"MLCVLab","fullname":"Chair for Machine Learning & Computer Vision ","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6888b31e795863924e46d249/N5eV3SbCRFQMfH4PpPZ7E.png"}},"publishedAt":"2025-11-25T07:25:20.000Z","title":"Uplifting Table Tennis: A Robust, Real-World Application for 3D Trajectory and Spin Estimation","summary":"Obtaining the precise 3D motion of a table tennis ball from standard monocular videos is a challenging problem, as existing methods trained on synthetic data struggle to generalize to the noisy, imperfect ball and table detections of the real world. This is primarily due to the inherent lack of 3D ground truth trajectories and spin annotations for real-world video. To overcome this, we propose a novel two-stage pipeline that divides the problem into a front-end perception task and a back-end 2D-to-3D uplifting task. This separation allows us to train the front-end components with abundant 2D supervision from our newly created TTHQ dataset, while the back-end uplifting network is trained exclusively on physically-correct synthetic data. We specifically re-engineer the uplifting model to be robust to common real-world artifacts, such as missing detections and varying frame rates. By integrating a ball detector and a table keypoint detector, our approach transforms a proof-of-concept uplifting method into a practical, robust, and high-performing end-to-end application for 3D table tennis trajectory and spin analysis.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6888b31e795863924e46d249/9GpDiys7y2tpt24fmBJc6.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20250.png","numComments":2,"submittedBy":{"_id":"6888b31e795863924e46d249","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/jpCbNuOAKsXjlT5ZpoNIY.png","fullname":"Daniel Kienzle","name":"KieDani","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1},"organization":{"_id":"68a43e693f883647998a0a57","name":"MLCVLab","fullname":"Chair for Machine Learning & Computer Vision ","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6888b31e795863924e46d249/N5eV3SbCRFQMfH4PpPZ7E.png"},"isAuthorParticipating":false},{"paper":{"id":"2511.20643","authors":[{"_id":"692727b6243b2216fb75cc8c","name":"Adhiraj Ghosh","hidden":false},{"_id":"692727b6243b2216fb75cc8d","name":"Vishaal Udandarao","hidden":false},{"_id":"692727b6243b2216fb75cc8e","name":"Thao Nguyen","hidden":false},{"_id":"692727b6243b2216fb75cc8f","name":"Matteo Farina","hidden":false},{"_id":"692727b6243b2216fb75cc90","name":"Mehdi Cherti","hidden":false},{"_id":"692727b6243b2216fb75cc91","name":"Jenia Jitsev","hidden":false},{"_id":"692727b6243b2216fb75cc92","name":"Sewoong Oh","hidden":false},{"_id":"692727b6243b2216fb75cc93","name":"Elisa Ricci","hidden":false},{"_id":"692727b6243b2216fb75cc94","name":"Ludwig Schmidt","hidden":false},{"_id":"692727b6243b2216fb75cc95","name":"Matthias Bethge","hidden":false}],"publishedAt":"2025-11-25T18:58:07.000Z","submittedOnDailyAt":"2025-11-26T13:46:53.102Z","title":"Concept-Aware Batch Sampling Improves Language-Image Pretraining","submittedOnDailyBy":{"_id":"6321b4417bb41a713da9a1f4","avatarUrl":"/avatars/f2cde57366791eabee12d456c833baae.svg","isPro":false,"fullname":"Adhiraj Ghosh","user":"adhiraj1998","type":"user"},"summary":"What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.","upvotes":1,"discussionId":"692727b6243b2216fb75cc96","ai_summary":"Concept-Aware Batch Sampling (CABS) improves vision-language model performance by flexibly curating training data based on specific concept distributions.","ai_keywords":["DataConcept","Concept-Aware Batch Sampling","CABS","Diversity Maximization","Frequency Maximization","CLIP","SigLIP"],"organization":{"_id":"65745368a4ee9a4fe778ed92","name":"bethgelab","fullname":"Bethgelab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6304da46ce6b12280b1bd575/XH-3Q4A8p-P7JFR1LxSgx.jpeg"}},"publishedAt":"2025-11-25T13:58:07.000Z","title":"Concept-Aware Batch Sampling Improves Language-Image Pretraining","summary":"What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20643.png","numComments":2,"submittedBy":{"_id":"6321b4417bb41a713da9a1f4","avatarUrl":"/avatars/f2cde57366791eabee12d456c833baae.svg","fullname":"Adhiraj Ghosh","name":"adhiraj1998","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"organization":{"_id":"65745368a4ee9a4fe778ed92","name":"bethgelab","fullname":"Bethgelab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6304da46ce6b12280b1bd575/XH-3Q4A8p-P7JFR1LxSgx.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2511.20647","authors":[{"_id":"69272506243b2216fb75cc87","user":{"_id":"66fec311343c151be4fb0b73","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FXeHseCn4DKSVUzk-VKdA.png","isPro":false,"fullname":"Tahira Kazimi","user":"tahirakazimi77","type":"user"},"name":"Tahira Kazimi","status":"claimed_verified","statusLastChangedAt":"2025-11-27T09:59:28.303Z","hidden":false},{"_id":"69272506243b2216fb75cc88","name":"Connor Dunlop","hidden":false},{"_id":"69272506243b2216fb75cc89","name":"Pinar Yanardag","hidden":false}],"publishedAt":"2025-11-25T18:59:45.000Z","submittedOnDailyAt":"2025-11-26T13:35:37.458Z","title":"Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization","submittedOnDailyBy":{"_id":"66fec311343c151be4fb0b73","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FXeHseCn4DKSVUzk-VKdA.png","isPro":false,"fullname":"Tahira Kazimi","user":"tahirakazimi77","type":"user"},"summary":"While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.","upvotes":2,"discussionId":"69272506243b2216fb75cc8a","projectPage":"https://diverse-video.github.io/","ai_summary":"A framework combining Determinantal Point Processes and Group Relative Policy Optimization enhances diversity in text-to-video generation without compromising quality.","ai_keywords":["text-to-video (T2V) diffusion models","set-level policy optimization","Determinantal Point Processes (DPPs)","Group Relative Policy Optimization (GRPO)","diverse video generation","visual appearance","camera motions","scene structure","VBench","VideoScore"],"organization":{"_id":"641f3b58a390e539522a6f88","name":"VirginiaTech","fullname":"Virginia Polytechnic Institute and State University"}},"publishedAt":"2025-11-25T13:59:45.000Z","title":"Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization","summary":"While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20647.png","numComments":2,"submittedBy":{"_id":"66fec311343c151be4fb0b73","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FXeHseCn4DKSVUzk-VKdA.png","fullname":"Tahira Kazimi","name":"tahirakazimi77","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"organization":{"_id":"641f3b58a390e539522a6f88","name":"VirginiaTech","fullname":"Virginia Polytechnic Institute and State University"},"isAuthorParticipating":true},{"paper":{"id":"2511.18886","authors":[{"_id":"6926f72b243b2216fb75cc37","name":"Guangyuan Li","hidden":false},{"_id":"6926f72b243b2216fb75cc38","name":"Siming Zheng","hidden":false},{"_id":"6926f72b243b2216fb75cc39","name":"Shuolin Xu","hidden":false},{"_id":"6926f72b243b2216fb75cc3a","name":"Jinwei Chen","hidden":false},{"_id":"6926f72b243b2216fb75cc3b","name":"Bo Li","hidden":false},{"_id":"6926f72b243b2216fb75cc3c","name":"Xiaobin Hu","hidden":false},{"_id":"6926f72b243b2216fb75cc3d","name":"Lei Zhao","hidden":false},{"_id":"6926f72b243b2216fb75cc3e","user":{"_id":"6423efbdb77cc3daf8429755","avatarUrl":"/avatars/a5b480713b4dd1dae8191545cb4c6f94.svg","isPro":false,"fullname":"Peng-Tao Jiang","user":"ptjiang","type":"user"},"name":"Peng-Tao Jiang","status":"claimed_verified","statusLastChangedAt":"2025-11-27T09:59:34.171Z","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6423efbdb77cc3daf8429755/6jxbJARvlMKsVKSaaVDVP.qt"],"publishedAt":"2025-11-24T08:41:28.000Z","submittedOnDailyAt":"2025-11-26T12:58:15.558Z","title":"MagicWorld: Interactive Geometry-driven Video World Exploration","submittedOnDailyBy":{"_id":"6423efbdb77cc3daf8429755","avatarUrl":"/avatars/a5b480713b4dd1dae8191545cb4c6f94.svg","isPro":false,"fullname":"Peng-Tao Jiang","user":"ptjiang","type":"user"},"summary":"Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.","upvotes":16,"discussionId":"6926f72b243b2216fb75cc3f","projectPage":"https://vivocameraresearch.github.io/magicworld/","ai_summary":"MagicWorld, an interactive video world model, integrates 3D geometry and historical retrieval to improve scene stability and continuity under user instructions.","ai_keywords":["3D geometric priors","historical retrieval","Action-Guided 3D Geometry Module","AG3D","point cloud","viewpoint transitions","History Cache Retrieval","HCR","scene evolution","dynamic scene evolution","autoregressive synthesis"]},"publishedAt":"2025-11-24T03:41:28.000Z","title":"MagicWorld: Interactive Geometry-driven Video World Exploration","summary":"Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6423efbdb77cc3daf8429755/6jxbJARvlMKsVKSaaVDVP.qt"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18886.png","numComments":3,"submittedBy":{"_id":"6423efbdb77cc3daf8429755","avatarUrl":"/avatars/a5b480713b4dd1dae8191545cb4c6f94.svg","fullname":"Peng-Tao Jiang","name":"ptjiang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1},"isAuthorParticipating":true},{"paper":{"id":"2511.19430","authors":[{"_id":"69271171243b2216fb75cc51","name":"Dingkang Liang","hidden":false},{"_id":"69271171243b2216fb75cc52","user":{"_id":"6478657c91398856110aba40","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6478657c91398856110aba40/SwWRmWozOWiC4aoLSuRqD.jpeg","isPro":false,"fullname":"Cheng Zhang","user":"Younai","type":"user"},"name":"Cheng Zhang","status":"claimed_verified","statusLastChangedAt":"2025-11-27T09:59:30.501Z","hidden":false},{"_id":"69271171243b2216fb75cc53","name":"Xiaopeng Xu","hidden":false},{"_id":"69271171243b2216fb75cc54","name":"Jianzhong Ju","hidden":false},{"_id":"69271171243b2216fb75cc55","name":"Zhenbo Luo","hidden":false},{"_id":"69271171243b2216fb75cc56","name":"Xiang Bai","hidden":false}],"publishedAt":"2025-11-24T18:59:17.000Z","submittedOnDailyAt":"2025-11-26T12:11:33.720Z","title":"Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution","submittedOnDailyBy":{"_id":"67467b5979406f42a14517e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67467b5979406f42a14517e9/wgnUxTd8vWOo0Cr2gaoyG.jpeg","isPro":false,"fullname":"Dingkang Liang","user":"dkliang","type":"user"},"summary":"Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT","upvotes":7,"discussionId":"69271171243b2216fb75cc57","projectPage":"https://h-embodvis.github.io/GRANT/","githubRepo":"https://github.com/H-EmbodVis/GRANT","ai_summary":"ORS3D, a new task requiring language understanding, 3D grounding, and efficient scheduling, is introduced with a large dataset and an embodied multi-modal model named GRANT that uses a scheduling token mechanism for effective task management.","ai_keywords":["Operations Research knowledge-based 3D Grounded Task Scheduling","ORS3D","3D grounding","efficiency optimization","parallelizable subtasks","embodied multi-modal large language model","GRANT","scheduling token mechanism"],"githubStars":137,"organization":{"_id":"687cebf73858638f66e59f56","name":"H-EmbodVis","fullname":"H-EmbodVis","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67467b5979406f42a14517e9/AnffLBETBMQyF-4ZK9o7B.png"}},"publishedAt":"2025-11-24T13:59:17.000Z","title":"Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution","summary":"Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19430.png","numComments":2,"submittedBy":{"_id":"67467b5979406f42a14517e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67467b5979406f42a14517e9/wgnUxTd8vWOo0Cr2gaoyG.jpeg","fullname":"Dingkang Liang","name":"dkliang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"organization":{"_id":"687cebf73858638f66e59f56","name":"H-EmbodVis","fullname":"H-EmbodVis","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67467b5979406f42a14517e9/AnffLBETBMQyF-4ZK9o7B.png"},"isAuthorParticipating":false},{"paper":{"id":"2511.18394","authors":[{"_id":"69270769243b2216fb75cc4d","name":"Chinmay Karkar","hidden":false},{"_id":"69270769243b2216fb75cc4e","name":"Paras Chopra","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/69020c76ec2616129dea30b4/yPTYGUk_v9rwjLRKSz87V.jpeg"],"publishedAt":"2025-11-23T10:41:19.000Z","submittedOnDailyAt":"2025-11-26T11:29:19.123Z","title":"Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking","submittedOnDailyBy":{"_id":"69020c76ec2616129dea30b4","avatarUrl":"/avatars/44d59b5d951a4303def15ea1c6e3387e.svg","isPro":false,"fullname":"Paras Chopra","user":"paraslossfunk","type":"user"},"summary":"Large Language Models (LLMs) demonstrate partial forecasting competence across social, political, and economic events. Yet, their predictive ability varies sharply with domain structure and prompt framing. We investigate how forecasting performance varies with different model families on real-world questions about events that happened beyond the model cutoff date. We analyze how context, question type, and external knowledge affect accuracy and calibration, and how adding factual news context modifies belief formation and failure modes. Our results show that forecasting ability is highly variable as it depends on what, and how, we ask.","upvotes":1,"discussionId":"69270769243b2216fb75cc4f","ai_summary":"Forecasting performance of Large Language Models varies significantly across different domains and question types, influenced by context and external knowledge.","ai_keywords":["Large Language Models","forecasting competence","domain structure","prompt framing","model families","real-world questions","model cutoff date","context","question type","external knowledge","accuracy","calibration","belief formation","failure modes"],"organization":{"_id":"67a1298ea7d77f4454f936a2","name":"Lossfunk","fullname":"Lossfunk","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67a128ecc9bce54bbf006876/1lIRZ5gmsWovgosI_HIXy.jpeg"}},"publishedAt":"2025-11-23T05:41:19.000Z","title":"Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking","summary":"Large Language Models (LLMs) demonstrate partial forecasting competence across social, political, and economic events. Yet, their predictive ability varies sharply with domain structure and prompt framing. We investigate how forecasting performance varies with different model families on real-world questions about events that happened beyond the model cutoff date. We analyze how context, question type, and external knowledge affect accuracy and calibration, and how adding factual news context modifies belief formation and failure modes. Our results show that forecasting ability is highly variable as it depends on what, and how, we ask.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/69020c76ec2616129dea30b4/yPTYGUk_v9rwjLRKSz87V.jpeg"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18394.png","numComments":2,"submittedBy":{"_id":"69020c76ec2616129dea30b4","avatarUrl":"/avatars/44d59b5d951a4303def15ea1c6e3387e.svg","fullname":"Paras Chopra","name":"paraslossfunk","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"organization":{"_id":"67a1298ea7d77f4454f936a2","name":"Lossfunk","fullname":"Lossfunk","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67a128ecc9bce54bbf006876/1lIRZ5gmsWovgosI_HIXy.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2511.16660","authors":[{"_id":"6925dec84b7b0d870b18278f","name":"Priyanka Kargupta","hidden":false},{"_id":"6925dec84b7b0d870b182790","name":"Shuyue Stella Li","hidden":false},{"_id":"6925dec84b7b0d870b182791","name":"Haocheng Wang","hidden":false},{"_id":"6925dec84b7b0d870b182792","name":"Jinu Lee","hidden":false},{"_id":"6925dec84b7b0d870b182793","name":"Shan Chen","hidden":false},{"_id":"6925dec84b7b0d870b182794","name":"Orevaoghene Ahia","hidden":false},{"_id":"6925dec84b7b0d870b182795","name":"Dean Light","hidden":false},{"_id":"6925dec84b7b0d870b182796","name":"Thomas L. Griffiths","hidden":false},{"_id":"6925dec84b7b0d870b182797","name":"Max Kleiman-Weiner","hidden":false},{"_id":"6925dec84b7b0d870b182798","name":"Jiawei Han","hidden":false},{"_id":"6925dec84b7b0d870b182799","name":"Asli Celikyilmaz","hidden":false},{"_id":"6925dec84b7b0d870b18279a","name":"Yulia Tsvetkov","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/axZxaLRAK4_474L0EJ2RM.png","https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/WcB-LRJRGhSqkkvZKs9Eg.png","https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/n2MDymrFJkRHzHKyDT3sW.png"],"publishedAt":"2025-11-20T18:59:00.000Z","submittedOnDailyAt":"2025-11-26T11:22:36.257Z","title":"Cognitive Foundations for Reasoning and Their Manifestation in LLMs","submittedOnDailyBy":{"_id":"6476ae4083d4fdaedddf405f","avatarUrl":"/avatars/08b23ccfa1f3bede6ade5a1aef06931d.svg","isPro":false,"fullname":"Priyanka Kargupta","user":"pkargupta","type":"user"},"summary":"Large language models (LLMs) solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. To understand this gap, we synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning reasoning invariants, meta-cognitive controls, representations for organizing reasoning & knowledge, and transformation operations. We introduce a fine-grained evaluation framework and conduct the first large-scale empirical analysis of 192K traces from 18 models across text, vision, and audio, complemented by 54 human think-aloud traces, which we make publicly available. We find that models under-utilize cognitive elements correlated with success, narrowing to rigid sequential processing on ill-structured problems where diverse representations and meta-cognitive monitoring are critical. Human traces show more abstraction and conceptual processing, while models default to surface-level enumeration. Meta-analysis of 1.6K LLM reasoning papers reveals the research community concentrates on easily quantifiable elements (sequential organization: 55%, decomposition: 60%) but neglecting meta-cognitive controls (self-awareness: 16%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 66.7% on complex problems. By establishing a shared vocabulary between cognitive science and LLM research, our framework enables systematic diagnosis of reasoning failures and principled development of models that reason through robust cognitive mechanisms rather than spurious shortcuts, while providing tools to test theories of human cognition at scale.","upvotes":7,"discussionId":"6925dec94b7b0d870b18279b","projectPage":"https://tinyurl.com/cognitive-foundations","githubRepo":"https://github.com/pkargupta/cognitive_foundations/","ai_summary":"LLMs exhibit reasoning gaps compared to humans, underutilizing cognitive elements and failing to deploy meta-cognitive controls, but test-time guidance can improve their performance on complex problems.","ai_keywords":["cognitive elements","reasoning invariants","meta-cognitive controls","representations","transformation operations","fine-grained evaluation framework","think-aloud traces","sequential processing","abstraction","conceptual processing","surface-level enumeration","self-awareness","reasoning guidance","robust cognitive mechanisms","spurious shortcuts"],"githubStars":4},"publishedAt":"2025-11-20T13:59:00.000Z","title":"Cognitive Foundations for Reasoning and Their Manifestation in LLMs","summary":"Large language models (LLMs) solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. To understand this gap, we synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning reasoning invariants, meta-cognitive controls, representations for organizing reasoning & knowledge, and transformation operations. We introduce a fine-grained evaluation framework and conduct the first large-scale empirical analysis of 192K traces from 18 models across text, vision, and audio, complemented by 54 human think-aloud traces, which we make publicly available. We find that models under-utilize cognitive elements correlated with success, narrowing to rigid sequential processing on ill-structured problems where diverse representations and meta-cognitive monitoring are critical. Human traces show more abstraction and conceptual processing, while models default to surface-level enumeration. Meta-analysis of 1.6K LLM reasoning papers reveals the research community concentrates on easily quantifiable elements (sequential organization: 55%, decomposition: 60%) but neglecting meta-cognitive controls (self-awareness: 16%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 66.7% on complex problems. By establishing a shared vocabulary between cognitive science and LLM research, our framework enables systematic diagnosis of reasoning failures and principled development of models that reason through robust cognitive mechanisms rather than spurious shortcuts, while providing tools to test theories of human cognition at scale.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/axZxaLRAK4_474L0EJ2RM.png","https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/WcB-LRJRGhSqkkvZKs9Eg.png","https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/n2MDymrFJkRHzHKyDT3sW.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16660.png","numComments":3,"submittedBy":{"_id":"6476ae4083d4fdaedddf405f","avatarUrl":"/avatars/08b23ccfa1f3bede6ade5a1aef06931d.svg","fullname":"Priyanka Kargupta","name":"pkargupta","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"isAuthorParticipating":false},{"paper":{"id":"2511.20102","authors":[{"_id":"69269351243b2216fb75cb22","user":{"_id":"641b31a4ec5b871c0bcd6932","avatarUrl":"/avatars/bad42279ad99918b0846053f2fa95ac8.svg","isPro":false,"fullname":"Zhenyi Shen","user":"zen-E","type":"user"},"name":"Zhenyi Shen","status":"claimed_verified","statusLastChangedAt":"2025-11-26T11:31:23.403Z","hidden":false},{"_id":"69269351243b2216fb75cb23","user":{"_id":"64ddbc3f1f2dad27e1a05ac1","avatarUrl":"/avatars/4fb2753b7998c8536bfd4780d3b10a6d.svg","isPro":false,"fullname":"Junrulu","user":"Junrulu","type":"user"},"name":"Junru Lu","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:16.085Z","hidden":false},{"_id":"69269351243b2216fb75cb24","name":"Lin Gui","hidden":false},{"_id":"69269351243b2216fb75cb25","user":{"_id":"64be88e9af05eb17c702787c","avatarUrl":"/avatars/8953032acd739a0780e33cc46b0f9b56.svg","isPro":false,"fullname":"J Li","user":"jiazhengli","type":"user"},"name":"Jiazheng Li","status":"claimed_verified","statusLastChangedAt":"2025-11-26T11:53:51.303Z","hidden":false},{"_id":"69269351243b2216fb75cb26","name":"Yulan He","hidden":false},{"_id":"69269351243b2216fb75cb27","name":"Di Yin","hidden":false},{"_id":"69269351243b2216fb75cb28","user":{"_id":"647401e50da364bd0d002f2a","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png","isPro":false,"fullname":"XING SUN","user":"tedsun","type":"user"},"name":"Xing Sun","status":"claimed_verified","statusLastChangedAt":"2025-11-27T09:59:40.138Z","hidden":false}],"publishedAt":"2025-11-25T09:21:57.000Z","submittedOnDailyAt":"2025-11-26T08:37:31.990Z","title":"SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space","submittedOnDailyBy":{"_id":"641b31a4ec5b871c0bcd6932","avatarUrl":"/avatars/bad42279ad99918b0846053f2fa95ac8.svg","isPro":false,"fullname":"Zhenyi Shen","user":"zen-E","type":"user"},"summary":"The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention, which may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas, with SSA demonstrating the strongest extrapolation capability.","upvotes":23,"discussionId":"69269351243b2216fb75cb29","ai_summary":"SSA, a unified training framework for sparse attention in LLMs, achieves state-of-the-art performance by aligning sparse attention with full attention, improving long-context processing and extrapolation.","ai_keywords":["full attention","sparse attention","native sparse-attention methods","NSA","MoBA","gradient update deficiency","SSA","Sparse Sparse Attention","bidirectional alignment","long-context extrapolation","sink areas"]},"publishedAt":"2025-11-25T04:21:57.000Z","title":"SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space","summary":"The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention, which may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas, with SSA demonstrating the strongest extrapolation capability.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20102.png","numComments":3,"submittedBy":{"_id":"641b31a4ec5b871c0bcd6932","avatarUrl":"/avatars/bad42279ad99918b0846053f2fa95ac8.svg","fullname":"Zhenyi Shen","name":"zen-E","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5},"isAuthorParticipating":true},{"paper":{"id":"2511.20211","authors":[{"_id":"6926ae80243b2216fb75cb3b","user":{"_id":"669205f1ccca14aa8f13f770","avatarUrl":"/avatars/11ce274e93345fe3790ac9fa687e2bcb.svg","isPro":false,"fullname":"Hao Yu","user":"Longin-Yu","type":"user"},"name":"Hao Yu","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:10.152Z","hidden":false},{"_id":"6926ae80243b2216fb75cb3c","name":"Jiabo Zhan","hidden":false},{"_id":"6926ae80243b2216fb75cb3d","name":"Zile Wang","hidden":false},{"_id":"6926ae80243b2216fb75cb3e","name":"Jinglin Wang","hidden":false},{"_id":"6926ae80243b2216fb75cb3f","name":"Huaisong Zhang","hidden":false},{"_id":"6926ae80243b2216fb75cb40","name":"Hongyu Li","hidden":false},{"_id":"6926ae80243b2216fb75cb41","name":"Xinrui Chen","hidden":false},{"_id":"6926ae80243b2216fb75cb42","name":"Yongxian Wei","hidden":false},{"_id":"6926ae80243b2216fb75cb43","name":"Chun Yuan","hidden":false}],"publishedAt":"2025-11-25T11:34:51.000Z","submittedOnDailyAt":"2025-11-26T07:06:21.590Z","title":"OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation","submittedOnDailyBy":{"_id":"669205f1ccca14aa8f13f770","avatarUrl":"/avatars/11ce274e93345fe3790ac9fa687e2bcb.svg","isPro":false,"fullname":"Hao Yu","user":"Longin-Yu","type":"user"},"summary":"Generative models have excelled in RGB synthesis, but real-world applications require RGBA manipulation. This has led to a fragmented landscape: specialized, single-task models handle alpha but lack versatility, while unified multi-task frameworks are confined to the RGB domain. To bridge this critical gap, we propose OmniAlpha, the first unified, multi-task generative framework for sequence-to-sequence RGBA image generation and editing. Its architecture features MSRoPE-BiL, a novel RoPE method with a bi-directionally extendable layer axis for its Diffusion Transformer (DiT) backbone, enabling the concurrent processing of multiple input and target RGBA layers. To power this framework, we introduce AlphaLayers, a new dataset of 1,000 high-quality, multi-layer triplets, built via a novel automated synthesis and filter pipeline. Jointly training OmniAlpha on this dataset across a comprehensive suite of 21 diverse tasks, extensive experiments demonstrate that our unified approach consistently outperforms strong, specialized baselines. Most notably, OmniAlpha achieves a dramatic 84.8% relative reduction in SAD for mask-free matting on AIM-500 and wins over 90% of human preferences in layer-conditioned completion. Our work proves that a unified, multi-task model can learn a superior shared representation for RGBA, paving the way for more powerful, layer-aware generative systems.","upvotes":12,"discussionId":"6926ae80243b2216fb75cb44","githubRepo":"https://github.com/Longin-Yu/OmniAlpha","ai_summary":"OmniAlpha, a unified multi-task generative framework, excels in RGBA image generation and editing using a Diffusion Transformer with a novel MSRoPE-BiL method, outperforming specialized models across various tasks.","ai_keywords":["generative models","RGBA manipulation","OmniAlpha","MSRoPE-BiL","Diffusion Transformer","AlphaLayers","mask-free matting","layer-conditioned completion"],"githubStars":10,"organization":{"_id":"64cc8e9b214a472dd85e7e1d","name":"THU1911","fullname":"Tsinghua University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61f8e5934a8e5a275b2b3e5a/oKO6FK_rTzzPHXihicZou.jpeg"}},"publishedAt":"2025-11-25T06:34:51.000Z","title":"OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation","summary":"Generative models have excelled in RGB synthesis, but real-world applications require RGBA manipulation. This has led to a fragmented landscape: specialized, single-task models handle alpha but lack versatility, while unified multi-task frameworks are confined to the RGB domain. To bridge this critical gap, we propose OmniAlpha, the first unified, multi-task generative framework for sequence-to-sequence RGBA image generation and editing. Its architecture features MSRoPE-BiL, a novel RoPE method with a bi-directionally extendable layer axis for its Diffusion Transformer (DiT) backbone, enabling the concurrent processing of multiple input and target RGBA layers. To power this framework, we introduce AlphaLayers, a new dataset of 1,000 high-quality, multi-layer triplets, built via a novel automated synthesis and filter pipeline. Jointly training OmniAlpha on this dataset across a comprehensive suite of 21 diverse tasks, extensive experiments demonstrate that our unified approach consistently outperforms strong, specialized baselines. Most notably, OmniAlpha achieves a dramatic 84.8% relative reduction in SAD for mask-free matting on AIM-500 and wins over 90% of human preferences in layer-conditioned completion. Our work proves that a unified, multi-task model can learn a superior shared representation for RGBA, paving the way for more powerful, layer-aware generative systems.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20211.png","numComments":2,"submittedBy":{"_id":"669205f1ccca14aa8f13f770","avatarUrl":"/avatars/11ce274e93345fe3790ac9fa687e2bcb.svg","fullname":"Hao Yu","name":"Longin-Yu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2},"organization":{"_id":"64cc8e9b214a472dd85e7e1d","name":"THU1911","fullname":"Tsinghua University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61f8e5934a8e5a275b2b3e5a/oKO6FK_rTzzPHXihicZou.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2511.18734","authors":[{"_id":"6926bf18243b2216fb75cba1","name":"Keyang Lu","hidden":false},{"_id":"6926bf18243b2216fb75cba2","name":"Sifan Zhou","hidden":false},{"_id":"6926bf18243b2216fb75cba3","name":"Hongbin Xu","hidden":false},{"_id":"6926bf18243b2216fb75cba4","name":"Gang Xu","hidden":false},{"_id":"6926bf18243b2216fb75cba5","name":"Zhifei Yang","hidden":false},{"_id":"6926bf18243b2216fb75cba6","name":"Yikai Wang","hidden":false},{"_id":"6926bf18243b2216fb75cba7","name":"Zhen Xiao","hidden":false},{"_id":"6926bf18243b2216fb75cba8","name":"Jieyi Long","hidden":false},{"_id":"6926bf18243b2216fb75cba9","name":"Ming Li","hidden":false}],"publishedAt":"2025-11-24T04:02:48.000Z","submittedOnDailyAt":"2025-11-26T07:01:13.501Z","title":"Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion","submittedOnDailyBy":{"_id":"67f12ff4b3a4cfc21293847a","avatarUrl":"/avatars/d86f1bcfdad2717f526a90819837d883.svg","isPro":false,"fullname":"Lucas","user":"KeyangLu","type":"user"},"summary":"Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo'City first conceptualize the city through a top-down planning strategy that defines a hierarchical \"City-District-Grid\" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a \"produce-refine-evaluate\" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.","upvotes":6,"discussionId":"6926bf19243b2216fb75cbaa","ai_summary":"Yo'City is an agentic framework that uses off-the-shelf large models to generate user-customized, infinitely expandable 3D city scenes with spatial coherence and high quality across multiple evaluation metrics.","ai_keywords":["diffusion model","top-down planning","City-District-Grid","Global Planner","Local Designer","produce-refine-evaluate","isometric image synthesis","image-to-3D generation","scene graph","distance-aware","semantics-aware","layout optimization"]},"publishedAt":"2025-11-23T23:02:48.000Z","title":"Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion","summary":"Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo'City first conceptualize the city through a top-down planning strategy that defines a hierarchical \"City-District-Grid\" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a \"produce-refine-evaluate\" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18734.png","numComments":2,"submittedBy":{"_id":"67f12ff4b3a4cfc21293847a","avatarUrl":"/avatars/d86f1bcfdad2717f526a90819837d883.svg","fullname":"Lucas","name":"KeyangLu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"isAuthorParticipating":false},{"paper":{"id":"2511.17943","authors":[{"_id":"6926c0ef243b2216fb75cbac","user":{"_id":"68f84ad4ba36e04db877ac28","avatarUrl":"/avatars/8ac2d4b6c549082ccdfbfc7b02bcde8c.svg","isPro":false,"fullname":"Dr-Loser","user":"Dr-Loser","type":"user"},"name":"Zhiyu Xu","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:26:49.611Z","hidden":false},{"_id":"6926c0ef243b2216fb75cbad","name":"Weilong Yan","hidden":false},{"_id":"6926c0ef243b2216fb75cbae","name":"Yufei Shi","hidden":false},{"_id":"6926c0ef243b2216fb75cbaf","name":"Xin Meng","hidden":false},{"_id":"6926c0ef243b2216fb75cbb0","name":"Tao He","hidden":false},{"_id":"6926c0ef243b2216fb75cbb1","name":"Huiping Zhuang","hidden":false},{"_id":"6926c0ef243b2216fb75cbb2","name":"Ming Li","hidden":false},{"_id":"6926c0ef243b2216fb75cbb3","name":"Hehe Fan","hidden":false}],"publishedAt":"2025-11-22T06:54:16.000Z","submittedOnDailyAt":"2025-11-26T06:59:01.497Z","title":"SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System","submittedOnDailyBy":{"_id":"68f84ad4ba36e04db877ac28","avatarUrl":"/avatars/8ac2d4b6c549082ccdfbfc7b02bcde8c.svg","isPro":false,"fullname":"Dr-Loser","user":"Dr-Loser","type":"user"},"summary":"Recent advancements in multimodal large language models (MLLMs) and video agent systems have significantly improved general video understanding. However, when applied to scientific video understanding and educating, a domain that demands external professional knowledge integration and rigorous step-wise reasoning, existing approaches often struggle. To bridge this gap, we propose SciEducator, the first iterative self-evolving multi-agent system for scientific video comprehension and education. Rooted in the classical Deming Cycle from management science, our design reformulates its Plan-Do-Study-Act philosophy into a self-evolving reasoning and feedback mechanism, which facilitates the interpretation of intricate scientific activities in videos. Moreover, SciEducator can produce multimodal educational content tailored to specific scientific processes, including textual instructions, visual guides, audio narrations, and interactive references. To support evaluation, we construct SciVBench, a benchmark consisting of 500 expert-verified and literature-grounded science QA pairs across five categories, covering physical, chemical, and everyday phenomena. Extensive experiments demonstrate that SciEducator substantially outperforms leading closed-source MLLMs (e.g., Gemini, GPT-4o) and state-of-the-art video agents on the benchmark, establishing a new paradigm for the community.","upvotes":2,"discussionId":"6926c0ef243b2216fb75cbb4","ai_summary":"SciEducator, an iterative self-evolving multi-agent system, enhances scientific video understanding and education by integrating professional knowledge and step-wise reasoning, outperforming existing models on a new benchmark.","ai_keywords":["multimodal large language models","multi-agent system","Deming Cycle","Plan-Do-Study-Act","scientific video comprehension","multimodal educational content","textual instructions","visual guides","audio narrations","interactive references","SciVBench","science QA pairs","physical phenomena","chemical phenomena","everyday phenomena"],"organization":{"_id":"64e56b3bffba51d24f572668","name":"gml-cn","fullname":"Guang Ming Laboratory"}},"publishedAt":"2025-11-22T01:54:16.000Z","title":"SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System","summary":"Recent advancements in multimodal large language models (MLLMs) and video agent systems have significantly improved general video understanding. However, when applied to scientific video understanding and educating, a domain that demands external professional knowledge integration and rigorous step-wise reasoning, existing approaches often struggle. To bridge this gap, we propose SciEducator, the first iterative self-evolving multi-agent system for scientific video comprehension and education. Rooted in the classical Deming Cycle from management science, our design reformulates its Plan-Do-Study-Act philosophy into a self-evolving reasoning and feedback mechanism, which facilitates the interpretation of intricate scientific activities in videos. Moreover, SciEducator can produce multimodal educational content tailored to specific scientific processes, including textual instructions, visual guides, audio narrations, and interactive references. To support evaluation, we construct SciVBench, a benchmark consisting of 500 expert-verified and literature-grounded science QA pairs across five categories, covering physical, chemical, and everyday phenomena. Extensive experiments demonstrate that SciEducator substantially outperforms leading closed-source MLLMs (e.g., Gemini, GPT-4o) and state-of-the-art video agents on the benchmark, establishing a new paradigm for the community.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17943.png","numComments":2,"submittedBy":{"_id":"68f84ad4ba36e04db877ac28","avatarUrl":"/avatars/8ac2d4b6c549082ccdfbfc7b02bcde8c.svg","fullname":"Dr-Loser","name":"Dr-Loser","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"organization":{"_id":"64e56b3bffba51d24f572668","name":"gml-cn","fullname":"Guang Ming Laboratory"},"isAuthorParticipating":true},{"paper":{"id":"2511.20415","authors":[{"_id":"6926bc60243b2216fb75cb8c","user":{"_id":"6487e158f675b4a7867f45fa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6487e158f675b4a7867f45fa/J0sls6zZ682o-SH7iQs7B.jpeg","isPro":false,"fullname":"Zilong Huang","user":"SereinH","type":"user"},"name":"Zilong Huang","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:26:57.421Z","hidden":false},{"_id":"6926bc60243b2216fb75cb8d","name":"Jun He","hidden":false},{"_id":"6926bc60243b2216fb75cb8e","name":"Xiaobin Huang","hidden":false},{"_id":"6926bc60243b2216fb75cb8f","name":"Ziyi Xiong","hidden":false},{"_id":"6926bc60243b2216fb75cb90","name":"Yang Luo","hidden":false},{"_id":"6926bc60243b2216fb75cb91","name":"Junyan Ye","hidden":false},{"_id":"6926bc60243b2216fb75cb92","name":"Weijia Li","hidden":false},{"_id":"6926bc60243b2216fb75cb93","name":"Yiping Chen","hidden":false},{"_id":"6926bc60243b2216fb75cb94","name":"Ting Han","hidden":false}],"publishedAt":"2025-11-25T15:40:12.000Z","submittedOnDailyAt":"2025-11-26T06:12:00.248Z","title":"MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts","submittedOnDailyBy":{"_id":"6487e158f675b4a7867f45fa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6487e158f675b4a7867f45fa/J0sls6zZ682o-SH7iQs7B.jpeg","isPro":false,"fullname":"Zilong Huang","user":"SereinH","type":"user"},"summary":"Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents a city as a composition of controllable layouts, assets, and materials, and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent} that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop a practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our dataset and code will be released at https://github.com/LongHZ140516/MajutsuCity.","upvotes":8,"discussionId":"6926bc61243b2216fb75cb95","projectPage":"https://longhz140516.github.io/MajutsuCity/","githubRepo":"https://github.com/LongHZ140516/MajutsuCity","ai_summary":"MajutsuCity is a natural language-driven framework that synthesizes 3D urban scenes with high structural consistency, stylistic diversity, and controllability through a four-stage pipeline and interactive editing agent.","ai_keywords":["natural language-driven","aesthetically adaptive","structurally consistent","stylistically diverse","3D urban scenes","controllable layouts","assets","materials","interactive language-grounded editing agent","semantic layouts","height maps","3D building assets","PBR materials","skyboxes","detailed annotations","evaluation metrics","structural consistency","scene complexity","material fidelity","lighting atmosphere","CityDreamer","CityCraft","AQS","RDR scores","geometric fidelity","stylistic adaptability","semantic controllability"],"githubStars":10,"organization":{"_id":"656b30b8edd446c42b243426","name":"SunYatsen","fullname":"Sun Yat-Sen University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/Mn9lkuoOwVkUVziPpg2XZ.png"}},"publishedAt":"2025-11-25T10:40:12.000Z","title":"MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts","summary":"Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents a city as a composition of controllable layouts, assets, and materials, and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent} that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop a practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our dataset and code will be released at https://github.com/LongHZ140516/MajutsuCity.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20415.png","numComments":2,"submittedBy":{"_id":"6487e158f675b4a7867f45fa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6487e158f675b4a7867f45fa/J0sls6zZ682o-SH7iQs7B.jpeg","fullname":"Zilong Huang","name":"SereinH","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1},"organization":{"_id":"656b30b8edd446c42b243426","name":"SunYatsen","fullname":"Sun Yat-Sen University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/Mn9lkuoOwVkUVziPpg2XZ.png"},"isAuthorParticipating":true},{"paper":{"id":"2511.17592","authors":[{"_id":"6926bc70243b2216fb75cb97","name":"Valentin Khrulkov","hidden":false},{"_id":"6926bc70243b2216fb75cb98","user":{"_id":"661e44cf1d8ffc49b57ba07e","avatarUrl":"/avatars/3e937cc4f784b369b9f996ba82d1b81d.svg","isPro":false,"fullname":"Andrey Galichin","user":"andreuka18","type":"user"},"name":"Andrey Galichin","status":"admin_assigned","statusLastChangedAt":"2025-11-26T11:53:55.369Z","hidden":false},{"_id":"6926bc70243b2216fb75cb99","name":"Denis Bashkirov","hidden":false},{"_id":"6926bc70243b2216fb75cb9a","name":"Dmitry Vinichenko","hidden":false},{"_id":"6926bc70243b2216fb75cb9b","user":{"_id":"6926d952a511b9ef1838188f","avatarUrl":"/avatars/f3692c867f3969d94dcc050a3f8807c3.svg","isPro":false,"fullname":"Oleg Travkin","user":"Oitravkin","type":"user"},"name":"Oleg Travkin","status":"claimed_verified","statusLastChangedAt":"2025-11-27T09:59:38.388Z","hidden":false},{"_id":"6926bc70243b2216fb75cb9c","name":"Roman Alferov","hidden":false},{"_id":"6926bc70243b2216fb75cb9d","name":"Andrey Kuznetsov","hidden":false},{"_id":"6926bc70243b2216fb75cb9e","name":"Ivan Oseledets","hidden":false}],"publishedAt":"2025-11-17T14:44:47.000Z","submittedOnDailyAt":"2025-11-26T06:09:36.043Z","title":"GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms","submittedOnDailyBy":{"_id":"643984dceb7c5616ef3f5d54","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg","isPro":false,"fullname":"Andrey Kuznetsov","user":"kuznetsoffandrey","type":"user"},"summary":"Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve (Novikov et al., 2025; Georgiev et al., 2025), have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. However, the high-level descriptions in published work leave many implementation details unspecified, hindering reproducibility and further research. In this report we present GigaEvo, an extensible open-source framework that enables researchers to study and experiment with hybrid LLM-evolution approaches inspired by AlphaEvolve. Our system provides modular implementations of key components: MAP-Elites quality-diversity algorithms, asynchronous DAG-based evaluation pipelines, LLM-driven mutation operators with insight generation and bidirectional lineage tracking, and flexible multi-island evolutionary strategies. In order to assess reproducibility and validate our implementation we evaluate GigaEvo on challenging problems from the AlphaEvolve paper: Heilbronn triangle placement, circle packing in squares, and high-dimensional kissing numbers. The framework emphasizes modularity, concurrency, and ease of experimentation, enabling rapid prototyping through declarative configuration. We provide detailed descriptions of system architecture, implementation decisions, and experimental methodology to support further research in LLM driven evolutionary methods. The GigaEvo framework and all experimental code are available at https://github.com/AIRI-Institute/gigaevo-core.","upvotes":106,"discussionId":"6926bc70243b2216fb75cb9f","projectPage":"https://airi-institute.github.io/gigaevo-cover/","githubRepo":"https://github.com/FusionBrainLab/gigaevo-core","ai_summary":"GigaEvo is an open-source framework for LLM-guided evolutionary computation, offering modular and concurrent tools for research and experimentation in solving complex optimization problems.","ai_keywords":["LLM-guided evolutionary computation","AlphaEvolve","MAP-Elites","quality-diversity algorithms","asynchronous DAG-based evaluation","LLM-driven mutation","bidirectional lineage tracking","multi-island evolutionary strategies","Heilbronn triangle placement","circle packing","high-dimensional kissing numbers"],"githubStars":41,"organization":{"_id":"62a1fefca12f9cb8a15a5219","name":"AIRI-Institute","fullname":" AIRI - Artificial Intelligence Research Institute","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1654783663739-62a1fdd62cfb273c7f41333e.png"}},"publishedAt":"2025-11-17T09:44:47.000Z","title":"GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms","summary":"Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve (Novikov et al., 2025; Georgiev et al., 2025), have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. However, the high-level descriptions in published work leave many implementation details unspecified, hindering reproducibility and further research. In this report we present GigaEvo, an extensible open-source framework that enables researchers to study and experiment with hybrid LLM-evolution approaches inspired by AlphaEvolve. Our system provides modular implementations of key components: MAP-Elites quality-diversity algorithms, asynchronous DAG-based evaluation pipelines, LLM-driven mutation operators with insight generation and bidirectional lineage tracking, and flexible multi-island evolutionary strategies. In order to assess reproducibility and validate our implementation we evaluate GigaEvo on challenging problems from the AlphaEvolve paper: Heilbronn triangle placement, circle packing in squares, and high-dimensional kissing numbers. The framework emphasizes modularity, concurrency, and ease of experimentation, enabling rapid prototyping through declarative configuration. We provide detailed descriptions of system architecture, implementation decisions, and experimental methodology to support further research in LLM driven evolutionary methods. The GigaEvo framework and all experimental code are available at https://github.com/AIRI-Institute/gigaevo-core.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17592.png","numComments":3,"submittedBy":{"_id":"643984dceb7c5616ef3f5d54","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg","fullname":"Andrey Kuznetsov","name":"kuznetsoffandrey","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":25},"organization":{"_id":"62a1fefca12f9cb8a15a5219","name":"AIRI-Institute","fullname":" AIRI - Artificial Intelligence Research Institute","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1654783663739-62a1fdd62cfb273c7f41333e.png"},"isAuthorParticipating":false},{"paper":{"id":"2511.19111","authors":[{"_id":"6926b7e1243b2216fb75cb5d","name":"Hai Ci","hidden":false},{"_id":"6926b7e1243b2216fb75cb5e","name":"Ziheng Peng","hidden":false},{"_id":"6926b7e1243b2216fb75cb5f","name":"Pei Yang","hidden":false},{"_id":"6926b7e1243b2216fb75cb60","name":"Yingxin Xuan","hidden":false},{"_id":"6926b7e1243b2216fb75cb61","name":"Mike Zheng Shou","hidden":false}],"publishedAt":"2025-11-24T13:43:54.000Z","submittedOnDailyAt":"2025-11-26T06:06:51.371Z","title":"DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection","submittedOnDailyBy":{"_id":"647352516a972f252de1fd58","avatarUrl":"/avatars/8e93ca66e01f047c5fb28e1c4f737e8e.svg","isPro":false,"fullname":"Hai Ci","user":"HaiCi","type":"user"},"summary":"Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images--we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models--local edits using eight SOTA diffusion models; 3) Multi-turn editing--each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/Diffseg30k","upvotes":3,"discussionId":"6926b7e1243b2216fb75cb62","projectPage":"https://huggingface.co/datasets/Chaos2629/Diffseg30k","ai_summary":"DiffSeg30k, a dataset of 30k diffusion-edited images, supports fine-grained detection of AI-generated content through semantic segmentation.","ai_keywords":["diffusion-based editing","DiffSeg30k","pixel-level annotations","in-the-wild images","diffusion models","multi-turn editing","vision-language model","semantic segmentation","image distortions","forgery classifiers","cross-generator generalization"]},"publishedAt":"2025-11-24T08:43:54.000Z","title":"DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection","summary":"Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images--we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models--local edits using eight SOTA diffusion models; 3) Multi-turn editing--each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/Diffseg30k","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19111.png","numComments":2,"submittedBy":{"_id":"647352516a972f252de1fd58","avatarUrl":"/avatars/8e93ca66e01f047c5fb28e1c4f737e8e.svg","fullname":"Hai Ci","name":"HaiCi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"isAuthorParticipating":false},{"paper":{"id":"2511.20347","authors":[{"_id":"69268ffb243b2216fb75cb0b","name":"Chang Gao","hidden":false},{"_id":"69268ffb243b2216fb75cb0c","name":"Chujie Zheng","hidden":false},{"_id":"69268ffb243b2216fb75cb0d","name":"Xiong-Hui Chen","hidden":false},{"_id":"69268ffb243b2216fb75cb0e","name":"Kai Dang","hidden":false},{"_id":"69268ffb243b2216fb75cb0f","name":"Shixuan Liu","hidden":false},{"_id":"69268ffb243b2216fb75cb10","name":"Bowen Yu","hidden":false},{"_id":"69268ffb243b2216fb75cb11","user":{"_id":"62088594a5943c8a8fc94560","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1644733028938-62088594a5943c8a8fc94560.png","isPro":false,"fullname":"An Yang","user":"yangapku","type":"user"},"name":"An Yang","status":"claimed_verified","statusLastChangedAt":"2025-11-26T11:31:25.265Z","hidden":false},{"_id":"69268ffb243b2216fb75cb12","name":"Shuai Bai","hidden":false},{"_id":"69268ffb243b2216fb75cb13","name":"Jingren Zhou","hidden":false},{"_id":"69268ffb243b2216fb75cb14","name":"Junyang Lin","hidden":false}],"publishedAt":"2025-11-25T14:25:19.000Z","submittedOnDailyAt":"2025-11-26T04:31:56.255Z","title":"Soft Adaptive Policy Optimization","submittedOnDailyBy":{"_id":"63451cf0a05b51f7ded25505","avatarUrl":"/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg","isPro":false,"fullname":"shuai bai","user":"ShuaiBai623","type":"user"},"summary":"Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.","upvotes":26,"discussionId":"69268ffb243b2216fb75cb15","ai_summary":"Soft Adaptive Policy Optimization (SAPO) enhances the stability and performance of reinforcement learning in large language models by adaptively attenuating off-policy updates with a smooth, temperature-controlled gate, leading to improved training stability and performance.","ai_keywords":["reinforcement learning","large language models","policy optimization","token-level importance ratios","Mixture-of-Experts models","GSPO","GRPO","Soft Adaptive Policy Optimization","sequence-coherent","token-adaptive","sequence-level coherence","off-policy updates","sample efficiency","Pass@1 performance","Qwen3-VL model series"],"organization":{"_id":"64c8b5837fe12ecd0a7e92eb","name":"Qwen","fullname":"Qwen","avatar":"https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}},"publishedAt":"2025-11-25T09:25:19.000Z","title":"Soft Adaptive Policy Optimization","summary":"Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20347.png","numComments":3,"submittedBy":{"_id":"63451cf0a05b51f7ded25505","avatarUrl":"/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg","fullname":"shuai bai","name":"ShuaiBai623","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":39},"organization":{"_id":"64c8b5837fe12ecd0a7e92eb","name":"Qwen","fullname":"Qwen","avatar":"https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"},"isAuthorParticipating":false},{"paper":{"id":"2511.20573","authors":[{"_id":"69266e53243b2216fb75ca64","name":"Chenhui Gou","hidden":false},{"_id":"69266e53243b2216fb75ca65","name":"Zilong Chen","hidden":false},{"_id":"69266e53243b2216fb75ca66","name":"Zeyu Wang","hidden":false},{"_id":"69266e53243b2216fb75ca67","name":"Feng Li","hidden":false},{"_id":"69266e53243b2216fb75ca68","name":"Deyao Zhu","hidden":false},{"_id":"69266e53243b2216fb75ca69","user":{"_id":"64e4446756b920ef00f2031c","avatarUrl":"/avatars/86305d1eac3917bf9859ac0ddbb4e845.svg","isPro":false,"fullname":"Zicheng Duan","user":"ZichengD","type":"user"},"name":"Zicheng Duan","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:35.069Z","hidden":false},{"_id":"69266e53243b2216fb75ca6a","name":"Kunchang Li","hidden":false},{"_id":"69266e53243b2216fb75ca6b","name":"Chaorui Deng","hidden":false},{"_id":"69266e53243b2216fb75ca6c","name":"Hongyi Yuan","hidden":false},{"_id":"69266e53243b2216fb75ca6d","name":"Haoqi Fan","hidden":false},{"_id":"69266e53243b2216fb75ca6e","name":"Cihang Xie","hidden":false},{"_id":"69266e53243b2216fb75ca6f","name":"Jianfei Cai","hidden":false},{"_id":"69266e53243b2216fb75ca70","name":"Hamid Rezatofighi","hidden":false}],"publishedAt":"2025-11-25T18:06:22.000Z","submittedOnDailyAt":"2025-11-26T03:38:13.028Z","title":"VQ-VA World: Towards High-Quality Visual Question-Visual Answering","submittedOnDailyBy":{"_id":"652e9c5774d1b0d7ff73d091","avatarUrl":"/avatars/a6d2098b3dde4a8b7488a193f0ecb776.svg","isPro":true,"fullname":"Chenhui Gou","user":"gouc","type":"user"},"summary":"This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.","upvotes":7,"discussionId":"69266e53243b2216fb75ca71","ai_summary":"A data-centric framework and benchmark for Visual Question-Visual Answering (VQ-VA) improve open-source model performance, narrowing the gap with proprietary systems.","ai_keywords":["Visual Question-Visual Answering","VQ-VA","agentic pipeline","web-scale deployment","image-text samples","IntelligentBench","world knowledge","design knowledge","reasoning","LightFusion","UniWorld-V1","NanoBanana","GPT-Image"]},"publishedAt":"2025-11-25T13:06:22.000Z","title":"VQ-VA World: Towards High-Quality Visual Question-Visual Answering","summary":"This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20573.png","numComments":2,"submittedBy":{"_id":"652e9c5774d1b0d7ff73d091","avatarUrl":"/avatars/a6d2098b3dde4a8b7488a193f0ecb776.svg","fullname":"Chenhui Gou","name":"gouc","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3},"isAuthorParticipating":false},{"paper":{"id":"2511.20123","authors":[{"_id":"6926928b243b2216fb75cb17","name":"Min Zhao","hidden":false},{"_id":"6926928b243b2216fb75cb18","user":{"_id":"64c269a52d73768f07ac266c","avatarUrl":"/avatars/d497a960f8aef6a974907b68ed750c1c.svg","isPro":false,"fullname":"Zhu Hongzhou","user":"zhuhz22","type":"user"},"name":"Hongzhou Zhu","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:18.068Z","hidden":false},{"_id":"6926928b243b2216fb75cb19","name":"Yingze Wang","hidden":false},{"_id":"6926928b243b2216fb75cb1a","name":"Bokai Yan","hidden":false},{"_id":"6926928b243b2216fb75cb1b","name":"Jintao Zhang","hidden":false},{"_id":"6926928b243b2216fb75cb1c","name":"Guande He","hidden":false},{"_id":"6926928b243b2216fb75cb1d","name":"Ling Yang","hidden":false},{"_id":"6926928b243b2216fb75cb1e","name":"Chongxuan Li","hidden":false},{"_id":"6926928b243b2216fb75cb1f","name":"Jun Zhu","hidden":false}],"publishedAt":"2025-11-25T09:44:10.000Z","submittedOnDailyAt":"2025-11-26T03:16:49.152Z","title":"UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers","submittedOnDailyBy":{"_id":"64c269a52d73768f07ac266c","avatarUrl":"/avatars/d497a960f8aef6a974907b68ed750c1c.svg","isPro":false,"fullname":"Zhu Hongzhou","user":"zhuhz22","type":"user"},"summary":"Despite advances, video diffusion transformers still struggle to generalize beyond their training length, a challenge we term video length extrapolation. We identify two failure modes: model-specific periodic content repetition and a universal quality degradation. Prior works attempt to solve repetition via positional encodings, overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from a more fundamental view: attention maps, which directly govern how context influences outputs. We identify that both failure modes arise from a unified cause: attention dispersion, where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as a special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings. Building on this insight, we propose UltraViCo, a training-free, plug-and-play method that suppresses attention for tokens beyond the training window via a constant decay factor. By jointly addressing both failure modes, we outperform a broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2x to 4x. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4x extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing.","upvotes":15,"discussionId":"6926928b243b2216fb75cb20","projectPage":"https://thu-ml.github.io/UltraViCo.github.io/","githubRepo":"https://github.com/thu-ml/DiT-Extrapolation","ai_summary":"UltraViCo addresses video length extrapolation by suppressing attention dispersion, improving quality and reducing repetition beyond training length.","ai_keywords":["video diffusion transformers","video length extrapolation","positional encodings","attention maps","attention dispersion","UltraViCo","Dynamic Degree","Imaging Quality","controllable video synthesis","editing"],"githubStars":745,"organization":{"_id":"640d3084536d9fe0f005cac3","name":"thu-ml","fullname":"Tsinghua Machine Learning Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1678587085174-633131798ef21f47308ce49b.jpeg"}},"publishedAt":"2025-11-25T04:44:10.000Z","title":"UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers","summary":"Despite advances, video diffusion transformers still struggle to generalize beyond their training length, a challenge we term video length extrapolation. We identify two failure modes: model-specific periodic content repetition and a universal quality degradation. Prior works attempt to solve repetition via positional encodings, overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from a more fundamental view: attention maps, which directly govern how context influences outputs. We identify that both failure modes arise from a unified cause: attention dispersion, where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as a special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings. Building on this insight, we propose UltraViCo, a training-free, plug-and-play method that suppresses attention for tokens beyond the training window via a constant decay factor. By jointly addressing both failure modes, we outperform a broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2x to 4x. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4x extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20123.png","numComments":2,"submittedBy":{"_id":"64c269a52d73768f07ac266c","avatarUrl":"/avatars/d497a960f8aef6a974907b68ed750c1c.svg","fullname":"Zhu Hongzhou","name":"zhuhz22","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3},"organization":{"_id":"640d3084536d9fe0f005cac3","name":"thu-ml","fullname":"Tsinghua Machine Learning Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1678587085174-633131798ef21f47308ce49b.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2511.19827","authors":[{"_id":"69268cde243b2216fb75caf9","user":{"_id":"653929a66da48e0d21e65e17","avatarUrl":"/avatars/e34ae3411d689b4280ff34c1b680f283.svg","isPro":false,"fullname":"Byeongjun Park","user":"byeongjun-park","type":"user"},"name":"Byeongjun Park","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:20.640Z","hidden":false},{"_id":"69268cde243b2216fb75cafa","name":"Byung-Hoon Kim","hidden":false},{"_id":"69268cde243b2216fb75cafb","name":"Hyungjin Chung","hidden":false},{"_id":"69268cde243b2216fb75cafc","name":"Jong Chul Ye","hidden":false}],"publishedAt":"2025-11-25T01:38:56.000Z","submittedOnDailyAt":"2025-11-26T02:48:52.934Z","title":"ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding","submittedOnDailyBy":{"_id":"653929a66da48e0d21e65e17","avatarUrl":"/avatars/e34ae3411d689b4280ff34c1b680f283.svg","isPro":false,"fullname":"Byeongjun Park","user":"byeongjun-park","type":"user"},"summary":"We present ReDirector, a novel camera-controlled video retake generation method for dynamically captured variable-length videos. In particular, we rectify a common misuse of RoPE in previous works by aligning the spatiotemporal positions of the input video and the target retake. Moreover, we introduce Rotary Camera Encoding (RoCE), a camera-conditioned RoPE phase shift that captures and integrates multi-view relationships within and across the input and target videos. By integrating camera conditions into RoPE, our method generalizes to out-of-distribution camera trajectories and video lengths, yielding improved dynamic object localization and static background preservation. Extensive experiments further demonstrate significant improvements in camera controllability, geometric consistency, and video quality across various trajectories and lengths.","upvotes":11,"discussionId":"69268cdf243b2216fb75cafd","projectPage":"https://byeongjun-park.github.io/ReDirector/","githubRepo":"https://github.com/byeongjun-park/ReDirector","ai_summary":"ReDirector uses a novel camera-controlled video retake method with Rotary Camera Encoding (RoCE) to improve dynamic object localization and static background preservation in variable-length videos.","ai_keywords":["RoPE","Rotary Camera Encoding","RoCE","camera-controlled video retake","spatiotemporal positions","multi-view relationships","dynamic object localization","static background preservation","camera controllability","geometric consistency","video quality"],"githubStars":9,"organization":{"_id":"64ab689073790912c7a8717a","name":"everex","fullname":"EverEx","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64ab6841723beceb2f45c9da/cdIuwsqJw-2tlEhnco0kI.png"}},"publishedAt":"2025-11-24T20:38:56.000Z","title":"ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding","summary":"We present ReDirector, a novel camera-controlled video retake generation method for dynamically captured variable-length videos. In particular, we rectify a common misuse of RoPE in previous works by aligning the spatiotemporal positions of the input video and the target retake. Moreover, we introduce Rotary Camera Encoding (RoCE), a camera-conditioned RoPE phase shift that captures and integrates multi-view relationships within and across the input and target videos. By integrating camera conditions into RoPE, our method generalizes to out-of-distribution camera trajectories and video lengths, yielding improved dynamic object localization and static background preservation. Extensive experiments further demonstrate significant improvements in camera controllability, geometric consistency, and video quality across various trajectories and lengths.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19827.png","numComments":2,"submittedBy":{"_id":"653929a66da48e0d21e65e17","avatarUrl":"/avatars/e34ae3411d689b4280ff34c1b680f283.svg","fullname":"Byeongjun Park","name":"byeongjun-park","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1},"organization":{"_id":"64ab689073790912c7a8717a","name":"everex","fullname":"EverEx","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64ab6841723beceb2f45c9da/cdIuwsqJw-2tlEhnco0kI.png"},"isAuthorParticipating":true},{"paper":{"id":"2511.19900","authors":[{"_id":"6926877a243b2216fb75caca","user":{"_id":"684ff37fa383bc5d6b0ff77f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0JPr-cd_rxQz3k6rmzBOF.png","isPro":false,"fullname":"JiaqiLiu","user":"JiaaqiLiu","type":"user"},"name":"Jiaqi Liu","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:22.964Z","hidden":false},{"_id":"6926877a243b2216fb75cacb","name":"Kaiwen Xiong","hidden":false},{"_id":"6926877a243b2216fb75cacc","user":{"_id":"643e9ee6f6bb3c31a26e7bc4","avatarUrl":"/avatars/acfaa7d6a23dada24c86b954c3be116a.svg","isPro":false,"fullname":"Peng Xia","user":"richardxp888","type":"user"},"name":"Peng Xia","status":"claimed_verified","statusLastChangedAt":"2025-11-27T09:59:53.380Z","hidden":false},{"_id":"6926877a243b2216fb75cacd","name":"Yiyang Zhou","hidden":false},{"_id":"6926877a243b2216fb75cace","name":"Haonian Ji","hidden":false},{"_id":"6926877a243b2216fb75cacf","name":"Lu Feng","hidden":false},{"_id":"6926877a243b2216fb75cad0","name":"Siwei Han","hidden":false},{"_id":"6926877a243b2216fb75cad1","name":"Mingyu Ding","hidden":false},{"_id":"6926877a243b2216fb75cad2","name":"Huaxiu Yao","hidden":false}],"publishedAt":"2025-11-25T04:15:14.000Z","submittedOnDailyAt":"2025-11-26T02:25:56.245Z","title":"Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning","submittedOnDailyBy":{"_id":"684ff37fa383bc5d6b0ff77f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0JPr-cd_rxQz3k6rmzBOF.png","isPro":false,"fullname":"JiaqiLiu","user":"JiaaqiLiu","type":"user"},"summary":"Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at https://github.com/aiming-lab/Agent0/Agent0-VL{this https URL}.","upvotes":42,"discussionId":"6926877a243b2216fb75cad3","projectPage":"https://github.com/aiming-lab/Agent0/tree/main/Agent0-VL","githubRepo":"https://github.com/aiming-lab/Agent0/tree/main/Agent0-VL","ai_summary":"Agent0-VL, a self-evolving vision-language agent, incorporates tool usage into both reasoning and self-evaluation, enabling continual improvement through evidence-grounded analysis and reinforcement learning.","ai_keywords":["self-rewarding approaches","tool-integrated reasoning","self-evaluation","self-repair","introspect","evidence-grounded analysis","Solver","Verifier","Self-Evolving Reasoning Cycle","reinforcement learning","geometric problem solving","visual scientific analysis"],"githubStars":544,"organization":{"_id":"669f9d1fec8789263c0e355a","name":"UNC-ChapelHill","fullname":"University of North Carolina at Chapel Hill","avatar":"https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"}},"publishedAt":"2025-11-24T23:15:14.000Z","title":"Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning","summary":"Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at https://github.com/aiming-lab/Agent0/Agent0-VL{this https URL}.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19900.png","numComments":2,"submittedBy":{"_id":"684ff37fa383bc5d6b0ff77f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0JPr-cd_rxQz3k6rmzBOF.png","fullname":"JiaqiLiu","name":"JiaaqiLiu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2},"organization":{"_id":"669f9d1fec8789263c0e355a","name":"UNC-ChapelHill","fullname":"University of North Carolina at Chapel Hill","avatar":"https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"},"isAuthorParticipating":true},{"paper":{"id":"2511.19773","authors":[{"_id":"69268437243b2216fb75cab7","name":"Meng Lu","hidden":false},{"_id":"69268437243b2216fb75cab8","name":"Ran Xu","hidden":false},{"_id":"69268437243b2216fb75cab9","name":"Yi Fang","hidden":false},{"_id":"69268437243b2216fb75caba","name":"Wenxuan Zhang","hidden":false},{"_id":"69268437243b2216fb75cabb","name":"Yue Yu","hidden":false},{"_id":"69268437243b2216fb75cabc","name":"Gaurav Srivastava","hidden":false},{"_id":"69268437243b2216fb75cabd","name":"Yuchen Zhuang","hidden":false},{"_id":"69268437243b2216fb75cabe","name":"Mohamed Elhoseiny","hidden":false},{"_id":"69268437243b2216fb75cabf","name":"Charles Fleming","hidden":false},{"_id":"69268437243b2216fb75cac0","name":"Carl Yang","hidden":false},{"_id":"69268437243b2216fb75cac1","name":"Zhengzhong Tu","hidden":false},{"_id":"69268437243b2216fb75cac2","name":"Yang Xie","hidden":false},{"_id":"69268437243b2216fb75cac3","name":"Guanghua Xiao","hidden":false},{"_id":"69268437243b2216fb75cac4","name":"Hanrui Wang","hidden":false},{"_id":"69268437243b2216fb75cac5","name":"Di Jin","hidden":false},{"_id":"69268437243b2216fb75cac6","user":{"_id":"65cae89119683f9817c049ea","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65cae89119683f9817c049ea/A0XxjmaJldu28JhFvWmpP.jpeg","isPro":false,"fullname":"Wenqi Shi","user":"wshi83","type":"user"},"name":"Wenqi Shi","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:25.162Z","hidden":false},{"_id":"69268437243b2216fb75cac7","name":"Xuan Wang","hidden":false}],"publishedAt":"2025-11-24T22:58:26.000Z","submittedOnDailyAt":"2025-11-26T02:09:15.051Z","title":"Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs","submittedOnDailyBy":{"_id":"65cae89119683f9817c049ea","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65cae89119683f9817c049ea/A0XxjmaJldu28JhFvWmpP.jpeg","isPro":false,"fullname":"Wenqi Shi","user":"wshi83","type":"user"},"summary":"While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to \"think with images\", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.","upvotes":8,"discussionId":"69268438243b2216fb75cac8","ai_summary":"VISTA-Gym enhances vision-language models' tool-integrated visual reasoning through a scalable training environment with diverse multimodal tasks and reinforcement learning.","ai_keywords":["vision-language models","VLMs","VISTA-Gym","multimodal reasoning","visual tools","executable interaction loops","verifiable feedback signals","trajectory logging","visual agentic reinforcement learning","tool selection","tool invocation","tool coordination","multi-turn trajectory sampling","end-to-end reinforcement learning","VQA benchmarks","VISTA-R1-8B"],"organization":{"_id":"688ec8e2ea39631b70dc34db","name":"eigen-ai-labs","fullname":"Eigen AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6514d59cf95f39fd02acd9dc/oELb680wyZPzGhhO-w1pG.png"}},"publishedAt":"2025-11-24T17:58:26.000Z","title":"Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs","summary":"While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to \"think with images\", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19773.png","numComments":2,"submittedBy":{"_id":"65cae89119683f9817c049ea","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65cae89119683f9817c049ea/A0XxjmaJldu28JhFvWmpP.jpeg","fullname":"Wenqi Shi","name":"wshi83","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"organization":{"_id":"688ec8e2ea39631b70dc34db","name":"eigen-ai-labs","fullname":"Eigen AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6514d59cf95f39fd02acd9dc/oELb680wyZPzGhhO-w1pG.png"},"isAuthorParticipating":true},{"paper":{"id":"2511.20561","authors":[{"_id":"692671a3243b2216fb75ca91","name":"Yuwei Niu","hidden":false},{"_id":"692671a3243b2216fb75ca92","user":{"_id":"66608add236f958513d21d2e","avatarUrl":"/avatars/53eca0891c98cbb93be899885160a983.svg","isPro":false,"fullname":"Weiyang Jin","user":"Wayne-King","type":"user"},"name":"Weiyang Jin","status":"claimed_verified","statusLastChangedAt":"2025-11-27T09:59:57.589Z","hidden":false},{"_id":"692671a3243b2216fb75ca93","name":"Jiaqi Liao","hidden":false},{"_id":"692671a3243b2216fb75ca94","name":"Chaoran Feng","hidden":false},{"_id":"692671a3243b2216fb75ca95","name":"Peng Jin","hidden":false},{"_id":"692671a3243b2216fb75ca96","name":"Bin Lin","hidden":false},{"_id":"692671a3243b2216fb75ca97","name":"Zongjian Li","hidden":false},{"_id":"692671a3243b2216fb75ca98","name":"Bin Zhu","hidden":false},{"_id":"692671a3243b2216fb75ca99","name":"Weihao Yu","hidden":false},{"_id":"692671a3243b2216fb75ca9a","name":"Li Yuan","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/66608add236f958513d21d2e/LUJu9PnWzLTa49ONkbY5H.png"],"publishedAt":"2025-11-25T17:58:48.000Z","submittedOnDailyAt":"2025-11-26T01:19:47.715Z","title":"Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward","submittedOnDailyBy":{"_id":"66608add236f958513d21d2e","avatarUrl":"/avatars/53eca0891c98cbb93be899885160a983.svg","isPro":false,"fullname":"Weiyang Jin","user":"Wayne-King","type":"user"},"summary":"Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox","upvotes":28,"discussionId":"692671a3243b2216fb75ca9b","githubRepo":"https://github.com/PKU-YuanGroup/UniSandBox","ai_summary":"UniSandbox evaluates Unified Multimodal Models, revealing a gap between understanding and generation, and identifies Chain-of-Thought and self-training as means to bridge this gap.","ai_keywords":["Unified Multimodal Models","UniSandbox","decoupled evaluation framework","synthetic datasets","understanding-generation gap","reasoning generation","knowledge transfer","Chain-of-Thought","self-training","query-based architectures"],"githubStars":25,"organization":{"_id":"61dcd8e344f59573371b5cb6","name":"PekingUniversity","fullname":"Peking University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}},"publishedAt":"2025-11-25T12:58:48.000Z","title":"Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward","summary":"Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/66608add236f958513d21d2e/LUJu9PnWzLTa49ONkbY5H.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20561.png","numComments":2,"submittedBy":{"_id":"66608add236f958513d21d2e","avatarUrl":"/avatars/53eca0891c98cbb93be899885160a983.svg","fullname":"Weiyang Jin","name":"Wayne-King","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4},"organization":{"_id":"61dcd8e344f59573371b5cb6","name":"PekingUniversity","fullname":"Peking University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"},"isAuthorParticipating":true},{"paper":{"id":"2511.19046","authors":[{"_id":"69267465243b2216fb75ca9d","user":{"_id":"66214b651fc3a06144ef8f4b","avatarUrl":"/avatars/dab669ad56b67805d55fef8c4fcf1326.svg","isPro":false,"fullname":"Anglin Liu","user":"lal-Joey","type":"user"},"name":"Anglin Liu","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:33.112Z","hidden":false},{"_id":"69267465243b2216fb75ca9e","name":"Rundong Xue","hidden":false},{"_id":"69267465243b2216fb75ca9f","user":{"_id":"5fc9f05d52770aca770bd3d9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/5fc9f05d52770aca770bd3d9/axiKCXPYttXEWC-RqCSfh.jpeg","isPro":true,"fullname":"Xu Cao","user":"IrohXu","type":"user"},"name":"Xu R. Cao","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:30.698Z","hidden":false},{"_id":"69267465243b2216fb75caa0","user":{"_id":"65e387095132c2edd193ae49","avatarUrl":"/avatars/39278e5b026bcbdde88c560fc54018c5.svg","isPro":false,"fullname":"Yifan Shen","user":"SivanSX","type":"user"},"name":"Yifan Shen","status":"claimed_verified","statusLastChangedAt":"2025-11-27T09:59:55.425Z","hidden":false},{"_id":"69267465243b2216fb75caa1","name":"Yi Lu","hidden":false},{"_id":"69267465243b2216fb75caa2","name":"Xiang Li","hidden":false},{"_id":"69267465243b2216fb75caa3","name":"Qianqian Chen","hidden":false},{"_id":"69267465243b2216fb75caa4","name":"Jintai Chen","hidden":false}],"publishedAt":"2025-11-24T12:34:38.000Z","submittedOnDailyAt":"2025-11-26T01:01:06.095Z","title":"MedSAM3: Delving into Segment Anything with Medical Concepts","submittedOnDailyBy":{"_id":"65e387095132c2edd193ae49","avatarUrl":"/avatars/39278e5b026bcbdde88c560fc54018c5.svg","isPro":false,"fullname":"Yifan Shen","user":"SivanSX","type":"user"},"summary":"Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.","upvotes":44,"discussionId":"69267466243b2216fb75caa5","githubRepo":"https://github.com/Joey-S-Liu/MedSAM3","ai_summary":"MedSAM-3, a text-promptable medical segmentation model fine-tuned on SAM 3 architecture, achieves superior performance across various medical imaging modalities using semantic conceptual labels and multimodal large language models.","ai_keywords":["Segment Anything Model (SAM)","text promptable","medical segmentation","Promptable Concept Segmentation (PCS)","Multimodal Large Language Models (MLLMs)","X-ray","MRI","Ultrasound","CT","video"],"githubStars":59},"publishedAt":"2025-11-24T07:34:38.000Z","title":"MedSAM3: Delving into Segment Anything with Medical Concepts","summary":"Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19046.png","numComments":3,"submittedBy":{"_id":"65e387095132c2edd193ae49","avatarUrl":"/avatars/39278e5b026bcbdde88c560fc54018c5.svg","fullname":"Yifan Shen","name":"SivanSX","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1},"isAuthorParticipating":true},{"paper":{"id":"2511.19663","authors":[{"_id":"69266fb9243b2216fb75ca7c","name":"Ahmed Awadallah","hidden":false},{"_id":"69266fb9243b2216fb75ca7d","name":"Yash Lara","hidden":false},{"_id":"69266fb9243b2216fb75ca7e","name":"Raghav Magazine","hidden":false},{"_id":"69266fb9243b2216fb75ca7f","name":"Hussein Mozannar","hidden":false},{"_id":"69266fb9243b2216fb75ca80","user":{"_id":"64aba383fddf117e6e5ba818","avatarUrl":"/avatars/ee7d25d865b34be5902872d060ad9153.svg","isPro":false,"fullname":"Akshay  Nambi","user":"akshaynambi","type":"user"},"name":"Akshay Nambi","status":"claimed_verified","statusLastChangedAt":"2025-11-27T09:59:59.467Z","hidden":false},{"_id":"69266fb9243b2216fb75ca81","name":"Yash Pandya","hidden":false},{"_id":"69266fb9243b2216fb75ca82","name":"Aravind Rajeswaran","hidden":false},{"_id":"69266fb9243b2216fb75ca83","name":"Corby Rosset","hidden":false},{"_id":"69266fb9243b2216fb75ca84","name":"Alexey Taymanov","hidden":false},{"_id":"69266fb9243b2216fb75ca85","name":"Vibhav Vineet","hidden":false},{"_id":"69266fb9243b2216fb75ca86","name":"Spencer Whitehead","hidden":false},{"_id":"69266fb9243b2216fb75ca87","name":"Andrew Zhao","hidden":false}],"publishedAt":"2025-11-24T19:56:28.000Z","submittedOnDailyAt":"2025-11-26T00:41:03.809Z","title":"Fara-7B: An Efficient Agentic Model for Computer Use","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.","upvotes":8,"discussionId":"69266fb9243b2216fb75ca88","ai_summary":"FaraGen creates synthetic datasets for computer use agents, enabling the training of efficient and high-performing models like Fara-7B on diverse web tasks, outperforming larger models on benchmarks.","ai_keywords":["FaraGen","synthetic data generation","multi-step web tasks","CUA trajectories","verifiers","Fara-7B","on-device model","native CUA model","screenshots","predicted coordinates","WebVoyager","Online-Mind2Web","WebTailBench"],"organization":{"_id":"5e6485f787403103f9f1055e","name":"microsoft","fullname":"Microsoft","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"}},"publishedAt":"2025-11-24T14:56:28.000Z","title":"Fara-7B: An Efficient Agentic Model for Computer Use","summary":"Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19663.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":171},"organization":{"_id":"5e6485f787403103f9f1055e","name":"microsoft","fullname":"Microsoft","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"},"isAuthorParticipating":false}]
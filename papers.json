[{"paper":{"id":"2511.20647","authors":[{"_id":"69272506243b2216fb75cc87","name":"Tahira Kazimi","hidden":false},{"_id":"69272506243b2216fb75cc88","name":"Connor Dunlop","hidden":false},{"_id":"69272506243b2216fb75cc89","name":"Pinar Yanardag","hidden":false}],"publishedAt":"2025-11-25T18:59:45.000Z","submittedOnDailyAt":"2025-11-26T13:35:37.458Z","title":"Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization","submittedOnDailyBy":{"_id":"66fec311343c151be4fb0b73","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FXeHseCn4DKSVUzk-VKdA.png","isPro":false,"fullname":"Tahira Kazimi","user":"tahirakazimi77","type":"user"},"summary":"While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.","upvotes":0,"discussionId":"69272506243b2216fb75cc8a","projectPage":"https://diverse-video.github.io/","ai_summary":"A framework combining Determinantal Point Processes and Group Relative Policy Optimization enhances diversity in text-to-video generation without compromising quality.","ai_keywords":["text-to-video (T2V) diffusion models","set-level policy optimization","Determinantal Point Processes (DPPs)","Group Relative Policy Optimization (GRPO)","diverse video generation","visual appearance","camera motions","scene structure","VBench","VideoScore"],"organization":{"_id":"641f3b58a390e539522a6f88","name":"VirginiaTech","fullname":"Virginia Polytechnic Institute and State University"}},"publishedAt":"2025-11-25T13:59:45.000Z","title":"Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization","summary":"While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20647.png","numComments":1,"submittedBy":{"_id":"66fec311343c151be4fb0b73","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FXeHseCn4DKSVUzk-VKdA.png","fullname":"Tahira Kazimi","name":"tahirakazimi77","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"organization":{"_id":"641f3b58a390e539522a6f88","name":"VirginiaTech","fullname":"Virginia Polytechnic Institute and State University"},"isAuthorParticipating":false},{"paper":{"id":"2511.18886","authors":[{"_id":"6926f72b243b2216fb75cc37","name":"Guangyuan Li","hidden":false},{"_id":"6926f72b243b2216fb75cc38","name":"Siming Zheng","hidden":false},{"_id":"6926f72b243b2216fb75cc39","name":"Shuolin Xu","hidden":false},{"_id":"6926f72b243b2216fb75cc3a","name":"Jinwei Chen","hidden":false},{"_id":"6926f72b243b2216fb75cc3b","name":"Bo Li","hidden":false},{"_id":"6926f72b243b2216fb75cc3c","name":"Xiaobin Hu","hidden":false},{"_id":"6926f72b243b2216fb75cc3d","name":"Lei Zhao","hidden":false},{"_id":"6926f72b243b2216fb75cc3e","name":"Peng-Tao Jiang","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6423efbdb77cc3daf8429755/6jxbJARvlMKsVKSaaVDVP.qt"],"publishedAt":"2025-11-24T08:41:28.000Z","submittedOnDailyAt":"2025-11-26T12:58:15.558Z","title":"MagicWorld: Interactive Geometry-driven Video World Exploration","submittedOnDailyBy":{"_id":"6423efbdb77cc3daf8429755","avatarUrl":"/avatars/a5b480713b4dd1dae8191545cb4c6f94.svg","isPro":false,"fullname":"Peng-Tao Jiang","user":"ptjiang","type":"user"},"summary":"Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.","upvotes":3,"discussionId":"6926f72b243b2216fb75cc3f","ai_summary":"MagicWorld, an interactive video world model, integrates 3D geometry and historical retrieval to improve scene stability and continuity under user instructions.","ai_keywords":["3D geometric priors","historical retrieval","Action-Guided 3D Geometry Module","AG3D","point cloud","viewpoint transitions","History Cache Retrieval","HCR","scene evolution","dynamic scene evolution","autoregressive synthesis"]},"publishedAt":"2025-11-24T03:41:28.000Z","title":"MagicWorld: Interactive Geometry-driven Video World Exploration","summary":"Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6423efbdb77cc3daf8429755/6jxbJARvlMKsVKSaaVDVP.qt"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18886.png","numComments":2,"submittedBy":{"_id":"6423efbdb77cc3daf8429755","avatarUrl":"/avatars/a5b480713b4dd1dae8191545cb4c6f94.svg","fullname":"Peng-Tao Jiang","name":"ptjiang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1},"isAuthorParticipating":false},{"paper":{"id":"2511.19430","authors":[{"_id":"69271171243b2216fb75cc51","name":"Dingkang Liang","hidden":false},{"_id":"69271171243b2216fb75cc52","name":"Cheng Zhang","hidden":false},{"_id":"69271171243b2216fb75cc53","name":"Xiaopeng Xu","hidden":false},{"_id":"69271171243b2216fb75cc54","name":"Jianzhong Ju","hidden":false},{"_id":"69271171243b2216fb75cc55","name":"Zhenbo Luo","hidden":false},{"_id":"69271171243b2216fb75cc56","name":"Xiang Bai","hidden":false}],"publishedAt":"2025-11-24T18:59:17.000Z","submittedOnDailyAt":"2025-11-26T12:11:33.720Z","title":"Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution","submittedOnDailyBy":{"_id":"67467b5979406f42a14517e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67467b5979406f42a14517e9/wgnUxTd8vWOo0Cr2gaoyG.jpeg","isPro":false,"fullname":"Dingkang Liang","user":"dkliang","type":"user"},"summary":"Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT","upvotes":5,"discussionId":"69271171243b2216fb75cc57","projectPage":"https://h-embodvis.github.io/GRANT/","githubRepo":"https://github.com/H-EmbodVis/GRANT","ai_summary":"ORS3D, a new task requiring language understanding, 3D grounding, and efficient scheduling, is introduced with a large dataset and an embodied multi-modal model named GRANT that uses a scheduling token mechanism for effective task management.","ai_keywords":["Operations Research knowledge-based 3D Grounded Task Scheduling","ORS3D","3D grounding","efficiency optimization","parallelizable subtasks","embodied multi-modal large language model","GRANT","scheduling token mechanism"],"githubStars":97,"organization":{"_id":"687cebf73858638f66e59f56","name":"H-EmbodVis","fullname":"H-EmbodVis","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67467b5979406f42a14517e9/AnffLBETBMQyF-4ZK9o7B.png"}},"publishedAt":"2025-11-24T13:59:17.000Z","title":"Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution","summary":"Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19430.png","numComments":1,"submittedBy":{"_id":"67467b5979406f42a14517e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/67467b5979406f42a14517e9/wgnUxTd8vWOo0Cr2gaoyG.jpeg","fullname":"Dingkang Liang","name":"dkliang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"organization":{"_id":"687cebf73858638f66e59f56","name":"H-EmbodVis","fullname":"H-EmbodVis","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67467b5979406f42a14517e9/AnffLBETBMQyF-4ZK9o7B.png"},"isAuthorParticipating":false},{"paper":{"id":"2511.18394","authors":[{"_id":"69270769243b2216fb75cc4d","name":"Chinmay Karkar","hidden":false},{"_id":"69270769243b2216fb75cc4e","name":"Paras Chopra","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/69020c76ec2616129dea30b4/yPTYGUk_v9rwjLRKSz87V.jpeg"],"publishedAt":"2025-11-23T10:41:19.000Z","submittedOnDailyAt":"2025-11-26T11:29:19.123Z","title":"Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking","submittedOnDailyBy":{"_id":"69020c76ec2616129dea30b4","avatarUrl":"/avatars/44d59b5d951a4303def15ea1c6e3387e.svg","isPro":false,"fullname":"Paras Chopra","user":"paraslossfunk","type":"user"},"summary":"Large Language Models (LLMs) demonstrate partial forecasting competence across social, political, and economic events. Yet, their predictive ability varies sharply with domain structure and prompt framing. We investigate how forecasting performance varies with different model families on real-world questions about events that happened beyond the model cutoff date. We analyze how context, question type, and external knowledge affect accuracy and calibration, and how adding factual news context modifies belief formation and failure modes. Our results show that forecasting ability is highly variable as it depends on what, and how, we ask.","upvotes":0,"discussionId":"69270769243b2216fb75cc4f","ai_summary":"Forecasting performance of Large Language Models varies significantly across different domains and question types, influenced by context and external knowledge.","ai_keywords":["Large Language Models","forecasting competence","domain structure","prompt framing","model families","real-world questions","model cutoff date","context","question type","external knowledge","accuracy","calibration","belief formation","failure modes"],"organization":{"_id":"67a1298ea7d77f4454f936a2","name":"Lossfunk","fullname":"Lossfunk","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67a128ecc9bce54bbf006876/1lIRZ5gmsWovgosI_HIXy.jpeg"}},"publishedAt":"2025-11-23T05:41:19.000Z","title":"Future Is Unevenly Distributed: Forecasting Ability of LLMs Depends on What We're Asking","summary":"Large Language Models (LLMs) demonstrate partial forecasting competence across social, political, and economic events. Yet, their predictive ability varies sharply with domain structure and prompt framing. We investigate how forecasting performance varies with different model families on real-world questions about events that happened beyond the model cutoff date. We analyze how context, question type, and external knowledge affect accuracy and calibration, and how adding factual news context modifies belief formation and failure modes. Our results show that forecasting ability is highly variable as it depends on what, and how, we ask.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/69020c76ec2616129dea30b4/yPTYGUk_v9rwjLRKSz87V.jpeg"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18394.png","numComments":1,"submittedBy":{"_id":"69020c76ec2616129dea30b4","avatarUrl":"/avatars/44d59b5d951a4303def15ea1c6e3387e.svg","fullname":"Paras Chopra","name":"paraslossfunk","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"organization":{"_id":"67a1298ea7d77f4454f936a2","name":"Lossfunk","fullname":"Lossfunk","avatar":"https://cdn-uploads.huggingface.co/production/uploads/67a128ecc9bce54bbf006876/1lIRZ5gmsWovgosI_HIXy.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2511.16660","authors":[{"_id":"6925dec84b7b0d870b18278f","name":"Priyanka Kargupta","hidden":false},{"_id":"6925dec84b7b0d870b182790","name":"Shuyue Stella Li","hidden":false},{"_id":"6925dec84b7b0d870b182791","name":"Haocheng Wang","hidden":false},{"_id":"6925dec84b7b0d870b182792","name":"Jinu Lee","hidden":false},{"_id":"6925dec84b7b0d870b182793","name":"Shan Chen","hidden":false},{"_id":"6925dec84b7b0d870b182794","name":"Orevaoghene Ahia","hidden":false},{"_id":"6925dec84b7b0d870b182795","name":"Dean Light","hidden":false},{"_id":"6925dec84b7b0d870b182796","name":"Thomas L. Griffiths","hidden":false},{"_id":"6925dec84b7b0d870b182797","name":"Max Kleiman-Weiner","hidden":false},{"_id":"6925dec84b7b0d870b182798","name":"Jiawei Han","hidden":false},{"_id":"6925dec84b7b0d870b182799","name":"Asli Celikyilmaz","hidden":false},{"_id":"6925dec84b7b0d870b18279a","name":"Yulia Tsvetkov","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/axZxaLRAK4_474L0EJ2RM.png","https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/WcB-LRJRGhSqkkvZKs9Eg.png","https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/n2MDymrFJkRHzHKyDT3sW.png"],"publishedAt":"2025-11-20T18:59:00.000Z","submittedOnDailyAt":"2025-11-26T11:22:36.257Z","title":"Cognitive Foundations for Reasoning and Their Manifestation in LLMs","submittedOnDailyBy":{"_id":"6476ae4083d4fdaedddf405f","avatarUrl":"/avatars/08b23ccfa1f3bede6ade5a1aef06931d.svg","isPro":false,"fullname":"Priyanka Kargupta","user":"pkargupta","type":"user"},"summary":"Large language models (LLMs) solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. To understand this gap, we synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning reasoning invariants, meta-cognitive controls, representations for organizing reasoning & knowledge, and transformation operations. We introduce a fine-grained evaluation framework and conduct the first large-scale empirical analysis of 192K traces from 18 models across text, vision, and audio, complemented by 54 human think-aloud traces, which we make publicly available. We find that models under-utilize cognitive elements correlated with success, narrowing to rigid sequential processing on ill-structured problems where diverse representations and meta-cognitive monitoring are critical. Human traces show more abstraction and conceptual processing, while models default to surface-level enumeration. Meta-analysis of 1.6K LLM reasoning papers reveals the research community concentrates on easily quantifiable elements (sequential organization: 55%, decomposition: 60%) but neglecting meta-cognitive controls (self-awareness: 16%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 66.7% on complex problems. By establishing a shared vocabulary between cognitive science and LLM research, our framework enables systematic diagnosis of reasoning failures and principled development of models that reason through robust cognitive mechanisms rather than spurious shortcuts, while providing tools to test theories of human cognition at scale.","upvotes":1,"discussionId":"6925dec94b7b0d870b18279b","projectPage":"https://tinyurl.com/cognitive-foundations","githubRepo":"https://github.com/pkargupta/cognitive_foundations/","ai_summary":"LLMs exhibit reasoning gaps compared to humans, underutilizing cognitive elements and failing to deploy meta-cognitive controls, but test-time guidance can improve their performance on complex problems.","ai_keywords":["cognitive elements","reasoning invariants","meta-cognitive controls","representations","transformation operations","fine-grained evaluation framework","think-aloud traces","sequential processing","abstraction","conceptual processing","surface-level enumeration","self-awareness","reasoning guidance","robust cognitive mechanisms","spurious shortcuts"],"githubStars":2},"publishedAt":"2025-11-20T13:59:00.000Z","title":"Cognitive Foundations for Reasoning and Their Manifestation in LLMs","summary":"Large language models (LLMs) solve complex problems yet fail on simpler variants, suggesting they achieve correct outputs through mechanisms fundamentally different from human reasoning. To understand this gap, we synthesize cognitive science research into a taxonomy of 28 cognitive elements spanning reasoning invariants, meta-cognitive controls, representations for organizing reasoning & knowledge, and transformation operations. We introduce a fine-grained evaluation framework and conduct the first large-scale empirical analysis of 192K traces from 18 models across text, vision, and audio, complemented by 54 human think-aloud traces, which we make publicly available. We find that models under-utilize cognitive elements correlated with success, narrowing to rigid sequential processing on ill-structured problems where diverse representations and meta-cognitive monitoring are critical. Human traces show more abstraction and conceptual processing, while models default to surface-level enumeration. Meta-analysis of 1.6K LLM reasoning papers reveals the research community concentrates on easily quantifiable elements (sequential organization: 55%, decomposition: 60%) but neglecting meta-cognitive controls (self-awareness: 16%) that correlate with success. Models possess behavioral repertoires associated with success but fail to deploy them spontaneously. Leveraging these patterns, we develop test-time reasoning guidance that automatically scaffold successful structures, improving performance by up to 66.7% on complex problems. By establishing a shared vocabulary between cognitive science and LLM research, our framework enables systematic diagnosis of reasoning failures and principled development of models that reason through robust cognitive mechanisms rather than spurious shortcuts, while providing tools to test theories of human cognition at scale.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/axZxaLRAK4_474L0EJ2RM.png","https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/WcB-LRJRGhSqkkvZKs9Eg.png","https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/n2MDymrFJkRHzHKyDT3sW.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16660.png","numComments":1,"submittedBy":{"_id":"6476ae4083d4fdaedddf405f","avatarUrl":"/avatars/08b23ccfa1f3bede6ade5a1aef06931d.svg","fullname":"Priyanka Kargupta","name":"pkargupta","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"isAuthorParticipating":false},{"paper":{"id":"2511.20102","authors":[{"_id":"69269351243b2216fb75cb22","user":{"_id":"641b31a4ec5b871c0bcd6932","avatarUrl":"/avatars/bad42279ad99918b0846053f2fa95ac8.svg","isPro":false,"fullname":"Zhenyi Shen","user":"zen-E","type":"user"},"name":"Zhenyi Shen","status":"claimed_verified","statusLastChangedAt":"2025-11-26T11:31:23.403Z","hidden":false},{"_id":"69269351243b2216fb75cb23","user":{"_id":"64ddbc3f1f2dad27e1a05ac1","avatarUrl":"/avatars/4fb2753b7998c8536bfd4780d3b10a6d.svg","isPro":false,"fullname":"Junrulu","user":"Junrulu","type":"user"},"name":"Junru Lu","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:16.085Z","hidden":false},{"_id":"69269351243b2216fb75cb24","name":"Lin Gui","hidden":false},{"_id":"69269351243b2216fb75cb25","user":{"_id":"64be88e9af05eb17c702787c","avatarUrl":"/avatars/8953032acd739a0780e33cc46b0f9b56.svg","isPro":false,"fullname":"J Li","user":"jiazhengli","type":"user"},"name":"Jiazheng Li","status":"claimed_verified","statusLastChangedAt":"2025-11-26T11:53:51.303Z","hidden":false},{"_id":"69269351243b2216fb75cb26","name":"Yulan He","hidden":false},{"_id":"69269351243b2216fb75cb27","name":"Di Yin","hidden":false},{"_id":"69269351243b2216fb75cb28","name":"Xing Sun","hidden":false}],"publishedAt":"2025-11-25T09:21:57.000Z","submittedOnDailyAt":"2025-11-26T08:37:31.990Z","title":"SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space","submittedOnDailyBy":{"_id":"641b31a4ec5b871c0bcd6932","avatarUrl":"/avatars/bad42279ad99918b0846053f2fa95ac8.svg","isPro":false,"fullname":"Zhenyi Shen","user":"zen-E","type":"user"},"summary":"The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention, which may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas, with SSA demonstrating the strongest extrapolation capability.","upvotes":19,"discussionId":"69269351243b2216fb75cb29","ai_summary":"SSA, a unified training framework for sparse attention in LLMs, achieves state-of-the-art performance by aligning sparse attention with full attention, improving long-context processing and extrapolation.","ai_keywords":["full attention","sparse attention","native sparse-attention methods","NSA","MoBA","gradient update deficiency","SSA","Sparse Sparse Attention","bidirectional alignment","long-context extrapolation","sink areas"]},"publishedAt":"2025-11-25T04:21:57.000Z","title":"SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space","summary":"The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention, which may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas, with SSA demonstrating the strongest extrapolation capability.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20102.png","numComments":2,"submittedBy":{"_id":"641b31a4ec5b871c0bcd6932","avatarUrl":"/avatars/bad42279ad99918b0846053f2fa95ac8.svg","fullname":"Zhenyi Shen","name":"zen-E","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":5},"isAuthorParticipating":true},{"paper":{"id":"2511.20211","authors":[{"_id":"6926ae80243b2216fb75cb3b","user":{"_id":"669205f1ccca14aa8f13f770","avatarUrl":"/avatars/11ce274e93345fe3790ac9fa687e2bcb.svg","isPro":false,"fullname":"Hao Yu","user":"Longin-Yu","type":"user"},"name":"Hao Yu","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:10.152Z","hidden":false},{"_id":"6926ae80243b2216fb75cb3c","name":"Jiabo Zhan","hidden":false},{"_id":"6926ae80243b2216fb75cb3d","name":"Zile Wang","hidden":false},{"_id":"6926ae80243b2216fb75cb3e","name":"Jinglin Wang","hidden":false},{"_id":"6926ae80243b2216fb75cb3f","name":"Huaisong Zhang","hidden":false},{"_id":"6926ae80243b2216fb75cb40","name":"Hongyu Li","hidden":false},{"_id":"6926ae80243b2216fb75cb41","name":"Xinrui Chen","hidden":false},{"_id":"6926ae80243b2216fb75cb42","name":"Yongxian Wei","hidden":false},{"_id":"6926ae80243b2216fb75cb43","name":"Chun Yuan","hidden":false}],"publishedAt":"2025-11-25T11:34:51.000Z","submittedOnDailyAt":"2025-11-26T07:06:21.590Z","title":"OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation","submittedOnDailyBy":{"_id":"669205f1ccca14aa8f13f770","avatarUrl":"/avatars/11ce274e93345fe3790ac9fa687e2bcb.svg","isPro":false,"fullname":"Hao Yu","user":"Longin-Yu","type":"user"},"summary":"Generative models have excelled in RGB synthesis, but real-world applications require RGBA manipulation. This has led to a fragmented landscape: specialized, single-task models handle alpha but lack versatility, while unified multi-task frameworks are confined to the RGB domain. To bridge this critical gap, we propose OmniAlpha, the first unified, multi-task generative framework for sequence-to-sequence RGBA image generation and editing. Its architecture features MSRoPE-BiL, a novel RoPE method with a bi-directionally extendable layer axis for its Diffusion Transformer (DiT) backbone, enabling the concurrent processing of multiple input and target RGBA layers. To power this framework, we introduce AlphaLayers, a new dataset of 1,000 high-quality, multi-layer triplets, built via a novel automated synthesis and filter pipeline. Jointly training OmniAlpha on this dataset across a comprehensive suite of 21 diverse tasks, extensive experiments demonstrate that our unified approach consistently outperforms strong, specialized baselines. Most notably, OmniAlpha achieves a dramatic 84.8% relative reduction in SAD for mask-free matting on AIM-500 and wins over 90% of human preferences in layer-conditioned completion. Our work proves that a unified, multi-task model can learn a superior shared representation for RGBA, paving the way for more powerful, layer-aware generative systems.","upvotes":12,"discussionId":"6926ae80243b2216fb75cb44","githubRepo":"https://github.com/Longin-Yu/OmniAlpha","ai_summary":"OmniAlpha, a unified multi-task generative framework, excels in RGBA image generation and editing using a Diffusion Transformer with a novel MSRoPE-BiL method, outperforming specialized models across various tasks.","ai_keywords":["generative models","RGBA manipulation","OmniAlpha","MSRoPE-BiL","Diffusion Transformer","AlphaLayers","mask-free matting","layer-conditioned completion"],"githubStars":6,"organization":{"_id":"64cc8e9b214a472dd85e7e1d","name":"THU1911","fullname":"Tsinghua University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61f8e5934a8e5a275b2b3e5a/oKO6FK_rTzzPHXihicZou.jpeg"}},"publishedAt":"2025-11-25T06:34:51.000Z","title":"OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation","summary":"Generative models have excelled in RGB synthesis, but real-world applications require RGBA manipulation. This has led to a fragmented landscape: specialized, single-task models handle alpha but lack versatility, while unified multi-task frameworks are confined to the RGB domain. To bridge this critical gap, we propose OmniAlpha, the first unified, multi-task generative framework for sequence-to-sequence RGBA image generation and editing. Its architecture features MSRoPE-BiL, a novel RoPE method with a bi-directionally extendable layer axis for its Diffusion Transformer (DiT) backbone, enabling the concurrent processing of multiple input and target RGBA layers. To power this framework, we introduce AlphaLayers, a new dataset of 1,000 high-quality, multi-layer triplets, built via a novel automated synthesis and filter pipeline. Jointly training OmniAlpha on this dataset across a comprehensive suite of 21 diverse tasks, extensive experiments demonstrate that our unified approach consistently outperforms strong, specialized baselines. Most notably, OmniAlpha achieves a dramatic 84.8% relative reduction in SAD for mask-free matting on AIM-500 and wins over 90% of human preferences in layer-conditioned completion. Our work proves that a unified, multi-task model can learn a superior shared representation for RGBA, paving the way for more powerful, layer-aware generative systems.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20211.png","numComments":1,"submittedBy":{"_id":"669205f1ccca14aa8f13f770","avatarUrl":"/avatars/11ce274e93345fe3790ac9fa687e2bcb.svg","fullname":"Hao Yu","name":"Longin-Yu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1},"organization":{"_id":"64cc8e9b214a472dd85e7e1d","name":"THU1911","fullname":"Tsinghua University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61f8e5934a8e5a275b2b3e5a/oKO6FK_rTzzPHXihicZou.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2511.18734","authors":[{"_id":"6926bf18243b2216fb75cba1","name":"Keyang Lu","hidden":false},{"_id":"6926bf18243b2216fb75cba2","name":"Sifan Zhou","hidden":false},{"_id":"6926bf18243b2216fb75cba3","name":"Hongbin Xu","hidden":false},{"_id":"6926bf18243b2216fb75cba4","name":"Gang Xu","hidden":false},{"_id":"6926bf18243b2216fb75cba5","name":"Zhifei Yang","hidden":false},{"_id":"6926bf18243b2216fb75cba6","name":"Yikai Wang","hidden":false},{"_id":"6926bf18243b2216fb75cba7","name":"Zhen Xiao","hidden":false},{"_id":"6926bf18243b2216fb75cba8","name":"Jieyi Long","hidden":false},{"_id":"6926bf18243b2216fb75cba9","name":"Ming Li","hidden":false}],"publishedAt":"2025-11-24T04:02:48.000Z","submittedOnDailyAt":"2025-11-26T07:01:13.501Z","title":"Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion","submittedOnDailyBy":{"_id":"67f12ff4b3a4cfc21293847a","avatarUrl":"/avatars/d86f1bcfdad2717f526a90819837d883.svg","isPro":false,"fullname":"Lucas","user":"KeyangLu","type":"user"},"summary":"Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo'City first conceptualize the city through a top-down planning strategy that defines a hierarchical \"City-District-Grid\" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a \"produce-refine-evaluate\" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.","upvotes":2,"discussionId":"6926bf19243b2216fb75cbaa","ai_summary":"Yo'City is an agentic framework that uses off-the-shelf large models to generate user-customized, infinitely expandable 3D city scenes with spatial coherence and high quality across multiple evaluation metrics.","ai_keywords":["diffusion model","top-down planning","City-District-Grid","Global Planner","Local Designer","produce-refine-evaluate","isometric image synthesis","image-to-3D generation","scene graph","distance-aware","semantics-aware","layout optimization"]},"publishedAt":"2025-11-23T23:02:48.000Z","title":"Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion","summary":"Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo'City first conceptualize the city through a top-down planning strategy that defines a hierarchical \"City-District-Grid\" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a \"produce-refine-evaluate\" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18734.png","numComments":1,"submittedBy":{"_id":"67f12ff4b3a4cfc21293847a","avatarUrl":"/avatars/d86f1bcfdad2717f526a90819837d883.svg","fullname":"Lucas","name":"KeyangLu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"isAuthorParticipating":false},{"paper":{"id":"2511.17943","authors":[{"_id":"6926c0ef243b2216fb75cbac","user":{"_id":"68f84ad4ba36e04db877ac28","avatarUrl":"/avatars/8ac2d4b6c549082ccdfbfc7b02bcde8c.svg","isPro":false,"fullname":"Dr-Loser","user":"Dr-Loser","type":"user"},"name":"Zhiyu Xu","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:26:49.611Z","hidden":false},{"_id":"6926c0ef243b2216fb75cbad","name":"Weilong Yan","hidden":false},{"_id":"6926c0ef243b2216fb75cbae","name":"Yufei Shi","hidden":false},{"_id":"6926c0ef243b2216fb75cbaf","name":"Xin Meng","hidden":false},{"_id":"6926c0ef243b2216fb75cbb0","name":"Tao He","hidden":false},{"_id":"6926c0ef243b2216fb75cbb1","name":"Huiping Zhuang","hidden":false},{"_id":"6926c0ef243b2216fb75cbb2","name":"Ming Li","hidden":false},{"_id":"6926c0ef243b2216fb75cbb3","name":"Hehe Fan","hidden":false}],"publishedAt":"2025-11-22T06:54:16.000Z","submittedOnDailyAt":"2025-11-26T06:59:01.497Z","title":"SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System","submittedOnDailyBy":{"_id":"68f84ad4ba36e04db877ac28","avatarUrl":"/avatars/8ac2d4b6c549082ccdfbfc7b02bcde8c.svg","isPro":false,"fullname":"Dr-Loser","user":"Dr-Loser","type":"user"},"summary":"Recent advancements in multimodal large language models (MLLMs) and video agent systems have significantly improved general video understanding. However, when applied to scientific video understanding and educating, a domain that demands external professional knowledge integration and rigorous step-wise reasoning, existing approaches often struggle. To bridge this gap, we propose SciEducator, the first iterative self-evolving multi-agent system for scientific video comprehension and education. Rooted in the classical Deming Cycle from management science, our design reformulates its Plan-Do-Study-Act philosophy into a self-evolving reasoning and feedback mechanism, which facilitates the interpretation of intricate scientific activities in videos. Moreover, SciEducator can produce multimodal educational content tailored to specific scientific processes, including textual instructions, visual guides, audio narrations, and interactive references. To support evaluation, we construct SciVBench, a benchmark consisting of 500 expert-verified and literature-grounded science QA pairs across five categories, covering physical, chemical, and everyday phenomena. Extensive experiments demonstrate that SciEducator substantially outperforms leading closed-source MLLMs (e.g., Gemini, GPT-4o) and state-of-the-art video agents on the benchmark, establishing a new paradigm for the community.","upvotes":1,"discussionId":"6926c0ef243b2216fb75cbb4","ai_summary":"SciEducator, an iterative self-evolving multi-agent system, enhances scientific video understanding and education by integrating professional knowledge and step-wise reasoning, outperforming existing models on a new benchmark.","ai_keywords":["multimodal large language models","multi-agent system","Deming Cycle","Plan-Do-Study-Act","scientific video comprehension","multimodal educational content","textual instructions","visual guides","audio narrations","interactive references","SciVBench","science QA pairs","physical phenomena","chemical phenomena","everyday phenomena"],"organization":{"_id":"64e56b3bffba51d24f572668","name":"gml-cn","fullname":"Guang Ming Laboratory"}},"publishedAt":"2025-11-22T01:54:16.000Z","title":"SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System","summary":"Recent advancements in multimodal large language models (MLLMs) and video agent systems have significantly improved general video understanding. However, when applied to scientific video understanding and educating, a domain that demands external professional knowledge integration and rigorous step-wise reasoning, existing approaches often struggle. To bridge this gap, we propose SciEducator, the first iterative self-evolving multi-agent system for scientific video comprehension and education. Rooted in the classical Deming Cycle from management science, our design reformulates its Plan-Do-Study-Act philosophy into a self-evolving reasoning and feedback mechanism, which facilitates the interpretation of intricate scientific activities in videos. Moreover, SciEducator can produce multimodal educational content tailored to specific scientific processes, including textual instructions, visual guides, audio narrations, and interactive references. To support evaluation, we construct SciVBench, a benchmark consisting of 500 expert-verified and literature-grounded science QA pairs across five categories, covering physical, chemical, and everyday phenomena. Extensive experiments demonstrate that SciEducator substantially outperforms leading closed-source MLLMs (e.g., Gemini, GPT-4o) and state-of-the-art video agents on the benchmark, establishing a new paradigm for the community.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17943.png","numComments":1,"submittedBy":{"_id":"68f84ad4ba36e04db877ac28","avatarUrl":"/avatars/8ac2d4b6c549082ccdfbfc7b02bcde8c.svg","fullname":"Dr-Loser","name":"Dr-Loser","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"organization":{"_id":"64e56b3bffba51d24f572668","name":"gml-cn","fullname":"Guang Ming Laboratory"},"isAuthorParticipating":true},{"paper":{"id":"2511.20415","authors":[{"_id":"6926bc60243b2216fb75cb8c","user":{"_id":"6487e158f675b4a7867f45fa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6487e158f675b4a7867f45fa/J0sls6zZ682o-SH7iQs7B.jpeg","isPro":false,"fullname":"Zilong Huang","user":"SereinH","type":"user"},"name":"Zilong Huang","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:26:57.421Z","hidden":false},{"_id":"6926bc60243b2216fb75cb8d","name":"Jun He","hidden":false},{"_id":"6926bc60243b2216fb75cb8e","name":"Xiaobin Huang","hidden":false},{"_id":"6926bc60243b2216fb75cb8f","name":"Ziyi Xiong","hidden":false},{"_id":"6926bc60243b2216fb75cb90","name":"Yang Luo","hidden":false},{"_id":"6926bc60243b2216fb75cb91","name":"Junyan Ye","hidden":false},{"_id":"6926bc60243b2216fb75cb92","name":"Weijia Li","hidden":false},{"_id":"6926bc60243b2216fb75cb93","name":"Yiping Chen","hidden":false},{"_id":"6926bc60243b2216fb75cb94","name":"Ting Han","hidden":false}],"publishedAt":"2025-11-25T15:40:12.000Z","submittedOnDailyAt":"2025-11-26T06:12:00.248Z","title":"MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts","submittedOnDailyBy":{"_id":"6487e158f675b4a7867f45fa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6487e158f675b4a7867f45fa/J0sls6zZ682o-SH7iQs7B.jpeg","isPro":false,"fullname":"Zilong Huang","user":"SereinH","type":"user"},"summary":"Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents a city as a composition of controllable layouts, assets, and materials, and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent} that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop a practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our dataset and code will be released at https://github.com/LongHZ140516/MajutsuCity.","upvotes":6,"discussionId":"6926bc61243b2216fb75cb95","projectPage":"https://longhz140516.github.io/MajutsuCity/","githubRepo":"https://github.com/LongHZ140516/MajutsuCity","ai_summary":"MajutsuCity is a natural language-driven framework that synthesizes 3D urban scenes with high structural consistency, stylistic diversity, and controllability through a four-stage pipeline and interactive editing agent.","ai_keywords":["natural language-driven","aesthetically adaptive","structurally consistent","stylistically diverse","3D urban scenes","controllable layouts","assets","materials","interactive language-grounded editing agent","semantic layouts","height maps","3D building assets","PBR materials","skyboxes","detailed annotations","evaluation metrics","structural consistency","scene complexity","material fidelity","lighting atmosphere","CityDreamer","CityCraft","AQS","RDR scores","geometric fidelity","stylistic adaptability","semantic controllability"],"githubStars":3,"organization":{"_id":"656b30b8edd446c42b243426","name":"SunYatsen","fullname":"Sun Yat-Sen University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/Mn9lkuoOwVkUVziPpg2XZ.png"}},"publishedAt":"2025-11-25T10:40:12.000Z","title":"MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts","summary":"Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents a city as a composition of controllable layouts, assets, and materials, and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent} that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop a practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our dataset and code will be released at https://github.com/LongHZ140516/MajutsuCity.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20415.png","numComments":1,"submittedBy":{"_id":"6487e158f675b4a7867f45fa","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6487e158f675b4a7867f45fa/J0sls6zZ682o-SH7iQs7B.jpeg","fullname":"Zilong Huang","name":"SereinH","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1},"organization":{"_id":"656b30b8edd446c42b243426","name":"SunYatsen","fullname":"Sun Yat-Sen University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/Mn9lkuoOwVkUVziPpg2XZ.png"},"isAuthorParticipating":true},{"paper":{"id":"2511.17592","authors":[{"_id":"6926bc70243b2216fb75cb97","name":"Valentin Khrulkov","hidden":false},{"_id":"6926bc70243b2216fb75cb98","user":{"_id":"661e44cf1d8ffc49b57ba07e","avatarUrl":"/avatars/3e937cc4f784b369b9f996ba82d1b81d.svg","isPro":false,"fullname":"Andrey Galichin","user":"andreuka18","type":"user"},"name":"Andrey Galichin","status":"admin_assigned","statusLastChangedAt":"2025-11-26T11:53:55.369Z","hidden":false},{"_id":"6926bc70243b2216fb75cb99","name":"Denis Bashkirov","hidden":false},{"_id":"6926bc70243b2216fb75cb9a","name":"Dmitry Vinichenko","hidden":false},{"_id":"6926bc70243b2216fb75cb9b","name":"Oleg Travkin","hidden":false},{"_id":"6926bc70243b2216fb75cb9c","name":"Roman Alferov","hidden":false},{"_id":"6926bc70243b2216fb75cb9d","name":"Andrey Kuznetsov","hidden":false},{"_id":"6926bc70243b2216fb75cb9e","name":"Ivan Oseledets","hidden":false}],"publishedAt":"2025-11-17T14:44:47.000Z","submittedOnDailyAt":"2025-11-26T06:09:36.043Z","title":"GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms","submittedOnDailyBy":{"_id":"643984dceb7c5616ef3f5d54","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg","isPro":false,"fullname":"Andrey Kuznetsov","user":"kuznetsoffandrey","type":"user"},"summary":"Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve (Novikov et al., 2025; Georgiev et al., 2025), have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. However, the high-level descriptions in published work leave many implementation details unspecified, hindering reproducibility and further research. In this report we present GigaEvo, an extensible open-source framework that enables researchers to study and experiment with hybrid LLM-evolution approaches inspired by AlphaEvolve. Our system provides modular implementations of key components: MAP-Elites quality-diversity algorithms, asynchronous DAG-based evaluation pipelines, LLM-driven mutation operators with insight generation and bidirectional lineage tracking, and flexible multi-island evolutionary strategies. In order to assess reproducibility and validate our implementation we evaluate GigaEvo on challenging problems from the AlphaEvolve paper: Heilbronn triangle placement, circle packing in squares, and high-dimensional kissing numbers. The framework emphasizes modularity, concurrency, and ease of experimentation, enabling rapid prototyping through declarative configuration. We provide detailed descriptions of system architecture, implementation decisions, and experimental methodology to support further research in LLM driven evolutionary methods. The GigaEvo framework and all experimental code are available at https://github.com/AIRI-Institute/gigaevo-core.","upvotes":73,"discussionId":"6926bc70243b2216fb75cb9f","projectPage":"https://airi-institute.github.io/gigaevo-cover/","githubRepo":"https://github.com/FusionBrainLab/gigaevo-core","ai_summary":"GigaEvo is an open-source framework for LLM-guided evolutionary computation, offering modular and concurrent tools for research and experimentation in solving complex optimization problems.","ai_keywords":["LLM-guided evolutionary computation","AlphaEvolve","MAP-Elites","quality-diversity algorithms","asynchronous DAG-based evaluation","LLM-driven mutation","bidirectional lineage tracking","multi-island evolutionary strategies","Heilbronn triangle placement","circle packing","high-dimensional kissing numbers"],"githubStars":23,"organization":{"_id":"62a1fefca12f9cb8a15a5219","name":"AIRI-Institute","fullname":" AIRI - Artificial Intelligence Research Institute","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1654783663739-62a1fdd62cfb273c7f41333e.png"}},"publishedAt":"2025-11-17T09:44:47.000Z","title":"GigaEvo: An Open Source Optimization Framework Powered By LLMs And Evolution Algorithms","summary":"Recent advances in LLM-guided evolutionary computation, particularly AlphaEvolve (Novikov et al., 2025; Georgiev et al., 2025), have demonstrated remarkable success in discovering novel mathematical constructions and solving challenging optimization problems. However, the high-level descriptions in published work leave many implementation details unspecified, hindering reproducibility and further research. In this report we present GigaEvo, an extensible open-source framework that enables researchers to study and experiment with hybrid LLM-evolution approaches inspired by AlphaEvolve. Our system provides modular implementations of key components: MAP-Elites quality-diversity algorithms, asynchronous DAG-based evaluation pipelines, LLM-driven mutation operators with insight generation and bidirectional lineage tracking, and flexible multi-island evolutionary strategies. In order to assess reproducibility and validate our implementation we evaluate GigaEvo on challenging problems from the AlphaEvolve paper: Heilbronn triangle placement, circle packing in squares, and high-dimensional kissing numbers. The framework emphasizes modularity, concurrency, and ease of experimentation, enabling rapid prototyping through declarative configuration. We provide detailed descriptions of system architecture, implementation decisions, and experimental methodology to support further research in LLM driven evolutionary methods. The GigaEvo framework and all experimental code are available at https://github.com/AIRI-Institute/gigaevo-core.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17592.png","numComments":1,"submittedBy":{"_id":"643984dceb7c5616ef3f5d54","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg","fullname":"Andrey Kuznetsov","name":"kuznetsoffandrey","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":24},"organization":{"_id":"62a1fefca12f9cb8a15a5219","name":"AIRI-Institute","fullname":" AIRI - Artificial Intelligence Research Institute","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1654783663739-62a1fdd62cfb273c7f41333e.png"},"isAuthorParticipating":false},{"paper":{"id":"2511.19111","authors":[{"_id":"6926b7e1243b2216fb75cb5d","name":"Hai Ci","hidden":false},{"_id":"6926b7e1243b2216fb75cb5e","name":"Ziheng Peng","hidden":false},{"_id":"6926b7e1243b2216fb75cb5f","name":"Pei Yang","hidden":false},{"_id":"6926b7e1243b2216fb75cb60","name":"Yingxin Xuan","hidden":false},{"_id":"6926b7e1243b2216fb75cb61","name":"Mike Zheng Shou","hidden":false}],"publishedAt":"2025-11-24T13:43:54.000Z","submittedOnDailyAt":"2025-11-26T06:06:51.371Z","title":"DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection","submittedOnDailyBy":{"_id":"647352516a972f252de1fd58","avatarUrl":"/avatars/8e93ca66e01f047c5fb28e1c4f737e8e.svg","isPro":false,"fullname":"Hai Ci","user":"HaiCi","type":"user"},"summary":"Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images--we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models--local edits using eight SOTA diffusion models; 3) Multi-turn editing--each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/Diffseg30k","upvotes":3,"discussionId":"6926b7e1243b2216fb75cb62","projectPage":"https://huggingface.co/datasets/Chaos2629/Diffseg30k","ai_summary":"DiffSeg30k, a dataset of 30k diffusion-edited images, supports fine-grained detection of AI-generated content through semantic segmentation.","ai_keywords":["diffusion-based editing","DiffSeg30k","pixel-level annotations","in-the-wild images","diffusion models","multi-turn editing","vision-language model","semantic segmentation","image distortions","forgery classifiers","cross-generator generalization"]},"publishedAt":"2025-11-24T08:43:54.000Z","title":"DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection","summary":"Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images--we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models--local edits using eight SOTA diffusion models; 3) Multi-turn editing--each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/Diffseg30k","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19111.png","numComments":1,"submittedBy":{"_id":"647352516a972f252de1fd58","avatarUrl":"/avatars/8e93ca66e01f047c5fb28e1c4f737e8e.svg","fullname":"Hai Ci","name":"HaiCi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"isAuthorParticipating":false},{"paper":{"id":"2511.20347","authors":[{"_id":"69268ffb243b2216fb75cb0b","name":"Chang Gao","hidden":false},{"_id":"69268ffb243b2216fb75cb0c","name":"Chujie Zheng","hidden":false},{"_id":"69268ffb243b2216fb75cb0d","name":"Xiong-Hui Chen","hidden":false},{"_id":"69268ffb243b2216fb75cb0e","name":"Kai Dang","hidden":false},{"_id":"69268ffb243b2216fb75cb0f","name":"Shixuan Liu","hidden":false},{"_id":"69268ffb243b2216fb75cb10","name":"Bowen Yu","hidden":false},{"_id":"69268ffb243b2216fb75cb11","user":{"_id":"62088594a5943c8a8fc94560","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1644733028938-62088594a5943c8a8fc94560.png","isPro":false,"fullname":"An Yang","user":"yangapku","type":"user"},"name":"An Yang","status":"claimed_verified","statusLastChangedAt":"2025-11-26T11:31:25.265Z","hidden":false},{"_id":"69268ffb243b2216fb75cb12","name":"Shuai Bai","hidden":false},{"_id":"69268ffb243b2216fb75cb13","name":"Jingren Zhou","hidden":false},{"_id":"69268ffb243b2216fb75cb14","name":"Junyang Lin","hidden":false}],"publishedAt":"2025-11-25T14:25:19.000Z","submittedOnDailyAt":"2025-11-26T04:31:56.255Z","title":"Soft Adaptive Policy Optimization","submittedOnDailyBy":{"_id":"63451cf0a05b51f7ded25505","avatarUrl":"/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg","isPro":false,"fullname":"shuai bai","user":"ShuaiBai623","type":"user"},"summary":"Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.","upvotes":18,"discussionId":"69268ffb243b2216fb75cb15","ai_summary":"Soft Adaptive Policy Optimization (SAPO) enhances the stability and performance of reinforcement learning in large language models by adaptively attenuating off-policy updates with a smooth, temperature-controlled gate, leading to improved training stability and performance.","ai_keywords":["reinforcement learning","large language models","policy optimization","token-level importance ratios","Mixture-of-Experts models","GSPO","GRPO","Soft Adaptive Policy Optimization","sequence-coherent","token-adaptive","sequence-level coherence","off-policy updates","sample efficiency","Pass@1 performance","Qwen3-VL model series"],"organization":{"_id":"64c8b5837fe12ecd0a7e92eb","name":"Qwen","fullname":"Qwen","avatar":"https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}},"publishedAt":"2025-11-25T09:25:19.000Z","title":"Soft Adaptive Policy Optimization","summary":"Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20347.png","numComments":2,"submittedBy":{"_id":"63451cf0a05b51f7ded25505","avatarUrl":"/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg","fullname":"shuai bai","name":"ShuaiBai623","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":38},"organization":{"_id":"64c8b5837fe12ecd0a7e92eb","name":"Qwen","fullname":"Qwen","avatar":"https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"},"isAuthorParticipating":false},{"paper":{"id":"2511.20573","authors":[{"_id":"69266e53243b2216fb75ca64","name":"Chenhui Gou","hidden":false},{"_id":"69266e53243b2216fb75ca65","name":"Zilong Chen","hidden":false},{"_id":"69266e53243b2216fb75ca66","name":"Zeyu Wang","hidden":false},{"_id":"69266e53243b2216fb75ca67","name":"Feng Li","hidden":false},{"_id":"69266e53243b2216fb75ca68","name":"Deyao Zhu","hidden":false},{"_id":"69266e53243b2216fb75ca69","user":{"_id":"64e4446756b920ef00f2031c","avatarUrl":"/avatars/86305d1eac3917bf9859ac0ddbb4e845.svg","isPro":false,"fullname":"Zicheng Duan","user":"ZichengD","type":"user"},"name":"Zicheng Duan","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:35.069Z","hidden":false},{"_id":"69266e53243b2216fb75ca6a","name":"Kunchang Li","hidden":false},{"_id":"69266e53243b2216fb75ca6b","name":"Chaorui Deng","hidden":false},{"_id":"69266e53243b2216fb75ca6c","name":"Hongyi Yuan","hidden":false},{"_id":"69266e53243b2216fb75ca6d","name":"Haoqi Fan","hidden":false},{"_id":"69266e53243b2216fb75ca6e","name":"Cihang Xie","hidden":false},{"_id":"69266e53243b2216fb75ca6f","name":"Jianfei Cai","hidden":false},{"_id":"69266e53243b2216fb75ca70","name":"Hamid Rezatofighi","hidden":false}],"publishedAt":"2025-11-25T18:06:22.000Z","submittedOnDailyAt":"2025-11-26T03:38:13.028Z","title":"VQ-VA World: Towards High-Quality Visual Question-Visual Answering","submittedOnDailyBy":{"_id":"652e9c5774d1b0d7ff73d091","avatarUrl":"/avatars/a6d2098b3dde4a8b7488a193f0ecb776.svg","isPro":true,"fullname":"Chenhui Gou","user":"gouc","type":"user"},"summary":"This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.","upvotes":7,"discussionId":"69266e53243b2216fb75ca71","ai_summary":"A data-centric framework and benchmark for Visual Question-Visual Answering (VQ-VA) improve open-source model performance, narrowing the gap with proprietary systems.","ai_keywords":["Visual Question-Visual Answering","VQ-VA","agentic pipeline","web-scale deployment","image-text samples","IntelligentBench","world knowledge","design knowledge","reasoning","LightFusion","UniWorld-V1","NanoBanana","GPT-Image"]},"publishedAt":"2025-11-25T13:06:22.000Z","title":"VQ-VA World: Towards High-Quality Visual Question-Visual Answering","summary":"This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20573.png","numComments":1,"submittedBy":{"_id":"652e9c5774d1b0d7ff73d091","avatarUrl":"/avatars/a6d2098b3dde4a8b7488a193f0ecb776.svg","fullname":"Chenhui Gou","name":"gouc","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3},"isAuthorParticipating":false},{"paper":{"id":"2511.20123","authors":[{"_id":"6926928b243b2216fb75cb17","name":"Min Zhao","hidden":false},{"_id":"6926928b243b2216fb75cb18","user":{"_id":"64c269a52d73768f07ac266c","avatarUrl":"/avatars/d497a960f8aef6a974907b68ed750c1c.svg","isPro":false,"fullname":"Zhu Hongzhou","user":"zhuhz22","type":"user"},"name":"Hongzhou Zhu","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:18.068Z","hidden":false},{"_id":"6926928b243b2216fb75cb19","name":"Yingze Wang","hidden":false},{"_id":"6926928b243b2216fb75cb1a","name":"Bokai Yan","hidden":false},{"_id":"6926928b243b2216fb75cb1b","name":"Jintao Zhang","hidden":false},{"_id":"6926928b243b2216fb75cb1c","name":"Guande He","hidden":false},{"_id":"6926928b243b2216fb75cb1d","name":"Ling Yang","hidden":false},{"_id":"6926928b243b2216fb75cb1e","name":"Chongxuan Li","hidden":false},{"_id":"6926928b243b2216fb75cb1f","name":"Jun Zhu","hidden":false}],"publishedAt":"2025-11-25T09:44:10.000Z","submittedOnDailyAt":"2025-11-26T03:16:49.152Z","title":"UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers","submittedOnDailyBy":{"_id":"64c269a52d73768f07ac266c","avatarUrl":"/avatars/d497a960f8aef6a974907b68ed750c1c.svg","isPro":false,"fullname":"Zhu Hongzhou","user":"zhuhz22","type":"user"},"summary":"Despite advances, video diffusion transformers still struggle to generalize beyond their training length, a challenge we term video length extrapolation. We identify two failure modes: model-specific periodic content repetition and a universal quality degradation. Prior works attempt to solve repetition via positional encodings, overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from a more fundamental view: attention maps, which directly govern how context influences outputs. We identify that both failure modes arise from a unified cause: attention dispersion, where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as a special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings. Building on this insight, we propose UltraViCo, a training-free, plug-and-play method that suppresses attention for tokens beyond the training window via a constant decay factor. By jointly addressing both failure modes, we outperform a broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2x to 4x. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4x extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing.","upvotes":13,"discussionId":"6926928b243b2216fb75cb20","projectPage":"https://thu-ml.github.io/UltraViCo.github.io/","githubRepo":"https://github.com/thu-ml/DiT-Extrapolation","ai_summary":"UltraViCo addresses video length extrapolation by suppressing attention dispersion, improving quality and reducing repetition beyond training length.","ai_keywords":["video diffusion transformers","video length extrapolation","positional encodings","attention maps","attention dispersion","UltraViCo","Dynamic Degree","Imaging Quality","controllable video synthesis","editing"],"githubStars":741,"organization":{"_id":"640d3084536d9fe0f005cac3","name":"thu-ml","fullname":"Tsinghua Machine Learning Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1678587085174-633131798ef21f47308ce49b.jpeg"}},"publishedAt":"2025-11-25T04:44:10.000Z","title":"UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers","summary":"Despite advances, video diffusion transformers still struggle to generalize beyond their training length, a challenge we term video length extrapolation. We identify two failure modes: model-specific periodic content repetition and a universal quality degradation. Prior works attempt to solve repetition via positional encodings, overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from a more fundamental view: attention maps, which directly govern how context influences outputs. We identify that both failure modes arise from a unified cause: attention dispersion, where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as a special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings. Building on this insight, we propose UltraViCo, a training-free, plug-and-play method that suppresses attention for tokens beyond the training window via a constant decay factor. By jointly addressing both failure modes, we outperform a broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2x to 4x. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4x extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20123.png","numComments":1,"submittedBy":{"_id":"64c269a52d73768f07ac266c","avatarUrl":"/avatars/d497a960f8aef6a974907b68ed750c1c.svg","fullname":"Zhu Hongzhou","name":"zhuhz22","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2},"organization":{"_id":"640d3084536d9fe0f005cac3","name":"thu-ml","fullname":"Tsinghua Machine Learning Group","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1678587085174-633131798ef21f47308ce49b.jpeg"},"isAuthorParticipating":true},{"paper":{"id":"2511.19827","authors":[{"_id":"69268cde243b2216fb75caf9","user":{"_id":"653929a66da48e0d21e65e17","avatarUrl":"/avatars/e34ae3411d689b4280ff34c1b680f283.svg","isPro":false,"fullname":"Byeongjun Park","user":"byeongjun-park","type":"user"},"name":"Byeongjun Park","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:20.640Z","hidden":false},{"_id":"69268cde243b2216fb75cafa","name":"Byung-Hoon Kim","hidden":false},{"_id":"69268cde243b2216fb75cafb","name":"Hyungjin Chung","hidden":false},{"_id":"69268cde243b2216fb75cafc","name":"Jong Chul Ye","hidden":false}],"publishedAt":"2025-11-25T01:38:56.000Z","submittedOnDailyAt":"2025-11-26T02:48:52.934Z","title":"ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding","submittedOnDailyBy":{"_id":"653929a66da48e0d21e65e17","avatarUrl":"/avatars/e34ae3411d689b4280ff34c1b680f283.svg","isPro":false,"fullname":"Byeongjun Park","user":"byeongjun-park","type":"user"},"summary":"We present ReDirector, a novel camera-controlled video retake generation method for dynamically captured variable-length videos. In particular, we rectify a common misuse of RoPE in previous works by aligning the spatiotemporal positions of the input video and the target retake. Moreover, we introduce Rotary Camera Encoding (RoCE), a camera-conditioned RoPE phase shift that captures and integrates multi-view relationships within and across the input and target videos. By integrating camera conditions into RoPE, our method generalizes to out-of-distribution camera trajectories and video lengths, yielding improved dynamic object localization and static background preservation. Extensive experiments further demonstrate significant improvements in camera controllability, geometric consistency, and video quality across various trajectories and lengths.","upvotes":9,"discussionId":"69268cdf243b2216fb75cafd","projectPage":"https://byeongjun-park.github.io/ReDirector/","githubRepo":"https://github.com/byeongjun-park/ReDirector","ai_summary":"ReDirector uses a novel camera-controlled video retake method with Rotary Camera Encoding (RoCE) to improve dynamic object localization and static background preservation in variable-length videos.","ai_keywords":["RoPE","Rotary Camera Encoding","RoCE","camera-controlled video retake","spatiotemporal positions","multi-view relationships","dynamic object localization","static background preservation","camera controllability","geometric consistency","video quality"],"githubStars":7,"organization":{"_id":"64ab689073790912c7a8717a","name":"everex","fullname":"EverEx","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64ab6841723beceb2f45c9da/cdIuwsqJw-2tlEhnco0kI.png"}},"publishedAt":"2025-11-24T20:38:56.000Z","title":"ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding","summary":"We present ReDirector, a novel camera-controlled video retake generation method for dynamically captured variable-length videos. In particular, we rectify a common misuse of RoPE in previous works by aligning the spatiotemporal positions of the input video and the target retake. Moreover, we introduce Rotary Camera Encoding (RoCE), a camera-conditioned RoPE phase shift that captures and integrates multi-view relationships within and across the input and target videos. By integrating camera conditions into RoPE, our method generalizes to out-of-distribution camera trajectories and video lengths, yielding improved dynamic object localization and static background preservation. Extensive experiments further demonstrate significant improvements in camera controllability, geometric consistency, and video quality across various trajectories and lengths.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19827.png","numComments":1,"submittedBy":{"_id":"653929a66da48e0d21e65e17","avatarUrl":"/avatars/e34ae3411d689b4280ff34c1b680f283.svg","fullname":"Byeongjun Park","name":"byeongjun-park","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1},"organization":{"_id":"64ab689073790912c7a8717a","name":"everex","fullname":"EverEx","avatar":"https://cdn-uploads.huggingface.co/production/uploads/64ab6841723beceb2f45c9da/cdIuwsqJw-2tlEhnco0kI.png"},"isAuthorParticipating":true},{"paper":{"id":"2511.19900","authors":[{"_id":"6926877a243b2216fb75caca","user":{"_id":"684ff37fa383bc5d6b0ff77f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0JPr-cd_rxQz3k6rmzBOF.png","isPro":false,"fullname":"JiaqiLiu","user":"JiaaqiLiu","type":"user"},"name":"Jiaqi Liu","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:22.964Z","hidden":false},{"_id":"6926877a243b2216fb75cacb","name":"Kaiwen Xiong","hidden":false},{"_id":"6926877a243b2216fb75cacc","name":"Peng Xia","hidden":false},{"_id":"6926877a243b2216fb75cacd","name":"Yiyang Zhou","hidden":false},{"_id":"6926877a243b2216fb75cace","name":"Haonian Ji","hidden":false},{"_id":"6926877a243b2216fb75cacf","name":"Lu Feng","hidden":false},{"_id":"6926877a243b2216fb75cad0","name":"Siwei Han","hidden":false},{"_id":"6926877a243b2216fb75cad1","name":"Mingyu Ding","hidden":false},{"_id":"6926877a243b2216fb75cad2","name":"Huaxiu Yao","hidden":false}],"publishedAt":"2025-11-25T04:15:14.000Z","submittedOnDailyAt":"2025-11-26T02:25:56.245Z","title":"Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning","submittedOnDailyBy":{"_id":"684ff37fa383bc5d6b0ff77f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0JPr-cd_rxQz3k6rmzBOF.png","isPro":false,"fullname":"JiaqiLiu","user":"JiaaqiLiu","type":"user"},"summary":"Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at https://github.com/aiming-lab/Agent0/Agent0-VL{this https URL}.","upvotes":33,"discussionId":"6926877a243b2216fb75cad3","projectPage":"https://github.com/aiming-lab/Agent0/tree/main/Agent0-VL","githubRepo":"https://github.com/aiming-lab/Agent0/tree/main/Agent0-VL","ai_summary":"Agent0-VL, a self-evolving vision-language agent, incorporates tool usage into both reasoning and self-evaluation, enabling continual improvement through evidence-grounded analysis and reinforcement learning.","ai_keywords":["self-rewarding approaches","tool-integrated reasoning","self-evaluation","self-repair","introspect","evidence-grounded analysis","Solver","Verifier","Self-Evolving Reasoning Cycle","reinforcement learning","geometric problem solving","visual scientific analysis"],"githubStars":318,"organization":{"_id":"669f9d1fec8789263c0e355a","name":"UNC-ChapelHill","fullname":"University of North Carolina at Chapel Hill","avatar":"https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"}},"publishedAt":"2025-11-24T23:15:14.000Z","title":"Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning","summary":"Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at https://github.com/aiming-lab/Agent0/Agent0-VL{this https URL}.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19900.png","numComments":1,"submittedBy":{"_id":"684ff37fa383bc5d6b0ff77f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0JPr-cd_rxQz3k6rmzBOF.png","fullname":"JiaqiLiu","name":"JiaaqiLiu","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2},"organization":{"_id":"669f9d1fec8789263c0e355a","name":"UNC-ChapelHill","fullname":"University of North Carolina at Chapel Hill","avatar":"https://cdn-uploads.huggingface.co/production/uploads/669f9c85bd649dba3b88e581/H5uB8_MCewnMtxEUnAvTL.png"},"isAuthorParticipating":true},{"paper":{"id":"2511.19773","authors":[{"_id":"69268437243b2216fb75cab7","name":"Meng Lu","hidden":false},{"_id":"69268437243b2216fb75cab8","name":"Ran Xu","hidden":false},{"_id":"69268437243b2216fb75cab9","name":"Yi Fang","hidden":false},{"_id":"69268437243b2216fb75caba","name":"Wenxuan Zhang","hidden":false},{"_id":"69268437243b2216fb75cabb","name":"Yue Yu","hidden":false},{"_id":"69268437243b2216fb75cabc","name":"Gaurav Srivastava","hidden":false},{"_id":"69268437243b2216fb75cabd","name":"Yuchen Zhuang","hidden":false},{"_id":"69268437243b2216fb75cabe","name":"Mohamed Elhoseiny","hidden":false},{"_id":"69268437243b2216fb75cabf","name":"Charles Fleming","hidden":false},{"_id":"69268437243b2216fb75cac0","name":"Carl Yang","hidden":false},{"_id":"69268437243b2216fb75cac1","name":"Zhengzhong Tu","hidden":false},{"_id":"69268437243b2216fb75cac2","name":"Yang Xie","hidden":false},{"_id":"69268437243b2216fb75cac3","name":"Guanghua Xiao","hidden":false},{"_id":"69268437243b2216fb75cac4","name":"Hanrui Wang","hidden":false},{"_id":"69268437243b2216fb75cac5","name":"Di Jin","hidden":false},{"_id":"69268437243b2216fb75cac6","user":{"_id":"65cae89119683f9817c049ea","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65cae89119683f9817c049ea/A0XxjmaJldu28JhFvWmpP.jpeg","isPro":false,"fullname":"Wenqi Shi","user":"wshi83","type":"user"},"name":"Wenqi Shi","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:25.162Z","hidden":false},{"_id":"69268437243b2216fb75cac7","name":"Xuan Wang","hidden":false}],"publishedAt":"2025-11-24T22:58:26.000Z","submittedOnDailyAt":"2025-11-26T02:09:15.051Z","title":"Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs","submittedOnDailyBy":{"_id":"65cae89119683f9817c049ea","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65cae89119683f9817c049ea/A0XxjmaJldu28JhFvWmpP.jpeg","isPro":false,"fullname":"Wenqi Shi","user":"wshi83","type":"user"},"summary":"While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to \"think with images\", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.","upvotes":5,"discussionId":"69268438243b2216fb75cac8","ai_summary":"VISTA-Gym enhances vision-language models' tool-integrated visual reasoning through a scalable training environment with diverse multimodal tasks and reinforcement learning.","ai_keywords":["vision-language models","VLMs","VISTA-Gym","multimodal reasoning","visual tools","executable interaction loops","verifiable feedback signals","trajectory logging","visual agentic reinforcement learning","tool selection","tool invocation","tool coordination","multi-turn trajectory sampling","end-to-end reinforcement learning","VQA benchmarks","VISTA-R1-8B"],"organization":{"_id":"688ec8e2ea39631b70dc34db","name":"eigen-ai-labs","fullname":"Eigen AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6514d59cf95f39fd02acd9dc/oELb680wyZPzGhhO-w1pG.png"}},"publishedAt":"2025-11-24T17:58:26.000Z","title":"Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs","summary":"While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to \"think with images\", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19773.png","numComments":1,"submittedBy":{"_id":"65cae89119683f9817c049ea","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65cae89119683f9817c049ea/A0XxjmaJldu28JhFvWmpP.jpeg","fullname":"Wenqi Shi","name":"wshi83","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"organization":{"_id":"688ec8e2ea39631b70dc34db","name":"eigen-ai-labs","fullname":"Eigen AI","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6514d59cf95f39fd02acd9dc/oELb680wyZPzGhhO-w1pG.png"},"isAuthorParticipating":true},{"paper":{"id":"2511.20561","authors":[{"_id":"692671a3243b2216fb75ca91","name":"Yuwei Niu","hidden":false},{"_id":"692671a3243b2216fb75ca92","name":"Weiyang Jin","hidden":false},{"_id":"692671a3243b2216fb75ca93","name":"Jiaqi Liao","hidden":false},{"_id":"692671a3243b2216fb75ca94","name":"Chaoran Feng","hidden":false},{"_id":"692671a3243b2216fb75ca95","name":"Peng Jin","hidden":false},{"_id":"692671a3243b2216fb75ca96","name":"Bin Lin","hidden":false},{"_id":"692671a3243b2216fb75ca97","name":"Zongjian Li","hidden":false},{"_id":"692671a3243b2216fb75ca98","name":"Bin Zhu","hidden":false},{"_id":"692671a3243b2216fb75ca99","name":"Weihao Yu","hidden":false},{"_id":"692671a3243b2216fb75ca9a","name":"Li Yuan","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/66608add236f958513d21d2e/LUJu9PnWzLTa49ONkbY5H.png"],"publishedAt":"2025-11-25T17:58:48.000Z","submittedOnDailyAt":"2025-11-26T01:19:47.715Z","title":"Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward","submittedOnDailyBy":{"_id":"66608add236f958513d21d2e","avatarUrl":"/avatars/53eca0891c98cbb93be899885160a983.svg","isPro":false,"fullname":"Weiyang Jin","user":"Wayne-King","type":"user"},"summary":"Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox","upvotes":24,"discussionId":"692671a3243b2216fb75ca9b","githubRepo":"https://github.com/PKU-YuanGroup/UniSandBox","ai_summary":"UniSandbox evaluates Unified Multimodal Models, revealing a gap between understanding and generation, and identifies Chain-of-Thought and self-training as means to bridge this gap.","ai_keywords":["Unified Multimodal Models","UniSandbox","decoupled evaluation framework","synthetic datasets","understanding-generation gap","reasoning generation","knowledge transfer","Chain-of-Thought","self-training","query-based architectures"],"githubStars":7,"organization":{"_id":"61dcd8e344f59573371b5cb6","name":"PekingUniversity","fullname":"Peking University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}},"publishedAt":"2025-11-25T12:58:48.000Z","title":"Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward","summary":"Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/66608add236f958513d21d2e/LUJu9PnWzLTa49ONkbY5H.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20561.png","numComments":1,"submittedBy":{"_id":"66608add236f958513d21d2e","avatarUrl":"/avatars/53eca0891c98cbb93be899885160a983.svg","fullname":"Weiyang Jin","name":"Wayne-King","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3},"organization":{"_id":"61dcd8e344f59573371b5cb6","name":"PekingUniversity","fullname":"Peking University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"},"isAuthorParticipating":false},{"paper":{"id":"2511.19046","authors":[{"_id":"69267465243b2216fb75ca9d","user":{"_id":"66214b651fc3a06144ef8f4b","avatarUrl":"/avatars/dab669ad56b67805d55fef8c4fcf1326.svg","isPro":false,"fullname":"Anglin Liu","user":"lal-Joey","type":"user"},"name":"Anglin Liu","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:33.112Z","hidden":false},{"_id":"69267465243b2216fb75ca9e","name":"Rundong Xue","hidden":false},{"_id":"69267465243b2216fb75ca9f","user":{"_id":"5fc9f05d52770aca770bd3d9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/5fc9f05d52770aca770bd3d9/axiKCXPYttXEWC-RqCSfh.jpeg","isPro":true,"fullname":"Xu Cao","user":"IrohXu","type":"user"},"name":"Xu R. Cao","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:30.698Z","hidden":false},{"_id":"69267465243b2216fb75caa0","name":"Yifan Shen","hidden":false},{"_id":"69267465243b2216fb75caa1","name":"Yi Lu","hidden":false},{"_id":"69267465243b2216fb75caa2","name":"Xiang Li","hidden":false},{"_id":"69267465243b2216fb75caa3","name":"Qianqian Chen","hidden":false},{"_id":"69267465243b2216fb75caa4","name":"Jintai Chen","hidden":false}],"publishedAt":"2025-11-24T12:34:38.000Z","submittedOnDailyAt":"2025-11-26T01:01:06.095Z","title":"MedSAM3: Delving into Segment Anything with Medical Concepts","submittedOnDailyBy":{"_id":"65e387095132c2edd193ae49","avatarUrl":"/avatars/39278e5b026bcbdde88c560fc54018c5.svg","isPro":false,"fullname":"Yifan Shen","user":"SivanSX","type":"user"},"summary":"Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.","upvotes":34,"discussionId":"69267466243b2216fb75caa5","githubRepo":"https://github.com/Joey-S-Liu/MedSAM3","ai_summary":"MedSAM-3, a text-promptable medical segmentation model fine-tuned on SAM 3 architecture, achieves superior performance across various medical imaging modalities using semantic conceptual labels and multimodal large language models.","ai_keywords":["Segment Anything Model (SAM)","text promptable","medical segmentation","Promptable Concept Segmentation (PCS)","Multimodal Large Language Models (MLLMs)","X-ray","MRI","Ultrasound","CT","video"],"githubStars":33},"publishedAt":"2025-11-24T07:34:38.000Z","title":"MedSAM3: Delving into Segment Anything with Medical Concepts","summary":"Medical image segmentation is fundamental for biomedical discovery. Existing methods lack generalizability and demand extensive, time-consuming manual annotation for new clinical application. Here, we propose MedSAM-3, a text promptable medical segmentation model for medical image and video segmentation. By fine-tuning the Segment Anything Model (SAM) 3 architecture on medical images paired with semantic conceptual labels, our MedSAM-3 enables medical Promptable Concept Segmentation (PCS), allowing precise targeting of anatomical structures via open-vocabulary text descriptions rather than solely geometric prompts. We further introduce the MedSAM-3 Agent, a framework that integrates Multimodal Large Language Models (MLLMs) to perform complex reasoning and iterative refinement in an agent-in-the-loop workflow. Comprehensive experiments across diverse medical imaging modalities, including X-ray, MRI, Ultrasound, CT, and video, demonstrate that our approach significantly outperforms existing specialist and foundation models. We will release our code and model at https://github.com/Joey-S-Liu/MedSAM3.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19046.png","numComments":1,"submittedBy":{"_id":"65e387095132c2edd193ae49","avatarUrl":"/avatars/39278e5b026bcbdde88c560fc54018c5.svg","fullname":"Yifan Shen","name":"SivanSX","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1},"isAuthorParticipating":false},{"paper":{"id":"2511.19663","authors":[{"_id":"69266fb9243b2216fb75ca7c","name":"Ahmed Awadallah","hidden":false},{"_id":"69266fb9243b2216fb75ca7d","name":"Yash Lara","hidden":false},{"_id":"69266fb9243b2216fb75ca7e","name":"Raghav Magazine","hidden":false},{"_id":"69266fb9243b2216fb75ca7f","name":"Hussein Mozannar","hidden":false},{"_id":"69266fb9243b2216fb75ca80","name":"Akshay Nambi","hidden":false},{"_id":"69266fb9243b2216fb75ca81","name":"Yash Pandya","hidden":false},{"_id":"69266fb9243b2216fb75ca82","name":"Aravind Rajeswaran","hidden":false},{"_id":"69266fb9243b2216fb75ca83","name":"Corby Rosset","hidden":false},{"_id":"69266fb9243b2216fb75ca84","name":"Alexey Taymanov","hidden":false},{"_id":"69266fb9243b2216fb75ca85","name":"Vibhav Vineet","hidden":false},{"_id":"69266fb9243b2216fb75ca86","name":"Spencer Whitehead","hidden":false},{"_id":"69266fb9243b2216fb75ca87","name":"Andrew Zhao","hidden":false}],"publishedAt":"2025-11-24T19:56:28.000Z","submittedOnDailyAt":"2025-11-26T00:41:03.809Z","title":"Fara-7B: An Efficient Agentic Model for Computer Use","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.","upvotes":5,"discussionId":"69266fb9243b2216fb75ca88","ai_summary":"FaraGen creates synthetic datasets for computer use agents, enabling the training of efficient and high-performing models like Fara-7B on diverse web tasks, outperforming larger models on benchmarks.","ai_keywords":["FaraGen","synthetic data generation","multi-step web tasks","CUA trajectories","verifiers","Fara-7B","on-device model","native CUA model","screenshots","predicted coordinates","WebVoyager","Online-Mind2Web","WebTailBench"],"organization":{"_id":"5e6485f787403103f9f1055e","name":"microsoft","fullname":"Microsoft","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"}},"publishedAt":"2025-11-24T14:56:28.000Z","title":"Fara-7B: An Efficient Agentic Model for Computer Use","summary":"Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19663.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":171},"organization":{"_id":"5e6485f787403103f9f1055e","name":"microsoft","fullname":"Microsoft","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"},"isAuthorParticipating":false},{"paper":{"id":"2511.20562","authors":[{"_id":"69266e71243b2216fb75ca73","name":"Haoze Zhang","hidden":false},{"_id":"69266e71243b2216fb75ca74","name":"Tianyu Huang","hidden":false},{"_id":"69266e71243b2216fb75ca75","name":"Zichen Wan","hidden":false},{"_id":"69266e71243b2216fb75ca76","name":"Xiaowei Jin","hidden":false},{"_id":"69266e71243b2216fb75ca77","name":"Hongzhi Zhang","hidden":false},{"_id":"69266e71243b2216fb75ca78","name":"Hui Li","hidden":false},{"_id":"69266e71243b2216fb75ca79","name":"Wangmeng Zuo","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/muW87bIo2158vk7hvH4P7.jpeg","https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/15nSdHYXMyYAhBaFAuxFt.qt"],"publishedAt":"2025-11-25T17:59:04.000Z","submittedOnDailyAt":"2025-11-26T00:37:01.908Z","title":"PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"While recent video generation models have achieved significant visual fidelity, they often suffer from the lack of explicit physical controllability and plausibility. To address this, some recent studies attempted to guide the video generation with physics-based rendering. However, these methods face inherent challenges in accurately modeling complex physical properties and effectively control ling the resulting physical behavior over extended temporal sequences. In this work, we introduce PhysChoreo, a novel framework that can generate videos with diverse controllability and physical realism from a single image. Our method consists of two stages: first, it estimates the static initial physical properties of all objects in the image through part-aware physical property reconstruction. Then, through temporally instructed and physically editable simulation, it synthesizes high-quality videos with rich dynamic behaviors and physical realism. Experimental results show that PhysChoreo can generate videos with rich behaviors and physical realism, outperforming state-of-the-art methods on multiple evaluation metrics.","upvotes":2,"discussionId":"69266e71243b2216fb75ca7a","ai_summary":"PhysChoreo generates physically realistic and controllable videos from a single image using part-aware physical property reconstruction and temporally instructed simulation.","ai_keywords":["video generation","physical controllability","plausibility","physics-based rendering","physical property reconstruction","temporally instructed simulation","physical realism"]},"publishedAt":"2025-11-25T12:59:04.000Z","title":"PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding","summary":"While recent video generation models have achieved significant visual fidelity, they often suffer from the lack of explicit physical controllability and plausibility. To address this, some recent studies attempted to guide the video generation with physics-based rendering. However, these methods face inherent challenges in accurately modeling complex physical properties and effectively control ling the resulting physical behavior over extended temporal sequences. In this work, we introduce PhysChoreo, a novel framework that can generate videos with diverse controllability and physical realism from a single image. Our method consists of two stages: first, it estimates the static initial physical properties of all objects in the image through part-aware physical property reconstruction. Then, through temporally instructed and physically editable simulation, it synthesizes high-quality videos with rich dynamic behaviors and physical realism. Experimental results show that PhysChoreo can generate videos with rich behaviors and physical realism, outperforming state-of-the-art methods on multiple evaluation metrics.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/muW87bIo2158vk7hvH4P7.jpeg","https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/15nSdHYXMyYAhBaFAuxFt.qt"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20562.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":171},"isAuthorParticipating":false},{"paper":{"id":"2511.20635","authors":[{"_id":"69266dec243b2216fb75ca56","name":"Zhoujie Fu","hidden":false},{"_id":"69266dec243b2216fb75ca57","name":"Xianfang Zeng","hidden":false},{"_id":"69266dec243b2216fb75ca58","name":"Jinghong Lan","hidden":false},{"_id":"69266dec243b2216fb75ca59","name":"Xinyao Liao","hidden":false},{"_id":"69266dec243b2216fb75ca5a","name":"Cheng Chen","hidden":false},{"_id":"69266dec243b2216fb75ca5b","name":"Junyi Chen","hidden":false},{"_id":"69266dec243b2216fb75ca5c","name":"Jiacheng Wei","hidden":false},{"_id":"69266dec243b2216fb75ca5d","user":{"_id":"64b914c8ace99c0723ad83a9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg","isPro":false,"fullname":"Wei Cheng","user":"wchengad","type":"user"},"name":"Wei Cheng","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:36.745Z","hidden":false},{"_id":"69266dec243b2216fb75ca5e","name":"Shiyu Liu","hidden":false},{"_id":"69266dec243b2216fb75ca5f","name":"Yunuo Chen","hidden":false},{"_id":"69266dec243b2216fb75ca60","name":"Gang Yu","hidden":false},{"_id":"69266dec243b2216fb75ca61","name":"Guosheng Lin","hidden":false}],"publishedAt":"2025-11-25T18:54:16.000Z","submittedOnDailyAt":"2025-11-26T00:33:21.436Z","title":"iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/.","upvotes":27,"discussionId":"69266dec243b2216fb75ca62","projectPage":"https://kr1sjfu.github.io/iMontage-web/","githubRepo":"https://github.com/Kr1sJFU/iMontage","ai_summary":"iMontage repurposes pre-trained video models to generate high-quality, diverse image sets with natural transitions and enhanced dynamics through a unified framework and tailored adaptation strategy.","ai_keywords":["pre-trained video models","temporal coherence","continuous nature","image data","content diversity","iMontage","unified framework","image generator","variable-length image sets","image generation","image editing","adaptation strategy","data curation","training paradigm","image manipulation","motion priors","cross-image contextual consistency"],"githubStars":35,"organization":{"_id":"66e43eae9d477f566f937935","name":"stepfun-ai","fullname":"StepFun","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}},"publishedAt":"2025-11-25T13:54:16.000Z","title":"iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation","summary":"Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20635.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":171},"organization":{"_id":"66e43eae9d477f566f937935","name":"stepfun-ai","fullname":"StepFun","avatar":"https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"},"isAuthorParticipating":false},{"paper":{"id":"2511.19575","authors":[{"_id":"69266d8a243b2216fb75ca2f","name":"Hunyuan Vision Team","hidden":false},{"_id":"69266d8a243b2216fb75ca30","name":"Pengyuan Lyu","hidden":false},{"_id":"69266d8a243b2216fb75ca31","name":"Xingyu Wan","hidden":false},{"_id":"69266d8a243b2216fb75ca32","name":"Gengluo Li","hidden":false},{"_id":"69266d8a243b2216fb75ca33","name":"Shangpin Peng","hidden":false},{"_id":"69266d8a243b2216fb75ca34","name":"Weinong Wang","hidden":false},{"_id":"69266d8a243b2216fb75ca35","name":"Liang Wu","hidden":false},{"_id":"69266d8a243b2216fb75ca36","name":"Huawen Shen","hidden":false},{"_id":"69266d8a243b2216fb75ca37","name":"Yu Zhou","hidden":false},{"_id":"69266d8a243b2216fb75ca38","name":"Canhui Tang","hidden":false},{"_id":"69266d8a243b2216fb75ca39","name":"Qi Yang","hidden":false},{"_id":"69266d8a243b2216fb75ca3a","name":"Qiming Peng","hidden":false},{"_id":"69266d8a243b2216fb75ca3b","name":"Bin Luo","hidden":false},{"_id":"69266d8a243b2216fb75ca3c","name":"Hower Yang","hidden":false},{"_id":"69266d8a243b2216fb75ca3d","name":"Houwen Peng","hidden":false},{"_id":"69266d8a243b2216fb75ca3e","name":"Hongming Yang","hidden":false},{"_id":"69266d8a243b2216fb75ca3f","name":"Senhao Xie","hidden":false},{"_id":"69266d8a243b2216fb75ca40","name":"Binghong Wu","hidden":false},{"_id":"69266d8a243b2216fb75ca41","name":"Mana Yang","hidden":false},{"_id":"69266d8a243b2216fb75ca42","name":"Sergey Wang","hidden":false},{"_id":"69266d8a243b2216fb75ca43","name":"Raccoon Liu","hidden":false},{"_id":"69266d8a243b2216fb75ca44","name":"Dick Zhu","hidden":false},{"_id":"69266d8a243b2216fb75ca45","name":"Jie Jiang","hidden":false},{"_id":"69266d8a243b2216fb75ca46","name":"Linus","hidden":false},{"_id":"69266d8a243b2216fb75ca47","name":"Han Hu","hidden":false},{"_id":"69266d8a243b2216fb75ca48","name":"Chengquan Zhang","hidden":false}],"publishedAt":"2025-11-24T17:59:59.000Z","submittedOnDailyAt":"2025-11-26T00:31:43.086Z","title":"HunyuanOCR Technical Report","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter. HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks (Text Spotting, Parsing) and excels in semantic tasks (IE, Text Image Translation), securing first place in the ICDAR 2025 DIMT Challenge (Small Model Track). Furthermore, it achieves state-of-the-art (SOTA) results on OCRBench among VLMs with fewer than 3B parameters.\n  HunyuanOCR achieves breakthroughs in three key aspects: 1) Unifying Versatility and Efficiency: We implement comprehensive support for core capabilities including spotting, parsing, IE, VQA, and translation within a lightweight framework. This addresses the limitations of narrow \"OCR expert models\" and inefficient \"General VLMs\". 2) Streamlined End-to-End Architecture: Adopting a pure end-to-end paradigm eliminates dependencies on pre-processing modules (e.g., layout analysis). This fundamentally resolves error propagation common in traditional pipelines and simplifies system deployment. 3) Data-Driven and RL Strategies: We confirm the critical role of high-quality data and, for the first time in the industry, demonstrate that Reinforcement Learning (RL) strategies yield significant performance gains in OCR tasks.\n  HunyuanOCR is officially open-sourced on HuggingFace. We also provide a high-performance deployment solution based on vLLM, placing its production efficiency in the top tier. We hope this model will advance frontier research and provide a solid foundation for industrial applications.","upvotes":7,"discussionId":"69266d8a243b2216fb75ca49","githubRepo":"https://github.com/Tencent-Hunyuan/HunyuanOCR","ai_summary":"HunyuanOCR, a lightweight Vision-Language Model, achieves state-of-the-art performance in OCR tasks through a unified end-to-end architecture combining Vision Transformer and lightweight LLM, supported by data-driven and RL strategies.","ai_keywords":["Vision-Language Model","Vision Transformer","lightweight LLM","MLP adapter","Text Spotting","Parsing","Information Extraction","VQA","Text Image Translation","end-to-end architecture","Reinforcement Learning"],"githubStars":459,"organization":{"_id":"6645f953c39288df638dbdd5","name":"Tencent-Hunyuan","fullname":"Tencent Hunyuan","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}},"publishedAt":"2025-11-24T12:59:59.000Z","title":"HunyuanOCR Technical Report","summary":"This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter. HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks (Text Spotting, Parsing) and excels in semantic tasks (IE, Text Image Translation), securing first place in the ICDAR 2025 DIMT Challenge (Small Model Track). Furthermore, it achieves state-of-the-art (SOTA) results on OCRBench among VLMs with fewer than 3B parameters.\n  HunyuanOCR achieves breakthroughs in three key aspects: 1) Unifying Versatility and Efficiency: We implement comprehensive support for core capabilities including spotting, parsing, IE, VQA, and translation within a lightweight framework. This addresses the limitations of narrow \"OCR expert models\" and inefficient \"General VLMs\". 2) Streamlined End-to-End Architecture: Adopting a pure end-to-end paradigm eliminates dependencies on pre-processing modules (e.g., layout analysis). This fundamentally resolves error propagation common in traditional pipelines and simplifies system deployment. 3) Data-Driven and RL Strategies: We confirm the critical role of high-quality data and, for the first time in the industry, demonstrate that Reinforcement Learning (RL) strategies yield significant performance gains in OCR tasks.\n  HunyuanOCR is officially open-sourced on HuggingFace. We also provide a high-performance deployment solution based on vLLM, placing its production efficiency in the top tier. We hope this model will advance frontier research and provide a solid foundation for industrial applications.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19575.png","numComments":0,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":171},"organization":{"_id":"6645f953c39288df638dbdd5","name":"Tencent-Hunyuan","fullname":"Tencent Hunyuan","avatar":"https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"},"isAuthorParticipating":false},{"paper":{"id":"2511.19861","authors":[{"_id":"69266d2b243b2216fb75ca14","name":"GigaWorld Team","hidden":false},{"_id":"69266d2b243b2216fb75ca15","name":"Angen Ye","hidden":false},{"_id":"69266d2b243b2216fb75ca16","name":"Boyuan Wang","hidden":false},{"_id":"69266d2b243b2216fb75ca17","name":"Chaojun Ni","hidden":false},{"_id":"69266d2b243b2216fb75ca18","name":"Guan Huang","hidden":false},{"_id":"69266d2b243b2216fb75ca19","name":"Guosheng Zhao","hidden":false},{"_id":"69266d2b243b2216fb75ca1a","name":"Haoyun Li","hidden":false},{"_id":"69266d2b243b2216fb75ca1b","name":"Jiagang Zhu","hidden":false},{"_id":"69266d2b243b2216fb75ca1c","name":"Kerui Li","hidden":false},{"_id":"69266d2b243b2216fb75ca1d","name":"Mengyuan Xu","hidden":false},{"_id":"69266d2b243b2216fb75ca1e","name":"Qiuping Deng","hidden":false},{"_id":"69266d2b243b2216fb75ca1f","name":"Siting Wang","hidden":false},{"_id":"69266d2b243b2216fb75ca20","name":"Wenkang Qin","hidden":false},{"_id":"69266d2b243b2216fb75ca21","name":"Xinze Chen","hidden":false},{"_id":"69266d2b243b2216fb75ca22","user":{"_id":"6426616ea5ec4a5cbc535634","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6426616ea5ec4a5cbc535634/hH6JsxnXeakH3mBTeNNmO.jpeg","isPro":false,"fullname":"JeffWang","user":"Jeff-Wang","type":"user"},"name":"Xiaofeng Wang","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:27:39.041Z","hidden":false},{"_id":"69266d2b243b2216fb75ca23","name":"Yankai Wang","hidden":false},{"_id":"69266d2b243b2216fb75ca24","name":"Yu Cao","hidden":false},{"_id":"69266d2b243b2216fb75ca25","name":"Yifan Chang","hidden":false},{"_id":"69266d2b243b2216fb75ca26","name":"Yuan Xu","hidden":false},{"_id":"69266d2b243b2216fb75ca27","name":"Yun Ye","hidden":false},{"_id":"69266d2b243b2216fb75ca28","name":"Yang Wang","hidden":false},{"_id":"69266d2b243b2216fb75ca29","name":"Yukun Zhou","hidden":false},{"_id":"69266d2b243b2216fb75ca2a","name":"Zhengyuan Zhang","hidden":false},{"_id":"69266d2b243b2216fb75ca2b","name":"Zhehao Dong","hidden":false},{"_id":"69266d2b243b2216fb75ca2c","name":"Zheng Zhu","hidden":false}],"publishedAt":"2025-11-25T03:00:42.000Z","submittedOnDailyAt":"2025-11-26T00:30:02.812Z","title":"GigaWorld-0: World Models as Data Engine to Empower Embodied AI","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.","upvotes":20,"discussionId":"69266d2b243b2216fb75ca2d","projectPage":"https://gigaworld0.github.io/","githubRepo":"https://github.com/open-gigaai/giga-world-0","ai_summary":"GigaWorld-0 is a unified world model framework that integrates video generation and 3D modeling to produce high-quality, diverse, and physically plausible VLA data, enabling strong real-world performance in embodied AI without real-world training.","ai_keywords":["GigaWorld-0","GigaWorld-0-Video","GigaWorld-0-3D","3D generative modeling","3D Gaussian Splatting","physically differentiable system identification","executable motion planning","GigaTrain","FP8-precision","sparse attention","VLA model","GigaBrain-0","embodied interaction data","real-world performance","task success"],"githubStars":49},"publishedAt":"2025-11-24T22:00:42.000Z","title":"GigaWorld-0: World Models as Data Engine to Empower Embodied AI","summary":"World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19861.png","numComments":3,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":171},"isAuthorParticipating":true},{"paper":{"id":"2511.19320","authors":[{"_id":"692525d916eb3a9f13103974","user":{"_id":"65645169ec7e239899136895","avatarUrl":"/avatars/59f7f485643f71aff078347d99ed9765.svg","isPro":false,"fullname":"Jiaming Zhang","user":"jiamingZ","type":"user"},"name":"Jiaming Zhang","status":"claimed_verified","statusLastChangedAt":"2025-11-25T12:10:05.653Z","hidden":false},{"_id":"692525d916eb3a9f13103975","name":"Shengming Cao","hidden":false},{"_id":"692525d916eb3a9f13103976","name":"Rui Li","hidden":false},{"_id":"692525d916eb3a9f13103977","name":"Xiaotong Zhao","hidden":false},{"_id":"692525d916eb3a9f13103978","name":"Yutao Cui","hidden":false},{"_id":"692525d916eb3a9f13103979","name":"Xinglin Hou","hidden":false},{"_id":"692525d916eb3a9f1310397a","name":"Gangshan Wu","hidden":false},{"_id":"692525d916eb3a9f1310397b","name":"Haolan Chen","hidden":false},{"_id":"692525d916eb3a9f1310397c","name":"Yu Xu","hidden":false},{"_id":"692525d916eb3a9f1310397d","name":"Limin Wang","hidden":false},{"_id":"692525d916eb3a9f1310397e","name":"Kai Ma","hidden":false}],"publishedAt":"2025-11-24T17:15:55.000Z","submittedOnDailyAt":"2025-11-26T00:24:22.899Z","title":"SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation","submittedOnDailyBy":{"_id":"65645169ec7e239899136895","avatarUrl":"/avatars/59f7f485643f71aff078347d99ed9765.svg","isPro":false,"fullname":"Jiaming Zhang","user":"jiamingZ","type":"user"},"summary":"Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.","upvotes":37,"discussionId":"692525d916eb3a9f1310397f","projectPage":"https://mcg-nju.github.io/steadydancer-web","githubRepo":"https://github.com/MCG-NJU/SteadyDancer","ai_summary":"SteadyDancer, an Image-to-Video framework, ensures first-frame identity preservation and precise motion control through harmonized conditions, adaptive pose representation, and hierarchical training objectives.","ai_keywords":["Image-to-Motion Binding","Reference-to-Video","Image-to-Video","Condition-Reconciliation Mechanism","Synergistic Pose Modulation Modules","Staged Decoupled-Objective Training Pipeline","motion fidelity","visual quality","temporal coherence"],"githubStars":24,"organization":{"_id":"62c77fde1e080b83746468bd","name":"MCG-NJU","fullname":"Multimedia Computing Group-Nanjing University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/OIFtHl-mDBM_5SqyR9T8f.png"}},"publishedAt":"2025-11-24T12:15:55.000Z","title":"SteadyDancer: Harmonized and Coherent Human Image Animation with First-Frame Preservation","summary":"Preserving first-frame identity while ensuring precise motion control is a fundamental challenge in human image animation. The Image-to-Motion Binding process of the dominant Reference-to-Video (R2V) paradigm overlooks critical spatio-temporal misalignments common in real-world applications, leading to failures such as identity drift and visual artifacts. We introduce SteadyDancer, an Image-to-Video (I2V) paradigm-based framework that achieves harmonized and coherent animation and is the first to ensure first-frame preservation robustly. Firstly, we propose a Condition-Reconciliation Mechanism to harmonize the two conflicting conditions, enabling precise control without sacrificing fidelity. Secondly, we design Synergistic Pose Modulation Modules to generate an adaptive and coherent pose representation that is highly compatible with the reference image. Finally, we employ a Staged Decoupled-Objective Training Pipeline that hierarchically optimizes the model for motion fidelity, visual quality, and temporal coherence. Experiments demonstrate that SteadyDancer achieves state-of-the-art performance in both appearance fidelity and motion control, while requiring significantly fewer training resources than comparable methods.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19320.png","numComments":1,"submittedBy":{"_id":"65645169ec7e239899136895","avatarUrl":"/avatars/59f7f485643f71aff078347d99ed9765.svg","fullname":"Jiaming Zhang","name":"jiamingZ","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1},"organization":{"_id":"62c77fde1e080b83746468bd","name":"MCG-NJU","fullname":"Multimedia Computing Group-Nanjing University","avatar":"https://cdn-uploads.huggingface.co/production/uploads/noauth/OIFtHl-mDBM_5SqyR9T8f.png"},"isAuthorParticipating":true},{"paper":{"id":"2511.15906","authors":[{"_id":"692069e58c38b39d6a482eb4","user":{"_id":"69111dd0eed4fe1e4fcf63dd","avatarUrl":"/avatars/1be6490daa72ea9e2e366a572ab7e438.svg","isPro":false,"fullname":"Matthieu Kirchmeyer","user":"mkirchmeyer","type":"user"},"name":"Matthieu Kirchmeyer","status":"claimed_verified","statusLastChangedAt":"2025-11-25T15:53:22.361Z","hidden":false},{"_id":"692069e58c38b39d6a482eb5","name":"Pedro O. Pinheiro","hidden":false},{"_id":"692069e58c38b39d6a482eb6","name":"Emma Willett","hidden":false},{"_id":"692069e58c38b39d6a482eb7","name":"Karolis Martinkus","hidden":false},{"_id":"692069e58c38b39d6a482eb8","name":"Joseph Kleinhenz","hidden":false},{"_id":"692069e58c38b39d6a482eb9","name":"Emily K. Makowski","hidden":false},{"_id":"692069e58c38b39d6a482eba","name":"Andrew M. Watkins","hidden":false},{"_id":"692069e58c38b39d6a482ebb","name":"Vladimir Gligorijevic","hidden":false},{"_id":"692069e58c38b39d6a482ebc","name":"Richard Bonneau","hidden":false},{"_id":"692069e58c38b39d6a482ebd","name":"Saeed Saremi","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/69111dd0eed4fe1e4fcf63dd/zrf-c-Jf9abcl8jqv7ymr.gif","https://cdn-uploads.huggingface.co/production/uploads/69111dd0eed4fe1e4fcf63dd/0miig01yongdkfn3ByH8M.gif","https://cdn-uploads.huggingface.co/production/uploads/69111dd0eed4fe1e4fcf63dd/bM0VQA4QSXXWXz-FEWHGM.gif","https://cdn-uploads.huggingface.co/production/uploads/69111dd0eed4fe1e4fcf63dd/KEssstpCbDKJDpmwqDdrD.gif"],"publishedAt":"2025-11-19T22:18:13.000Z","submittedOnDailyAt":"2025-11-26T00:22:36.233Z","title":"Unified all-atom molecule generation with neural fields","submittedOnDailyBy":{"_id":"69111dd0eed4fe1e4fcf63dd","avatarUrl":"/avatars/1be6490daa72ea9e2e366a572ab7e438.svg","isPro":false,"fullname":"Matthieu Kirchmeyer","user":"mkirchmeyer","type":"user"},"summary":"Generative models for structure-based drug design are often limited to a specific modality, restricting their broader applicability. To address this challenge, we introduce FuncBind, a framework based on computer vision to generate target-conditioned, all-atom molecules across atomic systems. FuncBind uses neural fields to represent molecules as continuous atomic densities and employs score-based generative models with modern architectures adapted from the computer vision literature. This modality-agnostic representation allows a single unified model to be trained on diverse atomic systems, from small to large molecules, and handle variable atom/residue counts, including non-canonical amino acids. FuncBind achieves competitive in silico performance in generating small molecules, macrocyclic peptides, and antibody complementarity-determining region loops, conditioned on target structures. FuncBind also generated in vitro novel antibody binders via de novo redesign of the complementarity-determining region H3 loop of two chosen co-crystal structures. As a final contribution, we introduce a new dataset and benchmark for structure-conditioned macrocyclic peptide generation. The code is available at https://github.com/prescient-design/funcbind.","upvotes":2,"discussionId":"692069e58c38b39d6a482ebe","githubRepo":"https://github.com/prescient-design/funcbind/","ai_summary":"FuncBind, a framework using neural fields and score-based generative models from computer vision, generates diverse atomic structures across modalities, achieving competitive performance in structure-conditioned molecular design.","ai_keywords":["neural fields","score-based generative models","structure-conditioned","target-conditioned","all-atom molecules","atomic systems","macrocyclic peptides","antibody complementarity-determining region loops","de novo redesign"],"githubStars":6,"organization":{"_id":"63053d3c9d2531fabd152815","name":"Genentech","fullname":"Genentech","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1661287901949-63053cd653b3899eeb61d17d.png"}},"publishedAt":"2025-11-19T17:18:13.000Z","title":"Unified all-atom molecule generation with neural fields","summary":"Generative models for structure-based drug design are often limited to a specific modality, restricting their broader applicability. To address this challenge, we introduce FuncBind, a framework based on computer vision to generate target-conditioned, all-atom molecules across atomic systems. FuncBind uses neural fields to represent molecules as continuous atomic densities and employs score-based generative models with modern architectures adapted from the computer vision literature. This modality-agnostic representation allows a single unified model to be trained on diverse atomic systems, from small to large molecules, and handle variable atom/residue counts, including non-canonical amino acids. FuncBind achieves competitive in silico performance in generating small molecules, macrocyclic peptides, and antibody complementarity-determining region loops, conditioned on target structures. FuncBind also generated in vitro novel antibody binders via de novo redesign of the complementarity-determining region H3 loop of two chosen co-crystal structures. As a final contribution, we introduce a new dataset and benchmark for structure-conditioned macrocyclic peptide generation. The code is available at https://github.com/prescient-design/funcbind.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/69111dd0eed4fe1e4fcf63dd/zrf-c-Jf9abcl8jqv7ymr.gif","https://cdn-uploads.huggingface.co/production/uploads/69111dd0eed4fe1e4fcf63dd/0miig01yongdkfn3ByH8M.gif","https://cdn-uploads.huggingface.co/production/uploads/69111dd0eed4fe1e4fcf63dd/bM0VQA4QSXXWXz-FEWHGM.gif","https://cdn-uploads.huggingface.co/production/uploads/69111dd0eed4fe1e4fcf63dd/KEssstpCbDKJDpmwqDdrD.gif"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.15906.png","numComments":1,"submittedBy":{"_id":"69111dd0eed4fe1e4fcf63dd","avatarUrl":"/avatars/1be6490daa72ea9e2e366a572ab7e438.svg","fullname":"Matthieu Kirchmeyer","name":"mkirchmeyer","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"organization":{"_id":"63053d3c9d2531fabd152815","name":"Genentech","fullname":"Genentech","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1661287901949-63053cd653b3899eeb61d17d.png"},"isAuthorParticipating":true},{"paper":{"id":"2511.20256","authors":[{"_id":"6926617a243b2216fb75c9f6","name":"Weijia Mao","hidden":false},{"_id":"6926617a243b2216fb75c9f7","name":"Hao Chen","hidden":false},{"_id":"6926617a243b2216fb75c9f8","name":"Zhenheng Yang","hidden":false},{"_id":"6926617a243b2216fb75c9f9","name":"Mike Zheng Shou","hidden":false}],"publishedAt":"2025-11-25T12:35:57.000Z","submittedOnDailyAt":"2025-11-25T23:57:53.540Z","title":"The Image as Its Own Reward: Reinforcement Learning with Adversarial Reward for Image Generation","submittedOnDailyBy":{"_id":"63f320ee0be81bdc5d8ecb88","avatarUrl":"/avatars/9d08cff6ed23a51887c869947bc03228.svg","isPro":true,"fullname":"Mao Weijia","user":"benzweijia","type":"user"},"summary":"A reliable reward function is essential for reinforcement learning (RL) in image generation. Most current RL approaches depend on pre-trained preference models that output scalar rewards to approximate human preferences. However, these rewards often fail to capture human perception and are vulnerable to reward hacking, where higher scores do not correspond to better images. To address this, we introduce Adv-GRPO, an RL framework with an adversarial reward that iteratively updates both the reward model and the generator. The reward model is supervised using reference images as positive samples and can largely avoid being hacked. Unlike KL regularization that constrains parameter updates, our learned reward directly guides the generator through its visual outputs, leading to higher-quality images. Moreover, while optimizing existing reward functions can alleviate reward hacking, their inherent biases remain. For instance, PickScore may degrade image quality, whereas OCR-based rewards often reduce aesthetic fidelity. To address this, we take the image itself as a reward, using reference images and vision foundation models (e.g., DINO) to provide rich visual rewards. These dense visual signals, instead of a single scalar, lead to consistent gains across image quality, aesthetics, and task-specific metrics. Finally, we show that combining reference samples with foundation-model rewards enables distribution transfer and flexible style customization. In human evaluation, our method outperforms Flow-GRPO and SD3, achieving 70.0% and 72.4% win rates in image quality and aesthetics, respectively. Code and models have been released.","upvotes":24,"discussionId":"6926617a243b2216fb75c9fa","ai_summary":"An adversarial reward learning framework in reinforcement learning for image generation improves image quality and aesthetics by using dense visual signals from vision foundation models.","ai_keywords":["reinforcement learning","reward function","preference models","reward hacking","Adv-GRPO","adversarial reward","reward model","generator","KL regularization","PickScore","OCR-based rewards","image quality","aesthetics","task-specific metrics","distribution transfer","style customization","vision foundation models","DINO"]},"publishedAt":"2025-11-25T07:35:57.000Z","title":"The Image as Its Own Reward: Reinforcement Learning with Adversarial Reward for Image Generation","summary":"A reliable reward function is essential for reinforcement learning (RL) in image generation. Most current RL approaches depend on pre-trained preference models that output scalar rewards to approximate human preferences. However, these rewards often fail to capture human perception and are vulnerable to reward hacking, where higher scores do not correspond to better images. To address this, we introduce Adv-GRPO, an RL framework with an adversarial reward that iteratively updates both the reward model and the generator. The reward model is supervised using reference images as positive samples and can largely avoid being hacked. Unlike KL regularization that constrains parameter updates, our learned reward directly guides the generator through its visual outputs, leading to higher-quality images. Moreover, while optimizing existing reward functions can alleviate reward hacking, their inherent biases remain. For instance, PickScore may degrade image quality, whereas OCR-based rewards often reduce aesthetic fidelity. To address this, we take the image itself as a reward, using reference images and vision foundation models (e.g., DINO) to provide rich visual rewards. These dense visual signals, instead of a single scalar, lead to consistent gains across image quality, aesthetics, and task-specific metrics. Finally, we show that combining reference samples with foundation-model rewards enables distribution transfer and flexible style customization. In human evaluation, our method outperforms Flow-GRPO and SD3, achieving 70.0% and 72.4% win rates in image quality and aesthetics, respectively. Code and models have been released.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20256.png","numComments":2,"submittedBy":{"_id":"63f320ee0be81bdc5d8ecb88","avatarUrl":"/avatars/9d08cff6ed23a51887c869947bc03228.svg","fullname":"Mao Weijia","name":"benzweijia","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3},"isAuthorParticipating":false},{"paper":{"id":"2511.17405","authors":[{"_id":"692526e316eb3a9f1310398a","user":{"_id":"657db8c97cf2a47c8cd790f3","avatarUrl":"/avatars/35d5dfae09b79f37d8c4d08ebc0bcd10.svg","isPro":false,"fullname":"yes liu","user":"zhizhou57","type":"user"},"name":"Yesheng Liu","status":"claimed_verified","statusLastChangedAt":"2025-11-25T09:05:35.186Z","hidden":false},{"_id":"692526e316eb3a9f1310398b","name":"Hao Li","hidden":false},{"_id":"692526e316eb3a9f1310398c","user":{"_id":"68b97ed277d4f089dfded859","avatarUrl":"/avatars/ff20a41afea3bd5fd310c3e050862843.svg","isPro":false,"fullname":"Xu Haiyu","user":"lindaxu525","type":"user"},"name":"Haiyu Xu","status":"claimed_verified","statusLastChangedAt":"2025-11-25T09:05:37.241Z","hidden":false},{"_id":"692526e316eb3a9f1310398d","name":"Baoqi Pei","hidden":false},{"_id":"692526e316eb3a9f1310398e","name":"Jiahao Wang","hidden":false},{"_id":"692526e316eb3a9f1310398f","name":"Mingxuan Zhao","hidden":false},{"_id":"692526e316eb3a9f13103990","name":"Jingshu Zheng","hidden":false},{"_id":"692526e316eb3a9f13103991","name":"Zheqi He","hidden":false},{"_id":"692526e316eb3a9f13103992","name":"JG Yao","hidden":false},{"_id":"692526e316eb3a9f13103993","name":"Bowen Qin","hidden":false},{"_id":"692526e316eb3a9f13103994","name":"Xi Yang","hidden":false},{"_id":"692526e316eb3a9f13103995","name":"Jiajun Zhang","hidden":false}],"publishedAt":"2025-11-21T17:06:37.000Z","submittedOnDailyAt":"2025-11-25T23:16:45.235Z","title":"Beyond Multiple Choice: Verifiable OpenQA for Robust Vision-Language RFT","submittedOnDailyBy":{"_id":"657db8c97cf2a47c8cd790f3","avatarUrl":"/avatars/35d5dfae09b79f37d8c4d08ebc0bcd10.svg","isPro":false,"fullname":"yes liu","user":"zhizhou57","type":"user"},"summary":"Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.","upvotes":9,"discussionId":"692526e316eb3a9f13103996","ai_summary":"ReVeL, a framework that converts multiple-choice questions to open-form questions, improves data efficiency and robustness in fine-tuning multimodal language models and reveals score inflation in MCQA benchmarks.","ai_keywords":["reinforcement fine-tuning","multimodal language models","multiple-choice question answering","ReVeL","automatic verification","open-form questions","answer verification","GRPO","Qwen2.5-VL","OpenQA","score inflation","judging accuracy"]},"publishedAt":"2025-11-21T12:06:37.000Z","title":"Beyond Multiple Choice: Verifiable OpenQA for Robust Vision-Language RFT","summary":"Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17405.png","numComments":2,"submittedBy":{"_id":"657db8c97cf2a47c8cd790f3","avatarUrl":"/avatars/35d5dfae09b79f37d8c4d08ebc0bcd10.svg","fullname":"yes liu","name":"zhizhou57","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1},"isAuthorParticipating":true},{"paper":{"id":"2511.18922","authors":[{"_id":"6925cee24b7b0d870b182778","user":{"_id":"6354bda206d707b33249c4c2","avatarUrl":"/avatars/bbd9f76274ac52214df92084d50bc7b5.svg","isPro":false,"fullname":"Zhenxing Mi","user":"Mifucius","type":"user"},"name":"Zhenxing Mi","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:28:08.858Z","hidden":false},{"_id":"6925cee24b7b0d870b182779","user":{"_id":"6707f8800812a88fd6d87ebe","avatarUrl":"/avatars/28cd99359799ce85d5fe503144b776d8.svg","isPro":true,"fullname":"YUXIN WANG","user":"yuxinhk","type":"user"},"name":"Yuxin Wang","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:28:06.912Z","hidden":false},{"_id":"6925cee24b7b0d870b18277a","name":"Dan Xu","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6354bda206d707b33249c4c2/xx0ZGu22MXJB7yx4zoNk-.mp4"],"publishedAt":"2025-11-24T09:31:23.000Z","submittedOnDailyAt":"2025-11-25T14:43:07.437Z","title":"One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control","submittedOnDailyBy":{"_id":"6354bda206d707b33249c4c2","avatarUrl":"/avatars/bbd9f76274ac52214df92084d50bc7b5.svg","isPro":false,"fullname":"Zhenxing Mi","user":"Mifucius","type":"user"},"summary":"We present One4D, a unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps. By consistently handling varying sparsities of conditioning frames through a Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from a single image, 4D reconstruction from a full video, and mixed generation and reconstruction from sparse frames. Our framework adapts a powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC), which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps, connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on a mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models. Project page: https://mizhenxing.github.io/One4D","upvotes":8,"discussionId":"6925cee24b7b0d870b18277b","projectPage":"https://mizhenxing.github.io/One4D","githubRepo":"https://github.com/MiZhenxing/One4D","ai_summary":"One4D is a unified framework for 4D generation and reconstruction that uses a Unified Masked Conditioning mechanism and Decoupled LoRA Control to generate synchronized RGB frames and pointmaps from varying sparsities of input.","ai_keywords":["Unified Masked Conditioning (UMC)","diffusion finetuning","depthmap","pointmap","Decoupled LoRA Control (DLC)","LoRA adapters","RGB frames","pointmaps","4D generation","4D reconstruction","synthetic datasets","real 4D datasets","video diffusion models"],"githubStars":20},"publishedAt":"2025-11-24T04:31:23.000Z","title":"One4D: Unified 4D Generation and Reconstruction via Decoupled LoRA Control","summary":"We present One4D, a unified framework for 4D generation and reconstruction that produces dynamic 4D content as synchronized RGB frames and pointmaps. By consistently handling varying sparsities of conditioning frames through a Unified Masked Conditioning (UMC) mechanism, One4D can seamlessly transition between 4D generation from a single image, 4D reconstruction from a full video, and mixed generation and reconstruction from sparse frames. Our framework adapts a powerful video generation model for joint RGB and pointmap generation, with carefully designed network architectures. The commonly used diffusion finetuning strategies for depthmap or pointmap reconstruction often fail on joint RGB and pointmap generation, quickly degrading the base video model. To address this challenge, we introduce Decoupled LoRA Control (DLC), which employs two modality-specific LoRA adapters to form decoupled computation branches for RGB frames and pointmaps, connected by lightweight, zero-initialized control links that gradually learn mutual pixel-level consistency. Trained on a mixture of synthetic and real 4D datasets under modest computational budgets, One4D produces high-quality RGB frames and accurate pointmaps across both generation and reconstruction tasks. This work represents a step toward general, high-quality geometry-based 4D world modeling using video diffusion models. Project page: https://mizhenxing.github.io/One4D","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6354bda206d707b33249c4c2/xx0ZGu22MXJB7yx4zoNk-.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18922.png","numComments":1,"submittedBy":{"_id":"6354bda206d707b33249c4c2","avatarUrl":"/avatars/bbd9f76274ac52214df92084d50bc7b5.svg","fullname":"Zhenxing Mi","name":"Mifucius","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2},"isAuthorParticipating":true},{"paper":{"id":"2511.16166","authors":[{"_id":"6925cb4a4b7b0d870b18275f","name":"Zeting Liu","hidden":false},{"_id":"6925cb4a4b7b0d870b182760","name":"Zida Yang","hidden":false},{"_id":"6925cb4a4b7b0d870b182761","user":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","isPro":false,"fullname":"Zeyu Zhang","user":"SteveZeyuZhang","type":"user"},"name":"Zeyu Zhang","status":"claimed_verified","statusLastChangedAt":"2025-11-25T15:53:07.061Z","hidden":false},{"_id":"6925cb4a4b7b0d870b182762","name":"Hao Tang","hidden":false}],"publishedAt":"2025-11-20T09:08:33.000Z","submittedOnDailyAt":"2025-11-25T12:59:58.784Z","title":"EvoVLA: Self-Evolving Vision-Language-Action Model","submittedOnDailyBy":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","isPro":false,"fullname":"Zeyu Zhang","user":"SteveZeyuZhang","type":"user"},"summary":"Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.","upvotes":4,"discussionId":"6925cb4a4b7b0d870b182763","projectPage":"https://aigeeksgroup.github.io/EvoVLA/","githubRepo":"https://github.com/AIGeeksGroup/EvoVLA","ai_summary":"EvoVLA, a self-supervised VLA framework, enhances long-horizon robotic manipulation by addressing stage hallucination through triplet contrastive learning, pose-based exploration, and long-horizon memory, achieving improved success rates and sample efficiency on both simulated and real-world tasks.","ai_keywords":["Stage-Aligned Reward","triplet contrastive learning","Gemini-generated hard negatives","Pose-Based Object Exploration","relative object-gripper pose","Long-Horizon Memory","selective context retention","gated fusion","Discoverse-L","OpenVLA-OFT","sim-to-real transfer"],"githubStars":9},"publishedAt":"2025-11-20T04:08:33.000Z","title":"EvoVLA: Self-Evolving Vision-Language-Action Model","summary":"Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16166.png","numComments":1,"submittedBy":{"_id":"64ec877bb93654d4ca5c92e9","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg","fullname":"Zeyu Zhang","name":"SteveZeyuZhang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":4},"isAuthorParticipating":true},{"paper":{"id":"2511.16301","authors":[{"_id":"6925b8d24b7b0d870b182711","name":"Minseok Seo","hidden":false},{"_id":"6925b8d24b7b0d870b182712","name":"Mark Hamilton","hidden":false},{"_id":"6925b8d24b7b0d870b182713","name":"Changick Kim","hidden":false}],"publishedAt":"2025-11-20T12:27:53.000Z","submittedOnDailyAt":"2025-11-25T11:44:25.614Z","title":"Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling","submittedOnDailyBy":{"_id":"679c6c454544b95cb91c8aec","avatarUrl":"/avatars/b69f7c6da667cbe739200ce722c6fdbb.svg","isPro":true,"fullname":"minseokseo","user":"minseok96","type":"user"},"summary":"We present Upsample Anything, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only approx0.419 s per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling. Project page: https://seominseok0429.github.io/Upsample-Anything/{https://seominseok0429.github.io/Upsample-Anything/}","upvotes":6,"discussionId":"6925b8d24b7b0d870b182714","projectPage":"https://seominseok0429.github.io/Upsample-Anything/","githubRepo":"https://github.com/seominseok0429/Upsample-Anything-A-Simple-and-Hard-to-Beat-Baseline-for-Feature-Upsampling","ai_summary":"Upsample Anything is a lightweight test-time optimization framework that enhances low-resolution features to high-resolution outputs without training, using an anisotropic Gaussian kernel for precise reconstruction in tasks like semantic segmentation and depth estimation.","ai_keywords":["test-time optimization","Vision Foundation Models","feature upsampling","anisotropic Gaussian kernel","Gaussian Splatting","Joint Bilateral Upsampling","semantic segmentation","depth estimation","probability map upsampling"],"githubStars":61},"publishedAt":"2025-11-20T07:27:53.000Z","title":"Upsample Anything: A Simple and Hard to Beat Baseline for Feature Upsampling","summary":"We present Upsample Anything, a lightweight test-time optimization (TTO) framework that restores low-resolution features to high-resolution, pixel-wise outputs without any training. Although Vision Foundation Models demonstrate strong generalization across diverse downstream tasks, their representations are typically downsampled by 14x/16x (e.g., ViT), which limits their direct use in pixel-level applications. Existing feature upsampling approaches depend on dataset-specific retraining or heavy implicit optimization, restricting scalability and generalization. Upsample Anything addresses these issues through a simple per-image optimization that learns an anisotropic Gaussian kernel combining spatial and range cues, effectively bridging Gaussian Splatting and Joint Bilateral Upsampling. The learned kernel acts as a universal, edge-aware operator that transfers seamlessly across architectures and modalities, enabling precise high-resolution reconstruction of features, depth, or probability maps. It runs in only approx0.419 s per 224x224 image and achieves state-of-the-art performance on semantic segmentation, depth estimation, and both depth and probability map upsampling. Project page: https://seominseok0429.github.io/Upsample-Anything/{https://seominseok0429.github.io/Upsample-Anything/}","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16301.png","numComments":1,"submittedBy":{"_id":"679c6c454544b95cb91c8aec","avatarUrl":"/avatars/b69f7c6da667cbe739200ce722c6fdbb.svg","fullname":"minseokseo","name":"minseok96","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":1},"isAuthorParticipating":false},{"paper":{"id":"2511.19166","authors":[{"_id":"6925b8904b7b0d870b18270b","name":"Samantha Dies","hidden":false},{"_id":"6925b8904b7b0d870b18270c","name":"Courtney Maynard","hidden":false},{"_id":"6925b8904b7b0d870b18270d","user":{"_id":"667092a1abdd1ea72bbd5beb","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5DWPaDwCEo7_l97gcirYe.png","isPro":false,"fullname":"Germans Savcisens","user":"carlomarxx","type":"user"},"name":"Germans Savcisens","status":"claimed_verified","statusLastChangedAt":"2025-11-25T15:53:11.665Z","hidden":false},{"_id":"6925b8904b7b0d870b18270e","name":"Tina Eliassi-Rad","hidden":false}],"publishedAt":"2025-11-24T14:28:50.000Z","submittedOnDailyAt":"2025-11-25T11:40:35.372Z","title":"Representational Stability of Truth in Large Language Models","submittedOnDailyBy":{"_id":"667092a1abdd1ea72bbd5beb","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5DWPaDwCEo7_l97gcirYe.png","isPro":false,"fullname":"Germans Savcisens","user":"carlomarxx","type":"user"},"summary":"Large language models (LLMs) are widely used for factual tasks such as \"What treats asthma?\" or \"What is the capital of Latvia?\". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to 40% flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes (leq 8.2%). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone.","upvotes":1,"discussionId":"6925b8914b7b0d870b18270f","ai_summary":"LLMs exhibit varying levels of stability in encoding truth representations, influenced more by epistemic familiarity than linguistic form, as assessed through perturbation analysis of their activations.","ai_keywords":["large language models","representational stability","veracity representations","linear probe","activations","truth judgements","unfamiliar neither statements","familiar neither statements","epistemic familiarity","semantic uncertainty"]},"publishedAt":"2025-11-24T09:28:50.000Z","title":"Representational Stability of Truth in Large Language Models","summary":"Large language models (LLMs) are widely used for factual tasks such as \"What treats asthma?\" or \"What is the capital of Latvia?\". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to 40% flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes (leq 8.2%). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19166.png","numComments":1,"submittedBy":{"_id":"667092a1abdd1ea72bbd5beb","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5DWPaDwCEo7_l97gcirYe.png","fullname":"Germans Savcisens","name":"carlomarxx","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"isAuthorParticipating":false},{"paper":{"id":"2511.12810","authors":[{"_id":"69250b1a16eb3a9f131038db","user":{"_id":"65becdfd443909bf23265819","avatarUrl":"/avatars/b2f493c1315822a184901440b26ff6e8.svg","isPro":false,"fullname":"Leena Alghamdi","user":"linaa98","type":"user"},"name":"Leena Alghamdi","status":"claimed_verified","statusLastChangedAt":"2025-11-25T09:05:47.384Z","hidden":false},{"_id":"69250b1a16eb3a9f131038dc","name":"Muhammad Usman","hidden":false},{"_id":"69250b1a16eb3a9f131038dd","name":"Hafeez Anwar","hidden":false},{"_id":"69250b1a16eb3a9f131038de","name":"Abdul Bais","hidden":false},{"_id":"69250b1a16eb3a9f131038df","name":"Saeed Anwar","hidden":false}],"publishedAt":"2025-11-16T22:29:06.000Z","submittedOnDailyAt":"2025-11-25T11:02:25.829Z","title":"MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection","submittedOnDailyBy":{"_id":"65becdfd443909bf23265819","avatarUrl":"/avatars/b2f493c1315822a184901440b26ff6e8.svg","isPro":false,"fullname":"Leena Alghamdi","user":"linaa98","type":"user"},"summary":"Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at https://github.com/linaagh98/MSRNet{https://github.com/linaagh98/MSRNet}.","upvotes":0,"discussionId":"69250b1a16eb3a9f131038e0","ai_summary":"A Multi-Scale Recursive Network using a Pyramid Vision Transformer and specialized units improves camouflaged object detection by enhancing feature extraction and recursive feature refinement.","ai_keywords":["Pyramid Vision Transformer","Attention-Based Scale Integration Units","Multi-Granularity Fusion Units","recursive-feedback decoding strategy","multi-scale learning","recursive feature optimization"],"organization":{"_id":"6338798e20fc636fd8b178ba","name":"KFUPM","fullname":"KFUPM","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1676198977195-6338792d76421c054310cb07.png"}},"publishedAt":"2025-11-16T17:29:06.000Z","title":"MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection","summary":"Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at https://github.com/linaagh98/MSRNet{https://github.com/linaagh98/MSRNet}.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.12810.png","numComments":1,"submittedBy":{"_id":"65becdfd443909bf23265819","avatarUrl":"/avatars/b2f493c1315822a184901440b26ff6e8.svg","fullname":"Leena Alghamdi","name":"linaa98","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"organization":{"_id":"6338798e20fc636fd8b178ba","name":"KFUPM","fullname":"KFUPM","avatar":"https://cdn-uploads.huggingface.co/production/uploads/1676198977195-6338792d76421c054310cb07.png"},"isAuthorParticipating":true},{"paper":{"id":"2511.16397","authors":[{"_id":"6923c67eb5612535ed95590c","name":"Ren Ma","hidden":false},{"_id":"6923c67eb5612535ed95590d","name":"Jiantao Qiu","hidden":false},{"_id":"6923c67eb5612535ed95590e","name":"Chao Xu","hidden":false},{"_id":"6923c67eb5612535ed95590f","name":"Pei Chu","hidden":false},{"_id":"6923c67eb5612535ed955910","name":"Kaiwen Liu","hidden":false},{"_id":"6923c67eb5612535ed955911","name":"Pengli Ren","hidden":false},{"_id":"6923c67eb5612535ed955912","name":"Yuan Qu","hidden":false},{"_id":"6923c67eb5612535ed955913","name":"Jiahui Peng","hidden":false},{"_id":"6923c67eb5612535ed955914","name":"Linfeng Hou","hidden":false},{"_id":"6923c67eb5612535ed955915","name":"Mengjie Liu","hidden":false},{"_id":"6923c67eb5612535ed955916","name":"Lindong Lu","hidden":false},{"_id":"6923c67eb5612535ed955917","name":"Wenchang Ning","hidden":false},{"_id":"6923c67eb5612535ed955918","name":"Jia Yu","hidden":false},{"_id":"6923c67eb5612535ed955919","name":"Rui Min","hidden":false},{"_id":"6923c67eb5612535ed95591a","name":"Jin Shi","hidden":false},{"_id":"6923c67eb5612535ed95591b","name":"Haojiong Chen","hidden":false},{"_id":"6923c67eb5612535ed95591c","name":"Peng Zhang","hidden":false},{"_id":"6923c67eb5612535ed95591d","name":"Wenjian Zhang","hidden":false},{"_id":"6923c67eb5612535ed95591e","name":"Qian Jiang","hidden":false},{"_id":"6923c67eb5612535ed95591f","user":{"_id":"66fe2be698f30194f8816e81","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/66fe2be698f30194f8816e81/RJgGWkn6Tc-EP7OvvmN91.jpeg","isPro":false,"fullname":"Zengjie Hu","user":"FloSophorae","type":"user"},"name":"Zengjie Hu","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:28:21.069Z","hidden":false},{"_id":"6923c67eb5612535ed955920","name":"Guoqiang Yang","hidden":false},{"_id":"6923c67eb5612535ed955921","name":"Zhenxiang Li","hidden":false},{"_id":"6923c67eb5612535ed955922","name":"Fukai Shang","hidden":false},{"_id":"6923c67eb5612535ed955923","name":"Zhongying Tu","hidden":false},{"_id":"6923c67eb5612535ed955924","name":"Wentao Zhang","hidden":false},{"_id":"6923c67eb5612535ed955925","name":"Dahua Lin","hidden":false},{"_id":"6923c67eb5612535ed955926","name":"Conghui He","hidden":false}],"publishedAt":"2025-11-20T14:15:23.000Z","submittedOnDailyAt":"2025-11-25T07:09:22.609Z","title":"AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser","submittedOnDailyBy":{"_id":"5f17f0a0925b9863e28ad517","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/5f17f0a0925b9863e28ad517/fXIY5i9RLsIa1v3CCuVtt.jpeg","isPro":true,"fullname":"Victor Mustar","user":"victor","type":"user"},"summary":"While web data quality is crucial for large language models, most curation efforts focus on filtering and deduplication,treating HTML-to-text extraction as a fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, codes, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, a novel extraction pipeline that reformulates content extraction as a sequence labeling problem solved by a 0.6B-parameter language model. Unlike text-density heuristics, MinerU-HTML leverages semantic understanding and employs a two-stage formatting pipeline that explicitly categorizes semantic elements before converting to Markdown. Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On MainWebBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.8\\% ROUGE-N F1 compared to Trafilatura's 63.6\\%, with exceptional structured element preservation (90.9\\% for code blocks, 94.0\\% for formulas). Using MinerU-HTML, we construct AICC (AI-ready Common Crawl), a 7.3-trillion token multilingual corpus from two Common Crawl snapshots. In controlled pretraining experiments where AICC and Trafilatura-extracted TfCC undergo identical filtering, models trained on AICC (62B tokens) achieve 50.8\\% average accuracy across 13 benchmarks, outperforming TfCC by 1.08pp-providing direct evidence that extraction quality significantly impacts model capabilities. AICC also surpasses RefinedWeb and FineWeb on key benchmarks. We publicly release MainWebBench, MinerU-HTML, and AICC, demonstrating that HTML extraction is a critical, often underestimated component of web corpus construction.","upvotes":5,"discussionId":"6923c67eb5612535ed955927","ai_summary":"A novel extraction pipeline using a language model improves web data quality, significantly enhancing the performance of large language models trained on extracted corpora.","ai_keywords":["sequence labeling","language model","semantic understanding","two-stage formatting pipeline","Markdown","ROUGE-N F1","structured element preservation","multilingual corpus","pretraining experiments","benchmarks","AI-ready Common Crawl","RefinedWeb","FineWeb","HTML extraction"],"organization":{"_id":"66ce9d1f5e180b9b9c8e6f31","name":"opendatalab","fullname":"OpenDataLab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"}},"publishedAt":"2025-11-20T09:15:23.000Z","title":"AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser","summary":"While web data quality is crucial for large language models, most curation efforts focus on filtering and deduplication,treating HTML-to-text extraction as a fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, codes, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, a novel extraction pipeline that reformulates content extraction as a sequence labeling problem solved by a 0.6B-parameter language model. Unlike text-density heuristics, MinerU-HTML leverages semantic understanding and employs a two-stage formatting pipeline that explicitly categorizes semantic elements before converting to Markdown. Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On MainWebBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.8\\% ROUGE-N F1 compared to Trafilatura's 63.6\\%, with exceptional structured element preservation (90.9\\% for code blocks, 94.0\\% for formulas). Using MinerU-HTML, we construct AICC (AI-ready Common Crawl), a 7.3-trillion token multilingual corpus from two Common Crawl snapshots. In controlled pretraining experiments where AICC and Trafilatura-extracted TfCC undergo identical filtering, models trained on AICC (62B tokens) achieve 50.8\\% average accuracy across 13 benchmarks, outperforming TfCC by 1.08pp-providing direct evidence that extraction quality significantly impacts model capabilities. AICC also surpasses RefinedWeb and FineWeb on key benchmarks. We publicly release MainWebBench, MinerU-HTML, and AICC, demonstrating that HTML extraction is a critical, often underestimated component of web corpus construction.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.16397.png","numComments":1,"submittedBy":{"_id":"5f17f0a0925b9863e28ad517","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/5f17f0a0925b9863e28ad517/fXIY5i9RLsIa1v3CCuVtt.jpeg","fullname":"Victor Mustar","name":"victor","type":"user","isPro":true,"isHf":true,"isHfAdmin":true,"isMod":false,"followerCount":4887},"organization":{"_id":"66ce9d1f5e180b9b9c8e6f31","name":"opendatalab","fullname":"OpenDataLab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"},"isAuthorParticipating":false},{"paper":{"id":"2511.18945","authors":[{"_id":"692573ed16eb3a9f13103b05","user":{"_id":"63e5017caf814a2149ec772d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63e5017caf814a2149ec772d/W1Vfd4Na5kTmxeHoChSIB.jpeg","isPro":false,"fullname":"German G","user":"grgera","type":"user"},"name":"German Gritsai","status":"claimed_verified","statusLastChangedAt":"2025-11-25T09:34:27.825Z","hidden":false},{"_id":"692573ed16eb3a9f13103b06","name":"Megan Richards","hidden":false},{"_id":"692573ed16eb3a9f13103b07","user":{"_id":"641c5ae9dad248407395e3ab","avatarUrl":"/avatars/27c0bcac855c8683017443d6c713f3c8.svg","isPro":false,"fullname":"Maxime M","user":"pie3636","type":"user"},"name":"Maxime Mloux","status":"claimed_verified","statusLastChangedAt":"2025-11-25T15:53:13.298Z","hidden":false},{"_id":"692573ed16eb3a9f13103b08","name":"Kyunghyun Cho","hidden":false},{"_id":"692573ed16eb3a9f13103b09","user":{"_id":"6369394dd322a76e1ea4bdf6","avatarUrl":"/avatars/a4e5ab0167025fbbfc970d54630ce754.svg","isPro":false,"fullname":"Maxime Peyrard","user":"peyrardm","type":"user"},"name":"Maxime Peyrard","status":"claimed_verified","statusLastChangedAt":"2025-11-25T12:10:03.931Z","hidden":false}],"publishedAt":"2025-11-24T09:55:28.000Z","submittedOnDailyAt":"2025-11-25T06:50:17.276Z","title":"MIST: Mutual Information Via Supervised Training","submittedOnDailyBy":{"_id":"63e5017caf814a2149ec772d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63e5017caf814a2149ec772d/W1Vfd4Na5kTmxeHoChSIB.jpeg","isPro":false,"fullname":"German G","user":"grgera","type":"user"},"summary":"We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions.","upvotes":8,"discussionId":"692573ed16eb3a9f13103b0a","githubRepo":"https://github.com/grgera/mist","ai_summary":"A data-driven neural network approach estimates mutual information using a meta-dataset of synthetic distributions, offering flexibility, efficiency, and uncertainty quantification.","ai_keywords":["mutual information","MI estimator","neural network","MIST","meta-dataset","two-dimensional attention","permutation invariance","quantile regression","quantile-based intervals","bootstrap-based confidence intervals","normalizing flows"],"githubStars":1},"publishedAt":"2025-11-24T04:55:28.000Z","title":"MIST: Mutual Information Via Supervised Training","summary":"We propose a fully data-driven approach to designing mutual information (MI) estimators. Since any MI estimator is a function of the observed sample from two random variables, we parameterize this function with a neural network (MIST) and train it end-to-end to predict MI values. Training is performed on a large meta-dataset of 625,000 synthetic joint distributions with known ground-truth MI. To handle variable sample sizes and dimensions, we employ a two-dimensional attention scheme ensuring permutation invariance across input samples. To quantify uncertainty, we optimize a quantile regression loss, enabling the estimator to approximate the sampling distribution of MI rather than return a single point estimate. This research program departs from prior work by taking a fully empirical route, trading universal theoretical guarantees for flexibility and efficiency. Empirically, the learned estimators largely outperform classical baselines across sample sizes and dimensions, including on joint distributions unseen during training. The resulting quantile-based intervals are well-calibrated and more reliable than bootstrap-based confidence intervals, while inference is orders of magnitude faster than existing neural baselines. Beyond immediate empirical gains, this framework yields trainable, fully differentiable estimators that can be embedded into larger learning pipelines. Moreover, exploiting MI's invariance to invertible transformations, meta-datasets can be adapted to arbitrary data modalities via normalizing flows, enabling flexible training for diverse target meta-distributions.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18945.png","numComments":1,"submittedBy":{"_id":"63e5017caf814a2149ec772d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63e5017caf814a2149ec772d/W1Vfd4Na5kTmxeHoChSIB.jpeg","fullname":"German G","name":"grgera","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":2},"isAuthorParticipating":true},{"paper":{"id":"2511.18024","authors":[{"_id":"6925312216eb3a9f13103a40","name":"Dor Arviv","hidden":false},{"_id":"6925312216eb3a9f13103a41","user":{"_id":"662a80358a721ebd0b4f358b","avatarUrl":"/avatars/5be5a3c8b13f6f663206a19d0525c18e.svg","isPro":false,"fullname":"Yehonatan Elisha","user":"Yoniel","type":"user"},"name":"Yehonatan Elisha","status":"admin_assigned","statusLastChangedAt":"2025-11-25T09:34:23.580Z","hidden":false},{"_id":"6925312216eb3a9f13103a42","name":"Oren Barkan","hidden":false},{"_id":"6925312216eb3a9f13103a43","user":{"_id":"669536cad4ca2767b9a9da91","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/itE8uwV6pji4yR38F3zCP.png","isPro":false,"fullname":"Noam Koenigstein","user":"deltalab-noam","type":"user"},"name":"Noam Koenigstein","status":"admin_assigned","statusLastChangedAt":"2025-11-25T09:34:43.704Z","hidden":false}],"publishedAt":"2025-11-22T11:27:32.000Z","submittedOnDailyAt":"2025-11-25T05:26:49.924Z","title":"Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems","submittedOnDailyBy":{"_id":"662a80358a721ebd0b4f358b","avatarUrl":"/avatars/5be5a3c8b13f6f663206a19d0525c18e.svg","isPro":false,"fullname":"Yehonatan Elisha","user":"Yoniel","type":"user"},"summary":"We present a method for extracting monosemantic neurons, defined as latent dimensions that align with coherent and interpretable concepts, from user and item embeddings in recommender systems. Our approach employs a Sparse Autoencoder (SAE) to reveal semantic structure within pretrained representations. In contrast to work on language models, monosemanticity in recommendation must preserve the interactions between separate user and item embeddings. To achieve this, we introduce a prediction aware training objective that backpropagates through a frozen recommender and aligns the learned latent structure with the model's user-item affinity predictions. The resulting neurons capture properties such as genre, popularity, and temporal trends, and support post hoc control operations including targeted filtering and content promotion without modifying the base model. Our method generalizes across different recommendation models and datasets, providing a practical tool for interpretable and controllable personalization. Code and evaluation resources are available at https://github.com/DeltaLabTLV/Monosemanticity4Rec.","upvotes":1,"discussionId":"6925312316eb3a9f13103a44","ai_summary":"A Sparse Autoencoder method extracts interpretable latent dimensions from user and item embeddings in recommender systems, aligning with model predictions and supporting controllable personalization.","ai_keywords":["Sparse Autoencoder","monosemantic neurons","latent dimensions","pretrained representations","prediction aware","user-item affinity predictions","targeted filtering","content promotion"]},"publishedAt":"2025-11-22T06:27:32.000Z","title":"Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems","summary":"We present a method for extracting monosemantic neurons, defined as latent dimensions that align with coherent and interpretable concepts, from user and item embeddings in recommender systems. Our approach employs a Sparse Autoencoder (SAE) to reveal semantic structure within pretrained representations. In contrast to work on language models, monosemanticity in recommendation must preserve the interactions between separate user and item embeddings. To achieve this, we introduce a prediction aware training objective that backpropagates through a frozen recommender and aligns the learned latent structure with the model's user-item affinity predictions. The resulting neurons capture properties such as genre, popularity, and temporal trends, and support post hoc control operations including targeted filtering and content promotion without modifying the base model. Our method generalizes across different recommendation models and datasets, providing a practical tool for interpretable and controllable personalization. Code and evaluation resources are available at https://github.com/DeltaLabTLV/Monosemanticity4Rec.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18024.png","numComments":1,"submittedBy":{"_id":"662a80358a721ebd0b4f358b","avatarUrl":"/avatars/5be5a3c8b13f6f663206a19d0525c18e.svg","fullname":"Yehonatan Elisha","name":"Yoniel","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"isAuthorParticipating":false},{"paper":{"id":"2511.18047","authors":[{"_id":"692521e016eb3a9f13103957","name":"Oren Barkan","hidden":false},{"_id":"692521e016eb3a9f13103958","user":{"_id":"67150b7f1798d13be9550412","avatarUrl":"/avatars/bfb5f1b2f1b2355155b491cd7f0af825.svg","isPro":false,"fullname":"Yahlly Schein","user":"Yahlly21","type":"user"},"name":"Yahlly Schein","status":"admin_assigned","statusLastChangedAt":"2025-11-25T09:35:29.071Z","hidden":false},{"_id":"692521e016eb3a9f13103959","user":{"_id":"662a80358a721ebd0b4f358b","avatarUrl":"/avatars/5be5a3c8b13f6f663206a19d0525c18e.svg","isPro":false,"fullname":"Yehonatan Elisha","user":"Yoniel","type":"user"},"name":"Yehonatan Elisha","status":"admin_assigned","statusLastChangedAt":"2025-11-25T09:34:58.953Z","hidden":false},{"_id":"692521e016eb3a9f1310395a","name":"Veronika Bogina","hidden":false},{"_id":"692521e016eb3a9f1310395b","user":{"_id":"6655c15ba57d0c383610e3e6","avatarUrl":"/avatars/457f8b8731fef7abfdc3555768b038e7.svg","isPro":false,"fullname":"Mikhail Baklanov","user":"mikhailbaklanov","type":"user"},"name":"Mikhail Baklanov","status":"admin_assigned","statusLastChangedAt":"2025-11-25T09:35:17.142Z","hidden":false},{"_id":"692521e016eb3a9f1310395c","user":{"_id":"669536cad4ca2767b9a9da91","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/itE8uwV6pji4yR38F3zCP.png","isPro":false,"fullname":"Noam Koenigstein","user":"deltalab-noam","type":"user"},"name":"Noam Koenigstein","status":"admin_assigned","statusLastChangedAt":"2025-11-25T09:35:09.571Z","hidden":false}],"publishedAt":"2025-11-22T12:59:04.000Z","submittedOnDailyAt":"2025-11-25T05:25:33.165Z","title":"Fidelity-Aware Recommendation Explanations via Stochastic Path Integration","submittedOnDailyBy":{"_id":"662a80358a721ebd0b4f358b","avatarUrl":"/avatars/5be5a3c8b13f6f663206a19d0525c18e.svg","isPro":false,"fullname":"Yehonatan Elisha","user":"Yoniel","type":"user"},"summary":"Explanation fidelity, which measures how accurately an explanation reflects a model's true reasoning, remains critically underexplored in recommender systems. We introduce SPINRec (Stochastic Path Integration for Neural Recommender Explanations), a model-agnostic approach that adapts path-integration techniques to the sparse and implicit nature of recommendation data. To overcome the limitations of prior methods, SPINRec employs stochastic baseline sampling: instead of integrating from a fixed or unrealistic baseline, it samples multiple plausible user profiles from the empirical data distribution and selects the most faithful attribution path. This design captures the influence of both observed and unobserved interactions, yielding more stable and personalized explanations. We conduct the most comprehensive fidelity evaluation to date across three models (MF, VAE, NCF), three datasets (ML1M, Yahoo! Music, Pinterest), and a suite of counterfactual metrics, including AUC-based perturbation curves and fixed-length diagnostics. SPINRec consistently outperforms all baselines, establishing a new benchmark for faithful explainability in recommendation. Code and evaluation tools are publicly available at https://github.com/DeltaLabTLV/SPINRec.","upvotes":1,"discussionId":"692521e016eb3a9f1310395d","ai_summary":"SPINRec, a model-agnostic approach, enhances explanation fidelity in recommender systems by using stochastic baseline sampling and path-integration techniques to capture both observed and unobserved interactions.","ai_keywords":["explanation fidelity","path-integration","stochastic baseline sampling","user profiles","empirical data distribution","attribution path","counterfactual metrics","AUC-based perturbation curves","fixed-length diagnostics","MF","VAE","NCF","ML1M","Yahoo! Music","Pinterest"]},"publishedAt":"2025-11-22T07:59:04.000Z","title":"Fidelity-Aware Recommendation Explanations via Stochastic Path Integration","summary":"Explanation fidelity, which measures how accurately an explanation reflects a model's true reasoning, remains critically underexplored in recommender systems. We introduce SPINRec (Stochastic Path Integration for Neural Recommender Explanations), a model-agnostic approach that adapts path-integration techniques to the sparse and implicit nature of recommendation data. To overcome the limitations of prior methods, SPINRec employs stochastic baseline sampling: instead of integrating from a fixed or unrealistic baseline, it samples multiple plausible user profiles from the empirical data distribution and selects the most faithful attribution path. This design captures the influence of both observed and unobserved interactions, yielding more stable and personalized explanations. We conduct the most comprehensive fidelity evaluation to date across three models (MF, VAE, NCF), three datasets (ML1M, Yahoo! Music, Pinterest), and a suite of counterfactual metrics, including AUC-based perturbation curves and fixed-length diagnostics. SPINRec consistently outperforms all baselines, establishing a new benchmark for faithful explainability in recommendation. Code and evaluation tools are publicly available at https://github.com/DeltaLabTLV/SPINRec.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18047.png","numComments":1,"submittedBy":{"_id":"662a80358a721ebd0b4f358b","avatarUrl":"/avatars/5be5a3c8b13f6f663206a19d0525c18e.svg","fullname":"Yehonatan Elisha","name":"Yoniel","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"isAuthorParticipating":false},{"paper":{"id":"2511.13288","authors":[{"_id":"69255d6f16eb3a9f13103aa3","name":"Haoyang Hong","hidden":false},{"_id":"69255d6f16eb3a9f13103aa4","name":"Jiajun Yin","hidden":false},{"_id":"69255d6f16eb3a9f13103aa5","name":"Yuan Wang","hidden":false},{"_id":"69255d6f16eb3a9f13103aa6","name":"Jingnan Liu","hidden":false},{"_id":"69255d6f16eb3a9f13103aa7","name":"Zhe Chen","hidden":false},{"_id":"69255d6f16eb3a9f13103aa8","name":"Ailing Yu","hidden":false},{"_id":"69255d6f16eb3a9f13103aa9","name":"Ji Li","hidden":false},{"_id":"69255d6f16eb3a9f13103aaa","name":"Zhiling Ye","hidden":false},{"_id":"69255d6f16eb3a9f13103aab","name":"Hansong Xiao","hidden":false},{"_id":"69255d6f16eb3a9f13103aac","name":"Yefei Chen","hidden":false},{"_id":"69255d6f16eb3a9f13103aad","name":"Hualei Zhou","hidden":false},{"_id":"69255d6f16eb3a9f13103aae","name":"Yun Yue","hidden":false},{"_id":"69255d6f16eb3a9f13103aaf","name":"Minghui Yang","hidden":false},{"_id":"69255d6f16eb3a9f13103ab0","name":"Chunxiao Guo","hidden":false},{"_id":"69255d6f16eb3a9f13103ab1","name":"Junwei Liu","hidden":false},{"_id":"69255d6f16eb3a9f13103ab2","name":"Peng Wei","hidden":false},{"_id":"69255d6f16eb3a9f13103ab3","name":"Jinjie Gu","hidden":false}],"publishedAt":"2025-11-17T12:06:30.000Z","submittedOnDailyAt":"2025-11-25T05:12:51.765Z","title":"Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO","submittedOnDailyBy":{"_id":"6826eb9bf609dcc6d6c6df52","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6826eb9bf609dcc6d6c6df52/L_uK87dUkyuDy7iu1g_6C.jpeg","isPro":false,"fullname":"Erinyu","user":"eerrr9","type":"user"},"summary":"Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.","upvotes":14,"discussionId":"69255d7016eb3a9f13103ab4","ai_summary":"M-GRPO, an extension of Group Relative Policy Optimization for hierarchical multi-agent systems, improves stability and efficiency in tool-augmented reasoning tasks by aligning heterogeneous trajectories and decoupling agent training.","ai_keywords":["multi-agent systems","large language model (LLM)","Group Relative Policy Optimization","hierarchical credit assignment","trajectory-alignment scheme","decoupled training pipeline","shared store","cross-server backpropagation","tool-augmented reasoning tasks"],"organization":{"_id":"68a4291ddb54c3044cf32600","name":"AQ-MedAI","fullname":"AQ","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6826eb9bf609dcc6d6c6df52/mOPPxrtf_etoO1rlLBtnV.jpeg"}},"publishedAt":"2025-11-17T07:06:30.000Z","title":"Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO","summary":"Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13288.png","numComments":1,"submittedBy":{"_id":"6826eb9bf609dcc6d6c6df52","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6826eb9bf609dcc6d6c6df52/L_uK87dUkyuDy7iu1g_6C.jpeg","fullname":"Erinyu","name":"eerrr9","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":3},"organization":{"_id":"68a4291ddb54c3044cf32600","name":"AQ-MedAI","fullname":"AQ","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6826eb9bf609dcc6d6c6df52/mOPPxrtf_etoO1rlLBtnV.jpeg"},"isAuthorParticipating":false},{"paper":{"id":"2511.19399","authors":[{"_id":"692531b316eb3a9f13103a46","name":"Rulin Shao","hidden":false},{"_id":"692531b316eb3a9f13103a47","user":{"_id":"6266e0cb7a1f5a1562c4e86e","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6266e0cb7a1f5a1562c4e86e/kkJZPssa76mdRPnKH-q3e.jpeg","isPro":false,"fullname":"Akari Asai","user":"akariasai","type":"user"},"name":"Akari Asai","status":"claimed_verified","statusLastChangedAt":"2025-11-25T15:53:14.992Z","hidden":false},{"_id":"692531b316eb3a9f13103a48","user":{"_id":"613f897ffbfd59f147a88c81","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1659455519119-613f897ffbfd59f147a88c81.jpeg","isPro":false,"fullname":"Shannon Shen","user":"shannons","type":"user"},"name":"Shannon Zejiang Shen","status":"claimed_verified","statusLastChangedAt":"2025-11-25T09:05:11.372Z","hidden":false},{"_id":"692531b316eb3a9f13103a49","user":{"_id":"62608fc2ffe8827cb1d89f9f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1654027835241-62608fc2ffe8827cb1d89f9f.png","isPro":false,"fullname":"Hamish Ivison","user":"hamishivi","type":"user"},"name":"Hamish Ivison","status":"claimed_verified","statusLastChangedAt":"2025-11-25T09:05:13.345Z","hidden":false},{"_id":"692531b316eb3a9f13103a4a","name":"Varsha Kishore","hidden":false},{"_id":"692531b316eb3a9f13103a4b","user":{"_id":"6530c7263976e5f4412ba737","avatarUrl":"/avatars/8b4d9d847a9e115c3da8cad629bd0a41.svg","isPro":false,"fullname":"Jingming Zhuo","user":"JingmingZ","type":"user"},"name":"Jingming Zhuo","status":"claimed_verified","statusLastChangedAt":"2025-11-25T09:34:32.721Z","hidden":false},{"_id":"692531b316eb3a9f13103a4c","name":"Xinran Zhao","hidden":false},{"_id":"692531b316eb3a9f13103a4d","name":"Molly Park","hidden":false},{"_id":"692531b316eb3a9f13103a4e","name":"Samuel G. Finlayson","hidden":false},{"_id":"692531b316eb3a9f13103a4f","name":"David Sontag","hidden":false},{"_id":"692531b316eb3a9f13103a50","user":{"_id":"65e5fefbe8cbae176d9ca005","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65e5fefbe8cbae176d9ca005/5SLREDsVzycEwVsPNv765.jpeg","isPro":false,"fullname":"Tyler Murray","user":"undfined","type":"user"},"name":"Tyler Murray","status":"admin_assigned","statusLastChangedAt":"2025-11-25T09:41:12.772Z","hidden":false},{"_id":"692531b316eb3a9f13103a51","user":{"_id":"63a76d0de27a6dbd485fe863","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/63a76d0de27a6dbd485fe863/qJJwHOuvyQGq1o0KscOF_.jpeg","isPro":false,"fullname":"Sewon Min","user":"sewon","type":"user"},"name":"Sewon Min","status":"admin_assigned","statusLastChangedAt":"2025-11-25T09:41:07.253Z","hidden":false},{"_id":"692531b316eb3a9f13103a52","user":{"_id":"6408fcc93461c51cf735a61e","avatarUrl":"/avatars/619f3653911d111f046a5a6c30fc8319.svg","isPro":false,"fullname":"Pradeep Dasigi","user":"pradeepd","type":"user"},"name":"Pradeep Dasigi","status":"admin_assigned","statusLastChangedAt":"2025-11-25T09:41:00.782Z","hidden":false},{"_id":"692531b316eb3a9f13103a53","name":"Luca Soldaini","hidden":false},{"_id":"692531b316eb3a9f13103a54","user":{"_id":"65282b8d578679aac7888aec","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/65282b8d578679aac7888aec/dibBkhH-z1c70mJZZxJ7u.jpeg","isPro":false,"fullname":"Faeze Brahman","user":"faezeb","type":"user"},"name":"Faeze Brahman","status":"admin_assigned","statusLastChangedAt":"2025-11-25T09:41:17.953Z","hidden":false},{"_id":"692531b316eb3a9f13103a55","name":"Wen-tau Yih","hidden":false},{"_id":"692531b316eb3a9f13103a56","name":"Tongshuang Wu","hidden":false},{"_id":"692531b316eb3a9f13103a57","name":"Luke Zettlemoyer","hidden":false},{"_id":"692531b316eb3a9f13103a58","name":"Yoon Kim","hidden":false},{"_id":"692531b316eb3a9f13103a59","name":"Hannaneh Hajishirzi","hidden":false},{"_id":"692531b316eb3a9f13103a5a","user":{"_id":"641b4263abfce26bcf7b27de","avatarUrl":"/avatars/e91b4205e4f74b0dd8c333c23203a924.svg","isPro":false,"fullname":"Pang Wei Koh","user":"pangwei","type":"user"},"name":"Pang Wei Koh","status":"admin_assigned","statusLastChangedAt":"2025-11-25T09:38:49.292Z","hidden":false}],"publishedAt":"2025-11-24T18:35:54.000Z","submittedOnDailyAt":"2025-11-25T02:20:40.847Z","title":"DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research","submittedOnDailyBy":{"_id":"62608fc2ffe8827cb1d89f9f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1654027835241-62608fc2ffe8827cb1d89f9f.png","isPro":false,"fullname":"Hamish Ivison","user":"hamishivi","type":"user"},"summary":"Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.","upvotes":46,"discussionId":"692531b316eb3a9f13103a5b","projectPage":"https://github.com/rlresearch/dr-tulu","githubRepo":"https://github.com/rlresearch/dr-tulu","ai_summary":"Reinforcement Learning with Evolving Rubrics (RLER) enables training of deep research models for long-form tasks, outperforming existing models and proprietary systems while being more cost-effective.","ai_keywords":["Reinforcement Learning with Verifiable Rewards (RLVR)","Reinforcement Learning with Evolving Rubrics (RLER)","Deep Research Tulu (DR Tulu-8B)","long-form deep research","deep research models","open-ended tasks","science benchmarks","healthcare benchmarks","general domain benchmarks","MCP-based agent infrastructure"],"githubStars":350,"organization":{"_id":"6916602cb89e7abe20e1da38","name":"rl-research","fullname":"RL ReSearch","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6334a0bd31a2be3938c59537/iySh9RAgtqs7m77mwfFOi.png"}},"publishedAt":"2025-11-24T13:35:54.000Z","title":"DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research","summary":"Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19399.png","numComments":1,"submittedBy":{"_id":"62608fc2ffe8827cb1d89f9f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1654027835241-62608fc2ffe8827cb1d89f9f.png","fullname":"Hamish Ivison","name":"hamishivi","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":17},"organization":{"_id":"6916602cb89e7abe20e1da38","name":"rl-research","fullname":"RL ReSearch","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6334a0bd31a2be3938c59537/iySh9RAgtqs7m77mwfFOi.png"},"isAuthorParticipating":true},{"paper":{"id":"2511.18373","authors":[{"_id":"6925336216eb3a9f13103a5d","name":"Xiyang Wu","hidden":false},{"_id":"6925336216eb3a9f13103a5e","name":"Zongxia Li","hidden":false},{"_id":"6925336216eb3a9f13103a5f","name":"Jihui Jin","hidden":false},{"_id":"6925336216eb3a9f13103a60","name":"Guangyao Shi","hidden":false},{"_id":"6925336216eb3a9f13103a61","name":"Gouthaman KV","hidden":false},{"_id":"6925336216eb3a9f13103a62","name":"Vishnu Raj","hidden":false},{"_id":"6925336216eb3a9f13103a63","name":"Nilotpal Sinha","hidden":false},{"_id":"6925336216eb3a9f13103a64","name":"Jingxi Chen","hidden":false},{"_id":"6925336216eb3a9f13103a65","name":"Fan Du","hidden":false},{"_id":"6925336216eb3a9f13103a66","name":"Dinesh Manocha","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64ea62f918d79efd533c93fe/4cDs65YDGfa_Q8AFZoryW.png"],"publishedAt":"2025-11-23T09:43:44.000Z","submittedOnDailyAt":"2025-11-25T02:13:07.225Z","title":"MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models","submittedOnDailyBy":{"_id":"64ea62f918d79efd533c93fe","avatarUrl":"/avatars/9985a789ce11b788de2cba12adfb72fc.svg","isPro":true,"fullname":"Xiyang Wu","user":"wuxiyang","type":"user"},"summary":"Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and comprehension. These results validate the effectiveness of our approach.","upvotes":5,"discussionId":"6925336216eb3a9f13103a67","ai_summary":"A method that enhances vision language models with spatial-temporal signals and motion tracking improves their performance on physics-driven video reasoning tasks.","ai_keywords":["Vision Language Models","VLMs","physics-driven reasoning","motion dynamics","spatial interactions","MASS-Bench","real-world videos","AIGC videos","video question-answering","visual detections","sub-segment grounding","3D motion tracking","MASS","model-agnostic method","depth-based 3D encoding","visual grounding","motion tracker","object dynamics","reinforcement fine-tuning","cross-modal alignment","physics reasoning","comprehension","Gemini-2.5-Flash"],"organization":{"_id":"68b3c3bbc375e05b059370b2","name":"UMCP","fullname":"University of Maryland College Park","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68b3c2c3a4ea236d1a97871a/bji3nI5ZWm2r4JX_-HLo0.png"}},"publishedAt":"2025-11-23T04:43:44.000Z","title":"MASS: Motion-Aware Spatial-Temporal Grounding for Physics Reasoning and Comprehension in Vision-Language Models","summary":"Vision Language Models (VLMs) perform well on standard video tasks but struggle with physics-driven reasoning involving motion dynamics and spatial interactions. This limitation reduces their ability to interpret real or AI-generated content (AIGC) videos and to generate physically consistent content. We present an approach that addresses this gap by translating physical-world context cues into interpretable representations aligned with VLMs' perception, comprehension, and reasoning. We introduce MASS-Bench, a comprehensive benchmark consisting of 4,350 real-world and AIGC videos and 8,361 free-form video question-answering pairs focused on physics-related comprehension tasks, with detailed annotations including visual detections, sub-segment grounding, and full-sequence 3D motion tracking of entities. We further present MASS, a model-agnostic method that injects spatial-temporal signals into the VLM language space via depth-based 3D encoding and visual grounding, coupled with a motion tracker for object dynamics. To strengthen cross-modal alignment and reasoning, we apply reinforcement fine-tuning. Experiments and ablations show that our refined VLMs outperform comparable and larger baselines, as well as prior state-of-the-art models, by 8.7% and 6.0%, achieving performance comparable to close-source SoTA VLMs such as Gemini-2.5-Flash on physics reasoning and comprehension. These results validate the effectiveness of our approach.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/64ea62f918d79efd533c93fe/4cDs65YDGfa_Q8AFZoryW.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18373.png","numComments":1,"submittedBy":{"_id":"64ea62f918d79efd533c93fe","avatarUrl":"/avatars/9985a789ce11b788de2cba12adfb72fc.svg","fullname":"Xiyang Wu","name":"wuxiyang","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false},"organization":{"_id":"68b3c3bbc375e05b059370b2","name":"UMCP","fullname":"University of Maryland College Park","avatar":"https://cdn-uploads.huggingface.co/production/uploads/68b3c2c3a4ea236d1a97871a/bji3nI5ZWm2r4JX_-HLo0.png"},"isAuthorParticipating":false},{"paper":{"id":"2511.17803","authors":[{"_id":"6925197016eb3a9f13103923","name":"Kumar Krishna Agrawal","hidden":false},{"_id":"6925197016eb3a9f13103924","name":"Longchao Liu","hidden":false},{"_id":"6925197016eb3a9f13103925","name":"Long Lian","hidden":false},{"_id":"6925197016eb3a9f13103926","name":"Michael Nercessian","hidden":false},{"_id":"6925197016eb3a9f13103927","name":"Natalia Harguindeguy","hidden":false},{"_id":"6925197016eb3a9f13103928","name":"Yufu Wu","hidden":false},{"_id":"6925197016eb3a9f13103929","name":"Peter Mikhael","hidden":false},{"_id":"6925197016eb3a9f1310392a","name":"Gigin Lin","hidden":false},{"_id":"6925197016eb3a9f1310392b","name":"Lecia V. Sequist","hidden":false},{"_id":"6925197016eb3a9f1310392c","name":"Florian Fintelmann","hidden":false},{"_id":"6925197016eb3a9f1310392d","name":"Trevor Darrell","hidden":false},{"_id":"6925197016eb3a9f1310392e","name":"Yutong Bai","hidden":false},{"_id":"6925197016eb3a9f1310392f","name":"Maggie Chung","hidden":false},{"_id":"6925197016eb3a9f13103930","user":{"_id":"6333a9195a032dcd095dda13","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/1664329996201-noauth.jpeg","isPro":true,"fullname":"Adam Yala","user":"yala","type":"user"},"name":"Adam Yala","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:28:17.228Z","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6254623af74165c7d2df0c18/-cTjmFF64vxY4SyK8UIAa.png"],"publishedAt":"2025-11-21T21:50:34.000Z","submittedOnDailyAt":"2025-11-25T01:49:24.068Z","title":"Pillar-0: A New Frontier for Radiology Foundation Models","submittedOnDailyBy":{"_id":"6254623af74165c7d2df0c18","avatarUrl":"/avatars/4ddd1aa858723dd9348fc51966f700c3.svg","isPro":false,"fullname":"Kumar Krishna Agrawal","user":"kumarkrishna","type":"user"},"summary":"Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth. Foundation models offer a path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. We introduce Pillar-0, a radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a large academic center, together with RATE, a scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using LLMs. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRIs, Pillar-0 establishes a new performance frontier, achieving mean AUROCs of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2\\% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford Abdominal CT dataset, including Merlin (82.2 vs 80.6 AUROC). Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction, where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection, Pillar-0 obtained a >95 AUROC when using only 1/20th of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open, clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints.","upvotes":17,"discussionId":"6925197016eb3a9f13103931","ai_summary":"Pillar-0, a radiology foundation model pretrained on diverse imaging datasets, outperforms existing models across various tasks and extends to new applications using RATE for label extraction.","ai_keywords":["foundation models","volumetric CT","MRI","grayscale contrast","RATE","structured labels","AUROC","MedGemma","MedImageInsight","Lingshu","Merlin","Stanford Abdominal CT dataset","long-horizon lung cancer risk prediction","Sybil","C-index","brain hemorrhage detection","sample efficiency","open","clinically rigorous"],"organization":{"_id":"66d1308fe801db14250a1cc6","name":"YalaLab","fullname":"Yala Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6333a9195a032dcd095dda13/r-7psCzH30cILTNWRQ60N.png"}},"publishedAt":"2025-11-21T16:50:34.000Z","title":"Pillar-0: A New Frontier for Radiology Foundation Models","summary":"Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth. Foundation models offer a path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. We introduce Pillar-0, a radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a large academic center, together with RATE, a scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using LLMs. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRIs, Pillar-0 establishes a new performance frontier, achieving mean AUROCs of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2\\% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford Abdominal CT dataset, including Merlin (82.2 vs 80.6 AUROC). Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction, where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection, Pillar-0 obtained a >95 AUROC when using only 1/20th of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open, clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6254623af74165c7d2df0c18/-cTjmFF64vxY4SyK8UIAa.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17803.png","numComments":1,"submittedBy":{"_id":"6254623af74165c7d2df0c18","avatarUrl":"/avatars/4ddd1aa858723dd9348fc51966f700c3.svg","fullname":"Kumar Krishna Agrawal","name":"kumarkrishna","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"organization":{"_id":"66d1308fe801db14250a1cc6","name":"YalaLab","fullname":"Yala Lab","avatar":"https://cdn-uploads.huggingface.co/production/uploads/6333a9195a032dcd095dda13/r-7psCzH30cILTNWRQ60N.png"},"isAuthorParticipating":false},{"paper":{"id":"2511.19418","authors":[{"_id":"69252cfc16eb3a9f13103a29","name":"Yiming Qin","hidden":false},{"_id":"69252cfc16eb3a9f13103a2a","name":"Bomin Wei","hidden":false},{"_id":"69252cfc16eb3a9f13103a2b","name":"Jiaxin Ge","hidden":false},{"_id":"69252cfc16eb3a9f13103a2c","name":"Konstantinos Kallidromitis","hidden":false},{"_id":"69252cfc16eb3a9f13103a2d","name":"Stephanie Fu","hidden":false},{"_id":"69252cfc16eb3a9f13103a2e","name":"Trevor Darrell","hidden":false},{"_id":"69252cfc16eb3a9f13103a2f","name":"Xudong Wang","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/yfpo2h56TTWsr_Aqwsdp5.png"],"publishedAt":"2025-11-24T18:55:19.000Z","submittedOnDailyAt":"2025-11-25T01:44:14.396Z","title":"Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.","upvotes":20,"discussionId":"69252cfd16eb3a9f13103a30","projectPage":"https://wakalsprojectpage.github.io/comt-website/","ai_summary":"Chain-of-Visual-Thought (COVT) enables Vision-Language Models to reason through visual tokens, improving their performance on perceptual tasks by capturing dense visual information.","ai_keywords":["Chain-of-Visual-Thought","COVT","continuous visual tokens","lightweight vision experts","dense visual information","spatial dimensions","2D appearance","3D geometry","spatial layout","edge structure","autoregressive prediction","dense supervision signals","depth","segmentation","edges","DINO features","CV-Bench","MMVP","RealWorldQA","MMStar","WorldMedQA","HRBench","Qwen2.5-VL","LLaVA","multimodal intelligence"]},"publishedAt":"2025-11-24T13:55:19.000Z","title":"Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens","summary":"Vision-Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness. This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions. We introduce Chain-of-Visual-Thought (COVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens-compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, COVT distills knowledge from lightweight vision experts, capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure. During training, the VLM with COVT autoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features). At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability. Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating COVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/yfpo2h56TTWsr_Aqwsdp5.png"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19418.png","numComments":2,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":171},"isAuthorParticipating":false},{"paper":{"id":"2511.18870","authors":[{"_id":"69252c0d16eb3a9f131039d6","name":"Bing Wu","hidden":false},{"_id":"69252c0d16eb3a9f131039d7","name":"Chang Zou","hidden":false},{"_id":"69252c0d16eb3a9f131039d8","name":"Changlin Li","hidden":false},{"_id":"69252c0d16eb3a9f131039d9","name":"Duojun Huang","hidden":false},{"_id":"69252c0d16eb3a9f131039da","name":"Fang Yang","hidden":false},{"_id":"69252c0d16eb3a9f131039db","name":"Hao Tan","hidden":false},{"_id":"69252c0d16eb3a9f131039dc","name":"Jack Peng","hidden":false},{"_id":"69252c0d16eb3a9f131039dd","name":"Jianbing Wu","hidden":false},{"_id":"69252c0d16eb3a9f131039de","user":{"_id":"62a04f5e445b33e7d15e05ee","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vS1YjcyOucCgFrde0lAvu.png","isPro":false,"fullname":"Xiong","user":"Jefxiong","type":"user"},"name":"Jiangfeng Xiong","status":"claimed_verified","statusLastChangedAt":"2025-11-26T09:28:15.062Z","hidden":false},{"_id":"69252c0d16eb3a9f131039df","name":"Jie Jiang","hidden":false},{"_id":"69252c0d16eb3a9f131039e0","name":"Linus","hidden":false},{"_id":"69252c0d16eb3a9f131039e1","name":"Patrol","hidden":false},{"_id":"69252c0d16eb3a9f131039e2","user":{"_id":"670bc2aac9c7e01d3db61243","avatarUrl":"/avatars/2bd5fba261e710a4c287fcf2f78b584f.svg","isPro":false,"fullname":"Peizhen Zhang","user":"maxpzzhang","type":"user"},"name":"Peizhen Zhang","status":"admin_assigned","statusLastChangedAt":"2025-11-25T12:19:00.711Z","hidden":false},{"_id":"69252c0d16eb3a9f131039e3","name":"Peng Chen","hidden":false},{"_id":"69252c0d16eb3a9f131039e4","name":"Penghao Zhao","hidden":false},{"_id":"69252c0d16eb3a9f131039e5","name":"Qi Tian","hidden":false},{"_id":"69252c0d16eb3a9f131039e6","name":"Songtao Liu","hidden":false},{"_id":"69252c0d16eb3a9f131039e7","user":{"_id":"67f7ab7ef241f0b92ce4b087","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vDhBPKqfWjMI65K-kL2tm.jpeg","isPro":false,"fullname":"Weijie Kong","user":"Knightbreeze","type":"user"},"name":"Weijie Kong","status":"admin_assigned","statusLastChangedAt":"2025-11-25T12:19:44.134Z","hidden":false},{"_id":"69252c0d16eb3a9f131039e8","name":"Weiyan Wang","hidden":false},{"_id":"69252c0d16eb3a9f131039e9","name":"Xiao He","hidden":false},{"_id":"69252c0d16eb3a9f131039ea","name":"Xin Li","hidden":false},{"_id":"69252c0d16eb3a9f131039eb","name":"Xinchi Deng","hidden":false},{"_id":"69252c0d16eb3a9f131039ec","name":"Xuefei Zhe","hidden":false},{"_id":"69252c0d16eb3a9f131039ed","name":"Yang Li","hidden":false},{"_id":"69252c0d16eb3a9f131039ee","name":"Yanxin Long","hidden":false},{"_id":"69252c0d16eb3a9f131039ef","name":"Yuanbo Peng","hidden":false},{"_id":"69252c0d16eb3a9f131039f0","name":"Yue Wu","hidden":false},{"_id":"69252c0d16eb3a9f131039f1","name":"Yuhong Liu","hidden":false},{"_id":"69252c0d16eb3a9f131039f2","name":"Zhenyu Wang","hidden":false},{"_id":"69252c0d16eb3a9f131039f3","user":{"_id":"6316fe2a6d322c52490f90e8","avatarUrl":"/avatars/8c896b3733e183657009a426834a9dab.svg","isPro":false,"fullname":"Dai Zuozhuo","user":"zyand","type":"user"},"name":"Zuozhuo Dai","status":"admin_assigned","statusLastChangedAt":"2025-11-25T12:19:54.939Z","hidden":false},{"_id":"69252c0d16eb3a9f131039f4","name":"Bo Peng","hidden":false},{"_id":"69252c0d16eb3a9f131039f5","name":"Coopers Li","hidden":false},{"_id":"69252c0d16eb3a9f131039f6","name":"Gu Gong","hidden":false},{"_id":"69252c0d16eb3a9f131039f7","name":"Guojian Xiao","hidden":false},{"_id":"69252c0d16eb3a9f131039f8","name":"Jiahe Tian","hidden":false},{"_id":"69252c0d16eb3a9f131039f9","name":"Jiaxin Lin","hidden":false},{"_id":"69252c0d16eb3a9f131039fa","name":"Jie Liu","hidden":false},{"_id":"69252c0d16eb3a9f131039fb","name":"Jihong Zhang","hidden":false},{"_id":"69252c0d16eb3a9f131039fc","name":"Jiesong Lian","hidden":false},{"_id":"69252c0d16eb3a9f131039fd","name":"Kaihang Pan","hidden":false},{"_id":"69252c0d16eb3a9f131039fe","name":"Lei Wang","hidden":false},{"_id":"69252c0d16eb3a9f131039ff","name":"Lin Niu","hidden":false},{"_id":"69252c0d16eb3a9f13103a00","name":"Mingtao Chen","hidden":false},{"_id":"69252c0d16eb3a9f13103a01","name":"Mingyang Chen","hidden":false},{"_id":"69252c0d16eb3a9f13103a02","name":"Mingzhe Zheng","hidden":false},{"_id":"69252c0d16eb3a9f13103a03","name":"Miles Yang","hidden":false},{"_id":"69252c0d16eb3a9f13103a04","name":"Qiangqiang Hu","hidden":false},{"_id":"69252c0d16eb3a9f13103a05","name":"Qi Yang","hidden":false},{"_id":"69252c0d16eb3a9f13103a06","name":"Qiuyong Xiao","hidden":false},{"_id":"69252c0d16eb3a9f13103a07","name":"Runzhou Wu","hidden":false},{"_id":"69252c0d16eb3a9f13103a08","name":"Ryan Xu","hidden":false},{"_id":"69252c0d16eb3a9f13103a09","name":"Rui Yuan","hidden":false},{"_id":"69252c0d16eb3a9f13103a0a","name":"Shanshan Sang","hidden":false},{"_id":"69252c0d16eb3a9f13103a0b","name":"Shisheng Huang","hidden":false},{"_id":"69252c0d16eb3a9f13103a0c","name":"Siruis Gong","hidden":false},{"_id":"69252c0d16eb3a9f13103a0d","name":"Shuo Huang","hidden":false},{"_id":"69252c0d16eb3a9f13103a0e","name":"Weiting Guo","hidden":false},{"_id":"69252c0d16eb3a9f13103a0f","name":"Xiang Yuan","hidden":false},{"_id":"69252c0d16eb3a9f13103a10","name":"Xiaojia Chen","hidden":false},{"_id":"69252c0d16eb3a9f13103a11","name":"Xiawei Hu","hidden":false},{"_id":"69252c0d16eb3a9f13103a12","name":"Wenzhi Sun","hidden":false},{"_id":"69252c0d16eb3a9f13103a13","name":"Xiele Wu","hidden":false},{"_id":"69252c0d16eb3a9f13103a14","name":"Xianshun Ren","hidden":false},{"_id":"69252c0d16eb3a9f13103a15","name":"Xiaoyan Yuan","hidden":false},{"_id":"69252c0d16eb3a9f13103a16","name":"Xiaoyue Mi","hidden":false},{"_id":"69252c0d16eb3a9f13103a17","name":"Yepeng Zhang","hidden":false},{"_id":"69252c0d16eb3a9f13103a18","name":"Yifu Sun","hidden":false},{"_id":"69252c0d16eb3a9f13103a19","name":"Yiting Lu","hidden":false},{"_id":"69252c0d16eb3a9f13103a1a","name":"Yitong Li","hidden":false},{"_id":"69252c0d16eb3a9f13103a1b","name":"You Huang","hidden":false},{"_id":"69252c0d16eb3a9f13103a1c","name":"Yu Tang","hidden":false},{"_id":"69252c0d16eb3a9f13103a1d","name":"Yixuan Li","hidden":false},{"_id":"69252c0d16eb3a9f13103a1e","name":"Yuhang Deng","hidden":false},{"_id":"69252c0d16eb3a9f13103a1f","name":"Yuan Zhou","hidden":false},{"_id":"69252c0d16eb3a9f13103a20","name":"Zhichao Hu","hidden":false},{"_id":"69252c0d16eb3a9f13103a21","user":{"_id":"64556769fbe00f9e73c1bddb","avatarUrl":"/avatars/dc0a2d9cf91190d722cb6613072a6556.svg","isPro":false,"fullname":"ZHIGUANG LIU","user":"lz7fdmu","type":"user"},"name":"Zhiguang Liu","status":"admin_assigned","statusLastChangedAt":"2025-11-25T12:23:02.416Z","hidden":false},{"_id":"69252c0d16eb3a9f13103a22","name":"Zhihe Yang","hidden":false},{"_id":"69252c0d16eb3a9f13103a23","name":"Zilin Yang","hidden":false},{"_id":"69252c0d16eb3a9f13103a24","name":"Zhenzhi Lu","hidden":false},{"_id":"69252c0d16eb3a9f13103a25","name":"Zixiang Zhou","hidden":false},{"_id":"69252c0d16eb3a9f13103a26","name":"Zhao Zhong","hidden":false}],"publishedAt":"2025-11-24T08:22:07.000Z","submittedOnDailyAt":"2025-11-25T01:39:58.047Z","title":"HunyuanVideo 1.5 Technical Report","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"We present HunyuanVideo 1.5, a lightweight yet powerful open-source video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture featuring selective and sliding tile attention (SSTA), enhanced bilingual understanding through glyph-aware text encoding, progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions.Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source video generation models. By releasing the code and model weights, we provide the community with a high-performance foundation that lowers the barrier to video creation and research, making advanced video generation accessible to a broader audience. All open-source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.","upvotes":19,"discussionId":"69252c0d16eb3a9f13103a27","ai_summary":"HunyuanVideo 1.5 is a lightweight video generation model with state-of-the-art visual quality and motion coherence, using a DiT architecture with SSTA and an efficient video super-resolution network.","ai_keywords":["DiT architecture","selective and sliding tile attention","SSTA","glyph-aware text encoding","progressive pre-training","post-training","video super-resolution network","text-to-video","image-to-video"]},"publishedAt":"2025-11-24T03:22:07.000Z","title":"HunyuanVideo 1.5 Technical Report","summary":"We present HunyuanVideo 1.5, a lightweight yet powerful open-source video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture featuring selective and sliding tile attention (SSTA), enhanced bilingual understanding through glyph-aware text encoding, progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions.Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source video generation models. By releasing the code and model weights, we provide the community with a high-performance foundation that lowers the barrier to video creation and research, making advanced video generation accessible to a broader audience. All open-source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18870.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":171},"isAuthorParticipating":false},{"paper":{"id":"2511.19428","authors":[{"_id":"69252a5216eb3a9f131039d0","name":"Shangyuan Tong","hidden":false},{"_id":"69252a5216eb3a9f131039d1","name":"Nanye Ma","hidden":false},{"_id":"69252a5216eb3a9f131039d2","name":"Saining Xie","hidden":false},{"_id":"69252a5216eb3a9f131039d3","name":"Tommi Jaakkola","hidden":false}],"publishedAt":"2025-11-24T18:58:55.000Z","submittedOnDailyAt":"2025-11-25T01:32:41.763Z","title":"Flow Map Distillation Without Data","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.","upvotes":2,"discussionId":"69252a5216eb3a9f131039d4","ai_summary":"A data-free framework that samples from the prior distribution surpasses data-based alternatives in flow map distillation, achieving state-of-the-art fidelity with minimal sampling steps.","ai_keywords":["flow models","iterative sampling","flow map distillation","Teacher-Data Mismatch","prior distribution","sampling path","FID","ImageNet"]},"publishedAt":"2025-11-24T13:58:55.000Z","title":"Flow Map Distillation Without Data","summary":"State-of-the-art flow models achieve remarkable quality but require slow, iterative sampling. To accelerate this, flow maps can be distilled from pre-trained teachers, a procedure that conventionally requires sampling from an external dataset. We argue that this data-dependency introduces a fundamental risk of Teacher-Data Mismatch, as a static dataset may provide an incomplete or even misaligned representation of the teacher's full generative capabilities. This leads us to question whether this reliance on data is truly necessary for successful flow map distillation. In this work, we explore a data-free alternative that samples only from the prior distribution, a distribution the teacher is guaranteed to follow by construction, thereby circumventing the mismatch risk entirely. To demonstrate the practical viability of this philosophy, we introduce a principled framework that learns to predict the teacher's sampling path while actively correcting for its own compounding errors to ensure high fidelity. Our approach surpasses all data-based counterparts and establishes a new state-of-the-art by a significant margin. Specifically, distilling from SiT-XL/2+REPA, our method reaches an impressive FID of 1.45 on ImageNet 256x256, and 1.49 on ImageNet 512x512, both with only 1 sampling step. We hope our work establishes a more robust paradigm for accelerating generative models and motivates the broader adoption of flow map distillation without data.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19428.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":171},"isAuthorParticipating":false},{"paper":{"id":"2511.19319","authors":[{"_id":"6925284016eb3a9f131039a9","user":{"_id":"62ebd791fee90fca4742ead8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62ebd791fee90fca4742ead8/8-iJYS9Wk41l6JTGtJrNf.jpeg","isPro":false,"fullname":"levon dang","user":"levondang","type":"user"},"name":"Lingwei Dang","status":"claimed_verified","statusLastChangedAt":"2025-11-25T09:05:15.579Z","hidden":false},{"_id":"6925284016eb3a9f131039aa","name":"Zonghan Li","hidden":false},{"_id":"6925284016eb3a9f131039ab","user":{"_id":"6654763c965ea394ee475878","avatarUrl":"/avatars/92e1eb4c88f209646cc03fc269025f0f.svg","isPro":false,"fullname":"Li Juntong","user":"qzfm","type":"user"},"name":"Juntong Li","status":"admin_assigned","statusLastChangedAt":"2025-11-25T09:36:22.719Z","hidden":false},{"_id":"6925284016eb3a9f131039ac","name":"Hongwen Zhang","hidden":false},{"_id":"6925284016eb3a9f131039ad","name":"Liang An","hidden":false},{"_id":"6925284016eb3a9f131039ae","user":{"_id":"62e14dbe4db2175cd2735a80","avatarUrl":"/avatars/e6385dcedcb97e1b36281b49210321aa.svg","isPro":false,"fullname":"Yebin Liu","user":"YebinLiu","type":"user"},"name":"Yebin Liu","status":"admin_assigned","statusLastChangedAt":"2025-11-25T09:36:04.701Z","hidden":false},{"_id":"6925284016eb3a9f131039af","name":"Qingyao Wu","hidden":false}],"publishedAt":"2025-11-24T17:14:19.000Z","submittedOnDailyAt":"2025-11-25T01:30:42.586Z","title":"SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis","submittedOnDailyBy":{"_id":"62ebd791fee90fca4742ead8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62ebd791fee90fca4742ead8/8-iJYS9Wk41l6JTGtJrNf.jpeg","isPro":false,"fullname":"levon dang","user":"levondang","type":"user"},"summary":"Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.","upvotes":0,"discussionId":"6925284016eb3a9f131039b0","projectPage":"https://droliven.github.io/SyncMV4D/","ai_summary":"SyncMV4D generates realistic and consistent multi-view 3D Hand-Object Interaction videos and 4D motions by integrating visual priors, motion dynamics, and multi-view geometry.","ai_keywords":["Hand-Object Interaction","HOI","SyncMV4D","Multi-view Joint Diffusion","MJD","Diffusion Points Aligner","DPA","diffusion denoising","4D motions","multi-view consistency","visual realism","motion plausibility"]},"publishedAt":"2025-11-24T12:14:19.000Z","title":"SyncMV4D: Synchronized Multi-view Joint Diffusion of Appearance and Motion for Hand-Object Interaction Synthesis","summary":"Hand-Object Interaction (HOI) generation plays a critical role in advancing applications across animation and robotics. Current video-based methods are predominantly single-view, which impedes comprehensive 3D geometry perception and often results in geometric distortions or unrealistic motion patterns. While 3D HOI approaches can generate dynamically plausible motions, their dependence on high-quality 3D data captured in controlled laboratory settings severely limits their generalization to real-world scenarios. To overcome these limitations, we introduce SyncMV4D, the first model that jointly generates synchronized multi-view HOI videos and 4D motions by unifying visual prior, motion dynamics, and multi-view geometry. Our framework features two core innovations: (1) a Multi-view Joint Diffusion (MJD) model that co-generates HOI videos and intermediate motions, and (2) a Diffusion Points Aligner (DPA) that refines the coarse intermediate motion into globally aligned 4D metric point tracks. To tightly couple 2D appearance with 4D dynamics, we establish a closed-loop, mutually enhancing cycle. During the diffusion denoising process, the generated video conditions the refinement of the 4D motion, while the aligned 4D point tracks are reprojected to guide next-step joint generation. Experimentally, our method demonstrates superior performance to state-of-the-art alternatives in visual realism, motion plausibility, and multi-view consistency.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19319.png","numComments":2,"submittedBy":{"_id":"62ebd791fee90fca4742ead8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/62ebd791fee90fca4742ead8/8-iJYS9Wk41l6JTGtJrNf.jpeg","fullname":"levon dang","name":"levondang","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false},"isAuthorParticipating":true},{"paper":{"id":"2511.17792","authors":[{"_id":"692529de16eb3a9f131039be","name":"Dingrui Wang","hidden":false},{"_id":"692529de16eb3a9f131039bf","name":"Hongyuan Ye","hidden":false},{"_id":"692529de16eb3a9f131039c0","name":"Zhihao Liang","hidden":false},{"_id":"692529de16eb3a9f131039c1","name":"Zhexiao Sun","hidden":false},{"_id":"692529de16eb3a9f131039c2","name":"Zhaowei Lu","hidden":false},{"_id":"692529de16eb3a9f131039c3","name":"Yuchen Zhang","hidden":false},{"_id":"692529de16eb3a9f131039c4","name":"Yuyu Zhao","hidden":false},{"_id":"692529de16eb3a9f131039c5","name":"Yuan Gao","hidden":false},{"_id":"692529de16eb3a9f131039c6","name":"Marvin Seegert","hidden":false},{"_id":"692529de16eb3a9f131039c7","name":"Finn Schfer","hidden":false},{"_id":"692529de16eb3a9f131039c8","name":"Haotong Qin","hidden":false},{"_id":"692529de16eb3a9f131039c9","name":"Wei Li","hidden":false},{"_id":"692529de16eb3a9f131039ca","name":"Luigi Palmieri","hidden":false},{"_id":"692529de16eb3a9f131039cb","name":"Felix Jahncke","hidden":false},{"_id":"692529de16eb3a9f131039cc","name":"Mattia Piccinini","hidden":false},{"_id":"692529de16eb3a9f131039cd","name":"Johannes Betz","hidden":false}],"mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Cl6TTW-8t_j4VWw0YNaDD.mp4"],"publishedAt":"2025-11-21T21:36:02.000Z","submittedOnDailyAt":"2025-11-25T01:30:39.799Z","title":"Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified. We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments. Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories. Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics that quantify target-reaching capability, trajectory accuracy, and directional consistency. We evaluate state-of-the-art models including Sora 2, Veo 3.1, and the Wan series. The best off-the-shelf model (Wan2.2-Flash) achieves only 0.299 overall score, revealing significant limitations in current world models for robotic planning tasks. We show that fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score -- an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model. We will open-source the code and dataset.","upvotes":3,"discussionId":"692529de16eb3a9f131039ce","projectPage":"https://target-bench.github.io/","githubRepo":"https://github.com/TUM-AVS/target-bench","ai_summary":"Target-Bench evaluates state-of-the-shelf world models on mapless path planning tasks, showing significant improvement through fine-tuning.","ai_keywords":["world models","Target-Bench","robot-collected video sequences","SLAM-based ground truth trajectories","camera motion recovery","trajectory accuracy","directional consistency","Sora 2","Veo 3.1","Wan series","fine-tuning","5B-parameter model"],"githubStars":2},"publishedAt":"2025-11-21T16:36:02.000Z","title":"Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?","summary":"While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified. We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments. Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories. Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics that quantify target-reaching capability, trajectory accuracy, and directional consistency. We evaluate state-of-the-art models including Sora 2, Veo 3.1, and the Wan series. The best off-the-shelf model (Wan2.2-Flash) achieves only 0.299 overall score, revealing significant limitations in current world models for robotic planning tasks. We show that fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score -- an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model. We will open-source the code and dataset.","mediaUrls":["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Cl6TTW-8t_j4VWw0YNaDD.mp4"],"thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17792.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":171},"isAuthorParticipating":false},{"paper":{"id":"2511.18050","authors":[{"_id":"692525c216eb3a9f1310396f","name":"Tian Ye","hidden":false},{"_id":"692525c216eb3a9f13103970","name":"Song Fei","hidden":false},{"_id":"692525c216eb3a9f13103971","name":"Lei Zhu","hidden":false}],"publishedAt":"2025-11-22T13:07:21.000Z","submittedOnDailyAt":"2025-11-25T01:29:51.780Z","title":"UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios","submittedOnDailyBy":{"_id":"66015e8aa4d296af07de538e","avatarUrl":"/avatars/a1295c631cc2646282c545859975ce4c.svg","isPro":false,"fullname":"Owen","user":"Owen777","type":"user"},"summary":"Diffusion transformers have recently delivered strong text-to-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes a tightly coupled failure mode spanning positional encoding, VAE compression, and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take a data-model co-design view and introduce UltraFlux, a Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, a 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolution- and AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) a simple, non-adversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) a Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield a stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval at 4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics, and-with a LLM prompt refiner-matches or surpasses the proprietary Seedream 4.0.","upvotes":34,"discussionId":"692525c216eb3a9f13103972","projectPage":"https://github.com/W2GenAI-Lab/UltraFlux","githubRepo":"https://github.com/W2GenAI-Lab/UltraFlux","ai_summary":"UltraFlux, a Flux-based DiT trained on a 4K dataset, addresses failures in diffusion transformers at 4K resolution through enhanced positional encoding, improved VAE compression, gradient rebalancing, and aesthetic curriculum learning, achieving superior performance compared to existing models.","ai_keywords":["diffusion transformers","text-to-image generation","4K resolution","positional encoding","VAE compression","Resonance 2D RoPE","YaRN","VAE post-training","SNR-Aware Huber Wavelet","Stage-wise Aesthetic Curriculum Learning","aesthetic evaluation","Aesthetic-Eval","Seedream 4.0"],"githubStars":43,"organization":{"_id":"68b2b2167f881fc640bb9d21","name":"W2GenAI","fullname":"W2GenAI Lab"}},"publishedAt":"2025-11-22T08:07:21.000Z","title":"UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios","summary":"Diffusion transformers have recently delivered strong text-to-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes a tightly coupled failure mode spanning positional encoding, VAE compression, and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take a data-model co-design view and introduce UltraFlux, a Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, a 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolution- and AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) a simple, non-adversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) a Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield a stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval at 4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics, and-with a LLM prompt refiner-matches or surpasses the proprietary Seedream 4.0.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18050.png","numComments":1,"submittedBy":{"_id":"66015e8aa4d296af07de538e","avatarUrl":"/avatars/a1295c631cc2646282c545859975ce4c.svg","fullname":"Owen","name":"Owen777","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":39},"organization":{"_id":"68b2b2167f881fc640bb9d21","name":"W2GenAI","fullname":"W2GenAI Lab"},"isAuthorParticipating":false},{"paper":{"id":"2511.17986","authors":[{"_id":"6925296c16eb3a9f131039b2","name":"Lun Huang","hidden":false},{"_id":"6925296c16eb3a9f131039b3","name":"You Xie","hidden":false},{"_id":"6925296c16eb3a9f131039b4","name":"Hongyi Xu","hidden":false},{"_id":"6925296c16eb3a9f131039b5","name":"Tianpei Gu","hidden":false},{"_id":"6925296c16eb3a9f131039b6","name":"Chenxu Zhang","hidden":false},{"_id":"6925296c16eb3a9f131039b7","name":"Guoxian Song","hidden":false},{"_id":"6925296c16eb3a9f131039b8","name":"Zenan Li","hidden":false},{"_id":"6925296c16eb3a9f131039b9","name":"Xiaochen Zhao","hidden":false},{"_id":"6925296c16eb3a9f131039ba","name":"Linjie Luo","hidden":false},{"_id":"6925296c16eb3a9f131039bb","name":"Guillermo Sapiro","hidden":false}],"publishedAt":"2025-11-22T08:59:09.000Z","submittedOnDailyAt":"2025-11-25T01:28:44.671Z","title":"Plan-X: Instruct Video Generation via Semantic Planning","submittedOnDailyBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","isPro":true,"fullname":"taesiri","user":"taesiri","type":"user"},"summary":"Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner, a learnable multimodal language model that reasons over the user's intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to high-level text prompt guidance, serve as structured \"semantic sketches\" over time for the video diffusion model, which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.","upvotes":16,"discussionId":"6925296c16eb3a9f131039bc","projectPage":"https://byteaigc.github.io/Plan-X/","ai_summary":"Plan-X integrates a Semantic Planner and diffusion models to reduce visual hallucinations and improve instruction-aligned video generation by using multimodal semantic tokens.","ai_keywords":["Diffusion Transformers","visual synthesis","high-level semantic reasoning","long-horizon planning","visual hallucinations","Semantic Planner","multimodal language model","text-grounded spatio-temporal semantic tokens","video diffusion model","photorealistic video synthesis"]},"publishedAt":"2025-11-22T03:59:09.000Z","title":"Plan-X: Instruct Video Generation via Semantic Planning","summary":"Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner, a learnable multimodal language model that reasons over the user's intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to high-level text prompt guidance, serve as structured \"semantic sketches\" over time for the video diffusion model, which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.17986.png","numComments":1,"submittedBy":{"_id":"6039478ab3ecf716b1a5fd4d","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg","fullname":"taesiri","name":"taesiri","type":"user","isPro":true,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":171},"isAuthorParticipating":false},{"paper":{"id":"2511.19304","authors":[{"_id":"6925274e16eb3a9f13103998","user":{"_id":"65f40e83653c231cbaf7defe","avatarUrl":"/avatars/afa5ce72324112739e539865c9aee26b.svg","isPro":false,"fullname":"Jiayi Zhang","user":"didiforhugface","type":"user"},"name":"Jiayi Zhang","status":"claimed_verified","statusLastChangedAt":"2025-11-25T09:05:29.887Z","hidden":false},{"_id":"6925274e16eb3a9f13103999","name":"Yiran Peng","hidden":false},{"_id":"6925274e16eb3a9f1310399a","user":{"_id":"6621e02cf34ab6caed18e9c6","avatarUrl":"/avatars/15888b2060d1cc56be9fa55fd4b34005.svg","isPro":false,"fullname":"Fanqi Kong","user":"Fancylalala","type":"user"},"name":"Fanqi Kong","status":"claimed_verified","statusLastChangedAt":"2025-11-25T09:05:25.655Z","hidden":false},{"_id":"6925274e16eb3a9f1310399b","user":{"_id":"67c443afb753bd020f9c97d8","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xbACBNLSopWmN5G1K8h_Y.png","isPro":false,"fullname":"Cheng","user":"YangC777","type":"user"},"name":"Yang Cheng","status":"claimed_verified","statusLastChangedAt":"2025-11-25T09:05:18.820Z","hidden":false},{"_id":"6925274e16eb3a9f1310399c","name":"Yifan Wu","hidden":false},{"_id":"6925274e16eb3a9f1310399d","name":"Zhaoyang Yu","hidden":false},{"_id":"6925274e16eb3a9f1310399e","user":{"_id":"649ea7106282cb41e77760bc","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/649ea7106282cb41e77760bc/HlWjaqxr03ob93vdKg_LQ.jpeg","isPro":false,"fullname":"Isaac","user":"XiangJinYu","type":"user"},"name":"Jinyu Xiang","status":"claimed_verified","statusLastChangedAt":"2025-11-25T09:05:23.607Z","hidden":false},{"_id":"6925274e16eb3a9f1310399f","user":{"_id":"68a435cc22fdf7356962ccb9","avatarUrl":"/avatars/467f4732ade5f47b42433ff354acdeef.svg","isPro":false,"fullname":"jianhao ruan","user":"Aurorra1123","type":"user"},"name":"Jianhao Ruan","status":"claimed_verified","statusLastChangedAt":"2025-11-25T15:53:17.306Z","hidden":false},{"_id":"6925274e16eb3a9f131039a0","name":"Jinlin Wang","hidden":false},{"_id":"6925274e16eb3a9f131039a1","name":"Maojia Song","hidden":false},{"_id":"6925274e16eb3a9f131039a2","user":{"_id":"6632160088f75d987d1a156f","avatarUrl":"https://cdn-avatars.huggingface.co/v1/production/uploads/6632160088f75d987d1a156f/mYlMQfK1BGWeEbOSMmeSb.jpeg","isPro":false,"fullname":"Hongzhang Liu","user":"Alphamasterliu","type":"user"},"name":"HongZhang Liu","status":"claimed_verified","statusLastChangedAt":"2025-11-25T09:05:27.575Z","hidden":false},{"_id":"6925274e16eb3a9f131039a3","name":"Xiangru Tang","hidden":false},{"_id":"6925274e16eb3a9f131039a4","name":"Bang Liu","hidden":false},{"_id":"6925274e16eb3a9f131039a5","name":"Chenglin Wu","hidden":false},{"_id":"6925274e16eb3a9f131039a6","name":"Yuyu Luo","hidden":false}],"publishedAt":"2025-11-24T16:54:23.000Z","submittedOnDailyAt":"2025-11-25T01:26:13.029Z","title":"AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning","submittedOnDailyBy":{"_id":"65f40e83653c231cbaf7defe","avatarUrl":"/avatars/afa5ce72324112739e539865c9aee26b.svg","isPro":false,"fullname":"Jiayi Zhang","user":"didiforhugface","type":"user"},"summary":"Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.","upvotes":82,"discussionId":"6925274f16eb3a9f131039a7","ai_summary":"AutoEnv and AutoEnv-36 provide a standardized framework and dataset for evaluating cross-environment learning in agents, highlighting the challenges and limitations of existing learning methods.","ai_keywords":["AutoEnv","factorizable distributions","heterogeneous environments","AutoEnv-36","language models","normalized reward","component-centric process","Selection","Optimization","Evaluation","learning methods","environment-adaptive selection"]},"publishedAt":"2025-11-24T11:54:23.000Z","title":"AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning","summary":"Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.","thumbnail":"https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19304.png","numComments":2,"submittedBy":{"_id":"65f40e83653c231cbaf7defe","avatarUrl":"/avatars/afa5ce72324112739e539865c9aee26b.svg","fullname":"Jiayi Zhang","name":"didiforhugface","type":"user","isPro":false,"isHf":false,"isHfAdmin":false,"isMod":false,"followerCount":17},"isAuthorParticipating":true}]
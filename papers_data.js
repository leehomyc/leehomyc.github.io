window.trendingPapers = {
    "today": [{"paper": {"id": "2601.17058", "authors": [{"_id": "69782c96026bdf0473116e06", "user": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "name": "Wei Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:49.701Z", "hidden": false}, {"_id": "69782c96026bdf0473116e07", "name": "Jun Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e08", "name": "Haoyu Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e09", "name": "Zhenghao Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0a", "name": "Qikang He", "hidden": false}, {"_id": "69782c96026bdf0473116e0b", "name": "Shaokun Han", "hidden": false}, {"_id": "69782c96026bdf0473116e0c", "name": "Guoliang Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0d", "user": {"_id": "64ef522242da8d2a897d62da", "avatarUrl": "/avatars/03611010d247da66696ac8976d4d3ed3.svg", "isPro": false, "fullname": "xuanhe zhou", "user": "zhouxh19", "type": "user"}, "name": "Xuanhe Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:19.930Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0e", "user": {"_id": "674fa2f067c963c50a066594", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674fa2f067c963c50a066594/hKZ46Mwm_UEguzBt63ys_.jpeg", "isPro": false, "fullname": "yeye he", "user": "yeyehe", "type": "user"}, "name": "Yeye He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:27.638Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0f", "name": "Chunwei Liu", "hidden": false}, {"_id": "69782c96026bdf0473116e10", "user": {"_id": "66724ce47e7ff5d8bd069c7c", "avatarUrl": "/avatars/953f66585390dbdb202c1d7b7250d7bd.svg", "isPro": false, "fullname": "Zirui Tang", "user": "TerryTang", "type": "user"}, "name": "Zirui Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:49.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e11", "name": "Bin Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e12", "user": {"_id": "695612aabf3c8959a3a05f9c", "avatarUrl": "/avatars/c18885f6dea6f3ee019405cd8cf6f484.svg", "isPro": false, "fullname": "ShenTang990", "user": "shentang", "type": "user"}, "name": "Shen Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:56.579Z", "hidden": false}, {"_id": "69782c96026bdf0473116e13", "name": "Kai Zuo", "hidden": false}, {"_id": "69782c96026bdf0473116e14", "user": {"_id": "67efa8a2ed790a2e999dc216", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0S4lQCJX61uCF8EkSLMkk.png", "isPro": false, "fullname": "Yuyu Luo", "user": "luoyuyu", "type": "user"}, "name": "Yuyu Luo", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:02.233Z", "hidden": false}, {"_id": "69782c96026bdf0473116e15", "name": "Zhenzhe Zheng", "hidden": false}, {"_id": "69782c96026bdf0473116e16", "user": {"_id": "63f9fca8d4349b157a109eec", "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg", "isPro": false, "fullname": "Conghui He", "user": "conghui", "type": "user"}, "name": "Conghui He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:57:14.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e17", "name": "Jingren Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e18", "name": "Fan Wu", "hidden": false}], "publishedAt": "2026-01-22T12:02:45.000Z", "submittedOnDailyAt": "2026-01-27T00:42:38.464Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "submittedOnDailyBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "upvotes": 127, "discussionId": "69782c97026bdf0473116e19", "projectPage": "https://github.com/weAIDB/awesome-data-llm", "githubRepo": "https://github.com/weAIDB/awesome-data-llm", "githubRepoAddedBy": "user", "ai_summary": "LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.", "ai_keywords": ["data preparation", "large language models", "prompt-driven workflows", "agentic workflows", "data cleaning", "data integration", "data enrichment", "entity matching", "schema matching", "data annotation", "data profiling"], "githubStars": 644, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u6570\u636e\u51c6\u5907\u7684\u76ee\u7684\u662f\u53bb\u566a\u58f0\u3001\u53d1\u73b0\u8de8\u6570\u636e\u96c6\u7684\u5173\u7cfb\u548c\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u5bf9\u6570\u636e\u9a71\u52a8\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>\u7531\u4e8e\u5bf9\u5e94\u7528\u51c6\u5907\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\u548c\u5f3a\u5927\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6280\u672f\u7684\u51fa\u73b0\uff0cLLM\u589e\u5f3a\u7684\u65b9\u6cd5\u6b63\u5728\u6210\u4e3a\u6570\u636e\u51c6\u5907\u7684\u4e3b\u6d41\u3002</li>\n    <li>\u672c\u6587\u5bf9LLM\u6280\u672f\u5728\u6570\u636e\u51c6\u5907\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u5ba1\uff0c\u5173\u6ce8\u6570\u636e\u6e05\u7406\u3001\u6570\u636e\u6574\u5408\u548c\u6570\u636e\u4e30\u5bcc\u7b49\u4e3b\u8981\u4efb\u52a1\u3002</li>\n    <li>\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u8c03\u67e5\u4e86\u4ee3\u8868\u6027\u6280\u672f\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u4f18\u7f3a\u70b9\uff0c\u5982\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u4f46\u4e5f\u5b58\u5728\u6210\u672c\u9ad8\u548c\u8bc4\u4f30\u4e0d\u5339\u914d\u7b49\u95ee\u9898\u3002</li>\n    <li>\u8ba8\u8bba\u4e86\u5f00\u653e\u7684\u7814\u7a76\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u524d\u77bb\u6027\u8def\u7ebf\u56fe\uff0c\u5f3a\u8c03\u53ef\u6269\u5c55\u7684LLM-\u6570\u636e\u7cfb\u7edf\u548c\u53ef\u9760\u7684\u5de5\u4f5c\u6d41\u7a0b\u8bbe\u8ba1\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Data preparation is important for cleaning datasets and finding useful information for various applications like analytics and decision-making.</li>\n    <li>Recent advances in large language models (LLMs) and flexible data infrastructures are changing how data is prepared, moving from traditional methods to new, more adaptable approaches.</li>\n    <li>This paper reviews the use of LLM techniques for data preparation, focusing on three main tasks: data cleaning, data integration, and data enrichment.</li>\n    <li>It discusses the benefits and drawbacks of different methods, such as better understanding of data but challenges like high costs and issues with accuracy.</li>\n    <li>The paper also highlights ongoing challenges in the field and suggests future directions for improving data preparation systems with LLMs.</li>\n</ul>"}, "publishedAt": "2026-01-22T07:02:45.000Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17058.png", "numComments": 2, "submittedBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "fullname": "Wei Zhou", "name": "weizhoudb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.18418", "authors": [{"_id": "69785315026bdf0473116f6a", "user": {"_id": "62dce08bb2c60f29c3d0a5da", "avatarUrl": "/avatars/87ce03e61c4c6eb686c9491ef4fda225.svg", "isPro": false, "fullname": "Ji Zeng", "user": "stargazerzj", "type": "user"}, "name": "Ji Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T08:31:51.245Z", "hidden": false}, {"_id": "69785315026bdf0473116f6b", "name": "Dayuan Fu", "hidden": false}, {"_id": "69785315026bdf0473116f6c", "name": "Tiantian Mi", "hidden": false}, {"_id": "69785315026bdf0473116f6d", "name": "Yumin Zhuang", "hidden": false}, {"_id": "69785315026bdf0473116f6e", "user": {"_id": "6865e6b362fc5689c5e67733", "avatarUrl": "/avatars/186f3d248791d961b0a810d5225167cc.svg", "isPro": false, "fullname": "Yaxing Huang", "user": "Rookie-Noob-Newbie", "type": "user"}, "name": "Yaxing Huang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:30.220Z", "hidden": false}, {"_id": "69785315026bdf0473116f6f", "user": {"_id": "67638cc0d63e4b348e8a5fa3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67638cc0d63e4b348e8a5fa3/BZNlw1uTGUcumCrXKkerx.png", "isPro": false, "fullname": "Xuefeng Li", "user": "drxuefeng", "type": "user"}, "name": "Xuefeng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:37.248Z", "hidden": false}, {"_id": "69785315026bdf0473116f70", "name": "Lyumanshan Ye", "hidden": false}, {"_id": "69785315026bdf0473116f71", "name": "Muhang Xie", "hidden": false}, {"_id": "69785315026bdf0473116f72", "name": "Qishuo Hua", "hidden": false}, {"_id": "69785315026bdf0473116f73", "name": "Zhen Huang", "hidden": false}, {"_id": "69785315026bdf0473116f74", "user": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "isPro": false, "fullname": "Mohan Jiang (SII)", "user": "mhjiang0408", "type": "user"}, "name": "Mohan Jiang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:23.438Z", "hidden": false}, {"_id": "69785315026bdf0473116f75", "name": "Hanning Wang", "hidden": false}, {"_id": "69785315026bdf0473116f76", "user": {"_id": "66fa544c54f87b607fbffd2e", "avatarUrl": "/avatars/94195dcda0eb68e8fd20d80718744697.svg", "isPro": false, "fullname": "Jifan Lin", "user": "evanlin2570", "type": "user"}, "name": "Jifan Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:57.029Z", "hidden": false}, {"_id": "69785315026bdf0473116f77", "name": "Yang Xiao", "hidden": false}, {"_id": "69785315026bdf0473116f78", "name": "Jie Sun", "hidden": false}, {"_id": "69785315026bdf0473116f79", "user": {"_id": "684faf712acd915b5afc055f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/684faf712acd915b5afc055f/K7icmL08HxniWDgdph73i.jpeg", "isPro": false, "fullname": "Yunze Wu", "user": "wyzmike", "type": "user"}, "name": "Yunze Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:01:06.063Z", "hidden": false}, {"_id": "69785315026bdf0473116f7a", "name": "Pengfei Liu", "hidden": false}], "publishedAt": "2026-01-26T12:20:18.000Z", "submittedOnDailyAt": "2026-01-27T03:34:37.777Z", "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "submittedOnDailyBy": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "isPro": false, "fullname": "Mohan Jiang (SII)", "user": "mhjiang0408", "type": "user"}, "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...", "upvotes": 104, "discussionId": "69785315026bdf0473116f7b", "githubRepo": "https://github.com/GAIR-NLP/daVinci-Dev", "githubRepoAddedBy": "user", "ai_summary": "Agentic mid-training enables large language models to develop autonomous software engineering capabilities through specialized data synthesis techniques that bridge the gap between static training data and dynamic development environments.", "ai_keywords": ["Large Language Model", "agentic software engineering", "mid-training", "distribution mismatch", "agent-native data", "contextually-native trajectories", "environmentally-native trajectories", "SWE-Bench Verified", "Kimi-Dev", "resolution rates"], "githubStars": 22, "organization": {"_id": "630bc2d186b8b9904c33ce1b", "name": "GAIR", "fullname": "SII - GAIR", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u80fd\u529b\u6b63\u5728\u4ece\u5355\u6b21\u4ee3\u7801\u751f\u6210\u8f6c\u5411\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\u3002</li>\n    <li>\u4e2d\u671f\u8bad\u7ec3\uff08MT\uff09\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u8fdb\u884c\uff0c\u4ee5\u652f\u6301\u8fd9\u4e00\u81ea\u4e3b\u5f00\u53d1\u8fc7\u7a0b\uff0c\u4f46\u76ee\u524d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002</li>\n    <li>\u6709\u6548\u7684\u4e2d\u671f\u8bad\u7ec3\u9762\u4e34\u9759\u6001\u8bad\u7ec3\u6570\u636e\u4e0e\u52a8\u6001\u5f00\u53d1\u73af\u5883\u4e4b\u95f4\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u7684\u4e2d\u671f\u8bad\u7ec3\u7814\u7a76\uff0c\u5efa\u7acb\u4e86\u6570\u636e\u5408\u6210\u539f\u5219\u548c\u8bad\u7ec3\u65b9\u6cd5\u3002</li>\n    <li>\u6a21\u578b\u5728`SWE-Bench Verified`\u4e0a\u9a8c\u8bc1\u4e86\u5176\u81ea\u4e3b\u80fd\u529b\uff0c\u5e76\u5728\u4e2d\u671f\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are evolving from simple code generation to more complex software engineering tasks where they can autonomously navigate and edit code.</li>\n    <li>Mid-training on large-scale data that simulates real-world software development is an underused method that could help develop these agentic behaviors more efficiently than traditional methods.</li>\n    <li>A key challenge is the difference between static training data and the dynamic environments where software is actually developed.</li>\n    <li>This study introduces a new method for agentic mid-training, focusing on two types of data: contextually-native (covering diverse experiences) and environmentally-native (from real tool usage and tests).</li>\n    <li>The results show that our models outperform previous methods with fewer tokens used, achieving notable resolution rates in software engineering tasks.</li>\n</ul>"}, "publishedAt": "2026-01-26T07:20:18.000Z", "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18418.png", "numComments": 2, "submittedBy": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "fullname": "Mohan Jiang (SII)", "name": "mhjiang0408", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "630bc2d186b8b9904c33ce1b", "name": "GAIR", "fullname": "SII - GAIR", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.17737", "authors": [{"_id": "6978310b026bdf0473116e44", "user": {"_id": "64545c77a7ce0a8fde809912", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VDaMEM77Xv09dP6B5v3sK.jpeg", "isPro": false, "fullname": "ChenYuMu", "user": "ChenYuMu", "type": "user"}, "name": "Chenyu Mu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:01:21.462Z", "hidden": false}, {"_id": "6978310b026bdf0473116e45", "user": {"_id": "6527a2df1eb78901534b0cc6", "avatarUrl": "/avatars/f811d8c108930b41e2612c609d35e2eb.svg", "isPro": false, "fullname": "Xin He", "user": "Kleinhe", "type": "user"}, "name": "Xin He", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:03:20.414Z", "hidden": false}, {"_id": "6978310b026bdf0473116e46", "user": {"_id": "64300415b009240418dac70c", "avatarUrl": "/avatars/5175cdbc7683b0b52d5c742e93d3be83.svg", "isPro": false, "fullname": "Qu Yang", "user": "quyang22", "type": "user"}, "name": "Qu Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:03:22.475Z", "hidden": false}, {"_id": "6978310b026bdf0473116e47", "name": "Wanshun Chen", "hidden": false}, {"_id": "6978310b026bdf0473116e48", "name": "Jiadi Yao", "hidden": false}, {"_id": "6978310b026bdf0473116e49", "name": "Huang Liu", "hidden": false}, {"_id": "6978310b026bdf0473116e4a", "name": "Zihao Yi", "hidden": false}, {"_id": "6978310b026bdf0473116e4b", "name": "Bo Zhao", "hidden": false}, {"_id": "6978310b026bdf0473116e4c", "name": "Xingyu Chen", "hidden": false}, {"_id": "6978310b026bdf0473116e4d", "name": "Ruotian Ma", "hidden": false}, {"_id": "6978310b026bdf0473116e4e", "name": "Fanghua Ye", "hidden": false}, {"_id": "6978310b026bdf0473116e4f", "name": "Erkun Yang", "hidden": false}, {"_id": "6978310b026bdf0473116e50", "name": "Cheng Deng", "hidden": false}, {"_id": "6978310b026bdf0473116e51", "name": "Zhaopeng Tu", "hidden": false}, {"_id": "6978310b026bdf0473116e52", "name": "Xiaolong Li", "hidden": false}, {"_id": "6978310b026bdf0473116e53", "name": "Linus", "hidden": false}], "publishedAt": "2026-01-25T08:10:28.000Z", "submittedOnDailyAt": "2026-01-27T01:05:46.612Z", "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation", "submittedOnDailyBy": {"_id": "67485743561b1e6f9579389f", "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg", "isPro": false, "fullname": "Zhaopeng Tu", "user": "zptu", "type": "user"}, "summary": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.", "upvotes": 46, "discussionId": "6978310b026bdf0473116e54", "projectPage": "https://xd-mu.github.io/ScriptIsAllYouNeed/", "githubRepo": "https://github.com/Tencent/digitalhuman/tree/main/ScriptAgent", "githubRepoAddedBy": "user", "ai_summary": "A novel end-to-end agentic framework translates dialogue into cinematic videos through specialized agents that generate and orchestrate video content while maintaining narrative coherence.", "ai_keywords": ["video generation", "dialogue-to-cinematic-video", "ScripterAgent", "DirectorAgent", "cross-scene continuous generation", "ScriptBench", "Visual-Script Alignment", "CriticAgent"], "githubStars": 228, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "summary_zh": "<ul>\n    <li>\u6700\u65b0\u7684\u89c6\u9891\u751f\u6210\u6280\u672f\u53ef\u4ee5\u6839\u636e\u7b80\u5355\u7684\u6587\u5b57\u63d0\u793a\u751f\u6210\u7cbe\u5f69\u7684\u89c6\u89c9\u5185\u5bb9\uff0c\u4f46\u5728\u751f\u6210\u8fde\u8d2f\u7684\u957f\u7bc7\u5bf9\u8bdd\u65f6\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u8bdd\u5230\u7535\u5f71\u89c6\u9891\u751f\u6210\u7684\u6846\u67b6\uff0c\u540d\u4e3aScripterAgent\u3002</li>\n    <li>ScripterAgent\u80fd\u591f\u5c06\u7c97\u7565\u7684\u5bf9\u8bdd\u8f6c\u5316\u4e3a\u8be6\u7ec6\u7684\u53ef\u6267\u884c\u7535\u5f71\u5267\u672c\u3002</li>\n    <li>\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6ScriptBench\uff0c\u63d0\u4f9b\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\uff0c\u4ee5\u652f\u6301\u5267\u672c\u751f\u6210\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u5267\u672c\u7684\u771f\u5b9e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u89c6\u89c9\u6548\u679c\u4e0e\u5267\u672c\u9075\u5faa\u4e4b\u95f4\u7684\u6743\u8861\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New models can create impressive videos from text, but struggle with long, coherent stories.</li>\n    <li>We propose a new system to improve how dialogue turns into cinematic videos.</li>\n    <li>The system includes ScripterAgent, which converts rough dialogue into detailed scripts.</li>\n    <li>We created ScriptBench, a large set of data to help train and evaluate the model.</li>\n    <li>Our evaluation shows better script accuracy and video continuity, highlighting trade-offs in current video models.</li>\n</ul>"}, "publishedAt": "2026-01-25T03:10:28.000Z", "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation", "summary": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17737.png", "numComments": 3, "submittedBy": {"_id": "67485743561b1e6f9579389f", "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg", "fullname": "Zhaopeng Tu", "name": "zptu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.17027", "authors": [{"_id": "69782c6d026bdf0473116dfa", "user": {"_id": "640d99628512ec51d7ef71c7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d99628512ec51d7ef71c7/fcBkqnxfxuuuZTqfN_BGy.jpeg", "isPro": false, "fullname": "Honglin Lin", "user": "LHL3341", "type": "user"}, "name": "Honglin Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T13:56:57.457Z", "hidden": false}, {"_id": "69782c6d026bdf0473116dfb", "user": {"_id": "67b30bb2c2e25cfcdeda4a2f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67b30bb2c2e25cfcdeda4a2f/K5ePD5uWNkwlpkI_43Oe1.jpeg", "isPro": false, "fullname": "Qin, Chonghan", "user": "J017athan", "type": "user"}, "name": "Chonghan Qin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:42:44.133Z", "hidden": false}, {"_id": "69782c6d026bdf0473116dfc", "user": {"_id": "6625ef13605f46d05c1d0031", "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg", "isPro": false, "fullname": "Zheng Liu", "user": "starriver030515", "type": "user"}, "name": "Zheng Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:42:39.432Z", "hidden": false}, {"_id": "69782c6d026bdf0473116dfd", "name": "Qizhi Pei", "hidden": false}, {"_id": "69782c6d026bdf0473116dfe", "name": "Yu Li", "hidden": false}, {"_id": "69782c6d026bdf0473116dff", "user": {"_id": "6875f5b55096cad81398a5af", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6875f5b55096cad81398a5af/CwXl53Jdp3LsBuRudT_CM.jpeg", "isPro": false, "fullname": "Zhanping Zhong", "user": "ChampionZhong", "type": "user"}, "name": "Zhanping Zhong", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:42:41.958Z", "hidden": false}, {"_id": "69782c6d026bdf0473116e00", "name": "Xin Gao", "hidden": false}, {"_id": "69782c6d026bdf0473116e01", "name": "Yanfeng Wang", "hidden": false}, {"_id": "69782c6d026bdf0473116e02", "name": "Conghui He", "hidden": false}, {"_id": "69782c6d026bdf0473116e03", "name": "Lijun Wu", "hidden": false}], "publishedAt": "2026-01-17T14:18:36.000Z", "submittedOnDailyAt": "2026-01-27T00:43:13.802Z", "title": "Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility", "submittedOnDailyBy": {"_id": "640d99628512ec51d7ef71c7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d99628512ec51d7ef71c7/fcBkqnxfxuuuZTqfN_BGy.jpeg", "isPro": false, "fullname": "Honglin Lin", "user": "LHL3341", "type": "user"}, "summary": "While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit \"understand - plan - code\" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.", "upvotes": 34, "discussionId": "69782c6e026bdf0473116e04", "projectPage": "https://scigenbench.github.io/", "githubRepo": "https://github.com/SciGenBench/SciGenBench", "githubRepoAddedBy": "user", "ai_summary": "Scientific image synthesis using logic-driven frameworks like ImgCoder improves multimodal reasoning by addressing visual-logic divergence through structured generation and evaluation benchmarks.", "ai_keywords": ["Text-to-Image models", "Large Multimodal Models", "scientific correctness", "visual-logic divergence", "ImgCoder", "SciGenBench", "pixel-based generation", "programmatic synthesis", "logical validity", "information utility"], "githubStars": 9, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u5408\u6210\u6570\u636e\u5728\u6587\u672c\u9886\u57df\u6709\u6548\u63d0\u5347\u79d1\u5b66\u63a8\u7406\uff0c\u4f46\u591a\u6a21\u6001\u63a8\u7406\u4ecd\u53d7\u9650\u4e8e\u79d1\u5b66\u56fe\u50cf\u5408\u6210\u7684\u96be\u5ea6\u3002</li>\n    <li>\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5e38\u5e38\u751f\u6210\u89c6\u89c9\u4e0a\u5408\u7406\u4f46\u79d1\u5b66\u4e0a\u4e0d\u6b63\u786e\u7684\u56fe\u50cf\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5b9e\u9645\u5e94\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86 ImgCoder\uff0c\u4e00\u4e2a\u57fa\u4e8e\u903b\u8f91\u7684\u6846\u67b6\uff0c\u91c7\u7528\u201c\u7406\u89e3 - \u8ba1\u5212 - \u7f16\u7801\u201d\u7684\u5de5\u4f5c\u6d41\u7a0b\u6765\u63d0\u9ad8\u7ed3\u6784\u7cbe\u5ea6\u3002</li>\n    <li>\u5f15\u5165 SciGenBench \u6765\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u7684\u4fe1\u606f\u6548\u7528\u548c\u903b\u8f91\u6709\u6548\u6027\uff0c\u53d1\u73b0\u50cf\u7d20\u6a21\u578b\u7684\u7cfb\u7edf\u6027\u5931\u8d25\u6a21\u5f0f\u3002</li>\n    <li>\u5bf9\u7ecf\u8fc7\u4e25\u683c\u9a8c\u8bc1\u7684\u5408\u6210\u79d1\u5b66\u56fe\u50cf\u8fdb\u884c\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\u7684\u5fae\u8c03\uff0c\u80fd\u663e\u8457\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u663e\u793a\u51fa\u9ad8\u4fdd\u771f\u79d1\u5b66\u5408\u6210\u7684\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Synthetic data helps improve scientific reasoning in text, but creating accurate scientific images is challenging.</li>\n    <li>Current Text-to-Image models often look good but can be scientifically incorrect, which limits their usefulness.</li>\n    <li>The study introduces ImgCoder, a new framework to create more accurate scientific images by focusing on logic and structure.</li>\n    <li>They also created SciGenBench to evaluate the quality and accuracy of generated images.</li>\n    <li>Fine-tuning large multimodal models with accurate synthetic images improves reasoning capabilities, similar to advancements in text generation.</li>\n</ul>"}, "publishedAt": "2026-01-17T09:18:36.000Z", "title": "Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility", "summary": "While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit \"understand - plan - code\" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17027.png", "numComments": 2, "submittedBy": {"_id": "640d99628512ec51d7ef71c7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d99628512ec51d7ef71c7/fcBkqnxfxuuuZTqfN_BGy.jpeg", "fullname": "Honglin Lin", "name": "LHL3341", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.17367", "authors": [{"_id": "6978295b026bdf0473116db5", "user": {"_id": "64096ef79e9f790c905b846d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg", "isPro": false, "fullname": "Zecheng Tang", "user": "ZetangForward", "type": "user"}, "name": "Zecheng Tang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:42:48.253Z", "hidden": false}, {"_id": "6978295b026bdf0473116db6", "user": {"_id": "6732fb1d24b316be87acaafe", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6732fb1d24b316be87acaafe/BzD8HL4vhh3mfeSF3rm_1.jpeg", "isPro": false, "fullname": "Quantong Qiu", "user": "QQTang1223", "type": "user"}, "name": "Quantong Qiu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:27:08.590Z", "hidden": false}, {"_id": "6978295b026bdf0473116db7", "name": "Yi Yang", "hidden": false}, {"_id": "6978295b026bdf0473116db8", "name": "Zhiyi Hong", "hidden": false}, {"_id": "6978295b026bdf0473116db9", "name": "Haiya Xiang", "hidden": false}, {"_id": "6978295b026bdf0473116dba", "user": {"_id": "6875e95998c692330d72dc9a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/wY7yjIix_IptruwlcG5Q4.png", "isPro": false, "fullname": "Kebin Liu", "user": "KebinLiu", "type": "user"}, "name": "Kebin Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:27:45.073Z", "hidden": false}, {"_id": "6978295b026bdf0473116dbb", "user": {"_id": "68cce9276e4618473d590342", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/UqBWbWZ58EPwJ5iRUsJsB.png", "isPro": false, "fullname": "Qingqing Dang", "user": "DaisyGrace", "type": "user"}, "name": "Qingqing Dang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:27:50.734Z", "hidden": false}, {"_id": "6978295b026bdf0473116dbc", "user": {"_id": "6670e285b0c03c4e9d6e0985", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6670e285b0c03c4e9d6e0985/j9Zr-lOtrRmpFz5f4x420.jpeg", "isPro": false, "fullname": "Juntao Li", "user": "douvleplus", "type": "user"}, "name": "Juntao Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:27:56.908Z", "hidden": false}, {"_id": "6978295b026bdf0473116dbd", "name": "Min Zhang", "hidden": false}], "publishedAt": "2026-01-24T08:22:07.000Z", "submittedOnDailyAt": "2026-01-27T00:51:37.565Z", "title": "Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers", "submittedOnDailyBy": {"_id": "64096ef79e9f790c905b846d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg", "isPro": false, "fullname": "Zecheng Tang", "user": "ZetangForward", "type": "user"}, "summary": "The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.", "upvotes": 26, "discussionId": "6978295b026bdf0473116dbe", "projectPage": "https://github.com/LCM-Lab/Elastic-Attention", "githubRepo": "https://github.com/LCM-Lab/Elastic-Attention", "githubRepoAddedBy": "user", "ai_summary": "Elastic Attention enables dynamic adjustment of attention sparsity during inference by integrating a lightweight Attention Router into pretrained models, achieving efficient long-context processing.", "ai_keywords": ["standard attention mechanisms", "sparse attention", "full attention", "hybrid attention strategies", "attention router", "attention heads", "long-context scenarios", "pretrained models", "computational efficiency"], "githubStars": 11, "organization": {"_id": "61f8e653129c9ff1b911293d", "name": "SUDA", "fullname": "Soochow University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643701821817-61f8e5934a8e5a275b2b3e5a.png"}, "summary_zh": "<ul>\n    <li>\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u5b58\u5728\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u5f71\u54cd\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6269\u5c55\u6027\u3002</li>\n    <li>\u6df7\u5408\u6ce8\u610f\u529b\u7b56\u7565\u7ed3\u5408\u7a00\u758f\u548c\u5168\u6ce8\u610f\u529b\uff0c\u4f46\u901a\u5e38\u4f7f\u7528\u56fa\u5b9a\u7684\u8ba1\u7b97\u6bd4\u4f8b\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u7684\u9700\u6c42\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u5f39\u6027\u6ce8\u610f\u529b\uff0c\u6a21\u578b\u53ef\u4ee5\u6839\u636e\u8f93\u5165\u52a8\u6001\u8c03\u6574\u7a00\u758f\u6027\u3002</li>\n    <li>\u901a\u8fc7\u96c6\u6210\u8f7b\u91cf\u7ea7\u7684\u6ce8\u610f\u529b\u8def\u7531\u5668\uff0c\u52a8\u6001\u5206\u914d\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u7684\u8ba1\u7b97\u6a21\u5f0f\u3002</li>\n    <li>\u7ecf\u8fc712\u5c0f\u65f6\u7684\u8bad\u7ec3\uff0c\u6a21\u578b\u5728\u5f3a\u6027\u80fd\u548c\u9ad8\u6548\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u7684\u4f18\u52bf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Standard attention mechanisms are slow and inefficient for large language models when processing long texts.</li>\n    <li>Hybrid attention combines sparse and full attention, but usually uses fixed ratios and cannot adapt to different tasks.</li>\n    <li>Elastic Attention solves this by allowing the model to change its attention strategy based on the input dynamically.</li>\n    <li>A lightweight Attention Router is added to the pretrained model to manage different attention modes for each head.</li>\n    <li>After just 12 hours of training, this method shows improved performance and faster processing on long-context tasks.</li>\n</ul>"}, "publishedAt": "2026-01-24T03:22:07.000Z", "title": "Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers", "summary": "The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17367.png", "numComments": 1, "submittedBy": {"_id": "64096ef79e9f790c905b846d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg", "fullname": "Zecheng Tang", "name": "ZetangForward", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "61f8e653129c9ff1b911293d", "name": "SUDA", "fullname": "Soochow University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643701821817-61f8e5934a8e5a275b2b3e5a.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.17124", "authors": [{"_id": "6978407f026bdf0473116f26", "name": "Bin Lin", "hidden": false}, {"_id": "6978407f026bdf0473116f27", "user": {"_id": "646df3c04ad7f907279f14c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646df3c04ad7f907279f14c3/WZjSDtAmezmjbczLtCP2B.jpeg", "isPro": false, "fullname": "Zongjian Li", "user": "chestnutlzj", "type": "user"}, "name": "Zongjian Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T14:27:26.565Z", "hidden": false}, {"_id": "6978407f026bdf0473116f28", "user": {"_id": "66915a572c1a3a8edcc977b4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66915a572c1a3a8edcc977b4/2tANTgj48VQMgCcEcdkwE.jpeg", "isPro": false, "fullname": "Yuwei Niu", "user": "Yuwei-Niu", "type": "user"}, "name": "Yuwei Niu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:28:08.456Z", "hidden": false}, {"_id": "6978407f026bdf0473116f29", "user": {"_id": "642e427f6748dd4f8eeb2f38", "avatarUrl": "/avatars/07158ff6aa1803c846403594c5d55a34.svg", "isPro": false, "fullname": "Kaixiong Gong", "user": "kxgong", "type": "user"}, "name": "Kaixiong Gong", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:28:14.260Z", "hidden": false}, {"_id": "6978407f026bdf0473116f2a", "user": {"_id": "646de6402fd5a8eb8c518aa6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646de6402fd5a8eb8c518aa6/HYWb8-fT1kTm-ROBr1-0X.jpeg", "isPro": false, "fullname": "yunyangge", "user": "yunyangge", "type": "user"}, "name": "Yunyang Ge", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:28:20.323Z", "hidden": false}, {"_id": "6978407f026bdf0473116f2b", "name": "Yunlong Lin", "hidden": false}, {"_id": "6978407f026bdf0473116f2c", "user": {"_id": "62f0c4abe2999b231e5a893c", "avatarUrl": "/avatars/90da268b877a7ffe6665075c84018a83.svg", "isPro": false, "fullname": "Mingzhe Zheng", "user": "Dunge0nMaster", "type": "user"}, "name": "Mingzhe Zheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:28:29.309Z", "hidden": false}, {"_id": "6978407f026bdf0473116f2d", "name": "JianWei Zhang", "hidden": false}, {"_id": "6978407f026bdf0473116f2e", "user": {"_id": "680d79e978e732e7141e9280", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/YzX3HGEEHCXDcFymaiREA.png", "isPro": false, "fullname": "Miles Yang", "user": "dovahkiin648", "type": "user"}, "name": "Miles Yang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:28:41.196Z", "hidden": false}, {"_id": "6978407f026bdf0473116f2f", "name": "Zhao Zhong", "hidden": false}, {"_id": "6978407f026bdf0473116f30", "user": {"_id": "63d0cc736b985b0f25d0412c", "avatarUrl": "/avatars/3eb8c79f9a7c4c819038ea7b04e323dd.svg", "isPro": false, "fullname": "Bo", "user": "Liefeng", "type": "user"}, "name": "Liefeng Bo", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:28:53.581Z", "hidden": false}, {"_id": "6978407f026bdf0473116f31", "name": "Li Yuan", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6367a8175bb06007ea099b8f/xWMb_HtZh0Ud4FGpmfnyj.png", "https://cdn-uploads.huggingface.co/production/uploads/6367a8175bb06007ea099b8f/gVa2qUTv-eqFCiHr_gHUe.png"], "publishedAt": "2026-01-23T19:00:35.000Z", "submittedOnDailyAt": "2026-01-27T02:06:14.421Z", "title": "iFSQ: Improving FSQ for Image Generation with 1 Line of Code", "submittedOnDailyBy": {"_id": "6367a8175bb06007ea099b8f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6367a8175bb06007ea099b8f/IjG7HyWyWRlVt_XwRbxRW.jpeg", "isPro": false, "fullname": "linbin", "user": "LanguageBind", "type": "user"}, "summary": "The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ", "upvotes": 24, "discussionId": "6978407f026bdf0473116f32", "githubRepo": "https://github.com/Tencent-Hunyuan/iFSQ", "githubRepoAddedBy": "user", "ai_summary": "Finite Scalar Quantization with improved activation mapping enables unified modeling of discrete and continuous image generation approaches, revealing optimal representation balance and performance characteristics.", "ai_keywords": ["autoregressive models", "diffusion models", "VQ-VAEs", "VAEs", "finite scalar quantization", "activation collapse", "distribution-matching mapping", "iFSQ", "representation alignment", "LlamaGen-REPA"], "githubStars": 59, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u751f\u6210\u9886\u57df\u5206\u4e3a\u81ea\u56de\u5f52\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\uff0c\u8fd9\u5f71\u54cd\u4e86\u7edf\u4e00\u5efa\u6a21\u548c\u516c\u5e73\u57fa\u51c6\u6d4b\u8bd5\u3002</li>\n    <li>\u6709\u9650\u6807\u91cf\u91cf\u5316\uff08FSQ\uff09\u63d0\u4f9b\u4e86\u7406\u8bba\u6865\u6881\uff0c\u4f46\u5b58\u5728\u6fc0\u6d3b\u5d29\u6e83\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u91cd\u5efa\u8d28\u91cf\u548c\u4fe1\u606f\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002</li>\n    <li>\u901a\u8fc7\u5c06FSQ\u4e2d\u7684\u6fc0\u6d3b\u51fd\u6570\u66ff\u6362\u4e3a\u5339\u914d\u5206\u5e03\u7684\u6620\u5c04\uff0c\u63d0\u51fa\u4e86iFSQ\uff0c\u786e\u4fdd\u6700\u4f73\u7684\u7bb1\u5b50\u5229\u7528\u7387\u548c\u91cd\u5efa\u7cbe\u5ea6\u3002</li>\n    <li>\u4f7f\u7528iFSQ\u4f5c\u4e3a\u57fa\u51c6\uff0c\u6211\u4eec\u53d1\u73b0\u6700\u4f73\u7684\u79bb\u6563\u548c\u8fde\u7eed\u8868\u793a\u5e73\u8861\u5927\u7ea6\u5728\u6bcf\u7ef44\u4f4d\u3002</li>\n    <li>\u5728\u76f8\u540c\u91cd\u5efa\u7ea6\u675f\u4e0b\uff0c\u81ea\u56de\u5f52\u6a21\u578b\u521d\u671f\u6536\u655b\u5feb\uff0c\u800c\u6269\u6563\u6a21\u578b\u7684\u8868\u73b0\u4e0a\u9650\u66f4\u9ad8\uff0c\u8868\u660e\u4e25\u683c\u7684\u987a\u5e8f\u53ef\u80fd\u9650\u5236\u751f\u6210\u8d28\u91cf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Image generation models are divided into two types: autoregressive (AR) models and diffusion models.</li>\n    <li>Finite Scalar Quantization (FSQ) can connect these two types, but it has a flaw that affects performance.</li>\n    <li>The authors introduce iFSQ, a simple fix that improves both efficiency and quality in image reconstruction.</li>\n    <li>They find that the best balance between discrete and continuous image representations is around 4 bits per dimension.</li>\n    <li>AR models learn quickly but diffusion models can produce higher quality images in the end.</li>\n</ul>"}, "publishedAt": "2026-01-23T14:00:35.000Z", "title": "iFSQ: Improving FSQ for Image Generation with 1 Line of Code", "summary": "The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6367a8175bb06007ea099b8f/xWMb_HtZh0Ud4FGpmfnyj.png", "https://cdn-uploads.huggingface.co/production/uploads/6367a8175bb06007ea099b8f/gVa2qUTv-eqFCiHr_gHUe.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17124.png", "numComments": 1, "submittedBy": {"_id": "6367a8175bb06007ea099b8f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6367a8175bb06007ea099b8f/IjG7HyWyWRlVt_XwRbxRW.jpeg", "fullname": "linbin", "name": "LanguageBind", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 197, "isUserFollowing": false}, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.18778", "authors": [{"_id": "69783108026bdf0473116e3c", "user": {"_id": "6328ab511558dac67c45af92", "avatarUrl": "/avatars/1134657afe749b782f89fcabe960b774.svg", "isPro": false, "fullname": "Shobhita Sundaram", "user": "ssundaram", "type": "user"}, "name": "Shobhita Sundaram", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:03:27.764Z", "hidden": false}, {"_id": "69783108026bdf0473116e3d", "name": "John Quan", "hidden": false}, {"_id": "69783108026bdf0473116e3e", "name": "Ariel Kwiatkowski", "hidden": false}, {"_id": "69783108026bdf0473116e3f", "name": "Kartik Ahuja", "hidden": false}, {"_id": "69783108026bdf0473116e40", "name": "Yann Ollivier", "hidden": false}, {"_id": "69783108026bdf0473116e41", "user": {"_id": "65ce30e06da01df536eded5a", "avatarUrl": "/avatars/04c32cba7a3bbaf9ea5dee88c96cf87b.svg", "isPro": false, "fullname": "Julia Kempe", "user": "Knykny", "type": "user"}, "name": "Julia Kempe", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:03:24.999Z", "hidden": false}], "publishedAt": "2026-01-26T18:46:56.000Z", "submittedOnDailyAt": "2026-01-27T03:59:09.071Z", "title": "Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability", "submittedOnDailyBy": {"_id": "65ce30e06da01df536eded5a", "avatarUrl": "/avatars/04c32cba7a3bbaf9ea5dee88c96cf87b.svg", "isPro": false, "fullname": "Julia Kempe", "user": "Knykny", "type": "user"}, "summary": "Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.", "upvotes": 22, "discussionId": "69783109026bdf0473116e42", "ai_summary": "A self-improvement framework enables pretrained language models to generate automated curricula for solving previously unsolvable problems by leveraging latent knowledge and meta-reinforcement learning.", "ai_keywords": ["pretrained LLM", "reinforcement learning", "finetuning", "meta-RL", "automated curriculum", "self-improvement framework", "teacher-student model", "binary rewards", "sparse rewards", "latent capacity", "stepping stones", "structural quality", "well-posedness"], "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "summary_zh": "<ul>\n    <li>\u7814\u7a76\u5982\u4f55\u8ba9\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5b66\u4e60\u505c\u6ede\u65f6\u7ee7\u7eed\u8fdb\u6b65\u3002</li>\n    <li>\u8bbe\u8ba1\u4e86SOAR\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u95ee\u9898\u5e2e\u52a9\u6a21\u578b\u5b66\u4e60\u3002</li>\n    <li>\u4f7f\u7528\u57fa\u4e8e\u5b66\u751f\u8fdb\u6b65\u7684\u5956\u52b1\u673a\u5236\uff0c\u800c\u975e\u4f20\u7edf\u7684\u5956\u52b1\u65b9\u5f0f\u3002</li>\n    <li>\u53d1\u73b0\u751f\u6210\u7684\u95ee\u9898\u7684\u7ed3\u6784\u548c\u8d28\u91cf\u5bf9\u5b66\u4e60\u8fdb\u5c55\u66f4\u91cd\u8981\uff0c\u800c\u4e0d\u662f\u89e3\u51b3\u6b63\u786e\u6027\u3002</li>\n    <li>\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u53ef\u4ee5\u521b\u9020\u5b66\u4e60\u7684\u673a\u4f1a\uff0c\u800c\u4e0d\u9700\u8981\u5148\u89e3\u51b3\u96be\u9898\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning can struggle when improving models on difficult tasks with low success rates.</li>\n    <li>The study introduces SOAR, a framework where one model acts as a teacher to help another model learn by creating synthetic problems.</li>\n    <li>SOAR rewards the teacher model based on how well the student model improves on tough problems.</li>\n    <li>Using grounded rewards (based on actual progress) is more effective than previous reward methods, leading to more stable learning.</li>\n    <li>Important findings suggest that creating well-structured problems is key to helping models learn, even if they can\u2019t solve the hard problems directly.</li>\n</ul>"}, "publishedAt": "2026-01-26T13:46:56.000Z", "title": "Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability", "summary": "Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18778.png", "numComments": 1, "submittedBy": {"_id": "65ce30e06da01df536eded5a", "avatarUrl": "/avatars/04c32cba7a3bbaf9ea5dee88c96cf87b.svg", "fullname": "Julia Kempe", "name": "Knykny", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.18577", "authors": [{"_id": "697837d0026bdf0473116ed3", "name": "Sangwon Jang", "hidden": false}, {"_id": "697837d0026bdf0473116ed4", "user": {"_id": "66b57c77778c98d29446c8ec", "avatarUrl": "/avatars/c176bb7c072f3093f6a0786c87d384d8.svg", "isPro": false, "fullname": "Taekyung Ki", "user": "taekyungki", "type": "user"}, "name": "Taekyung Ki", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:00:25.179Z", "hidden": false}, {"_id": "697837d0026bdf0473116ed5", "user": {"_id": "65eaf5f185b81b33fc8653ed", "avatarUrl": "/avatars/a223129cd0258cb0fdf3356eba178bae.svg", "isPro": false, "fullname": "Jaehyeong Jo", "user": "harry9704", "type": "user"}, "name": "Jaehyeong Jo", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:29:04.004Z", "hidden": false}, {"_id": "697837d0026bdf0473116ed6", "user": {"_id": "6596422646624a86ff3b3bda", "avatarUrl": "/avatars/216e12b77e45ac5f1fa20932f5745411.svg", "isPro": false, "fullname": "Saining Xie", "user": "sainx", "type": "user"}, "name": "Saining Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:29:18.724Z", "hidden": false}, {"_id": "697837d0026bdf0473116ed7", "user": {"_id": "652066649004117947e46ed6", "avatarUrl": "/avatars/972c97df6f26d2c3d6ce71ec579984bb.svg", "isPro": false, "fullname": "Jaehong Yoon", "user": "jaehong31", "type": "user"}, "name": "Jaehong Yoon", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:00:27.411Z", "hidden": false}, {"_id": "697837d0026bdf0473116ed8", "name": "Sung Ju Hwang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Ffm5MpnJjIL7Rznm-he0z.mp4"], "publishedAt": "2026-01-26T15:22:27.000Z", "submittedOnDailyAt": "2026-01-27T01:32:21.601Z", "title": "Self-Refining Video Sampling", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\\% human preference compared to the default sampler and guidance-based sampler.", "upvotes": 16, "discussionId": "697837d0026bdf0473116ed9", "projectPage": "https://agwmon.github.io/self-refine-video/", "githubRepo": "https://github.com/agwmon/self-refine-video", "githubRepoAddedBy": "user", "ai_summary": "Self-refining video sampling improves motion coherence and physics alignment by using a pre-trained video generator as its own denoising autoencoder for iterative refinement with uncertainty-aware region selection.", "ai_keywords": ["video generators", "denoising autoencoder", "iterative inner-loop refinement", "self-refining video sampling", "uncertainty-aware refinement", "motion coherence", "physics alignment"], "githubStars": 43, "summary_zh": "<ul>\n    <li>\u73b0\u4ee3\u89c6\u9891\u751f\u6210\u5668\u5728\u590d\u6742\u7269\u7406\u52a8\u6001\u4e0a\u4ecd\u7136\u5b58\u5728\u56f0\u96be\uff0c\u7f3a\u4e4f\u7269\u7406\u771f\u5b9e\u611f\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5916\u90e8\u9a8c\u8bc1\u6216\u989d\u5916\u8bad\u7ec3\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6355\u6349\u7ec6\u5fae\u8fd0\u52a8\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u81ea\u6211\u7cbe\u70bc\u89c6\u9891\u91c7\u6837\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u9891\u751f\u6210\u5668\u8fdb\u884c\u81ea\u6211\u6539\u8fdb\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u8fed\u4ee3\u7cbe\u70bc\uff0c\u65e0\u9700\u5916\u90e8\u9a8c\u8bc1\u6216\u989d\u5916\u8bad\u7ec3\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u8fd0\u52a8\u4e00\u81f4\u6027\u548c\u7269\u7406\u5bf9\u9f50\uff0c\u8d85\u8fc770%\u7684\u7528\u6237\u504f\u597d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Modern video generators often struggle to produce realistic physical movements.</li>\n    <li>Current solutions use extra checks or additional training, which can be costly and still miss detailed motions.</li>\n    <li>This work introduces a method called self-refining video sampling that uses a pre-trained generator to improve itself during use.</li>\n    <li>The method includes a strategy that focuses on refining only certain areas to avoid mistakes from too much refinement.</li>\n    <li>Tests show this method greatly enhances motion quality and realism, with over 70% of people preferring it to traditional methods.</li>\n</ul>"}, "publishedAt": "2026-01-26T10:22:27.000Z", "title": "Self-Refining Video Sampling", "summary": "Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\\% human preference compared to the default sampler and guidance-based sampler.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Ffm5MpnJjIL7Rznm-he0z.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18577.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 218, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.18184", "authors": [{"_id": "697832bd026bdf0473116e89", "user": {"_id": "68e3ba91fc9f7cbda9a2be02", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5h166wHkdQSG7IusNV5wT.png", "isPro": false, "fullname": "pengzhiliang", "user": "zhiliang2", "type": "user"}, "name": "Zhiliang Peng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:29:35.447Z", "hidden": false}, {"_id": "697832bd026bdf0473116e8a", "name": "Jianwei Yu", "hidden": false}, {"_id": "697832bd026bdf0473116e8b", "user": {"_id": "65e552cb4dbf9514fb0c3110", "avatarUrl": "/avatars/655dd9a36e85c1290855fb2c296472f8.svg", "isPro": false, "fullname": "Yaoyao Chang", "user": "YaoyaoChang", "type": "user"}, "name": "Yaoyao Chang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:30:15.096Z", "hidden": false}, {"_id": "697832bd026bdf0473116e8c", "name": "Zilong Wang", "hidden": false}, {"_id": "697832bd026bdf0473116e8d", "name": "Li Dong", "hidden": false}, {"_id": "697832bd026bdf0473116e8e", "user": {"_id": "672b6da69380700b60c92367", "avatarUrl": "/avatars/651b8bd4d1d6bbd047c6f0d6010a0ea3.svg", "isPro": false, "fullname": "Yingbo Hao", "user": "YingboHao", "type": "user"}, "name": "Yingbo Hao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:30:33.802Z", "hidden": false}, {"_id": "697832bd026bdf0473116e8f", "name": "Yujie Tu", "hidden": false}, {"_id": "697832bd026bdf0473116e90", "name": "Chenyu Yang", "hidden": false}, {"_id": "697832bd026bdf0473116e91", "user": {"_id": "661396d69ef83c1509d41c3f", "avatarUrl": "/avatars/9760ac72f2d44320e6033d30e2ce7bd5.svg", "isPro": false, "fullname": "Wenhui Wang", "user": "stonewh1", "type": "user"}, "name": "Wenhui Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:30:44.557Z", "hidden": false}, {"_id": "697832bd026bdf0473116e92", "name": "Songchen Xu", "hidden": false}, {"_id": "697832bd026bdf0473116e93", "name": "Yutao Sun", "hidden": false}, {"_id": "697832bd026bdf0473116e94", "name": "Hangbo Bao", "hidden": false}, {"_id": "697832bd026bdf0473116e95", "user": {"_id": "66f4cc8dcb62f781535a27ac", "avatarUrl": "/avatars/2b6cf6513a0c93f932115634092f7d30.svg", "isPro": false, "fullname": "Weijiang Xu", "user": "WeijiangXU", "type": "user"}, "name": "Weijiang Xu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:30:57.860Z", "hidden": false}, {"_id": "697832bd026bdf0473116e96", "name": "Yi Zhu", "hidden": false}, {"_id": "697832bd026bdf0473116e97", "user": {"_id": "67b358e504fec7fd742ae8c1", "avatarUrl": "/avatars/3f404b20df769bcf87e8d8d7d5fc9ed6.svg", "isPro": false, "fullname": "Zehua Wang", "user": "zehuawang", "type": "user"}, "name": "Zehua Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:31:04.270Z", "hidden": false}, {"_id": "697832bd026bdf0473116e98", "name": "Ting Song", "hidden": false}, {"_id": "697832bd026bdf0473116e99", "name": "Yan Xia", "hidden": false}, {"_id": "697832bd026bdf0473116e9a", "user": {"_id": "60f6d61f89b21b8fd2d471c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f6d61f89b21b8fd2d471c6/RmLFf97vUoXMoCT3rWbhm.jpeg", "isPro": false, "fullname": "Zewen Chi", "user": "CZWin32768", "type": "user"}, "name": "Zewen Chi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:31:10.007Z", "hidden": false}, {"_id": "697832bd026bdf0473116e9b", "name": "Shaohan Huang", "hidden": false}, {"_id": "697832bd026bdf0473116e9c", "name": "Liang Wang", "hidden": false}, {"_id": "697832bd026bdf0473116e9d", "name": "Chuang Ding", "hidden": false}, {"_id": "697832bd026bdf0473116e9e", "name": "Shuai Wang", "hidden": false}, {"_id": "697832bd026bdf0473116e9f", "name": "Xie Chen", "hidden": false}, {"_id": "697832bd026bdf0473116ea0", "user": {"_id": "6368c512fbfe97c16a40baba", "avatarUrl": "/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg", "isPro": false, "fullname": "Furu Wei", "user": "thegenerality", "type": "user"}, "name": "Furu Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:31:19.998Z", "hidden": false}], "publishedAt": "2026-01-26T06:11:51.000Z", "submittedOnDailyAt": "2026-01-27T01:18:16.537Z", "title": "VIBEVOICE-ASR Technical Report", "submittedOnDailyBy": {"_id": "67ecd6178647cfa1775f75ed", "avatarUrl": "/avatars/98882cc58dc0a5de94df765d523d92c9.svg", "isPro": false, "fullname": "Furu Wei", "user": "frontierai", "type": "user"}, "summary": "This report presents VibeVoice-ASR, a general-purpose speech understanding framework built upon VibeVoice, designed to address the persistent challenges of context fragmentation and multi-speaker complexity in long-form audio (e.g., meetings, podcasts) that remain despite recent advancements in short-form speech recognition. Unlike traditional pipelined approaches that rely on audio chunking, VibeVoice-ASRsupports single-pass processing for up to 60 minutes of audio. It unifies Automatic Speech Recognition, Speaker Diarization, and Timestamping into a single end-to-end generation task. In addition, VibeVoice-ASR supports over 50 languages, requires no explicit language setting, and natively handles code-switching within and across utterances. Furthermore, we introduce a prompt-based context injection mechanism that allows users to supply customized conetxt, significantly improving accuracy on domain-specific terminology and polyphonic character disambiguation.", "upvotes": 11, "discussionId": "697832bd026bdf0473116ea1", "ai_summary": "VibeVoice-ASR is a unified end-to-end speech understanding framework that processes long-form audio in a single pass while supporting multilingual, code-switching, and domain-specific context injection.", "ai_keywords": ["speech understanding framework", "VibeVoice", "Automatic Speech Recognition", "Speaker Diarization", "Timestamping", "end-to-end generation", "long-form audio", "single-pass processing", "multilingual support", "code-switching", "prompt-based context injection"], "organization": {"_id": "5e6485f787403103f9f1055e", "name": "microsoft", "fullname": "Microsoft", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"}, "summary_zh": "<ul>\n    <li>VibeVoice-ASR \u662f\u4e00\u4e2a\u901a\u7528\u7684\u8bed\u97f3\u7406\u89e3\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u957f\u97f3\u9891\uff08\u5982\u4f1a\u8bae\u548c\u64ad\u5ba2\uff09\u4e2d\u7684\u4e0a\u4e0b\u6587\u788e\u7247\u548c\u591a\u8bf4\u8bdd\u8005\u590d\u6742\u6027\u95ee\u9898\u3002</li>\n    <li>\u4e0e\u4f20\u7edf\u7684\u97f3\u9891\u5206\u6bb5\u65b9\u6cd5\u4e0d\u540c\uff0cVibeVoice-ASR \u652f\u6301\u4e00\u6b21\u6027\u5904\u7406\u957f\u8fbe 60 \u5206\u949f\u7684\u97f3\u9891\u3002</li>\n    <li>\u5b83\u5c06\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u3001\u8bf4\u8bdd\u8005\u8bc6\u522b\u548c\u65f6\u95f4\u6233\u6574\u5408\u4e3a\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u751f\u6210\u4efb\u52a1\u3002</li>\n    <li>\u652f\u6301\u8d85\u8fc7 50 \u79cd\u8bed\u8a00\uff0c\u65e0\u9700\u7528\u6237\u660e\u786e\u8bbe\u7f6e\u8bed\u8a00\uff0c\u5e76\u80fd\u591f\u5904\u7406\u8bed\u8a00\u5207\u6362\u3002</li>\n    <li>\u5f15\u5165\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u4e0a\u4e0b\u6587\u6ce8\u5165\u673a\u5236\uff0c\u5141\u8bb8\u7528\u6237\u63d0\u4f9b\u5b9a\u5236\u7684\u4e0a\u4e0b\u6587\uff0c\u63d0\u9ad8\u4e86\u7279\u5b9a\u9886\u57df\u672f\u8bed\u548c\u591a\u58f0\u9053\u89d2\u8272\u7684\u8bc6\u522b\u51c6\u786e\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>VibeVoice-ASR is a new framework for understanding speech, especially in long audio like meetings and podcasts.</li>\n    <li>It processes up to 60 minutes of audio in one go, instead of breaking it into smaller parts.</li>\n    <li>The system combines speech recognition, speaker identification, and timestamping into one task.</li>\n    <li>It works in over 50 languages without needing users to set a specific language and can handle code-switching (mixing languages) easily.</li>\n    <li>A new feature allows users to add their own context to improve accuracy for specific terms and distinguishing between speakers.</li>\n</ul>"}, "publishedAt": "2026-01-26T01:11:51.000Z", "title": "VIBEVOICE-ASR Technical Report", "summary": "This report presents VibeVoice-ASR, a general-purpose speech understanding framework built upon VibeVoice, designed to address the persistent challenges of context fragmentation and multi-speaker complexity in long-form audio (e.g., meetings, podcasts) that remain despite recent advancements in short-form speech recognition. Unlike traditional pipelined approaches that rely on audio chunking, VibeVoice-ASRsupports single-pass processing for up to 60 minutes of audio. It unifies Automatic Speech Recognition, Speaker Diarization, and Timestamping into a single end-to-end generation task. In addition, VibeVoice-ASR supports over 50 languages, requires no explicit language setting, and natively handles code-switching within and across utterances. Furthermore, we introduce a prompt-based context injection mechanism that allows users to supply customized conetxt, significantly improving accuracy on domain-specific terminology and polyphonic character disambiguation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18184.png", "numComments": 1, "submittedBy": {"_id": "67ecd6178647cfa1775f75ed", "avatarUrl": "/avatars/98882cc58dc0a5de94df765d523d92c9.svg", "fullname": "Furu Wei", "name": "frontierai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 15, "isUserFollowing": false}, "organization": {"_id": "5e6485f787403103f9f1055e", "name": "microsoft", "fullname": "Microsoft", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.18137", "authors": [{"_id": "69784aee026bdf0473116f4e", "name": "Yinger Zhang", "hidden": false}, {"_id": "69784aee026bdf0473116f4f", "user": {"_id": "6746c6c99700a50f13a0eda9", "avatarUrl": "/avatars/8e8b82a9b73ff807d976fc48bb2e3edc.svg", "isPro": false, "fullname": "Shutong Jiang", "user": "Stjiang", "type": "user"}, "name": "Shutong Jiang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T10:23:07.229Z", "hidden": false}, {"_id": "69784aee026bdf0473116f50", "user": {"_id": "64abc87cf79cb0c313821c11", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64abc87cf79cb0c313821c11/tJa4iX5_f5XoSIzXt-rwF.jpeg", "isPro": false, "fullname": "Renhao Li", "user": "RioLee", "type": "user"}, "name": "Renhao Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T10:23:38.820Z", "hidden": false}, {"_id": "69784aee026bdf0473116f51", "user": {"_id": "654bead777401b47e6424f88", "avatarUrl": "/avatars/7bcbdbb051c93b004f0dc3ad36c4a0ce.svg", "isPro": false, "fullname": "Jianhong Tu", "user": "JianhongTu", "type": "user"}, "name": "Jianhong Tu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T10:23:44.994Z", "hidden": false}, {"_id": "69784aee026bdf0473116f52", "name": "Yang Su", "hidden": false}, {"_id": "69784aee026bdf0473116f53", "name": "Lianghao Deng", "hidden": false}, {"_id": "69784aee026bdf0473116f54", "name": "Xudong Guo", "hidden": false}, {"_id": "69784aee026bdf0473116f55", "name": "Chenxu Lv", "hidden": false}, {"_id": "69784aee026bdf0473116f56", "user": {"_id": "620760a26e3b7210c2ff1943", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg", "isPro": false, "fullname": "Junyang Lin", "user": "JustinLin610", "type": "user"}, "name": "Junyang Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T10:24:12.538Z", "hidden": false}], "publishedAt": "2026-01-26T04:43:49.000Z", "submittedOnDailyAt": "2026-01-27T07:52:17.482Z", "title": "DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints", "submittedOnDailyBy": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "isPro": false, "fullname": "Adina Yakefu", "user": "AdinaY", "type": "user"}, "summary": "While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.", "upvotes": 10, "discussionId": "69784aef026bdf0473116f57", "ai_summary": "DeepPlanning benchmark addresses limitations of current LLM planning assessments by introducing complex, real-world tasks requiring both global optimization and local constraint reasoning.", "ai_keywords": ["agent evaluation", "long-horizon tasks", "global constrained optimization", "local constrained reasoning", "agentic LLMs", "explicit reasoning patterns", "parallel tool use"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u4ee3\u7406\u8bc4\u4f30\u4e3b\u8981\u96c6\u4e2d\u5728\u77ed\u671f\u4efb\u52a1\uff0c\u800c\u957f\u8fdc\u4efb\u52a1\u7684\u8bc4\u4f30\u4e0d\u8db3\u3002</li>\n    <li>\u5927\u591a\u6570\u57fa\u51c6\u6d4b\u8bd5\u5f3a\u8c03\u5c40\u90e8\u63a8\u7406\uff0c\u800c\u4e0d\u662f\u5168\u7403\u4f18\u5316\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86DeepPlanning\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u957f\u671f\u89c4\u5212\u4e2d\u7684\u5b9e\u9645\u4efb\u52a1\u3002</li>\n    <li>DeepPlanning \u5305\u62ec\u591a\u5929\u65c5\u884c\u548c\u591a\u4ea7\u54c1\u8d2d\u7269\uff0c\u9700\u8981\u4e3b\u52a8\u83b7\u53d6\u4fe1\u606f\u548c\u5904\u7406\u5c40\u90e8\u7ea6\u675f\u3002</li>\n    <li>\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u4e5f\u9762\u4e34\u6311\u6218\uff0c\u5f3a\u8c03\u4e86\u53ef\u9760\u63a8\u7406\u6a21\u5f0f\u7684\u91cd\u8981\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current benchmarks often focus on short-term tasks instead of long-term planning that requires handling budgets and constraints.</li>\n    <li>DeepPlanning is a new benchmark designed for long-term agent planning, featuring tasks like multi-day travel and shopping.</li>\n    <li>These tasks require gathering information and considering both local and global constraints.</li>\n    <li>Tests show that even advanced AI models have difficulty with these long-term planning challenges.</li>\n    <li>The authors provide code and data for others to use in research on improving AI planning abilities.</li>\n</ul>"}, "publishedAt": "2026-01-25T23:43:49.000Z", "title": "DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints", "summary": "While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18137.png", "numComments": 1, "submittedBy": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "fullname": "Adina Yakefu", "name": "AdinaY", "type": "user", "isPro": false, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 1207, "isUserFollowing": false}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2601.16725", "authors": [{"_id": "6976d5405d41524304c13537", "name": "Meituan LongCat Team", "hidden": false}, {"_id": "6976d5405d41524304c13538", "name": "Anchun Gui", "hidden": false}, {"_id": "6976d5405d41524304c13539", "name": "Bei Li", "hidden": false}, {"_id": "6976d5405d41524304c1353a", "name": "Bingyang Tao", "hidden": false}, {"_id": "6976d5405d41524304c1353b", "name": "Bole Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1353c", "name": "Borun Chen", "hidden": false}, {"_id": "6976d5405d41524304c1353e", "name": "Chao Zhang", "hidden": false}, {"_id": "69772bc15d41524304c13739", "name": "Chao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1353f", "name": "Chen Gao", "hidden": false}, {"_id": "6976d5405d41524304c13540", "name": "Chen Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13541", "name": "Chengcheng Han", "hidden": false}, {"_id": "6976d5405d41524304c13542", "name": "Chenhui Yang", "hidden": false}, {"_id": "6976d5405d41524304c13543", "name": "Chuyu Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13544", "name": "Cong Chen", "hidden": false}, {"_id": "6976d5405d41524304c13545", "name": "Cunguang Wang", "hidden": false}, {"_id": "6976d5405d41524304c13546", "name": "Daoru Pan", "hidden": false}, {"_id": "6976d5405d41524304c13547", "name": "Defei Bu", "hidden": false}, {"_id": "6976d5405d41524304c13548", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13549", "name": "Di Xiu", "hidden": false}, {"_id": "6976d5405d41524304c1354a", "name": "Dishan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1354b", "name": "Dongyu Ru", "hidden": false}, {"_id": "6976d5405d41524304c1354c", "name": "Dunwei Tu", "hidden": false}, {"_id": "6976d5405d41524304c1354d", "name": "Fan Wu", "hidden": false}, {"_id": "6976d5405d41524304c1354e", "name": "Fengcheng Yuan", "hidden": false}, {"_id": "6976d5405d41524304c1354f", "name": "Fengcun Li", "hidden": false}, {"_id": "6976d5405d41524304c13550", "name": "Gang Xu", "hidden": false}, {"_id": "6976d5405d41524304c13551", "name": "Guanyu Wu", "hidden": false}, {"_id": "6976d5405d41524304c13552", "name": "Guoyuan Lin", "hidden": false}, {"_id": "6976d5405d41524304c13553", "name": "Haibin Wang", "hidden": false}, {"_id": "6976d5405d41524304c13554", "name": "Hansi Yang", "hidden": false}, {"_id": "6976d5405d41524304c13555", "name": "Hao Yang", "hidden": false}, {"_id": "6976d5405d41524304c13556", "name": "Haonan Yan", "hidden": false}, {"_id": "6976d5405d41524304c13557", "name": "Haoxiang Ma", "hidden": false}, {"_id": "6976d5405d41524304c13558", "name": "Haoxing Wen", "hidden": false}, {"_id": "6976d5405d41524304c13559", "name": "Hongyan Hao", "hidden": false}, {"_id": "6976d5405d41524304c1355a", "name": "Hongyin Tang", "hidden": false}, {"_id": "6976d5405d41524304c1355b", "name": "Hongyu Zang", "hidden": false}, {"_id": "6976d5405d41524304c1355c", "name": "Hongzhi Ni", "hidden": false}, {"_id": "6976d5405d41524304c1355d", "name": "Hui Su", "hidden": false}, {"_id": "6976d5405d41524304c1355e", "name": "Jiacheng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1355f", "name": "Jiahong Zhou", "hidden": false}, {"_id": "6976d5405d41524304c13560", "name": "Jiahuan Li", "hidden": false}, {"_id": "6976d5405d41524304c13561", "name": "Jiaming Wang", "hidden": false}, {"_id": "6976d5405d41524304c13562", "name": "Jian Yang", "hidden": false}, {"_id": "6976d5405d41524304c13563", "user": {"_id": "64008a0af4ff62c2616d8858", "avatarUrl": "/avatars/b52c98857916fba5377ace8089d658b2.svg", "isPro": false, "fullname": "zhangjf", "user": "zhangjf", "type": "user"}, "name": "Jianfei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:09.272Z", "hidden": false}, {"_id": "6976d5405d41524304c13564", "name": "Jianhao Xu", "hidden": false}, {"_id": "6976d5405d41524304c13565", "name": "Jianing Wang", "hidden": false}, {"_id": "6976d5405d41524304c13566", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13567", "name": "Jiaqi Sun", "hidden": false}, {"_id": "6976d5405d41524304c13568", "name": "Jiarong Shi", "hidden": false}, {"_id": "6976d5405d41524304c13569", "name": "Jiarui Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1356a", "name": "Jingang Wang", "hidden": false}, {"_id": "6976d5405d41524304c1356b", "user": {"_id": "6592472fccbc1e2cc7250903", "avatarUrl": "/avatars/6f04ae66944eb2ce65c5aca7927bab10.svg", "isPro": false, "fullname": "Jinluan Yang", "user": "Jinluan", "type": "user"}, "name": "Jinluan Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:47.175Z", "hidden": false}, {"_id": "6976d5405d41524304c1356c", "name": "Jinrui Ding", "hidden": false}, {"_id": "6976d5405d41524304c1356d", "name": "Jinwei Xiao", "hidden": false}, {"_id": "6976d5405d41524304c1356e", "name": "Jiyuan He", "hidden": false}, {"_id": "6976d5405d41524304c1356f", "name": "Juncan Xu", "hidden": false}, {"_id": "6976d5405d41524304c13570", "name": "Kefeng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13571", "name": "Keheng Wang", "hidden": false}, {"_id": "6976d5405d41524304c13572", "name": "Li Wei", "hidden": false}, {"_id": "6976d5405d41524304c13573", "name": "Lianhui Ma", "hidden": false}, {"_id": "6976d5405d41524304c13574", "name": "Lin Qiu", "hidden": false}, {"_id": "6976d5405d41524304c13575", "name": "Lingbing Kong", "hidden": false}, {"_id": "6976d5405d41524304c13576", "name": "Lingchuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c13577", "name": "Linsen Guo", "hidden": false}, {"_id": "6976d5405d41524304c13578", "name": "Mengshen Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13579", "name": "Mengxia Shen", "hidden": false}, {"_id": "6976d5405d41524304c1357a", "name": "Mingyang Zhu", "hidden": false}, {"_id": "6976d5405d41524304c1357b", "name": "Peiguang Li", "hidden": false}, {"_id": "6976d5405d41524304c1357c", "name": "Peng Pei", "hidden": false}, {"_id": "6976d5405d41524304c1357d", "name": "Pengcheng Jia", "hidden": false}, {"_id": "6976d5405d41524304c1357e", "name": "Pengtao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1357f", "name": "Peng Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13580", "name": "Qi Gu", "hidden": false}, {"_id": "6976d5405d41524304c13581", "name": "Qiong Huang", "hidden": false}, {"_id": "6976d5405d41524304c13582", "name": "Qiyuan Duan", "hidden": false}, {"_id": "6976d5405d41524304c13583", "name": "Quanchi Weng", "hidden": false}, {"_id": "6976d5405d41524304c13584", "name": "Rongxiang Weng", "hidden": false}, {"_id": "6976d5405d41524304c13585", "name": "Rongzhi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13586", "name": "Rumei Li", "hidden": false}, {"_id": "6976d5405d41524304c13587", "name": "Shanglin Lei", "hidden": false}, {"_id": "6976d5405d41524304c13588", "user": {"_id": "64db5f5dd68a6ddcc7bd89e9", "avatarUrl": "/avatars/69375ec915927b855813df8a6d486837.svg", "isPro": false, "fullname": "Shengnan An", "user": "ShengnanAn", "type": "user"}, "name": "Shengnan An", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:11.410Z", "hidden": false}, {"_id": "6976d5405d41524304c13589", "name": "Shijun Dai", "hidden": false}, {"_id": "6976d5405d41524304c1358a", "name": "Shuaikang Liu", "hidden": false}, {"_id": "6976d5405d41524304c1358b", "name": "Shuang Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1358c", "name": "Shuo Wang", "hidden": false}, {"_id": "6976d5405d41524304c1358d", "name": "Songyuan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1358e", "name": "Tao Liang", "hidden": false}, {"_id": "6976d5405d41524304c1358f", "name": "Tianhao Hu", "hidden": false}, {"_id": "6976d5405d41524304c13590", "name": "Tianze Chen", "hidden": false}, {"_id": "6976d5405d41524304c13591", "name": "Wei Liu", "hidden": false}, {"_id": "6976d5405d41524304c13592", "name": "Wei Shi", "hidden": false}, {"_id": "6976d5405d41524304c13593", "name": "Wei Wang", "hidden": false}, {"_id": "6976d5405d41524304c13594", "name": "Weifeng Tang", "hidden": false}, {"_id": "6976d5405d41524304c13595", "name": "Wenjie Shi", "hidden": false}, {"_id": "6976d5405d41524304c13596", "name": "Wenlong Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13597", "name": "Wentao Chen", "hidden": false}, {"_id": "6976d5405d41524304c13598", "name": "Wentao Shi", "hidden": false}, {"_id": "6976d5405d41524304c13599", "name": "Xi Su", "hidden": false}, {"_id": "6976d5405d41524304c1359a", "name": "Xiangcheng Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359b", "name": "Xiandi Ma", "hidden": false}, {"_id": "6976d5405d41524304c1359c", "user": {"_id": "63edb098679c2cc40abc6c2e", "avatarUrl": "/avatars/288c7229937c2c3f29fda6d17c7df2eb.svg", "isPro": false, "fullname": "Xiangyu", "user": "xixy", "type": "user"}, "name": "Xiangyu Xi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:13.312Z", "hidden": false}, {"_id": "6976d5405d41524304c1359d", "name": "Xiangyuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359e", "name": "Xiangzhou Huang", "hidden": false}, {"_id": "6976d5405d41524304c1359f", "name": "Xiao Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a0", "name": "Xiaodong Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a1", "name": "Xiaolong Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a2", "name": "Xiaowei Shi", "hidden": false}, {"_id": "6976d5405d41524304c135a3", "name": "Xiaoyu Li", "hidden": false}, {"_id": "6976d5405d41524304c135a4", "name": "Xin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a5", "name": "Xingchen Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a6", "name": "Xuan Huang", "hidden": false}, {"_id": "6976d5405d41524304c135a7", "name": "Xuezhi Cao", "hidden": false}, {"_id": "6976d5405d41524304c135a8", "name": "Xunliang Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a9", "name": "Yan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135aa", "user": {"_id": "63fc1c420aab06079200c15c", "avatarUrl": "/avatars/8e8e82a9a6552848581ca9f65011263c.svg", "isPro": false, "fullname": "yang bai", "user": "byang", "type": "user"}, "name": "Yang Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:07.036Z", "hidden": false}, {"_id": "6976d5405d41524304c135ab", "name": "Yang Liu", "hidden": false}, {"_id": "6976d5405d41524304c135ac", "name": "Yang Yang", "hidden": false}, {"_id": "6976d5405d41524304c135ad", "name": "Yang Zheng", "hidden": false}, {"_id": "6976d5405d41524304c135ae", "name": "Yaoming Wang", "hidden": false}, {"_id": "6976d5405d41524304c135af", "name": "Yaoming Zhu", "hidden": false}, {"_id": "6976d5405d41524304c135b0", "name": "Yaqi Huo", "hidden": false}, {"_id": "6976d5405d41524304c135b1", "name": "Yanyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b2", "name": "Yaorui Shi", "hidden": false}, {"_id": "6976d5405d41524304c135b3", "name": "Yerui Sun", "hidden": false}, {"_id": "6976d5405d41524304c135b4", "name": "Yi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b5", "name": "Yihao Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b6", "name": "Yi-Kai Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b7", "name": "Yifan Lu", "hidden": false}, {"_id": "6976d5405d41524304c135b8", "name": "Yifan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135b9", "name": "Yitao Zhai", "hidden": false}, {"_id": "6976d5405d41524304c135ba", "name": "Yongjing Yin", "hidden": false}, {"_id": "6976d5405d41524304c135bb", "name": "Yongwei Zhou", "hidden": false}, {"_id": "6976d5405d41524304c135bc", "name": "Youshao Xiao", "hidden": false}, {"_id": "6976d5405d41524304c135bd", "name": "Yuchuan Dai", "hidden": false}, {"_id": "6976d5405d41524304c135be", "name": "Yuchen Xie", "hidden": false}, {"_id": "6976d5405d41524304c135bf", "name": "Yuchen Yu", "hidden": false}, {"_id": "6976d5405d41524304c135c0", "name": "Yufei Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135c1", "name": "Yuhuai Wei", "hidden": false}, {"_id": "6976d5405d41524304c135c2", "name": "Yulei Qian", "hidden": false}, {"_id": "6976d5405d41524304c135c3", "name": "Yunfan Liang", "hidden": false}, {"_id": "6976d5405d41524304c135c4", "name": "Yunke Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135c5", "name": "Yuwei Jiang", "hidden": false}, {"_id": "6976d5405d41524304c135c6", "name": "Yuxin Bian", "hidden": false}, {"_id": "6976d5405d41524304c135c7", "name": "Yuxin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135c8", "name": "Yuxin Liu", "hidden": false}, {"_id": "6976d5405d41524304c135c9", "name": "Yue Xu", "hidden": false}, {"_id": "6976d5405d41524304c135ca", "name": "Yueqing Sun", "hidden": false}, {"_id": "6976d5405d41524304c135cb", "name": "Zeyang Yu", "hidden": false}, {"_id": "6976d5405d41524304c135cc", "name": "Zhao Yang", "hidden": false}, {"_id": "6976d5405d41524304c135cd", "name": "Zhengsheng Huang", "hidden": false}, {"_id": "6976d5405d41524304c135ce", "name": "Zhengyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135cf", "name": "Zhijian Liu", "hidden": false}, {"_id": "6976d5405d41524304c135d0", "name": "Zhikang Xia", "hidden": false}, {"_id": "6976d5405d41524304c135d1", "name": "Zhimin Lin", "hidden": false}, {"_id": "6976d5405d41524304c135d2", "name": "Zhiyuan Yao", "hidden": false}, {"_id": "6976d5405d41524304c135d3", "name": "Zhuofan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135d4", "name": "Zhuowen Han", "hidden": false}, {"_id": "6976d5405d41524304c135d5", "name": "Zijian Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135d6", "name": "Ziran Li", "hidden": false}, {"_id": "6976d5405d41524304c135d7", "name": "Ziwen Wang", "hidden": false}, {"_id": "6976d5405d41524304c135d8", "name": "Ziyuan Zhuang", "hidden": false}], "publishedAt": "2026-01-23T13:20:09.000Z", "submittedOnDailyAt": "2026-01-26T00:15:28.340Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "upvotes": 136, "discussionId": "6976d5405d41524304c135d9", "ai_summary": "A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for real-world robustness and complex reasoning.", "ai_keywords": ["Mixture-of-Experts", "agentic reasoning", "domain-parallel expert training", "fusion", "asynchronous reinforcement learning", "DORA", "long-tailed generation", "multi-turn interactions", "real-world noise patterns", "test-time scaling", "reasoning depth", "reasoning width", "parallel thinking"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86LongCat-Flash-Thinking-2601\uff0c\u8fd9\u662f\u4e00\u4e2a\u62e5\u67095600\u4ebf\u53c2\u6570\u7684\u5f00\u6e90\u63a8\u7406\u6a21\u578b\uff0c\u5177\u5907\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u4ee3\u7406\u641c\u7d22\u3001\u5de5\u5177\u4f7f\u7528\u548c\u5de5\u5177\u6574\u5408\u63a8\u7406\u3002</li>\n    <li>\u6a21\u578b\u80fd\u591f\u5728\u590d\u6742\u5de5\u5177\u4e92\u52a8\u548c\u5608\u6742\u7684\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u7a33\u5065\u6027\u3002</li>\n    <li>\u91c7\u7528\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u9886\u57df\u5e76\u884c\u4e13\u5bb6\u8bad\u7ec3\u548c\u540e\u7eed\u878d\u5408\uff0c\u5b9e\u73b0\u4e86\u4ece\u9884\u8bad\u7ec3\u5230\u540e\u8bad\u7ec3\u7684\u5168\u9762\u8bbe\u8ba1\u3002</li>\n    <li>\u5f15\u5165\u4e86Heavy Thinking\u6a21\u5f0f\uff0c\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u5e76\u884c\u601d\u8003\u6709\u6548\u6269\u5c55\u63a8\u7406\u7684\u6df1\u5ea6\u548c\u5bbd\u5ea6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongCat-Flash-Thinking-2601 is a large open-source model with 560 billion parameters designed for advanced reasoning tasks.</li>\n    <li>It performs extremely well on various benchmarks related to agentic reasoning, such as searching and using tools effectively.</li>\n    <li>The model is capable of handling complex interactions and works well in noisy, real-world situations.</li>\n    <li>Its training combines expert knowledge and systematic design to improve its performance across many different environments.</li>\n    <li>A special feature called Heavy Thinking helps the model enhance its reasoning capabilities during testing.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:20:09.000Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16725.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.17058", "authors": [{"_id": "69782c96026bdf0473116e06", "user": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "name": "Wei Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:49.701Z", "hidden": false}, {"_id": "69782c96026bdf0473116e07", "name": "Jun Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e08", "name": "Haoyu Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e09", "name": "Zhenghao Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0a", "name": "Qikang He", "hidden": false}, {"_id": "69782c96026bdf0473116e0b", "name": "Shaokun Han", "hidden": false}, {"_id": "69782c96026bdf0473116e0c", "name": "Guoliang Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0d", "user": {"_id": "64ef522242da8d2a897d62da", "avatarUrl": "/avatars/03611010d247da66696ac8976d4d3ed3.svg", "isPro": false, "fullname": "xuanhe zhou", "user": "zhouxh19", "type": "user"}, "name": "Xuanhe Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:19.930Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0e", "user": {"_id": "674fa2f067c963c50a066594", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674fa2f067c963c50a066594/hKZ46Mwm_UEguzBt63ys_.jpeg", "isPro": false, "fullname": "yeye he", "user": "yeyehe", "type": "user"}, "name": "Yeye He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:27.638Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0f", "name": "Chunwei Liu", "hidden": false}, {"_id": "69782c96026bdf0473116e10", "user": {"_id": "66724ce47e7ff5d8bd069c7c", "avatarUrl": "/avatars/953f66585390dbdb202c1d7b7250d7bd.svg", "isPro": false, "fullname": "Zirui Tang", "user": "TerryTang", "type": "user"}, "name": "Zirui Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:49.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e11", "name": "Bin Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e12", "user": {"_id": "695612aabf3c8959a3a05f9c", "avatarUrl": "/avatars/c18885f6dea6f3ee019405cd8cf6f484.svg", "isPro": false, "fullname": "ShenTang990", "user": "shentang", "type": "user"}, "name": "Shen Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:56.579Z", "hidden": false}, {"_id": "69782c96026bdf0473116e13", "name": "Kai Zuo", "hidden": false}, {"_id": "69782c96026bdf0473116e14", "user": {"_id": "67efa8a2ed790a2e999dc216", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0S4lQCJX61uCF8EkSLMkk.png", "isPro": false, "fullname": "Yuyu Luo", "user": "luoyuyu", "type": "user"}, "name": "Yuyu Luo", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:02.233Z", "hidden": false}, {"_id": "69782c96026bdf0473116e15", "name": "Zhenzhe Zheng", "hidden": false}, {"_id": "69782c96026bdf0473116e16", "user": {"_id": "63f9fca8d4349b157a109eec", "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg", "isPro": false, "fullname": "Conghui He", "user": "conghui", "type": "user"}, "name": "Conghui He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:57:14.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e17", "name": "Jingren Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e18", "name": "Fan Wu", "hidden": false}], "publishedAt": "2026-01-22T12:02:45.000Z", "submittedOnDailyAt": "2026-01-27T00:42:38.464Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "submittedOnDailyBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "upvotes": 127, "discussionId": "69782c97026bdf0473116e19", "projectPage": "https://github.com/weAIDB/awesome-data-llm", "githubRepo": "https://github.com/weAIDB/awesome-data-llm", "githubRepoAddedBy": "user", "ai_summary": "LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.", "ai_keywords": ["data preparation", "large language models", "prompt-driven workflows", "agentic workflows", "data cleaning", "data integration", "data enrichment", "entity matching", "schema matching", "data annotation", "data profiling"], "githubStars": 644, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u6570\u636e\u51c6\u5907\u7684\u76ee\u7684\u662f\u53bb\u566a\u58f0\u3001\u53d1\u73b0\u8de8\u6570\u636e\u96c6\u7684\u5173\u7cfb\u548c\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u5bf9\u6570\u636e\u9a71\u52a8\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>\u7531\u4e8e\u5bf9\u5e94\u7528\u51c6\u5907\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\u548c\u5f3a\u5927\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6280\u672f\u7684\u51fa\u73b0\uff0cLLM\u589e\u5f3a\u7684\u65b9\u6cd5\u6b63\u5728\u6210\u4e3a\u6570\u636e\u51c6\u5907\u7684\u4e3b\u6d41\u3002</li>\n    <li>\u672c\u6587\u5bf9LLM\u6280\u672f\u5728\u6570\u636e\u51c6\u5907\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u5ba1\uff0c\u5173\u6ce8\u6570\u636e\u6e05\u7406\u3001\u6570\u636e\u6574\u5408\u548c\u6570\u636e\u4e30\u5bcc\u7b49\u4e3b\u8981\u4efb\u52a1\u3002</li>\n    <li>\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u8c03\u67e5\u4e86\u4ee3\u8868\u6027\u6280\u672f\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u4f18\u7f3a\u70b9\uff0c\u5982\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u4f46\u4e5f\u5b58\u5728\u6210\u672c\u9ad8\u548c\u8bc4\u4f30\u4e0d\u5339\u914d\u7b49\u95ee\u9898\u3002</li>\n    <li>\u8ba8\u8bba\u4e86\u5f00\u653e\u7684\u7814\u7a76\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u524d\u77bb\u6027\u8def\u7ebf\u56fe\uff0c\u5f3a\u8c03\u53ef\u6269\u5c55\u7684LLM-\u6570\u636e\u7cfb\u7edf\u548c\u53ef\u9760\u7684\u5de5\u4f5c\u6d41\u7a0b\u8bbe\u8ba1\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Data preparation is important for cleaning datasets and finding useful information for various applications like analytics and decision-making.</li>\n    <li>Recent advances in large language models (LLMs) and flexible data infrastructures are changing how data is prepared, moving from traditional methods to new, more adaptable approaches.</li>\n    <li>This paper reviews the use of LLM techniques for data preparation, focusing on three main tasks: data cleaning, data integration, and data enrichment.</li>\n    <li>It discusses the benefits and drawbacks of different methods, such as better understanding of data but challenges like high costs and issues with accuracy.</li>\n    <li>The paper also highlights ongoing challenges in the field and suggests future directions for improving data preparation systems with LLMs.</li>\n</ul>"}, "publishedAt": "2026-01-22T07:02:45.000Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17058.png", "numComments": 2, "submittedBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "fullname": "Wei Zhou", "name": "weizhoudb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.18418", "authors": [{"_id": "69785315026bdf0473116f6a", "user": {"_id": "62dce08bb2c60f29c3d0a5da", "avatarUrl": "/avatars/87ce03e61c4c6eb686c9491ef4fda225.svg", "isPro": false, "fullname": "Ji Zeng", "user": "stargazerzj", "type": "user"}, "name": "Ji Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T08:31:51.245Z", "hidden": false}, {"_id": "69785315026bdf0473116f6b", "name": "Dayuan Fu", "hidden": false}, {"_id": "69785315026bdf0473116f6c", "name": "Tiantian Mi", "hidden": false}, {"_id": "69785315026bdf0473116f6d", "name": "Yumin Zhuang", "hidden": false}, {"_id": "69785315026bdf0473116f6e", "user": {"_id": "6865e6b362fc5689c5e67733", "avatarUrl": "/avatars/186f3d248791d961b0a810d5225167cc.svg", "isPro": false, "fullname": "Yaxing Huang", "user": "Rookie-Noob-Newbie", "type": "user"}, "name": "Yaxing Huang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:30.220Z", "hidden": false}, {"_id": "69785315026bdf0473116f6f", "user": {"_id": "67638cc0d63e4b348e8a5fa3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67638cc0d63e4b348e8a5fa3/BZNlw1uTGUcumCrXKkerx.png", "isPro": false, "fullname": "Xuefeng Li", "user": "drxuefeng", "type": "user"}, "name": "Xuefeng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:37.248Z", "hidden": false}, {"_id": "69785315026bdf0473116f70", "name": "Lyumanshan Ye", "hidden": false}, {"_id": "69785315026bdf0473116f71", "name": "Muhang Xie", "hidden": false}, {"_id": "69785315026bdf0473116f72", "name": "Qishuo Hua", "hidden": false}, {"_id": "69785315026bdf0473116f73", "name": "Zhen Huang", "hidden": false}, {"_id": "69785315026bdf0473116f74", "user": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "isPro": false, "fullname": "Mohan Jiang (SII)", "user": "mhjiang0408", "type": "user"}, "name": "Mohan Jiang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:23.438Z", "hidden": false}, {"_id": "69785315026bdf0473116f75", "name": "Hanning Wang", "hidden": false}, {"_id": "69785315026bdf0473116f76", "user": {"_id": "66fa544c54f87b607fbffd2e", "avatarUrl": "/avatars/94195dcda0eb68e8fd20d80718744697.svg", "isPro": false, "fullname": "Jifan Lin", "user": "evanlin2570", "type": "user"}, "name": "Jifan Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:57.029Z", "hidden": false}, {"_id": "69785315026bdf0473116f77", "name": "Yang Xiao", "hidden": false}, {"_id": "69785315026bdf0473116f78", "name": "Jie Sun", "hidden": false}, {"_id": "69785315026bdf0473116f79", "user": {"_id": "684faf712acd915b5afc055f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/684faf712acd915b5afc055f/K7icmL08HxniWDgdph73i.jpeg", "isPro": false, "fullname": "Yunze Wu", "user": "wyzmike", "type": "user"}, "name": "Yunze Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:01:06.063Z", "hidden": false}, {"_id": "69785315026bdf0473116f7a", "name": "Pengfei Liu", "hidden": false}], "publishedAt": "2026-01-26T12:20:18.000Z", "submittedOnDailyAt": "2026-01-27T03:34:37.777Z", "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "submittedOnDailyBy": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "isPro": false, "fullname": "Mohan Jiang (SII)", "user": "mhjiang0408", "type": "user"}, "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...", "upvotes": 104, "discussionId": "69785315026bdf0473116f7b", "githubRepo": "https://github.com/GAIR-NLP/daVinci-Dev", "githubRepoAddedBy": "user", "ai_summary": "Agentic mid-training enables large language models to develop autonomous software engineering capabilities through specialized data synthesis techniques that bridge the gap between static training data and dynamic development environments.", "ai_keywords": ["Large Language Model", "agentic software engineering", "mid-training", "distribution mismatch", "agent-native data", "contextually-native trajectories", "environmentally-native trajectories", "SWE-Bench Verified", "Kimi-Dev", "resolution rates"], "githubStars": 22, "organization": {"_id": "630bc2d186b8b9904c33ce1b", "name": "GAIR", "fullname": "SII - GAIR", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u80fd\u529b\u6b63\u5728\u4ece\u5355\u6b21\u4ee3\u7801\u751f\u6210\u8f6c\u5411\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\u3002</li>\n    <li>\u4e2d\u671f\u8bad\u7ec3\uff08MT\uff09\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u8fdb\u884c\uff0c\u4ee5\u652f\u6301\u8fd9\u4e00\u81ea\u4e3b\u5f00\u53d1\u8fc7\u7a0b\uff0c\u4f46\u76ee\u524d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002</li>\n    <li>\u6709\u6548\u7684\u4e2d\u671f\u8bad\u7ec3\u9762\u4e34\u9759\u6001\u8bad\u7ec3\u6570\u636e\u4e0e\u52a8\u6001\u5f00\u53d1\u73af\u5883\u4e4b\u95f4\u7684\u5206\u5e03\u4e0d\u5339\u914d\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u7684\u4e2d\u671f\u8bad\u7ec3\u7814\u7a76\uff0c\u5efa\u7acb\u4e86\u6570\u636e\u5408\u6210\u539f\u5219\u548c\u8bad\u7ec3\u65b9\u6cd5\u3002</li>\n    <li>\u6a21\u578b\u5728`SWE-Bench Verified`\u4e0a\u9a8c\u8bc1\u4e86\u5176\u81ea\u4e3b\u80fd\u529b\uff0c\u5e76\u5728\u4e2d\u671f\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are evolving from simple code generation to more complex software engineering tasks where they can autonomously navigate and edit code.</li>\n    <li>Mid-training on large-scale data that simulates real-world software development is an underused method that could help develop these agentic behaviors more efficiently than traditional methods.</li>\n    <li>A key challenge is the difference between static training data and the dynamic environments where software is actually developed.</li>\n    <li>This study introduces a new method for agentic mid-training, focusing on two types of data: contextually-native (covering diverse experiences) and environmentally-native (from real tool usage and tests).</li>\n    <li>The results show that our models outperform previous methods with fewer tokens used, achieving notable resolution rates in software engineering tasks.</li>\n</ul>"}, "publishedAt": "2026-01-26T07:20:18.000Z", "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18418.png", "numComments": 2, "submittedBy": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "fullname": "Mohan Jiang (SII)", "name": "mhjiang0408", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "630bc2d186b8b9904c33ce1b", "name": "GAIR", "fullname": "SII - GAIR", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16746", "authors": [{"_id": "6976d4105d41524304c13517", "name": "Yuhang Wang", "hidden": false}, {"_id": "6976d4105d41524304c13518", "user": {"_id": "645b0c3ec35da9c7afd95421", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg", "isPro": false, "fullname": "Yuling", "user": "YerbaPage", "type": "user"}, "name": "Yuling Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:56.805Z", "hidden": false}, {"_id": "6976d4105d41524304c13519", "name": "Mo Yang", "hidden": false}, {"_id": "6976d4105d41524304c1351a", "name": "Rongrui Zhang", "hidden": false}, {"_id": "6976d4105d41524304c1351b", "name": "Shilin He", "hidden": false}, {"_id": "6976d4105d41524304c1351c", "name": "Heng Lian", "hidden": false}, {"_id": "6976d4105d41524304c1351d", "name": "Yuting Chen", "hidden": false}, {"_id": "6976d4105d41524304c1351e", "name": "Siyu Ye", "hidden": false}, {"_id": "6976d4105d41524304c1351f", "name": "Kai Cai", "hidden": false}, {"_id": "6976d4105d41524304c13520", "name": "Xiaodong Gu", "hidden": false}], "publishedAt": "2026-01-23T13:51:59.000Z", "submittedOnDailyAt": "2026-01-26T00:10:23.451Z", "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As a result, they frequently disrupt syntactic and logical structure and fail to retain critical implementation details. In this paper, we propose SWE-Pruner, a self-adaptive context pruning framework tailored for coding agents. Drawing inspiration from how human programmers \"selectively skim\" source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. Given the current task, the agent formulates an explicit goal (e.g., \"focus on error handling\") as a hint to guide the pruning targets. A lightweight neural skimmer (0.6B parameters) is trained to dynamically select relevant lines from the surrounding context given the goal. Evaluations across four benchmarks and multiple models validate SWE-Pruner's effectiveness in various scenarios, achieving 23-54% token reduction on agent tasks like SWE-Bench Verified and up to 14.84x compression on single-turn tasks like LongCodeQA with minimal performance impact.", "upvotes": 63, "discussionId": "6976d4105d41524304c13521", "githubRepo": "https://github.com/Ayanami1314/swe-pruner", "githubRepoAddedBy": "user", "ai_summary": "SWE-Pruner is a self-adaptive context pruning framework for coding agents that uses task-aware pruning to reduce token usage while maintaining performance.", "ai_keywords": ["context compression", "LongLLMLingua", "PPL", "code understanding", "task-aware adaptive pruning", "neural skimmer", "token reduction", "SWE-Bench Verified", "LongCodeQA"], "githubStars": 35, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>LLM\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u957f\u4ea4\u4e92\u4e0a\u4e0b\u6587\u5bfc\u81f4\u9ad8API\u6210\u672c\u548c\u5ef6\u8fdf\u3002</li>\n    <li>\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6cd5\uff08\u5982LongLLMLingua\uff09\u5e38\u5e38\u5ffd\u89c6\u4ee3\u7801\u7406\u89e3\u7684\u4efb\u52a1\u7279\u6027\uff0c\u6613\u7834\u574f\u8bed\u6cd5\u548c\u903b\u8f91\u7ed3\u6784\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86SWE-Pruner\uff0c\u4e00\u4e2a\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u4fee\u526a\u6846\u67b6\uff0c\u4e13\u4e3a\u7f16\u7801\u4ee3\u7406\u8bbe\u8ba1\u3002</li>\n    <li>SWE-Pruner\u901a\u8fc7\u8bbe\u5b9a\u660e\u786e\u7684\u76ee\u6807\u6765\u6307\u5bfc\u4e0a\u4e0b\u6587\u4fee\u526a\uff0c\u7c7b\u4f3c\u4eba\u7c7b\u7a0b\u5e8f\u5458\u5728\u5f00\u53d1\u65f6\u7684\u9009\u62e9\u6027\u6d4f\u89c8\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cSWE-Pruner\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\uff0c\u5b9e\u73b0\u4e8623-54%\u7684\u4ee4\u724c\u51cf\u5c11\uff0c\u4e14\u5bf9\u6027\u80fd\u5f71\u54cd\u6700\u5c0f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LLM agents are good at software development but struggle with long interactions due to high costs and delays.</li>\n    <li>Current solutions, like LongLLMLingua, often miss important coding details and disrupt code structure.</li>\n    <li>The paper introduces SWE-Pruner, a new method for efficiently managing long code contexts by focusing on what's important for the task.</li>\n    <li>SWE-Pruner allows the agent to set specific goals to guide which parts of the code to focus on, similar to how human programmers work.</li>\n    <li>Tests show SWE-Pruner can reduce the amount of information needed by 23-54% without hurting performance on coding tasks.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:51:59.000Z", "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents", "summary": "LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As a result, they frequently disrupt syntactic and logical structure and fail to retain critical implementation details. In this paper, we propose SWE-Pruner, a self-adaptive context pruning framework tailored for coding agents. Drawing inspiration from how human programmers \"selectively skim\" source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. Given the current task, the agent formulates an explicit goal (e.g., \"focus on error handling\") as a hint to guide the pruning targets. A lightweight neural skimmer (0.6B parameters) is trained to dynamically select relevant lines from the surrounding context given the goal. Evaluations across four benchmarks and multiple models validate SWE-Pruner's effectiveness in various scenarios, achieving 23-54% token reduction on agent tasks like SWE-Bench Verified and up to 14.84x compression on single-turn tasks like LongCodeQA with minimal performance impact.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16746.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.15876", "authors": [{"_id": "6972d8d5fb12c92b735b73a2", "name": "Taofeng Xue", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a3", "user": {"_id": "6801cbd5f06f2d08f3fd5455", "avatarUrl": "/avatars/c728b48f6938c0d7cb6b14011927ede8.svg", "isPro": false, "fullname": "chong.peng", "user": "KleinChong", "type": "user"}, "name": "Chong Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T20:15:18.698Z", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a4", "name": "Mianqiu Huang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a5", "name": "Linsen Guo", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a6", "user": {"_id": "6764279e684ed3b61b2316a4", "avatarUrl": "/avatars/63a6b6c3b9f442b42480679425951187.svg", "isPro": false, "fullname": "SII-TianchengHAN", "user": "GenSouKai", "type": "user"}, "name": "Tiancheng Han", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:30.106Z", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a7", "name": "Haozhe Wang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a8", "name": "Jianing Wang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a9", "name": "Xiaocheng Zhang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73aa", "name": "Xin Yang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ab", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ac", "name": "Jinrui Ding", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ad", "name": "Xiandi Ma", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ae", "name": "Yuchen Xie", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73af", "name": "Peng Pei", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73b0", "name": "Xunliang Cai", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73b1", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-01-22T11:36:43.000Z", "submittedOnDailyAt": "2026-01-23T07:54:00.525Z", "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience", "submittedOnDailyBy": {"_id": "6459c7c10aba070266e41bb1", "avatarUrl": "/avatars/2178cac69cf4123db5e85191160f3795.svg", "isPro": false, "fullname": "mqhuang", "user": "LutherXD", "type": "user"}, "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.", "upvotes": 62, "discussionId": "6972d8d5fb12c92b735b73b2", "ai_summary": "EvoCUA introduces an evolutionary approach to computer-use agents that combines autonomous task generation with policy optimization to achieve superior performance in complex, long-horizon tasks.", "ai_keywords": ["computer-use agents", "native computer-use agents", "data generation", "policy optimization", "evolutionary cycle", "verifiable synthesis engine", "executable validators", "sandbox rollouts", "iterative evolving learning", "capability boundaries", "error analysis", "self-correction", "OSWorld benchmark", "foundation models"], "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u63d0\u51fa\u4e86EvoCUA\uff0c\u4e00\u4e2a\u65b0\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u6a21\u578b\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u9759\u6001\u6a21\u4eff\u65b9\u6cd5\u3002</li>\n    <li>EvoCUA\u901a\u8fc7\u81ea\u6211\u751f\u6210\u6570\u636e\u548c\u4f18\u5316\u7b56\u7565\uff0c\u5f62\u6210\u4e00\u4e2a\u81ea\u6211\u7ef4\u6301\u7684\u8fdb\u5316\u5faa\u73af\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002</li>\n    <li>\u8be5\u6a21\u578b\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u80fd\u591f\u8fdb\u884c\u6570\u4e07\u4e2a\u5f02\u6b65\u6c99\u76d2\u6d4b\u8bd5\uff0c\u4ece\u800c\u83b7\u53d6\u5927\u91cf\u7ecf\u9a8c\u3002</li>\n    <li>EvoCUA\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f9756.7%\u7684\u6210\u529f\u7387\uff0c\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u6700\u4f73\u5f00\u6e90\u6a21\u578bOpenCUA-72B\u548c\u5176\u4ed6\u9886\u5148\u6a21\u578b\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u57fa\u4e8e\u7ecf\u9a8c\u5b66\u4e60\u7684\u8fdb\u5316\u8303\u5f0f\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u57fa\u7840\u6a21\u578b\u4e2d\u90fd\u80fd\u53d6\u5f97\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>EvoCUA is a new type of computer-use agent that improves multimodal AI by overcoming limits of static data.</li>\n    <li>It uses a self-sustaining cycle of data generation and learning, unlike previous models that just mimic static datasets.</li>\n    <li>A special engine creates diverse tasks and checks their execution to help with data scarcity.</li>\n    <li>EvoCUA has been tested and shows better success rates than previous models, achieving 56.7% on the OSWorld benchmark.</li>\n    <li>This approach is adaptable and works well across different sizes of foundational models, making it a promising development for AI capabilities.</li>\n</ul>"}, "publishedAt": "2026-01-22T06:36:43.000Z", "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience", "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15876.png", "numComments": 1, "submittedBy": {"_id": "6459c7c10aba070266e41bb1", "avatarUrl": "/avatars/2178cac69cf4123db5e85191160f3795.svg", "fullname": "mqhuang", "name": "LutherXD", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.15165", "authors": [{"_id": "6971933ac1c7409747bf9597", "name": "Zanlin Ni", "hidden": false}, {"_id": "6971933ac1c7409747bf9598", "user": {"_id": "6486dde1f74857df3f1a5828", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg", "isPro": false, "fullname": "Shenzhi Wang", "user": "shenzhi-wang", "type": "user"}, "name": "Shenzhi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:54.254Z", "hidden": false}, {"_id": "6971933ac1c7409747bf9599", "name": "Yang Yue", "hidden": false}, {"_id": "6971933ac1c7409747bf959a", "name": "Tianyu Yu", "hidden": false}, {"_id": "6971933ac1c7409747bf959b", "name": "Weilin Zhao", "hidden": false}, {"_id": "6971933ac1c7409747bf959c", "name": "Yeguo Hua", "hidden": false}, {"_id": "6971933ac1c7409747bf959d", "name": "Tianyi Chen", "hidden": false}, {"_id": "6971933ac1c7409747bf959e", "name": "Jun Song", "hidden": false}, {"_id": "6971933ac1c7409747bf959f", "name": "Cheng Yu", "hidden": false}, {"_id": "6971933ac1c7409747bf95a0", "name": "Bo Zheng", "hidden": false}, {"_id": "6971933ac1c7409747bf95a1", "name": "Gao Huang", "hidden": false}], "publishedAt": "2026-01-21T16:41:58.000Z", "submittedOnDailyAt": "2026-01-23T00:11:51.141Z", "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models", "submittedOnDailyBy": {"_id": "63987ffb2ceb55aabe0852f3", "avatarUrl": "/avatars/343b796ff6b8906203904e8c620d7eb5.svg", "isPro": false, "fullname": "Zanlin Ni", "user": "nzl-thu", "type": "user"}, "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap", "upvotes": 55, "discussionId": "6971933ac1c7409747bf95a2", "projectPage": "https://nzl-thu.github.io/the-flexibility-trap", "githubRepo": "https://github.com/LeapLabTHU/JustGRPO", "githubRepoAddedBy": "user", "ai_summary": "Arbitrary order generation in diffusion large language models limits reasoning capability by causing premature solution space collapse, making standard policy optimization more effective.", "ai_keywords": ["diffusion large language models", "left-to-right constraint", "token generation", "reinforcement learning", "reasoning potential", "mathematical reasoning", "coding tasks", "combinatorial trajectories", "likelihoods", "Group Relative Policy Optimization", "GRPO", "parallel decoding"], "githubStars": 66, "organization": {"_id": "69719700e3846c07669d13ee", "name": "Tsinghua-LeapLab", "fullname": "Tsinghua-LeapLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63987ffb2ceb55aabe0852f3/hflTWNTGxeJx83xNkYrDB.png"}, "summary_zh": "<ul>\n    <li>\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u6253\u7834\u4e86\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u7684\u5de6\u5230\u53f3\u751f\u6210\u9650\u5236\uff0c\u53ef\u4ee5\u4ee5\u4efb\u610f\u987a\u5e8f\u751f\u6210\u6807\u8bb0\u3002</li>\n    <li>\u8fd9\u79cd\u7075\u6d3b\u6027\u539f\u672c\u5e94\u8be5\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5b9e\u9645\u4e0a\u9650\u5236\u4e86dLLMs\u7684\u63a8\u7406\u8fb9\u754c\u3002</li>\n    <li>dLLMs\u503e\u5411\u4e8e\u5229\u7528\u987a\u5e8f\u7075\u6d3b\u6027\u6765\u89c4\u907f\u9ad8\u4e0d\u786e\u5b9a\u6027\u7684\u6807\u8bb0\uff0c\u5bfc\u81f4\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u8fc7\u65e9\u5d29\u6e83\u3002</li>\n    <li>\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5904\u7406\u8fd9\u79cd\u7075\u6d3b\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u7684JustGRPO\u65b9\u6cd5\u5728\u4fdd\u7559dLLMs\u5e76\u884c\u89e3\u7801\u80fd\u529b\u7684\u540c\u65f6\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u4fc3\u8fdb\u63a8\u7406\uff0c\u53d6\u5f97\u4e8689.1%\u7684\u51c6\u786e\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion Large Language Models (dLLMs) can generate text in any order, unlike traditional models that generate it left-to-right.</li>\n    <li>This flexibility might suggest better reasoning abilities, especially for tasks like mathematics and coding.</li>\n    <li>However, the study shows that this ability to generate in any order actually limits the reasoning capabilities of dLLMs.</li>\n    <li>dLLMs often avoid difficult tokens that are important for problem-solving, which can lead to less effective reasoning.</li>\n    <li>The researchers propose a simpler method called JustGRPO that improves reasoning without losing the benefits of the dLLMs' flexible structure.</li>\n</ul>"}, "publishedAt": "2026-01-21T11:41:58.000Z", "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models", "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15165.png", "numComments": 1, "submittedBy": {"_id": "63987ffb2ceb55aabe0852f3", "avatarUrl": "/avatars/343b796ff6b8906203904e8c620d7eb5.svg", "fullname": "Zanlin Ni", "name": "nzl-thu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "69719700e3846c07669d13ee", "name": "Tsinghua-LeapLab", "fullname": "Tsinghua-LeapLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63987ffb2ceb55aabe0852f3/hflTWNTGxeJx83xNkYrDB.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.14724", "authors": [{"_id": "6972ee7afb12c92b735b74b4", "user": {"_id": "637169557a5e5d8efdc3e58e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg", "isPro": false, "fullname": "Haowei Zhang", "user": "freesky", "type": "user"}, "name": "Haowei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:23.639Z", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b5", "name": "Shudong Yang", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b6", "name": "Jinlan Fu", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b7", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b8", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-01-21T07:26:15.000Z", "submittedOnDailyAt": "2026-01-23T01:43:37.582Z", "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding", "submittedOnDailyBy": {"_id": "637169557a5e5d8efdc3e58e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg", "isPro": false, "fullname": "Haowei Zhang", "user": "freesky", "type": "user"}, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10times faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.", "upvotes": 52, "discussionId": "6972ee7bfb12c92b735b74b9", "projectPage": "https://hermes-streaming.github.io/", "githubRepo": "https://github.com/haowei-freesky/HERMES", "githubRepoAddedBy": "user", "ai_summary": "HERMES is a training-free architecture that enables real-time video stream understanding by utilizing a hierarchical memory framework based on KV cache reuse, achieving faster response times and maintained accuracy even with reduced video token input.", "ai_keywords": ["Multimodal Large Language Models", "video understanding", "streaming video inputs", "real-time responses", "KV cache", "hierarchical memory framework", "mechanistic attention", "video tokens", "TTFT"], "githubStars": 24, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u79bb\u7ebf\u89c6\u9891\u7406\u89e3\u65b9\u9762\u6709\u4e86\u663e\u8457\u8fdb\u5c55\u3002</li>\n    <li>\u5c06\u8fd9\u4e9b\u80fd\u529b\u6269\u5c55\u5230\u6d41\u89c6\u9891\u8f93\u5165\u4ecd\u7136\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u7a33\u5b9a\u6027\u3001\u5b9e\u65f6\u54cd\u5e94\u548c\u4f4eGPU\u5185\u5b58\u4f7f\u7528\u65b9\u9762\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HERMES\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u8bad\u7ec3\u67b6\u6784\uff0c\u65e8\u5728\u5b9e\u73b0\u5b9e\u65f6\u548c\u51c6\u786e\u7684\u89c6\u9891\u6d41\u7406\u89e3\u3002</li>\n    <li>HERMES\u5229\u7528\u7d27\u51d1\u7684KV\u7f13\u5b58\uff0c\u5728\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u9ad8\u6548\u7406\u89e3\u89c6\u9891\u6d41\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u786e\u4fdd\u5b9e\u65f6\u54cd\u5e94\u3002</li>\n    <li>\u4e0e\u5148\u524d\u7684\u6280\u672f\u76f8\u6bd4\uff0cHERMES\u5728\u5904\u7406\u901f\u5ea6\u4e0a\u63d0\u9ad8\u4e8610\u500d\uff0c\u5e76\u5728\u89c6\u9891\u4ee4\u724c\u51cf\u5c11\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent developments in Multimodal Large Language Models (MLLMs) have improved video understanding, but real-time streaming video analysis is still difficult.</li>\n    <li>HERMES is a new architecture designed to understand video streams accurately and in real-time without needing additional training.</li>\n    <li>It uses a special memory system called KV cache to efficiently manage video information and reduce the need for GPU resources.</li>\n    <li>HERMES can respond to user queries instantly and is 10 times faster than previous models in processing time for video streams.</li>\n    <li>Even with fewer video tokens, HERMES maintains or improves accuracy on various benchmarks, showing significant gains on streaming datasets.</li>\n</ul>"}, "publishedAt": "2026-01-21T02:26:15.000Z", "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10times faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14724.png", "numComments": 2, "submittedBy": {"_id": "637169557a5e5d8efdc3e58e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg", "fullname": "Haowei Zhang", "name": "freesky", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.15197", "authors": [{"_id": "6971c608c1c7409747bf96a5", "user": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "isPro": false, "fullname": "Shijie Lian", "user": "LiamLian0727", "type": "user"}, "name": "Shijie Lian", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:44:25.547Z", "hidden": false}, {"_id": "6971c608c1c7409747bf96a6", "name": "Bin Yu", "hidden": false}, {"_id": "6971c608c1c7409747bf96a7", "name": "Xiaopeng Lin", "hidden": false}, {"_id": "6971c608c1c7409747bf96a8", "name": "Laurence T. Yang", "hidden": false}, {"_id": "6971c608c1c7409747bf96a9", "name": "Zhaolong Shen", "hidden": false}, {"_id": "6971c608c1c7409747bf96aa", "name": "Changti Wu", "hidden": false}, {"_id": "6971c608c1c7409747bf96ab", "name": "Yuzhuo Miao", "hidden": false}, {"_id": "6971c608c1c7409747bf96ac", "name": "Cong Huang", "hidden": false}, {"_id": "6971c608c1c7409747bf96ad", "name": "Kai Chen", "hidden": false}], "publishedAt": "2026-01-21T17:15:22.000Z", "submittedOnDailyAt": "2026-01-23T00:45:20.588Z", "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries", "submittedOnDailyBy": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "isPro": false, "fullname": "Shijie Lian", "user": "LiamLian0727", "type": "user"}, "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior \u03c0(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.", "upvotes": 50, "discussionId": "6971c609c1c7409747bf96ae", "projectPage": "https://github.com/ZGC-EmbodyAI/BayesianVLA", "githubRepo": "https://github.com/ZGC-EmbodyAI/BayesianVLA", "githubRepoAddedBy": "user", "ai_summary": "BayesianVLA addresses language-action grounding issues in robot manipulation by using Bayesian decomposition to prevent information collapse and improve out-of-distribution generalization.", "ai_keywords": ["Vision-Language-Action models", "Information Collapse", "Bayesian decomposition", "latent action queries", "conditional Pointwise Mutual Information", "vision-only policies", "out-of-distribution generalization", "SimplerEnv", "RoboCasa"], "githubStars": 11, "organization": {"_id": "68896d3a716ee5bfb1428441", "name": "ZGCA", "fullname": "Zhongguancun Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\uff08VLA\uff09\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u65b0\u6307\u4ee4\u548c\u590d\u6742\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002</li>\n    <li>\u5f53\u524d\u7684\u8bad\u7ec3\u65b9\u5f0f\u5bfc\u81f4\u6570\u636e\u96c6\u504f\u5dee\uff0c\u9020\u6210\u6307\u4ee4\u4e0e\u89c6\u89c9\u89c2\u5bdf\u4e4b\u95f4\u7684\u9ad8\u53ef\u9884\u6d4b\u6027\uff0c\u5bfc\u81f4\u4fe1\u606f\u6d88\u5931\u73b0\u8c61\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86BayesianVLA\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u5206\u89e3\u6765\u5f3a\u5316\u6307\u4ee4\u9075\u5faa\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u6f5c\u5728\u884c\u52a8\u67e5\u8be2\uff0c\u6784\u5efa\u53cc\u5206\u652f\u67b6\u6784\u6765\u4f30\u8ba1\u89c6\u89c9\u4f18\u5148\u548c\u8bed\u8a00\u6761\u4ef6\u4e0b\u7684\u540e\u9a8c\u3002</li>\n    <li>BayesianVLA\u5728\u4e0d\u9700\u8981\u65b0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5305\u62ec\u5728\u56f0\u96be\u7684OOD\u57fa\u51c6\u4e0a\u63d0\u5347\u4e8611.3%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current Vision-Language-Action (VLA) models have difficulty handling new instructions and complex tasks.</li>\n    <li>A problem called \"Information Collapse\" occurs when datasets are biased, making language instructions predictable from visual data alone.</li>\n    <li>This leads to models relying only on visual data and ignoring language, resulting in poor performance on new tasks.</li>\n    <li>The proposed solution, BayesianVLA, uses a new framework that helps models better follow language instructions by using a dual-branch system.</li>\n    <li>BayesianVLA improves generalization without needing new data, showing significant performance gains in experiments.</li>\n</ul>"}, "publishedAt": "2026-01-21T12:15:22.000Z", "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries", "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior \u03c0(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15197.png", "numComments": 2, "submittedBy": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "fullname": "Shijie Lian", "name": "LiamLian0727", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "68896d3a716ee5bfb1428441", "name": "ZGCA", "fullname": "Zhongguancun Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16206", "authors": [{"_id": "6972e04dfb12c92b735b73cf", "user": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "isPro": false, "fullname": "Daixuan Cheng", "user": "daixuancheng", "type": "user"}, "name": "Daixuan Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:28.070Z", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d0", "name": "Shaohan Huang", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d1", "name": "Yuxian Gu", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d2", "name": "Huatong Song", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d3", "name": "Guoxin Chen", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d4", "name": "Li Dong", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d5", "name": "Wayne Xin Zhao", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d6", "name": "Ji-Rong Wen", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d7", "name": "Furu Wei", "hidden": false}], "publishedAt": "2026-01-22T18:57:09.000Z", "submittedOnDailyAt": "2026-01-23T00:27:12.305Z", "title": "LLM-in-Sandbox Elicits General Agentic Intelligence", "submittedOnDailyBy": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "isPro": false, "fullname": "Daixuan Cheng", "user": "daixuancheng", "type": "user"}, "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.", "upvotes": 46, "discussionId": "6972e04dfb12c92b735b73d8", "projectPage": "https://llm-in-sandbox.github.io", "githubRepo": "https://github.com/llm-in-sandbox/llm-in-sandbox", "githubRepoAddedBy": "user", "ai_summary": "LLM-in-Sandbox enables large language models to perform general intelligence tasks across diverse domains by allowing them to explore a code sandbox environment, achieving robust generalization without additional training.", "ai_keywords": ["LLM-in-Sandbox", "code sandbox", "virtual computer", "reinforcement learning", "non-agentic data", "sandbox exploration", "general intelligence", "long-context understanding", "instruction following"], "githubStars": 38, "organization": {"_id": "68151d0f51add3813f3f7d1b", "name": "MicrosoftResearch", "fullname": "Microsoft Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86LLM-in-Sandbox\uff0c\u5141\u8bb8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u6c99\u76d2\u4e2d\u63a2\u7d22\uff0c\u4ece\u800c\u5c55\u73b0\u51fa\u4e00\u822c\u667a\u80fd\u7684\u80fd\u529b\u3002</li>\n    <li>\u5f3a\u5927\u7684\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u5728\u6ca1\u6709\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u4ee3\u7801\u6c99\u76d2\u5904\u7406\u975e\u4ee3\u7801\u4efb\u52a1\uff0c\u5982\u83b7\u53d6\u65b0\u77e5\u8bc6\u548c\u6267\u884c\u811a\u672c\u3002</li>\n    <li>\u901a\u8fc7LLM-in-Sandbox\u5f3a\u5316\u5b66\u4e60\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u8fd9\u4e9b\u6a21\u578b\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u4f7f\u7528\u975e\u667a\u80fd\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cLLM-in-Sandbox\u5728\u591a\u4e2a\u9886\u57df\uff08\u5982\u6570\u5b66\u3001\u7269\u7406\u3001\u5316\u5b66\u3001\u751f\u7269\u533b\u5b66\u7b49\uff09\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n    <li>\u5206\u6790\u4e86LLM-in-Sandbox\u7684\u8ba1\u7b97\u548c\u7cfb\u7edf\u6548\u7387\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3aPython\u5305\u5f00\u6e90\uff0c\u4fbf\u4e8e\u5b9e\u9645\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LLM-in-Sandbox allows language models (LLMs) to work in a safe virtual environment to develop general intelligence beyond just coding tasks.</li>\n    <li>These LLMs can use the sandbox to access new information, manage long texts, and run scripts without needing extra training.</li>\n    <li>With additional training through LLM-in-Sandbox Reinforcement Learning, the models can better explore and perform tasks in the sandbox.</li>\n    <li>Experiments show that LLM-in-Sandbox can effectively handle various subjects like math, science, and understanding instructions.</li>\n    <li>The system is efficient and has been made available as a Python package for easier use in real-world applications.</li>\n</ul>"}, "publishedAt": "2026-01-22T13:57:09.000Z", "title": "LLM-in-Sandbox Elicits General Agentic Intelligence", "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16206.png", "numComments": 2, "submittedBy": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "fullname": "Daixuan Cheng", "name": "daixuancheng", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 16, "isUserFollowing": false}, "organization": {"_id": "68151d0f51add3813f3f7d1b", "name": "MicrosoftResearch", "fullname": "Microsoft Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.17737", "authors": [{"_id": "6978310b026bdf0473116e44", "user": {"_id": "64545c77a7ce0a8fde809912", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VDaMEM77Xv09dP6B5v3sK.jpeg", "isPro": false, "fullname": "ChenYuMu", "user": "ChenYuMu", "type": "user"}, "name": "Chenyu Mu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:01:21.462Z", "hidden": false}, {"_id": "6978310b026bdf0473116e45", "user": {"_id": "6527a2df1eb78901534b0cc6", "avatarUrl": "/avatars/f811d8c108930b41e2612c609d35e2eb.svg", "isPro": false, "fullname": "Xin He", "user": "Kleinhe", "type": "user"}, "name": "Xin He", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:03:20.414Z", "hidden": false}, {"_id": "6978310b026bdf0473116e46", "user": {"_id": "64300415b009240418dac70c", "avatarUrl": "/avatars/5175cdbc7683b0b52d5c742e93d3be83.svg", "isPro": false, "fullname": "Qu Yang", "user": "quyang22", "type": "user"}, "name": "Qu Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:03:22.475Z", "hidden": false}, {"_id": "6978310b026bdf0473116e47", "name": "Wanshun Chen", "hidden": false}, {"_id": "6978310b026bdf0473116e48", "name": "Jiadi Yao", "hidden": false}, {"_id": "6978310b026bdf0473116e49", "name": "Huang Liu", "hidden": false}, {"_id": "6978310b026bdf0473116e4a", "name": "Zihao Yi", "hidden": false}, {"_id": "6978310b026bdf0473116e4b", "name": "Bo Zhao", "hidden": false}, {"_id": "6978310b026bdf0473116e4c", "name": "Xingyu Chen", "hidden": false}, {"_id": "6978310b026bdf0473116e4d", "name": "Ruotian Ma", "hidden": false}, {"_id": "6978310b026bdf0473116e4e", "name": "Fanghua Ye", "hidden": false}, {"_id": "6978310b026bdf0473116e4f", "name": "Erkun Yang", "hidden": false}, {"_id": "6978310b026bdf0473116e50", "name": "Cheng Deng", "hidden": false}, {"_id": "6978310b026bdf0473116e51", "name": "Zhaopeng Tu", "hidden": false}, {"_id": "6978310b026bdf0473116e52", "name": "Xiaolong Li", "hidden": false}, {"_id": "6978310b026bdf0473116e53", "name": "Linus", "hidden": false}], "publishedAt": "2026-01-25T08:10:28.000Z", "submittedOnDailyAt": "2026-01-27T01:05:46.612Z", "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation", "submittedOnDailyBy": {"_id": "67485743561b1e6f9579389f", "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg", "isPro": false, "fullname": "Zhaopeng Tu", "user": "zptu", "type": "user"}, "summary": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.", "upvotes": 46, "discussionId": "6978310b026bdf0473116e54", "projectPage": "https://xd-mu.github.io/ScriptIsAllYouNeed/", "githubRepo": "https://github.com/Tencent/digitalhuman/tree/main/ScriptAgent", "githubRepoAddedBy": "user", "ai_summary": "A novel end-to-end agentic framework translates dialogue into cinematic videos through specialized agents that generate and orchestrate video content while maintaining narrative coherence.", "ai_keywords": ["video generation", "dialogue-to-cinematic-video", "ScripterAgent", "DirectorAgent", "cross-scene continuous generation", "ScriptBench", "Visual-Script Alignment", "CriticAgent"], "githubStars": 228, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "summary_zh": "<ul>\n    <li>\u6700\u65b0\u7684\u89c6\u9891\u751f\u6210\u6280\u672f\u53ef\u4ee5\u6839\u636e\u7b80\u5355\u7684\u6587\u5b57\u63d0\u793a\u751f\u6210\u7cbe\u5f69\u7684\u89c6\u89c9\u5185\u5bb9\uff0c\u4f46\u5728\u751f\u6210\u8fde\u8d2f\u7684\u957f\u7bc7\u5bf9\u8bdd\u65f6\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u8bdd\u5230\u7535\u5f71\u89c6\u9891\u751f\u6210\u7684\u6846\u67b6\uff0c\u540d\u4e3aScripterAgent\u3002</li>\n    <li>ScripterAgent\u80fd\u591f\u5c06\u7c97\u7565\u7684\u5bf9\u8bdd\u8f6c\u5316\u4e3a\u8be6\u7ec6\u7684\u53ef\u6267\u884c\u7535\u5f71\u5267\u672c\u3002</li>\n    <li>\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6ScriptBench\uff0c\u63d0\u4f9b\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\uff0c\u4ee5\u652f\u6301\u5267\u672c\u751f\u6210\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u5267\u672c\u7684\u771f\u5b9e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u89c6\u89c9\u6548\u679c\u4e0e\u5267\u672c\u9075\u5faa\u4e4b\u95f4\u7684\u6743\u8861\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New models can create impressive videos from text, but struggle with long, coherent stories.</li>\n    <li>We propose a new system to improve how dialogue turns into cinematic videos.</li>\n    <li>The system includes ScripterAgent, which converts rough dialogue into detailed scripts.</li>\n    <li>We created ScriptBench, a large set of data to help train and evaluate the model.</li>\n    <li>Our evaluation shows better script accuracy and video continuity, highlighting trade-offs in current video models.</li>\n</ul>"}, "publishedAt": "2026-01-25T03:10:28.000Z", "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation", "summary": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17737.png", "numComments": 3, "submittedBy": {"_id": "67485743561b1e6f9579389f", "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg", "fullname": "Zhaopeng Tu", "name": "zptu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u73b0\u5b9e\u4e16\u754c\u7684\u89c6\u9891\u95ee\u7b54\u5e38\u5e38\u53ea\u63d0\u4f9b\u5c40\u90e8\u89c6\u89c9\u7ebf\u7d22\uff0c\u800c\u53ef\u9a8c\u8bc1\u7684\u7b54\u6848\u5206\u6563\u5728\u7f51\u7edc\u4e0a\u3002</li>\n    <li>\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0cVideoDR\u3002</li>\n    <li>VideoDR \u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u4e0b\u7684\u5f00\u653e\u9886\u57df\u89c6\u9891\u95ee\u7b54\uff0c\u8981\u6c42\u63d0\u53d6\u8de8\u5e27\u89c6\u89c9\u951a\u70b9\u3001\u4ea4\u4e92\u5f0f\u7f51\u7edc\u68c0\u7d22\u548c\u591a\u8df3\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u201cAgentic\u201d\u65b9\u6cd5\u4e0d\u4e00\u5b9a\u4f18\u4e8e\u201cWorkflow\u201d\uff0c\u5176\u6548\u679c\u53d6\u51b3\u4e8e\u6a21\u578b\u5728\u957f\u68c0\u7d22\u94fe\u4e2d\u4fdd\u6301\u521d\u59cb\u89c6\u9891\u951a\u70b9\u7684\u80fd\u529b\u3002</li>\n    <li>VideoDR \u4e3a\u7814\u7a76\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u89c6\u9891\u4ee3\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u4e0b\u4e00\u4ee3\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video question answering often requires understanding both video content and information from the web.</li>\n    <li>The VideoDR benchmark was created to help improve video question answering by focusing on visual clues, web retrieval, and reasoning.</li>\n    <li>It includes high-quality samples from various subjects, thanks to careful human annotation.</li>\n    <li>Tests on different large language models showed mixed results between two approaches, highlighting challenges in keeping track of information over long retrieval processes.</li>\n    <li>VideoDR helps identify key issues for future improvements in video question answering technology.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u76ee\u524d\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u7406\u89e3\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u6765\u5f25\u8865\u8fd9\u4e00\u7f3a\u9677\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u6700\u65b0\u7684MLLMs\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u751a\u81f3\u4e09\u5c81\u7684\u5b69\u5b50\u4e5f\u80fd\u8f7b\u677e\u5b8c\u6210\u8fd9\u4e9b\u4efb\u52a1\u3002</li>\n    <li>\u4e3a\u8bc4\u4f30MLLMs\u7684\u6838\u5fc3\u89c6\u89c9\u80fd\u529b\uff0c\u7814\u7a76\u56e2\u961f\u63a8\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aBabyVision\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6388\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a22\u7c7b\u3002</li>\n    <li>\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0c\u9886\u5148\u7684MLLMs\uff08\u5982Gemini3-Pro-Preview\uff09\u5f97\u5206\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u5e73\u5747\u6c34\u5e73\uff0c\u8868\u73b0\u4e0d\u59826\u5c81\u7684\u5b69\u5b50\u3002</li>\n    <li>BabyVision\u7684\u8fdb\u5c55\u4e3a\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7684\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u540c\u65f6\u63d0\u51fa\u4e86BabyVision-Gen\u548c\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans develop visual skills before learning language, but MLLMs still depend on language for understanding visuals.</li>\n    <li>Current MLLMs struggle with basic visual tasks that even young children can do easily.</li>\n    <li>BabyVision is a new benchmark created to test visual skills of MLLMs without using language.</li>\n    <li>BabyVision includes 388 tasks in 22 subclasses across four main areas.</li>\n    <li>Results show MLLMs like Gemini3-Pro-Preview score much lower than humans, indicating they lack basic visual understanding.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u542b\u6709\u4e30\u5bcc\u7684\u8bed\u4e49\u5b9e\u4f53\uff0c\u536b\u661f\u56fe\u50cf\u4e2d\u5206\u5272\u8fd9\u4e9b\u5b9e\u4f53\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u76ee\u524d\u7684\u5206\u5272\u6a21\u578b\u80fd\u591f\u5904\u7406\u7269\u7406\u5c5e\u6027\u7684\u5b9e\u4f53\uff0c\u4f46\u5bf9\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u672c\u7814\u7a76\u5f15\u5165\u4e86\u540d\u4e3aSocioSeg\u7684\u57ce\u5e02\u793e\u4f1a\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u6846\u67b6SocioReasoner\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u8bc6\u522b\u548c\u591a\u9636\u6bb5\u63a8\u7406\u6765\u8bc6\u522b\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5177\u6709\u5f88\u5f3a\u7684\u96f6-shot \u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban surfaces contain many important social and physical features that need to be identified from satellite images.</li>\n    <li>Current models can identify physical features well but struggle with social categories like schools and parks.</li>\n    <li>The study introduces a new dataset called SocioSeg, which includes satellite images and labels for social entities.</li>\n    <li>A new framework called SocioReasoner is proposed, which uses vision and language together to identify these social entities more effectively.</li>\n    <li>Experiments show that this new approach performs better than existing models and can generalize well to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.16725", "authors": [{"_id": "6976d5405d41524304c13537", "name": "Meituan LongCat Team", "hidden": false}, {"_id": "6976d5405d41524304c13538", "name": "Anchun Gui", "hidden": false}, {"_id": "6976d5405d41524304c13539", "name": "Bei Li", "hidden": false}, {"_id": "6976d5405d41524304c1353a", "name": "Bingyang Tao", "hidden": false}, {"_id": "6976d5405d41524304c1353b", "name": "Bole Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1353c", "name": "Borun Chen", "hidden": false}, {"_id": "6976d5405d41524304c1353e", "name": "Chao Zhang", "hidden": false}, {"_id": "69772bc15d41524304c13739", "name": "Chao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1353f", "name": "Chen Gao", "hidden": false}, {"_id": "6976d5405d41524304c13540", "name": "Chen Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13541", "name": "Chengcheng Han", "hidden": false}, {"_id": "6976d5405d41524304c13542", "name": "Chenhui Yang", "hidden": false}, {"_id": "6976d5405d41524304c13543", "name": "Chuyu Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13544", "name": "Cong Chen", "hidden": false}, {"_id": "6976d5405d41524304c13545", "name": "Cunguang Wang", "hidden": false}, {"_id": "6976d5405d41524304c13546", "name": "Daoru Pan", "hidden": false}, {"_id": "6976d5405d41524304c13547", "name": "Defei Bu", "hidden": false}, {"_id": "6976d5405d41524304c13548", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13549", "name": "Di Xiu", "hidden": false}, {"_id": "6976d5405d41524304c1354a", "name": "Dishan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1354b", "name": "Dongyu Ru", "hidden": false}, {"_id": "6976d5405d41524304c1354c", "name": "Dunwei Tu", "hidden": false}, {"_id": "6976d5405d41524304c1354d", "name": "Fan Wu", "hidden": false}, {"_id": "6976d5405d41524304c1354e", "name": "Fengcheng Yuan", "hidden": false}, {"_id": "6976d5405d41524304c1354f", "name": "Fengcun Li", "hidden": false}, {"_id": "6976d5405d41524304c13550", "name": "Gang Xu", "hidden": false}, {"_id": "6976d5405d41524304c13551", "name": "Guanyu Wu", "hidden": false}, {"_id": "6976d5405d41524304c13552", "name": "Guoyuan Lin", "hidden": false}, {"_id": "6976d5405d41524304c13553", "name": "Haibin Wang", "hidden": false}, {"_id": "6976d5405d41524304c13554", "name": "Hansi Yang", "hidden": false}, {"_id": "6976d5405d41524304c13555", "name": "Hao Yang", "hidden": false}, {"_id": "6976d5405d41524304c13556", "name": "Haonan Yan", "hidden": false}, {"_id": "6976d5405d41524304c13557", "name": "Haoxiang Ma", "hidden": false}, {"_id": "6976d5405d41524304c13558", "name": "Haoxing Wen", "hidden": false}, {"_id": "6976d5405d41524304c13559", "name": "Hongyan Hao", "hidden": false}, {"_id": "6976d5405d41524304c1355a", "name": "Hongyin Tang", "hidden": false}, {"_id": "6976d5405d41524304c1355b", "name": "Hongyu Zang", "hidden": false}, {"_id": "6976d5405d41524304c1355c", "name": "Hongzhi Ni", "hidden": false}, {"_id": "6976d5405d41524304c1355d", "name": "Hui Su", "hidden": false}, {"_id": "6976d5405d41524304c1355e", "name": "Jiacheng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1355f", "name": "Jiahong Zhou", "hidden": false}, {"_id": "6976d5405d41524304c13560", "name": "Jiahuan Li", "hidden": false}, {"_id": "6976d5405d41524304c13561", "name": "Jiaming Wang", "hidden": false}, {"_id": "6976d5405d41524304c13562", "name": "Jian Yang", "hidden": false}, {"_id": "6976d5405d41524304c13563", "user": {"_id": "64008a0af4ff62c2616d8858", "avatarUrl": "/avatars/b52c98857916fba5377ace8089d658b2.svg", "isPro": false, "fullname": "zhangjf", "user": "zhangjf", "type": "user"}, "name": "Jianfei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:09.272Z", "hidden": false}, {"_id": "6976d5405d41524304c13564", "name": "Jianhao Xu", "hidden": false}, {"_id": "6976d5405d41524304c13565", "name": "Jianing Wang", "hidden": false}, {"_id": "6976d5405d41524304c13566", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13567", "name": "Jiaqi Sun", "hidden": false}, {"_id": "6976d5405d41524304c13568", "name": "Jiarong Shi", "hidden": false}, {"_id": "6976d5405d41524304c13569", "name": "Jiarui Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1356a", "name": "Jingang Wang", "hidden": false}, {"_id": "6976d5405d41524304c1356b", "user": {"_id": "6592472fccbc1e2cc7250903", "avatarUrl": "/avatars/6f04ae66944eb2ce65c5aca7927bab10.svg", "isPro": false, "fullname": "Jinluan Yang", "user": "Jinluan", "type": "user"}, "name": "Jinluan Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:47.175Z", "hidden": false}, {"_id": "6976d5405d41524304c1356c", "name": "Jinrui Ding", "hidden": false}, {"_id": "6976d5405d41524304c1356d", "name": "Jinwei Xiao", "hidden": false}, {"_id": "6976d5405d41524304c1356e", "name": "Jiyuan He", "hidden": false}, {"_id": "6976d5405d41524304c1356f", "name": "Juncan Xu", "hidden": false}, {"_id": "6976d5405d41524304c13570", "name": "Kefeng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13571", "name": "Keheng Wang", "hidden": false}, {"_id": "6976d5405d41524304c13572", "name": "Li Wei", "hidden": false}, {"_id": "6976d5405d41524304c13573", "name": "Lianhui Ma", "hidden": false}, {"_id": "6976d5405d41524304c13574", "name": "Lin Qiu", "hidden": false}, {"_id": "6976d5405d41524304c13575", "name": "Lingbing Kong", "hidden": false}, {"_id": "6976d5405d41524304c13576", "name": "Lingchuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c13577", "name": "Linsen Guo", "hidden": false}, {"_id": "6976d5405d41524304c13578", "name": "Mengshen Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13579", "name": "Mengxia Shen", "hidden": false}, {"_id": "6976d5405d41524304c1357a", "name": "Mingyang Zhu", "hidden": false}, {"_id": "6976d5405d41524304c1357b", "name": "Peiguang Li", "hidden": false}, {"_id": "6976d5405d41524304c1357c", "name": "Peng Pei", "hidden": false}, {"_id": "6976d5405d41524304c1357d", "name": "Pengcheng Jia", "hidden": false}, {"_id": "6976d5405d41524304c1357e", "name": "Pengtao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1357f", "name": "Peng Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13580", "name": "Qi Gu", "hidden": false}, {"_id": "6976d5405d41524304c13581", "name": "Qiong Huang", "hidden": false}, {"_id": "6976d5405d41524304c13582", "name": "Qiyuan Duan", "hidden": false}, {"_id": "6976d5405d41524304c13583", "name": "Quanchi Weng", "hidden": false}, {"_id": "6976d5405d41524304c13584", "name": "Rongxiang Weng", "hidden": false}, {"_id": "6976d5405d41524304c13585", "name": "Rongzhi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13586", "name": "Rumei Li", "hidden": false}, {"_id": "6976d5405d41524304c13587", "name": "Shanglin Lei", "hidden": false}, {"_id": "6976d5405d41524304c13588", "user": {"_id": "64db5f5dd68a6ddcc7bd89e9", "avatarUrl": "/avatars/69375ec915927b855813df8a6d486837.svg", "isPro": false, "fullname": "Shengnan An", "user": "ShengnanAn", "type": "user"}, "name": "Shengnan An", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:11.410Z", "hidden": false}, {"_id": "6976d5405d41524304c13589", "name": "Shijun Dai", "hidden": false}, {"_id": "6976d5405d41524304c1358a", "name": "Shuaikang Liu", "hidden": false}, {"_id": "6976d5405d41524304c1358b", "name": "Shuang Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1358c", "name": "Shuo Wang", "hidden": false}, {"_id": "6976d5405d41524304c1358d", "name": "Songyuan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1358e", "name": "Tao Liang", "hidden": false}, {"_id": "6976d5405d41524304c1358f", "name": "Tianhao Hu", "hidden": false}, {"_id": "6976d5405d41524304c13590", "name": "Tianze Chen", "hidden": false}, {"_id": "6976d5405d41524304c13591", "name": "Wei Liu", "hidden": false}, {"_id": "6976d5405d41524304c13592", "name": "Wei Shi", "hidden": false}, {"_id": "6976d5405d41524304c13593", "name": "Wei Wang", "hidden": false}, {"_id": "6976d5405d41524304c13594", "name": "Weifeng Tang", "hidden": false}, {"_id": "6976d5405d41524304c13595", "name": "Wenjie Shi", "hidden": false}, {"_id": "6976d5405d41524304c13596", "name": "Wenlong Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13597", "name": "Wentao Chen", "hidden": false}, {"_id": "6976d5405d41524304c13598", "name": "Wentao Shi", "hidden": false}, {"_id": "6976d5405d41524304c13599", "name": "Xi Su", "hidden": false}, {"_id": "6976d5405d41524304c1359a", "name": "Xiangcheng Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359b", "name": "Xiandi Ma", "hidden": false}, {"_id": "6976d5405d41524304c1359c", "user": {"_id": "63edb098679c2cc40abc6c2e", "avatarUrl": "/avatars/288c7229937c2c3f29fda6d17c7df2eb.svg", "isPro": false, "fullname": "Xiangyu", "user": "xixy", "type": "user"}, "name": "Xiangyu Xi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:13.312Z", "hidden": false}, {"_id": "6976d5405d41524304c1359d", "name": "Xiangyuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359e", "name": "Xiangzhou Huang", "hidden": false}, {"_id": "6976d5405d41524304c1359f", "name": "Xiao Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a0", "name": "Xiaodong Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a1", "name": "Xiaolong Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a2", "name": "Xiaowei Shi", "hidden": false}, {"_id": "6976d5405d41524304c135a3", "name": "Xiaoyu Li", "hidden": false}, {"_id": "6976d5405d41524304c135a4", "name": "Xin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a5", "name": "Xingchen Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a6", "name": "Xuan Huang", "hidden": false}, {"_id": "6976d5405d41524304c135a7", "name": "Xuezhi Cao", "hidden": false}, {"_id": "6976d5405d41524304c135a8", "name": "Xunliang Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a9", "name": "Yan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135aa", "user": {"_id": "63fc1c420aab06079200c15c", "avatarUrl": "/avatars/8e8e82a9a6552848581ca9f65011263c.svg", "isPro": false, "fullname": "yang bai", "user": "byang", "type": "user"}, "name": "Yang Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:07.036Z", "hidden": false}, {"_id": "6976d5405d41524304c135ab", "name": "Yang Liu", "hidden": false}, {"_id": "6976d5405d41524304c135ac", "name": "Yang Yang", "hidden": false}, {"_id": "6976d5405d41524304c135ad", "name": "Yang Zheng", "hidden": false}, {"_id": "6976d5405d41524304c135ae", "name": "Yaoming Wang", "hidden": false}, {"_id": "6976d5405d41524304c135af", "name": "Yaoming Zhu", "hidden": false}, {"_id": "6976d5405d41524304c135b0", "name": "Yaqi Huo", "hidden": false}, {"_id": "6976d5405d41524304c135b1", "name": "Yanyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b2", "name": "Yaorui Shi", "hidden": false}, {"_id": "6976d5405d41524304c135b3", "name": "Yerui Sun", "hidden": false}, {"_id": "6976d5405d41524304c135b4", "name": "Yi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b5", "name": "Yihao Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b6", "name": "Yi-Kai Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b7", "name": "Yifan Lu", "hidden": false}, {"_id": "6976d5405d41524304c135b8", "name": "Yifan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135b9", "name": "Yitao Zhai", "hidden": false}, {"_id": "6976d5405d41524304c135ba", "name": "Yongjing Yin", "hidden": false}, {"_id": "6976d5405d41524304c135bb", "name": "Yongwei Zhou", "hidden": false}, {"_id": "6976d5405d41524304c135bc", "name": "Youshao Xiao", "hidden": false}, {"_id": "6976d5405d41524304c135bd", "name": "Yuchuan Dai", "hidden": false}, {"_id": "6976d5405d41524304c135be", "name": "Yuchen Xie", "hidden": false}, {"_id": "6976d5405d41524304c135bf", "name": "Yuchen Yu", "hidden": false}, {"_id": "6976d5405d41524304c135c0", "name": "Yufei Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135c1", "name": "Yuhuai Wei", "hidden": false}, {"_id": "6976d5405d41524304c135c2", "name": "Yulei Qian", "hidden": false}, {"_id": "6976d5405d41524304c135c3", "name": "Yunfan Liang", "hidden": false}, {"_id": "6976d5405d41524304c135c4", "name": "Yunke Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135c5", "name": "Yuwei Jiang", "hidden": false}, {"_id": "6976d5405d41524304c135c6", "name": "Yuxin Bian", "hidden": false}, {"_id": "6976d5405d41524304c135c7", "name": "Yuxin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135c8", "name": "Yuxin Liu", "hidden": false}, {"_id": "6976d5405d41524304c135c9", "name": "Yue Xu", "hidden": false}, {"_id": "6976d5405d41524304c135ca", "name": "Yueqing Sun", "hidden": false}, {"_id": "6976d5405d41524304c135cb", "name": "Zeyang Yu", "hidden": false}, {"_id": "6976d5405d41524304c135cc", "name": "Zhao Yang", "hidden": false}, {"_id": "6976d5405d41524304c135cd", "name": "Zhengsheng Huang", "hidden": false}, {"_id": "6976d5405d41524304c135ce", "name": "Zhengyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135cf", "name": "Zhijian Liu", "hidden": false}, {"_id": "6976d5405d41524304c135d0", "name": "Zhikang Xia", "hidden": false}, {"_id": "6976d5405d41524304c135d1", "name": "Zhimin Lin", "hidden": false}, {"_id": "6976d5405d41524304c135d2", "name": "Zhiyuan Yao", "hidden": false}, {"_id": "6976d5405d41524304c135d3", "name": "Zhuofan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135d4", "name": "Zhuowen Han", "hidden": false}, {"_id": "6976d5405d41524304c135d5", "name": "Zijian Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135d6", "name": "Ziran Li", "hidden": false}, {"_id": "6976d5405d41524304c135d7", "name": "Ziwen Wang", "hidden": false}, {"_id": "6976d5405d41524304c135d8", "name": "Ziyuan Zhuang", "hidden": false}], "publishedAt": "2026-01-23T13:20:09.000Z", "submittedOnDailyAt": "2026-01-26T00:15:28.340Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "upvotes": 136, "discussionId": "6976d5405d41524304c135d9", "ai_summary": "A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for real-world robustness and complex reasoning.", "ai_keywords": ["Mixture-of-Experts", "agentic reasoning", "domain-parallel expert training", "fusion", "asynchronous reinforcement learning", "DORA", "long-tailed generation", "multi-turn interactions", "real-world noise patterns", "test-time scaling", "reasoning depth", "reasoning width", "parallel thinking"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86LongCat-Flash-Thinking-2601\uff0c\u8fd9\u662f\u4e00\u4e2a\u62e5\u67095600\u4ebf\u53c2\u6570\u7684\u5f00\u6e90\u63a8\u7406\u6a21\u578b\uff0c\u5177\u5907\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u4ee3\u7406\u641c\u7d22\u3001\u5de5\u5177\u4f7f\u7528\u548c\u5de5\u5177\u6574\u5408\u63a8\u7406\u3002</li>\n    <li>\u6a21\u578b\u80fd\u591f\u5728\u590d\u6742\u5de5\u5177\u4e92\u52a8\u548c\u5608\u6742\u7684\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u7a33\u5065\u6027\u3002</li>\n    <li>\u91c7\u7528\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u9886\u57df\u5e76\u884c\u4e13\u5bb6\u8bad\u7ec3\u548c\u540e\u7eed\u878d\u5408\uff0c\u5b9e\u73b0\u4e86\u4ece\u9884\u8bad\u7ec3\u5230\u540e\u8bad\u7ec3\u7684\u5168\u9762\u8bbe\u8ba1\u3002</li>\n    <li>\u5f15\u5165\u4e86Heavy Thinking\u6a21\u5f0f\uff0c\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u5e76\u884c\u601d\u8003\u6709\u6548\u6269\u5c55\u63a8\u7406\u7684\u6df1\u5ea6\u548c\u5bbd\u5ea6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongCat-Flash-Thinking-2601 is a large open-source model with 560 billion parameters designed for advanced reasoning tasks.</li>\n    <li>It performs extremely well on various benchmarks related to agentic reasoning, such as searching and using tools effectively.</li>\n    <li>The model is capable of handling complex interactions and works well in noisy, real-world situations.</li>\n    <li>Its training combines expert knowledge and systematic design to improve its performance across many different environments.</li>\n    <li>A special feature called Heavy Thinking helps the model enhance its reasoning capabilities during testing.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:20:09.000Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16725.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u591a\u6a21\u6001\u667a\u80fd\u7684\u6548\u7387\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u6d41\u7a0b\u5b9e\u73b0\uff0c\u4f7f\u7528\u4e86 1.2T \u7684\u591a\u6a21\u6001\u6570\u636e\u3002</li>\n    <li>\u5b83\u91c7\u7528\u4e86\u5e73\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\u6765\u63d0\u9ad8\u6d4b\u8bd5\u65f6\u7684\u8ba1\u7b97\u6548\u7387\u3002</li>\n    <li>\u5c3d\u7ba1\u6a21\u578b\u8f83\u5c0f\uff08\u4ec5 10B \u53c2\u6570\uff09\uff0c\u4f46\u5728\u6027\u80fd\u4e0a\u80fd\u4e0e\u592710\u523020\u500d\u7684\u6a21\u578b\u7ade\u4e89\uff0c\u751a\u81f3\u8d85\u8d8a\u4e00\u4e9b\u9876\u7ea7\u6a21\u578b\u3002</li>\n    <li>STEP3-VL-10B \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u3001\u9ad8\u6548\u4e14\u53ef\u91cd\u590d\u7684\u57fa\u7ebf\u4f9b\u793e\u533a\u4f7f\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a small, open-source model that balances efficiency with advanced multimodal intelligence.</li>\n    <li>It uses a unique training method that combines a language-focused encoder and a decoder, processing a large dataset of multimodal information.</li>\n    <li>The model includes over 1,000 iterations of reinforcement learning to enhance its capabilities after initial training.</li>\n    <li>It features a system called Parallel Coordinated Reasoning (PaCoRe) to improve how it handles complex visual tasks during testing.</li>\n    <li>Despite being much smaller than other leading models, STEP3-VL-10B achieves top performance scores in various benchmarks.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\uff0c\u5229\u7528\u89c6\u89c9\u7ebf\u7d22\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u672a\u5145\u5206\u5229\u7528\u4eba\u7c7b\u5e38\u7528\u7684\u5730\u56fe\u7b56\u7565\u3002</li>\n    <li>\u672c\u7814\u7a76\u4e3a\u6a21\u578b\u589e\u52a0\u4e86\u201c\u601d\u8003\u5730\u56fe\u201d\u7684\u80fd\u529b\uff0c\u5e76\u91c7\u7528\u4e86\u4ee3\u7406-\u5730\u56fe\u5faa\u73af\u7684\u5f62\u5f0f\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u65b9\u6cd5\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728500\u7c73\u51c6\u786e\u7387\u4e0a\u63d0\u9ad8\u81f322.1%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The task of image geolocalization is to find where a picture was taken on Earth using visual hints.</li>\n    <li>This study introduces a new model that uses maps, which is a strategy people commonly use.</li>\n    <li>The model is trained in two stages: first improving its decision-making with reinforcement learning, then allowing it to explore different options before making a final guess.</li>\n    <li>A new benchmark called MAPBench is created to test the model with real-world images.</li>\n    <li>The new method shows better performance than existing models, especially in accuracy at a distance of 500 meters.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.17058", "authors": [{"_id": "69782c96026bdf0473116e06", "user": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "name": "Wei Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:49.701Z", "hidden": false}, {"_id": "69782c96026bdf0473116e07", "name": "Jun Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e08", "name": "Haoyu Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e09", "name": "Zhenghao Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0a", "name": "Qikang He", "hidden": false}, {"_id": "69782c96026bdf0473116e0b", "name": "Shaokun Han", "hidden": false}, {"_id": "69782c96026bdf0473116e0c", "name": "Guoliang Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0d", "user": {"_id": "64ef522242da8d2a897d62da", "avatarUrl": "/avatars/03611010d247da66696ac8976d4d3ed3.svg", "isPro": false, "fullname": "xuanhe zhou", "user": "zhouxh19", "type": "user"}, "name": "Xuanhe Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:19.930Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0e", "user": {"_id": "674fa2f067c963c50a066594", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674fa2f067c963c50a066594/hKZ46Mwm_UEguzBt63ys_.jpeg", "isPro": false, "fullname": "yeye he", "user": "yeyehe", "type": "user"}, "name": "Yeye He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:27.638Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0f", "name": "Chunwei Liu", "hidden": false}, {"_id": "69782c96026bdf0473116e10", "user": {"_id": "66724ce47e7ff5d8bd069c7c", "avatarUrl": "/avatars/953f66585390dbdb202c1d7b7250d7bd.svg", "isPro": false, "fullname": "Zirui Tang", "user": "TerryTang", "type": "user"}, "name": "Zirui Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:49.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e11", "name": "Bin Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e12", "user": {"_id": "695612aabf3c8959a3a05f9c", "avatarUrl": "/avatars/c18885f6dea6f3ee019405cd8cf6f484.svg", "isPro": false, "fullname": "ShenTang990", "user": "shentang", "type": "user"}, "name": "Shen Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:56.579Z", "hidden": false}, {"_id": "69782c96026bdf0473116e13", "name": "Kai Zuo", "hidden": false}, {"_id": "69782c96026bdf0473116e14", "user": {"_id": "67efa8a2ed790a2e999dc216", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0S4lQCJX61uCF8EkSLMkk.png", "isPro": false, "fullname": "Yuyu Luo", "user": "luoyuyu", "type": "user"}, "name": "Yuyu Luo", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:02.233Z", "hidden": false}, {"_id": "69782c96026bdf0473116e15", "name": "Zhenzhe Zheng", "hidden": false}, {"_id": "69782c96026bdf0473116e16", "user": {"_id": "63f9fca8d4349b157a109eec", "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg", "isPro": false, "fullname": "Conghui He", "user": "conghui", "type": "user"}, "name": "Conghui He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:57:14.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e17", "name": "Jingren Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e18", "name": "Fan Wu", "hidden": false}], "publishedAt": "2026-01-22T12:02:45.000Z", "submittedOnDailyAt": "2026-01-27T00:42:38.464Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "submittedOnDailyBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "upvotes": 127, "discussionId": "69782c97026bdf0473116e19", "projectPage": "https://github.com/weAIDB/awesome-data-llm", "githubRepo": "https://github.com/weAIDB/awesome-data-llm", "githubRepoAddedBy": "user", "ai_summary": "LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.", "ai_keywords": ["data preparation", "large language models", "prompt-driven workflows", "agentic workflows", "data cleaning", "data integration", "data enrichment", "entity matching", "schema matching", "data annotation", "data profiling"], "githubStars": 644, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u6570\u636e\u51c6\u5907\u7684\u76ee\u7684\u662f\u53bb\u566a\u58f0\u3001\u53d1\u73b0\u8de8\u6570\u636e\u96c6\u7684\u5173\u7cfb\u548c\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u5bf9\u6570\u636e\u9a71\u52a8\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>\u7531\u4e8e\u5bf9\u5e94\u7528\u51c6\u5907\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\u548c\u5f3a\u5927\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6280\u672f\u7684\u51fa\u73b0\uff0cLLM\u589e\u5f3a\u7684\u65b9\u6cd5\u6b63\u5728\u6210\u4e3a\u6570\u636e\u51c6\u5907\u7684\u4e3b\u6d41\u3002</li>\n    <li>\u672c\u6587\u5bf9LLM\u6280\u672f\u5728\u6570\u636e\u51c6\u5907\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u5ba1\uff0c\u5173\u6ce8\u6570\u636e\u6e05\u7406\u3001\u6570\u636e\u6574\u5408\u548c\u6570\u636e\u4e30\u5bcc\u7b49\u4e3b\u8981\u4efb\u52a1\u3002</li>\n    <li>\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u8c03\u67e5\u4e86\u4ee3\u8868\u6027\u6280\u672f\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u4f18\u7f3a\u70b9\uff0c\u5982\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u4f46\u4e5f\u5b58\u5728\u6210\u672c\u9ad8\u548c\u8bc4\u4f30\u4e0d\u5339\u914d\u7b49\u95ee\u9898\u3002</li>\n    <li>\u8ba8\u8bba\u4e86\u5f00\u653e\u7684\u7814\u7a76\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u524d\u77bb\u6027\u8def\u7ebf\u56fe\uff0c\u5f3a\u8c03\u53ef\u6269\u5c55\u7684LLM-\u6570\u636e\u7cfb\u7edf\u548c\u53ef\u9760\u7684\u5de5\u4f5c\u6d41\u7a0b\u8bbe\u8ba1\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Data preparation is important for cleaning datasets and finding useful information for various applications like analytics and decision-making.</li>\n    <li>Recent advances in large language models (LLMs) and flexible data infrastructures are changing how data is prepared, moving from traditional methods to new, more adaptable approaches.</li>\n    <li>This paper reviews the use of LLM techniques for data preparation, focusing on three main tasks: data cleaning, data integration, and data enrichment.</li>\n    <li>It discusses the benefits and drawbacks of different methods, such as better understanding of data but challenges like high costs and issues with accuracy.</li>\n    <li>The paper also highlights ongoing challenges in the field and suggests future directions for improving data preparation systems with LLMs.</li>\n</ul>"}, "publishedAt": "2026-01-22T07:02:45.000Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17058.png", "numComments": 2, "submittedBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "fullname": "Wei Zhou", "name": "weizhoudb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.12538", "authors": [{"_id": "6971913fc1c7409747bf9564", "name": "Tianxin Wei", "hidden": false}, {"_id": "6971913fc1c7409747bf9565", "user": {"_id": "6742eb40924e80c3c80ebe13", "avatarUrl": "/avatars/e6ccb1a89a1ea0bfca70779966f4f429.svg", "isPro": false, "fullname": "Ting-Wei Li", "user": "tingwl0122", "type": "user"}, "name": "Ting-Wei Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T17:12:21.531Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9566", "name": "Zhining Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf9567", "name": "Xuying Ning", "hidden": false}, {"_id": "6971913fc1c7409747bf9568", "name": "Ze Yang", "hidden": false}, {"_id": "6971913fc1c7409747bf9569", "name": "Jiaru Zou", "hidden": false}, {"_id": "6971913fc1c7409747bf956a", "name": "Zhichen Zeng", "hidden": false}, {"_id": "6971913fc1c7409747bf956b", "name": "Ruizhong Qiu", "hidden": false}, {"_id": "6971913fc1c7409747bf956c", "name": "Xiao Lin", "hidden": false}, {"_id": "6971913fc1c7409747bf956d", "name": "Dongqi Fu", "hidden": false}, {"_id": "6971913fc1c7409747bf956e", "name": "Zihao Li", "hidden": false}, {"_id": "6971913fc1c7409747bf956f", "user": {"_id": "653962e75c8e4863e1a2068f", "avatarUrl": "/avatars/d4f5f5da141f37d53ca1986ff17b325e.svg", "isPro": false, "fullname": "Mengting Ai", "user": "famous-blue-raincoat", "type": "user"}, "name": "Mengting Ai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:10.378Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9570", "user": {"_id": "677830bd3f2e3ec475576303", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dhwqUDkk66m4oSGSbcd7j.png", "isPro": false, "fullname": "Duo Zhou", "user": "Claudius7", "type": "user"}, "name": "Duo Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:12.476Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9571", "name": "Wenxuan Bao", "hidden": false}, {"_id": "6971913fc1c7409747bf9572", "user": {"_id": "646323556c27a7e33b23f198", "avatarUrl": "/avatars/17fe142f689ab4be3c2374d1d90393db.svg", "isPro": false, "fullname": "Yunzhe Li", "user": "yunzhel2", "type": "user"}, "name": "Yunzhe Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:14.383Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9573", "name": "Gaotang Li", "hidden": false}, {"_id": "6971913fc1c7409747bf9574", "name": "Cheng Qian", "hidden": false}, {"_id": "6971913fc1c7409747bf9575", "name": "Yu Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf9576", "name": "Xiangru Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf9577", "name": "Yin Xiao", "hidden": false}, {"_id": "6971913fc1c7409747bf9578", "name": "Liri Fang", "hidden": false}, {"_id": "6971913fc1c7409747bf9579", "name": "Hui Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf957a", "name": "Xianfeng Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf957b", "name": "Yuji Zhang", "hidden": false}, {"_id": "6971913fc1c7409747bf957c", "name": "Chi Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf957d", "name": "Jiaxuan You", "hidden": false}, {"_id": "6971913fc1c7409747bf957e", "name": "Heng Ji", "hidden": false}, {"_id": "6971913fc1c7409747bf957f", "name": "Hanghang Tong", "hidden": false}, {"_id": "6971913fc1c7409747bf9580", "name": "Jingrui He", "hidden": false}], "publishedAt": "2026-01-18T18:58:23.000Z", "submittedOnDailyAt": "2026-01-22T00:27:25.162Z", "title": "Agentic Reasoning for Large Language Models", "submittedOnDailyBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "isPro": false, "fullname": "Jiaru Zou", "user": "jiaruz2", "type": "user"}, "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "upvotes": 125, "discussionId": "69719140c1c7409747bf9581", "githubRepo": "https://github.com/weitianxin/Awesome-Agentic-Reasoning", "githubRepoAddedBy": "user", "ai_summary": "Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.", "ai_keywords": ["large language models", "agentic reasoning", "autonomous agents", "planning", "tool use", "search", "feedback", "memory", "adaptation", "collaborative settings", "coordination", "knowledge sharing", "reinforcement learning", "supervised fine-tuning", "in-context reasoning", "post-training reasoning", "real-world applications", "benchmarks", "thought and action", "world modeling", "scalable multi-agent training", "governance"], "githubStars": 105, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "summary_zh": "<ul>\n    <li>\u63a8\u7406\u662f\u63a8\u65ad\u3001\u89e3\u51b3\u95ee\u9898\u548c\u51b3\u7b56\u7684\u57fa\u672c\u8ba4\u77e5\u8fc7\u7a0b\u3002</li>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5c01\u95ed\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5f00\u653e\u548c\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u901a\u8fc7\u6301\u7eed\u4e92\u52a8\u8fdb\u884c\u89c4\u5212\u3001\u884c\u52a8\u548c\u5b66\u4e60\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u5206\u4e3a\u4e09\u4e2a\u5c42\u6b21\uff1a\u57fa\u7840\u4ee3\u7406\u63a8\u7406\u3001\u81ea\u6211\u6f14\u53d8\u4ee3\u7406\u63a8\u7406\u548c\u96c6\u4f53\u591a\u4ee3\u7406\u63a8\u7406\u3002</li>\n    <li>\u7efc\u8ff0\u4e2d\u8fd8\u63a2\u8ba8\u4e86\u4ee3\u7406\u63a8\u7406\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u7684\u6846\u67b6\uff0c\u4ee5\u53ca\u672a\u6765\u7684\u6311\u6218\u548c\u65b9\u5411\uff0c\u5982\u4e2a\u6027\u5316\u548c\u591a\u4ee3\u7406\u8bad\u7ec3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reasoning is essential for making decisions and solving problems, but large language models (LLMs) have difficulty in changing environments.</li>\n    <li>Agentic reasoning treats LLMs as independent agents that can learn and adapt through ongoing interactions.</li>\n    <li>The survey breaks down agentic reasoning into three levels: basic single-agent skills, self-improvement through feedback, and teamwork among multiple agents.</li>\n    <li>It distinguishes between two types of reasoning: in-context reasoning for real-time interaction and post-training reasoning for improving behaviors.</li>\n    <li>The survey also highlights applications in various fields and discusses future challenges like personalization and effective teamwork in training.</li>\n</ul>"}, "publishedAt": "2026-01-18T13:58:23.000Z", "title": "Agentic Reasoning for Large Language Models", "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12538.png", "numComments": 3, "submittedBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "fullname": "Jiaru Zou", "name": "jiaruz2", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.08763", "authors": [{"_id": "6969b0a232f0333869ff946a", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:38.232Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946b", "user": {"_id": "6891c906f3c31445cc040ab1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891c906f3c31445cc040ab1/NBqxXOY7al4CD0XBj8ke2.jpeg", "isPro": false, "fullname": "Yucheng Wang", "user": "DevilEnfant", "type": "user"}, "name": "Yucheng Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:48.080Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946c", "name": "Yufei He", "hidden": false}, {"_id": "6969b0a232f0333869ff946d", "user": {"_id": "682deb444988bd82847e2b03", "avatarUrl": "/avatars/15da087e84386ea72c6fa2db63571420.svg", "isPro": false, "fullname": "Jia-Ying Wu", "user": "EricaWu", "type": "user"}, "name": "Jiaying Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:59.692Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946e", "name": "Yilun Zhao", "hidden": false}, {"_id": "6969b0a232f0333869ff946f", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0a232f0333869ff9470", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:06.327Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9471", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:12.181Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9472", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:17.979Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9473", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:23.007Z", "hidden": false}], "publishedAt": "2026-01-13T17:48:43.000Z", "submittedOnDailyAt": "2026-01-16T01:00:36.686Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "upvotes": 111, "discussionId": "6969b0a232f0333869ff9474", "ai_summary": "Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.", "ai_keywords": ["reinforcement learning", "large language models", "exploration collapse", "pass@k", "pass@1", "rollout-level objective", "high-level solution strategies", "clustering", "policy advantages", "AUC@K"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u540e\u8bad\u7ec3\u4e2d\u53d8\u5f97\u975e\u5e38\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u5e38\u5e38\u5bfc\u81f4\u63a2\u7d22\u5d29\u6e83\uff0c\u6a21\u578b\u53ea\u5173\u6ce8\u5c11\u6570\u4e3b\u5bfc\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u591a\u6837\u6027\u548c\u6539\u8fdb\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u2014\u2014\u72ec\u7279\u6027\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\uff0c\u9f13\u52b1\u91c7\u7528\u7a00\u6709\u7684\u9ad8\u5c42\u7b56\u7565\uff0c\u5956\u52b1\u6b63\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u8bc4\u5224\u673a\u5236\u5bf9\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u805a\u7c7b\uff0c\u4f18\u5316\u5956\u52b1\u4ee5\u9f13\u52b1\u65b0\u9896\u7684\u7b56\u7565\u3002</li>\n    <li>\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u533b\u5b66\u63a8\u7406\u7684\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u8868\u73b0\uff0c\u5e76\u589e\u52a0\u4e86\u591a\u6837\u6027\uff0c\u800c\u4e0d\u727a\u7272\u57fa\u7840\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) helps improve large language models (LLMs) for complex reasoning tasks but can lead to exploration collapse.</li>\n    <li>This collapse occurs when the model focuses too much on a few dominant reasoning patterns, limiting diversity in solutions.</li>\n    <li>The proposed solution is Uniqueness-Aware Reinforcement Learning, which rewards unique and correct solutions that use rare strategies.</li>\n    <li>The method uses a judge to group solutions based on high-level strategies and adjusts rewards to favor less common approaches.</li>\n    <li>This approach has shown improvements in various reasoning tasks without losing accuracy on standard measures, encouraging more diverse solutions.</li>\n</ul>"}, "publishedAt": "2026-01-13T12:48:43.000Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08763.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 109, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 105, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7b56\u7565\u7528\u4e8e\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u8bbe\u8ba1\u4e3b\u8981\u4f5c\u4e3a\u88ab\u52a8\u5b58\u50a8\uff0c\u5ffd\u89c6\u4e86\u4e8b\u5b9e\u4e4b\u95f4\u7684\u91cd\u8981\u5173\u8054\uff0c\u5bfc\u81f4\u63a8\u7406\u80fd\u529b\u6709\u9650\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HGMem\uff0c\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u80fd\u52a8\u6001\u5730\u8868\u793a\u548c\u5904\u7406\u590d\u6742\u63a8\u7406\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u8fb9\u8fde\u63a5\u4e0d\u540c\u7684\u8bb0\u5fc6\u5355\u5143\uff0c\u5f62\u6210\u66f4\u9ad8\u9636\u7684\u4e92\u52a8\uff0c\u6709\u52a9\u4e8e\u66f4\u6df1\u5165\u7684\u63a8\u7406\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHGMem\u5728\u591a\u4e2a\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u591a\u6b65\u9aa4RAG\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step RAG helps large language models understand complex tasks better by using a working memory module.</li>\n    <li>Current memory designs mainly store isolated facts, which limits their effectiveness for reasoning and understanding.</li>\n    <li>HGMem introduces a hypergraph-based memory structure that allows for dynamic connections between facts, improving reasoning.</li>\n    <li>This new memory system evolves into a comprehensive knowledge structure, enhancing the model's ability to think and reason deeply.</li>\n    <li>Experiments show that HGMem significantly improves performance on multi-step reasoning tasks compared to existing methods.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Jan 28, 2026";
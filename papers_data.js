window.trendingPapers = {
    "today": [{"paper": {"id": "2601.12538", "authors": [{"_id": "6971913fc1c7409747bf9564", "name": "Tianxin Wei", "hidden": false}, {"_id": "6971913fc1c7409747bf9565", "user": {"_id": "6742eb40924e80c3c80ebe13", "avatarUrl": "/avatars/e6ccb1a89a1ea0bfca70779966f4f429.svg", "isPro": false, "fullname": "Ting-Wei Li", "user": "tingwl0122", "type": "user"}, "name": "Ting-Wei Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T17:12:21.531Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9566", "name": "Zhining Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf9567", "name": "Xuying Ning", "hidden": false}, {"_id": "6971913fc1c7409747bf9568", "name": "Ze Yang", "hidden": false}, {"_id": "6971913fc1c7409747bf9569", "name": "Jiaru Zou", "hidden": false}, {"_id": "6971913fc1c7409747bf956a", "name": "Zhichen Zeng", "hidden": false}, {"_id": "6971913fc1c7409747bf956b", "name": "Ruizhong Qiu", "hidden": false}, {"_id": "6971913fc1c7409747bf956c", "name": "Xiao Lin", "hidden": false}, {"_id": "6971913fc1c7409747bf956d", "name": "Dongqi Fu", "hidden": false}, {"_id": "6971913fc1c7409747bf956e", "name": "Zihao Li", "hidden": false}, {"_id": "6971913fc1c7409747bf956f", "user": {"_id": "653962e75c8e4863e1a2068f", "avatarUrl": "/avatars/d4f5f5da141f37d53ca1986ff17b325e.svg", "isPro": false, "fullname": "Mengting Ai", "user": "famous-blue-raincoat", "type": "user"}, "name": "Mengting Ai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:10.378Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9570", "user": {"_id": "677830bd3f2e3ec475576303", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dhwqUDkk66m4oSGSbcd7j.png", "isPro": false, "fullname": "Duo Zhou", "user": "Claudius7", "type": "user"}, "name": "Duo Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:12.476Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9571", "name": "Wenxuan Bao", "hidden": false}, {"_id": "6971913fc1c7409747bf9572", "user": {"_id": "646323556c27a7e33b23f198", "avatarUrl": "/avatars/17fe142f689ab4be3c2374d1d90393db.svg", "isPro": false, "fullname": "Yunzhe Li", "user": "yunzhel2", "type": "user"}, "name": "Yunzhe Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:14.383Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9573", "name": "Gaotang Li", "hidden": false}, {"_id": "6971913fc1c7409747bf9574", "name": "Cheng Qian", "hidden": false}, {"_id": "6971913fc1c7409747bf9575", "name": "Yu Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf9576", "name": "Xiangru Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf9577", "name": "Yin Xiao", "hidden": false}, {"_id": "6971913fc1c7409747bf9578", "name": "Liri Fang", "hidden": false}, {"_id": "6971913fc1c7409747bf9579", "name": "Hui Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf957a", "name": "Xianfeng Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf957b", "name": "Yuji Zhang", "hidden": false}, {"_id": "6971913fc1c7409747bf957c", "name": "Chi Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf957d", "name": "Jiaxuan You", "hidden": false}, {"_id": "6971913fc1c7409747bf957e", "name": "Heng Ji", "hidden": false}, {"_id": "6971913fc1c7409747bf957f", "name": "Hanghang Tong", "hidden": false}, {"_id": "6971913fc1c7409747bf9580", "name": "Jingrui He", "hidden": false}], "publishedAt": "2026-01-18T18:58:23.000Z", "submittedOnDailyAt": "2026-01-22T00:27:25.162Z", "title": "Agentic Reasoning for Large Language Models", "submittedOnDailyBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "isPro": false, "fullname": "Jiaru Zou", "user": "jiaruz2", "type": "user"}, "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "upvotes": 125, "discussionId": "69719140c1c7409747bf9581", "githubRepo": "https://github.com/weitianxin/Awesome-Agentic-Reasoning", "githubRepoAddedBy": "user", "ai_summary": "Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.", "ai_keywords": ["large language models", "agentic reasoning", "autonomous agents", "planning", "tool use", "search", "feedback", "memory", "adaptation", "collaborative settings", "coordination", "knowledge sharing", "reinforcement learning", "supervised fine-tuning", "in-context reasoning", "post-training reasoning", "real-world applications", "benchmarks", "thought and action", "world modeling", "scalable multi-agent training", "governance"], "githubStars": 105, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "summary_zh": "<ul>\n    <li>\u63a8\u7406\u662f\u63a8\u65ad\u3001\u89e3\u51b3\u95ee\u9898\u548c\u51b3\u7b56\u7684\u57fa\u672c\u8ba4\u77e5\u8fc7\u7a0b\u3002</li>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5c01\u95ed\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5f00\u653e\u548c\u52a8\u6001\u73af\u5883\u4e2d\u9762\u4e34\u6311\u6218\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u5c06LLMs\u91cd\u65b0\u5b9a\u4e49\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u901a\u8fc7\u6301\u7eed\u4e92\u52a8\u8fdb\u884c\u8ba1\u5212\u3001\u884c\u52a8\u548c\u5b66\u4e60\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u5206\u4e3a\u4e09\u4e2a\u5c42\u6b21\uff1a\u57fa\u7840\u4ee3\u7406\u63a8\u7406\uff08\u7a33\u5b9a\u73af\u5883\u4e2d\u7684\u6838\u5fc3\u80fd\u529b\uff09\u3001\u81ea\u6211\u6f14\u5316\u4ee3\u7406\u63a8\u7406\uff08\u901a\u8fc7\u53cd\u9988\u548c\u9002\u5e94\u6539\u8fdb\u80fd\u529b\uff09\u548c\u96c6\u4f53\u591a\u4ee3\u7406\u63a8\u7406\uff08\u5728\u534f\u4f5c\u73af\u5883\u4e2d\u5171\u4eab\u77e5\u8bc6\u548c\u76ee\u6807\uff09\u3002</li>\n    <li>\u672c\u6587\u56de\u987e\u4e86\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u4ee3\u7406\u63a8\u7406\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7684\u6311\u6218\u548c\u65b9\u5411\uff0c\u5982\u4e2a\u6027\u5316\u3001\u957f\u671f\u4e92\u52a8\u548c\u53ef\u6269\u5c55\u7684\u591a\u4ee3\u7406\u8bad\u7ec3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reasoning is crucial for making decisions and solving problems, but large language models (LLMs) struggle in changing environments.</li>\n    <li>Agentic reasoning views LLMs as independent agents that can learn and adapt through interactions.</li>\n    <li>It is organized into three levels: basic abilities for planning and tool use, self-improvement through feedback, and teamwork among multiple agents.</li>\n    <li>The survey discusses how reasoning can be improved through structured interactions and training methods.</li>\n    <li>It also highlights future challenges, such as personalizing AI, improving long-term interactions, and ensuring safe real-world use.</li>\n</ul>"}, "publishedAt": "2026-01-18T13:58:23.000Z", "title": "Agentic Reasoning for Large Language Models", "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12538.png", "numComments": 3, "submittedBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "fullname": "Jiaru Zou", "name": "jiaruz2", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.12346", "authors": [{"_id": "697145b5c1c7409747bf94c7", "name": "Peizhou Huang", "hidden": false}, {"_id": "697145b5c1c7409747bf94c8", "name": "Zixuan Zhong", "hidden": false}, {"_id": "697145b5c1c7409747bf94c9", "name": "Zhongwei Wan", "hidden": false}, {"_id": "697145b5c1c7409747bf94ca", "user": {"_id": "67136093d2e50f1e8c9fad52", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0q49MyGuav8lJ9CIeyLhu.png", "isPro": false, "fullname": "Donghao Zhou", "user": "donghao-zhou", "type": "user"}, "name": "Donghao Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:36.605Z", "hidden": false}, {"_id": "697145b5c1c7409747bf94cb", "name": "Samiul Alam", "hidden": false}, {"_id": "697145b5c1c7409747bf94cc", "name": "Xin Wang", "hidden": false}, {"_id": "697145b5c1c7409747bf94cd", "name": "Zexin Li", "hidden": false}, {"_id": "697145b5c1c7409747bf94ce", "name": "Zhihao Dou", "hidden": false}, {"_id": "697145b5c1c7409747bf94cf", "name": "Li Zhu", "hidden": false}, {"_id": "697145b5c1c7409747bf94d0", "name": "Jing Xiong", "hidden": false}, {"_id": "697145b5c1c7409747bf94d1", "name": "Chaofan Tao", "hidden": false}, {"_id": "697145b5c1c7409747bf94d2", "name": "Yan Xu", "hidden": false}, {"_id": "697145b5c1c7409747bf94d3", "name": "Dimitrios Dimitriadis", "hidden": false}, {"_id": "697145b5c1c7409747bf94d4", "name": "Tuo Zhang", "hidden": false}, {"_id": "697145b5c1c7409747bf94d5", "name": "Mi Zhang", "hidden": false}], "publishedAt": "2026-01-18T10:41:33.000Z", "submittedOnDailyAt": "2026-01-22T02:19:13.211Z", "title": "MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents", "submittedOnDailyBy": {"_id": "67136093d2e50f1e8c9fad52", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0q49MyGuav8lJ9CIeyLhu.png", "isPro": false, "fullname": "Donghao Zhou", "user": "donghao-zhou", "type": "user"}, "summary": "Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.", "upvotes": 41, "discussionId": "697145b5c1c7409747bf94d6", "projectPage": "https://mmdeepresearch-bench.github.io/", "githubRepo": "https://github.com/AIoT-MLSys-Lab/MMDeepResearch-Bench", "githubRepoAddedBy": "user", "ai_summary": "MMDeepResearch-Bench evaluates multimodal research agents on report generation with visual evidence, revealing trade-offs between prose quality, citation accuracy, and visual grounding.", "ai_keywords": ["multimodal evidence use", "citation-grounded report generation", "multimodal understanding", "deep research agents", "Formula-LLM Adaptive Evaluation", "Trustworthy Retrieval-Aligned Citation Evaluation", "Multimodal Support-Aligned Integrity Check"], "githubStars": 10, "summary_zh": "<ul>\n    <li>\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff08DRA\uff09\u901a\u8fc7\u591a\u6b65\u9aa4\u641c\u7d22\u548c\u7efc\u5408\u751f\u6210\u5f15\u7528\u4e30\u5bcc\u7684\u62a5\u544a\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u9488\u5bf9\u6587\u672c\u6216\u77ed\u683c\u5f0f\u591a\u6a21\u6001\u95ee\u7b54\uff0c\u7f3a\u4e4f\u7aef\u5230\u7aef\u591a\u6a21\u6001\u8bc1\u636e\u4f7f\u7528\u7684\u6d4b\u8bd5\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86MMDeepResearch-Bench\uff08MMDR-Bench\uff09\uff0c\u5305\u542b140\u4e2a\u4e13\u5bb6\u8bbe\u8ba1\u7684\u4efb\u52a1\uff0c\u6db5\u76d621\u4e2a\u9886\u57df\uff0c\u6bcf\u4e2a\u4efb\u52a1\u63d0\u4f9b\u56fe\u6587\u7ec4\u5408\u4ee5\u8bc4\u4f30\u591a\u6a21\u6001\u7406\u89e3\u548c\u57fa\u4e8e\u5f15\u7528\u7684\u62a5\u544a\u751f\u6210\u3002</li>\n    <li>\u4e0e\u4e4b\u524d\u7684\u8bbe\u7f6e\u76f8\u6bd4\uff0cMMDR-Bench\u66f4\u5f3a\u8c03\u62a5\u544a\u98ce\u683c\u7684\u7efc\u5408\uff0c\u8981\u6c42\u6a21\u578b\u5c06\u89c6\u89c9\u6750\u6599\u4e0e\u5f15\u7528\u7684\u4e3b\u5f20\u76f8\u8fde\u63a5\uff0c\u5e76\u5728\u53d9\u8ff0\u3001\u5f15\u7528\u548c\u89c6\u89c9\u53c2\u8003\u4e4b\u95f4\u4fdd\u6301\u4e00\u81f4\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\u6d41\u7a0b\uff0c\u5305\u62ec\u62a5\u544a\u8d28\u91cf\u8bc4\u4f30\uff08FLAE\uff09\u3001\u5f15\u7528\u8bc1\u636e\u5bf9\u9f50\u8bc4\u4f30\uff08TRACE\uff09\u548c\u6587\u672c\u89c6\u89c9\u5b8c\u6574\u6027\u68c0\u67e5\uff08MOSAIC\uff09\uff0c\u6bcf\u79cd\u65b9\u6cd5\u90fd\u63d0\u4f9b\u7ec6\u81f4\u7684\u4fe1\u53f7\u4ee5\u652f\u6301\u9519\u8bef\u8bca\u65ad\u3002</li>\n    <li>\u572825\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u7684\u5b9e\u9a8c\u4e2d\uff0c\u53d1\u73b0\u751f\u6210\u8d28\u91cf\u3001\u5f15\u7528\u89c4\u8303\u548c\u591a\u6a21\u6001\u57fa\u7840\u4e4b\u95f4\u5b58\u5728\u7cfb\u7edf\u6027\u7684\u6743\u8861\uff0c\u5f3a\u6709\u529b\u7684\u6587\u672c\u8868\u8fbe\u4e0d\u80fd\u4fdd\u8bc1\u5fe0\u5b9e\u7684\u8bc1\u636e\u4f7f\u7528\uff0c\u591a\u6a21\u6001\u5b8c\u6574\u6027\u4ecd\u7136\u662f\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u5173\u952e\u74f6\u9888\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Deep Research Agents create detailed reports by combining information from text and images, but current benchmarks focus mainly on text or short answers, missing the use of all types of evidence.</li>\n    <li>MMDeepResearch-Bench (MMDR-Bench) is a new benchmark with 140 tasks across 21 areas, designed to test how well models understand and generate reports using both text and images.</li>\n    <li>MMDR-Bench focuses on creating report-style outputs that clearly use evidence, requiring models to link visuals to claims and ensure consistency in their reports.</li>\n    <li>The study introduces a new evaluation system with multiple components to assess report quality, citation accuracy, and the alignment of text and visuals, offering detailed feedback for improvement.</li>\n    <li>Tests on 25 advanced models show that good writing doesn't always mean good use of evidence, and maintaining the connection between text and visuals is a significant challenge for Deep Research Agents.</li>\n</ul>"}, "publishedAt": "2026-01-18T05:41:33.000Z", "title": "MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents", "summary": "Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12346.png", "numComments": 1, "submittedBy": {"_id": "67136093d2e50f1e8c9fad52", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0q49MyGuav8lJ9CIeyLhu.png", "fullname": "Donghao Zhou", "name": "donghao-zhou", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.15282", "authors": [{"_id": "69719a70c1c7409747bf9601", "user": {"_id": "68fce03ed1d0efce7ca87075", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68fce03ed1d0efce7ca87075/GRKTeVIaLZD_M-KoJE8YF.png", "isPro": false, "fullname": "yfdeng", "user": "yfdeng10", "type": "user"}, "name": "Yufan Deng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:44:51.909Z", "hidden": false}, {"_id": "69719a70c1c7409747bf9602", "name": "Zilin Pan", "hidden": false}, {"_id": "69719a70c1c7409747bf9603", "name": "Hongyu Zhang", "hidden": false}, {"_id": "69719a70c1c7409747bf9604", "name": "Xiaojie Li", "hidden": false}, {"_id": "69719a70c1c7409747bf9605", "name": "Ruoqing Hu", "hidden": false}, {"_id": "69719a70c1c7409747bf9606", "user": {"_id": "6661917459720067b2a15bd6", "avatarUrl": "/avatars/f1afe7dd1c538d209016eb5740772d8b.svg", "isPro": false, "fullname": "dyflional10", "user": "dyflional10", "type": "user"}, "name": "Yufei Ding", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:44:59.616Z", "hidden": true}, {"_id": "69719a70c1c7409747bf9607", "name": "Yiming Zou", "hidden": false}, {"_id": "69719a70c1c7409747bf9608", "name": "Yan Zeng", "hidden": false}, {"_id": "69719a70c1c7409747bf9609", "name": "Daquan Zhou", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/l2EDevN63IoqsgFC9Xn13.mp4"], "publishedAt": "2026-01-21T18:59:18.000Z", "submittedOnDailyAt": "2026-01-22T01:05:21.353Z", "title": "Rethinking Video Generation Model for the Embodied World", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.", "upvotes": 36, "discussionId": "69719a70c1c7409747bf960a", "projectPage": "https://dagroup-pku.github.io/ReVidgen.github.io/", "githubRepo": "https://github.com/DAGroup-PKU/ReVidgen/", "githubRepoAddedBy": "user", "ai_summary": "A comprehensive robotics benchmark evaluates video generation models across multiple task domains and robot embodiments, revealing significant gaps in physical realism and introducing a large-scale dataset to address training data limitations.", "ai_keywords": ["video generation models", "embodied intelligence", "robotics benchmark", "robot-oriented video generation", "task domains", "physical plausibility", "action completeness", "Spearman correlation coefficient", "RoVid-X", "data pipeline", "robotic dataset", "video models", "embodied AI", "general intelligence"], "githubStars": 7, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u751f\u6210\u6a21\u578b\u4fc3\u8fdb\u4e86\u673a\u5668\u4eba\u7684\u667a\u80fd\u53d1\u5c55\uff0c\u4f46\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u6807\u51c6\u5316\u8bc4\u4f30\u7684\u7f3a\u4e4f\uff0c\u63d0\u51fa\u4e86RBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u673a\u5668\u4eba\u89c6\u9891\u751f\u6210\u7684\u4e94\u4e2a\u4efb\u52a1\u9886\u57df\u548c\u56db\u79cd\u4e0d\u540c\u5f62\u5f0f\u3002</li>\n    <li>RBench\u901a\u8fc7\u7ed3\u6784\u4e00\u81f4\u6027\u3001\u7269\u7406 plausibility \u548c\u52a8\u4f5c\u5b8c\u6574\u6027\u7b49\u6307\u6807\u8bc4\u4f30\u751f\u6210\u89c6\u9891\u7684\u6b63\u786e\u6027\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u3002</li>\n    <li>\u8bc4\u4f30\u663e\u793a\uff0c\u8bb8\u591a\u4ee3\u8868\u6027\u6a21\u578b\u5728\u751f\u6210\u7269\u7406\u771f\u5b9e\u7684\u673a\u5668\u4eba\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63a8\u51fa\u4e86RoVid-X\u6570\u636e\u96c6\uff0c\u5305\u542b400\u4e07\u4e2a\u6807\u6ce8\u89c6\u9891\u7247\u6bb5\uff0c\u65e8\u5728\u4e3a\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video generation models have improved how robots understand and interact with the physical world, but making realistic videos is still difficult.</li>\n    <li>A new benchmark called RBench has been created to evaluate robot video generation across five tasks and four robot types, focusing on accuracy and visual quality.</li>\n    <li>RBench uses specific metrics to measure how well the videos show real robot actions and has shown a strong agreement with human evaluations.</li>\n    <li>Current models struggle to produce realistic robot behaviors, indicating a need for better training data.</li>\n    <li>To address this, a new dataset called RoVid-X has been introduced, containing 4 million annotated video clips to support training and evaluation of video models for robots.</li>\n</ul>"}, "publishedAt": "2026-01-21T13:59:18.000Z", "title": "Rethinking Video Generation Model for the Embodied World", "summary": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/l2EDevN63IoqsgFC9Xn13.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15282.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 211, "isUserFollowing": false}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.14171", "authors": [{"_id": "69710b60c1c7409747bf9431", "user": {"_id": "6448b2f53e7b3c11be684348", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg", "isPro": true, "fullname": "Qianli Ma", "user": "Mqleet", "type": "user"}, "name": "Qianli Ma", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:46:28.336Z", "hidden": false}, {"_id": "69710b60c1c7409747bf9432", "name": "Chang Guo", "hidden": false}, {"_id": "69710b60c1c7409747bf9433", "name": "Zhiheng Tian", "hidden": false}, {"_id": "69710b60c1c7409747bf9434", "name": "Siyu Wang", "hidden": false}, {"_id": "69710b60c1c7409747bf9435", "name": "Jipeng Xiao", "hidden": false}, {"_id": "69710b60c1c7409747bf9436", "name": "Yuanhao Yue", "hidden": false}, {"_id": "69710b60c1c7409747bf9437", "name": "Zhipeng Zhang", "hidden": false}], "publishedAt": "2026-01-20T17:23:51.000Z", "submittedOnDailyAt": "2026-01-22T01:11:23.304Z", "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance", "submittedOnDailyBy": {"_id": "6448b2f53e7b3c11be684348", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg", "isPro": true, "fullname": "Qianli Ma", "user": "Mqleet", "type": "user"}, "summary": "Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce RebuttalAgent, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, RebuttalAgent ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed RebuttalBench and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.", "upvotes": 35, "discussionId": "69710b60c1c7409747bf9438", "projectPage": "https://mqleet.github.io/Paper2Rebuttal_ProjectPage/", "githubRepo": "https://github.com/AutoLab-SAI-SJTU/Paper2Rebuttal", "githubRepoAddedBy": "user", "ai_summary": "RebuttalAgent is a multi-agent framework that reframes rebuttal generation as an evidence-centric planning task, improving coverage, faithfulness, and strategic coherence in academic peer review.", "ai_keywords": ["multi-agents framework", "evidence-centric planning", "rebuttal generation", "peer review", "strategic coherence", "faithful generation", "external search module"], "githubStars": 146, "organization": {"_id": "68ee0edd23dc954f7744ac27", "name": "AutoLab-SJTU", "fullname": "AutoLab"}, "summary_zh": "<ul>\n    <li>\u64b0\u5199\u6709\u6548\u7684\u53cd\u9a73\u9700\u8981\u7cbe\u786e\u5bf9\u9f50\u5ba1\u7a3f\u4eba\u610f\u56fe\u548c\u624b\u7a3f\u7ec6\u8282\uff0c\u8fdc\u4e0d\u6b62\u8bed\u8a00\u6d41\u5229\u3002</li>\n    <li>\u76ee\u524d\u7684\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u5e7b\u89c9\u3001\u5ffd\u89c6\u6279\u8bc4\u548c\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u4f9d\u636e\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86RebuttalAgent\uff0c\u4e00\u4e2a\u57fa\u4e8e\u8bc1\u636e\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u5c06\u53cd\u9a73\u751f\u6210\u89c6\u4e3a\u89c4\u5212\u4efb\u52a1\u3002</li>\n    <li>\u8be5\u7cfb\u7edf\u5c06\u590d\u6742\u53cd\u9988\u5206\u89e3\u4e3a\u57fa\u672c\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u538b\u7f29\u6458\u8981\u4e0e\u9ad8\u8d28\u91cf\u6587\u672c\u751f\u6210\u54cd\u5e94\u8ba1\u5212\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u8868\u660eRebuttalAgent\u5728\u8986\u76d6\u8303\u56f4\u3001\u53ef\u4fe1\u5ea6\u548c\u6218\u7565\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c06\u53d1\u5e03\u4ee3\u7801\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Writing rebuttals is challenging and needs more than just good language skills; it requires matching the reviewer's intentions with the details in the manuscript.</li>\n    <li>Current methods for generating rebuttals often make mistakes, miss important critiques, and lack solid evidence.</li>\n    <li>We present RebuttalAgent, a new tool that treats rebuttal writing as a task focused on using evidence effectively.</li>\n    <li>RebuttalAgent breaks down complex feedback into smaller parts and builds a response plan that includes both summaries and detailed text.</li>\n    <li>Our system has been tested and shows better results than existing methods in providing comprehensive, accurate, and coherent responses, and we will share the code publicly.</li>\n</ul>"}, "publishedAt": "2026-01-20T12:23:51.000Z", "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance", "summary": "Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce RebuttalAgent, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, RebuttalAgent ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed RebuttalBench and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14171.png", "numComments": 1, "submittedBy": {"_id": "6448b2f53e7b3c11be684348", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg", "fullname": "Qianli Ma", "name": "Mqleet", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "68ee0edd23dc954f7744ac27", "name": "AutoLab-SJTU", "fullname": "AutoLab"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.13572", "authors": [{"_id": "69722bbec1c7409747bf97ed", "user": {"_id": "658071c1da07e791d8a6e9dc", "avatarUrl": "/avatars/66348bae88b5945cc54012c52cff6aa2.svg", "isPro": false, "fullname": "Xiangchi Yuan", "user": "Xiangchi", "type": "user"}, "name": "Xiangchi Yuan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T17:11:49.186Z", "hidden": false}, {"_id": "69722bbec1c7409747bf97ee", "name": "Dachuan Shi", "hidden": false}, {"_id": "69722bbec1c7409747bf97ef", "name": "Chunhui Zhang", "hidden": false}, {"_id": "69722bbec1c7409747bf97f0", "name": "Zheyuan Liu", "hidden": false}, {"_id": "69722bbec1c7409747bf97f1", "name": "Shenglong Yao", "hidden": false}, {"_id": "69722bbec1c7409747bf97f2", "name": "Soroush Vosoughi", "hidden": false}, {"_id": "69722bbec1c7409747bf97f3", "name": "Wenke Lee", "hidden": false}], "publishedAt": "2026-01-20T03:56:53.000Z", "submittedOnDailyAt": "2026-01-22T11:55:21.343Z", "title": "Behavior Knowledge Merge in Reinforced Agentic Models", "submittedOnDailyBy": {"_id": "658071c1da07e791d8a6e9dc", "avatarUrl": "/avatars/66348bae88b5945cc54012c52cff6aa2.svg", "isPro": false, "fullname": "Xiangchi Yuan", "user": "Xiangchi", "type": "user"}, "summary": "Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL's non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.", "upvotes": 15, "discussionId": "69722bbec1c7409747bf97f4", "projectPage": "https://xiangchi-yuan.github.io/ram-project/", "githubRepo": "https://github.com/xiangchi-yuan/mrl", "githubRepoAddedBy": "user", "ai_summary": "Reinforced Agent Merging (RAM) addresses the limitations of traditional merging methods for reinforcement learning-trained agents by distinguishing shared and task-specific parameters to preserve critical behaviors during model integration.", "ai_keywords": ["reinforcement learning", "model merging", "agentic models", "task vectors", "supervised fine-tuning", "on-policy RL", "global averaging", "parameter updates", "distribution-aware merging", "synergistic potential"], "githubStars": 1, "organization": {"_id": "64155eaa95fb6f824b237c3d", "name": "GeorgiaTech", "fullname": "Georgia Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64155e8abe60230f2f40b03a/3i-AL3LrNkaTarSKnaGy8.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5bf9\u4e8e\u540e\u671f\u8bad\u7ec3\u975e\u5e38\u91cd\u8981\uff0c\u7279\u522b\u662f\u5bf9\u9700\u8981\u7279\u6b8a\u63a8\u7406\u884c\u4e3a\u7684\u667a\u80fd\u6a21\u578b\u3002</li>\n    <li>\u6a21\u578b\u5408\u5e76\u662f\u4e00\u79cd\u5c06\u591a\u4e2aRL\u8bad\u7ec3\u7684\u4ee3\u7406\u6574\u5408\u4e3a\u5355\u4e00\u901a\u7528\u6a21\u578b\u7684\u6709\u6548\u65b9\u6cd5\u3002</li>\n    <li>\u73b0\u6709\u7684\u5408\u5e76\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u4e0d\u9002\u5408\u4fdd\u7559RL\u8bad\u7ec3\u6a21\u578b\u7684\u4efb\u52a1\u7279\u5b9a\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u5f3a\u5316\u4ee3\u7406\u5408\u5e76\u201d\uff08RAM\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u4e3aRL\u8bad\u7ec3\u6a21\u578b\u8bbe\u8ba1\u7684\u5408\u5e76\u6846\u67b6\u3002</li>\n    <li>RAM\u901a\u8fc7\u5206\u79bb\u5171\u4eab\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u53c2\u6570\u66f4\u65b0\uff0c\u6539\u5584\u4e86\u5408\u5e76\u6548\u679c\uff0c\u63d0\u5347\u4e86\u4ee3\u7406\u7684\u6574\u4f53\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) is important for training models that need specific reasoning skills.</li>\n    <li>Model merging can combine RL-trained agents from different tasks into one general model but current methods are not effective for RL models.</li>\n    <li>Existing merging methods assume that task data is similar and dense, which doesn't work well with RL's unique and sparse task data.</li>\n    <li>The proposed solution, Reinforced Agent Merging (RAM), better handles the merging of RL models by focusing on both shared and unique updates.</li>\n    <li>Tests show that RAM improves performance compared to existing methods and even allows agents to work better together than when they are specialized for individual tasks.</li>\n</ul>"}, "publishedAt": "2026-01-19T22:56:53.000Z", "title": "Behavior Knowledge Merge in Reinforced Agentic Models", "summary": "Reinforcement learning (RL) is central to post-training, particularly for agentic models that require specialized reasoning behaviors. In this setting, model merging offers a practical mechanism for integrating multiple RL-trained agents from different tasks into a single generalist model. However, existing merging methods are designed for supervised fine-tuning (SFT), and they are suboptimal to preserve task-specific capabilities on RL-trained agentic models. The root is a task-vector mismatch between RL and SFT: on-policy RL induces task vectors that are highly sparse and heterogeneous, whereas SFT-style merging implicitly assumes dense and globally comparable task vectors. When standard global averaging is applied under this mismatch, RL's non-overlapping task vectors that encode critical task-specific behaviors are reduced and parameter updates are diluted. To address this issue, we propose Reinforced Agent Merging (RAM), a distribution-aware merging framework explicitly designed for RL-trained agentic models. RAM disentangles shared and task-specific unique parameter updates, averaging shared components while selectively preserving and rescaling unique ones to counteract parameter update dilution. Experiments across multiple agent domains and model architectures demonstrate that RAM not only surpasses merging baselines, but also unlocks synergistic potential among agents to achieve performance superior to that of specialized agents in their domains.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13572.png", "numComments": 1, "submittedBy": {"_id": "658071c1da07e791d8a6e9dc", "avatarUrl": "/avatars/66348bae88b5945cc54012c52cff6aa2.svg", "fullname": "Xiangchi Yuan", "name": "Xiangchi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "64155eaa95fb6f824b237c3d", "name": "GeorgiaTech", "fullname": "Georgia Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64155e8abe60230f2f40b03a/3i-AL3LrNkaTarSKnaGy8.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.14750", "authors": [{"_id": "697191c6c1c7409747bf9583", "name": "Yifan Wang", "hidden": false}, {"_id": "697191c6c1c7409747bf9584", "name": "Shiyu Li", "hidden": false}, {"_id": "697191c6c1c7409747bf9585", "name": "Peiming Li", "hidden": false}, {"_id": "697191c6c1c7409747bf9586", "name": "Xiaochen Yang", "hidden": false}, {"_id": "697191c6c1c7409747bf9587", "name": "Yang Tang", "hidden": false}, {"_id": "697191c6c1c7409747bf9588", "name": "Zheng Wei", "hidden": false}], "publishedAt": "2026-01-21T08:09:25.000Z", "submittedOnDailyAt": "2026-01-22T00:26:21.515Z", "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT", "upvotes": 14, "discussionId": "697191c7c1c7409747bf9589", "ai_summary": "Render-of-Thought framework converts textual reasoning steps into images using vision-language models to improve reasoning traceability and efficiency while maintaining competitive performance.", "ai_keywords": ["Chain-of-Thought prompting", "Large Language Models", "vision encoders", "Vision Language Models", "token compression", "inference acceleration", "reasoning chain", "semantic anchors", "latent reasoning", "traceability"], "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>Chain-of-Thought (CoT) \u63d0\u793a\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5c3d\u7ba1 CoT \u6709\u52a9\u4e8e\u63a8\u7406\uff0c\u4f46\u5b83\u7684\u5197\u957f\u6027\u589e\u52a0\u4e86\u8ba1\u7b97\u8d1f\u62c5\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86 Render-of-Thought (RoT) \u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u63a8\u7406\u6b65\u9aa4\u8f6c\u5316\u4e3a\u56fe\u50cf\uff0c\u4f7f\u63a8\u7406\u94fe\u66f4\u52a0\u6e05\u6670\u53ef\u8ffd\u8e2a\u3002</li>\n    <li>RoT \u5229\u7528\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u786e\u4fdd\u517c\u5bb9\u6027\u5e76\u51cf\u5c11\u989d\u5916\u7684\u9884\u8bad\u7ec3\u5f00\u9500\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cRoT \u65b9\u6cd5\u5728\u6570\u5b66\u548c\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86 3-4 \u500d\u7684 token \u538b\u7f29\u548c\u663e\u8457\u7684\u63a8\u7406\u52a0\u901f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Chain-of-Thought (CoT) prompting helps Large Language Models (LLMs) reason better but is often too wordy and slow.</li>\n    <li>Many existing methods focus only on the final answers and do not supervise the reasoning steps, making it hard to analyze the thought process.</li>\n    <li>Render-of-Thought (RoT) is a new framework that turns reasoning steps into images, making them clear and easy to follow.</li>\n    <li>RoT uses existing models to connect images and text without needing extra training, making it easy to implement.</li>\n    <li>Tests show RoT is much faster and uses fewer tokens while still performing well compared to other methods.</li>\n</ul>"}, "publishedAt": "2026-01-21T03:09:25.000Z", "title": "Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning", "summary": "Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14750.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 211, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.14490", "authors": [{"_id": "6971e218c1c7409747bf96fa", "user": {"_id": "632f536d2636f057d586cf5b", "avatarUrl": "/avatars/dab7b28734b17a2ba66fd4ae40af000e.svg", "isPro": false, "fullname": "Hunter Heidenreich", "user": "hheiden", "type": "user"}, "name": "Hunter Heidenreich", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T17:12:09.233Z", "hidden": false}, {"_id": "6971e218c1c7409747bf96fb", "user": {"_id": "6970fcdb686be84c0f5a295a", "avatarUrl": "/avatars/c34ccbc616ba2433836f04457e936f32.svg", "isPro": false, "fullname": "Ben Elliott", "user": "bene-roots", "type": "user"}, "name": "Ben Elliott", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T17:11:27.411Z", "hidden": false}, {"_id": "6971e218c1c7409747bf96fc", "name": "Olivia Dinica", "hidden": false}, {"_id": "6971e218c1c7409747bf96fd", "name": "Yosheb Getachew", "hidden": false}], "publishedAt": "2026-01-20T21:26:15.000Z", "submittedOnDailyAt": "2026-01-22T14:07:58.844Z", "title": "GutenOCR: A Grounded Vision-Language Front-End for Documents", "submittedOnDailyBy": {"_id": "632f536d2636f057d586cf5b", "avatarUrl": "/avatars/dab7b28734b17a2ba66fd4ae40af000e.svg", "isPro": false, "fullname": "Hunter Heidenreich", "user": "hheiden", "type": "user"}, "summary": "GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?'' queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.", "upvotes": 14, "discussionId": "6971e219c1c7409747bf96fe", "projectPage": "https://ocr.roots.ai/", "githubRepo": "https://github.com/Roots-Automation/GutenOCR", "githubRepoAddedBy": "user", "ai_summary": "GutenOCR enhances vision-language models for document understanding by enabling unified reading, detection, and grounding through prompt-based interfaces trained on diverse document types.", "ai_keywords": ["vision-language models", "fine-tuning", "grounded OCR", "prompt-based interface", "document understanding", "OCR evaluation protocol", "page-level linearization", "text-detection recall"], "githubStars": 2, "organization": {"_id": "662171ba64e84619e5565f7d", "name": "rootsautomation", "fullname": "Roots.AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66215d0369a5d33b332a19ab/ptAbQGJespFBIeYVk5ce0.jpeg"}, "summary_zh": "<ul>\n    <li>GutenOCR\u662f\u4e00\u4e2a\u7ecf\u8fc7\u5fae\u8c03\u7684OCR\u524d\u7aef\uff0c\u57fa\u4e8eQwen2.5-VL-3B\u548cQwen2.5-VL-7B\u6a21\u578b\u3002</li>\n    <li>\u8fd9\u4e9b\u6a21\u578b\u80fd\u591f\u5904\u7406\u5546\u4e1a\u6587\u4ef6\u3001\u79d1\u5b66\u6587\u7ae0\u548c\u5408\u6210\u6570\u636e\uff0c\u652f\u6301\u5b8c\u6574\u9875\u9762\u548c\u5c40\u90e8\u9605\u8bfb\u3002</li>\n    <li>GutenOCR\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684OCR\u8bc4\u4f30\u534f\u8bae\uff0c\u6548\u679c\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u4e1a\u52a1\u548c\u79d1\u5b66\u9875\u9762\u7684\u8bc4\u5206\u4e0a\u3002</li>\n    <li>\u5728Fox\u548cOmniDocBench v1.5\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u533a\u57df\u548c\u884c\u7ea7OCR\u53ca\u6587\u672c\u68c0\u6d4b\u53ec\u56de\u7387\u65b9\u9762\u6709\u663e\u8457\u6539\u5584\u3002</li>\n    <li>\u4f46\u662f\u5728\u9875\u9762\u7ea7\u7ebf\u6027\u5316\u3001\u989c\u8272\u5f15\u5bfcOCR\u548c\u516c\u5f0f\u91cd\u5e03\u5c40\u65b9\u9762\u5b58\u5728\u4e00\u4e9b\u6743\u8861\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>GutenOCR is a set of advanced OCR models created by improving the Qwen2.5-VL models.</li>\n    <li>These models can read and detect text in documents using a simple, prompt-based method.</li>\n    <li>They are trained on various types of documents and can handle both full-page and specific sections of text.</li>\n    <li>GutenOCR-7B shows a significant improvement in performance on business and scientific documents compared to its earlier version.</li>\n    <li>While it performs well in many areas, there are some challenges with specific types of layouts and formats.</li>\n</ul>"}, "publishedAt": "2026-01-20T16:26:15.000Z", "title": "GutenOCR: A Grounded Vision-Language Front-End for Documents", "summary": "GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?'' queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14490.png", "numComments": 2, "submittedBy": {"_id": "632f536d2636f057d586cf5b", "avatarUrl": "/avatars/dab7b28734b17a2ba66fd4ae40af000e.svg", "fullname": "Hunter Heidenreich", "name": "hheiden", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "662171ba64e84619e5565f7d", "name": "rootsautomation", "fullname": "Roots.AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66215d0369a5d33b332a19ab/ptAbQGJespFBIeYVk5ce0.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.14722", "authors": [{"_id": "6971c5eac1c7409747bf969e", "user": {"_id": "64030afa56038547951c6114", "avatarUrl": "/avatars/79bde57576e75815e1ba383c3bd2eea9.svg", "isPro": false, "fullname": "Surapon Nonesung", "user": "Suraponn", "type": "user"}, "name": "Surapon Nonesung", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:44:35.059Z", "hidden": false}, {"_id": "6971c5eac1c7409747bf969f", "user": {"_id": "64705d3890482b0e0f6591ed", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64705d3890482b0e0f6591ed/PFJs66YhXDogcreVHH1OL.png", "isPro": true, "fullname": "Natapong Nitarach (Schwyter)", "user": "natnitaract", "type": "user"}, "name": "Natapong Nitarach", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:44:30.601Z", "hidden": false}, {"_id": "6971c5eac1c7409747bf96a0", "name": "Teetouch Jaknamon", "hidden": false}, {"_id": "6971c5eac1c7409747bf96a1", "user": {"_id": "615313b0793ef66b3324da1f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg", "isPro": false, "fullname": "Pittawat Taveekitworachai", "user": "pittawat", "type": "user"}, "name": "Pittawat Taveekitworachai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:44:28.142Z", "hidden": false}, {"_id": "6971c5eac1c7409747bf96a2", "name": "Kunat Pipatanakul", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64030afa56038547951c6114/HYCwGave_wMfhAWriEe-b.jpeg"], "publishedAt": "2026-01-21T07:24:32.000Z", "submittedOnDailyAt": "2026-01-22T04:13:18.445Z", "title": "Typhoon OCR: Open Vision-Language Model For Thai Document Extraction", "submittedOnDailyBy": {"_id": "64030afa56038547951c6114", "avatarUrl": "/avatars/79bde57576e75815e1ba383c3bd2eea9.svg", "isPro": false, "fullname": "Surapon Nonesung", "user": "Suraponn", "type": "user"}, "summary": "Document extraction is a core component of digital workflows, yet existing vision-language models (VLMs) predominantly favor high-resource languages. Thai presents additional challenges due to script complexity from non-latin letters, the absence of explicit word boundaries, and the prevalence of highly unstructured real-world documents, limiting the effectiveness of current open-source models. This paper presents Typhoon OCR, an open VLM for document extraction tailored for Thai and English. The model is fine-tuned from vision-language backbones using a Thai-focused training dataset. The dataset is developed using a multi-stage data construction pipeline that combines traditional OCR, VLM-based restructuring, and curated synthetic data. Typhoon OCR is a unified framework capable of text transcription, layout reconstruction, and document-level structural consistency. The latest iteration of our model, Typhoon OCR V1.5, is a compact and inference-efficient model designed to reduce reliance on metadata and simplify deployment. Comprehensive evaluations across diverse Thai document categories, including financial reports, government forms, books, infographics, and handwritten documents, show that Typhoon OCR achieves performance comparable to or exceeding larger frontier proprietary models, despite substantially lower computational cost. The results demonstrate that open vision-language OCR models can achieve accurate text extraction and layout reconstruction for Thai documents, reaching performance comparable to proprietary systems while remaining lightweight and deployable.", "upvotes": 12, "discussionId": "6971c5eac1c7409747bf96a3", "projectPage": "https://opentyphoon.ai/model/typhoon-ocr", "githubRepo": "https://github.com/scb-10x/typhoon-ocr", "githubRepoAddedBy": "user", "ai_summary": "Thai-focused vision-language model for document extraction combining OCR, layout reconstruction, and structural consistency with reduced computational requirements.", "ai_keywords": ["vision-language models", "document extraction", "OCR", "layout reconstruction", "structural consistency", "multi-stage data construction", "synthetic data", "inference efficiency", "compact model"], "githubStars": 85, "organization": {"_id": "63e9cdf9dd2c4effdd6d39c0", "name": "typhoon-ai", "fullname": "Typhoon", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/679c6a57a3d5c3ba94fb1289/13sACxi2PL23wCeKHzwrF.jpeg"}, "summary_zh": "<ul>\n    <li>\u6587\u6863\u63d0\u53d6\u662f\u6570\u5b57\u5de5\u4f5c\u6d41\u7a0b\u7684\u6838\u5fc3\uff0c\u4f46\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u652f\u6301\u9ad8\u8d44\u6e90\u8bed\u8a00\u3002</li>\n    <li>\u6cf0\u8bed\u7531\u4e8e\u5b57\u6bcd\u590d\u6742\u6027\u3001\u7f3a\u4e4f\u660e\u786e\u7684\u5355\u8bcd\u8fb9\u754c\u548c\u975e\u7ed3\u6784\u5316\u6587\u6863\uff0c\u7ed9\u6587\u6863\u63d0\u53d6\u5e26\u6765\u4e86\u6311\u6218\u3002</li>\n    <li>Typhoon OCR \u662f\u9488\u5bf9\u6cf0\u8bed\u548c\u82f1\u8bed\u7684\u5f00\u6e90\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u6587\u6863\u63d0\u53d6\u3002</li>\n    <li>\u8be5\u6a21\u578b\u7ecf\u8fc7\u9488\u5bf9\u6cf0\u8bed\u7684\u6570\u636e\u96c6\u5fae\u8c03\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u6570\u636e\u6784\u5efa\u7ba1\u9053\u5f00\u53d1\u3002</li>\n    <li>Typhoon OCR V1.5 \u5728\u591a\u79cd\u6cf0\u8bed\u6587\u6863\u7c7b\u522b\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u6027\u80fd\u4e0e\u5927\u578b\u4e13\u6709\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Document extraction is important for digital workflows, but most existing models focus on high-resource languages, making it hard for Thai due to its complex script and unstructured documents.</li>\n    <li>Typhoon OCR is a new open vision-language model designed specifically for extracting documents in Thai and English.</li>\n    <li>The model is trained using a specialized dataset created through various techniques, including traditional OCR and synthetic data creation.</li>\n    <li>Typhoon OCR can transcribe text, reconstruct document layouts, and ensure structural consistency in documents.</li>\n    <li>The latest version, Typhoon OCR V1.5, is efficient and performs well across different types of Thai documents, matching or exceeding the performance of larger proprietary models while being more cost-effective and easier to deploy.</li>\n</ul>"}, "publishedAt": "2026-01-21T02:24:32.000Z", "title": "Typhoon OCR: Open Vision-Language Model For Thai Document Extraction", "summary": "Document extraction is a core component of digital workflows, yet existing vision-language models (VLMs) predominantly favor high-resource languages. Thai presents additional challenges due to script complexity from non-latin letters, the absence of explicit word boundaries, and the prevalence of highly unstructured real-world documents, limiting the effectiveness of current open-source models. This paper presents Typhoon OCR, an open VLM for document extraction tailored for Thai and English. The model is fine-tuned from vision-language backbones using a Thai-focused training dataset. The dataset is developed using a multi-stage data construction pipeline that combines traditional OCR, VLM-based restructuring, and curated synthetic data. Typhoon OCR is a unified framework capable of text transcription, layout reconstruction, and document-level structural consistency. The latest iteration of our model, Typhoon OCR V1.5, is a compact and inference-efficient model designed to reduce reliance on metadata and simplify deployment. Comprehensive evaluations across diverse Thai document categories, including financial reports, government forms, books, infographics, and handwritten documents, show that Typhoon OCR achieves performance comparable to or exceeding larger frontier proprietary models, despite substantially lower computational cost. The results demonstrate that open vision-language OCR models can achieve accurate text extraction and layout reconstruction for Thai documents, reaching performance comparable to proprietary systems while remaining lightweight and deployable.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64030afa56038547951c6114/HYCwGave_wMfhAWriEe-b.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14722.png", "numComments": 1, "submittedBy": {"_id": "64030afa56038547951c6114", "avatarUrl": "/avatars/79bde57576e75815e1ba383c3bd2eea9.svg", "fullname": "Surapon Nonesung", "name": "Suraponn", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14, "isUserFollowing": false}, "organization": {"_id": "63e9cdf9dd2c4effdd6d39c0", "name": "typhoon-ai", "fullname": "Typhoon", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/679c6a57a3d5c3ba94fb1289/13sACxi2PL23wCeKHzwrF.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.13044", "authors": [{"_id": "6970467fa8be625b19c2aebe", "user": {"_id": "63d371bcd0b503b7f239ef9d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d371bcd0b503b7f239ef9d/RCHFx6Y6fnh0nZYyF52sS.jpeg", "isPro": true, "fullname": "Sirichotedumrong", "user": "Warit", "type": "user"}, "name": "Warit Sirichotedumrong", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:47:21.592Z", "hidden": false}, {"_id": "6970467fa8be625b19c2aebf", "name": "Adisai Na-Thalang", "hidden": false}, {"_id": "6970467fa8be625b19c2aec0", "name": "Potsawee Manakul", "hidden": false}, {"_id": "6970467fa8be625b19c2aec1", "user": {"_id": "615313b0793ef66b3324da1f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg", "isPro": false, "fullname": "Pittawat Taveekitworachai", "user": "pittawat", "type": "user"}, "name": "Pittawat Taveekitworachai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:47:23.874Z", "hidden": false}, {"_id": "6970467fa8be625b19c2aec2", "name": "Sittipong Sripaisarnmongkol", "hidden": false}, {"_id": "6970467fa8be625b19c2aec3", "name": "Kunat Pipatanakul", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63d371bcd0b503b7f239ef9d/Nw4VZrCtiKVy08No1FwE9.png"], "publishedAt": "2026-01-19T13:28:17.000Z", "submittedOnDailyAt": "2026-01-22T04:16:06.196Z", "title": "Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition", "submittedOnDailyBy": {"_id": "63d371bcd0b503b7f239ef9d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d371bcd0b503b7f239ef9d/RCHFx6Y6fnh0nZYyF52sS.jpeg", "isPro": true, "fullname": "Sirichotedumrong", "user": "Warit", "type": "user"}, "summary": "Large encoder-decoder models like Whisper achieve strong offline transcription but remain impractical for streaming applications due to high latency. However, due to the accessibility of pre-trained checkpoints, the open Thai ASR landscape remains dominated by these offline architectures, leaving a critical gap in efficient streaming solutions. We present Typhoon ASR Real-time, a 115M-parameter FastConformer-Transducer model for low-latency Thai speech recognition. We demonstrate that rigorous text normalization can match the impact of model scaling: our compact model achieves a 45x reduction in computational cost compared to Whisper Large-v3 while delivering comparable accuracy. Our normalization pipeline resolves systemic ambiguities in Thai transcription --including context-dependent number verbalization and repetition markers (mai yamok) --creating consistent training targets. We further introduce a two-stage curriculum learning approach for Isan (north-eastern) dialect adaptation that preserves Central Thai performance. To address reproducibility challenges in Thai ASR, we release the Typhoon ASR Benchmark, a gold-standard human-labeled datasets with transcriptions following established Thai linguistic conventions, providing standardized evaluation protocols for the research community.", "upvotes": 11, "discussionId": "6970467fa8be625b19c2aec4", "projectPage": "https://opentyphoon.ai/model/typhoon-asr-realtime", "githubRepo": "https://github.com/scb-10x/typhoon-asr", "githubRepoAddedBy": "user", "ai_summary": "A 115M-parameter FastConformer-Transducer model achieves low-latency Thai speech recognition with reduced computational cost through text normalization and curriculum learning, accompanied by a benchmark dataset for standardized evaluation.", "ai_keywords": ["FastConformer-Transducer", "streaming applications", "text normalization", "curriculum learning", "Thai ASR", "speech recognition", "computational cost", "model scaling", "linguistic conventions", "benchmark dataset"], "githubStars": 38, "organization": {"_id": "63e9cdf9dd2c4effdd6d39c0", "name": "typhoon-ai", "fullname": "Typhoon", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/679c6a57a3d5c3ba94fb1289/13sACxi2PL23wCeKHzwrF.jpeg"}, "summary_zh": "<ul>\n    <li>Typhoon ASR Real-time \u662f\u4e00\u4e2a\u9488\u5bf9\u6cf0\u8bed\u8bed\u97f3\u8bc6\u522b\u7684\u4f4e\u5ef6\u8fdf\u6a21\u578b\uff0c\u53c2\u6570\u91cf\u4e3a115M\u3002</li>\n    <li>\u901a\u8fc7\u4e25\u683c\u7684\u6587\u672c\u89c4\u8303\u5316\uff0c\u8be5\u6a21\u578b\u5728\u8ba1\u7b97\u6210\u672c\u4e0a\u6bd4 Whisper Large-v3 \u964d\u4f4e\u4e8645\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u4f3c\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>\u6a21\u578b\u89e3\u51b3\u4e86\u6cf0\u8bed\u8f6c\u5f55\u4e2d\u7684\u4e00\u4e9b\u7cfb\u7edf\u6027\u6b67\u4e49\uff0c\u5982\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u6570\u5b57\u8868\u8fbe\u548c\u91cd\u590d\u6807\u8bb0\u3002</li>\n    <li>\u5f15\u5165\u4e86\u4e24\u9636\u6bb5\u7684\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u9002\u5e94\u4f0a\u6851\u65b9\u8a00\uff0c\u540c\u65f6\u4fdd\u6301\u4e2d\u592e\u6cf0\u8bed\u7684\u6027\u80fd\u3002</li>\n    <li>\u53d1\u5e03\u4e86 Typhoon ASR \u57fa\u51c6\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u89e3\u51b3\u6cf0\u8bed ASR \u7684\u53ef\u91cd\u590d\u6027\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Many large models like Whisper work well for offline transcription but are too slow for real-time use.</li>\n    <li>The Thai speech recognition field mainly uses these offline models, creating a need for faster solutions.</li>\n    <li>We introduce Typhoon ASR Real-time, a smaller model that performs Thai speech recognition with low latency.</li>\n    <li>This model uses effective text normalization techniques to reduce costs and improve accuracy compared to larger models.</li>\n    <li>We also provide a new benchmark with high-quality Thai transcriptions to help researchers in the field.</li>\n</ul>"}, "publishedAt": "2026-01-19T08:28:17.000Z", "title": "Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition", "summary": "Large encoder-decoder models like Whisper achieve strong offline transcription but remain impractical for streaming applications due to high latency. However, due to the accessibility of pre-trained checkpoints, the open Thai ASR landscape remains dominated by these offline architectures, leaving a critical gap in efficient streaming solutions. We present Typhoon ASR Real-time, a 115M-parameter FastConformer-Transducer model for low-latency Thai speech recognition. We demonstrate that rigorous text normalization can match the impact of model scaling: our compact model achieves a 45x reduction in computational cost compared to Whisper Large-v3 while delivering comparable accuracy. Our normalization pipeline resolves systemic ambiguities in Thai transcription --including context-dependent number verbalization and repetition markers (mai yamok) --creating consistent training targets. We further introduce a two-stage curriculum learning approach for Isan (north-eastern) dialect adaptation that preserves Central Thai performance. To address reproducibility challenges in Thai ASR, we release the Typhoon ASR Benchmark, a gold-standard human-labeled datasets with transcriptions following established Thai linguistic conventions, providing standardized evaluation protocols for the research community.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63d371bcd0b503b7f239ef9d/Nw4VZrCtiKVy08No1FwE9.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13044.png", "numComments": 1, "submittedBy": {"_id": "63d371bcd0b503b7f239ef9d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d371bcd0b503b7f239ef9d/RCHFx6Y6fnh0nZYyF52sS.jpeg", "fullname": "Sirichotedumrong", "name": "Warit", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "63e9cdf9dd2c4effdd6d39c0", "name": "typhoon-ai", "fullname": "Typhoon", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/679c6a57a3d5c3ba94fb1289/13sACxi2PL23wCeKHzwrF.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.14027", "authors": [{"_id": "6971b78cc1c7409747bf9668", "name": "Junqi Liu", "hidden": false}, {"_id": "6971b78cc1c7409747bf9669", "name": "Zihao Zhou", "hidden": false}, {"_id": "6971b78cc1c7409747bf966a", "name": "Zekai Zhu", "hidden": false}, {"_id": "6971b78cc1c7409747bf966b", "name": "Marco Dos Santos", "hidden": false}, {"_id": "6971b78cc1c7409747bf966c", "name": "Weikun He", "hidden": false}, {"_id": "6971b78cc1c7409747bf966d", "name": "Jiawei Liu", "hidden": false}, {"_id": "6971b78cc1c7409747bf966e", "name": "Ran Wang", "hidden": false}, {"_id": "6971b78cc1c7409747bf966f", "name": "Yunzhou Xie", "hidden": false}, {"_id": "6971b78cc1c7409747bf9670", "name": "Junqiao Zhao", "hidden": false}, {"_id": "6971b78cc1c7409747bf9671", "name": "Qiufeng Wang", "hidden": false}, {"_id": "6971b78cc1c7409747bf9672", "name": "Lihong Zhi", "hidden": false}, {"_id": "6971b78cc1c7409747bf9673", "name": "Jia Li", "hidden": false}, {"_id": "6971b78cc1c7409747bf9674", "name": "Wenda Li", "hidden": false}], "publishedAt": "2026-01-20T14:51:45.000Z", "submittedOnDailyAt": "2026-01-22T03:16:22.205Z", "title": "Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics", "submittedOnDailyBy": {"_id": "65683c6efd8939c271447e9b", "avatarUrl": "/avatars/e4a97ba3550ddcb24b906a7d4c87c861.svg", "isPro": false, "fullname": "ZihaoZhou", "user": "ZihaoZhou", "type": "user"}, "summary": "Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.", "upvotes": 9, "discussionId": "6971b78dc1c7409747bf9675", "ai_summary": "A general coding agent paradigm enables flexible formal theorem proving by directly interfacing with proof assistants and retrieving relevant theorems without task-specific training.", "ai_keywords": ["agentic systems", "formal theorem proving", "general coding agent", "Lean", "Numina-Lean-MCP", "Claude Code", "Claude Opus", "Putnam 2025", "Brascamp-Lieb theorem"], "organization": {"_id": "659a9f55eabe0f3e9812aa63", "name": "AI-MO", "fullname": "Project-Numina", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/661d3f3e85f70e208d6f20db/kaXbnZDekN1gPzmRG_aFV.png"}, "summary_zh": "<ul>\n    <li>\u4ee3\u7406\u7cfb\u7edf\u6210\u4e3a\u6b63\u5f0f\u5b9a\u7406\u8bc1\u660e\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u901a\u8fc7\u534f\u8c03\u591a\u4e2a\u6a21\u578b\u548c\u5de5\u5177\u53d6\u5f97\u826f\u597d\u8868\u73b0\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u4efb\u52a1\u7684\u6d41\u7a0b\u548c\u8bad\u7ec3\u7684\u5f62\u5f0f\u8bc1\u660e\u5668\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\uff0c\u4f7f\u7528\u901a\u7528\u7f16\u7801\u4ee3\u7406\u4f5c\u4e3a\u6b63\u5f0f\u6570\u5b66\u63a8\u7406\u8005\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u63a8\u7406\u4efb\u52a1\u3002</li>\n    <li>Numina-Lean-Agent\u7ed3\u5408Claude Code\u4e0eNumina-Lean-MCP\uff0c\u5b9e\u73b0\u4e0eLean\u7684\u81ea\u4e3b\u4ea4\u4e92\u53ca\u76f8\u5173\u5b9a\u7406\u7684\u68c0\u7d22\u3002</li>\n    <li>\u901a\u8fc7\u4e0e\u6570\u5b66\u5bb6\u4e92\u52a8\uff0c\u6210\u529f\u5f62\u5f0f\u5316Brascamp-Lieb\u5b9a\u7406\uff0c\u5e76\u5728GitHub\u4e0a\u53d1\u5e03\u4e86\u76f8\u5173\u89e3\u51b3\u65b9\u6848\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Agentic systems are now the main way to prove formal theorems, but they often use specific tools that limit flexibility.</li>\n  <li>This paper suggests using a general coding agent as a formal math reasoner for better versatility in reasoning tasks.</li>\n  <li>The new approach allows easy performance improvements by changing the base model without retraining.</li>\n  <li>Numina-Lean-Agent is introduced, which can interact with Lean and solve various math problems, achieving top results.</li>\n  <li>The agent successfully formalized the Brascamp-Lieb theorem and is available for use at a provided GitHub link.</li>\n</ul>"}, "publishedAt": "2026-01-20T09:51:45.000Z", "title": "Numina-Lean-Agent: An Open and General Agentic Reasoning System for Formal Mathematics", "summary": "Agentic systems have recently become the dominant paradigm for formal theorem proving, achieving strong performance by coordinating multiple models and tools. However, existing approaches often rely on task-specific pipelines and trained formal provers, limiting their flexibility and reproducibility. In this paper, we propose the paradigm that directly uses a general coding agent as a formal math reasoner. This paradigm is motivated by (1) A general coding agent provides a natural interface for diverse reasoning tasks beyond proving, (2) Performance can be improved by simply replacing the underlying base model, without training, and (3) MCP enables flexible extension and autonomous calling of specialized tools, avoiding complex design. Based on this paradigm, we introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to enable autonomous interaction with Lean, retrieval of relevant theorems, informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12), matching the best closed-source system. Beyond benchmark evaluation, we further demonstrate its generality by interacting with mathematicians to successfully formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all solutions at https://github.com/project-numina/numina-lean-agent.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14027.png", "numComments": 1, "submittedBy": {"_id": "65683c6efd8939c271447e9b", "avatarUrl": "/avatars/e4a97ba3550ddcb24b906a7d4c87c861.svg", "fullname": "ZihaoZhou", "name": "ZihaoZhou", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "659a9f55eabe0f3e9812aa63", "name": "AI-MO", "fullname": "Project-Numina", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/661d3f3e85f70e208d6f20db/kaXbnZDekN1gPzmRG_aFV.png"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2601.12538", "authors": [{"_id": "6971913fc1c7409747bf9564", "name": "Tianxin Wei", "hidden": false}, {"_id": "6971913fc1c7409747bf9565", "user": {"_id": "6742eb40924e80c3c80ebe13", "avatarUrl": "/avatars/e6ccb1a89a1ea0bfca70779966f4f429.svg", "isPro": false, "fullname": "Ting-Wei Li", "user": "tingwl0122", "type": "user"}, "name": "Ting-Wei Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T17:12:21.531Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9566", "name": "Zhining Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf9567", "name": "Xuying Ning", "hidden": false}, {"_id": "6971913fc1c7409747bf9568", "name": "Ze Yang", "hidden": false}, {"_id": "6971913fc1c7409747bf9569", "name": "Jiaru Zou", "hidden": false}, {"_id": "6971913fc1c7409747bf956a", "name": "Zhichen Zeng", "hidden": false}, {"_id": "6971913fc1c7409747bf956b", "name": "Ruizhong Qiu", "hidden": false}, {"_id": "6971913fc1c7409747bf956c", "name": "Xiao Lin", "hidden": false}, {"_id": "6971913fc1c7409747bf956d", "name": "Dongqi Fu", "hidden": false}, {"_id": "6971913fc1c7409747bf956e", "name": "Zihao Li", "hidden": false}, {"_id": "6971913fc1c7409747bf956f", "user": {"_id": "653962e75c8e4863e1a2068f", "avatarUrl": "/avatars/d4f5f5da141f37d53ca1986ff17b325e.svg", "isPro": false, "fullname": "Mengting Ai", "user": "famous-blue-raincoat", "type": "user"}, "name": "Mengting Ai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:10.378Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9570", "user": {"_id": "677830bd3f2e3ec475576303", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dhwqUDkk66m4oSGSbcd7j.png", "isPro": false, "fullname": "Duo Zhou", "user": "Claudius7", "type": "user"}, "name": "Duo Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:12.476Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9571", "name": "Wenxuan Bao", "hidden": false}, {"_id": "6971913fc1c7409747bf9572", "user": {"_id": "646323556c27a7e33b23f198", "avatarUrl": "/avatars/17fe142f689ab4be3c2374d1d90393db.svg", "isPro": false, "fullname": "Yunzhe Li", "user": "yunzhel2", "type": "user"}, "name": "Yunzhe Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:14.383Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9573", "name": "Gaotang Li", "hidden": false}, {"_id": "6971913fc1c7409747bf9574", "name": "Cheng Qian", "hidden": false}, {"_id": "6971913fc1c7409747bf9575", "name": "Yu Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf9576", "name": "Xiangru Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf9577", "name": "Yin Xiao", "hidden": false}, {"_id": "6971913fc1c7409747bf9578", "name": "Liri Fang", "hidden": false}, {"_id": "6971913fc1c7409747bf9579", "name": "Hui Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf957a", "name": "Xianfeng Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf957b", "name": "Yuji Zhang", "hidden": false}, {"_id": "6971913fc1c7409747bf957c", "name": "Chi Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf957d", "name": "Jiaxuan You", "hidden": false}, {"_id": "6971913fc1c7409747bf957e", "name": "Heng Ji", "hidden": false}, {"_id": "6971913fc1c7409747bf957f", "name": "Hanghang Tong", "hidden": false}, {"_id": "6971913fc1c7409747bf9580", "name": "Jingrui He", "hidden": false}], "publishedAt": "2026-01-18T18:58:23.000Z", "submittedOnDailyAt": "2026-01-22T00:27:25.162Z", "title": "Agentic Reasoning for Large Language Models", "submittedOnDailyBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "isPro": false, "fullname": "Jiaru Zou", "user": "jiaruz2", "type": "user"}, "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "upvotes": 125, "discussionId": "69719140c1c7409747bf9581", "githubRepo": "https://github.com/weitianxin/Awesome-Agentic-Reasoning", "githubRepoAddedBy": "user", "ai_summary": "Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.", "ai_keywords": ["large language models", "agentic reasoning", "autonomous agents", "planning", "tool use", "search", "feedback", "memory", "adaptation", "collaborative settings", "coordination", "knowledge sharing", "reinforcement learning", "supervised fine-tuning", "in-context reasoning", "post-training reasoning", "real-world applications", "benchmarks", "thought and action", "world modeling", "scalable multi-agent training", "governance"], "githubStars": 105, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "summary_zh": "<ul>\n    <li>\u63a8\u7406\u662f\u63a8\u65ad\u3001\u89e3\u51b3\u95ee\u9898\u548c\u51b3\u7b56\u7684\u57fa\u672c\u8ba4\u77e5\u8fc7\u7a0b\u3002</li>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5c01\u95ed\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5f00\u653e\u548c\u52a8\u6001\u73af\u5883\u4e2d\u9762\u4e34\u6311\u6218\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u5c06LLMs\u91cd\u65b0\u5b9a\u4e49\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u901a\u8fc7\u6301\u7eed\u4e92\u52a8\u8fdb\u884c\u8ba1\u5212\u3001\u884c\u52a8\u548c\u5b66\u4e60\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u5206\u4e3a\u4e09\u4e2a\u5c42\u6b21\uff1a\u57fa\u7840\u4ee3\u7406\u63a8\u7406\uff08\u7a33\u5b9a\u73af\u5883\u4e2d\u7684\u6838\u5fc3\u80fd\u529b\uff09\u3001\u81ea\u6211\u6f14\u5316\u4ee3\u7406\u63a8\u7406\uff08\u901a\u8fc7\u53cd\u9988\u548c\u9002\u5e94\u6539\u8fdb\u80fd\u529b\uff09\u548c\u96c6\u4f53\u591a\u4ee3\u7406\u63a8\u7406\uff08\u5728\u534f\u4f5c\u73af\u5883\u4e2d\u5171\u4eab\u77e5\u8bc6\u548c\u76ee\u6807\uff09\u3002</li>\n    <li>\u672c\u6587\u56de\u987e\u4e86\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u4ee3\u7406\u63a8\u7406\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7684\u6311\u6218\u548c\u65b9\u5411\uff0c\u5982\u4e2a\u6027\u5316\u3001\u957f\u671f\u4e92\u52a8\u548c\u53ef\u6269\u5c55\u7684\u591a\u4ee3\u7406\u8bad\u7ec3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reasoning is crucial for making decisions and solving problems, but large language models (LLMs) struggle in changing environments.</li>\n    <li>Agentic reasoning views LLMs as independent agents that can learn and adapt through interactions.</li>\n    <li>It is organized into three levels: basic abilities for planning and tool use, self-improvement through feedback, and teamwork among multiple agents.</li>\n    <li>The survey discusses how reasoning can be improved through structured interactions and training methods.</li>\n    <li>It also highlights future challenges, such as personalizing AI, improving long-term interactions, and ensuring safe real-world use.</li>\n</ul>"}, "publishedAt": "2026-01-18T13:58:23.000Z", "title": "Agentic Reasoning for Large Language Models", "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12538.png", "numComments": 3, "submittedBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "fullname": "Jiaru Zou", "name": "jiaruz2", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.12993", "authors": [{"_id": "69705709a8be625b19c2af1f", "user": {"_id": "6708cbdcf8a1d7b26732c038", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/CN1WHMPKfjQ8wmfwOe0ni.png", "isPro": false, "fullname": "Hao Luo", "user": "Lightet", "type": "user"}, "name": "Hao Luo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T10:15:59.988Z", "hidden": false}, {"_id": "69705709a8be625b19c2af20", "name": "Ye Wang", "hidden": false}, {"_id": "69705709a8be625b19c2af21", "user": {"_id": "640dd700fdeaae139081f598", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg", "isPro": false, "fullname": "Wanpeng Zhang", "user": "zawnpn", "type": "user"}, "name": "Wanpeng Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:19:56.371Z", "hidden": false}, {"_id": "69705709a8be625b19c2af22", "user": {"_id": "64eac1f496f42afd627d439c", "avatarUrl": "/avatars/aa46265122b8a1170f57475494d7922e.svg", "isPro": false, "fullname": "Sipeng Zheng", "user": "sipeng9527", "type": "user"}, "name": "Sipeng Zheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:38.507Z", "hidden": false}, {"_id": "69705709a8be625b19c2af23", "user": {"_id": "66c84a9eab23d3d7dfb2a368", "avatarUrl": "/avatars/b0a50133c6a95ed340dfb462e87820f4.svg", "isPro": false, "fullname": "ziheng xi", "user": "zhenqis123", "type": "user"}, "name": "Ziheng Xi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:45.981Z", "hidden": false}, {"_id": "69705709a8be625b19c2af24", "user": {"_id": "64bdd5cc76a6e2efccb22100", "avatarUrl": "/avatars/5a0edc24283616dafc76ce5ec97ab5a0.svg", "isPro": false, "fullname": "xuchaoyi", "user": "co1one", "type": "user"}, "name": "Chaoyi Xu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:56.014Z", "hidden": false}, {"_id": "69705709a8be625b19c2af25", "user": {"_id": "68872ff6c18b7e1e13115564", "avatarUrl": "/avatars/f908fc3cc89cd81493105359093f299d.svg", "isPro": false, "fullname": "Haiweng Xu", "user": "Seaman05", "type": "user"}, "name": "Haiweng Xu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:50:01.261Z", "hidden": false}, {"_id": "69705709a8be625b19c2af26", "user": {"_id": "644560657a7b94ddc2d445a3", "avatarUrl": "/avatars/09d6447da6ff1bd0b2b00c899c9f1b28.svg", "isPro": false, "fullname": "Haoqi Yuan", "user": "Yaya041", "type": "user"}, "name": "Haoqi Yuan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:50:06.048Z", "hidden": false}, {"_id": "69705709a8be625b19c2af27", "name": "Chi Zhang", "hidden": false}, {"_id": "69705709a8be625b19c2af28", "name": "Yiqing Wang", "hidden": false}, {"_id": "69705709a8be625b19c2af29", "name": "Yicheng Feng", "hidden": false}, {"_id": "69705709a8be625b19c2af2a", "user": {"_id": "67d905c0e27ba28109384f5c", "avatarUrl": "/avatars/26712594ac9d43c8d1a3e75e36b5df16.svg", "isPro": false, "fullname": "Zongqing Lu", "user": "chungtsing", "type": "user"}, "name": "Zongqing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:50:24.833Z", "hidden": false}], "publishedAt": "2026-01-19T12:20:38.000Z", "submittedOnDailyAt": "2026-01-21T02:12:40.880Z", "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization", "submittedOnDailyBy": {"_id": "640dd700fdeaae139081f598", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg", "isPro": false, "fullname": "Wanpeng Zhang", "user": "zawnpn", "type": "user"}, "summary": "We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal \"mother tongue\" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.", "upvotes": 59, "discussionId": "69705709a8be625b19c2af2b", "projectPage": "https://research.beingbeyond.com/being-h05", "githubRepo": "https://github.com/BeingBeyond/Being-H", "githubRepoAddedBy": "user", "ai_summary": "Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.", "ai_keywords": ["Vision-Language-Action", "cross-embodiment generalization", "human-centric learning", "multimodal data", "Unified Action Space", "Mixture-of-Transformers", "Mixture-of-Flow", "manifold-preserving gating", "universal async chunking"], "githubStars": 265, "organization": {"_id": "687a8ba5aedd77694bc94386", "name": "BeingBeyond", "fullname": "BeingBeyond", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/A6vd2S9BqXgtEWHaPy5qW.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86Being-H0.5\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u7840\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u65e8\u5728\u589e\u5f3a\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u4e4b\u95f4\u7684\u901a\u7528\u6027\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u4eba\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u4eba\u7c7b\u4ea4\u4e92\u89c6\u4e3a\u7269\u7406\u4e92\u52a8\u7684\u201c\u6bcd\u8bed\u201d\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u5f62\u6001\u5f02\u8d28\u6027\u548c\u6570\u636e\u7a00\u7f3a\u65b9\u9762\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86UniHand-2.0\uff0c\u8fd9\u662f\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u4f53\u6001\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc735,000\u5c0f\u65f6\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u6db5\u76d630\u79cd\u4e0d\u540c\u7684\u673a\u5668\u4eba\u5f62\u5f0f\u3002</li>\n    <li>Being-H0.5\u91c7\u7528\u4e86\u4e00\u79cd\u6df7\u5408\u53d8\u6362\u5668\u8bbe\u8ba1\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u9896\u7684\u6df7\u5408\u6d41\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u8de8\u4e0d\u540c\u673a\u5668\u4eba\u63a7\u5236\u7684\u6280\u80fd\u8fc1\u79fb\u3002</li>\n    <li>\u6211\u4eec\u5728\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u7684\u7ed3\u679c\uff0c\u5e76\u5728\u4e94\u4e2a\u5e73\u53f0\u4e0a\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8de8\u5f62\u5f0f\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Being-H0.5 is a new model that helps robots understand and perform actions across different types of robotic systems.</li>\n    <li>The model uses human interaction data as a guide for robots to learn from, addressing issues of varying robot shapes and limited data.</li>\n    <li>It includes a large dataset called UniHand-2.0, which has over 35,000 hours of information from 30 types of robots.</li>\n    <li>The system features a unique design that allows robots to share skills, making it easier for less advanced robots to learn from more advanced ones.</li>\n    <li>Tests show that Being-H0.5 performs very well on benchmarks and can effectively work across multiple robotic platforms.</li>\n</ul>"}, "publishedAt": "2026-01-19T07:20:38.000Z", "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization", "summary": "We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal \"mother tongue\" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12993.png", "numComments": 1, "submittedBy": {"_id": "640dd700fdeaae139081f598", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg", "fullname": "Wanpeng Zhang", "name": "zawnpn", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11, "isUserFollowing": false}, "organization": {"_id": "687a8ba5aedd77694bc94386", "name": "BeingBeyond", "fullname": "BeingBeyond", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/A6vd2S9BqXgtEWHaPy5qW.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.11077", "authors": [{"_id": "696da04e3f1837bfb89709c2", "user": {"_id": "66206a2136201a18e5329631", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66206a2136201a18e5329631/9aJiGOKshh1tZ171zDw_D.png", "isPro": false, "fullname": "yangjie", "user": "red-fox-yj", "type": "user"}, "name": "Jie Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-20T09:41:07.348Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709c3", "user": {"_id": "638ef0b0c67af472d31674a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638ef0b0c67af472d31674a6/zXQjC3DdY3jpVkATkpms6.png", "isPro": false, "fullname": "Honglin Guo", "user": "KYLN24", "type": "user"}, "name": "Honglin Guo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-20T09:41:05.334Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709c4", "name": "Li Ji", "hidden": false}, {"_id": "696da04e3f1837bfb89709c5", "user": {"_id": "683c6a19a4b3e38a3e23d50a", "avatarUrl": "/avatars/ae9f212acaa9d1a65b4a5d86c5f7a355.svg", "isPro": false, "fullname": "Jiazheng Zhou", "user": "HaZ-K", "type": "user"}, "name": "Jiazheng Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:42:49.533Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709c6", "name": "Rui Zheng", "hidden": false}, {"_id": "696da04e3f1837bfb89709c7", "name": "Zhikai Lei", "hidden": false}, {"_id": "696da04e3f1837bfb89709c8", "user": {"_id": "6334f2f1259c518276efa730", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6334f2f1259c518276efa730/z_SH_OBkDyj4RCN9mqsKS.jpeg", "isPro": false, "fullname": "Shuo Zhang", "user": "Meteonis", "type": "user"}, "name": "Shuo Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-20T14:48:19.478Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709c9", "user": {"_id": "653a6e5cae155b92bae77b74", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg", "isPro": false, "fullname": "Zhiheng Xi", "user": "WooooDyy", "type": "user"}, "name": "Zhiheng Xi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:43:14.561Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709ca", "user": {"_id": "65435cad429b80b14922ab8d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/N8oWq4ZZn3dRxmXi18FrA.jpeg", "isPro": false, "fullname": "Shichun Liu", "user": "Liusc2020", "type": "user"}, "name": "Shichun Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:43:20.286Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709cb", "name": "Yuxin Wang", "hidden": false}, {"_id": "696da04e3f1837bfb89709cc", "name": "Bo Wang", "hidden": false}, {"_id": "696da04e3f1837bfb89709cd", "user": {"_id": "64b7495a75b23e68c538f4c0", "avatarUrl": "/avatars/ce06f3b89f9e09dcbe748b208eec1e9d.svg", "isPro": false, "fullname": "Yining Zheng", "user": "WillQvQ", "type": "user"}, "name": "Yining Zheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:43:29.765Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709ce", "name": "Tao Gui", "hidden": false}, {"_id": "696da04e3f1837bfb89709cf", "user": {"_id": "61457b8deff2c9fdb4de4988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg", "isPro": false, "fullname": "Xipeng Qiu", "user": "xpqiu", "type": "user"}, "name": "Xipeng Qiu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:43:35.123Z", "hidden": false}], "publishedAt": "2026-01-16T08:23:52.000Z", "submittedOnDailyAt": "2026-01-20T00:57:45.521Z", "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development", "submittedOnDailyBy": {"_id": "66206a2136201a18e5329631", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66206a2136201a18e5329631/9aJiGOKshh1tZ171zDw_D.png", "isPro": false, "fullname": "yangjie", "user": "red-fox-yj", "type": "user"}, "summary": "The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving. However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in backend development which demands rigorous environment configuration and service deployment. To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic backend coding within a realistic, executable workflow. Using a scalable automated pipeline, we curated 224 practical tasks spanning 8 languages and 19 frameworks from open-source repositories. Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests. Our extensive evaluation reveals that even state-of-the-art models struggle to deliver reliable performance on these holistic tasks, highlighting a substantial disparity between current model capabilities and the demands of practical backend engineering. Our code is available at https://github.com/OpenMOSS/ABC-Bench.", "upvotes": 49, "discussionId": "696da04e3f1837bfb89709d0", "projectPage": "https://dawning-road.github.io/blog/abc-bench", "githubRepo": "https://github.com/OpenMOSS/ABC-Bench", "githubRepoAddedBy": "user", "ai_summary": "ABC-Bench evaluates LLM agents on realistic backend coding tasks requiring full development lifecycle management from repository exploration to containerized service deployment and API testing.", "ai_keywords": ["Large Language Models", "agentic backend coding", "executable workflow", "development lifecycle", "containerized services", "end-to-end API tests"], "githubStars": 8, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u7ecf\u53d1\u5c55\u6210\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u80fd\u591f\u8fdb\u884c\u590d\u6742\u7684\u7f16\u7801\u548c\u95ee\u9898\u89e3\u51b3\u3002</li>\n    <li>\u73b0\u6709\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u4ee3\u7801\u903b\u8f91\uff0c\u5ffd\u89c6\u4e86\u5b9e\u9645\u5de5\u7a0b\u4e2d\u52a8\u6001\u7684\u5b8c\u6574\u6d41\u7a0b\u9700\u6c42\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86ABC-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u540e\u7aef\u7f16\u7801\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6a21\u62df\u771f\u5b9e\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>ABC-Bench\u5305\u62ec224\u4e2a\u5b9e\u9645\u4efb\u52a1\uff0c\u6db5\u76d68\u79cd\u8bed\u8a00\u548c19\u79cd\u6846\u67b6\uff0c\u8981\u6c42\u4ee3\u7406\u7ba1\u7406\u6574\u4e2a\u5f00\u53d1\u751f\u547d\u5468\u671f\u3002</li>\n    <li>\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u8fd9\u4e9b\u5168\u9762\u4efb\u52a1\u4e0a\u4e5f\u96be\u4ee5\u63d0\u4f9b\u53ef\u9760\u7684\u6027\u80fd\uff0c\u8868\u660e\u6a21\u578b\u80fd\u529b\u4e0e\u5b9e\u9645\u9700\u6c42\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are now being used as autonomous agents for complex coding tasks.</li>\n    <li>Current evaluations mostly focus on code logic in static situations, missing the real-world needs of backend development.</li>\n    <li>ABC-Bench is a new benchmark created to test coding abilities in realistic, executable workflows.</li>\n    <li>It includes 224 tasks across 8 programming languages and 19 frameworks, requiring full management of development processes.</li>\n    <li>The evaluation shows that even advanced models struggle with these comprehensive backend tasks, indicating a gap in their capabilities.</li>\n</ul>"}, "publishedAt": "2026-01-16T03:23:52.000Z", "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development", "summary": "The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving. However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in backend development which demands rigorous environment configuration and service deployment. To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic backend coding within a realistic, executable workflow. Using a scalable automated pipeline, we curated 224 practical tasks spanning 8 languages and 19 frameworks from open-source repositories. Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests. Our extensive evaluation reveals that even state-of-the-art models struggle to deliver reliable performance on these holistic tasks, highlighting a substantial disparity between current model capabilities and the demands of practical backend engineering. Our code is available at https://github.com/OpenMOSS/ABC-Bench.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11077.png", "numComments": 3, "submittedBy": {"_id": "66206a2136201a18e5329631", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66206a2136201a18e5329631/9aJiGOKshh1tZ171zDw_D.png", "fullname": "yangjie", "name": "red-fox-yj", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.12346", "authors": [{"_id": "697145b5c1c7409747bf94c7", "name": "Peizhou Huang", "hidden": false}, {"_id": "697145b5c1c7409747bf94c8", "name": "Zixuan Zhong", "hidden": false}, {"_id": "697145b5c1c7409747bf94c9", "name": "Zhongwei Wan", "hidden": false}, {"_id": "697145b5c1c7409747bf94ca", "user": {"_id": "67136093d2e50f1e8c9fad52", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0q49MyGuav8lJ9CIeyLhu.png", "isPro": false, "fullname": "Donghao Zhou", "user": "donghao-zhou", "type": "user"}, "name": "Donghao Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:36.605Z", "hidden": false}, {"_id": "697145b5c1c7409747bf94cb", "name": "Samiul Alam", "hidden": false}, {"_id": "697145b5c1c7409747bf94cc", "name": "Xin Wang", "hidden": false}, {"_id": "697145b5c1c7409747bf94cd", "name": "Zexin Li", "hidden": false}, {"_id": "697145b5c1c7409747bf94ce", "name": "Zhihao Dou", "hidden": false}, {"_id": "697145b5c1c7409747bf94cf", "name": "Li Zhu", "hidden": false}, {"_id": "697145b5c1c7409747bf94d0", "name": "Jing Xiong", "hidden": false}, {"_id": "697145b5c1c7409747bf94d1", "name": "Chaofan Tao", "hidden": false}, {"_id": "697145b5c1c7409747bf94d2", "name": "Yan Xu", "hidden": false}, {"_id": "697145b5c1c7409747bf94d3", "name": "Dimitrios Dimitriadis", "hidden": false}, {"_id": "697145b5c1c7409747bf94d4", "name": "Tuo Zhang", "hidden": false}, {"_id": "697145b5c1c7409747bf94d5", "name": "Mi Zhang", "hidden": false}], "publishedAt": "2026-01-18T10:41:33.000Z", "submittedOnDailyAt": "2026-01-22T02:19:13.211Z", "title": "MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents", "submittedOnDailyBy": {"_id": "67136093d2e50f1e8c9fad52", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0q49MyGuav8lJ9CIeyLhu.png", "isPro": false, "fullname": "Donghao Zhou", "user": "donghao-zhou", "type": "user"}, "summary": "Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.", "upvotes": 41, "discussionId": "697145b5c1c7409747bf94d6", "projectPage": "https://mmdeepresearch-bench.github.io/", "githubRepo": "https://github.com/AIoT-MLSys-Lab/MMDeepResearch-Bench", "githubRepoAddedBy": "user", "ai_summary": "MMDeepResearch-Bench evaluates multimodal research agents on report generation with visual evidence, revealing trade-offs between prose quality, citation accuracy, and visual grounding.", "ai_keywords": ["multimodal evidence use", "citation-grounded report generation", "multimodal understanding", "deep research agents", "Formula-LLM Adaptive Evaluation", "Trustworthy Retrieval-Aligned Citation Evaluation", "Multimodal Support-Aligned Integrity Check"], "githubStars": 10, "summary_zh": "<ul>\n    <li>\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff08DRA\uff09\u901a\u8fc7\u591a\u6b65\u9aa4\u641c\u7d22\u548c\u7efc\u5408\u751f\u6210\u5f15\u7528\u4e30\u5bcc\u7684\u62a5\u544a\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u9488\u5bf9\u6587\u672c\u6216\u77ed\u683c\u5f0f\u591a\u6a21\u6001\u95ee\u7b54\uff0c\u7f3a\u4e4f\u7aef\u5230\u7aef\u591a\u6a21\u6001\u8bc1\u636e\u4f7f\u7528\u7684\u6d4b\u8bd5\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86MMDeepResearch-Bench\uff08MMDR-Bench\uff09\uff0c\u5305\u542b140\u4e2a\u4e13\u5bb6\u8bbe\u8ba1\u7684\u4efb\u52a1\uff0c\u6db5\u76d621\u4e2a\u9886\u57df\uff0c\u6bcf\u4e2a\u4efb\u52a1\u63d0\u4f9b\u56fe\u6587\u7ec4\u5408\u4ee5\u8bc4\u4f30\u591a\u6a21\u6001\u7406\u89e3\u548c\u57fa\u4e8e\u5f15\u7528\u7684\u62a5\u544a\u751f\u6210\u3002</li>\n    <li>\u4e0e\u4e4b\u524d\u7684\u8bbe\u7f6e\u76f8\u6bd4\uff0cMMDR-Bench\u66f4\u5f3a\u8c03\u62a5\u544a\u98ce\u683c\u7684\u7efc\u5408\uff0c\u8981\u6c42\u6a21\u578b\u5c06\u89c6\u89c9\u6750\u6599\u4e0e\u5f15\u7528\u7684\u4e3b\u5f20\u76f8\u8fde\u63a5\uff0c\u5e76\u5728\u53d9\u8ff0\u3001\u5f15\u7528\u548c\u89c6\u89c9\u53c2\u8003\u4e4b\u95f4\u4fdd\u6301\u4e00\u81f4\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\u6d41\u7a0b\uff0c\u5305\u62ec\u62a5\u544a\u8d28\u91cf\u8bc4\u4f30\uff08FLAE\uff09\u3001\u5f15\u7528\u8bc1\u636e\u5bf9\u9f50\u8bc4\u4f30\uff08TRACE\uff09\u548c\u6587\u672c\u89c6\u89c9\u5b8c\u6574\u6027\u68c0\u67e5\uff08MOSAIC\uff09\uff0c\u6bcf\u79cd\u65b9\u6cd5\u90fd\u63d0\u4f9b\u7ec6\u81f4\u7684\u4fe1\u53f7\u4ee5\u652f\u6301\u9519\u8bef\u8bca\u65ad\u3002</li>\n    <li>\u572825\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u7684\u5b9e\u9a8c\u4e2d\uff0c\u53d1\u73b0\u751f\u6210\u8d28\u91cf\u3001\u5f15\u7528\u89c4\u8303\u548c\u591a\u6a21\u6001\u57fa\u7840\u4e4b\u95f4\u5b58\u5728\u7cfb\u7edf\u6027\u7684\u6743\u8861\uff0c\u5f3a\u6709\u529b\u7684\u6587\u672c\u8868\u8fbe\u4e0d\u80fd\u4fdd\u8bc1\u5fe0\u5b9e\u7684\u8bc1\u636e\u4f7f\u7528\uff0c\u591a\u6a21\u6001\u5b8c\u6574\u6027\u4ecd\u7136\u662f\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u5173\u952e\u74f6\u9888\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Deep Research Agents create detailed reports by combining information from text and images, but current benchmarks focus mainly on text or short answers, missing the use of all types of evidence.</li>\n    <li>MMDeepResearch-Bench (MMDR-Bench) is a new benchmark with 140 tasks across 21 areas, designed to test how well models understand and generate reports using both text and images.</li>\n    <li>MMDR-Bench focuses on creating report-style outputs that clearly use evidence, requiring models to link visuals to claims and ensure consistency in their reports.</li>\n    <li>The study introduces a new evaluation system with multiple components to assess report quality, citation accuracy, and the alignment of text and visuals, offering detailed feedback for improvement.</li>\n    <li>Tests on 25 advanced models show that good writing doesn't always mean good use of evidence, and maintaining the connection between text and visuals is a significant challenge for Deep Research Agents.</li>\n</ul>"}, "publishedAt": "2026-01-18T05:41:33.000Z", "title": "MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents", "summary": "Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12346.png", "numComments": 1, "submittedBy": {"_id": "67136093d2e50f1e8c9fad52", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0q49MyGuav8lJ9CIeyLhu.png", "fullname": "Donghao Zhou", "name": "donghao-zhou", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.11496", "authors": [{"_id": "696de89f3f1837bfb8970ab3", "user": {"_id": "64802fb6c57f629056c59966", "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg", "isPro": false, "fullname": "Eilam Shapira", "user": "EilamSha", "type": "user"}, "name": "Eilam Shapira", "status": "admin_assigned", "statusLastChangedAt": "2026-01-19T14:37:41.780Z", "hidden": false}, {"_id": "696de89f3f1837bfb8970ab4", "name": "Roi Reichart", "hidden": false}, {"_id": "696de89f3f1837bfb8970ab5", "name": "Moshe Tennenholtz", "hidden": false}], "publishedAt": "2026-01-16T18:18:03.000Z", "submittedOnDailyAt": "2026-01-19T06:58:50.740Z", "title": "The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents", "submittedOnDailyBy": {"_id": "64802fb6c57f629056c59966", "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg", "isPro": false, "fullname": "Eilam Shapira", "user": "EilamSha", "type": "user"}, "summary": "The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the \"Poisoned Apple\" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator's choice of market design in their favor. This strategic release improves the releaser's welfare at the expense of their opponent and the regulator's fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.", "upvotes": 39, "discussionId": "696de8a03f1837bfb8970ab6", "organization": {"_id": "6393322be2364bc1eea56e45", "name": "Technion", "fullname": "Technion Israel institute of technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"}, "summary_zh": "<ul>\n    <li>\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u6574\u5408\u6539\u53d8\u4e86\u7ecf\u6d4e\u5e02\u573a\u7684\u6218\u7565\u4e92\u52a8\u683c\u5c40\u3002</li>\n    <li>\u7814\u7a76\u4e86\u5728\u4e09\u79cd\u535a\u5f08\u7406\u8bba\u73af\u5883\u4e2d\uff0c\u6269\u5c55\u53ef\u7528\u6280\u672f\u7684\u7ecf\u6d4e\u5f71\u54cd\uff1a\u8d44\u6e90\u5206\u914d\u3001\u4fe1\u606f\u4e0d\u5bf9\u79f0\u4ea4\u6613\u548c\u6218\u7565\u4fe1\u606f\u4f20\u9012\u3002</li>\n    <li>\u589e\u52a0\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u9009\u62e9\u4f1a\u663e\u8457\u6539\u53d8\u5747\u8861\u6536\u76ca\u548c\u76d1\u7ba1\u7ed3\u679c\uff0c\u53ef\u80fd\u4fc3\u4f7f\u76d1\u7ba1\u8005\u79ef\u6781\u5f00\u53d1\u65b0\u6280\u672f\u3002</li>\n    <li>\u53d1\u73b0\u4e00\u79cd\u201c\u6709\u6bd2\u82f9\u679c\u201d\u6548\u5e94\uff1a\u4ee3\u7406\u53ef\u80fd\u53d1\u5e03\u65b0\u6280\u672f\u6765\u64cd\u63a7\u76d1\u7ba1\u8005\u7684\u5e02\u573a\u8bbe\u8ba1\u9009\u62e9\uff0c\u4ece\u4e2d\u83b7\u76ca\u3002</li>\n    <li>\u9759\u6001\u76d1\u7ba1\u6846\u67b6\u5bb9\u6613\u88ab\u6280\u672f\u6269\u5c55\u64cd\u63a7\uff0c\u56e0\u6b64\u9700\u8981\u52a8\u6001\u5e02\u573a\u8bbe\u8ba1\u4ee5\u9002\u5e94\u4eba\u5de5\u667a\u80fd\u80fd\u529b\u7684\u53d8\u5316\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Integrating AI agents into markets changes how strategies are formed in economic interactions.</li>\n    <li>Expanding technology options impacts outcomes in bargaining, negotiation, and persuasion scenarios.</li>\n    <li>More choices for AI can significantly alter payoffs and encourage regulators to create new technologies.</li>\n    <li>The \"Poisoned Apple\" effect can occur when an agent releases a technology to manipulate market rules in their favor.</li>\n    <li>Current regulatory systems can be easily manipulated, highlighting the need for adaptable market designs.</li>\n</ul>"}, "publishedAt": "2026-01-16T13:18:03.000Z", "title": "The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents", "summary": "The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the \"Poisoned Apple\" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator's choice of market design in their favor. This strategic release improves the releaser's welfare at the expense of their opponent and the regulator's fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11496.png", "numComments": 2, "submittedBy": {"_id": "64802fb6c57f629056c59966", "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg", "fullname": "Eilam Shapira", "name": "EilamSha", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "6393322be2364bc1eea56e45", "name": "Technion", "fullname": "Technion Israel institute of technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.15282", "authors": [{"_id": "69719a70c1c7409747bf9601", "user": {"_id": "68fce03ed1d0efce7ca87075", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68fce03ed1d0efce7ca87075/GRKTeVIaLZD_M-KoJE8YF.png", "isPro": false, "fullname": "yfdeng", "user": "yfdeng10", "type": "user"}, "name": "Yufan Deng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:44:51.909Z", "hidden": false}, {"_id": "69719a70c1c7409747bf9602", "name": "Zilin Pan", "hidden": false}, {"_id": "69719a70c1c7409747bf9603", "name": "Hongyu Zhang", "hidden": false}, {"_id": "69719a70c1c7409747bf9604", "name": "Xiaojie Li", "hidden": false}, {"_id": "69719a70c1c7409747bf9605", "name": "Ruoqing Hu", "hidden": false}, {"_id": "69719a70c1c7409747bf9606", "user": {"_id": "6661917459720067b2a15bd6", "avatarUrl": "/avatars/f1afe7dd1c538d209016eb5740772d8b.svg", "isPro": false, "fullname": "dyflional10", "user": "dyflional10", "type": "user"}, "name": "Yufei Ding", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:44:59.616Z", "hidden": true}, {"_id": "69719a70c1c7409747bf9607", "name": "Yiming Zou", "hidden": false}, {"_id": "69719a70c1c7409747bf9608", "name": "Yan Zeng", "hidden": false}, {"_id": "69719a70c1c7409747bf9609", "name": "Daquan Zhou", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/l2EDevN63IoqsgFC9Xn13.mp4"], "publishedAt": "2026-01-21T18:59:18.000Z", "submittedOnDailyAt": "2026-01-22T01:05:21.353Z", "title": "Rethinking Video Generation Model for the Embodied World", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.", "upvotes": 36, "discussionId": "69719a70c1c7409747bf960a", "projectPage": "https://dagroup-pku.github.io/ReVidgen.github.io/", "githubRepo": "https://github.com/DAGroup-PKU/ReVidgen/", "githubRepoAddedBy": "user", "ai_summary": "A comprehensive robotics benchmark evaluates video generation models across multiple task domains and robot embodiments, revealing significant gaps in physical realism and introducing a large-scale dataset to address training data limitations.", "ai_keywords": ["video generation models", "embodied intelligence", "robotics benchmark", "robot-oriented video generation", "task domains", "physical plausibility", "action completeness", "Spearman correlation coefficient", "RoVid-X", "data pipeline", "robotic dataset", "video models", "embodied AI", "general intelligence"], "githubStars": 7, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u751f\u6210\u6a21\u578b\u4fc3\u8fdb\u4e86\u673a\u5668\u4eba\u7684\u667a\u80fd\u53d1\u5c55\uff0c\u4f46\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u6807\u51c6\u5316\u8bc4\u4f30\u7684\u7f3a\u4e4f\uff0c\u63d0\u51fa\u4e86RBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u673a\u5668\u4eba\u89c6\u9891\u751f\u6210\u7684\u4e94\u4e2a\u4efb\u52a1\u9886\u57df\u548c\u56db\u79cd\u4e0d\u540c\u5f62\u5f0f\u3002</li>\n    <li>RBench\u901a\u8fc7\u7ed3\u6784\u4e00\u81f4\u6027\u3001\u7269\u7406 plausibility \u548c\u52a8\u4f5c\u5b8c\u6574\u6027\u7b49\u6307\u6807\u8bc4\u4f30\u751f\u6210\u89c6\u9891\u7684\u6b63\u786e\u6027\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u3002</li>\n    <li>\u8bc4\u4f30\u663e\u793a\uff0c\u8bb8\u591a\u4ee3\u8868\u6027\u6a21\u578b\u5728\u751f\u6210\u7269\u7406\u771f\u5b9e\u7684\u673a\u5668\u4eba\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63a8\u51fa\u4e86RoVid-X\u6570\u636e\u96c6\uff0c\u5305\u542b400\u4e07\u4e2a\u6807\u6ce8\u89c6\u9891\u7247\u6bb5\uff0c\u65e8\u5728\u4e3a\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video generation models have improved how robots understand and interact with the physical world, but making realistic videos is still difficult.</li>\n    <li>A new benchmark called RBench has been created to evaluate robot video generation across five tasks and four robot types, focusing on accuracy and visual quality.</li>\n    <li>RBench uses specific metrics to measure how well the videos show real robot actions and has shown a strong agreement with human evaluations.</li>\n    <li>Current models struggle to produce realistic robot behaviors, indicating a need for better training data.</li>\n    <li>To address this, a new dataset called RoVid-X has been introduced, containing 4 million annotated video clips to support training and evaluation of video models for robots.</li>\n</ul>"}, "publishedAt": "2026-01-21T13:59:18.000Z", "title": "Rethinking Video Generation Model for the Embodied World", "summary": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/l2EDevN63IoqsgFC9Xn13.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15282.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 211, "isUserFollowing": false}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.14171", "authors": [{"_id": "69710b60c1c7409747bf9431", "user": {"_id": "6448b2f53e7b3c11be684348", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg", "isPro": true, "fullname": "Qianli Ma", "user": "Mqleet", "type": "user"}, "name": "Qianli Ma", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:46:28.336Z", "hidden": false}, {"_id": "69710b60c1c7409747bf9432", "name": "Chang Guo", "hidden": false}, {"_id": "69710b60c1c7409747bf9433", "name": "Zhiheng Tian", "hidden": false}, {"_id": "69710b60c1c7409747bf9434", "name": "Siyu Wang", "hidden": false}, {"_id": "69710b60c1c7409747bf9435", "name": "Jipeng Xiao", "hidden": false}, {"_id": "69710b60c1c7409747bf9436", "name": "Yuanhao Yue", "hidden": false}, {"_id": "69710b60c1c7409747bf9437", "name": "Zhipeng Zhang", "hidden": false}], "publishedAt": "2026-01-20T17:23:51.000Z", "submittedOnDailyAt": "2026-01-22T01:11:23.304Z", "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance", "submittedOnDailyBy": {"_id": "6448b2f53e7b3c11be684348", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg", "isPro": true, "fullname": "Qianli Ma", "user": "Mqleet", "type": "user"}, "summary": "Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce RebuttalAgent, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, RebuttalAgent ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed RebuttalBench and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.", "upvotes": 35, "discussionId": "69710b60c1c7409747bf9438", "projectPage": "https://mqleet.github.io/Paper2Rebuttal_ProjectPage/", "githubRepo": "https://github.com/AutoLab-SAI-SJTU/Paper2Rebuttal", "githubRepoAddedBy": "user", "ai_summary": "RebuttalAgent is a multi-agent framework that reframes rebuttal generation as an evidence-centric planning task, improving coverage, faithfulness, and strategic coherence in academic peer review.", "ai_keywords": ["multi-agents framework", "evidence-centric planning", "rebuttal generation", "peer review", "strategic coherence", "faithful generation", "external search module"], "githubStars": 146, "organization": {"_id": "68ee0edd23dc954f7744ac27", "name": "AutoLab-SJTU", "fullname": "AutoLab"}, "summary_zh": "<ul>\n    <li>\u64b0\u5199\u6709\u6548\u7684\u53cd\u9a73\u9700\u8981\u7cbe\u786e\u5bf9\u9f50\u5ba1\u7a3f\u4eba\u610f\u56fe\u548c\u624b\u7a3f\u7ec6\u8282\uff0c\u8fdc\u4e0d\u6b62\u8bed\u8a00\u6d41\u5229\u3002</li>\n    <li>\u76ee\u524d\u7684\u89e3\u51b3\u65b9\u6848\u5b58\u5728\u5e7b\u89c9\u3001\u5ffd\u89c6\u6279\u8bc4\u548c\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u4f9d\u636e\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86RebuttalAgent\uff0c\u4e00\u4e2a\u57fa\u4e8e\u8bc1\u636e\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u5c06\u53cd\u9a73\u751f\u6210\u89c6\u4e3a\u89c4\u5212\u4efb\u52a1\u3002</li>\n    <li>\u8be5\u7cfb\u7edf\u5c06\u590d\u6742\u53cd\u9988\u5206\u89e3\u4e3a\u57fa\u672c\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u538b\u7f29\u6458\u8981\u4e0e\u9ad8\u8d28\u91cf\u6587\u672c\u751f\u6210\u54cd\u5e94\u8ba1\u5212\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u8868\u660eRebuttalAgent\u5728\u8986\u76d6\u8303\u56f4\u3001\u53ef\u4fe1\u5ea6\u548c\u6218\u7565\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c06\u53d1\u5e03\u4ee3\u7801\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Writing rebuttals is challenging and needs more than just good language skills; it requires matching the reviewer's intentions with the details in the manuscript.</li>\n    <li>Current methods for generating rebuttals often make mistakes, miss important critiques, and lack solid evidence.</li>\n    <li>We present RebuttalAgent, a new tool that treats rebuttal writing as a task focused on using evidence effectively.</li>\n    <li>RebuttalAgent breaks down complex feedback into smaller parts and builds a response plan that includes both summaries and detailed text.</li>\n    <li>Our system has been tested and shows better results than existing methods in providing comprehensive, accurate, and coherent responses, and we will share the code publicly.</li>\n</ul>"}, "publishedAt": "2026-01-20T12:23:51.000Z", "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance", "summary": "Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce RebuttalAgent, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, RebuttalAgent ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed RebuttalBench and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14171.png", "numComments": 1, "submittedBy": {"_id": "6448b2f53e7b3c11be684348", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg", "fullname": "Qianli Ma", "name": "Mqleet", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "68ee0edd23dc954f7744ac27", "name": "AutoLab-SJTU", "fullname": "AutoLab"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.14250", "authors": [{"_id": "69705b78a8be625b19c2af4c", "user": {"_id": "6697765937d24838267b41e7", "avatarUrl": "/avatars/682bb4cf0a0009812b42748dc26916f9.svg", "isPro": false, "fullname": "PangzeCheung", "user": "PangzeCheung", "type": "user"}, "name": "Pengze Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:19:32.663Z", "hidden": false}, {"_id": "69705b78a8be625b19c2af4d", "name": "Yanze Wu", "hidden": false}, {"_id": "69705b78a8be625b19c2af4e", "user": {"_id": "6805bdfb344d6d8a8fd5b07a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dGLeWvN2CXLmxVP_b9S4R.png", "isPro": false, "fullname": "Mengtian Li", "user": "LemonSky1995", "type": "user"}, "name": "Mengtian Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:52:30.670Z", "hidden": false}, {"_id": "69705b78a8be625b19c2af4f", "name": "Xu Bai", "hidden": false}, {"_id": "69705b78a8be625b19c2af50", "name": "Songtao Zhao", "hidden": false}, {"_id": "69705b78a8be625b19c2af51", "user": {"_id": "6339029a76421c0543167075", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6339029a76421c0543167075/3npT0NxTMV-MLWDThyBy8.png", "isPro": false, "fullname": "fulong ye", "user": "Alon77777", "type": "user"}, "name": "Fulong Ye", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T11:46:59.343Z", "hidden": false}, {"_id": "69705b78a8be625b19c2af52", "name": "Chong Mou", "hidden": false}, {"_id": "69705b78a8be625b19c2af53", "user": {"_id": "6752cd83ffaeeb979db974ae", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png", "isPro": false, "fullname": "Xinghui Li", "user": "Crayon-Shinchan", "type": "user"}, "name": "Xinghui Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:52:58.973Z", "hidden": false}, {"_id": "69705b78a8be625b19c2af54", "user": {"_id": "6304e2dabad6ce7fc0287d57", "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg", "isPro": false, "fullname": "Zhuowei_Chen", "user": "ZhuoweiChen", "type": "user"}, "name": "Zhuowei Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:52:20.177Z", "hidden": false}, {"_id": "69705b78a8be625b19c2af55", "name": "Qian He", "hidden": false}, {"_id": "69705b78a8be625b19c2af56", "user": {"_id": "671aa30b496f0bc5ae04da4b", "avatarUrl": "/avatars/902d7f9fd56f84953d67d9229bd9d6b7.svg", "isPro": false, "fullname": "Mingyuan Gao", "user": "GMY1999", "type": "user"}, "name": "Mingyuan Gao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:52:13.464Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6697765937d24838267b41e7/309vLCCbUoiHGJkN0u3LH.mp4"], "publishedAt": "2026-01-20T18:58:11.000Z", "submittedOnDailyAt": "2026-01-21T02:29:32.365Z", "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer", "submittedOnDailyBy": {"_id": "6697765937d24838267b41e7", "avatarUrl": "/avatars/682bb4cf0a0009812b42748dc26916f9.svg", "isPro": false, "fullname": "PangzeCheung", "user": "PangzeCheung", "type": "user"}, "summary": "Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.", "upvotes": 29, "discussionId": "69705b78a8be625b19c2af57", "projectPage": "https://pangzecheung.github.io/OmniTransfer/", "githubRepo": "https://github.com/PangzeCheung/OmniTransfer", "githubRepoAddedBy": "user", "ai_summary": "OmniTransfer presents a unified framework for spatio-temporal video transfer that enhances appearance consistency and temporal control through multi-view information and multimodal semantic guidance.", "ai_keywords": ["video customization", "spatio-temporal video transfer", "multi-view information", "temporal cues", "temporal alignment", "appearance consistency", "reference-decoupled causal learning", "task-adaptive multimodal alignment", "pose-guided methods"], "githubStars": 54, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u6bd4\u56fe\u50cf\u6216\u6587\u672c\u4f20\u9012\u66f4\u4e30\u5bcc\u7684\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u89c6\u9891\u4e2d\u7684\u65f6\u7a7a\u4fe1\u606f\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOmniTransfer\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u65e8\u5728\u8fdb\u884c\u65f6\u7a7a\u89c6\u9891\u8f6c\u79fb\u3002</li>\n    <li>OmniTransfer\u901a\u8fc7\u591a\u89c6\u89d2\u4fe1\u606f\u63d0\u9ad8\u5916\u89c2\u4e00\u81f4\u6027\uff0c\u5e76\u5229\u7528\u65f6\u95f4\u7ebf\u7d22\u5b9e\u73b0\u7cbe\u7ec6\u7684\u65f6\u95f4\u63a7\u5236\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\uff0c\u63d0\u5347\u4e86\u89c6\u9891\u8f6c\u79fb\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOmniTransfer\u5728\u5916\u89c2\u548c\u65f6\u95f4\u8f6c\u79fb\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u8fd0\u52a8\u8f6c\u79fb\u4e2d\u8868\u73b0\u51fa\u8272\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Videos provide more detailed information than images or text, capturing both space and time.</li>\n    <li>Current video customization methods do not fully utilize the rich information in videos, limiting their effectiveness.</li>\n    <li>OmniTransfer is a new framework that improves video transfer by using multi-view information and temporal cues for better consistency and control.</li>\n    <li>It includes three key features: adaptive use of reference videos for better alignment, separation of reference and target information for efficiency, and dynamic handling of different tasks with multimodal guidance.</li>\n    <li>OmniTransfer has been shown to perform better than existing methods in appearance and temporal transfer, while maintaining high quality in motion transfer without needing pose information.</li>\n</ul>"}, "publishedAt": "2026-01-20T13:58:11.000Z", "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer", "summary": "Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6697765937d24838267b41e7/309vLCCbUoiHGJkN0u3LH.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14250.png", "numComments": 4, "submittedBy": {"_id": "6697765937d24838267b41e7", "avatarUrl": "/avatars/682bb4cf0a0009812b42748dc26916f9.svg", "fullname": "PangzeCheung", "name": "PangzeCheung", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.14192", "authors": [{"_id": "69705a68a8be625b19c2af3a", "user": {"_id": "6745c589d2d740914ec2574f", "avatarUrl": "/avatars/7b2ff6848d42cd140a775df0c2bc9384.svg", "isPro": false, "fullname": "Xiaofang Yang", "user": "fffovo", "type": "user"}, "name": "Xiaofang Yang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:48:39.583Z", "hidden": false}, {"_id": "69705a68a8be625b19c2af3b", "name": "Lijun Li", "hidden": false}, {"_id": "69705a68a8be625b19c2af3c", "user": {"_id": "660d17d6c9be0dcd31a30b3d", "avatarUrl": "/avatars/3743fe9b695c488ebe33f0d8fd607a8a.svg", "isPro": false, "fullname": "Zhou Heng", "user": "henggg", "type": "user"}, "name": "Heng Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:19:49.600Z", "hidden": false}, {"_id": "69705a68a8be625b19c2af3d", "name": "Tong Zhu", "hidden": false}, {"_id": "69705a68a8be625b19c2af3e", "name": "Xiaoye Qu", "hidden": false}, {"_id": "69705a68a8be625b19c2af3f", "name": "Yuchen Fan", "hidden": false}, {"_id": "69705a68a8be625b19c2af40", "user": {"_id": "6952244bfbddb08cb2562f3b", "avatarUrl": "/avatars/70d67319af29604129378fee3f216757.svg", "isPro": false, "fullname": "qianshan wei", "user": "b1intern", "type": "user"}, "name": "Qianshan Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:04.253Z", "hidden": false}, {"_id": "69705a68a8be625b19c2af41", "name": "Rui Ye", "hidden": false}, {"_id": "69705a68a8be625b19c2af42", "name": "Li Kang", "hidden": false}, {"_id": "69705a68a8be625b19c2af43", "name": "Yiran Qin", "hidden": false}, {"_id": "69705a68a8be625b19c2af44", "name": "Zhiqiang Kou", "hidden": false}, {"_id": "69705a68a8be625b19c2af45", "name": "Daizong Liu", "hidden": false}, {"_id": "69705a68a8be625b19c2af46", "name": "Qi Li", "hidden": false}, {"_id": "69705a68a8be625b19c2af47", "name": "Ning Ding", "hidden": false}, {"_id": "69705a68a8be625b19c2af48", "user": {"_id": "65257545b017be1fc1915364", "avatarUrl": "/avatars/9bffd3fb567d2fa1e5c3546d77560b43.svg", "isPro": false, "fullname": "Siheng Chen", "user": "sihengchen", "type": "user"}, "name": "Siheng Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:26.261Z", "hidden": false}, {"_id": "69705a68a8be625b19c2af49", "name": "Jing Shao", "hidden": false}], "publishedAt": "2026-01-20T17:51:56.000Z", "submittedOnDailyAt": "2026-01-21T02:28:39.429Z", "title": "Toward Efficient Agents: Memory, Tool learning, and Planning", "submittedOnDailyBy": {"_id": "641d3efac3983aa9491677b9", "avatarUrl": "/avatars/53565486351c16a1ac8ea863963e2d9b.svg", "isPro": false, "fullname": "Lijun Li", "user": "adwardlee", "type": "user"}, "summary": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.", "upvotes": 29, "discussionId": "69705a69a8be625b19c2af4a", "projectPage": "https://efficient-agents.github.io/", "githubRepo": "https://github.com/yxf203/Awesome-Efficient-Agents", "githubRepoAddedBy": "user", "ai_summary": "Efficiency in agentic systems is examined across memory, tool learning, and planning components, analyzing trade-offs between effectiveness and computational costs through various optimization strategies and benchmarks.", "ai_keywords": ["large language models", "agentic systems", "memory", "tool learning", "planning", "latency", "tokens", "steps", "reinforcement learning", "controlled search mechanisms", "Pareto frontier", "efficiency metrics", "evaluation protocols"], "githubStars": 27, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>\u8fd1\u5e74\u6765\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u88ab\u9010\u6e10\u6269\u5c55\u4e3a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5f15\u8d77\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002</li>\n    <li>\u672c\u6587\u7814\u7a76\u4e86\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6548\u7387\uff0c\u91cd\u70b9\u5173\u6ce8\u5185\u5b58\u3001\u5de5\u5177\u5b66\u4e60\u548c\u89c4\u5212\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u3002</li>\n    <li>\u6211\u4eec\u56de\u987e\u4e86\u591a\u79cd\u4e0d\u540c\u7684\u7814\u7a76\u65b9\u6cd5\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u9ad8\u5c42\u539f\u5219\u4e0a\u6709\u5f88\u591a\u76f8\u4f3c\u4e4b\u5904\uff0c\u4f8b\u5982\u901a\u8fc7\u538b\u7f29\u548c\u7ba1\u7406\u6765\u9650\u5236\u4e0a\u4e0b\u6587\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u6548\u7387\u8bc4\u4f30\u65b9\u5f0f\uff1a\u5728\u56fa\u5b9a\u6210\u672c\u9884\u7b97\u4e0b\u6bd4\u8f83\u6548\u679c\uff0c\u4ee5\u53ca\u5728\u76f8\u4f3c\u6548\u679c\u6c34\u5e73\u4e0b\u6bd4\u8f83\u6210\u672c\u3002</li>\n    <li>\u6587\u7ae0\u8fd8\u8ba8\u8bba\u4e86\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\uff0c\u4ee5\u63d0\u4f9b\u6709\u5e0c\u671b\u7684\u89c1\u89e3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper focuses on improving the efficiency of large language models used as agents, which is important for real-world applications.</li>\n    <li>It examines three main areas of agent efficiency: memory, tool learning, and planning, looking at costs like time and resource usage.</li>\n    <li>The authors review various recent methods that share common principles for enhancing efficiency, such as compressing information and optimizing rewards.</li>\n    <li>Efficiency is analyzed by comparing performance under fixed costs and assessing costs at similar effectiveness levels, using the Pareto frontier concept.</li>\n    <li>The paper also outlines challenges and future directions for research, aiming to provide useful insights for improving agent efficiency.</li>\n</ul>"}, "publishedAt": "2026-01-20T12:51:56.000Z", "title": "Toward Efficient Agents: Memory, Tool learning, and Planning", "summary": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14192.png", "numComments": 2, "submittedBy": {"_id": "641d3efac3983aa9491677b9", "avatarUrl": "/avatars/53565486351c16a1ac8ea863963e2d9b.svg", "fullname": "Lijun Li", "name": "adwardlee", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": false}, {"paper": {"id": "2601.13029", "authors": [{"_id": "69708ffea8be625b19c2b04c", "user": {"_id": "6575702b15b1ca184b0b2700", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575702b15b1ca184b0b2700/O9cEodqQmG-gyqMiO_edR.jpeg", "isPro": false, "fullname": "Zaibin Zhang", "user": "MrBean2024", "type": "user"}, "name": "Zaibin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T11:47:45.167Z", "hidden": false}, {"_id": "69708ffea8be625b19c2b04d", "name": "Yuhan Wu", "hidden": false}, {"_id": "69708ffea8be625b19c2b04e", "name": "Lianjie Jia", "hidden": false}, {"_id": "69708ffea8be625b19c2b04f", "name": "Yifan Wang", "hidden": false}, {"_id": "69708ffea8be625b19c2b050", "name": "Zhongbo Zhang", "hidden": false}, {"_id": "69708ffea8be625b19c2b051", "user": {"_id": "6965e7d00aa591efb07b220c", "avatarUrl": "/avatars/d0f65aafc3b652084213f02a4f93c453.svg", "isPro": false, "fullname": "Yijiang Li", "user": "luciasnowblack", "type": "user"}, "name": "Yijiang Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T11:59:41.952Z", "hidden": false}, {"_id": "69708ffea8be625b19c2b052", "name": "Binghao Ran", "hidden": false}, {"_id": "69708ffea8be625b19c2b053", "name": "Fuxi Zhang", "hidden": false}, {"_id": "69708ffea8be625b19c2b054", "user": {"_id": "68ad6a9106bcf0ebe9624dc5", "avatarUrl": "/avatars/309e383889f848c828d4b1eb4542b54a.svg", "isPro": false, "fullname": "SunZhuohan", "user": "sunz525", "type": "user"}, "name": "Zhuohan Sun", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T11:59:29.495Z", "hidden": false}, {"_id": "69708ffea8be625b19c2b055", "user": {"_id": "64e314ad24809d7fa0f20fbc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bHE0w_hjDFvU-Aul0_E7g.jpeg", "isPro": false, "fullname": "Zhenfei Yin", "user": "JeremyYin", "type": "user"}, "name": "Zhenfei Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T11:59:18.401Z", "hidden": false}, {"_id": "69708ffea8be625b19c2b056", "name": "Lijun Wang", "hidden": false}, {"_id": "69708ffea8be625b19c2b057", "name": "Huchuan Lu", "hidden": false}], "publishedAt": "2026-01-19T13:13:54.000Z", "submittedOnDailyAt": "2026-01-21T06:09:04.854Z", "title": "Think3D: Thinking with Space for Spatial Reasoning", "submittedOnDailyBy": {"_id": "6419309f22270b3ccf177c77", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6419309f22270b3ccf177c77/KQa1586iBBKqucUlfpuPp.jpeg", "isPro": false, "fullname": "William Li", "user": "williamium", "type": "user"}, "summary": "Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.", "upvotes": 29, "discussionId": "69708fffa8be625b19c2b058", "githubRepo": "https://github.com/zhangzaibin/spagent", "githubRepoAddedBy": "user", "ai_summary": "Think3D enhances vision-language models' 3D reasoning capabilities by enabling interactive spatial exploration through 3D reconstruction and camera-based operations, improving performance without additional training.", "ai_keywords": ["vision large models", "3D reconstruction models", "point clouds", "camera poses", "spatial reasoning", "3D chain-of-thought process", "reinforcement learning policy", "multimodal agents", "3D reasoning"], "githubStars": 32, "summary_zh": "<ul>\n    <li>\u7406\u89e3\u7269\u7406\u4e16\u754c\u9700\u8981\u7a7a\u95f4\u667a\u80fd\uff0c\u80fd\u591f\u8d85\u8d8a\u4e8c\u7ef4\u611f\u77e5\u8fdb\u884c\u4e09\u7ef4\u63a8\u7406\u3002</li>\n    <li>Think3D\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u5e2e\u52a9\u89c6\u89c9\u5927\u578b\u6a21\u578b\uff08VLM\uff09\u8fdb\u884c\u4e09\u7ef4\u601d\u8003\u3002</li>\n    <li>\u901a\u8fc73D\u91cd\u5efa\u6a21\u578b\uff0cThink3D\u5141\u8bb8\u4ee3\u7406\u4e3b\u52a8\u64cd\u4f5c\u7a7a\u95f4\uff0c\u63d0\u5347\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5e94\u7528Think3D\u540e\uff0cGPT-4.1\u548cGemini 2.5 Pro\u7684\u7a7a\u95f4\u63a8\u7406\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002</li>\n    <li>\u5c0f\u6a21\u578b\u5728\u5f3a\u5316\u5b66\u4e60\u653f\u7b56\u652f\u6301\u4e0b\u4e5f\u80fd\u663e\u8457\u6539\u5584\u7a7a\u95f4\u63a2\u7d22\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Think3D is a new framework that helps vision models understand and think in 3D space, going beyond their usual 2D capabilities.</li>\n    <li>It uses 3D reconstruction models to help agents actively manipulate and reason about 3D environments through camera movements.</li>\n    <li>Think3D improves the spatial reasoning abilities of advanced models like GPT-4.1 and Gemini 2.5 Pro without needing extra training, with notable performance gains on specific tests.</li>\n    <li>Smaller models that struggle with spatial tasks benefit from a reinforcement learning approach, which helps them choose better viewpoints and operations for improved performance.</li>\n    <li>The results suggest that using tools for spatial exploration can lead to more flexible and human-like 3D reasoning in multimodal AI systems.</li>\n</ul>"}, "publishedAt": "2026-01-19T08:13:54.000Z", "title": "Think3D: Thinking with Space for Spatial Reasoning", "summary": "Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13029.png", "numComments": 1, "submittedBy": {"_id": "6419309f22270b3ccf177c77", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6419309f22270b3ccf177c77/KQa1586iBBKqucUlfpuPp.jpeg", "fullname": "William Li", "name": "williamium", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u5728\u89c6\u9891\u95ee\u7b54\u573a\u666f\u4e2d\uff0c\u89c6\u9891\u901a\u5e38\u53ea\u63d0\u4f9b\u5c40\u90e8\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u7b54\u6848\u9700\u8981\u4ece\u4e92\u8054\u7f51\u4e0a\u83b7\u53d6\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0c\u53eb\u505aVideoDR\u3002</li>\n    <li>VideoDR\u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u4e0b\u7684\u5f00\u653e\u9886\u57df\u89c6\u9891\u95ee\u7b54\uff0c\u9700\u8981\u63d0\u53d6\u8de8\u5e27\u89c6\u89c9\u951a\u70b9\u3001\u8fdb\u884c\u4ea4\u4e92\u5f0f\u7f51\u9875\u68c0\u7d22\u548c\u591a\u8df3\u63a8\u7406\u9a8c\u8bc1\u3002</li>\n    <li>\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u201cAgentic\u201d\u65b9\u6cd5\u5e76\u4e0d\u603b\u662f\u4f18\u4e8e\u201cWorkflow\u201d\u65b9\u6cd5\uff0c\u6548\u679c\u4f9d\u8d56\u4e8e\u6a21\u578b\u5728\u957f\u671f\u68c0\u7d22\u94fe\u4e2d\u4fdd\u6301\u521d\u59cb\u89c6\u9891\u951a\u70b9\u7684\u80fd\u529b\u3002</li>\n    <li>VideoDR\u4e3a\u7814\u7a76\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u89c6\u9891\u4ee3\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u4e0b\u4e00\u4ee3\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u5173\u952e\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Real-world video question answering needs to extract clues from different frames, retrieve information from the web, and verify answers through reasoning.</li>\n    <li>VideoDR is a new benchmark created for video question answering that combines video information with web research.</li>\n    <li>It involves extracting visual clues from videos, retrieving data online, and reasoning across multiple pieces of evidence.</li>\n    <li>Tests on various language models showed no clear winner between two approaches called Workflow and Agentic; success varies based on how well models handle visual information during retrieval.</li>\n    <li>VideoDR highlights important challenges like keeping focus on the original video content and maintaining consistency over long research processes.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u5f53\u4eba\u7c7b\u5728\u53d1\u5c55\u57fa\u672c\u89c6\u89c9\u6280\u80fd\u65f6\uff0c\u73b0\u4ee3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4ecd\u7136\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u6765\u5f25\u8865\u89c6\u89c9\u7406\u89e3\u7684\u4e0d\u8db3\u3002</li>\n    <li>\u6211\u4eec\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684MLLMs\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u8fd9\u4e9b\u4efb\u52a1\u4eba\u7c7b\uff08\u751a\u81f33\u5c81\u513f\u7ae5\uff09\u90fd\u80fd\u8f7b\u677e\u5b8c\u6210\u3002</li>\n    <li>\u4e3a\u6b64\uff0c\u6211\u4eec\u63a8\u51fa\u4e86BabyVision\u57fa\u51c6\uff0c\u65e8\u5728\u8bc4\u4f30MLLMs\u5728\u4e0d\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u7684\u6838\u5fc3\u89c6\u89c9\u80fd\u529b\u3002</li>\n    <li>BabyVision\u6db5\u76d6388\u4e2a\u9879\u76ee\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\u548c\u56db\u4e2a\u4e3b\u8981\u7c7b\u522b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u9886\u5148\u7684MLLMs\u8868\u73b0\u660e\u663e\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c6\u3002</li>\n    <li>\u5f53\u524dMLLMs\u5728\u57fa\u7840\u89c6\u89c9\u80fd\u529b\u65b9\u9762\u4ecd\u7136\u4e0d\u8db3\uff0cBabyVision\u7684\u8fdb\u5c55\u662f\u671d\u7740\u4eba\u7c7b\u7ea7\u522b\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans develop visual skills early, but modern Multimodal LLMs (MLLMs) rely too much on language to understand visuals.</li>\n    <li>State-of-the-art MLLMs struggle with basic visual tasks that even young children can do easily.</li>\n    <li>BabyVision is a new benchmark created to test visual abilities of MLLMs without using language knowledge.</li>\n    <li>BabyVision includes 388 tasks across 22 subclasses and shows that leading MLLMs score much lower than humans.</li>\n    <li>Results indicate that while MLLMs perform well in knowledge-heavy tasks, they still lack basic visual understanding.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u542b\u6709\u4e30\u5bcc\u7684\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\uff0c\u91cd\u8981\u4e8e\u4ece\u536b\u661f\u5f71\u50cf\u4e2d\u8fdb\u884c\u5206\u5272\u3002</li>\n    <li>\u76ee\u524d\u7684\u5206\u5272\u6a21\u578b\u80fd\u6709\u6548\u5904\u7406\u7269\u7406\u5c5e\u6027\u7684\u5b9e\u4f53\uff0c\u4f46\u5bf9\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\uff08\u5982\u5b66\u6821\u3001\u516c\u56ed\uff09\u4ecd\u6709\u56f0\u96be\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u201cSocioSeg\u201d\u6570\u636e\u96c6\uff0c\u5305\u62ec\u536b\u661f\u5f71\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u65b0\u7684\u6846\u67b6\u201cSocioReasoner\u201d\uff0c\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u6807\u6ce8\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban surfaces contain many different types of entities that can be identified from satellite images.</li>\n    <li>Current models can segment physical entities well but struggle with socially defined categories like schools and parks.</li>\n    <li>We created a new dataset called SocioSeg, which includes satellite images, digital maps, and detailed labels for social entities.</li>\n    <li>Our framework, SocioReasoner, helps identify these social entities using a method similar to human reasoning and learning techniques.</li>\n    <li>Experiments show that our method performs better than existing models and can generalize well to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u5e73\u8861\u7d27\u51d1\u6548\u7387\u4e0e\u591a\u6a21\u6001\u667a\u80fd\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u6d41\u7a0b\u5b9e\u73b0\uff0c\u4f7f\u7528\u4e861.2T\u7684\u591a\u6a21\u6001\u6570\u636e\u3002</li>\n    <li>\u5f15\u5165\u4e86\u5e76\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\uff0c\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u65f6\u7684\u8ba1\u7b97\u6548\u7387\u3002</li>\n    <li>\u5c3d\u7ba1\u6a21\u578b\u8f83\u5c0f\uff08\u4ec510\u4ebf\u53c2\u6570\uff09\uff0c\u4f46\u5176\u6027\u80fd\u8d85\u8fc7\u4e86\u8bb8\u591a\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u9ad8\u6548\u7684\u57fa\u51c6\u6a21\u578b\u4f9b\u793e\u533a\u4f7f\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a small, open-source AI model that balances efficiency and advanced intelligence across different types of data.</li>\n    <li>The model uses a unique training method with a large dataset to combine language understanding and visual perception.</li>\n    <li>It includes a special reinforcement learning process to enhance its performance after initial training.</li>\n    <li>Despite being smaller than many other models, STEP3-VL-10B performs as well or better than much larger models and top commercial systems.</li>\n    <li>The creators are sharing the model to help others in the community work with it easily and effectively.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u6839\u636e\u89c6\u89c9\u7ebf\u7d22\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u672a\u5145\u5206\u5229\u7528\u4eba\u7c7b\u5e38\u7528\u7684\u5730\u56fe\u7b56\u7565\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u601d\u7ef4\u5730\u56fe\u201d\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u8bbe\u8ba1\u4e3a\u4e00\u4e2a\u57fa\u4e8e\u5730\u56fe\u7684\u4ee3\u7406\u5faa\u73af\u3002</li>\n    <li>\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728Acc@500m\u6307\u6807\u4e0a\u4ece8.0%\u63d0\u5347\u81f322.1%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The image geolocalization task aims to find out where a photo was taken anywhere on Earth using visual clues.</li>\n    <li>Current models use advanced reasoning but often ignore a common human strategy: using maps.</li>\n    <li>This study introduces a new approach that enhances models with map-using abilities through a two-step training process.</li>\n    <li>The method includes agentic reinforcement learning for better decision-making and parallel test-time scaling to explore different paths before making a final prediction.</li>\n    <li>Tests show that this new method significantly outperforms existing models, especially in predicting locations accurately within 500 meters.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.12538", "authors": [{"_id": "6971913fc1c7409747bf9564", "name": "Tianxin Wei", "hidden": false}, {"_id": "6971913fc1c7409747bf9565", "user": {"_id": "6742eb40924e80c3c80ebe13", "avatarUrl": "/avatars/e6ccb1a89a1ea0bfca70779966f4f429.svg", "isPro": false, "fullname": "Ting-Wei Li", "user": "tingwl0122", "type": "user"}, "name": "Ting-Wei Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T17:12:21.531Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9566", "name": "Zhining Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf9567", "name": "Xuying Ning", "hidden": false}, {"_id": "6971913fc1c7409747bf9568", "name": "Ze Yang", "hidden": false}, {"_id": "6971913fc1c7409747bf9569", "name": "Jiaru Zou", "hidden": false}, {"_id": "6971913fc1c7409747bf956a", "name": "Zhichen Zeng", "hidden": false}, {"_id": "6971913fc1c7409747bf956b", "name": "Ruizhong Qiu", "hidden": false}, {"_id": "6971913fc1c7409747bf956c", "name": "Xiao Lin", "hidden": false}, {"_id": "6971913fc1c7409747bf956d", "name": "Dongqi Fu", "hidden": false}, {"_id": "6971913fc1c7409747bf956e", "name": "Zihao Li", "hidden": false}, {"_id": "6971913fc1c7409747bf956f", "user": {"_id": "653962e75c8e4863e1a2068f", "avatarUrl": "/avatars/d4f5f5da141f37d53ca1986ff17b325e.svg", "isPro": false, "fullname": "Mengting Ai", "user": "famous-blue-raincoat", "type": "user"}, "name": "Mengting Ai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:10.378Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9570", "user": {"_id": "677830bd3f2e3ec475576303", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dhwqUDkk66m4oSGSbcd7j.png", "isPro": false, "fullname": "Duo Zhou", "user": "Claudius7", "type": "user"}, "name": "Duo Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:12.476Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9571", "name": "Wenxuan Bao", "hidden": false}, {"_id": "6971913fc1c7409747bf9572", "user": {"_id": "646323556c27a7e33b23f198", "avatarUrl": "/avatars/17fe142f689ab4be3c2374d1d90393db.svg", "isPro": false, "fullname": "Yunzhe Li", "user": "yunzhel2", "type": "user"}, "name": "Yunzhe Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:14.383Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9573", "name": "Gaotang Li", "hidden": false}, {"_id": "6971913fc1c7409747bf9574", "name": "Cheng Qian", "hidden": false}, {"_id": "6971913fc1c7409747bf9575", "name": "Yu Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf9576", "name": "Xiangru Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf9577", "name": "Yin Xiao", "hidden": false}, {"_id": "6971913fc1c7409747bf9578", "name": "Liri Fang", "hidden": false}, {"_id": "6971913fc1c7409747bf9579", "name": "Hui Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf957a", "name": "Xianfeng Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf957b", "name": "Yuji Zhang", "hidden": false}, {"_id": "6971913fc1c7409747bf957c", "name": "Chi Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf957d", "name": "Jiaxuan You", "hidden": false}, {"_id": "6971913fc1c7409747bf957e", "name": "Heng Ji", "hidden": false}, {"_id": "6971913fc1c7409747bf957f", "name": "Hanghang Tong", "hidden": false}, {"_id": "6971913fc1c7409747bf9580", "name": "Jingrui He", "hidden": false}], "publishedAt": "2026-01-18T18:58:23.000Z", "submittedOnDailyAt": "2026-01-22T00:27:25.162Z", "title": "Agentic Reasoning for Large Language Models", "submittedOnDailyBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "isPro": false, "fullname": "Jiaru Zou", "user": "jiaruz2", "type": "user"}, "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "upvotes": 125, "discussionId": "69719140c1c7409747bf9581", "githubRepo": "https://github.com/weitianxin/Awesome-Agentic-Reasoning", "githubRepoAddedBy": "user", "ai_summary": "Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.", "ai_keywords": ["large language models", "agentic reasoning", "autonomous agents", "planning", "tool use", "search", "feedback", "memory", "adaptation", "collaborative settings", "coordination", "knowledge sharing", "reinforcement learning", "supervised fine-tuning", "in-context reasoning", "post-training reasoning", "real-world applications", "benchmarks", "thought and action", "world modeling", "scalable multi-agent training", "governance"], "githubStars": 105, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "summary_zh": "<ul>\n    <li>\u63a8\u7406\u662f\u63a8\u65ad\u3001\u89e3\u51b3\u95ee\u9898\u548c\u51b3\u7b56\u7684\u57fa\u672c\u8ba4\u77e5\u8fc7\u7a0b\u3002</li>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5c01\u95ed\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5f00\u653e\u548c\u52a8\u6001\u73af\u5883\u4e2d\u9762\u4e34\u6311\u6218\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u5c06LLMs\u91cd\u65b0\u5b9a\u4e49\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u901a\u8fc7\u6301\u7eed\u4e92\u52a8\u8fdb\u884c\u8ba1\u5212\u3001\u884c\u52a8\u548c\u5b66\u4e60\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u5206\u4e3a\u4e09\u4e2a\u5c42\u6b21\uff1a\u57fa\u7840\u4ee3\u7406\u63a8\u7406\uff08\u7a33\u5b9a\u73af\u5883\u4e2d\u7684\u6838\u5fc3\u80fd\u529b\uff09\u3001\u81ea\u6211\u6f14\u5316\u4ee3\u7406\u63a8\u7406\uff08\u901a\u8fc7\u53cd\u9988\u548c\u9002\u5e94\u6539\u8fdb\u80fd\u529b\uff09\u548c\u96c6\u4f53\u591a\u4ee3\u7406\u63a8\u7406\uff08\u5728\u534f\u4f5c\u73af\u5883\u4e2d\u5171\u4eab\u77e5\u8bc6\u548c\u76ee\u6807\uff09\u3002</li>\n    <li>\u672c\u6587\u56de\u987e\u4e86\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u4ee3\u7406\u63a8\u7406\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7684\u6311\u6218\u548c\u65b9\u5411\uff0c\u5982\u4e2a\u6027\u5316\u3001\u957f\u671f\u4e92\u52a8\u548c\u53ef\u6269\u5c55\u7684\u591a\u4ee3\u7406\u8bad\u7ec3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reasoning is crucial for making decisions and solving problems, but large language models (LLMs) struggle in changing environments.</li>\n    <li>Agentic reasoning views LLMs as independent agents that can learn and adapt through interactions.</li>\n    <li>It is organized into three levels: basic abilities for planning and tool use, self-improvement through feedback, and teamwork among multiple agents.</li>\n    <li>The survey discusses how reasoning can be improved through structured interactions and training methods.</li>\n    <li>It also highlights future challenges, such as personalizing AI, improving long-term interactions, and ensuring safe real-world use.</li>\n</ul>"}, "publishedAt": "2026-01-18T13:58:23.000Z", "title": "Agentic Reasoning for Large Language Models", "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12538.png", "numComments": 3, "submittedBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "fullname": "Jiaru Zou", "name": "jiaruz2", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.08763", "authors": [{"_id": "6969b0a232f0333869ff946a", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:38.232Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946b", "user": {"_id": "6891c906f3c31445cc040ab1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891c906f3c31445cc040ab1/NBqxXOY7al4CD0XBj8ke2.jpeg", "isPro": false, "fullname": "Yucheng Wang", "user": "DevilEnfant", "type": "user"}, "name": "Yucheng Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:48.080Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946c", "name": "Yufei He", "hidden": false}, {"_id": "6969b0a232f0333869ff946d", "user": {"_id": "682deb444988bd82847e2b03", "avatarUrl": "/avatars/15da087e84386ea72c6fa2db63571420.svg", "isPro": false, "fullname": "Jia-Ying Wu", "user": "EricaWu", "type": "user"}, "name": "Jiaying Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:59.692Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946e", "name": "Yilun Zhao", "hidden": false}, {"_id": "6969b0a232f0333869ff946f", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0a232f0333869ff9470", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:06.327Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9471", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:12.181Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9472", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:17.979Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9473", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:23.007Z", "hidden": false}], "publishedAt": "2026-01-13T17:48:43.000Z", "submittedOnDailyAt": "2026-01-16T01:00:36.686Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "upvotes": 111, "discussionId": "6969b0a232f0333869ff9474", "ai_summary": "Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.", "ai_keywords": ["reinforcement learning", "large language models", "exploration collapse", "pass@k", "pass@1", "rollout-level objective", "high-level solution strategies", "clustering", "policy advantages", "AUC@K"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\u5728\u540e\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u53d8\u5f97\u975e\u5e38\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u3002</li>\n    <li>\u5f3a\u5316\u5b66\u4e60\u5e38\u5e38\u9762\u4e34\u63a2\u7d22\u5d29\u6e83\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u653f\u7b56\u8fc7\u65e9\u96c6\u4e2d\u4e8e\u5c11\u6570\u4e3b\u5bfc\u63a8\u7406\u6a21\u5f0f\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u53eb\u505a\u72ec\u7279\u6027\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\uff0c\u5956\u52b1\u90a3\u4e9b\u5c55\u73b0\u7a00\u6709\u9ad8\u5c42\u7b56\u7565\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u805a\u7c7b\u76f8\u540c\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5173\u6ce8\u9ad8\u5c42\u7b56\u7565\uff0c\u800c\u5ffd\u7565\u8868\u9762\u5dee\u5f02\uff0c\u91cd\u65b0\u52a0\u6743\u653f\u7b56\u4f18\u52bf\u3002</li>\n    <li>\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u533b\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86pass@k\u7684\u8868\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u4e86pass@1\u7684\u6210\u7ee9\uff0c\u589e\u52a0\u4e86\u591a\u6837\u5316\u7684\u89e3\u51b3\u7b56\u7565\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) is important for improving large language models (LLMs) in complex reasoning tasks.</li>\n    <li>Current RL methods can lead to \"exploration collapse,\" where models focus too much on a few dominant reasoning patterns.</li>\n    <li>The proposed solution, called Uniqueness-Aware Reinforcement Learning, encourages diversity in problem-solving strategies.</li>\n    <li>This method rewards unique and correct solutions more than common ones by grouping similar strategies and adjusting rewards based on their uniqueness.</li>\n    <li>The approach has shown to improve performance in various fields, increasing the success rate over multiple attempts while maintaining consistent results.</li>\n</ul>"}, "publishedAt": "2026-01-13T12:48:43.000Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08763.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 106, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 98, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u662f\u4e00\u79cd\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u7684\u7b56\u7565\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u6a21\u5757\u4e3b\u8981\u4f5c\u4e3a\u88ab\u52a8\u5b58\u50a8\uff0c\u7f3a\u4e4f\u5bf9\u4fe1\u606f\u4e4b\u95f4\u9ad8\u9636\u5173\u8054\u7684\u5904\u7406\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51faHGMem\uff0c\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u65e8\u5728\u589e\u5f3a\u8bb0\u5fc6\u7684\u52a8\u6001\u6027\u548c\u8868\u8fbe\u80fd\u529b\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u8fb9\u8fde\u63a5\u4e0d\u540c\u7684\u8bb0\u5fc6\u5355\u5143\uff0c\u4fc3\u8fdb\u66f4\u9ad8\u9636\u7684\u4ea4\u4e92\u548c\u7efc\u5408\u7406\u89e3\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660eHGMem\u5728\u591a\u4e2a\u5168\u7403\u7406\u89e3\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u591a\u6b65\u9aa4RAG\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps large language models understand complex tasks better.</li>\n    <li>Current memory systems in RAG mostly store information passively and do not effectively connect related facts.</li>\n    <li>This lack of connection can lead to weak reasoning and understanding over time.</li>\n    <li>HGMem introduces a new memory system that uses a hypergraph structure to better connect and organize information.</li>\n    <li>Tests show that HGMem improves multi-step reasoning and performs better than existing systems on various tasks.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u63d0\u5347\uff0c\u7528\u6237\u5e0c\u671b\u5b83\u4eec\u63d0\u4f9b\u51c6\u786e\u7684\u56de\u7b54\u548c\u7b26\u5408\u591a\u6837\u5316\u4eba\u7c7b\u504f\u597d\u7684\u884c\u4e3a\u3002</li>\n    <li>\u4e3a\u6b64\uff0c\u5f3a\u5316\u5b66\u4e60\u5f00\u59cb\u4f7f\u7528\u591a\u4e2a\u5956\u52b1\u6765\u5f15\u5bfc\u6a21\u578b\u8fbe\u5230\u671f\u671b\u7684\u884c\u4e3a\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\u76f4\u63a5\u5e94\u7528\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u4f1a\u5bfc\u81f4\u5956\u52b1\u7ec4\u5408\u5931\u53bb\u5dee\u5f02\uff0c\u964d\u4f4e\u8bad\u7ec3\u6548\u679c\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5GDPO\uff0c\u89e3\u51b3\u4e86\u5956\u52b1\u5f52\u4e00\u5316\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u5728\u5de5\u5177\u8c03\u7528\u3001\u6570\u5b66\u63a8\u7406\u548c\u7f16\u7801\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\uff0cGDPO\u7684\u8868\u73b0 consistently \u4f18\u4e8e GRPO\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Users expect language models to give accurate answers and behave according to different human preferences.</li>\n    <li>Reinforcement learning (RL) uses multiple rewards to guide models in achieving these behaviors.</li>\n    <li>Applying Group Relative Policy Optimization (GRPO) to these rewards can lead to problems, such as reduced training effectiveness and early failures.</li>\n    <li>The new method, Group reward-Decoupled Normalization Policy Optimization (GDPO), improves upon GRPO by keeping individual rewards separate and preserving their differences.</li>\n    <li>GDPO has shown better results than GRPO in tasks like tool calling, math reasoning, and coding reasoning, both in accuracy and adherence to constraints.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.08521", "authors": [{"_id": "69674059c5e371f6b235d1d8", "user": {"_id": "68920f91bcf2b25e8e121cf6", "avatarUrl": "/avatars/4bc69f43828a346a3ee24b026e0edbb4.svg", "isPro": false, "fullname": "Fengkai Yang", "user": "ShortCatisLong", "type": "user"}, "name": "Fengkai Yang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-19T14:33:21.899Z", "hidden": false}, {"_id": "69674059c5e371f6b235d1d9", "user": {"_id": "6969715fb2636f5f23a9a8c5", "avatarUrl": "/avatars/5e75043891ee41bb980f71fb9e3a33ab.svg", "isPro": false, "fullname": "Zherui Chen", "user": "chenzherui007", "type": "user"}, "name": "Zherui Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:33:53.078Z", "hidden": false}, {"_id": "69674059c5e371f6b235d1da", "name": "Xiaohan Wang", "hidden": false}, {"_id": "69674059c5e371f6b235d1db", "name": "Xiaodong Lu", "hidden": false}, {"_id": "69674059c5e371f6b235d1dc", "user": {"_id": "666eb642a119281ee0bfa443", "avatarUrl": "/avatars/71317810b00978754ad439837b04faff.svg", "isPro": false, "fullname": "Jiajun Chai", "user": "PandaChai", "type": "user"}, "name": "Jiajun Chai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-19T14:37:17.404Z", "hidden": false}, {"_id": "69674059c5e371f6b235d1dd", "name": "Guojun Yin", "hidden": false}, {"_id": "69674059c5e371f6b235d1de", "name": "Wei Lin", "hidden": false}, {"_id": "69674059c5e371f6b235d1df", "name": "Shuai Ma", "hidden": false}, {"_id": "69674059c5e371f6b235d1e0", "name": "Fuzhen Zhuang", "hidden": false}, {"_id": "69674059c5e371f6b235d1e1", "name": "Deqing Wang", "hidden": false}, {"_id": "69674059c5e371f6b235d1e2", "name": "Yaodong Yang", "hidden": false}, {"_id": "69674059c5e371f6b235d1e3", "name": "Jianxin Li", "hidden": false}, {"_id": "69674059c5e371f6b235d1e4", "user": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "isPro": false, "fullname": "Yikun Ban", "user": "Yikunb", "type": "user"}, "name": "Yikun Ban", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:33:50.655Z", "hidden": false}], "publishedAt": "2026-01-13T13:03:15.000Z", "submittedOnDailyAt": "2026-01-19T00:20:58.837Z", "title": "Your Group-Relative Advantage Is Biased", "submittedOnDailyBy": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "isPro": false, "fullname": "Yikun Ban", "user": "Yikunb", "type": "user"}, "summary": "Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.\n  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.", "upvotes": 95, "discussionId": "6967405ac5e371f6b235d1e5", "ai_summary": "Group-based reinforcement learning from verifier rewards suffers from biased advantage estimation that underestimates hard prompts and overestimates easy prompts, which is addressed through a history-aware adaptive difficulty weighting method that improves performance on mathematical reasoning benchmarks.", "ai_keywords": ["Reinforcement Learning from Verifier Rewards", "group-based methods", "GRPO", "advantage estimation", "bias correction", "adaptive reweighting", "difficulty weighting", "mathematical reasoning", "benchmark evaluation"], "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u9a8c\u8bc1\u8005\u5956\u52b1\uff08RLVR\uff09\u88ab\u5e7f\u6cdb\u7528\u4e8e\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u63a8\u7406\u4efb\u52a1\u3002</li>\n    <li>\u76ee\u524d\u7684\u7fa4\u4f53\u65b9\u6cd5\uff08\u5982GRPO\uff09\u5728\u4f7f\u7528\u7fa4\u4f53\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\u65f6\u5b58\u5728\u7406\u8bba\u6027\u95ee\u9898\uff0c\u4e3b\u8981\u662f\u4f30\u8ba1\u5b58\u5728\u504f\u5dee\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u7fa4\u4f53\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\u5728\u56f0\u96be\u63d0\u793a\u4e0b\u901a\u5e38\u4f4e\u4f30\u4f18\u52bf\uff0c\u800c\u5728\u7b80\u5355\u63d0\u793a\u4e0b\u5219\u9ad8\u4f30\u4f18\u52bf\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff1a\u5386\u53f2\u611f\u77e5\u81ea\u9002\u5e94\u96be\u5ea6\u52a0\u6743\uff08HA-DW\uff09\uff0c\u5b83\u6839\u636e\u96be\u5ea6\u53d8\u5316\u8c03\u6574\u4f18\u52bf\u4f30\u8ba1\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06HA-DW\u6574\u5408\u5230GRPO\u53ca\u5176\u53d8\u4f53\u4e2d\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u7ea0\u6b63\u504f\u5dee\u5bf9\u8bad\u7ec3\u7684\u6709\u6548\u6027\u81f3\u5173\u91cd\u8981\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement Learning from Verifier Rewards (RLVR) is popular for training large language models on reasoning tasks.</li>\n    <li>Group-based methods like GRPO are commonly used but have issues with understanding their theoretical properties.</li>\n    <li>The study finds that group-relative advantage estimators are biased, underestimating hard prompts and overestimating easy ones.</li>\n    <li>This bias causes problems with exploration and exploitation in learning.</li>\n    <li>The authors propose a solution called History-Aware Adaptive Difficulty Weighting (HA-DW) that improves performance by correcting these biases.</li>\n</ul>"}, "publishedAt": "2026-01-13T08:03:15.000Z", "title": "Your Group-Relative Advantage Is Biased", "summary": "Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.\n  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08521.png", "numComments": 5, "submittedBy": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "fullname": "Yikun Ban", "name": "Yikunb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Jan 23, 2026";
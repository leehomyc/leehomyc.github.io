window.trendingPapers = {
    "today": [{"paper": {"id": "2512.13586", "authors": [{"_id": "6940d86d65f1e24a117805cd", "user": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "isPro": false, "fullname": "Jia-Nan Li", "user": "JinaLeejnl", "type": "user"}, "name": "Jia-Nan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:38:49.797Z", "hidden": false}, {"_id": "6940d86d65f1e24a117805ce", "name": "Jian Guan", "hidden": false}, {"_id": "6940d86d65f1e24a117805cf", "name": "Wei Wu", "hidden": false}, {"_id": "6940d86d65f1e24a117805d0", "name": "Chongxuan Li", "hidden": false}], "publishedAt": "2025-12-15T17:41:19.000Z", "submittedOnDailyAt": "2025-12-16T01:39:12.101Z", "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "submittedOnDailyBy": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "isPro": false, "fullname": "Jia-Nan Li", "user": "JinaLeejnl", "type": "user"}, "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.", "upvotes": 74, "discussionId": "6940d86d65f1e24a117805d1", "githubRepo": "https://github.com/ML-GSAI/ReFusion", "githubRepoAddedBy": "user", "ai_summary": "ReFusion, a novel masked diffusion model, improves performance and efficiency by using slot-based parallel decoding, achieving superior results compared to autoregressive models and traditional masked diffusion models.", "ai_keywords": ["autoregressive models", "masked diffusion models", "Key-Value caching", "parallel decoding", "token level", "slot level", "diffusion-based planning", "autoregressive infilling", "slot-based design", "slot-level permutation space"], "githubStars": 18, "summary_zh": "<ul>\n    <li>\u81ea\u56de\u5f52\u6a21\u578b\uff08ARMs\uff09\u5728\u63a8\u7406\u65f6\u901f\u5ea6\u8f83\u6162\u3002</li>\n    <li>\u63a9\u853d\u6269\u6563\u6a21\u578b\uff08MDMs\uff09\u867d\u7136\u53ef\u4ee5\u5e76\u884c\u5904\u7406\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u751f\u6210\u4e0d\u8fde\u8d2f\u7684\u95ee\u9898\u3002</li>\n    <li>ReFusion\u662f\u4e00\u4e2a\u65b0\u578b\u7684\u63a9\u853d\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u63d0\u5347\u5e76\u884c\u89e3\u7801\u81f3\u66f4\u9ad8\u7684\u69fd\u7ea7\u522b\u6765\u63d0\u9ad8\u6027\u80fd\u548c\u6548\u7387\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u201c\u8ba1\u5212\u4e0e\u586b\u5145\u201d\u7684\u8fed\u4ee3\u89e3\u7801\u8fc7\u7a0b\uff0c\u5148\u8bc6\u522b\u5f31\u4f9d\u8d56\u69fd\uff0c\u518d\u5e76\u884c\u89e3\u7801\u8fd9\u4e9b\u69fd\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cReFusion\u5728\u6027\u80fd\u4e0a\u6bd4\u4e4b\u524d\u7684MDMs\u63d0\u9ad8\u4e8634%\uff0c\u901f\u5ea6\u5e73\u5747\u63d0\u5347\u8d85\u8fc718\u500d\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u63a5\u8fd1\u5f3a\u81ea\u56de\u5f52\u6a21\u578b\u540c\u65f6\u4fdd\u63012.33\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoregressive models (ARMs) have slow inference times, while masked diffusion models (MDMs) face issues like high computational costs and incoherent outputs.</li>\n    <li>ReFusion is a new masked diffusion model that improves efficiency by processing data in larger segments called slots instead of individual tokens.</li>\n    <li>It uses a two-step process: first, it plans which slots to fill, and then it fills them in parallel.</li>\n    <li>This slot-based approach allows better use of memory (KV cache) and simplifies learning by reducing complexity.</li>\n    <li>Tests show that ReFusion outperforms previous MDMs by 34% and is over 18 times faster on average, while also being faster than traditional ARMs.</li>\n</ul>"}, "publishedAt": "2025-12-15T12:41:19.000Z", "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13586.png", "numComments": 2, "submittedBy": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "fullname": "Jia-Nan Li", "name": "JinaLeejnl", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "isAuthorParticipating": true}, {"paper": {"id": "2512.13687", "authors": [{"_id": "6940ee0665f1e24a1178066d", "user": {"_id": "67756c9c846a267749304255", "avatarUrl": "/avatars/01f09805b561887c55d1b9ad4e96b461.svg", "isPro": false, "fullname": "Jingfeng Yao", "user": "MapleF9", "type": "user"}, "name": "Jingfeng Yao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T10:34:03.365Z", "hidden": false}, {"_id": "6940ee0665f1e24a1178066e", "user": {"_id": "6264bf5a1ed8d81e47ae3a62", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650769741842-noauth.jpeg", "isPro": false, "fullname": "Yuda Song", "user": "IDKiro", "type": "user"}, "name": "Yuda Song", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:38.071Z", "hidden": false}, {"_id": "6940ee0665f1e24a1178066f", "user": {"_id": "64192280d459c9e7fbb03aa1", "avatarUrl": "/avatars/89935de92f3f9107d7b768b82fb27e70.svg", "isPro": false, "fullname": "Zhou", "user": "Yucong", "type": "user"}, "name": "Yucong Zhou", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T10:34:50.881Z", "hidden": false}, {"_id": "6940ee0665f1e24a11780670", "user": {"_id": "62600de6d47e3dbae32ce1ce", "avatarUrl": "/avatars/a536417cfec6e10ac415091bd1829426.svg", "isPro": false, "fullname": "Xinggang Wang", "user": "xinggangw", "type": "user"}, "name": "Xinggang Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T10:34:40.199Z", "hidden": false}], "publishedAt": "2025-12-15T18:59:54.000Z", "submittedOnDailyAt": "2025-12-16T07:11:50.997Z", "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation", "submittedOnDailyBy": {"_id": "67756c9c846a267749304255", "avatarUrl": "/avatars/01f09805b561887c55d1b9ad4e96b461.svg", "isPro": false, "fullname": "Jingfeng Yao", "user": "MapleF9", "type": "user"}, "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.", "upvotes": 66, "discussionId": "6940ee0665f1e24a11780671", "githubRepo": "https://github.com/MiniMax-AI/VTP", "githubRepoAddedBy": "user", "ai_summary": "A unified visual tokenizer pre-training framework (VTP) improves generative performance by optimizing image-text contrastive, self-supervised, and reconstruction losses, leading to better scaling properties and higher zero-shot accuracy and faster convergence.", "ai_keywords": ["latent space", "visual tokenizers", "VAEs", "reconstruction-based training", "low-level information", "pre-training scaling problem", "high-level semantics", "VTP", "image-text contrastive", "self-supervised", "reconstruction losses", "generative performance", "ImageNet", "zero-shot accuracy", "rFID", "FLOPS", "DiT", "advanced distillation methods"], "githubStars": 92, "organization": {"_id": "6778fc29920093dbc0c24917", "name": "MiniMaxAI", "fullname": "MiniMax", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/676e38ad04af5bec20bc9faf/dUd-LsZEX0H_d4qefO_g6.jpeg"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u6807\u8bb0\u5668\uff08\u5982VAE\uff09\u7684\u6f5c\u5728\u7a7a\u95f4\u8d28\u91cf\u5bf9\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>\u6807\u51c6\u7684\u91cd\u5efa\u8bad\u7ec3\u65b9\u6cd5\u5bfc\u81f4\u6f5c\u5728\u7a7a\u95f4\u504f\u5411\u4f4e\u7ea7\u4fe1\u606f\uff0c\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86VTP\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u56fe\u50cf-\u6587\u672c\u5bf9\u6bd4\u3001\u81ea\u76d1\u7763\u548c\u91cd\u5efa\u635f\u5931\u6765\u6539\u5584\u6f5c\u5728\u7a7a\u95f4\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u7406\u89e3\u80fd\u529b\u662f\u751f\u6210\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u5e76\u4e14\u751f\u6210\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u3001\u53c2\u6570\u548c\u6570\u636e\u7684\u89c4\u6a21\u6709\u6548\u5173\u8054\u3002</li>\n    <li>\u7ecf\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0cVTP\u6a21\u578b\u5728\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\u6536\u655b\u901f\u5ea6\u66f4\u5feb\u3002</li>\n</ul>", "summary_simple": "Summary unavailable"}, "publishedAt": "2025-12-15T13:59:54.000Z", "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation", "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13687.png", "numComments": 1, "submittedBy": {"_id": "67756c9c846a267749304255", "avatarUrl": "/avatars/01f09805b561887c55d1b9ad4e96b461.svg", "fullname": "Jingfeng Yao", "name": "MapleF9", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "6778fc29920093dbc0c24917", "name": "MiniMaxAI", "fullname": "MiniMax", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/676e38ad04af5bec20bc9faf/dUd-LsZEX0H_d4qefO_g6.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.13564", "authors": [{"_id": "6940d68565f1e24a1178056c", "user": {"_id": "6544b9b646dbdeca34ee5f52", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png", "isPro": false, "fullname": "Yuyang Hu", "user": "namespace-ERI", "type": "user"}, "name": "Yuyang Hu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:40.040Z", "hidden": false}, {"_id": "6940d68565f1e24a1178056d", "user": {"_id": "65435cad429b80b14922ab8d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/N8oWq4ZZn3dRxmXi18FrA.jpeg", "isPro": false, "fullname": "Shichun Liu", "user": "Liusc2020", "type": "user"}, "name": "Shichun Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:40.844Z", "hidden": false}, {"_id": "6940d68565f1e24a1178056e", "name": "Yanwei Yue", "hidden": false}, {"_id": "6940d68565f1e24a1178056f", "name": "Guibin Zhang", "hidden": false}, {"_id": "6940d68565f1e24a11780570", "name": "Boyang Liu", "hidden": false}, {"_id": "6940d68565f1e24a11780571", "name": "Fangyi Zhu", "hidden": false}, {"_id": "6940d68565f1e24a11780572", "name": "Jiahang Lin", "hidden": false}, {"_id": "6940d68565f1e24a11780573", "user": {"_id": "638ef0b0c67af472d31674a6", "avatarUrl": "/avatars/02df97d15a0f46b47f9162221733b121.svg", "isPro": false, "fullname": "Honglin Guo", "user": "KYLN24", "type": "user"}, "name": "Honglin Guo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:38.698Z", "hidden": false}, {"_id": "6940d68565f1e24a11780574", "name": "Shihan Dou", "hidden": false}, {"_id": "6940d68565f1e24a11780575", "name": "Zhiheng Xi", "hidden": false}, {"_id": "6940d68565f1e24a11780576", "name": "Senjie Jin", "hidden": false}, {"_id": "6940d68565f1e24a11780577", "user": {"_id": "62e52483a944e2a56cd2c6ca", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg", "isPro": false, "fullname": "Jiejun Tan", "user": "zstanjj", "type": "user"}, "name": "Jiejun Tan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:42.726Z", "hidden": false}, {"_id": "6940d68565f1e24a11780578", "name": "Yanbin Yin", "hidden": false}, {"_id": "6940d68565f1e24a11780579", "name": "Jiongnan Liu", "hidden": false}, {"_id": "6940d68565f1e24a1178057a", "name": "Zeyu Zhang", "hidden": false}, {"_id": "6940d68565f1e24a1178057b", "user": {"_id": "6309bfdab8d7b3889319b588", "avatarUrl": "/avatars/572acdad470f765ef2e058ead3741e24.svg", "isPro": false, "fullname": "SunZX", "user": "Jeryi", "type": "user"}, "name": "Zhongxiang Sun", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T22:32:57.018Z", "hidden": false}, {"_id": "6940d68565f1e24a1178057c", "name": "Yutao Zhu", "hidden": false}, {"_id": "6940d68565f1e24a1178057d", "name": "Hao Sun", "hidden": false}, {"_id": "6940d68565f1e24a1178057e", "name": "Boci Peng", "hidden": false}, {"_id": "6940d68565f1e24a1178057f", "name": "Zhenrong Cheng", "hidden": false}, {"_id": "6940d68565f1e24a11780580", "name": "Xuanbo Fan", "hidden": false}, {"_id": "6940d68565f1e24a11780581", "name": "Jiaxin Guo", "hidden": false}, {"_id": "6940d68565f1e24a11780582", "name": "Xinlei Yu", "hidden": false}, {"_id": "6940d68565f1e24a11780583", "name": "Zhenhong Zhou", "hidden": false}, {"_id": "6940d68565f1e24a11780584", "name": "Zewen Hu", "hidden": false}, {"_id": "6940d68565f1e24a11780585", "name": "Jiahao Huo", "hidden": false}, {"_id": "6940d68565f1e24a11780586", "name": "Junhao Wang", "hidden": false}, {"_id": "6940d68565f1e24a11780587", "name": "Yuwei Niu", "hidden": false}, {"_id": "6940d68565f1e24a11780588", "name": "Yu Wang", "hidden": false}, {"_id": "6940d68565f1e24a11780589", "name": "Zhenfei Yin", "hidden": false}, {"_id": "6940d68565f1e24a1178058a", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6940d68565f1e24a1178058b", "name": "Yue Liao", "hidden": false}, {"_id": "6940d68565f1e24a1178058c", "name": "Qiankun Li", "hidden": false}, {"_id": "6940d68565f1e24a1178058d", "name": "Kun Wang", "hidden": false}, {"_id": "6940d68565f1e24a1178058e", "name": "Wangchunshu Zhou", "hidden": false}, {"_id": "6940d68565f1e24a1178058f", "name": "Yixin Liu", "hidden": false}, {"_id": "6940d68565f1e24a11780590", "name": "Dawei Cheng", "hidden": false}, {"_id": "6940d68565f1e24a11780591", "name": "Qi Zhang", "hidden": false}, {"_id": "6940d68565f1e24a11780592", "name": "Tao Gui", "hidden": false}, {"_id": "6940d68565f1e24a11780593", "name": "Shirui Pan", "hidden": false}, {"_id": "6940d68565f1e24a11780594", "name": "Yan Zhang", "hidden": false}, {"_id": "6940d68565f1e24a11780595", "name": "Philip Torr", "hidden": false}, {"_id": "6940d68565f1e24a11780596", "name": "Zhicheng Dou", "hidden": false}, {"_id": "6940d68565f1e24a11780597", "name": "Ji-Rong Wen", "hidden": false}, {"_id": "6940d68565f1e24a11780598", "name": "Xuanjing Huang", "hidden": false}, {"_id": "6940d68565f1e24a11780599", "name": "Yu-Gang Jiang", "hidden": false}, {"_id": "6940d68565f1e24a1178059a", "name": "Shuicheng Yan", "hidden": false}], "publishedAt": "2025-12-15T17:22:34.000Z", "submittedOnDailyAt": "2025-12-16T01:18:34.363Z", "title": "Memory in the Age of AI Agents", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.", "upvotes": 62, "discussionId": "6940d68565f1e24a1178059b", "githubRepo": "https://github.com/Shichun-Liu/Agent-Memory-Paper-List", "githubRepoAddedBy": "user", "ai_summary": "This survey provides an updated overview of agent memory research, distinguishing its forms, functions, and dynamics, and highlights emerging research directions.", "ai_keywords": ["agent memory", "LLM memory", "retrieval augmented generation (RAG)", "context engineering", "token-level memory", "parametric memory", "latent memory", "factual memory", "experiential memory", "working memory", "memory benchmarks", "open-source frameworks", "memory automation", "reinforcement learning integration", "multimodal memory", "multi-agent memory", "trustworthiness issues"], "githubStars": 115, "summary_zh": "<ul>\n    <li>\u8bb0\u5fc6\u662f\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u667a\u80fd\u4f53\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u5e76\u5c06\u6301\u7eed\u53d7\u5230\u5173\u6ce8\u3002</li>\n    <li>\u76ee\u524d\u7684\u667a\u80fd\u4f53\u8bb0\u5fc6\u7814\u7a76\u9886\u57df\u5b58\u5728\u660e\u663e\u7684\u788e\u7247\u5316\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u5b9a\u4e49\u548c\u5206\u7c7b\u3002</li>\n    <li>\u672c\u6587\u660e\u786e\u533a\u5206\u667a\u80fd\u4f53\u8bb0\u5fc6\u4e0e\u76f8\u5173\u6982\u5ff5\uff0c\u5e76\u4ece\u5f62\u5f0f\u3001\u529f\u80fd\u548c\u52a8\u6001\u4e09\u4e2a\u89d2\u5ea6\u8fdb\u884c\u5206\u6790\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u66f4\u7ec6\u81f4\u7684\u5206\u7c7b\uff0c\u5305\u62ec\u4e09\u79cd\u4e3b\u8981\u7684\u8bb0\u5fc6\u5f62\u5f0f\u548c\u4e09\u79cd\u529f\u80fd\u7c7b\u578b\u3002</li>\n    <li>\u603b\u7ed3\u4e86\u8bb0\u5fc6\u57fa\u51c6\u548c\u5f00\u6e90\u6846\u67b6\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u8bb0\u5fc6\u81ea\u52a8\u5316\u548c\u591a\u6a21\u6001\u8bb0\u5fc6\u7b49\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Memory is a key feature in agents that use foundation models, and research on this topic is quickly growing.</li>\n    <li>Current research on agent memory is diverse and often unclear, with various definitions and methods used.</li>\n    <li>This work aims to clarify agent memory by distinguishing it from similar concepts and outlining its different forms, functions, and dynamics.</li>\n    <li>Three main types of agent memory identified are token-level, parametric, and latent memory, along with categories like factual and experiential memory.</li>\n    <li>The survey also discusses future research directions, including memory automation and multi-agent memory, and provides resources for practical development.</li>\n</ul>"}, "publishedAt": "2025-12-15T12:22:34.000Z", "title": "Memory in the Age of AI Agents", "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13564.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 185}, "isAuthorParticipating": false}, {"paper": {"id": "2512.12967", "authors": [{"_id": "6940d25d65f1e24a11780417", "user": {"_id": "64777a346e6c7ac608c1e9bf", "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg", "isPro": false, "fullname": "Weizhou Shen", "user": "shenwzh3", "type": "user"}, "name": "Weizhou Shen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:42.079Z", "hidden": false}, {"_id": "6940d25d65f1e24a11780418", "user": {"_id": "64c9b0f28d2d187c24d1e6c1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1CPnAaB3gsupdpiNWaoDc.png", "isPro": false, "fullname": "ZiYi Yang", "user": "AALF", "type": "user"}, "name": "Ziyi Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:56.062Z", "hidden": false}, {"_id": "6940d25d65f1e24a11780419", "name": "Chenliang Li", "hidden": false}, {"_id": "6940d25d65f1e24a1178041a", "name": "Zhiyuan Lu", "hidden": false}, {"_id": "6940d25d65f1e24a1178041b", "name": "Miao Peng", "hidden": false}, {"_id": "6940d25d65f1e24a1178041c", "name": "Huashan Sun", "hidden": false}, {"_id": "6940d25d65f1e24a1178041d", "name": "Yingcheng Shi", "hidden": false}, {"_id": "6940d25d65f1e24a1178041e", "name": "Shengyi Liao", "hidden": false}, {"_id": "6940d25d65f1e24a1178041f", "name": "Shaopeng Lai", "hidden": false}, {"_id": "6940d25d65f1e24a11780420", "name": "Bo Zhang", "hidden": false}, {"_id": "6940d25d65f1e24a11780421", "name": "Dayiheng Liu", "hidden": false}, {"_id": "6940d25d65f1e24a11780422", "name": "Fei Huang", "hidden": false}, {"_id": "6940d25d65f1e24a11780423", "name": "Jingren Zhou", "hidden": false}, {"_id": "6940d25d65f1e24a11780424", "name": "Ming Yan", "hidden": false}], "publishedAt": "2025-12-15T04:11:11.000Z", "submittedOnDailyAt": "2025-12-16T01:29:41.007Z", "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.", "upvotes": 54, "discussionId": "6940d25d65f1e24a11780425", "githubRepo": "https://github.com/Tongyi-Zhiwen/Qwen-Doc", "githubRepoAddedBy": "user", "ai_summary": "QwenLong-L1.5 enhances long-context reasoning through data synthesis, stabilized reinforcement learning, and memory-augmented architecture, achieving superior performance on benchmarks and general domains.", "ai_keywords": ["Long-Context Data Synthesis Pipeline", "atomic facts", "verifiable reasoning questions", "Stabilized Reinforcement Learning", "task-balanced sampling", "task-specific advantage estimation", "Adaptive Entropy-Controlled Policy Optimization", "Memory-Augmented Architecture", "multi-stage fusion RL training", "single-pass reasoning", "iterative memory-based processing", "memory-agent framework"], "githubStars": 311, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51faQwenLong-L1.5\u6a21\u578b\uff0c\u5177\u5907\u5353\u8d8a\u7684\u957f\u6587\u672c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5efa\u7acb\u4e86\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u751f\u6210\u9700\u8981\u591a\u6b65\u63a8\u7406\u7684\u96be\u9898\u3002</li>\n    <li>\u4f7f\u7528\u7a33\u5b9a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\uff0c\u6539\u5584\u4e86\u5956\u52b1\u504f\u5dee\u95ee\u9898\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u589e\u5f3a\u8bb0\u5fc6\u67b6\u6784\uff0c\u652f\u6301\u5904\u7406\u8d85\u8fc7400\u4e07\u6807\u8bb0\u7684\u8d85\u957f\u4efb\u52a1\u3002</li>\n    <li>\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cQwenLong-L1.5\u7684\u8868\u73b0\u8d85\u8fc7GPT-5\u548cGemini-2.5-Pro\uff0c\u5e73\u5747\u63d0\u9ad89.90\u5206\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>QwenLong-L1.5 is a new model that improves long-context reasoning using advanced training methods.</li>\n    <li>It features a data synthesis pipeline that creates complex reasoning tasks by breaking down documents into facts and relationships.</li>\n    <li>The model uses a special training method to stabilize long-context reinforcement learning and improve decision-making.</li>\n    <li>It includes a memory management system that helps handle very long text sequences of over 4 million tokens.</li>\n    <li>QwenLong-L1.5 performs as well as or better than other leading models in long-context reasoning and improves performance in areas like scientific reasoning and extended dialogue.</li>\n</ul>"}, "publishedAt": "2025-12-14T23:11:11.000Z", "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management", "summary": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12967.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 185}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.13604", "authors": [{"_id": "6940d44165f1e24a11780535", "name": "Jianxiong Gao", "hidden": false}, {"_id": "6940d44165f1e24a11780536", "name": "Zhaoxi Chen", "hidden": false}, {"_id": "6940d44165f1e24a11780537", "name": "Xian Liu", "hidden": false}, {"_id": "6940d44165f1e24a11780538", "user": {"_id": "64970d3d9c3b29dca8633f87", "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg", "isPro": false, "fullname": "JunhaoZhuang", "user": "JunhaoZhuang", "type": "user"}, "name": "Junhao Zhuang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:51.620Z", "hidden": false}, {"_id": "6940d44165f1e24a11780539", "name": "Chengming Xu", "hidden": false}, {"_id": "6940d44165f1e24a1178053a", "name": "Jianfeng Feng", "hidden": false}, {"_id": "6940d44165f1e24a1178053b", "name": "Yu Qiao", "hidden": false}, {"_id": "6940d44165f1e24a1178053c", "name": "Yanwei Fu", "hidden": false}, {"_id": "6940d44165f1e24a1178053d", "user": {"_id": "635f8ed47c05eb9f59963d3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg", "isPro": false, "fullname": "ChenyangSi", "user": "ChenyangSi", "type": "user"}, "name": "Chenyang Si", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:49.025Z", "hidden": false}, {"_id": "6940d44165f1e24a1178053e", "name": "Ziwei Liu", "hidden": false}], "publishedAt": "2025-12-15T17:59:58.000Z", "submittedOnDailyAt": "2025-12-16T01:12:24.486Z", "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model", "submittedOnDailyBy": {"_id": "643815c4961bb61e463c5896", "avatarUrl": "/avatars/3b44592472f16c56105bff8c314d9939.svg", "isPro": false, "fullname": "Jianxiong Gao", "user": "Jianxiong", "type": "user"}, "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.", "upvotes": 52, "discussionId": "6940d44165f1e24a1178053f", "projectPage": "https://vchitect.github.io/LongVie2-project/", "ai_summary": "LongVie 2, an end-to-end autoregressive framework, enhances controllability, visual quality, and temporal consistency in video world models through three progressive training stages.", "ai_keywords": ["autoregressive framework", "multi-modal guidance", "degradation-aware training", "history-context guidance", "video world models", "controllability", "visual quality", "temporal consistency", "LongVGenBench", "state-of-the-art performance", "temporal coherence", "visual fidelity"], "summary_zh": "<ul>\n    <li>\u6784\u5efa\u89c6\u9891\u4e16\u754c\u6a21\u578b\u662f\u5b9e\u73b0\u7a7a\u95f4\u65f6\u95f4\u667a\u80fd\u7684\u91cd\u8981\u6b65\u9aa4\u3002</li>\n    <li>\u4e16\u754c\u6a21\u578b\u9700\u5177\u5907\u53ef\u63a7\u6027\u3001\u957f\u671f\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e09\u4e2a\u57fa\u672c\u7279\u6027\u3002</li>\n    <li>LongVie 2 \u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u8fdb\u884c\u8bad\u7ec3\u3002</li>\n    <li>\u5f15\u5165\u4e86 LongVGenBench\uff0c\u4e00\u4e2a\u5305\u542b100\u4e2a\u9ad8\u5206\u8fa8\u7387\u4e00\u5206\u949f\u89c6\u9891\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e LongVie 2 \u5728\u53ef\u63a7\u6027\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongVie 2 is a new system designed to create better video world models for understanding and generating videos.</li>\n    <li>It focuses on three key features: control over the video content, high visual quality over time, and consistency in how things change in the video.</li>\n    <li>The system is developed in three stages to improve these features, starting with better control, then maintaining quality, and finally ensuring consistency across video clips.</li>\n    <li>LongVGenBench is a new benchmark with 100 high-quality videos from various environments to test and compare video generation systems.</li>\n    <li>LongVie 2 has shown to perform better than previous systems in controlling video, maintaining visual quality, and keeping things consistent over time.</li>\n</ul>"}, "publishedAt": "2025-12-15T12:59:58.000Z", "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model", "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13604.png", "numComments": 1, "submittedBy": {"_id": "643815c4961bb61e463c5896", "avatarUrl": "/avatars/3b44592472f16c56105bff8c314d9939.svg", "fullname": "Jianxiong Gao", "name": "Jianxiong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13168", "authors": [{"_id": "6940eedd65f1e24a11780673", "user": {"_id": "637b08057ce76c3b834da15d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PFmI1HNcZlHF0RoLjP3c2.png", "isPro": false, "fullname": "Haoyu Dong", "user": "HaoyuDong", "type": "user"}, "name": "Haoyu Dong", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:37:23.964Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780674", "user": {"_id": "684ae4a3a6d604475ef72f22", "avatarUrl": "/avatars/70e61614ab115f07361c0baec351255a.svg", "isPro": false, "fullname": "Pengkun Zhang", "user": "logicluo", "type": "user"}, "name": "Pengkun Zhang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T15:09:52.673Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780675", "user": {"_id": "69369fc780cb6886b08dcf5b", "avatarUrl": "/avatars/1159e73666a55706e551afd4518adf0b.svg", "isPro": false, "fullname": "Yan Gao", "user": "gaoyansdyt", "type": "user"}, "name": "Yan Gao", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:47:22.721Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780676", "user": {"_id": "69377b51e5af592ad600e1c5", "avatarUrl": "/avatars/72612a4a69da2bd565ed4777287cff86.svg", "isPro": false, "fullname": "Xuanyu Dong", "user": "KcNcooo", "type": "user"}, "name": "Xuanyu Dong", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T14:13:14.674Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780677", "user": {"_id": "6777bb751ee6b20009098ab6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6777bb751ee6b20009098ab6/OumKsg54DNDH8qkslg95x.jpeg", "isPro": false, "fullname": "Yilin Cheng", "user": "Yilin-Cheng24", "type": "user"}, "name": "Yilin Cheng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T10:18:24.145Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780678", "user": {"_id": "694135abb4ad5b6ca6c4c9fe", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/694135abb4ad5b6ca6c4c9fe/ZFl3u8k69DZpyErVLYRBk.jpeg", "isPro": false, "fullname": "Mingzhe Lu", "user": "metaphor2ml", "type": "user"}, "name": "Mingzhe Lu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:36.238Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780679", "user": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "isPro": false, "fullname": "Adina Yakefu", "user": "AdinaY", "type": "user"}, "name": "Adina Yakefu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T09:47:16.061Z", "hidden": false}, {"_id": "6940eedd65f1e24a1178067a", "name": "Shuxin Zheng", "hidden": false}], "publishedAt": "2025-12-15T10:28:45.000Z", "submittedOnDailyAt": "2025-12-16T09:53:57.731Z", "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows", "submittedOnDailyBy": {"_id": "637b08057ce76c3b834da15d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PFmI1HNcZlHF0RoLjP3c2.png", "isPro": false, "fullname": "Haoyu Dong", "user": "HaoyuDong", "type": "user"}, "summary": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.", "upvotes": 46, "discussionId": "6940eedd65f1e24a1178067b", "projectPage": "https://huggingface.co/datasets/FinWorkBench/Finch", "ai_summary": "Finch, a benchmark for AI agents in enterprise finance and accounting, evaluates performance across complex, real-world workflows using authentic data from Enron and other institutions.", "ai_keywords": ["LLM-assisted discovery", "expert annotation", "composite workflows", "spreadsheets", "PDFs", "AI systems", "GPT 5.1", "Claude Sonnet 4.5", "Gemini 3 Pro", "Grok 4", "Qwen 3 Max"], "organization": {"_id": "690e8a61e75888a0f707c0ee", "name": "FinWorkBench", "fullname": "Finch: Finance and Accounting Workflow Benchmark", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/637b08057ce76c3b834da15d/0HrDjcjNQK-5-FCZfkLzp.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u91d1\u878d\u4e0e\u4f1a\u8ba1\u57fa\u51c6\uff08Finch\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u590d\u6742\u7684\u4f01\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u8868\u73b0\u3002</li>\n    <li>Finch\u6765\u6e90\u4e8e\u771f\u5b9e\u4f01\u4e1a\u5de5\u4f5c\u7a7a\u95f4\u7684\u6570\u636e\uff0c\u5305\u62ecEnron\u7684150\u540d\u5458\u5de5\u768415000\u4e2a\u7535\u5b50\u8868\u683c\u548c50\u4e07\u5c01\u7535\u5b50\u90ae\u4ef6\uff0c\u6db5\u76d6\u4e86\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\u3002</li>\n    <li>\u6211\u4eec\u4f7f\u7528LLM\u8f85\u52a9\u53d1\u73b0\u548c\u4e13\u5bb6\u6ce8\u91ca\u7684\u65b9\u5f0f\u6784\u5efa\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7ecf\u8fc7700\u5c0f\u65f6\u7684\u4e13\u5bb6\u52aa\u529b\uff0c\u751f\u6210\u4e86172\u4e2a\u590d\u5408\u5de5\u4f5c\u6d41\u7a0b\u548c384\u4e2a\u4efb\u52a1\u3002</li>\n    <li>\u6211\u4eec\u5bf9\u591a\u4e2a\u524d\u6cbfAI\u7cfb\u7edf\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u53d1\u73b0GPT 5.1 Pro\u4ec5\u901a\u8fc738.4%\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u800cClaude Sonnet 4.5\u7684\u901a\u8fc7\u7387\u4ec5\u4e3a25.0%\u3002</li>\n    <li>\u6848\u4f8b\u7814\u7a76\u63ed\u793a\u4e86\u771f\u5b9e\u4f01\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u5bf9AI\u4ee3\u7406\u7684\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Finch is a new benchmark for testing AI agents on real business tasks like data entry, calculations, and reporting.</li>\n    <li>It uses real data from Enron and other companies, with a focus on messy, real-world information from emails and spreadsheets.</li>\n    <li>Experts spent over 700 hours creating and verifying 172 workflows made up of 384 tasks, incorporating data from 1,710 spreadsheets.</li>\n    <li>AI systems like GPT 5.1 and Claude Sonnet 4.5 were tested on these workflows, with GPT 5.1 passing only 38.4% of them.</li>\n    <li>The study highlights the difficulties AI faces in handling complex, real-world business processes.</li>\n</ul>"}, "publishedAt": "2025-12-15T05:28:45.000Z", "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows", "summary": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13168.png", "numComments": 1, "submittedBy": {"_id": "637b08057ce76c3b834da15d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PFmI1HNcZlHF0RoLjP3c2.png", "fullname": "Haoyu Dong", "name": "HaoyuDong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "organization": {"_id": "690e8a61e75888a0f707c0ee", "name": "FinWorkBench", "fullname": "Finch: Finance and Accounting Workflow Benchmark", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/637b08057ce76c3b834da15d/0HrDjcjNQK-5-FCZfkLzp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.12730", "authors": [{"_id": "6940cff365f1e24a117803e5", "user": {"_id": "6737eb499ab5a7623aaf038a", "avatarUrl": "/avatars/dfe21c7db767a34f2aff3e80cebcfa22.svg", "isPro": false, "fullname": "Jingzhe Ding", "user": "JingzheDing", "type": "user"}, "name": "Jingzhe Ding", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T10:18:25.834Z", "hidden": false}, {"_id": "6940cff365f1e24a117803e6", "name": "Shengda Long", "hidden": false}, {"_id": "6940cff365f1e24a117803e7", "name": "Changxin Pu", "hidden": false}, {"_id": "6940cff365f1e24a117803e8", "name": "Huan Zhou", "hidden": false}, {"_id": "6940cff365f1e24a117803e9", "name": "Hongwan Gao", "hidden": false}, {"_id": "6940cff365f1e24a117803ea", "user": {"_id": "671068b8243baf4c58fc29c2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/leMkq-kw1icPo2bCLivGv.png", "isPro": false, "fullname": "Xiang Gao", "user": "coffiney", "type": "user"}, "name": "Xiang Gao", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:47:27.017Z", "hidden": false}, {"_id": "6940cff365f1e24a117803eb", "name": "Chao He", "hidden": false}, {"_id": "6940cff365f1e24a117803ec", "user": {"_id": "694100a19d8967e631cf94cf", "avatarUrl": "/avatars/1c785c0fcd171a62e206ef4074061a81.svg", "isPro": false, "fullname": "Yue Hou", "user": "YueHou", "type": "user"}, "name": "Yue Hou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:47:24.853Z", "hidden": false}, {"_id": "6940cff365f1e24a117803ed", "name": "Fei Hu", "hidden": false}, {"_id": "6940cff365f1e24a117803ee", "name": "Zhaojian Li", "hidden": false}, {"_id": "6940cff365f1e24a117803ef", "name": "Weiran Shi", "hidden": false}, {"_id": "6940cff365f1e24a117803f0", "name": "Zaiyuan Wang", "hidden": false}, {"_id": "6940cff365f1e24a117803f1", "name": "Daoguang Zan", "hidden": false}, {"_id": "6940cff365f1e24a117803f2", "name": "Chenchen Zhang", "hidden": false}, {"_id": "6940cff365f1e24a117803f3", "name": "Xiaoxu Zhang", "hidden": false}, {"_id": "6940cff365f1e24a117803f4", "name": "Qizhi Chen", "hidden": false}, {"_id": "6940cff365f1e24a117803f5", "name": "Xianfu Cheng", "hidden": false}, {"_id": "6940cff365f1e24a117803f6", "name": "Bo Deng", "hidden": false}, {"_id": "6940cff365f1e24a117803f7", "name": "Qingshui Gu", "hidden": false}, {"_id": "6940cff365f1e24a117803f8", "user": {"_id": "64e851825ddcace745ba15bd", "avatarUrl": "/avatars/7b6612c411222974d9ea181784eef915.svg", "isPro": false, "fullname": "Kai Hua", "user": "kkish", "type": "user"}, "name": "Kai Hua", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:47:28.890Z", "hidden": false}, {"_id": "6940cff365f1e24a117803f9", "name": "Juntao Lin", "hidden": false}, {"_id": "6940cff365f1e24a117803fa", "name": "Pai Liu", "hidden": false}, {"_id": "6940cff365f1e24a117803fb", "name": "Mingchen Li", "hidden": false}, {"_id": "6940cff365f1e24a117803fc", "name": "Xuanguang Pan", "hidden": false}, {"_id": "6940cff365f1e24a117803fd", "name": "Zifan Peng", "hidden": false}, {"_id": "6940cff365f1e24a117803fe", "name": "Yujia Qin", "hidden": false}, {"_id": "6940cff365f1e24a117803ff", "user": {"_id": "64b8cc7ebe76d2ff0703bfb3", "avatarUrl": "/avatars/f1c7ff17fd923f1460d362333d9fbfe3.svg", "isPro": false, "fullname": "yong", "user": "yo37", "type": "user"}, "name": "Yong Shan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:43.971Z", "hidden": false}, {"_id": "6940cff365f1e24a11780400", "name": "Zhewen Tan", "hidden": false}, {"_id": "6940cff365f1e24a11780401", "name": "Weihao Xie", "hidden": false}, {"_id": "6940cff365f1e24a11780402", "name": "Zihan Wang", "hidden": false}, {"_id": "6940cff365f1e24a11780403", "name": "Yishuo Yuan", "hidden": false}, {"_id": "6940cff365f1e24a11780404", "name": "Jiayu Zhang", "hidden": false}, {"_id": "6940cff365f1e24a11780405", "name": "Enduo Zhao", "hidden": false}, {"_id": "6940cff365f1e24a11780406", "name": "Yunfei Zhao", "hidden": false}, {"_id": "6940cff365f1e24a11780407", "name": "He Zhu", "hidden": false}, {"_id": "6940cff365f1e24a11780408", "name": "Chenyang Zou", "hidden": false}, {"_id": "6940cff365f1e24a11780409", "name": "Ming Ding", "hidden": false}, {"_id": "6940cff365f1e24a1178040a", "name": "Jianpeng Jiao", "hidden": false}, {"_id": "6940cff365f1e24a1178040b", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6940cff365f1e24a1178040c", "name": "Minghao Liu", "hidden": false}, {"_id": "6940cff365f1e24a1178040d", "name": "Qian Liu", "hidden": false}, {"_id": "6940cff365f1e24a1178040e", "user": {"_id": "648db8f92eb481bc8bc60469", "avatarUrl": "/avatars/7a640980ed225e31117f750e09bcc18c.svg", "isPro": false, "fullname": "Chongyang Tao", "user": "chongyang09", "type": "user"}, "name": "Chongyao Tao", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:47:30.496Z", "hidden": false}, {"_id": "6940cff365f1e24a1178040f", "name": "Jian Yang", "hidden": false}, {"_id": "6940cff365f1e24a11780410", "name": "Tong Yang", "hidden": false}, {"_id": "6940cff365f1e24a11780411", "name": "Zhaoxiang Zhang", "hidden": false}, {"_id": "6940cff365f1e24a11780412", "name": "Xinjie Chen", "hidden": false}, {"_id": "6940cff365f1e24a11780413", "name": "Wenhao Huang", "hidden": false}, {"_id": "6940cff365f1e24a11780414", "name": "Ge Zhang", "hidden": false}], "publishedAt": "2025-12-14T15:12:13.000Z", "submittedOnDailyAt": "2025-12-16T00:51:09.038Z", "title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents", "submittedOnDailyBy": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "summary": "Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.", "upvotes": 38, "discussionId": "6940cff365f1e24a11780415", "githubRepo": "https://github.com/multimodal-art-projection/NL2RepoBench", "githubRepoAddedBy": "user", "ai_summary": "NL2Repo Bench evaluates long-horizon software development capabilities of coding agents by assessing their ability to generate complete Python libraries from natural-language requirements.", "ai_keywords": ["NL2Repo Bench", "long-horizon repository generation", "autonomous software development", "coding agents", "natural-language requirements", "architecture design", "dependency management", "multi-module logic", "installable Python library", "test pass rates", "global coherence", "cross-file dependencies", "long-horizon reasoning"], "githubStars": 25, "summary_zh": "<ul>\n    <li>\u8fd1\u671f\u7684\u7f16\u7801\u4ee3\u7406\u6280\u672f\u8fdb\u5c55\u5feb\u901f\uff0c\u4f46\u73b0\u6709\u7684\u8bc4\u4f30\u6807\u51c6\u65e0\u6cd5\u6709\u6548\u6d4b\u8bd5\u5f00\u53d1\u5b8c\u6574\u8f6f\u4ef6\u7cfb\u7edf\u6240\u9700\u7684\u957f\u65f6\u95f4\u80fd\u529b\u3002</li>\n    <li>\u5927\u591a\u6570\u8bc4\u4f30\u96c6\u4e2d\u5728\u5c40\u90e8\u4ee3\u7801\u751f\u6210\u548c\u77ed\u671f\u4fee\u590d\u4efb\u52a1\u4e0a\uff0c\u672a\u80fd\u9a8c\u8bc1\u4ee3\u7406\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u6784\u5efa\u8f6f\u4ef6\u5e93\u7684\u6301\u7eed\u63a8\u7406\u548c\u6267\u884c\u80fd\u529b\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86NL2Repo Bench\u57fa\u51c6\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u7f16\u7801\u4ee3\u7406\u7684\u957f\u65f6\u95f4\u5e93\u751f\u6210\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u663e\u793a\uff0c\u5c3d\u7ba1\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u957f\u65f6\u95f4\u5e93\u751f\u6210\u4ecd\u7136\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u4ee3\u7406\u7684\u5e73\u5747\u6d4b\u8bd5\u901a\u8fc7\u7387\u4f4e\u4e8e40%\u3002</li>\n    <li>\u5206\u6790\u63ed\u793a\u4e86\u957f\u65f6\u95f4\u5931\u8d25\u7684\u6839\u672c\u539f\u56e0\uff0c\u5305\u62ec\u8fc7\u65e9\u7ec8\u6b62\u3001\u5168\u5c40\u4e00\u81f4\u6027\u4e27\u5931\u548c\u4e0d\u5145\u5206\u7684\u89c4\u5212\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Advancements in coding agents show potential for creating software automatically, but current tests don't adequately measure their ability to handle long-term tasks.</li>\n    <li>Most existing tests focus on short coding tasks, not on building complete software systems, leaving a gap in evaluation.</li>\n    <li>The new benchmark, NL2Repo Bench, is designed to assess coding agents' ability to generate full software repositories from a single requirements document.</li>\n    <li>Tests reveal that even the best coding agents struggle with long-term tasks, achieving less than 40% success in completing software correctly.</li>\n    <li>Challenges identified include early task completion, lack of coherence, fragile dependencies, and poor long-term planning.</li>\n</ul>"}, "publishedAt": "2025-12-14T10:12:13.000Z", "title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents", "summary": "Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12730.png", "numComments": 1, "submittedBy": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "fullname": "Ge Zhang", "name": "zhangysk", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 74}, "isAuthorParticipating": false}, {"paper": {"id": "2512.12602", "authors": [{"_id": "6940cc8365f1e24a1177ff48", "user": {"_id": "65756c5e1488186315c6696d", "avatarUrl": "/avatars/2ec67e0d1921b6635c69669b591a3110.svg", "isPro": true, "fullname": "Jingdi Lei", "user": "huaXiaKyrie", "type": "user"}, "name": "Jingdi Lei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T10:18:27.530Z", "hidden": false}, {"_id": "6940cc8365f1e24a1177ff49", "user": {"_id": "64bce15bafd1e46c5504ad38", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png", "isPro": false, "fullname": "Di Zhang", "user": "di-zhang-fdu", "type": "user"}, "name": "Di Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:47:40.902Z", "hidden": false}, {"_id": "6940cc8365f1e24a1177ff4a", "name": "Soujanya Poria", "hidden": false}], "publishedAt": "2025-12-14T08:51:02.000Z", "submittedOnDailyAt": "2025-12-16T00:36:26.596Z", "title": "Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics", "submittedOnDailyBy": {"_id": "626b626405fe1cb65725aca1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/ZVSbhynzpQhVGq9kGywW6.png", "isPro": false, "fullname": "Soujanya Poria", "user": "soujanyaporia", "type": "user"}, "summary": "Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models.", "upvotes": 33, "discussionId": "6940cc8365f1e24a1177ff4b", "githubRepo": "https://github.com/declare-lab/EFLA", "githubRepoAddedBy": "user", "ai_summary": "Error-Free Linear Attention (EFLA) is a stable, parallelizable, and theoretically sound linear-time attention mechanism that outperforms DeltaNet in language modeling and downstream tasks.", "ai_keywords": ["linear-time attention", "state space models", "softmax attention", "error-free linear attention", "delta rule", "continuous-time dynamical system", "rank-1 structure", "infinite-order Runge-Kutta method", "language modeling perplexity", "DeltaNet"], "githubStars": 27, "organization": {"_id": "626ab9dac804c432c1b27a48", "name": "declare-lab", "fullname": "Deep Cognition and Language Research (DeCLaRe) Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/grq3rj2uj0WRjjPjAtR1I.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\uff0c\u79f0\u4e3a\u65e0\u8bef\u5dee\u7ebf\u6027\u6ce8\u610f\u529b\uff08EFLA\uff09\uff0c\u65e8\u5728\u89e3\u51b3\u957f\u6587\u672c\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\u3002</li>\n    <li>EFLA \u901a\u8fc7\u4e00\u79cd\u8fde\u7eed\u65f6\u95f4\u52a8\u6001\u7cfb\u7edf\u6765\u5b9e\u73b0\u5728\u7ebf\u5b66\u4e60\u66f4\u65b0\uff0c\u786e\u4fdd\u5728\u5168\u5e76\u884c\u7684\u60c5\u51b5\u4e0b\u4ee5\u7ebf\u6027\u65f6\u95f4\u8ba1\u7b97\u3002</li>\n    <li>\u5229\u7528\u52a8\u6001\u77e9\u9635\u7684\u79e9-1\u7ed3\u6784\uff0cEFLA \u5bfc\u51fa\u4e86\u7cbe\u786e\u7684\u95ed\u5f0f\u89e3\uff0c\u907f\u514d\u4e86\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002</li>\n    <li>\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86 EFLA \u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u4e14\u76f8\u6bd4 DeltaNet \u5728\u8bed\u8a00\u5efa\u6a21\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u66f4\u4f73\u3002</li>\n    <li>\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u9ad8\u4fdd\u771f\u5ea6\u3001\u53ef\u6269\u5c55\u7684\u7ebf\u6027\u65f6\u95f4\u6ce8\u610f\u529b\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Error-Free Linear Attention (EFLA) is a new method designed to improve long-context language models by solving the high computational cost of traditional attention mechanisms.</li>\n    <li>EFLA uses a continuous-time system for learning updates, allowing it to operate in linear time while maintaining full parallelism.</li>\n    <li>The method is designed to avoid error accumulation, effectively capturing continuous dynamics in language modeling.</li>\n    <li>In experiments, EFLA demonstrated better performance in noisy conditions and outperformed DeltaNet in language modeling without adding extra parameters.</li>\n    <li>This research lays the groundwork for creating efficient, high-quality language models with linear-time attention. </li>\n</ul>"}, "publishedAt": "2025-12-14T03:51:02.000Z", "title": "Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics", "summary": "Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12602.png", "numComments": 1, "submittedBy": {"_id": "626b626405fe1cb65725aca1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626b626405fe1cb65725aca1/ZVSbhynzpQhVGq9kGywW6.png", "fullname": "Soujanya Poria", "name": "soujanyaporia", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10}, "organization": {"_id": "626ab9dac804c432c1b27a48", "name": "declare-lab", "fullname": "Deep Cognition and Language Research (DeCLaRe) Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626b626405fe1cb65725aca1/grq3rj2uj0WRjjPjAtR1I.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13313", "authors": [{"_id": "6940d78b65f1e24a117805a4", "name": "Kling Team", "hidden": false}, {"_id": "6940d78b65f1e24a117805a5", "name": "Jialu Chen", "hidden": false}, {"_id": "6940d78b65f1e24a117805a6", "name": "Yikang Ding", "hidden": false}, {"_id": "6940d78b65f1e24a117805a7", "name": "Zhixue Fang", "hidden": false}, {"_id": "6940d78b65f1e24a117805a8", "name": "Kun Gai", "hidden": false}, {"_id": "6940d78b65f1e24a117805a9", "name": "Yuan Gao", "hidden": false}, {"_id": "6940d78b65f1e24a117805aa", "name": "Kang He", "hidden": false}, {"_id": "6940d78b65f1e24a117805ab", "name": "Jingyun Hua", "hidden": false}, {"_id": "6940d78b65f1e24a117805ac", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6940d78b65f1e24a117805ad", "name": "Mingming Lao", "hidden": false}, {"_id": "6940d78b65f1e24a117805ae", "name": "Xiaohan Li", "hidden": false}, {"_id": "6940d78b65f1e24a117805af", "name": "Hui Liu", "hidden": false}, {"_id": "6940d78b65f1e24a117805b0", "name": "Jiwen Liu", "hidden": false}, {"_id": "6940d78b65f1e24a117805b1", "name": "Xiaoqiang Liu", "hidden": false}, {"_id": "6940d78b65f1e24a117805b2", "name": "Yuan Liu", "hidden": false}, {"_id": "6940d78b65f1e24a117805b3", "name": "Shun Lu", "hidden": false}, {"_id": "6940d78b65f1e24a117805b4", "name": "Yongsen Mao", "hidden": false}, {"_id": "6940d78b65f1e24a117805b5", "name": "Yingchao Shao", "hidden": false}, {"_id": "6940d78b65f1e24a117805b6", "name": "Huafeng Shi", "hidden": false}, {"_id": "6940d78b65f1e24a117805b7", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6940d78b65f1e24a117805b8", "name": "Peiqin Sun", "hidden": false}, {"_id": "6940d78b65f1e24a117805b9", "name": "Songlin Tang", "hidden": false}, {"_id": "6940d78b65f1e24a117805ba", "name": "Pengfei Wan", "hidden": false}, {"_id": "6940d78b65f1e24a117805bb", "name": "Chao Wang", "hidden": false}, {"_id": "6940d78b65f1e24a117805bc", "name": "Xuebo Wang", "hidden": false}, {"_id": "6940d78b65f1e24a117805bd", "name": "Haoxian Zhang", "hidden": false}, {"_id": "6940d78b65f1e24a117805be", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6940d78b65f1e24a117805bf", "name": "Yan Zhou", "hidden": false}], "publishedAt": "2025-12-15T13:30:51.000Z", "submittedOnDailyAt": "2025-12-16T01:23:37.055Z", "title": "KlingAvatar 2.0 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.", "upvotes": 31, "discussionId": "6940d78c65f1e24a117805c0", "projectPage": "https://app.klingai.com/global/ai-human/image/new/", "ai_summary": "KlingAvatar 2.0 addresses inefficiencies in generating long-duration, high-resolution videos by using a spatio-temporal cascade framework with a Co-Reasoning Director and Negative Director for improved multimodal instruction alignment.", "ai_keywords": ["spatio-temporal cascade", "blueprint video keyframes", "high-resolution", "temporally coherent sub-clips", "first-last frame strategy", "smooth temporal transitions", "Co-Reasoning Director", "modality-specific large language model", "multi-turn dialogue", "Negative Director", "multimodal instruction following", "identity preservation", "lip synchronization"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Avatar\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u751f\u6210\u957f\u65f6\u95f4\u548c\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u65b9\u9762\u5b58\u5728\u6548\u7387\u95ee\u9898\uff0c\u5982\u65f6\u95f4\u6f02\u79fb\u548c\u8d28\u91cf\u4e0b\u964d\u3002</li>\n    <li>\u63d0\u51fa\u4e86KlingAvatar 2.0\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u65f6\u95f4\u7ea7\u8054\u63d0\u9ad8\u89c6\u9891\u8d28\u91cf\u3002</li>\n    <li>\u6846\u67b6\u5148\u751f\u6210\u4f4e\u5206\u8fa8\u7387\u89c6\u9891\u5173\u952e\u5e27\uff0c\u7136\u540e\u4f7f\u7528\u7b56\u7565\u4f18\u5316\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u7247\u6bb5\uff0c\u786e\u4fdd\u89c6\u9891\u6d41\u7545\u3002</li>\n    <li>\u5f15\u5165\u4e86Co-Reasoning Director\uff0c\u5229\u7528\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e13\u5bb6\u8fdb\u884c\u591a\u6a21\u6001\u6307\u4ee4\u878d\u5408\u548c\u7528\u6237\u610f\u56fe\u63a8\u65ad\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\u8be5\u6a21\u578b\u5728\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u65f6\uff0c\u89c6\u89c9\u6e05\u6670\u5ea6\u3001\u53e3\u578b\u540c\u6b65\u548c\u591a\u6a21\u6001\u6307\u4ee4\u9075\u5faa\u7b49\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Avatar video generation has improved, but existing models struggle with long, high-resolution videos, often causing quality issues and poor alignment with prompts.</li>\n    <li>The new KlingAvatar 2.0 uses a special framework to create low-resolution keyframes first, then refines them into high-resolution videos while ensuring smooth transitions.</li>\n    <li>It incorporates a Co-Reasoning Director with three language model experts to better understand user intentions and create detailed storylines through dialogue.</li>\n    <li>A Negative Director is included to refine negative prompts, improving how well the model follows instructions.</li>\n    <li>Tests show that KlingAvatar 2.0 effectively generates high-quality, coherent long videos with good visual detail and accurate character representation.</li>\n</ul>"}, "publishedAt": "2025-12-15T08:30:51.000Z", "title": "KlingAvatar 2.0 Technical Report", "summary": "Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13313.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 185}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.09636", "authors": [{"_id": "6940c58565f1e24a1177fbb2", "name": "Mengxi Xiao", "hidden": false}, {"_id": "6940c58565f1e24a1177fbb3", "user": {"_id": "646fc402e9c03ba436d5e93e", "avatarUrl": "/avatars/870c86dc99fb1cb6a348a7a0385b1a04.svg", "isPro": false, "fullname": "Kailai Yang", "user": "klyang", "type": "user"}, "name": "Kailai Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:45.859Z", "hidden": false}, {"_id": "6940c58565f1e24a1177fbb4", "name": "Pengde Zhao", "hidden": false}, {"_id": "6940c58565f1e24a1177fbb5", "name": "Enze Zhang", "hidden": false}, {"_id": "6940c58565f1e24a1177fbb6", "user": {"_id": "67fd030140e7539fcc5f6521", "avatarUrl": "/avatars/d79c0eed928d964c3b014a0cf1f01d72.svg", "isPro": false, "fullname": "Ziyan Kuang", "user": "plumjane", "type": "user"}, "name": "Ziyan Kuang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:47:51.153Z", "hidden": false}, {"_id": "6940c58565f1e24a1177fbb7", "name": "Zhiwei Liu", "hidden": false}, {"_id": "6940c58565f1e24a1177fbb8", "name": "Weiguang Han", "hidden": false}, {"_id": "6940c58565f1e24a1177fbb9", "name": "Shu Liao", "hidden": false}, {"_id": "6940c58565f1e24a1177fbba", "name": "Lianting Huang", "hidden": false}, {"_id": "6940c58565f1e24a1177fbbb", "name": "Jinpeng Hu", "hidden": false}, {"_id": "6940c58565f1e24a1177fbbc", "name": "Min Peng", "hidden": false}, {"_id": "6940c58565f1e24a1177fbbd", "user": {"_id": "6479f4317c18dca75e9a9324", "avatarUrl": "/avatars/9aa709230b057f57ee4415c04a622c63.svg", "isPro": false, "fullname": "Xie", "user": "QianqianXie1994", "type": "user"}, "name": "Qianqian Xie", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:47:53.066Z", "hidden": false}, {"_id": "6940c58565f1e24a1177fbbe", "name": "Sophia Ananiadou", "hidden": false}], "publishedAt": "2025-12-10T13:26:22.000Z", "submittedOnDailyAt": "2025-12-16T00:08:57.103Z", "title": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment", "submittedOnDailyBy": {"_id": "663adb42e14047f710dc1d29", "avatarUrl": "/avatars/7ca49d67a4a8b4cf0ee896e07646715f.svg", "isPro": false, "fullname": "Mengxi Xiao", "user": "ElsaShaw", "type": "user"}, "summary": "Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.", "upvotes": 21, "discussionId": "6940c58565f1e24a1177fbbf", "githubRepo": "https://github.com/elsa66666/MentraSuite", "githubRepoAddedBy": "user", "ai_summary": "MentraSuite, a unified framework, advances reliable mental health reasoning using Mindora, a post-trained model with hybrid SFT-RL, evaluated via MentraBench, a benchmark assessing task performance and reasoning quality.", "ai_keywords": ["Large language models (LLMs)", "mental-health reasoning", "MentraSuite", "MentraBench", "Mindora", "hybrid SFT-RL", "inconsistency-detection reward", "reasoning trajectory generation", "task performance", "reasoning quality", "conciseness", "coherence", "hallucination avoidance", "task understanding", "internal consistency"], "githubStars": 7, "organization": {"_id": "6693770d0bf3f05db3017f31", "name": "NextGenWhu", "fullname": "CLAIN-WHU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6479f4317c18dca75e9a9324/DydmQ18EutkpFraI3FGpy.png"}, "summary_zh": "<ul>\n    <li>\u5fc3\u7406\u5065\u5eb7\u969c\u788d\u5f71\u54cd\u5168\u7403\u6570\u4ebf\u4eba\uff0c\u7f51\u7edc\u6210\u4e3a\u83b7\u53d6\u652f\u6301\u548c\u4fe1\u606f\u7684\u4e3b\u8981\u6e20\u9053\u3002</li>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u4ee5\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5e2e\u52a9\uff0c\u4f46\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u5e94\u7528\u5b58\u5728\u98ce\u9669\u3002</li>\n    <li>\u73b0\u6709\u7684\u5fc3\u7406\u5b66LLMs\u4e3b\u8981\u5173\u6ce8\u60c5\u611f\u7406\u89e3\u548c\u77e5\u8bc6\u56de\u5fc6\uff0c\u4f46\u7f3a\u4e4f\u4e34\u5e8a\u6240\u9700\u7684\u9010\u6b65\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86MentraSuite\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u5fc3\u7406\u5065\u5eb7\u63a8\u7406\u7684\u53ef\u9760\u6027\u3002</li>\n    <li>Mindora\u662f\u7ecf\u8fc7\u4f18\u5316\u7684\u6a21\u578b\uff0c\u5728MentraBench\u57fa\u51c6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u7279\u522b\u662f\u5728\u63a8\u7406\u53ef\u9760\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Mental health disorders are widespread, and many people now use the internet for support and information.</li>\n    <li>Large language models (LLMs) can provide help, but they can be risky if their reasoning is not accurate or consistent.</li>\n    <li>Current psychological LLMs focus on emotions and knowledge but lack the detailed reasoning needed for mental health assessments.</li>\n    <li>MentraSuite is introduced as a framework to improve mental health reasoning, with MentraBench as a benchmark to evaluate performance across various tasks.</li>\n    <li>Mindora is a new model that has been trained to enhance reasoning reliability, achieving the best results in evaluations of reasoning quality.</li>\n</ul>"}, "publishedAt": "2025-12-10T08:26:22.000Z", "title": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment", "summary": "Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09636.png", "numComments": 1, "submittedBy": {"_id": "663adb42e14047f710dc1d29", "avatarUrl": "/avatars/7ca49d67a4a8b4cf0ee896e07646715f.svg", "fullname": "Mengxi Xiao", "name": "ElsaShaw", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "6693770d0bf3f05db3017f31", "name": "NextGenWhu", "fullname": "CLAIN-WHU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6479f4317c18dca75e9a9324/DydmQ18EutkpFraI3FGpy.png"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2512.13586", "authors": [{"_id": "6940d86d65f1e24a117805cd", "user": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "isPro": false, "fullname": "Jia-Nan Li", "user": "JinaLeejnl", "type": "user"}, "name": "Jia-Nan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:38:49.797Z", "hidden": false}, {"_id": "6940d86d65f1e24a117805ce", "name": "Jian Guan", "hidden": false}, {"_id": "6940d86d65f1e24a117805cf", "name": "Wei Wu", "hidden": false}, {"_id": "6940d86d65f1e24a117805d0", "name": "Chongxuan Li", "hidden": false}], "publishedAt": "2025-12-15T17:41:19.000Z", "submittedOnDailyAt": "2025-12-16T01:39:12.101Z", "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "submittedOnDailyBy": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "isPro": false, "fullname": "Jia-Nan Li", "user": "JinaLeejnl", "type": "user"}, "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.", "upvotes": 74, "discussionId": "6940d86d65f1e24a117805d1", "githubRepo": "https://github.com/ML-GSAI/ReFusion", "githubRepoAddedBy": "user", "ai_summary": "ReFusion, a novel masked diffusion model, improves performance and efficiency by using slot-based parallel decoding, achieving superior results compared to autoregressive models and traditional masked diffusion models.", "ai_keywords": ["autoregressive models", "masked diffusion models", "Key-Value caching", "parallel decoding", "token level", "slot level", "diffusion-based planning", "autoregressive infilling", "slot-based design", "slot-level permutation space"], "githubStars": 18, "summary_zh": "<ul>\n    <li>\u81ea\u56de\u5f52\u6a21\u578b\uff08ARMs\uff09\u5728\u63a8\u7406\u65f6\u901f\u5ea6\u8f83\u6162\u3002</li>\n    <li>\u63a9\u853d\u6269\u6563\u6a21\u578b\uff08MDMs\uff09\u867d\u7136\u53ef\u4ee5\u5e76\u884c\u5904\u7406\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u751f\u6210\u4e0d\u8fde\u8d2f\u7684\u95ee\u9898\u3002</li>\n    <li>ReFusion\u662f\u4e00\u4e2a\u65b0\u578b\u7684\u63a9\u853d\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u63d0\u5347\u5e76\u884c\u89e3\u7801\u81f3\u66f4\u9ad8\u7684\u69fd\u7ea7\u522b\u6765\u63d0\u9ad8\u6027\u80fd\u548c\u6548\u7387\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u201c\u8ba1\u5212\u4e0e\u586b\u5145\u201d\u7684\u8fed\u4ee3\u89e3\u7801\u8fc7\u7a0b\uff0c\u5148\u8bc6\u522b\u5f31\u4f9d\u8d56\u69fd\uff0c\u518d\u5e76\u884c\u89e3\u7801\u8fd9\u4e9b\u69fd\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cReFusion\u5728\u6027\u80fd\u4e0a\u6bd4\u4e4b\u524d\u7684MDMs\u63d0\u9ad8\u4e8634%\uff0c\u901f\u5ea6\u5e73\u5747\u63d0\u5347\u8d85\u8fc718\u500d\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u63a5\u8fd1\u5f3a\u81ea\u56de\u5f52\u6a21\u578b\u540c\u65f6\u4fdd\u63012.33\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoregressive models (ARMs) have slow inference times, while masked diffusion models (MDMs) face issues like high computational costs and incoherent outputs.</li>\n    <li>ReFusion is a new masked diffusion model that improves efficiency by processing data in larger segments called slots instead of individual tokens.</li>\n    <li>It uses a two-step process: first, it plans which slots to fill, and then it fills them in parallel.</li>\n    <li>This slot-based approach allows better use of memory (KV cache) and simplifies learning by reducing complexity.</li>\n    <li>Tests show that ReFusion outperforms previous MDMs by 34% and is over 18 times faster on average, while also being faster than traditional ARMs.</li>\n</ul>"}, "publishedAt": "2025-12-15T12:41:19.000Z", "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13586.png", "numComments": 2, "submittedBy": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "fullname": "Jia-Nan Li", "name": "JinaLeejnl", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "isAuthorParticipating": true}, {"paper": {"id": "2512.13687", "authors": [{"_id": "6940ee0665f1e24a1178066d", "user": {"_id": "67756c9c846a267749304255", "avatarUrl": "/avatars/01f09805b561887c55d1b9ad4e96b461.svg", "isPro": false, "fullname": "Jingfeng Yao", "user": "MapleF9", "type": "user"}, "name": "Jingfeng Yao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T10:34:03.365Z", "hidden": false}, {"_id": "6940ee0665f1e24a1178066e", "user": {"_id": "6264bf5a1ed8d81e47ae3a62", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650769741842-noauth.jpeg", "isPro": false, "fullname": "Yuda Song", "user": "IDKiro", "type": "user"}, "name": "Yuda Song", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:38.071Z", "hidden": false}, {"_id": "6940ee0665f1e24a1178066f", "user": {"_id": "64192280d459c9e7fbb03aa1", "avatarUrl": "/avatars/89935de92f3f9107d7b768b82fb27e70.svg", "isPro": false, "fullname": "Zhou", "user": "Yucong", "type": "user"}, "name": "Yucong Zhou", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T10:34:50.881Z", "hidden": false}, {"_id": "6940ee0665f1e24a11780670", "user": {"_id": "62600de6d47e3dbae32ce1ce", "avatarUrl": "/avatars/a536417cfec6e10ac415091bd1829426.svg", "isPro": false, "fullname": "Xinggang Wang", "user": "xinggangw", "type": "user"}, "name": "Xinggang Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T10:34:40.199Z", "hidden": false}], "publishedAt": "2025-12-15T18:59:54.000Z", "submittedOnDailyAt": "2025-12-16T07:11:50.997Z", "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation", "submittedOnDailyBy": {"_id": "67756c9c846a267749304255", "avatarUrl": "/avatars/01f09805b561887c55d1b9ad4e96b461.svg", "isPro": false, "fullname": "Jingfeng Yao", "user": "MapleF9", "type": "user"}, "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.", "upvotes": 66, "discussionId": "6940ee0665f1e24a11780671", "githubRepo": "https://github.com/MiniMax-AI/VTP", "githubRepoAddedBy": "user", "ai_summary": "A unified visual tokenizer pre-training framework (VTP) improves generative performance by optimizing image-text contrastive, self-supervised, and reconstruction losses, leading to better scaling properties and higher zero-shot accuracy and faster convergence.", "ai_keywords": ["latent space", "visual tokenizers", "VAEs", "reconstruction-based training", "low-level information", "pre-training scaling problem", "high-level semantics", "VTP", "image-text contrastive", "self-supervised", "reconstruction losses", "generative performance", "ImageNet", "zero-shot accuracy", "rFID", "FLOPS", "DiT", "advanced distillation methods"], "githubStars": 92, "organization": {"_id": "6778fc29920093dbc0c24917", "name": "MiniMaxAI", "fullname": "MiniMax", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/676e38ad04af5bec20bc9faf/dUd-LsZEX0H_d4qefO_g6.jpeg"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u6807\u8bb0\u5668\uff08\u5982VAE\uff09\u7684\u6f5c\u5728\u7a7a\u95f4\u8d28\u91cf\u5bf9\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>\u6807\u51c6\u7684\u91cd\u5efa\u8bad\u7ec3\u65b9\u6cd5\u5bfc\u81f4\u6f5c\u5728\u7a7a\u95f4\u504f\u5411\u4f4e\u7ea7\u4fe1\u606f\uff0c\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86VTP\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u56fe\u50cf-\u6587\u672c\u5bf9\u6bd4\u3001\u81ea\u76d1\u7763\u548c\u91cd\u5efa\u635f\u5931\u6765\u6539\u5584\u6f5c\u5728\u7a7a\u95f4\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u7406\u89e3\u80fd\u529b\u662f\u751f\u6210\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u5e76\u4e14\u751f\u6210\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u3001\u53c2\u6570\u548c\u6570\u636e\u7684\u89c4\u6a21\u6709\u6548\u5173\u8054\u3002</li>\n    <li>\u7ecf\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0cVTP\u6a21\u578b\u5728\u751f\u6210\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\u6536\u655b\u901f\u5ea6\u66f4\u5feb\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The quality of visual tokenizers is important for generative models, but current training methods focus too much on low-level details.</li>\n    <li>This focus leads to a problem called the \"pre-training scaling problem,\" where more computing power does not improve generation quality.</li>\n    <li>To fix this, a new approach called VTP is introduced, which optimizes different types of losses together to better capture high-level meanings.</li>\n    <li>VTP shows improved performance, including faster generation times and better accuracy compared to traditional methods.</li>\n    <li>The pre-trained models are available online for further use and research.</li>\n</ul>"}, "publishedAt": "2025-12-15T13:59:54.000Z", "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation", "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13687.png", "numComments": 1, "submittedBy": {"_id": "67756c9c846a267749304255", "avatarUrl": "/avatars/01f09805b561887c55d1b9ad4e96b461.svg", "fullname": "Jingfeng Yao", "name": "MapleF9", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "6778fc29920093dbc0c24917", "name": "MiniMaxAI", "fullname": "MiniMax", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/676e38ad04af5bec20bc9faf/dUd-LsZEX0H_d4qefO_g6.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.13564", "authors": [{"_id": "6940d68565f1e24a1178056c", "user": {"_id": "6544b9b646dbdeca34ee5f52", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png", "isPro": false, "fullname": "Yuyang Hu", "user": "namespace-ERI", "type": "user"}, "name": "Yuyang Hu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:40.040Z", "hidden": false}, {"_id": "6940d68565f1e24a1178056d", "user": {"_id": "65435cad429b80b14922ab8d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/N8oWq4ZZn3dRxmXi18FrA.jpeg", "isPro": false, "fullname": "Shichun Liu", "user": "Liusc2020", "type": "user"}, "name": "Shichun Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:40.844Z", "hidden": false}, {"_id": "6940d68565f1e24a1178056e", "name": "Yanwei Yue", "hidden": false}, {"_id": "6940d68565f1e24a1178056f", "name": "Guibin Zhang", "hidden": false}, {"_id": "6940d68565f1e24a11780570", "name": "Boyang Liu", "hidden": false}, {"_id": "6940d68565f1e24a11780571", "name": "Fangyi Zhu", "hidden": false}, {"_id": "6940d68565f1e24a11780572", "name": "Jiahang Lin", "hidden": false}, {"_id": "6940d68565f1e24a11780573", "user": {"_id": "638ef0b0c67af472d31674a6", "avatarUrl": "/avatars/02df97d15a0f46b47f9162221733b121.svg", "isPro": false, "fullname": "Honglin Guo", "user": "KYLN24", "type": "user"}, "name": "Honglin Guo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:38.698Z", "hidden": false}, {"_id": "6940d68565f1e24a11780574", "name": "Shihan Dou", "hidden": false}, {"_id": "6940d68565f1e24a11780575", "name": "Zhiheng Xi", "hidden": false}, {"_id": "6940d68565f1e24a11780576", "name": "Senjie Jin", "hidden": false}, {"_id": "6940d68565f1e24a11780577", "user": {"_id": "62e52483a944e2a56cd2c6ca", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg", "isPro": false, "fullname": "Jiejun Tan", "user": "zstanjj", "type": "user"}, "name": "Jiejun Tan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:42.726Z", "hidden": false}, {"_id": "6940d68565f1e24a11780578", "name": "Yanbin Yin", "hidden": false}, {"_id": "6940d68565f1e24a11780579", "name": "Jiongnan Liu", "hidden": false}, {"_id": "6940d68565f1e24a1178057a", "name": "Zeyu Zhang", "hidden": false}, {"_id": "6940d68565f1e24a1178057b", "user": {"_id": "6309bfdab8d7b3889319b588", "avatarUrl": "/avatars/572acdad470f765ef2e058ead3741e24.svg", "isPro": false, "fullname": "SunZX", "user": "Jeryi", "type": "user"}, "name": "Zhongxiang Sun", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T22:32:57.018Z", "hidden": false}, {"_id": "6940d68565f1e24a1178057c", "name": "Yutao Zhu", "hidden": false}, {"_id": "6940d68565f1e24a1178057d", "name": "Hao Sun", "hidden": false}, {"_id": "6940d68565f1e24a1178057e", "name": "Boci Peng", "hidden": false}, {"_id": "6940d68565f1e24a1178057f", "name": "Zhenrong Cheng", "hidden": false}, {"_id": "6940d68565f1e24a11780580", "name": "Xuanbo Fan", "hidden": false}, {"_id": "6940d68565f1e24a11780581", "name": "Jiaxin Guo", "hidden": false}, {"_id": "6940d68565f1e24a11780582", "name": "Xinlei Yu", "hidden": false}, {"_id": "6940d68565f1e24a11780583", "name": "Zhenhong Zhou", "hidden": false}, {"_id": "6940d68565f1e24a11780584", "name": "Zewen Hu", "hidden": false}, {"_id": "6940d68565f1e24a11780585", "name": "Jiahao Huo", "hidden": false}, {"_id": "6940d68565f1e24a11780586", "name": "Junhao Wang", "hidden": false}, {"_id": "6940d68565f1e24a11780587", "name": "Yuwei Niu", "hidden": false}, {"_id": "6940d68565f1e24a11780588", "name": "Yu Wang", "hidden": false}, {"_id": "6940d68565f1e24a11780589", "name": "Zhenfei Yin", "hidden": false}, {"_id": "6940d68565f1e24a1178058a", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6940d68565f1e24a1178058b", "name": "Yue Liao", "hidden": false}, {"_id": "6940d68565f1e24a1178058c", "name": "Qiankun Li", "hidden": false}, {"_id": "6940d68565f1e24a1178058d", "name": "Kun Wang", "hidden": false}, {"_id": "6940d68565f1e24a1178058e", "name": "Wangchunshu Zhou", "hidden": false}, {"_id": "6940d68565f1e24a1178058f", "name": "Yixin Liu", "hidden": false}, {"_id": "6940d68565f1e24a11780590", "name": "Dawei Cheng", "hidden": false}, {"_id": "6940d68565f1e24a11780591", "name": "Qi Zhang", "hidden": false}, {"_id": "6940d68565f1e24a11780592", "name": "Tao Gui", "hidden": false}, {"_id": "6940d68565f1e24a11780593", "name": "Shirui Pan", "hidden": false}, {"_id": "6940d68565f1e24a11780594", "name": "Yan Zhang", "hidden": false}, {"_id": "6940d68565f1e24a11780595", "name": "Philip Torr", "hidden": false}, {"_id": "6940d68565f1e24a11780596", "name": "Zhicheng Dou", "hidden": false}, {"_id": "6940d68565f1e24a11780597", "name": "Ji-Rong Wen", "hidden": false}, {"_id": "6940d68565f1e24a11780598", "name": "Xuanjing Huang", "hidden": false}, {"_id": "6940d68565f1e24a11780599", "name": "Yu-Gang Jiang", "hidden": false}, {"_id": "6940d68565f1e24a1178059a", "name": "Shuicheng Yan", "hidden": false}], "publishedAt": "2025-12-15T17:22:34.000Z", "submittedOnDailyAt": "2025-12-16T01:18:34.363Z", "title": "Memory in the Age of AI Agents", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.", "upvotes": 62, "discussionId": "6940d68565f1e24a1178059b", "githubRepo": "https://github.com/Shichun-Liu/Agent-Memory-Paper-List", "githubRepoAddedBy": "user", "ai_summary": "This survey provides an updated overview of agent memory research, distinguishing its forms, functions, and dynamics, and highlights emerging research directions.", "ai_keywords": ["agent memory", "LLM memory", "retrieval augmented generation (RAG)", "context engineering", "token-level memory", "parametric memory", "latent memory", "factual memory", "experiential memory", "working memory", "memory benchmarks", "open-source frameworks", "memory automation", "reinforcement learning integration", "multimodal memory", "multi-agent memory", "trustworthiness issues"], "githubStars": 115, "summary_zh": "<ul>\n    <li>\u8bb0\u5fc6\u662f\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u667a\u80fd\u4f53\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u5e76\u5c06\u6301\u7eed\u53d7\u5230\u5173\u6ce8\u3002</li>\n    <li>\u76ee\u524d\u7684\u667a\u80fd\u4f53\u8bb0\u5fc6\u7814\u7a76\u9886\u57df\u5b58\u5728\u660e\u663e\u7684\u788e\u7247\u5316\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u5b9a\u4e49\u548c\u5206\u7c7b\u3002</li>\n    <li>\u672c\u6587\u660e\u786e\u533a\u5206\u667a\u80fd\u4f53\u8bb0\u5fc6\u4e0e\u76f8\u5173\u6982\u5ff5\uff0c\u5e76\u4ece\u5f62\u5f0f\u3001\u529f\u80fd\u548c\u52a8\u6001\u4e09\u4e2a\u89d2\u5ea6\u8fdb\u884c\u5206\u6790\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u66f4\u7ec6\u81f4\u7684\u5206\u7c7b\uff0c\u5305\u62ec\u4e09\u79cd\u4e3b\u8981\u7684\u8bb0\u5fc6\u5f62\u5f0f\u548c\u4e09\u79cd\u529f\u80fd\u7c7b\u578b\u3002</li>\n    <li>\u603b\u7ed3\u4e86\u8bb0\u5fc6\u57fa\u51c6\u548c\u5f00\u6e90\u6846\u67b6\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5982\u8bb0\u5fc6\u81ea\u52a8\u5316\u548c\u591a\u6a21\u6001\u8bb0\u5fc6\u7b49\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Memory is a key feature in agents that use foundation models, and research on this topic is quickly growing.</li>\n    <li>Current research on agent memory is diverse and often unclear, with various definitions and methods used.</li>\n    <li>This work aims to clarify agent memory by distinguishing it from similar concepts and outlining its different forms, functions, and dynamics.</li>\n    <li>Three main types of agent memory identified are token-level, parametric, and latent memory, along with categories like factual and experiential memory.</li>\n    <li>The survey also discusses future research directions, including memory automation and multi-agent memory, and provides resources for practical development.</li>\n</ul>"}, "publishedAt": "2025-12-15T12:22:34.000Z", "title": "Memory in the Age of AI Agents", "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13564.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 185}, "isAuthorParticipating": false}, {"paper": {"id": "2512.10430", "authors": [{"_id": "693bba8d9874a2a5e4ffb3ab", "user": {"_id": "64f4c8739ee58d48e8507e0e", "avatarUrl": "/avatars/4be540dfb4a949f37cba2d3c3729fbde.svg", "isPro": false, "fullname": "Dmitrii Stoianov", "user": "heylimon", "type": "user"}, "name": "Dmitrii Stoianov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:40.198Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ac", "user": {"_id": "64fb054ebb362cbf2fe53159", "avatarUrl": "/avatars/936c37a77d46d0ea579d2f8a9aea9284.svg", "isPro": false, "fullname": "Danil Taranets", "user": "taranetsdan", "type": "user"}, "name": "Danil Taranets", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:29.281Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ad", "user": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "isPro": false, "fullname": "Olga Tsymboi", "user": "oltsy", "type": "user"}, "name": "Olga Tsymboi", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:38.180Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ae", "user": {"_id": "6780dcd6acf8d824c03864da", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6PeN6OXbSq0M-L4OxFTrn.png", "isPro": false, "fullname": "Ramil Latypov", "user": "kylecr4ne", "type": "user"}, "name": "Ramil Latypov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:23.437Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3af", "user": {"_id": "6513f03e86d74f32ed65e3b8", "avatarUrl": "/avatars/c327966623f775a2d1f3d984ca162ef6.svg", "isPro": false, "fullname": "Almaz Dautov", "user": "the-hir0", "type": "user"}, "name": "Almaz Dautov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:30.990Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b0", "user": {"_id": "621a8daf325b927e60fcef08", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621a8daf325b927e60fcef08/bM8W-of2u0yvL8FHeY2ra.jpeg", "isPro": false, "fullname": "Vladislav Kruglikov", "user": "vladislavkruglikov", "type": "user"}, "name": "Vladislav Kruglikov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:21.170Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b1", "name": "Nikita Surkov", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b2", "user": {"_id": "63188c428d698d8c1642a0d8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63188c428d698d8c1642a0d8/MlcBnU7CmnKdRF053JcY4.jpeg", "isPro": false, "fullname": "German Abramov", "user": "germanjke", "type": "user"}, "name": "German Abramov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T12:59:28.579Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b3", "name": "Pavel Gein", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b4", "user": {"_id": "636a9a07e3ad78bc68b1a5a2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668020490988-636a9a07e3ad78bc68b1a5a2.jpeg", "isPro": false, "fullname": "Dmitry Abulkhanov", "user": "mponty", "type": "user"}, "name": "Dmitry Abulkhanov", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:02:42.107Z", "hidden": true}, {"_id": "693bba8d9874a2a5e4ffb3b5", "user": {"_id": "658bc20cfdf2279d4721f218", "avatarUrl": "/avatars/5f1cb94373fbbbcfed9b848c5ebdd1ad.svg", "isPro": false, "fullname": "Mikhail Gashkov", "user": "MikeGashkov", "type": "user"}, "name": "Mikhail Gashkov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:32.809Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b6", "name": "Viktor Zelenkovskiy", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b7", "user": {"_id": "644f64bc17b6189cda54cae8", "avatarUrl": "/avatars/f684a65b35a9be06cbb16fb8f44a4782.svg", "isPro": false, "fullname": "Artem Batalov", "user": "batalovme", "type": "user"}, "name": "Artem Batalov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:27.497Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b8", "user": {"_id": "62609d224e6e4b84475eb8d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62609d224e6e4b84475eb8d9/PKQvuLm40PGg91VKRVSIb.jpeg", "isPro": false, "fullname": "Alex Medvedev", "user": "kenkaneki", "type": "user"}, "name": "Aleksandr Medvedev", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:36.035Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b9", "user": {"_id": "63f26358be95ed4c9a9b0583", "avatarUrl": "/avatars/133f2d28c5e5139d61048dfef5e9f4ff.svg", "isPro": false, "fullname": "Anatoly Potapov", "user": "AnatoliiPotapov", "type": "user"}, "name": "Anatolii Potapov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:25.666Z", "hidden": false}], "publishedAt": "2025-12-11T08:40:10.000Z", "submittedOnDailyAt": "2025-12-12T08:33:32.798Z", "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground", "submittedOnDailyBy": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "isPro": false, "fullname": "Olga Tsymboi", "user": "oltsy", "type": "user"}, "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.", "upvotes": 60, "discussionId": "693bba8e9874a2a5e4ffb3ba", "ai_summary": "T-pro 2.0 is an open-weight Russian LLM for hybrid reasoning and efficient inference, using a Cyrillic-dense tokenizer and EAGLE speculative-decoding pipeline.", "ai_keywords": ["Cyrillic-dense tokenizer", "EAGLE speculative-decoding pipeline", "hybrid reasoning", "efficient inference", "reasoning-trace generation"], "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86T-pro 2.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u653e\u6743\u91cd\u7684\u4fc4\u7f57\u65af\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u6df7\u5408\u63a8\u7406\u548c\u9ad8\u6548\u63a8\u7406\u3002</li>\n    <li>\u6a21\u578b\u53ef\u4ee5\u76f4\u63a5\u56de\u7b54\u95ee\u9898\uff0c\u5e76\u751f\u6210\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f7f\u7528\u4e86\u4e00\u79cd\u5bc6\u96c6\u7684\u897f\u91cc\u5c14\u5b57\u6bcd\u5206\u8bcd\u5668\u548c\u4f18\u5316\u7684EAGLE\u89e3\u7801\u7ba1\u9053\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u3002</li>\n    <li>\u4e3a\u4e86\u4fc3\u8fdb\u53ef\u91cd\u590d\u548c\u53ef\u6269\u5c55\u7684\u7814\u7a76\uff0c\u53d1\u5e03\u4e86\u6a21\u578b\u6743\u91cd\u3001T-Wix 500k\u6307\u4ee4\u8bed\u6599\u5e93\u3001T-Math\u63a8\u7406\u57fa\u51c6\u548cEAGLE\u6743\u91cd\u3002</li>\n    <li>\u8fd9\u4e9b\u8d44\u6e90\u4f7f\u7528\u6237\u80fd\u591f\u7814\u7a76\u4fc4\u8bed\u63a8\u7406\u5e76\u6269\u5c55\u6216\u8c03\u6574\u6a21\u578b\u548c\u63a8\u7406\u7ba1\u9053\u3002</li>\n    <li>\u63d0\u4f9b\u4e86\u4e00\u4e2a\u516c\u5171\u7f51\u7edc\u6f14\u793a\uff0c\u5c55\u793a\u4e86\u63a8\u7406\u548c\u975e\u63a8\u7406\u6a21\u5f0f\uff0c\u4ee5\u53ca\u6211\u4eec\u63a8\u7406\u7cfb\u7edf\u5728\u4e0d\u540c\u9886\u57df\u7684\u52a0\u901f\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>T-pro 2.0 is a new Russian language model designed for reasoning and fast responses.</li>\n    <li>The model uses a special tokenizer and a unique decoding method to work quickly.</li>\n    <li>Researchers can access the model, a large instruction dataset, and a reasoning test on Hugging Face.</li>\n    <li>A public demo shows how the model can reason and respond quickly in different areas.</li>\n    <li>T-pro 2.0 aims to help users create and test effective applications using Russian language models.</li>\n</ul>"}, "publishedAt": "2025-12-11T03:40:10.000Z", "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground", "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10430.png", "numComments": 1, "submittedBy": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "fullname": "Olga Tsymboi", "name": "oltsy", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.12967", "authors": [{"_id": "6940d25d65f1e24a11780417", "user": {"_id": "64777a346e6c7ac608c1e9bf", "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg", "isPro": false, "fullname": "Weizhou Shen", "user": "shenwzh3", "type": "user"}, "name": "Weizhou Shen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:42.079Z", "hidden": false}, {"_id": "6940d25d65f1e24a11780418", "user": {"_id": "64c9b0f28d2d187c24d1e6c1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1CPnAaB3gsupdpiNWaoDc.png", "isPro": false, "fullname": "ZiYi Yang", "user": "AALF", "type": "user"}, "name": "Ziyi Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:56.062Z", "hidden": false}, {"_id": "6940d25d65f1e24a11780419", "name": "Chenliang Li", "hidden": false}, {"_id": "6940d25d65f1e24a1178041a", "name": "Zhiyuan Lu", "hidden": false}, {"_id": "6940d25d65f1e24a1178041b", "name": "Miao Peng", "hidden": false}, {"_id": "6940d25d65f1e24a1178041c", "name": "Huashan Sun", "hidden": false}, {"_id": "6940d25d65f1e24a1178041d", "name": "Yingcheng Shi", "hidden": false}, {"_id": "6940d25d65f1e24a1178041e", "name": "Shengyi Liao", "hidden": false}, {"_id": "6940d25d65f1e24a1178041f", "name": "Shaopeng Lai", "hidden": false}, {"_id": "6940d25d65f1e24a11780420", "name": "Bo Zhang", "hidden": false}, {"_id": "6940d25d65f1e24a11780421", "name": "Dayiheng Liu", "hidden": false}, {"_id": "6940d25d65f1e24a11780422", "name": "Fei Huang", "hidden": false}, {"_id": "6940d25d65f1e24a11780423", "name": "Jingren Zhou", "hidden": false}, {"_id": "6940d25d65f1e24a11780424", "name": "Ming Yan", "hidden": false}], "publishedAt": "2025-12-15T04:11:11.000Z", "submittedOnDailyAt": "2025-12-16T01:29:41.007Z", "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.", "upvotes": 54, "discussionId": "6940d25d65f1e24a11780425", "githubRepo": "https://github.com/Tongyi-Zhiwen/Qwen-Doc", "githubRepoAddedBy": "user", "ai_summary": "QwenLong-L1.5 enhances long-context reasoning through data synthesis, stabilized reinforcement learning, and memory-augmented architecture, achieving superior performance on benchmarks and general domains.", "ai_keywords": ["Long-Context Data Synthesis Pipeline", "atomic facts", "verifiable reasoning questions", "Stabilized Reinforcement Learning", "task-balanced sampling", "task-specific advantage estimation", "Adaptive Entropy-Controlled Policy Optimization", "Memory-Augmented Architecture", "multi-stage fusion RL training", "single-pass reasoning", "iterative memory-based processing", "memory-agent framework"], "githubStars": 311, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51faQwenLong-L1.5\u6a21\u578b\uff0c\u5177\u5907\u5353\u8d8a\u7684\u957f\u6587\u672c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5efa\u7acb\u4e86\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u751f\u6210\u9700\u8981\u591a\u6b65\u63a8\u7406\u7684\u96be\u9898\u3002</li>\n    <li>\u4f7f\u7528\u7a33\u5b9a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\uff0c\u6539\u5584\u4e86\u5956\u52b1\u504f\u5dee\u95ee\u9898\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u589e\u5f3a\u8bb0\u5fc6\u67b6\u6784\uff0c\u652f\u6301\u5904\u7406\u8d85\u8fc7400\u4e07\u6807\u8bb0\u7684\u8d85\u957f\u4efb\u52a1\u3002</li>\n    <li>\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cQwenLong-L1.5\u7684\u8868\u73b0\u8d85\u8fc7GPT-5\u548cGemini-2.5-Pro\uff0c\u5e73\u5747\u63d0\u9ad89.90\u5206\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>QwenLong-L1.5 is a new model that improves long-context reasoning using advanced training methods.</li>\n    <li>It features a data synthesis pipeline that creates complex reasoning tasks by breaking down documents into facts and relationships.</li>\n    <li>The model uses a special training method to stabilize long-context reinforcement learning and improve decision-making.</li>\n    <li>It includes a memory management system that helps handle very long text sequences of over 4 million tokens.</li>\n    <li>QwenLong-L1.5 performs as well as or better than other leading models in long-context reasoning and improves performance in areas like scientific reasoning and extended dialogue.</li>\n</ul>"}, "publishedAt": "2025-12-14T23:11:11.000Z", "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management", "summary": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12967.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 185}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.13604", "authors": [{"_id": "6940d44165f1e24a11780535", "name": "Jianxiong Gao", "hidden": false}, {"_id": "6940d44165f1e24a11780536", "name": "Zhaoxi Chen", "hidden": false}, {"_id": "6940d44165f1e24a11780537", "name": "Xian Liu", "hidden": false}, {"_id": "6940d44165f1e24a11780538", "user": {"_id": "64970d3d9c3b29dca8633f87", "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg", "isPro": false, "fullname": "JunhaoZhuang", "user": "JunhaoZhuang", "type": "user"}, "name": "Junhao Zhuang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:51.620Z", "hidden": false}, {"_id": "6940d44165f1e24a11780539", "name": "Chengming Xu", "hidden": false}, {"_id": "6940d44165f1e24a1178053a", "name": "Jianfeng Feng", "hidden": false}, {"_id": "6940d44165f1e24a1178053b", "name": "Yu Qiao", "hidden": false}, {"_id": "6940d44165f1e24a1178053c", "name": "Yanwei Fu", "hidden": false}, {"_id": "6940d44165f1e24a1178053d", "user": {"_id": "635f8ed47c05eb9f59963d3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg", "isPro": false, "fullname": "ChenyangSi", "user": "ChenyangSi", "type": "user"}, "name": "Chenyang Si", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:49.025Z", "hidden": false}, {"_id": "6940d44165f1e24a1178053e", "name": "Ziwei Liu", "hidden": false}], "publishedAt": "2025-12-15T17:59:58.000Z", "submittedOnDailyAt": "2025-12-16T01:12:24.486Z", "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model", "submittedOnDailyBy": {"_id": "643815c4961bb61e463c5896", "avatarUrl": "/avatars/3b44592472f16c56105bff8c314d9939.svg", "isPro": false, "fullname": "Jianxiong Gao", "user": "Jianxiong", "type": "user"}, "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.", "upvotes": 52, "discussionId": "6940d44165f1e24a1178053f", "projectPage": "https://vchitect.github.io/LongVie2-project/", "ai_summary": "LongVie 2, an end-to-end autoregressive framework, enhances controllability, visual quality, and temporal consistency in video world models through three progressive training stages.", "ai_keywords": ["autoregressive framework", "multi-modal guidance", "degradation-aware training", "history-context guidance", "video world models", "controllability", "visual quality", "temporal consistency", "LongVGenBench", "state-of-the-art performance", "temporal coherence", "visual fidelity"], "summary_zh": "<ul>\n    <li>\u6784\u5efa\u89c6\u9891\u4e16\u754c\u6a21\u578b\u662f\u5b9e\u73b0\u7a7a\u95f4\u65f6\u95f4\u667a\u80fd\u7684\u91cd\u8981\u6b65\u9aa4\u3002</li>\n    <li>\u4e16\u754c\u6a21\u578b\u9700\u5177\u5907\u53ef\u63a7\u6027\u3001\u957f\u671f\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e09\u4e2a\u57fa\u672c\u7279\u6027\u3002</li>\n    <li>LongVie 2 \u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u8fdb\u884c\u8bad\u7ec3\u3002</li>\n    <li>\u5f15\u5165\u4e86 LongVGenBench\uff0c\u4e00\u4e2a\u5305\u542b100\u4e2a\u9ad8\u5206\u8fa8\u7387\u4e00\u5206\u949f\u89c6\u9891\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e LongVie 2 \u5728\u53ef\u63a7\u6027\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongVie 2 is a new system designed to create better video world models for understanding and generating videos.</li>\n    <li>It focuses on three key features: control over the video content, high visual quality over time, and consistency in how things change in the video.</li>\n    <li>The system is developed in three stages to improve these features, starting with better control, then maintaining quality, and finally ensuring consistency across video clips.</li>\n    <li>LongVGenBench is a new benchmark with 100 high-quality videos from various environments to test and compare video generation systems.</li>\n    <li>LongVie 2 has shown to perform better than previous systems in controlling video, maintaining visual quality, and keeping things consistent over time.</li>\n</ul>"}, "publishedAt": "2025-12-15T12:59:58.000Z", "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model", "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13604.png", "numComments": 1, "submittedBy": {"_id": "643815c4961bb61e463c5896", "avatarUrl": "/avatars/3b44592472f16c56105bff8c314d9939.svg", "fullname": "Jianxiong Gao", "name": "Jianxiong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13168", "authors": [{"_id": "6940eedd65f1e24a11780673", "user": {"_id": "637b08057ce76c3b834da15d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PFmI1HNcZlHF0RoLjP3c2.png", "isPro": false, "fullname": "Haoyu Dong", "user": "HaoyuDong", "type": "user"}, "name": "Haoyu Dong", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:37:23.964Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780674", "user": {"_id": "684ae4a3a6d604475ef72f22", "avatarUrl": "/avatars/70e61614ab115f07361c0baec351255a.svg", "isPro": false, "fullname": "Pengkun Zhang", "user": "logicluo", "type": "user"}, "name": "Pengkun Zhang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T15:09:52.673Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780675", "user": {"_id": "69369fc780cb6886b08dcf5b", "avatarUrl": "/avatars/1159e73666a55706e551afd4518adf0b.svg", "isPro": false, "fullname": "Yan Gao", "user": "gaoyansdyt", "type": "user"}, "name": "Yan Gao", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:47:22.721Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780676", "user": {"_id": "69377b51e5af592ad600e1c5", "avatarUrl": "/avatars/72612a4a69da2bd565ed4777287cff86.svg", "isPro": false, "fullname": "Xuanyu Dong", "user": "KcNcooo", "type": "user"}, "name": "Xuanyu Dong", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T14:13:14.674Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780677", "user": {"_id": "6777bb751ee6b20009098ab6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6777bb751ee6b20009098ab6/OumKsg54DNDH8qkslg95x.jpeg", "isPro": false, "fullname": "Yilin Cheng", "user": "Yilin-Cheng24", "type": "user"}, "name": "Yilin Cheng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T10:18:24.145Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780678", "user": {"_id": "694135abb4ad5b6ca6c4c9fe", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/694135abb4ad5b6ca6c4c9fe/ZFl3u8k69DZpyErVLYRBk.jpeg", "isPro": false, "fullname": "Mingzhe Lu", "user": "metaphor2ml", "type": "user"}, "name": "Mingzhe Lu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:36.238Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780679", "user": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "isPro": false, "fullname": "Adina Yakefu", "user": "AdinaY", "type": "user"}, "name": "Adina Yakefu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T09:47:16.061Z", "hidden": false}, {"_id": "6940eedd65f1e24a1178067a", "name": "Shuxin Zheng", "hidden": false}], "publishedAt": "2025-12-15T10:28:45.000Z", "submittedOnDailyAt": "2025-12-16T09:53:57.731Z", "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows", "submittedOnDailyBy": {"_id": "637b08057ce76c3b834da15d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PFmI1HNcZlHF0RoLjP3c2.png", "isPro": false, "fullname": "Haoyu Dong", "user": "HaoyuDong", "type": "user"}, "summary": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.", "upvotes": 46, "discussionId": "6940eedd65f1e24a1178067b", "projectPage": "https://huggingface.co/datasets/FinWorkBench/Finch", "ai_summary": "Finch, a benchmark for AI agents in enterprise finance and accounting, evaluates performance across complex, real-world workflows using authentic data from Enron and other institutions.", "ai_keywords": ["LLM-assisted discovery", "expert annotation", "composite workflows", "spreadsheets", "PDFs", "AI systems", "GPT 5.1", "Claude Sonnet 4.5", "Gemini 3 Pro", "Grok 4", "Qwen 3 Max"], "organization": {"_id": "690e8a61e75888a0f707c0ee", "name": "FinWorkBench", "fullname": "Finch: Finance and Accounting Workflow Benchmark", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/637b08057ce76c3b834da15d/0HrDjcjNQK-5-FCZfkLzp.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u91d1\u878d\u4e0e\u4f1a\u8ba1\u57fa\u51c6\uff08Finch\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u590d\u6742\u7684\u4f01\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u8868\u73b0\u3002</li>\n    <li>Finch\u6765\u6e90\u4e8e\u771f\u5b9e\u4f01\u4e1a\u5de5\u4f5c\u7a7a\u95f4\u7684\u6570\u636e\uff0c\u5305\u62ecEnron\u7684150\u540d\u5458\u5de5\u768415000\u4e2a\u7535\u5b50\u8868\u683c\u548c50\u4e07\u5c01\u7535\u5b50\u90ae\u4ef6\uff0c\u6db5\u76d6\u4e86\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\u3002</li>\n    <li>\u6211\u4eec\u4f7f\u7528LLM\u8f85\u52a9\u53d1\u73b0\u548c\u4e13\u5bb6\u6ce8\u91ca\u7684\u65b9\u5f0f\u6784\u5efa\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7ecf\u8fc7700\u5c0f\u65f6\u7684\u4e13\u5bb6\u52aa\u529b\uff0c\u751f\u6210\u4e86172\u4e2a\u590d\u5408\u5de5\u4f5c\u6d41\u7a0b\u548c384\u4e2a\u4efb\u52a1\u3002</li>\n    <li>\u6211\u4eec\u5bf9\u591a\u4e2a\u524d\u6cbfAI\u7cfb\u7edf\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u53d1\u73b0GPT 5.1 Pro\u4ec5\u901a\u8fc738.4%\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u800cClaude Sonnet 4.5\u7684\u901a\u8fc7\u7387\u4ec5\u4e3a25.0%\u3002</li>\n    <li>\u6848\u4f8b\u7814\u7a76\u63ed\u793a\u4e86\u771f\u5b9e\u4f01\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u5bf9AI\u4ee3\u7406\u7684\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Finch is a new benchmark for testing AI agents on real business tasks like data entry, calculations, and reporting.</li>\n    <li>It uses real data from Enron and other companies, with a focus on messy, real-world information from emails and spreadsheets.</li>\n    <li>Experts spent over 700 hours creating and verifying 172 workflows made up of 384 tasks, incorporating data from 1,710 spreadsheets.</li>\n    <li>AI systems like GPT 5.1 and Claude Sonnet 4.5 were tested on these workflows, with GPT 5.1 passing only 38.4% of them.</li>\n    <li>The study highlights the difficulties AI faces in handling complex, real-world business processes.</li>\n</ul>"}, "publishedAt": "2025-12-15T05:28:45.000Z", "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows", "summary": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13168.png", "numComments": 1, "submittedBy": {"_id": "637b08057ce76c3b834da15d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PFmI1HNcZlHF0RoLjP3c2.png", "fullname": "Haoyu Dong", "name": "HaoyuDong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "organization": {"_id": "690e8a61e75888a0f707c0ee", "name": "FinWorkBench", "fullname": "Finch: Finance and Accounting Workflow Benchmark", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/637b08057ce76c3b834da15d/0HrDjcjNQK-5-FCZfkLzp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.09363", "authors": [{"_id": "693a379e74fced5bf9c32412", "user": {"_id": "6486ff6561053da6442fef1a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486ff6561053da6442fef1a/72sdWErAwWtWNJIV5VZsy.jpeg", "isPro": false, "fullname": "KeXing", "user": "KXingLab", "type": "user"}, "name": "Ke Xing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-11T10:13:26.656Z", "hidden": false}, {"_id": "693a379e74fced5bf9c32413", "name": "Longfei Li", "hidden": false}, {"_id": "693a379e74fced5bf9c32414", "user": {"_id": "64b7ab4c037d6452a31910eb", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7ab4c037d6452a31910eb/0UaBtwyQTysBMndFWZdKu.png", "isPro": false, "fullname": "yuyangyin", "user": "yuyangyin", "type": "user"}, "name": "Yuyang Yin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:43:30.256Z", "hidden": false}, {"_id": "693a379e74fced5bf9c32415", "name": "Hanwen Liang", "hidden": false}, {"_id": "693a379e74fced5bf9c32416", "name": "Guixun Luo", "hidden": false}, {"_id": "693a379e74fced5bf9c32417", "name": "Chen Fang", "hidden": false}, {"_id": "693a379e74fced5bf9c32418", "name": "Jue Wang", "hidden": false}, {"_id": "693a379e74fced5bf9c32419", "name": "Konstantinos N. Plataniotis", "hidden": false}, {"_id": "693a379e74fced5bf9c3241a", "name": "Xiaojie Jin", "hidden": false}, {"_id": "693a379e74fced5bf9c3241b", "name": "Yao Zhao", "hidden": false}, {"_id": "693a379e74fced5bf9c3241c", "name": "Yunchao Wei", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/dFNy8Tf5Ts5qNeWXCvkcB.mp4"], "publishedAt": "2025-12-10T06:50:16.000Z", "submittedOnDailyAt": "2025-12-11T00:46:52.612Z", "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.", "upvotes": 44, "discussionId": "693a379e74fced5bf9c3241d", "projectPage": "https://ke-xing.github.io/StereoWorld/", "ai_summary": "StereoWorld generates high-quality stereo video from monocular input using a pretrained video generator with geometry-aware regularization and spatio-temporal tiling.", "ai_keywords": ["stereo video", "monocular-to-stereo", "pretrained video generator", "geometry-aware regularization", "spatio-temporal tiling", "high-definition stereo video dataset", "natural human interpupillary distance (IPD)", "visual fidelity", "geometric consistency"], "summary_zh": "<ul>\n    <li>XR\u8bbe\u5907\u7684\u666e\u53ca\u589e\u52a0\u4e86\u5bf9\u9ad8\u8d28\u91cf\u7acb\u4f53\u89c6\u9891\u7684\u9700\u6c42\uff0c\u4f46\u751f\u4ea7\u6210\u672c\u9ad8\u4e14\u5bb9\u6613\u51fa\u73b0\u4f2a\u5f71\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86StereoWorld\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ece\u5355\u76ee\u89c6\u9891\u751f\u6210\u9ad8\u4fdd\u771f\u7acb\u4f53\u89c6\u9891\u7684\u6846\u67b6\u3002</li>\n    <li>\u8be5\u6846\u67b6\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u7684\u6b63\u5219\u5316\u6765\u786e\u4fdd3D\u7ed3\u6784\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u91c7\u7528\u65f6\u7a7a\u5207\u7247\u65b9\u6848\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u9ad8\u5206\u8fa8\u7387\u5408\u6210\u3002</li>\n    <li>\u901a\u8fc7\u521b\u5efa\u5305\u542b\u8d85\u8fc71100\u4e07\u5e27\u7684\u9ad8\u6e05\u7acb\u4f53\u89c6\u9891\u6570\u636e\u96c6\uff0cStereoWorld\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The use of XR devices is increasing, creating a need for high-quality stereo video, but making it is expensive and often results in errors.</li>\n    <li>StereoWorld is a new system that improves the process of converting regular video into stereo video using advanced techniques.</li>\n    <li>The system uses a pre-trained video generator and special methods to maintain accurate 3D structures in the videos.</li>\n    <li>It also implements a method to efficiently create high-resolution videos by working on parts of the video over time.</li>\n    <li>The project has created a large dataset with over 11 million frames to help train and test the system, showing significant improvements over previous methods.</li>\n</ul>"}, "publishedAt": "2025-12-10T01:50:16.000Z", "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation", "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/dFNy8Tf5Ts5qNeWXCvkcB.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09363.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "isAuthorParticipating": false}, {"paper": {"id": "2512.12730", "authors": [{"_id": "6940cff365f1e24a117803e5", "user": {"_id": "6737eb499ab5a7623aaf038a", "avatarUrl": "/avatars/dfe21c7db767a34f2aff3e80cebcfa22.svg", "isPro": false, "fullname": "Jingzhe Ding", "user": "JingzheDing", "type": "user"}, "name": "Jingzhe Ding", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T10:18:25.834Z", "hidden": false}, {"_id": "6940cff365f1e24a117803e6", "name": "Shengda Long", "hidden": false}, {"_id": "6940cff365f1e24a117803e7", "name": "Changxin Pu", "hidden": false}, {"_id": "6940cff365f1e24a117803e8", "name": "Huan Zhou", "hidden": false}, {"_id": "6940cff365f1e24a117803e9", "name": "Hongwan Gao", "hidden": false}, {"_id": "6940cff365f1e24a117803ea", "user": {"_id": "671068b8243baf4c58fc29c2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/leMkq-kw1icPo2bCLivGv.png", "isPro": false, "fullname": "Xiang Gao", "user": "coffiney", "type": "user"}, "name": "Xiang Gao", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:47:27.017Z", "hidden": false}, {"_id": "6940cff365f1e24a117803eb", "name": "Chao He", "hidden": false}, {"_id": "6940cff365f1e24a117803ec", "user": {"_id": "694100a19d8967e631cf94cf", "avatarUrl": "/avatars/1c785c0fcd171a62e206ef4074061a81.svg", "isPro": false, "fullname": "Yue Hou", "user": "YueHou", "type": "user"}, "name": "Yue Hou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:47:24.853Z", "hidden": false}, {"_id": "6940cff365f1e24a117803ed", "name": "Fei Hu", "hidden": false}, {"_id": "6940cff365f1e24a117803ee", "name": "Zhaojian Li", "hidden": false}, {"_id": "6940cff365f1e24a117803ef", "name": "Weiran Shi", "hidden": false}, {"_id": "6940cff365f1e24a117803f0", "name": "Zaiyuan Wang", "hidden": false}, {"_id": "6940cff365f1e24a117803f1", "name": "Daoguang Zan", "hidden": false}, {"_id": "6940cff365f1e24a117803f2", "name": "Chenchen Zhang", "hidden": false}, {"_id": "6940cff365f1e24a117803f3", "name": "Xiaoxu Zhang", "hidden": false}, {"_id": "6940cff365f1e24a117803f4", "name": "Qizhi Chen", "hidden": false}, {"_id": "6940cff365f1e24a117803f5", "name": "Xianfu Cheng", "hidden": false}, {"_id": "6940cff365f1e24a117803f6", "name": "Bo Deng", "hidden": false}, {"_id": "6940cff365f1e24a117803f7", "name": "Qingshui Gu", "hidden": false}, {"_id": "6940cff365f1e24a117803f8", "user": {"_id": "64e851825ddcace745ba15bd", "avatarUrl": "/avatars/7b6612c411222974d9ea181784eef915.svg", "isPro": false, "fullname": "Kai Hua", "user": "kkish", "type": "user"}, "name": "Kai Hua", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:47:28.890Z", "hidden": false}, {"_id": "6940cff365f1e24a117803f9", "name": "Juntao Lin", "hidden": false}, {"_id": "6940cff365f1e24a117803fa", "name": "Pai Liu", "hidden": false}, {"_id": "6940cff365f1e24a117803fb", "name": "Mingchen Li", "hidden": false}, {"_id": "6940cff365f1e24a117803fc", "name": "Xuanguang Pan", "hidden": false}, {"_id": "6940cff365f1e24a117803fd", "name": "Zifan Peng", "hidden": false}, {"_id": "6940cff365f1e24a117803fe", "name": "Yujia Qin", "hidden": false}, {"_id": "6940cff365f1e24a117803ff", "user": {"_id": "64b8cc7ebe76d2ff0703bfb3", "avatarUrl": "/avatars/f1c7ff17fd923f1460d362333d9fbfe3.svg", "isPro": false, "fullname": "yong", "user": "yo37", "type": "user"}, "name": "Yong Shan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:43.971Z", "hidden": false}, {"_id": "6940cff365f1e24a11780400", "name": "Zhewen Tan", "hidden": false}, {"_id": "6940cff365f1e24a11780401", "name": "Weihao Xie", "hidden": false}, {"_id": "6940cff365f1e24a11780402", "name": "Zihan Wang", "hidden": false}, {"_id": "6940cff365f1e24a11780403", "name": "Yishuo Yuan", "hidden": false}, {"_id": "6940cff365f1e24a11780404", "name": "Jiayu Zhang", "hidden": false}, {"_id": "6940cff365f1e24a11780405", "name": "Enduo Zhao", "hidden": false}, {"_id": "6940cff365f1e24a11780406", "name": "Yunfei Zhao", "hidden": false}, {"_id": "6940cff365f1e24a11780407", "name": "He Zhu", "hidden": false}, {"_id": "6940cff365f1e24a11780408", "name": "Chenyang Zou", "hidden": false}, {"_id": "6940cff365f1e24a11780409", "name": "Ming Ding", "hidden": false}, {"_id": "6940cff365f1e24a1178040a", "name": "Jianpeng Jiao", "hidden": false}, {"_id": "6940cff365f1e24a1178040b", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6940cff365f1e24a1178040c", "name": "Minghao Liu", "hidden": false}, {"_id": "6940cff365f1e24a1178040d", "name": "Qian Liu", "hidden": false}, {"_id": "6940cff365f1e24a1178040e", "user": {"_id": "648db8f92eb481bc8bc60469", "avatarUrl": "/avatars/7a640980ed225e31117f750e09bcc18c.svg", "isPro": false, "fullname": "Chongyang Tao", "user": "chongyang09", "type": "user"}, "name": "Chongyao Tao", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:47:30.496Z", "hidden": false}, {"_id": "6940cff365f1e24a1178040f", "name": "Jian Yang", "hidden": false}, {"_id": "6940cff365f1e24a11780410", "name": "Tong Yang", "hidden": false}, {"_id": "6940cff365f1e24a11780411", "name": "Zhaoxiang Zhang", "hidden": false}, {"_id": "6940cff365f1e24a11780412", "name": "Xinjie Chen", "hidden": false}, {"_id": "6940cff365f1e24a11780413", "name": "Wenhao Huang", "hidden": false}, {"_id": "6940cff365f1e24a11780414", "name": "Ge Zhang", "hidden": false}], "publishedAt": "2025-12-14T15:12:13.000Z", "submittedOnDailyAt": "2025-12-16T00:51:09.038Z", "title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents", "submittedOnDailyBy": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "summary": "Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.", "upvotes": 38, "discussionId": "6940cff365f1e24a11780415", "githubRepo": "https://github.com/multimodal-art-projection/NL2RepoBench", "githubRepoAddedBy": "user", "ai_summary": "NL2Repo Bench evaluates long-horizon software development capabilities of coding agents by assessing their ability to generate complete Python libraries from natural-language requirements.", "ai_keywords": ["NL2Repo Bench", "long-horizon repository generation", "autonomous software development", "coding agents", "natural-language requirements", "architecture design", "dependency management", "multi-module logic", "installable Python library", "test pass rates", "global coherence", "cross-file dependencies", "long-horizon reasoning"], "githubStars": 25, "summary_zh": "<ul>\n    <li>\u8fd1\u671f\u7684\u7f16\u7801\u4ee3\u7406\u6280\u672f\u8fdb\u5c55\u5feb\u901f\uff0c\u4f46\u73b0\u6709\u7684\u8bc4\u4f30\u6807\u51c6\u65e0\u6cd5\u6709\u6548\u6d4b\u8bd5\u5f00\u53d1\u5b8c\u6574\u8f6f\u4ef6\u7cfb\u7edf\u6240\u9700\u7684\u957f\u65f6\u95f4\u80fd\u529b\u3002</li>\n    <li>\u5927\u591a\u6570\u8bc4\u4f30\u96c6\u4e2d\u5728\u5c40\u90e8\u4ee3\u7801\u751f\u6210\u548c\u77ed\u671f\u4fee\u590d\u4efb\u52a1\u4e0a\uff0c\u672a\u80fd\u9a8c\u8bc1\u4ee3\u7406\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u6784\u5efa\u8f6f\u4ef6\u5e93\u7684\u6301\u7eed\u63a8\u7406\u548c\u6267\u884c\u80fd\u529b\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86NL2Repo Bench\u57fa\u51c6\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u7f16\u7801\u4ee3\u7406\u7684\u957f\u65f6\u95f4\u5e93\u751f\u6210\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u663e\u793a\uff0c\u5c3d\u7ba1\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u957f\u65f6\u95f4\u5e93\u751f\u6210\u4ecd\u7136\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u4ee3\u7406\u7684\u5e73\u5747\u6d4b\u8bd5\u901a\u8fc7\u7387\u4f4e\u4e8e40%\u3002</li>\n    <li>\u5206\u6790\u63ed\u793a\u4e86\u957f\u65f6\u95f4\u5931\u8d25\u7684\u6839\u672c\u539f\u56e0\uff0c\u5305\u62ec\u8fc7\u65e9\u7ec8\u6b62\u3001\u5168\u5c40\u4e00\u81f4\u6027\u4e27\u5931\u548c\u4e0d\u5145\u5206\u7684\u89c4\u5212\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Advancements in coding agents show potential for creating software automatically, but current tests don't adequately measure their ability to handle long-term tasks.</li>\n    <li>Most existing tests focus on short coding tasks, not on building complete software systems, leaving a gap in evaluation.</li>\n    <li>The new benchmark, NL2Repo Bench, is designed to assess coding agents' ability to generate full software repositories from a single requirements document.</li>\n    <li>Tests reveal that even the best coding agents struggle with long-term tasks, achieving less than 40% success in completing software correctly.</li>\n    <li>Challenges identified include early task completion, lack of coherence, fragile dependencies, and poor long-term planning.</li>\n</ul>"}, "publishedAt": "2025-12-14T10:12:13.000Z", "title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents", "summary": "Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12730.png", "numComments": 1, "submittedBy": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "fullname": "Ge Zhang", "name": "zhangysk", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 74}, "isAuthorParticipating": false}, {"paper": {"id": "2512.11558", "authors": [{"_id": "693f7554f516c693246811d3", "name": "Zhenyang Cai", "hidden": false}, {"_id": "693f7554f516c693246811d4", "name": "Jiaming Zhang", "hidden": false}, {"_id": "693f7554f516c693246811d5", "name": "Junjie Zhao", "hidden": false}, {"_id": "693f7554f516c693246811d6", "user": {"_id": "660244de381ff94818335b67", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660244de381ff94818335b67/Vwt4S739tpURyqgP3IDgd.jpeg", "isPro": false, "fullname": "ZiyiZENG", "user": "CocoNutZENG", "type": "user"}, "name": "Ziyi Zeng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-15T08:08:39.120Z", "hidden": false}, {"_id": "693f7554f516c693246811d7", "name": "Yanchao Li", "hidden": false}, {"_id": "693f7554f516c693246811d8", "name": "Jingyi Liang", "hidden": false}, {"_id": "693f7554f516c693246811d9", "name": "Junying Chen", "hidden": false}, {"_id": "693f7554f516c693246811da", "name": "Yunjin Yang", "hidden": false}, {"_id": "693f7554f516c693246811db", "name": "Jiajun You", "hidden": false}, {"_id": "693f7554f516c693246811dc", "name": "Shuzhi Deng", "hidden": false}, {"_id": "693f7554f516c693246811dd", "name": "Tongfei Wang", "hidden": false}, {"_id": "693f7554f516c693246811de", "name": "Wanting Chen", "hidden": false}, {"_id": "693f7554f516c693246811df", "name": "Chunxiu Hao", "hidden": false}, {"_id": "693f7554f516c693246811e0", "name": "Ruiqi Xie", "hidden": false}, {"_id": "693f7554f516c693246811e1", "name": "Zhenwei Wen", "hidden": false}, {"_id": "693f7554f516c693246811e2", "name": "Xiangyi Feng", "hidden": false}, {"_id": "693f7554f516c693246811e3", "name": "Zou Ting", "hidden": false}, {"_id": "693f7554f516c693246811e4", "name": "Jin Zou Lin", "hidden": false}, {"_id": "693f7554f516c693246811e5", "name": "Jianquan Li", "hidden": false}, {"_id": "693f7554f516c693246811e6", "name": "Guangjun Yu", "hidden": false}, {"_id": "693f7554f516c693246811e7", "name": "Liangyi Chen", "hidden": false}, {"_id": "693f7554f516c693246811e8", "name": "Junwen Wang", "hidden": false}, {"_id": "693f7554f516c693246811e9", "name": "Shan Jiang", "hidden": false}, {"_id": "693f7554f516c693246811ea", "name": "Benyou Wang", "hidden": false}], "publishedAt": "2025-12-12T13:42:57.000Z", "submittedOnDailyAt": "2025-12-15T00:14:07.445Z", "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry", "submittedOnDailyBy": {"_id": "64f1a34f2c5c8b767916447e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1a34f2c5c8b767916447e/uak2CsMAnxW8q4dwyAOBN.jpeg", "isPro": false, "fullname": "Zhenyang Cai", "user": "Eric3200", "type": "user"}, "summary": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.", "upvotes": 37, "discussionId": "693f7555f516c693246811eb", "ai_summary": "DentalGPT, a specialized dental multimodal large language model, achieves superior performance in disease classification and dental VQA tasks through high-quality domain knowledge injection and reinforcement learning.", "ai_keywords": ["multimodal large language models", "dentalGPT", "domain knowledge injection", "reinforcement learning", "annotated multimodal dataset", "dental images", "visual understanding", "multimodal complex reasoning", "intraoral benchmarks", "panoramic benchmarks", "medical VQA benchmarks", "disease classification", "dental VQA", "parameters"], "summary_zh": "<ul>\n    <li>\u5f00\u53d1\u4e86DentalGPT\uff0c\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u7259\u79d1\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u3002</li>\n    <li>\u6784\u5efa\u4e86\u76ee\u524d\u6700\u5927\u7684\u7259\u79d1\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u62ec\u8d85\u8fc712\u4e07\u5f20\u7259\u79d1\u56fe\u50cf\u53ca\u8be6\u7ec6\u63cf\u8ff0\u3002</li>\n    <li>\u901a\u8fc7\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5bf9\u7259\u79d1\u6761\u4ef6\u7684\u89c6\u89c9\u7406\u89e3\u548c\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5728\u75c5\u75c7\u5206\u7c7b\u548c\u7259\u79d1\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0cDentalGPT\u8868\u73b0\u4f18\u4e8e\u8bb8\u591a\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u6a21\u578b\u3002</li>\n    <li>\u7ed3\u679c\u8868\u660e\uff0c\u4f18\u8d28\u7684\u7259\u79d1\u6570\u636e\u7ed3\u5408\u5206\u9636\u6bb5\u7684\u9002\u5e94\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u6784\u5efa\u4e13\u4e1a\u7684\u7259\u79d1MLLM\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DentalGPT is a new AI model designed to improve automated dental healthcare by better understanding dental images and details.</li>\n    <li>It was developed using a large dataset of over 120,000 dental images and their descriptions, making it the biggest dental dataset so far.</li>\n    <li>The model improves its ability to diagnose dental issues through a special learning process that enhances its reasoning skills.</li>\n    <li>Tests show that DentalGPT performs better than many other advanced models in classifying dental diseases and answering dental questions.</li>\n    <li>The success of DentalGPT highlights the importance of using high-quality dental data for creating specialized AI models in dentistry.</li>\n</ul>"}, "publishedAt": "2025-12-12T08:42:57.000Z", "title": "DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry", "summary": "Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.11558.png", "numComments": 2, "submittedBy": {"_id": "64f1a34f2c5c8b767916447e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1a34f2c5c8b767916447e/uak2CsMAnxW8q4dwyAOBN.jpeg", "fullname": "Zhenyang Cai", "name": "Eric3200", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2511.18538", "authors": [{"_id": "692e667137312eaa83fd8832", "user": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "name": "Jian Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:34.886Z", "hidden": false}, {"_id": "692e667137312eaa83fd8833", "name": "Xianglong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8834", "name": "Weifeng Lv", "hidden": false}, {"_id": "692e667137312eaa83fd8835", "name": "Ken Deng", "hidden": false}, {"_id": "692e667137312eaa83fd8836", "name": "Shawn Guo", "hidden": false}, {"_id": "692e667137312eaa83fd8837", "name": "Lin Jing", "hidden": false}, {"_id": "692e667137312eaa83fd8838", "name": "Yizhi Li", "hidden": false}, {"_id": "692e667137312eaa83fd8839", "name": "Shark Liu", "hidden": false}, {"_id": "692e667137312eaa83fd883a", "name": "Xianzhen Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883b", "name": "Yuyu Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883c", "name": "Changzai Pan", "hidden": false}, {"_id": "692e667137312eaa83fd883d", "name": "Ensheng Shi", "hidden": false}, {"_id": "692e667137312eaa83fd883e", "name": "Yingshui Tan", "hidden": false}, {"_id": "692e667137312eaa83fd883f", "name": "Renshuai Tao", "hidden": false}, {"_id": "692e667137312eaa83fd8840", "user": {"_id": "66a8e2538407031e388c501f", "avatarUrl": "/avatars/d16d51f7b1e111efd6d0985995b614be.svg", "isPro": false, "fullname": "wjj", "user": "wuyuverse", "type": "user"}, "name": "Jiajun Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:36.195Z", "hidden": false}, {"_id": "692e667137312eaa83fd8841", "name": "Xianjie Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8842", "name": "Zhenhe Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8843", "name": "Daoguang Zan", "hidden": false}, {"_id": "692e667137312eaa83fd8844", "name": "Chenchen Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8845", "user": {"_id": "672c9ba69380700b602c46c1", "avatarUrl": "/avatars/3d0fd966df540d34095d2c84ce449180.svg", "isPro": false, "fullname": "wei zhang", "user": "zwpride", "type": "user"}, "name": "Wei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:37.970Z", "hidden": false}, {"_id": "692e667137312eaa83fd8846", "name": "He Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd8847", "user": {"_id": "62b7fb545233925f253531c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg", "isPro": false, "fullname": "Terry Yue Zhuo", "user": "terryyz", "type": "user"}, "name": "Terry Yue Zhuo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T16:50:22.285Z", "hidden": false}, {"_id": "692e667137312eaa83fd8848", "name": "Kerui Cao", "hidden": false}, {"_id": "692e667137312eaa83fd8849", "name": "Xianfu Cheng", "hidden": false}, {"_id": "692e667137312eaa83fd884a", "name": "Jun Dong", "hidden": false}, {"_id": "692e667137312eaa83fd884b", "name": "Shengjie Fang", "hidden": false}, {"_id": "692e667137312eaa83fd884c", "name": "Zhiwei Fei", "hidden": false}, {"_id": "692e667137312eaa83fd884d", "name": "Xiangyuan Guan", "hidden": false}, {"_id": "692e667137312eaa83fd884e", "name": "Qipeng Guo", "hidden": false}, {"_id": "692e667137312eaa83fd884f", "name": "Zhiguang Han", "hidden": false}, {"_id": "692e667137312eaa83fd8850", "name": "Joseph James", "hidden": false}, {"_id": "692e667137312eaa83fd8851", "name": "Tianqi Luo", "hidden": false}, {"_id": "692e667137312eaa83fd8852", "user": {"_id": "67f1037cd5f976f3d4777390", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0cXH40AcE-M-H21cSNBqZ.png", "isPro": false, "fullname": "RenyuanLi", "user": "RenyuanLi", "type": "user"}, "name": "Renyuan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:17:45.344Z", "hidden": false}, {"_id": "692e667137312eaa83fd8853", "name": "Yuhang Li", "hidden": false}, {"_id": "692e667137312eaa83fd8854", "name": "Yiming Liang", "hidden": false}, {"_id": "692e667137312eaa83fd8855", "name": "Congnan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8856", "name": "Jiaheng Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8857", "name": "Qian Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8858", "name": "Ruitong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8859", "name": "Tyler Loakman", "hidden": false}, {"_id": "692e667137312eaa83fd885a", "name": "Xiangxin Meng", "hidden": false}, {"_id": "692e667137312eaa83fd885b", "name": "Chuang Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885c", "name": "Tianhao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885d", "name": "Jiajun Shi", "hidden": false}, {"_id": "692e667137312eaa83fd885e", "name": "Mingjie Tang", "hidden": false}, {"_id": "692e667137312eaa83fd885f", "name": "Boyang Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8860", "name": "Haowen Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8861", "name": "Yunli Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8862", "user": {"_id": "668619ce7374cac565759731", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668619ce7374cac565759731/tUtiyIQRGsMdq3HB2yYIL.jpeg", "isPro": false, "fullname": "Fanglin Xu", "user": "Tswatery", "type": "user"}, "name": "Fanglin Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T15:15:03.333Z", "hidden": false}, {"_id": "692e667137312eaa83fd8863", "name": "Zihan Xu", "hidden": false}, {"_id": "692e667137312eaa83fd8864", "name": "Fei Yuan", "hidden": false}, {"_id": "692e667137312eaa83fd8865", "user": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "name": "Ge Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:34.025Z", "hidden": false}, {"_id": "692e667137312eaa83fd8866", "user": {"_id": "65f40e83653c231cbaf7defe", "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg", "isPro": false, "fullname": "Jiayi Zhang", "user": "didiforhugface", "type": "user"}, "name": "Jiayi Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:32.149Z", "hidden": false}, {"_id": "692e667137312eaa83fd8867", "name": "Xinhao Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8868", "name": "Wangchunshu Zhou", "hidden": false}, {"_id": "692e667137312eaa83fd8869", "name": "Hualei Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886a", "name": "King Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886b", "name": "Brown Dai", "hidden": false}, {"_id": "692e667137312eaa83fd886c", "name": "Aishan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd886d", "name": "Zhoujun Li", "hidden": false}, {"_id": "692e667137312eaa83fd886e", "name": "Chenghua Lin", "hidden": false}, {"_id": "692e667137312eaa83fd886f", "name": "Tianyu Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8870", "name": "Chao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd8871", "name": "Kai Shen", "hidden": false}, {"_id": "692e667137312eaa83fd8872", "name": "Libo Qin", "hidden": false}, {"_id": "692e667137312eaa83fd8873", "name": "Shuangyong Song", "hidden": false}, {"_id": "692e667137312eaa83fd8874", "name": "Zizheng Zhan", "hidden": false}, {"_id": "692e667137312eaa83fd8875", "name": "Jiajun Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8876", "name": "Jie Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8877", "name": "Zhaoxiang Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8878", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-11-23T17:09:34.000Z", "submittedOnDailyAt": "2025-12-02T02:55:07.234Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "submittedOnDailyBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "upvotes": 240, "discussionId": "692e667237312eaa83fd8879", "ai_summary": "A comprehensive guide to code LLMs, covering their lifecycle from data curation to deployment, including techniques, trade-offs, and research-practice gaps.", "ai_keywords": ["Transformer-based architectures", "HumanEval", "prompting paradigms", "code pre-training", "supervised fine-tuning", "reinforcement learning", "autonomous coding agents", "GPT-4", "Claude", "LLaMA", "StarCoder", "Code LLaMA", "DeepSeek-Coder", "QwenCoder", "code correctness", "security", "contextual awareness", "software-related code tasks", "scaling law", "framework selection", "hyperparameter sensitivity", "model architectures", "dataset comparisons"], "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u76f4\u63a5\u8f6c\u6362\u4e3a\u529f\u80fd\u4ee3\u7801\uff0c\u6539\u53d8\u4e86\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u3002</li>\n    <li>\u8be5\u9886\u57df\u4ece\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\u53d1\u5c55\u5230\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002</li>\n    <li>\u672c\u6587\u63d0\u4f9b\u4e86\u5173\u4e8e\u4ee3\u7801LLMs\u7684\u5168\u9762\u603b\u7ed3\u548c\u5b9e\u7528\u6307\u5357\uff0c\u6db5\u76d6\u6a21\u578b\u751f\u547d\u5468\u671f\u7684\u5404\u4e2a\u9636\u6bb5\u3002</li>\n    <li>\u5206\u6790\u4e86\u901a\u7528LLMs\u548c\u4e13\u95e8\u5316\u4ee3\u7801LLMs\u7684\u80fd\u529b\uff0c\u5e76\u63a2\u8ba8\u4e86\u6280\u672f\u548c\u8bbe\u8ba1\u51b3\u7b56\u3002</li>\n    <li>\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5206\u6790\u4e86\u4ee3\u7801\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6548\u679c\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u6a21\u578b\u548c\u6570\u636e\u96c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) have changed software development by turning natural language descriptions into working code.</li>\n    <li>Tools like GitHub Copilot and others show how these models are becoming widely used.</li>\n    <li>This work provides a detailed guide on how code LLMs work, covering everything from data collection to training and usage.</li>\n    <li>The research looks at both general-purpose LLMs and those specialized for coding, comparing their strengths and weaknesses.</li>\n    <li>It also discusses the gap between academic research and real-world coding tasks, suggesting ways to improve practical applications.</li>\n</ul>"}, "publishedAt": "2025-11-23T12:09:34.000Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18538.png", "numComments": 11, "submittedBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "fullname": "Yang Jian", "name": "CSJianYang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 21}, "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.14993", "authors": [{"_id": "691e819a3c64d32b036458c0", "name": "Vladimir Arkhipkin", "hidden": false}, {"_id": "691e819a3c64d32b036458c1", "user": {"_id": "67bcb1012906865678a11f91", "avatarUrl": "/avatars/80fb0cc24f0d16c4740f9115b680df0f.svg", "isPro": false, "fullname": "Vladimir Korviakov", "user": "korviakov", "type": "user"}, "name": "Vladimir Korviakov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:03.925Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c2", "user": {"_id": "63cfa7ef3b7adfa99c0eb524", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674553277288-noauth.jpeg", "isPro": false, "fullname": "Nikolai Gerasimenko", "user": "nikgerasimenko", "type": "user"}, "name": "Nikolai Gerasimenko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:55.225Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c3", "name": "Denis Parkhomenko", "hidden": false}, {"_id": "691e819a3c64d32b036458c4", "user": {"_id": "64e4c7764af6c29a0697f57b", "avatarUrl": "/avatars/efc4e9f9b105586fd090b22a1bc7dbb7.svg", "isPro": false, "fullname": "Viacheslav Vasilev", "user": "vvasilev", "type": "user"}, "name": "Viacheslav Vasilev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:10.246Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c5", "user": {"_id": "68838d809080cc7010edf5e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68838d809080cc7010edf5e2/xBqg5ggt_PfLkiDLmsZxx.jpeg", "isPro": false, "fullname": "Alexey Letunovskiy", "user": "AlexeyLetunovskiy", "type": "user"}, "name": "Alexey Letunovskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:55.594Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c6", "user": {"_id": "678781c9e3c3c0163db4f99c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5Vi5J_XS9fbN2gDHfzHlh.png", "isPro": false, "fullname": "Kovaleva Maria", "user": "makovka2000", "type": "user"}, "name": "Maria Kovaleva", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T10:15:36.018Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c7", "user": {"_id": "67f38b14da604b256d393662", "avatarUrl": "/avatars/63445143f68995becc7702868387555b.svg", "isPro": false, "fullname": "Nikolay Vaulin", "user": "nvvaulin", "type": "user"}, "name": "Nikolai Vaulin", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:01.695Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c8", "user": {"_id": "62653f745f6f2e14d6ae128c", "avatarUrl": "/avatars/944b564ab810a5b31fa5e45f63bdf4ee.svg", "isPro": false, "fullname": "Ivan Kirillov", "user": "funnylittleman", "type": "user"}, "name": "Ivan Kirillov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T20:40:14.372Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c9", "user": {"_id": "60991602f7c9c7bf29603a88", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60991602f7c9c7bf29603a88/me8VFG_06ZOovTLldF-L7.jpeg", "isPro": false, "fullname": "Lev Novitskiy", "user": "leffff", "type": "user"}, "name": "Lev Novitskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:01:59.489Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ca", "name": "Denis Koposov", "hidden": false}, {"_id": "691e819a3c64d32b036458cb", "user": {"_id": "6628b73c35d27082500034f2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628b73c35d27082500034f2/CznOeIbjzJ9DmJaGzlWPD.jpeg", "isPro": false, "fullname": "Nikita Kiselev", "user": "kisnikser", "type": "user"}, "name": "Nikita Kiselev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:11.927Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cc", "user": {"_id": "654d4993938fbf1e695b589a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qY3MC94Uz3FGf_HQtHseK.png", "isPro": false, "fullname": "Varlamov Alexander", "user": "Alphonsce", "type": "user"}, "name": "Alexander Varlamov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:08.889Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cd", "user": {"_id": "6616719945336ca7746eaa38", "avatarUrl": "/avatars/ac77ebda8507d75376973144263beb83.svg", "isPro": false, "fullname": "Dmitrii Mikhailov", "user": "Botsman11", "type": "user"}, "name": "Dmitrii Mikhailov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:56.980Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ce", "name": "Vladimir Polovnikov", "hidden": false}, {"_id": "691e819a3c64d32b036458cf", "name": "Andrey Shutkin", "hidden": false}, {"_id": "691e819a3c64d32b036458d0", "name": "Ilya Vasiliev", "hidden": false}, {"_id": "691e819a3c64d32b036458d1", "name": "Julia Agafonova", "hidden": false}, {"_id": "691e819a3c64d32b036458d2", "name": "Anastasiia Kargapoltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d3", "user": {"_id": "65df46ac43bf08064bd8e656", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65df46ac43bf08064bd8e656/yR72X3fnBhdy_i34VqBxT.jpeg", "isPro": false, "fullname": "Anna Dmitrienko", "user": "dmitrienkoae", "type": "user"}, "name": "Anna Dmitrienko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:09.131Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d4", "name": "Anastasia Maltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d5", "user": {"_id": "66f1a9c87ce3d2d3938999ce", "avatarUrl": "/avatars/3016b15d4bae2591313537a4ea59b268.svg", "isPro": false, "fullname": "Anna Averchenkova", "user": "aaveraa", "type": "user"}, "name": "Anna Averchenkova", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:11.123Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d6", "name": "Olga Kim", "hidden": false}, {"_id": "691e819a3c64d32b036458d7", "name": "Tatiana Nikulina", "hidden": false}, {"_id": "691e819a3c64d32b036458d8", "user": {"_id": "6669a678465d1d802181e456", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6669a678465d1d802181e456/ZCthBBhDFQnh0bBkgUQUU.png", "isPro": false, "fullname": "Denis Dimitrov", "user": "dendimitrov", "type": "user"}, "name": "Denis Dimitrov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:08.661Z", "hidden": false}], "publishedAt": "2025-11-19T00:23:22.000Z", "submittedOnDailyAt": "2025-11-20T00:19:10.078Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "upvotes": 209, "discussionId": "691e819b3c64d32b036458d9", "projectPage": "https://kandinskylab.ai/", "githubRepo": "https://github.com/kandinskylab/kandinsky-5", "ai_summary": "Kandinsky 5.0 is a family of state-of-the-art generative models for high-resolution images and short videos, featuring model lineups with varying parameters and enhanced training techniques to achieve superior quality and performance.", "ai_keywords": ["foundation models", "high-resolution image synthesis", "10-second video synthesis", "image generation models", "text-to-video models", "image-to-video models", "multi-stage training pipeline", "self-supervised fine-tuning", "reinforcement learning", "pre-training", "quality-enhancement techniques", "architectural optimizations", "training optimizations", "inference optimizations", "human evaluation", "generative framework", "open-source code", "training checkpoints"], "githubStars": 477, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecdKandinsky 5.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c10\u79d2\u89c6\u9891\u5408\u6210\u7684\u5148\u8fdb\u57fa\u7840\u6a21\u578b\u7cfb\u5217\u3002</li>\n    <li>Kandinsky 5.0\u5305\u62ec\u4e09\u79cd\u6838\u5fc3\u6a21\u578b\uff1a6B\u53c2\u6570\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u30012B\u53c2\u6570\u7684\u5feb\u901f\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u548c19B\u53c2\u6570\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002</li>\n    <li>\u8be6\u7ec6\u8bf4\u660e\u4e86\u6570\u636e\u6574\u7406\u751f\u547d\u5468\u671f\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u5904\u7406\u3001\u8fc7\u6ee4\u548c\u805a\u7c7b\uff0c\u4ee5\u652f\u6301\u591a\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>\u5c55\u793a\u4e86\u65b0\u9896\u7684\u67b6\u6784\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u4f18\u5316\uff0c\u4f7fKandinsky 5.0\u5728\u751f\u6210\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002</li>\n    <li>\u5e0c\u671b\u901a\u8fc7\u53d1\u5e03\u5f00\u6e90\u4ee3\u7801\u548c\u8bad\u7ec3\u68c0\u67e5\u70b9\uff0c\u63a8\u52a8\u9ad8\u8d28\u91cf\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u548c\u4fbf\u5229\u6027\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Kandinsky 5.0 is a new set of advanced models for creating high-quality images and short videos.</li>\n    <li>It includes three main models: Kandinsky 5.0 Image Lite for image generation, Kandinsky 5.0 Video Lite for quick video creation, and Kandinsky 5.0 Video Pro for top-quality video generation.</li>\n    <li>The report details the process of gathering and preparing data for training these models, including various techniques to improve quality.</li>\n    <li>Kandinsky 5.0 features improvements in its design and training methods, allowing it to generate high-quality content quickly.</li>\n    <li>The models are open-source, making them accessible for researchers and developers to use in different creative projects.</li>\n</ul>"}, "publishedAt": "2025-11-18T19:23:22.000Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.14993.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "isAuthorParticipating": true}, {"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51faDeepSeek-V3.2\u6a21\u578b\uff0c\u517c\u5177\u9ad8\u6548\u8ba1\u7b97\u548c\u5353\u8d8a\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5f15\u5165\u6df1\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\uff08DSA\uff09\u673a\u5236\uff0c\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u957f\u6587\u672c\u573a\u666f\u4e2d\u7684\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7fDeepSeek-V3.2\u7684\u8868\u73b0\u4e0eGPT-5\u76f8\u5f53\uff0c\u7279\u6b8a\u7248\u672c\u751a\u81f3\u8d85\u8d8aGPT-5\u3002</li>\n    <li>\u5f00\u53d1\u5927\u89c4\u6a21\u667a\u80fd\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff0c\u7cfb\u7edf\u6027\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u63a8\u7406\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002</li>\n    <li>\u57282025\u5e74\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\uff08IMO\uff09\u548c\u56fd\u9645\u4fe1\u606f\u5b66\u5965\u6797\u5339\u514b\uff08IOI\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u83b7\u5f97\u91d1\u724c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that combines high efficiency with strong reasoning and performance for agents.</li>\n    <li>It features DeepSeek Sparse Attention (DSA), which makes the model faster and more efficient, especially with long contexts.</li>\n    <li>The model uses a scalable reinforcement learning framework that allows it to perform as well as GPT-5, and its special version outperforms GPT-5 and matches Gemini-3.0-Pro in reasoning.</li>\n    <li>DeepSeek-V3.2 achieved top results in prestigious competitions like the 2025 International Mathematical Olympiad and the International Olympiad in Informatics.</li>\n    <li>It includes a new data generation pipeline to improve reasoning in tasks that require using tools, enhancing its ability to adapt and follow instructions in complex situations.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.22699", "authors": [{"_id": "692d06234397b1ec214f6788", "name": "Z-Image Team", "hidden": false}, {"_id": "692d06234397b1ec214f6789", "user": {"_id": "692d0e6bb14ceb758205d0dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/692d0e6bb14ceb758205d0dd/gGVq2KSJE11Sr3LkVn-n5.jpeg", "isPro": false, "fullname": "Huanqia Cai", "user": "Orion-Cai", "type": "user"}, "name": "Huanqia Cai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:26.669Z", "hidden": false}, {"_id": "692d06234397b1ec214f678a", "user": {"_id": "67777b7a8376dfe003afa951", "avatarUrl": "/avatars/2af9d3181306d4c53329d047eeadaf1e.svg", "isPro": false, "fullname": "Sihan Cao", "user": "Sihan-Cao", "type": "user"}, "name": "Sihan Cao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:33.191Z", "hidden": false}, {"_id": "692d06234397b1ec214f678b", "user": {"_id": "64a54586c0f13de8e7093314", "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg", "isPro": false, "fullname": "Ruoyi Du", "user": "RuoyiDu", "type": "user"}, "name": "Ruoyi Du", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:18:53.948Z", "hidden": false}, {"_id": "692d06234397b1ec214f678c", "name": "Peng Gao", "hidden": false}, {"_id": "692d06234397b1ec214f678d", "name": "Steven Hoi", "hidden": false}, {"_id": "692d06234397b1ec214f678e", "name": "Shijie Huang", "hidden": false}, {"_id": "692d06234397b1ec214f678f", "name": "Zhaohui Hou", "hidden": false}, {"_id": "692d06234397b1ec214f6790", "user": {"_id": "662a0f2d4bab737c1a279843", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662a0f2d4bab737c1a279843/fC2p3mjMHkVpDQdEqkuR4.png", "isPro": false, "fullname": "Dengyang Jiang", "user": "DyJiang", "type": "user"}, "name": "Dengyang Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:15.555Z", "hidden": false}, {"_id": "692d06234397b1ec214f6791", "user": {"_id": "6537e8eab01250d1d6efed3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gMx73gwdfEhcCFioStGCE.jpeg", "isPro": false, "fullname": "Xin", "user": "Srameo", "type": "user"}, "name": "Xin Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:15.288Z", "hidden": false}, {"_id": "692d06234397b1ec214f6792", "name": "Liangchen Li", "hidden": false}, {"_id": "692d06234397b1ec214f6793", "user": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "name": "Zhen Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:16.899Z", "hidden": false}, {"_id": "692d06234397b1ec214f6794", "user": {"_id": "6740a5730bb4a675446a80ad", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740a5730bb4a675446a80ad/dmruwMdQK3zluJm7YXUtN.jpeg", "isPro": false, "fullname": "Zhong-Yu Li", "user": "lzyhha", "type": "user"}, "name": "Zhong-Yu Li", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:08.972Z", "hidden": false}, {"_id": "692d06234397b1ec214f6795", "name": "David Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6796", "name": "Dongyang Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6797", "user": {"_id": "66332475351231c428653b6b", "avatarUrl": "/avatars/3997bcde54158f7ff9770c85a20875f1.svg", "isPro": false, "fullname": "Junhan Shi", "user": "jshmsjh", "type": "user"}, "name": "Junhan Shi", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:38.865Z", "hidden": false}, {"_id": "692d06234397b1ec214f6798", "user": {"_id": "64379d79fac5ea753f1c10f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png", "isPro": false, "fullname": "Jerry Wu", "user": "QJerry", "type": "user"}, "name": "Qilong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:18.709Z", "hidden": false}, {"_id": "692d06234397b1ec214f6799", "name": "Feng Yu", "hidden": false}, {"_id": "692d06234397b1ec214f679a", "name": "Chi Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679b", "name": "Shifeng Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679c", "user": {"_id": "641988978e0baaeed5a066c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641988978e0baaeed5a066c6/TdCjJ63gw5gdX1RqTvy9a.png", "isPro": false, "fullname": "Shilin", "user": "zsLin", "type": "user"}, "name": "Shilin Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T16:24:44.624Z", "hidden": false}], "publishedAt": "2025-11-27T18:52:07.000Z", "submittedOnDailyAt": "2025-12-01T00:38:17.269Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "submittedOnDailyBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "upvotes": 155, "discussionId": "692d06234397b1ec214f679d", "projectPage": "https://tongyi-mai.github.io/Z-Image-blog/", "githubRepo": "https://github.com/Tongyi-MAI/Z-Image", "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.", "ai_keywords": ["Scalable Single-Stream Diffusion Transformer", "S3-DiT", "diffusion transformer", "omni-pre-training", "instruction-following capabilities", "photorealistic image generation", "bilingual text rendering", "distillation scheme", "reward post-training", "H800 GPU", "VRAM"], "githubStars": 5595, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "summary_zh": "<ul>\n    <li>\u5f53\u524d\u9ad8\u6027\u80fd\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e3b\u8981\u7531\u5546\u4e1a\u7cfb\u7edf\u4e3b\u5bfc\uff0c\u5982Nano Banana Pro\u548cSeedream 4.0\u3002</li>\n    <li>\u4e00\u4e9b\u9886\u5148\u7684\u5f00\u6e90\u66ff\u4ee3\u54c1\u53c2\u6570\u91cf\u5de8\u5927\uff0820B\u523080B\uff09\uff0c\u5728\u666e\u901a\u786c\u4ef6\u4e0a\u96be\u4ee5\u4f7f\u7528\u548c\u5fae\u8c03\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Z-Image\uff0c\u4e00\u4e2a\u9ad8\u6548\u76846B\u53c2\u6570\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5355\u6d41\u6269\u6563\u53d8\u538b\u5668\u67b6\u6784\u3002</li>\n    <li>Z-Image\u7684\u8bad\u7ec3\u5de5\u4f5c\u6d41\u7a0b\u4ec5\u9700314K H800 GPU\u5c0f\u65f6\uff0c\u6210\u672c\u7ea6\u4e3a630K\u7f8e\u5143\uff0c\u4e14\u652f\u6301\u6d88\u8d39\u8005\u7ea7\u786c\u4ef6\u3002</li>\n    <li>Z-Image\u5728\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u56fe\u50cf\u751f\u6210\u548c\u53cc\u8bed\u6587\u672c\u6e32\u67d3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u4e0e\u9876\u7ea7\u5546\u4e1a\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u597d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Most high-performance image generation models are proprietary, while open-source options have very large parameter counts, making them hard to use on regular hardware.</li>\n    <li>Z-Image is a new 6B-parameter model that uses a unique architecture, aiming to provide high-quality results without needing massive resources.</li>\n    <li>The training process for Z-Image is efficient, completing in about 314K GPU hours, which is significantly less costly compared to other models.</li>\n    <li>Z-Image can generate images quickly and works on consumer-grade hardware, making it more accessible for users.</li>\n    <li>The model performs very well in generating realistic images and handling bilingual text, showing that high-quality results can be achieved with less computational power.</li>\n</ul>"}, "publishedAt": "2025-11-27T13:52:07.000Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png", "numComments": 3, "submittedBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "fullname": "Zhen Li", "name": "Paper99", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 29}, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20626", "authors": [{"_id": "6927ab26243b2216fb75cd1b", "name": "Wei He", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1c", "user": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "name": "Kai Han", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:11.052Z", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1d", "name": "Hang Zhou", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1e", "name": "Hanting Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1f", "name": "Zhicheng Liu", "hidden": false}, {"_id": "6927ab26243b2216fb75cd20", "name": "Xinghao Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd21", "name": "Yunhe Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "publishedAt": "2025-11-25T18:48:05.000Z", "submittedOnDailyAt": "2025-11-26T23:08:13.066Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "submittedOnDailyBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "upvotes": 154, "discussionId": "6927ab27243b2216fb75cd22", "projectPage": "https://github.com/huawei-noah/noah-research/tree/master/ROOT", "githubRepo": "https://github.com/huawei-noah/noah-research", "ai_summary": "ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.", "ai_keywords": ["large language models", "LLMs", "momentum orthogonalization", "dimensional fragility", "outlier-induced noise", "adaptive Newton iterations", "proximal optimization", "Muon", "Adam-based optimizers", "robust optimizer"], "githubStars": 909, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u52a0\u5267\u4e86\u7b97\u6cd5\u4e0d\u7cbe\u786e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668ROOT\uff0c\u65e8\u5728\u901a\u8fc7\u53cc\u91cd\u7a33\u5065\u673a\u5236\u6765\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>ROOT\u91c7\u7528\u81ea\u9002\u5e94\u725b\u987f\u8fed\u4ee3\u7684\u7ef4\u5ea6\u7a33\u5065\u6b63\u4ea4\u5316\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4e0d\u540c\u67b6\u6784\u914d\u7f6e\u4e0b\u7684\u7cbe\u786e\u5ea6\u3002</li>\n    <li>ROOT\u8fd8\u901a\u8fc7\u90bb\u8fd1\u4f18\u5316\u6846\u67b6\u6291\u5236\u5f02\u5e38\u503c\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u610f\u4e49\u7684\u68af\u5ea6\u65b9\u5411\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cROOT\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e2d\u6bd4Muon\u548c\u57fa\u4e8eAdam\u7684\u4f18\u5316\u5668\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) face challenges with training stability and precision as they grow in size.</li>\n    <li>Recent optimizers have improved efficiency but struggle with robustness against noise and dimensional changes.</li>\n    <li>ROOT is a new optimizer that strengthens training stability using two innovative methods: dimension-robust orthogonalization and noise suppression.</li>\n    <li>Experiments show ROOT outperforms other optimizers like Muon and Adam in tough conditions, achieving faster training and better results.</li>\n    <li>The ROOT code will be available online for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:48:05.000Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20626.png", "numComments": 2, "submittedBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "fullname": "hankai", "name": "hankaixyz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20785", "authors": [{"_id": "692d430f4397b1ec214f696e", "user": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "name": "Zuhao Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:11.311Z", "hidden": false}, {"_id": "692d430f4397b1ec214f696f", "user": {"_id": "6690f58e2f9f6f9c88e91031", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6690f58e2f9f6f9c88e91031/QQ_VoEh7NlE6BUvii08zk.png", "isPro": false, "fullname": "Sudong Wang", "user": "xiao45791", "type": "user"}, "name": "Sudong Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:14.173Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6970", "user": {"_id": "64bb77e786e7fb5b8a317a43", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bb77e786e7fb5b8a317a43/J0jOrlZJ9gazdYaeSH2Bo.png", "isPro": false, "fullname": "kcz", "user": "kcz358", "type": "user"}, "name": "Kaichen Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T10:06:41.343Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6971", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:09.461Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6972", "name": "Sicong Leng", "hidden": false}, {"_id": "692d430f4397b1ec214f6973", "name": "Yifan Zhang", "hidden": false}, {"_id": "692d430f4397b1ec214f6974", "name": "Chengwei Qin", "hidden": false}, {"_id": "692d430f4397b1ec214f6975", "name": "Shijian Lu", "hidden": false}, {"_id": "692d430f4397b1ec214f6976", "name": "Xingxuan Li", "hidden": false}, {"_id": "692d430f4397b1ec214f6977", "user": {"_id": "6454685a548f22be598414c4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg", "isPro": false, "fullname": "Lidong Bing", "user": "LidongBing", "type": "user"}, "name": "Lidong Bing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:49:36.056Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "publishedAt": "2025-11-25T19:22:48.000Z", "submittedOnDailyAt": "2025-12-02T00:35:56.511Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "submittedOnDailyBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "upvotes": 148, "discussionId": "692d430f4397b1ec214f6978", "projectPage": "https://evolvinglmms-lab.github.io/LongVT/", "githubRepo": "https://github.com/EvolvingLMMs-Lab/LongVT", "ai_summary": "LongVT, an end-to-end framework, enhances long video reasoning by interleaving global and local analysis using multimodal tools, outperforming existing methods on challenging benchmarks.", "ai_keywords": ["multimodal models", "video reasoning", "textual Chain-of-Thought", "hallucinations", "long-form videos", "temporal grounding", "video cropping", "fine-grained question-answering", "VideoSIAH", "tool-integrated cold-start supervised fine-tuning", "agentic reinforcement learning", "agentic reinforcement fine-tuning"], "githubStars": 121, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u89c6\u9891\u63a8\u7406\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86LongVT\u6846\u67b6\uff0c\u6a21\u4eff\u4eba\u7c7b\u7406\u89e3\u957f\u89c6\u9891\u7684\u65b9\u5f0f\uff0c\u901a\u8fc7\u5168\u5c40\u6d4f\u89c8\u548c\u8be6\u7ec6\u67e5\u770b\u76f8\u5173\u7247\u6bb5\u8fdb\u884c\u63a8\u7406\u3002</li>\n    <li>LongVT\u5229\u7528LMMs\u7684\u65f6\u95f4\u57fa\u7840\u80fd\u529b\uff0c\u7f29\u653e\u5230\u7279\u5b9a\u89c6\u9891\u7247\u6bb5\u5e76\u91cd\u65b0\u91c7\u6837\u66f4\u7ec6\u7c92\u5ea6\u7684\u89c6\u9891\u5e27\u3002</li>\n    <li>\u6211\u4eec\u6574\u7406\u4e86\u540d\u4e3aVideoSIAH\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u957f\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5171\u542b\u6709247.9K\u8bad\u7ec3\u6837\u672c\u548c1280\u4e2aQA\u5bf9\u3002</li>\n    <li>LongVT\u5728\u56db\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u5f3a\u57fa\u7ebf\uff0c\u4ee3\u7801\u548c\u6570\u636e\u53ef\u516c\u5f00\u83b7\u53d6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large multimodal models (LMMs) struggle with video reasoning, especially for long videos with scattered information.</li>\n    <li>LongVT is a new framework that mimics human video comprehension by first skimming and then focusing on specific clips.</li>\n    <li>It uses LMMs to identify and zoom in on relevant video segments for better detail extraction.</li>\n    <li>A new dataset called VideoSIAH will be released to help train and evaluate long video reasoning models.</li>\n    <li>LongVT shows better performance than existing models in various video understanding tasks and is publicly available for use.</li>\n</ul>"}, "publishedAt": "2025-11-25T14:22:48.000Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20785.png", "numComments": 3, "submittedBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "fullname": "Zuhao Yang", "name": "mwxely", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u5230\u8ba1\u7b97\u987a\u5e8f\u548c\u957f\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u7684\u9650\u5236\uff0c\u96be\u4ee5\u5728\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u5f62\u8c61\u751f\u6210\u4e2d\u4f7f\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201cLive Avatar\u201d\uff0c\u4e00\u79cd\u9ad8\u6548\u3001\u9ad8\u6e05\u6670\u5ea6\u4e14\u65e0\u957f\u5ea6\u9650\u5236\u7684\u865a\u62df\u5f62\u8c61\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u7528\u4e86140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\uff08TPP\uff09\u6280\u672f\uff0c\u901a\u8fc7\u591a\u4e2aGPU\u5e76\u884c\u5904\u7406\u53bb\u566a\u6b65\u9aa4\uff0c\u964d\u4f4e\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6d41\u5a92\u4f53\u751f\u6210\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u6eda\u52a8\u6c47\u805a\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5916\u89c2\u6765\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u8eab\u4efd\u6f02\u79fb\u548c\u989c\u8272\u4f2a\u5f71\u3002</li>\n    <li>Live Avatar\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e8620\u5e27\u6bcf\u79d2\u7684\u7aef\u5230\u7aef\u751f\u6210\uff0c\u5c55\u793a\u4e86\u884c\u4e1a\u9886\u5148\u7684\u5b9e\u65f6\u9ad8\u4fdd\u771f\u865a\u62df\u5f62\u8c61\u751f\u6210\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new system for creating high-quality, real-time video avatars using a large diffusion model.</li>\n    <li>It uses a method called Timestep-forcing Pipeline Parallelism (TPP) to speed up the generation process by using multiple GPUs.</li>\n    <li>The system includes a technique called Rolling Sink Frame Mechanism (RSFM) to maintain consistent appearance and reduce visual errors.</li>\n    <li>Live Avatar achieves impressive performance, generating 20 frames per second on five GPUs, making it suitable for real-time applications.</li>\n    <li>This work sets a new standard for using advanced models in creating long videos for various industries.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2511.18423", "authors": [{"_id": "692518ff16eb3a9f1310391c", "name": "B. Y. Yan", "hidden": false}, {"_id": "692518ff16eb3a9f1310391d", "name": "Chaofan Li", "hidden": false}, {"_id": "692518ff16eb3a9f1310391e", "name": "Hongjin Qian", "hidden": false}, {"_id": "692518ff16eb3a9f1310391f", "user": {"_id": "6145b3fd35135ec7e8d4ca45", "avatarUrl": "/avatars/5dc25d18d6a8418c9b1a29ece9a48f5a.svg", "isPro": false, "fullname": "Shuqi Lu", "user": "shuqi", "type": "user"}, "name": "Shuqi Lu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:18:11.163Z", "hidden": false}, {"_id": "692518ff16eb3a9f13103920", "user": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "name": "Zheng Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:17:59.618Z", "hidden": false}], "publishedAt": "2025-11-23T12:29:33.000Z", "submittedOnDailyAt": "2025-11-25T00:25:04.757Z", "title": "General Agentic Memory Via Deep Research", "submittedOnDailyBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "upvotes": 140, "discussionId": "692518ff16eb3a9f13103921", "projectPage": "https://github.com/VectorSpaceLab/general-agentic-memory", "githubRepo": "https://github.com/VectorSpaceLab/general-agentic-memory", "ai_summary": "GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.", "ai_keywords": ["general agentic memory", "GAM", "just-in time compilation", "JIT compilation", "memorizer", "researcher", "universal page-store", "large language models", "LLMs", "reinforcement learning"], "githubStars": 246, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u5185\u5b58\u5bf9\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u9759\u6001\u5185\u5b58\u5bb9\u6613\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u79f0\u4e3a\u4e00\u822c\u4ee3\u7406\u5185\u5b58\uff08GAM\uff09\uff0c\u91c7\u7528\u201c\u53ca\u65f6\u7f16\u8bd1\u201d\uff08JIT\uff09\u539f\u5219\u3002</li>\n    <li>GAM\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u8bb0\u5fc6\u5668\u548c\u7814\u7a76\u8005\uff0c\u5206\u522b\u7528\u4e8e\u7ba1\u7406\u5386\u53f2\u4fe1\u606f\u548c\u5728\u7ebf\u68c0\u7d22\u6709\u7528\u4fe1\u606f\u3002</li>\n    <li>\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u6709\u6548\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6574\u4f53\u6027\u80fd\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cGAM\u5728\u591a\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684\u4efb\u52a1\u5b8c\u6210\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5185\u5b58\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Memory is very important for AI agents, but traditional static memory can lead to significant information loss.</li>\n  <li>We introduce a new system called general agentic memory (GAM) that creates memory when needed, rather than in advance.</li>\n  <li>GAM consists of two main parts: a Memorizer that keeps important past information, and a Researcher that finds useful information when needed.</li>\n  <li>This system helps AI agents use large language models more effectively and improves their overall performance.</li>\n  <li>Tests show that GAM performs much better in memory-related tasks compared to existing memory systems.</li>\n</ul>"}, "publishedAt": "2025-11-23T07:29:33.000Z", "title": "General Agentic Memory Via Deep Research", "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18423.png", "numComments": 2, "submittedBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "fullname": "zhengliu", "name": "lz1001", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp\u662f\u4e00\u4e2a\u5305\u542b210\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6a21\u62df\u4f01\u4e1a\u6570\u636e\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\uff08DE\uff09\u4efb\u52a1\u9700\u8981\u5728\u5de5\u4e1a\u6a21\u5f0f\u4e0b\u8fdb\u884c\u591a\u9636\u6bb5SQL\u7ba1\u9053\u7684\u8bbe\u8ba1\u548c\u6784\u5efa\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\uff08DA\uff09\u4efb\u52a1\u6d89\u53ca\u5f00\u653e\u5f0f\u5546\u4e1a\u95ee\u9898\uff0c\u9700\u8981\u6218\u7565\u89c4\u5212\u548c\u63a2\u7d22\u6027\u5206\u6790\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u7684\u667a\u80fd\u4ee3\u7406\u5728DAComp\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u4e0a\u7684\u6210\u529f\u7387\u4f4e\u4e8e20%\u3002</li>\n    <li>DAComp\u4e3a\u5f00\u53d1\u771f\u6b63\u6709\u6548\u7684\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e76\u63ed\u793a\u4e86\u5de5\u7a0b\u4e0e\u5206\u6790\u80fd\u529b\u7684\u5dee\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DAComp is a benchmark with 210 tasks that simulate real-world data workflows involving data engineering and data analysis.</li>\n    <li>Data engineering tasks involve building and modifying SQL pipelines for complex data systems.</li>\n    <li>Data analysis tasks require solving business problems through strategic planning and coding to produce actionable insights.</li>\n    <li>Results show that even advanced agents struggle with DAComp, especially with data engineering tasks, which have success rates below 20%.</li>\n    <li>DAComp highlights the need for better autonomous data agents by identifying key weaknesses in data processing and reasoning capabilities.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.13254", "authors": [{"_id": "691c1c836bfd5965c0fd39e4", "user": {"_id": "6419b34a9a27800807c34a63", "avatarUrl": "/avatars/ee1391a9a153bae0dd04323b1fa5b5d6.svg", "isPro": false, "fullname": "Shalini M", "user": "shalinimaiti", "type": "user"}, "name": "Shalini Maiti", "status": "claimed_verified", "statusLastChangedAt": "2025-11-18T14:06:47.486Z", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e5", "user": {"_id": "6687ee79eee600e418404cc9", "avatarUrl": "/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg", "isPro": false, "fullname": "Amar Budhiraja", "user": "ambud26", "type": "user"}, "name": "Amar Budhiraja", "status": "claimed_verified", "statusLastChangedAt": "2025-11-18T14:06:49.892Z", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e6", "user": {"_id": "60720704227ff331937110f4", "avatarUrl": "/avatars/8010bfb98256c138049aa3d237737b37.svg", "isPro": false, "fullname": "Bhavul Gauri", "user": "bhavul", "type": "user"}, "name": "Bhavul Gauri", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:31.124Z", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e7", "user": {"_id": "691c6b9b660c15d270b5838a", "avatarUrl": "/avatars/2dc40e079bd8b7af7ae4a4ac43acc552.svg", "isPro": false, "fullname": "Gaurav Chaurasia", "user": "gchauras", "type": "user"}, "name": "Gaurav Chaurasia", "status": "claimed_verified", "statusLastChangedAt": "2025-11-18T14:06:45.558Z", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e8", "name": "Anton Protopopov", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39e9", "name": "Alexis Audran-Reiss", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39ea", "name": "Michael Slater", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39eb", "name": "Despoina Magka", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39ec", "name": "Tatiana Shavrina", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39ed", "name": "Roberta Raileanu", "hidden": false}, {"_id": "691c1c836bfd5965c0fd39ee", "name": "Yoram Bachrach", "hidden": false}], "publishedAt": "2025-11-17T11:13:34.000Z", "submittedOnDailyAt": "2025-11-18T04:47:00.815Z", "title": "Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance", "submittedOnDailyBy": {"_id": "6687ee79eee600e418404cc9", "avatarUrl": "/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg", "isPro": false, "fullname": "Amar Budhiraja", "user": "ambud26", "type": "user"}, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.", "upvotes": 131, "discussionId": "691c1c846bfd5965c0fd39fc", "githubRepo": "https://github.com/facebookresearch/llm_souping", "githubStars": 60, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u8017\u65f6\u4e14\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002</li>\n    <li>\u6a21\u578b\u201c\u6df7\u5408\u201d\uff08model souping\uff09\u901a\u8fc7\u5bf9\u591a\u4e2a\u76f8\u540c\u67b6\u6784\u7684\u6a21\u578b\u8fdb\u884c\u52a0\u6743\u5e73\u5747\uff0c\u80fd\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6027\u80fd\u3002</li>\n    <li>\u672c\u6587\u4ecb\u7ecd\u4e86\u201c\u7c7b\u522b\u4e13\u5bb6\u6df7\u5408\u201d\uff08SoCE\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u51c6\u7ec4\u5408\u6765\u9009\u62e9\u6700\u4f73\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u975e\u5747\u5300\u52a0\u6743\u5e73\u5747\u6765\u4f18\u5316\u6027\u80fd\u3002</li>\n    <li>SoCE\u65b9\u6cd5\u5229\u7528\u57fa\u51c6\u7c7b\u522b\u4e4b\u95f4\u7684\u4f4e\u76f8\u5173\u6027\u6765\u8bc6\u522b\u201c\u4e13\u5bb6\u201d\u6a21\u578b\uff0c\u5e76\u5bf9\u5176\u8fdb\u884c\u4f18\u5316\u52a0\u6743\u5408\u5e76\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u80fd\u529b\u3001\u5de5\u5177\u8c03\u7528\u548c\u6570\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u4f2f\u514b\u5229\u51fd\u6570\u8c03\u7528\u6392\u884c\u699c\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u6210\u7ee9\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are powerful but require a lot of resources and time to train.</li>\n    <li>Model souping, or averaging weights from multiple models, can improve performance without needing extensive retraining.</li>\n    <li>This paper presents Soup Of Category Experts (SoCE), a new method for model souping that selects the best models based on benchmark categories.</li>\n    <li>SoCE uses weighted averaging to combine models, focusing on those that perform well in related areas instead of using equal weights for all models.</li>\n    <li>The method shows better performance and reliability in various tasks, achieving top results in certain benchmarks.</li>\n</ul>"}, "publishedAt": "2025-11-17T06:13:34.000Z", "title": "Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies \"expert\" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.13254.png", "numComments": 4, "submittedBy": {"_id": "6687ee79eee600e418404cc9", "avatarUrl": "/avatars/d73d3a360ab3b3ce353a6306c7270a13.svg", "fullname": "Amar Budhiraja", "name": "ambud26", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Dec 17, 2025";
window.trendingPapers = {
    "today": [{"paper": {"id": "2512.20578", "authors": [{"_id": "69534e6789916ff627aa3fbb", "user": {"_id": "6830c4be7f72826192827659", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0hTKkO_zytpy9W8_0s291.png", "isPro": false, "fullname": "Amirhosein Ghasemabadi", "user": "AmirhoseinGH", "type": "user"}, "name": "Amirhosein Ghasemabadi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T10:03:00.600Z", "hidden": false}, {"_id": "69534e6789916ff627aa3fbc", "name": "Di Niu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6830c4be7f72826192827659/qufIktW_mZHzzI_iR1NCR.gif", "https://cdn-uploads.huggingface.co/production/uploads/6830c4be7f72826192827659/KVw6GK90RxzX2Jv9aF7v2.jpeg"], "publishedAt": "2025-12-23T18:21:32.000Z", "submittedOnDailyAt": "2026-01-06T00:39:40.820Z", "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits", "submittedOnDailyBy": {"_id": "6830c4be7f72826192827659", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0hTKkO_zytpy9W8_0s291.png", "isPro": false, "fullname": "Amirhosein Ghasemabadi", "user": "AmirhoseinGH", "type": "user"}, "summary": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.", "upvotes": 47, "discussionId": "69534e6889916ff627aa3fc3", "githubRepo": "https://github.com/Amirhosein-gh98/Gnosis", "githubRepoAddedBy": "auto", "githubStars": 6, "organization": {"_id": "64237839df71531a9c440b06", "name": "UAlberta", "fullname": "University of Alberta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/tdtBlXSCOd6XG48TORSBZ.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8f93\u51fa\u6d41\u7545\u590d\u6742\uff0c\u4f46\u5e38\u5e38\u65e0\u6cd5\u8bc6\u522b\u81ea\u8eab\u9519\u8bef\u548c\u5e7b\u89c9\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u8bc4\u4f30\u3001\u591a\u4e2a\u6837\u672c\u4e00\u81f4\u6027\u6216\u57fa\u4e8e\u6587\u672c\u7684\u81ea\u6211\u6279\u8bc4\uff0c\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201cGnosis\u201d\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u81ea\u6211\u610f\u8bc6\u673a\u5236\uff0c\u8ba9LLMs\u901a\u8fc7\u5185\u90e8\u72b6\u6001\u68c0\u67e5\u6765\u9884\u6d4b\u81ea\u8eab\u7684\u5931\u8d25\u3002</li>\n    <li>Gnosis \u89c2\u6d4b\u5185\u90e8\u75d5\u8ff9\uff0c\u538b\u7f29\u6210\u56fa\u5b9a\u9884\u7b97\u7684\u63cf\u8ff0\u7b26\uff0c\u51e0\u4e4e\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGnosis \u5728\u51c6\u786e\u6027\u548c\u6821\u51c6\u6027\u4e0a\u5747\u4f18\u4e8e\u5f3a\u5927\u7684\u5185\u90e8\u57fa\u7ebf\u548c\u5927\u578b\u5916\u90e8\u8bc4\u4f30\u8005\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) often make mistakes and can't recognize them.</li>\n    <li>The new method, called Gnosis, helps LLMs check their own work by analyzing their internal processes.</li>\n    <li>Gnosis uses hidden data from the model to predict if its outputs are correct, adding only a small amount of extra computation.</li>\n    <li>It works well across different tasks, like math and answering questions, and outperforms other methods in accuracy.</li>\n    <li>Gnosis can also identify problems early in the generation process, making it a useful tool for improving LLM performance.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:21:32.000Z", "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits", "summary": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6830c4be7f72826192827659/qufIktW_mZHzzI_iR1NCR.gif", "https://cdn-uploads.huggingface.co/production/uploads/6830c4be7f72826192827659/KVw6GK90RxzX2Jv9aF7v2.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20578.png", "numComments": 2, "submittedBy": {"_id": "6830c4be7f72826192827659", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0hTKkO_zytpy9W8_0s291.png", "fullname": "Amirhosein Ghasemabadi", "name": "AmirhoseinGH", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "64237839df71531a9c440b06", "name": "UAlberta", "fullname": "University of Alberta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/tdtBlXSCOd6XG48TORSBZ.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.02204", "authors": [{"_id": "695c7d0d6aa73bc11f091433", "name": "Huichao Zhang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091434", "user": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "isPro": false, "fullname": "Liao Qu", "user": "leo1117", "type": "user"}, "name": "Liao Qu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:57:57.686Z", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091435", "name": "Yiheng Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091436", "name": "Hang Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091437", "name": "Yangyang Song", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091438", "name": "Yongsheng Dong", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091439", "name": "Shikun Sun", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143a", "name": "Xian Li", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143b", "name": "Xu Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143c", "user": {"_id": "6344dcb1cd37e44d9ed46508", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344dcb1cd37e44d9ed46508/J92UKSxKR3iziD2WJfih4.jpeg", "isPro": false, "fullname": "Yi Jiang", "user": "JiangYi", "type": "user"}, "name": "Yi Jiang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:57:55.158Z", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143d", "name": "Hu Ye", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143e", "name": "Bo Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143f", "name": "Yiming Gao", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091440", "name": "Peng Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091441", "name": "Akide Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091442", "name": "Zhipeng Yang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091443", "name": "Qili Deng", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091444", "name": "Linjie Xing", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091445", "name": "Jiyang Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091446", "name": "Zhao Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091447", "name": "Yang Zhou", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091448", "name": "Mingcong Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091449", "name": "Yi Zhang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144a", "name": "Qian He", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144b", "name": "Xiwei Hu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144c", "name": "Zhongqi Qi", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144d", "name": "Jie Shao", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144e", "name": "Zhiye Fu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144f", "name": "Shuai Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091450", "name": "Fangmin Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091451", "name": "Xuezhi Chai", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091452", "name": "Zhihua Wu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091453", "name": "Yitong Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091454", "name": "Zehuan Yuan", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091455", "name": "Daniel K. Du", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091456", "name": "Xinglong Wu", "hidden": false}], "publishedAt": "2026-01-05T15:27:04.000Z", "submittedOnDailyAt": "2026-01-06T00:52:35.953Z", "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation", "submittedOnDailyBy": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "isPro": false, "fullname": "Liao Qu", "user": "leo1117", "type": "user"}, "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.", "upvotes": 45, "discussionId": "695c7d0d6aa73bc11f091457", "githubRepo": "https://github.com/ByteVisionLab/NextFlow", "githubRepoAddedBy": "user", "ai_summary": "NextFlow is a unified decoder-only autoregressive transformer that processes interleaved text-image tokens, enabling fast multimodal generation through novel next-token and next-scale prediction strategies.", "ai_keywords": ["decoder-only autoregressive transformer", "interleaved text-image discrete tokens", "unified vision representation", "multimodal understanding", "multimodal generation", "next-token prediction", "next-scale prediction", "raster-scan methods", "visual generation", "prefix-tuning strategy", "reinforcement learning", "diffusion baselines"], "githubStars": 60, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>NextFlow \u662f\u4e00\u79cd\u7edf\u4e00\u7684\u89e3\u7801\u5668\uff0c\u4f7f\u7528\u4e86 6 \u4e07\u4ebf\u4e2a\u6587\u672c\u548c\u56fe\u50cf\u7684\u79bb\u6563\u6807\u8bb0\u8fdb\u884c\u8bad\u7ec3\u3002</li>\n    <li>\u5b83\u53ef\u4ee5\u81ea\u7136\u5730\u7406\u89e3\u548c\u751f\u6210\u591a\u6a21\u6001\u5185\u5bb9\uff0c\u5982\u56fe\u50cf\u7f16\u8f91\u548c\u89c6\u9891\u751f\u6210\u3002</li>\n    <li>NextFlow \u5bf9\u6587\u672c\u548c\u56fe\u50cf\u91c7\u7528\u4e0d\u540c\u7684\u751f\u6210\u65b9\u5f0f\uff0c\u6587\u672c\u4f7f\u7528\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\uff0c\u56fe\u50cf\u4f7f\u7528\u4e0b\u4e00\u4e2a\u5c3a\u5ea6\u9884\u6d4b\u3002</li>\n    <li>\u5b83\u80fd\u591f\u5728 5 \u79d2\u5185\u751f\u6210 1024x1024 \u50cf\u7d20\u7684\u56fe\u50cf\uff0c\u6bd4\u5176\u4ed6\u7c7b\u4f3c\u6a21\u578b\u5feb\u5f97\u591a\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cNextFlow \u5728\u7edf\u4e00\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u4e14\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u4e0e\u4e13\u4e1a\u7684\u6269\u6563\u6a21\u578b\u76f8\u5f53\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NextFlow is a powerful AI model that combines text and images, trained on a massive amount of data (6 trillion tokens).</li>\n    <li>It can understand and create content that involves both text and images, allowing for tasks like image editing and video generation.</li>\n    <li>The model uses different methods for generating text (next-token prediction) and images (next-scale prediction), making it much faster.</li>\n    <li>NextFlow can produce high-quality 1024x1024 images in just 5 seconds, which is much quicker than other similar models.</li>\n    <li>It performs very well in tests, matching or exceeding the quality of specialized models designed for visual tasks.</li>\n</ul>"}, "publishedAt": "2026-01-05T10:27:04.000Z", "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation", "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02204.png", "numComments": 1, "submittedBy": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "fullname": "Liao Qu", "name": "leo1117", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.01739", "authors": [{"_id": "695c72346aa73bc11f0913bf", "user": {"_id": "6044fd39e6aa3e130cb92867", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6044fd39e6aa3e130cb92867/L5hb8vpHY6SKMEL-Xacma.jpeg", "isPro": false, "fullname": "Eunbi Choi", "user": "unbiarirang", "type": "user"}, "name": "Eunbi Choi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:17.472Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c0", "user": {"_id": "64d31ca9465b6039259838df", "avatarUrl": "/avatars/b3bde5067ed3fcd908d3d91c00680bfb.svg", "isPro": false, "fullname": "kibong choi", "user": "bongchoi", "type": "user"}, "name": "Kibong Choi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:11.322Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c1", "name": "Seokhee Hong", "hidden": false}, {"_id": "695c72346aa73bc11f0913c2", "user": {"_id": "63c50e590c24c8b53958f75e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673858632881-noauth.png", "isPro": false, "fullname": "Junwon Hwang", "user": "nuxlear", "type": "user"}, "name": "Junwon Hwang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:09.333Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c3", "name": "Hyojin Jeon", "hidden": false}, {"_id": "695c72346aa73bc11f0913c4", "user": {"_id": "66a9e066a203add977948988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a9e066a203add977948988/mwVS-vt-8p-DFC5T9H9H3.jpeg", "isPro": false, "fullname": "hyunjik.jo", "user": "switiz87", "type": "user"}, "name": "Hyunjik Jo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:13.551Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c5", "name": "Joonkee Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c6", "name": "Seonghwan Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c7", "name": "Soyeon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c8", "name": "Sunkyoung Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c9", "name": "Yireun Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913ca", "name": "Yongil Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913cb", "name": "Haeju Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913cc", "name": "Jinsik Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913cd", "name": "Kyungmin Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ce", "name": "Sangha Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913cf", "name": "Heuiyeen Yeen", "hidden": false}, {"_id": "695c72346aa73bc11f0913d0", "name": "Hwan Chang", "hidden": false}, {"_id": "695c72346aa73bc11f0913d1", "name": "Stanley Jungkyu Choi", "hidden": false}, {"_id": "695c72346aa73bc11f0913d2", "name": "Yejin Choi", "hidden": false}, {"_id": "695c72346aa73bc11f0913d3", "name": "Jiwon Ham", "hidden": false}, {"_id": "695c72346aa73bc11f0913d4", "name": "Kijeong Jeon", "hidden": false}, {"_id": "695c72346aa73bc11f0913d5", "name": "Geunyeong Jeong", "hidden": false}, {"_id": "695c72346aa73bc11f0913d6", "name": "Gerrard Jeongwon Jo", "hidden": false}, {"_id": "695c72346aa73bc11f0913d7", "name": "Yonghwan Jo", "hidden": false}, {"_id": "695c72346aa73bc11f0913d8", "name": "Jiyeon Jung", "hidden": false}, {"_id": "695c72346aa73bc11f0913d9", "name": "Naeun Kang", "hidden": false}, {"_id": "695c72346aa73bc11f0913da", "name": "Dohoon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913db", "name": "Euisoon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913dc", "name": "Hayeon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913dd", "name": "Hyosang Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913de", "name": "Hyunseo Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913df", "name": "Jieun Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e0", "name": "Minu Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e1", "name": "Myoungshin Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e2", "name": "Unsol Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e3", "name": "Youchul Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e4", "name": "YoungJin Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e5", "name": "Chaeeun Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e6", "name": "Chaeyoon Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e7", "user": {"_id": "6399ab9e92e12136b99ef60e", "avatarUrl": "/avatars/b76895c53f0f046586555c20292c78a1.svg", "isPro": false, "fullname": "Changhun Lee", "user": "xvyaward", "type": "user"}, "name": "Changhun Lee", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:15.420Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913e8", "name": "Dahm Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e9", "name": "Edward Hwayoung Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ea", "name": "Honglak Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913eb", "name": "Jinsang Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ec", "name": "Jiyoung Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ed", "name": "Sangeun Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ee", "name": "Seungwon Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913ef", "name": "Solji Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913f0", "name": "Woohyung Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913f1", "name": "Chanwoo Moon", "hidden": false}, {"_id": "695c72346aa73bc11f0913f2", "name": "Jaewoo Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f3", "name": "Jinho Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f4", "name": "Yongmin Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f5", "name": "Hyerin Seo", "hidden": false}, {"_id": "695c72346aa73bc11f0913f6", "name": "Wooseok Seo", "hidden": false}, {"_id": "695c72346aa73bc11f0913f7", "name": "Yongwoo Song", "hidden": false}, {"_id": "695c72346aa73bc11f0913f8", "name": "Sejong Yang", "hidden": false}, {"_id": "695c72346aa73bc11f0913f9", "name": "Sihoon Yang", "hidden": false}, {"_id": "695c72346aa73bc11f0913fa", "name": "Chang En Yea", "hidden": false}, {"_id": "695c72346aa73bc11f0913fb", "name": "Sihyuk Yi", "hidden": false}, {"_id": "695c72346aa73bc11f0913fc", "name": "Chansik Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913fd", "name": "Dongkeun Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913fe", "name": "Sangyeon Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913ff", "name": "Hyeongu Yun", "hidden": false}], "publishedAt": "2026-01-05T02:30:59.000Z", "submittedOnDailyAt": "2026-01-06T01:03:14.011Z", "title": "K-EXAONE Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.", "upvotes": 44, "discussionId": "695c72356aa73bc11f091400", "githubRepo": "https://github.com/LG-AI-EXAONE/K-EXAONE", "githubRepoAddedBy": "user", "ai_summary": "K-EXAONE is a multilingual language model with a Mixture-of-Experts architecture that achieves competitive performance on various benchmarks while supporting multiple languages and long-context windows.", "ai_keywords": ["Mixture-of-Experts", "256K-token context window", "multilingual language model", "parameter-efficient fine-tuning"], "githubStars": 39, "organization": {"_id": "66a89bc1d96a5adbccbe85d4", "name": "LGAI-EXAONE", "fullname": "LG AI Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a899a72f11aaf66001a8dc/UfdrP3GMo9pNT62BaMnhw.png"}, "summary_zh": "<ul>\n    <li>K-EXAONE\u662fLG AI Research\u5f00\u53d1\u7684\u591a\u8bed\u8a00\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u62e5\u67092360\u4ebf\u53c2\u6570\uff0c\u5728\u63a8\u7406\u65f6\u6fc0\u6d3b230\u4ebf\u53c2\u6570\u3002</li>\n    <li>K-EXAONE\u652f\u6301256K\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u6db5\u76d6\u97e9\u8bed\u3001\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u5fb7\u8bed\u3001\u65e5\u8bed\u548c\u8d8a\u5357\u8bed\u516d\u79cd\u8bed\u8a00\u3002</li>\n    <li>\u5728\u591a\u4e2a\u8bc4\u4f30\u4e2d\uff0cK-EXAONE\u7684\u8868\u73b0\u4e0e\u7c7b\u4f3c\u89c4\u6a21\u7684\u5f00\u653e\u6743\u91cd\u6a21\u578b\u76f8\u5f53\u3002</li>\n    <li>K-EXAONE\u65e8\u5728\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u6210\u4e3a\u5e7f\u6cdb\u5de5\u4e1a\u548c\u7814\u7a76\u5e94\u7528\u7684\u5f3a\u5927\u57fa\u7840\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>K-EXAONE is a large multilingual language model created by LG AI Research.</li>\n    <li>It has a total of 236 billion parameters, but only 23 billion are used at a time during operation.</li>\n    <li>The model can handle a context of up to 256,000 tokens and supports six languages: Korean, English, Spanish, German, Japanese, and Vietnamese.</li>\n    <li>K-EXAONE has been evaluated on various skills and performs similarly to other large models.</li>\n    <li>The model aims to enhance AI technology for better everyday life and is suitable for various industrial and research uses.</li>\n</ul>"}, "publishedAt": "2026-01-04T21:30:59.000Z", "title": "K-EXAONE Technical Report", "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01739.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 200, "isUserFollowing": false}, "organization": {"_id": "66a89bc1d96a5adbccbe85d4", "name": "LGAI-EXAONE", "fullname": "LG AI Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a899a72f11aaf66001a8dc/UfdrP3GMo9pNT62BaMnhw.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.01425", "authors": [{"_id": "695c765d6aa73bc11f091402", "name": "Xu Guo", "hidden": false}, {"_id": "695c765d6aa73bc11f091403", "name": "Fulong Ye", "hidden": false}, {"_id": "695c765d6aa73bc11f091404", "name": "Xinghui Li", "hidden": false}, {"_id": "695c765d6aa73bc11f091405", "name": "Pengqi Tu", "hidden": false}, {"_id": "695c765d6aa73bc11f091406", "name": "Pengze Zhang", "hidden": false}, {"_id": "695c765d6aa73bc11f091407", "user": {"_id": "674566cb79d6f3a9da7be0de", "avatarUrl": "/avatars/b6a5384820e150405039aa2b9badac29.svg", "isPro": false, "fullname": "Qichao Sun", "user": "Simons212", "type": "user"}, "name": "Qichao Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:07.336Z", "hidden": false}, {"_id": "695c765d6aa73bc11f091408", "name": "Songtao Zhao", "hidden": false}, {"_id": "695c765d6aa73bc11f091409", "name": "Xiangwang Hou", "hidden": false}, {"_id": "695c765d6aa73bc11f09140a", "name": "Qian He", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/67d50738fed7787297d737d6/0nECkDL67gltfx3htZ82l.mp4"], "publishedAt": "2026-01-04T08:07:11.000Z", "submittedOnDailyAt": "2026-01-06T00:30:06.666Z", "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer", "submittedOnDailyBy": {"_id": "67d50738fed7787297d737d6", "avatarUrl": "/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg", "isPro": false, "fullname": "xuguo", "user": "XuGuo699", "type": "user"}, "summary": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.", "upvotes": 33, "discussionId": "695c765d6aa73bc11f09140b", "projectPage": "https://guoxu1233.github.io/DreamID-V/", "githubRepo": "https://github.com/bytedance/DreamID-V", "githubRepoAddedBy": "user", "ai_summary": "A novel video face swapping framework combines image face swapping techniques with diffusion transformers and curriculum learning to achieve superior identity preservation and visual realism.", "ai_keywords": ["Video Face Swapping", "Image Face Swapping", "diffusion transformer", "Modality-Aware Conditioning", "Synthetic-to-Real Curriculum", "Identity-Coherence Reinforcement Learning", "IDBench-V", "Identity-Anchored Video Synthesizer", "bidirectional ID quadruplets", "multi-model conditions"], "githubStars": 86, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u6362\u8138\u6280\u672f\u9700\u8981\u5728\u89c6\u9891\u4e2d\u65e0\u7f1d\u5730\u63d2\u5165\u6e90\u8eab\u4efd\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u7684\u59ff\u52bf\u3001\u8868\u60c5\u3001\u5149\u7167\u548c\u52a8\u6001\u4fe1\u606f\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u8eab\u4efd\u76f8\u4f3c\u6027\u548c\u5c5e\u6027\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u5c06\u56fe\u50cf\u6362\u8138\u6280\u672f\u7684\u4f18\u8d8a\u6027\u8f6c\u79fb\u5230\u89c6\u9891\u9886\u57df\u3002</li>\n    <li>\u6846\u67b6\u5305\u62ec\u4e00\u4e2a\u65b0\u6570\u636e\u7ba1\u9053SyncID-Pipe\uff0c\u7ed3\u5408\u4e86\u8eab\u4efd\u951a\u5b9a\u7684\u89c6\u9891\u5408\u6210\u5668\u548c\u56fe\u50cf\u6362\u8138\u6a21\u578b\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5f15\u5165\u4e86IDBench-V\u57fa\u51c6\uff0c\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u8be5\u6846\u67b6\u5728\u591a\u79cd\u6362\u8138\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video Face Swapping (VFS) aims to insert one person's face into a video while keeping the original video's details like pose and lighting intact.</li>\n    <li>Current methods have trouble maintaining identity and visual quality over time.</li>\n    <li>The proposed solution includes a new data approach called SyncID-Pipe, which helps improve the process by creating better training models.</li>\n    <li>A new framework, DreamID-V, uses advanced technology to improve face swapping quality and consistency.</li>\n    <li>The authors also created IDBench-V, a new benchmark for testing face swapping in various scenes, showing that their method outperforms existing techniques.</li>\n</ul>"}, "publishedAt": "2026-01-04T03:07:11.000Z", "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer", "summary": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/67d50738fed7787297d737d6/0nECkDL67gltfx3htZ82l.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01425.png", "numComments": 2, "submittedBy": {"_id": "67d50738fed7787297d737d6", "avatarUrl": "/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg", "fullname": "xuguo", "name": "XuGuo699", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.02256", "authors": [{"_id": "695c7d256aa73bc11f091459", "name": "Shikun Sun", "hidden": false}, {"_id": "695c7d256aa73bc11f09145a", "user": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "isPro": false, "fullname": "Liao Qu", "user": "leo1117", "type": "user"}, "name": "Liao Qu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:57:53.171Z", "hidden": false}, {"_id": "695c7d256aa73bc11f09145b", "name": "Huichao Zhang", "hidden": false}, {"_id": "695c7d256aa73bc11f09145c", "name": "Yiheng Liu", "hidden": false}, {"_id": "695c7d256aa73bc11f09145d", "name": "Yangyang Song", "hidden": false}, {"_id": "695c7d256aa73bc11f09145e", "name": "Xian Li", "hidden": false}, {"_id": "695c7d256aa73bc11f09145f", "name": "Xu Wang", "hidden": false}, {"_id": "695c7d256aa73bc11f091460", "name": "Yi Jiang", "hidden": false}, {"_id": "695c7d256aa73bc11f091461", "name": "Daniel K. Du", "hidden": false}, {"_id": "695c7d256aa73bc11f091462", "name": "Xinglong Wu", "hidden": false}, {"_id": "695c7d256aa73bc11f091463", "name": "Jia Jia", "hidden": false}], "publishedAt": "2026-01-05T16:36:40.000Z", "submittedOnDailyAt": "2026-01-06T00:51:20.686Z", "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation", "submittedOnDailyBy": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "isPro": false, "fullname": "Liao Qu", "user": "leo1117", "type": "user"}, "summary": "Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.", "upvotes": 28, "discussionId": "695c7d266aa73bc11f091464", "ai_summary": "Visual autoregressive models face training instability due to asynchronous policy conflicts, which are addressed through a novel framework enhancing group relative policy optimization with intermediate rewards, dynamic time-step reweighting, and mask propagation algorithms.", "ai_keywords": ["AutoRegressive", "diffusion", "Visual AutoRegressive", "reinforcement learning", "Group Relative Policy Optimization", "intermediate reward", "dynamic time-step reweighting", "mask propagation", "Reward Feedback Learning", "visual generation"], "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u751f\u6210\u4e3b\u8981\u6709\u4e09\u79cd\u6a21\u578b\uff1a\u81ea\u56de\u5f52\uff08AR\uff09\u3001\u6269\u6563\u548c\u89c6\u89c9\u81ea\u56de\u5f52\uff08VAR\uff09\u6a21\u578b\u3002</li>\n    <li>VAR\u6a21\u578b\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5904\u7406\u5f02\u6784\u8f93\u5165\uff0c\u9020\u6210\u4e25\u91cd\u7684\u653f\u7b56\u51b2\u7a81\uff0c\u7279\u522b\u662f\u5728\u5f3a\u5316\u5b66\u4e60\u573a\u666f\u4e2d\u3002</li>\n    <li>\u4e3a\u4e86\u6539\u5584\u8fd9\u79cd\u60c5\u51b5\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u589e\u5f3a\u4e86\u7fa4\u4f53\u76f8\u5bf9\u653f\u7b56\u4f18\u5316\uff08GRPO\uff09\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u4e92\u8865\u7684\u7ec4\u6210\u90e8\u5206\uff0c\u4ee5\u6709\u6548\u7ba1\u7406\u653f\u7b56\u51b2\u7a81\uff1a\u7a33\u5b9a\u7684\u4e2d\u95f4\u5956\u52b1\u3001\u52a8\u6001\u65f6\u95f4\u6b65\u91cd\u52a0\u6743\u548c\u65b0\u578b\u63a9\u7801\u4f20\u64ad\u7b97\u6cd5\u3002</li>\n    <li>\u6211\u4eec\u7684\u65b9\u6848\u5728\u6837\u672c\u8d28\u91cf\u548c\u76ee\u6807\u5bf9\u9f50\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684GRPO\uff0c\u63d0\u9ad8\u4e86VAR\u6a21\u578b\u7684\u4f18\u5316\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Visual generation models use three main approaches: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR).</li>\n    <li>VAR models face problems with asynchronous policy conflicts, especially in reinforcement learning, leading to unstable training.</li>\n    <li>To address these issues, a new framework is proposed to improve Group Relative Policy Optimization (GRPO).</li>\n    <li>This framework includes a stabilizing reward, a dynamic time-step reweighting scheme, and a mask propagation algorithm.</li>\n    <li>The new method shows better sample quality and alignment compared to the standard GRPO method, enhancing VAR model optimization.</li>\n</ul>"}, "publishedAt": "2026-01-05T11:36:40.000Z", "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation", "summary": "Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02256.png", "numComments": 1, "submittedBy": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "fullname": "Liao Qu", "name": "leo1117", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24138", "authors": [{"_id": "695c9b756aa73bc11f0914fb", "name": "Haoran He", "hidden": false}, {"_id": "695c9b756aa73bc11f0914fc", "name": "Yuxiao Ye", "hidden": false}, {"_id": "695c9b756aa73bc11f0914fd", "name": "Jie Liu", "hidden": false}, {"_id": "695c9b756aa73bc11f0914fe", "name": "Jiajun Liang", "hidden": false}, {"_id": "695c9b756aa73bc11f0914ff", "name": "Zhiyong Wang", "hidden": false}, {"_id": "695c9b756aa73bc11f091500", "name": "Ziyang Yuan", "hidden": false}, {"_id": "695c9b756aa73bc11f091501", "name": "Xintao Wang", "hidden": false}, {"_id": "695c9b756aa73bc11f091502", "name": "Hangyu Mao", "hidden": false}, {"_id": "695c9b756aa73bc11f091503", "name": "Pengfei Wan", "hidden": false}, {"_id": "695c9b756aa73bc11f091504", "name": "Ling Pan", "hidden": false}], "publishedAt": "2025-12-30T10:55:45.000Z", "submittedOnDailyAt": "2026-01-06T02:53:45.651Z", "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking", "submittedOnDailyBy": {"_id": "6672937ceac0fb1b9e516595", "avatarUrl": "/avatars/5eea5657016572f60b0ecd0fa9a7dae4.svg", "isPro": false, "fullname": "haoran he", "user": "haoranhe", "type": "user"}, "summary": "Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.", "upvotes": 23, "discussionId": "695c9b766aa73bc11f091505", "projectPage": "https://tinnerhrhe.github.io/gardo_project/", "githubRepo": "https://github.com/tinnerhrhe/GARDO", "githubRepoAddedBy": "user", "ai_summary": "Online reinforcement learning for diffusion model fine-tuning suffers from reward hacking due to proxy reward mismatches, which GARDO addresses through selective regularization, adaptive reference updates, and diversity-aware reward amplification.", "ai_keywords": ["diffusion models", "reinforcement learning", "reward hacking", "proxy reward", "regularization", "online policy", "reference policy", "mode collapse", "diversity-aware optimization", "adaptive regularization", "sample efficiency", "exploration"], "githubStars": 18, "summary_zh": "<ul>\n    <li>\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u53ef\u4ee5\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u7684\u5bf9\u9f50\u6548\u679c\uff0c\u4f46\u786e\u5b9a\u771f\u5b9e\u76ee\u6807\u5f88\u56f0\u96be\u3002</li>\n    <li>\u6a21\u578b\u901a\u5e38\u4f7f\u7528\u4ee3\u7406\u5956\u52b1\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u5956\u52b1\u64cd\u7eb5\u548c\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u3002</li>\n    <li>\u4e3a\u4e86\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u6709\u6548\u63a2\u7d22\uff0c\u63d0\u51fa\u4e86Gated and Adaptive Regularization with Diversity-aware Optimization\uff08GARDO\uff09\u6846\u67b6\u3002</li>\n    <li>GARDO\u901a\u8fc7\u9009\u62e9\u6027\u60e9\u7f5a\u9ad8\u4e0d\u786e\u5b9a\u6027\u7684\u6837\u672c\u6765\u8fdb\u884c\u6b63\u5219\u5316\uff0c\u5e76\u5b9a\u671f\u66f4\u65b0\u53c2\u8003\u6a21\u578b\u4ee5\u5339\u914d\u5728\u7ebf\u7b56\u7565\u7684\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cGARDO\u6709\u6548\u51cf\u5c11\u5956\u52b1\u64cd\u7eb5\uff0c\u63d0\u5347\u751f\u6210\u591a\u6837\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6837\u672c\u6548\u7387\u548c\u63a2\u6d4b\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Fine-tuning diffusion models using online reinforcement learning can improve text-to-image alignment, but finding the right goals for visual tasks is difficult.</li>\n    <li>Using proxy rewards can lead to reward hacking, where scores improve while actual image quality declines and variety decreases.</li>\n    <li>Current solutions often involve regularization, but this can slow down learning and limit exploration of better options.</li>\n    <li>The proposed method, GARDO, selectively penalizes uncertain samples and updates the reference model, improving exploration and reducing reward hacking.</li>\n    <li>GARDO enhances the quality and diversity of generated images without losing efficiency in learning or exploration, as shown in various experiments.</li>\n</ul>"}, "publishedAt": "2025-12-30T05:55:45.000Z", "title": "GARDO: Reinforcing Diffusion Models without Reward Hacking", "summary": "Fine-tuning diffusion models via online reinforcement learning (RL) has shown great potential for enhancing text-to-image alignment. However, since precisely specifying a ground-truth objective for visual tasks remains challenging, the models are often optimized using a proxy reward that only partially captures the true goal. This mismatch often leads to reward hacking, where proxy scores increase while real image quality deteriorates and generation diversity collapses. While common solutions add regularization against the reference policy to prevent reward hacking, they compromise sample efficiency and impede the exploration of novel, high-reward regions, as the reference policy is usually sub-optimal. To address the competing demands of sample efficiency, effective exploration, and mitigation of reward hacking, we propose Gated and Adaptive Regularization with Diversity-aware Optimization (GARDO), a versatile framework compatible with various RL algorithms. Our key insight is that regularization need not be applied universally; instead, it is highly effective to selectively penalize a subset of samples that exhibit high uncertainty. To address the exploration challenge, GARDO introduces an adaptive regularization mechanism wherein the reference model is periodically updated to match the capabilities of the online policy, ensuring a relevant regularization target. To address the mode collapse issue in RL, GARDO amplifies the rewards for high-quality samples that also exhibit high diversity, encouraging mode coverage without destabilizing the optimization process. Extensive experiments across diverse proxy rewards and hold-out unseen metrics consistently show that GARDO mitigates reward hacking and enhances generation diversity without sacrificing sample efficiency or exploration, highlighting its effectiveness and robustness.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24138.png", "numComments": 2, "submittedBy": {"_id": "6672937ceac0fb1b9e516595", "avatarUrl": "/avatars/5eea5657016572f60b0ecd0fa9a7dae4.svg", "fullname": "haoran he", "name": "haoranhe", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.02358", "authors": [{"_id": "695c80146aa73bc11f09146a", "name": "Junyi Chen", "hidden": false}, {"_id": "695c80146aa73bc11f09146b", "name": "Tong He", "hidden": false}, {"_id": "695c80146aa73bc11f09146c", "name": "Zhoujie Fu", "hidden": false}, {"_id": "695c80146aa73bc11f09146d", "name": "Pengfei Wan", "hidden": false}, {"_id": "695c80146aa73bc11f09146e", "name": "Kun Gai", "hidden": false}, {"_id": "695c80146aa73bc11f09146f", "name": "Weicai Ye", "hidden": false}], "publishedAt": "2026-01-05T18:56:34.000Z", "submittedOnDailyAt": "2026-01-06T00:53:36.146Z", "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.", "upvotes": 21, "discussionId": "695c80146aa73bc11f091470", "projectPage": "https://sotamak1r.github.io/VINO-web/", "githubRepo": "https://github.com/SOTAMak1r/VINO-code", "githubRepoAddedBy": "user", "ai_summary": "VINO is a unified visual generator that uses a shared diffusion backbone with multimodal inputs to perform image and video generation and editing tasks.", "ai_keywords": ["visual generator", "diffusion backbone", "vision-language model", "Multimodal Diffusion Transformer", "conditioning tokens", "multimodal inputs", "video generation", "image generation", "visual editing", "multi-stage training pipeline", "unified system", "in-context computation"], "githubStars": 42, "summary_zh": "<ul>\n    <li>VINO\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u89c9\u751f\u6210\u5668\uff0c\u53ef\u4ee5\u5728\u4e00\u4e2a\u6846\u67b6\u5185\u8fdb\u884c\u56fe\u50cf\u548c\u89c6\u9891\u7684\u751f\u6210\u53ca\u7f16\u8f91\u3002</li>\n    <li>\u5b83\u5229\u7528\u5171\u4eab\u7684\u6269\u6563\u9aa8\u5e72\u7f51\u7edc\uff0c\u6839\u636e\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u8fdb\u884c\u5904\u7406\uff0c\u800c\u4e0d\u9700\u8981\u4e13\u95e8\u7684\u6a21\u578b\u3002</li>\n    <li>VINO\u7ed3\u5408\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u6269\u6563\u53d8\u6362\u5668\uff0c\u652f\u6301\u591a\u79cd\u8f93\u5165\uff0c\u80fd\u6709\u6548\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\u3002</li>\n    <li>\u8be5\u7cfb\u7edf\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\u9010\u6b65\u6269\u5c55\uff0c\u4ece\u800c\u5b9e\u73b0\u56fe\u50cf\u548c\u89c6\u9891\u7684\u7edf\u4e00\u751f\u6210\u80fd\u529b\u3002</li>\n    <li>VINO\u5728\u591a\u4e2a\u751f\u6210\u548c\u7f16\u8f91\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u793a\u51fa\u9ad8\u8d28\u91cf\u7684\u89c6\u89c9\u6548\u679c\u548c\u66f4\u597d\u7684\u63a7\u5236\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>VINO is a single tool for creating and editing both images and videos.</li>\n    <li>It uses a shared model that processes text, images, and videos together, instead of separate models for each task.</li>\n    <li>VINO combines a vision-language model with a special type of transformer to guide the creation process.</li>\n    <li>The system can handle complex instructions and maintain consistent identities in both images and videos.</li>\n    <li>In tests, VINO showed high visual quality, good instruction following, and effective editing capabilities.</li>\n</ul>"}, "publishedAt": "2026-01-05T13:56:34.000Z", "title": "VINO: A Unified Visual Generator with Interleaved OmniModal Context", "summary": "We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02358.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 200, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.02281", "authors": [{"_id": "695ccd446aa73bc11f0915e3", "name": "Shuai Yuan", "hidden": false}, {"_id": "695ccd446aa73bc11f0915e4", "name": "Yantai Yang", "hidden": false}, {"_id": "695ccd446aa73bc11f0915e5", "name": "Xiaotian Yang", "hidden": false}, {"_id": "695ccd446aa73bc11f0915e6", "name": "Xupeng Zhang", "hidden": false}, {"_id": "695ccd446aa73bc11f0915e7", "name": "Zhonghao Zhao", "hidden": false}, {"_id": "695ccd446aa73bc11f0915e8", "name": "Lingming Zhang", "hidden": false}, {"_id": "695ccd446aa73bc11f0915e9", "name": "Zhipeng Zhang", "hidden": false}], "publishedAt": "2026-01-05T17:11:00.000Z", "submittedOnDailyAt": "2026-01-06T06:27:03.712Z", "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams", "submittedOnDailyBy": {"_id": "65b9f710e7c83813628a5cd0", "avatarUrl": "/avatars/47075fb646359211b2abe601fa8156d5.svg", "isPro": false, "fullname": "Yantai Yang", "user": "yantaiyang05", "type": "user"}, "summary": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT", "upvotes": 20, "discussionId": "695ccd446aa73bc11f0915ea", "githubRepo": "https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT", "githubRepoAddedBy": "user", "ai_summary": "InfiniteVGGT enables continuous 3D visual geometry understanding through a causal transformer with adaptive memory management, outperforming existing streaming methods in long-term stability while introducing a new benchmark for extended evaluation.", "ai_keywords": ["visual geometry transformer", "causal visual geometry transformer", "rolling memory", "KV cache", "attention-agnostic pruning", "FlashAttention", "streaming architectures", "long-term stability", "infinite-horizon inputs", "Long3D benchmark"], "githubStars": 76, "organization": {"_id": "68ee0edd23dc954f7744ac27", "name": "AutoLab-SJTU", "fullname": "AutoLab"}, "summary_zh": "<ul>\n    <li>\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aInfiniteVGGT\u7684\u89c6\u89c9\u51e0\u4f55\u53d8\u6362\u5668\uff0c\u4ee5\u89e3\u51b3\u5927\u89c4\u6a213D\u89c6\u89c9\u7406\u89e3\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u957f\u671f\u7a33\u5b9a\u6027\u95ee\u9898\u3002</li>\n    <li>InfiniteVGGT\u91c7\u7528\u4e86\u6eda\u52a8\u5185\u5b58\u7684\u6982\u5ff5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7684KV\u7f13\u5b58\u6765\u5904\u7406\u65e0\u9650\u8f93\u5165\u3002</li>\n    <li>\u8be5\u7cfb\u7edf\u4f7f\u7528\u4e86\u4e00\u79cd\u65e0\u8bad\u7ec3\u3001\u65e0\u6ce8\u610f\u529b\u7684\u526a\u679d\u7b56\u7565\uff0c\u667a\u80fd\u5730\u4e22\u5f03\u8fc7\u65f6\u4fe1\u606f\uff0c\u4ee5\u4fdd\u6301\u5185\u5b58\u7684\u6709\u6548\u6027\u3002</li>\n    <li>InfiniteVGGT\u4e0eFlashAttention\u5b8c\u5168\u517c\u5bb9\uff0c\u80fd\u591f\u5728\u957f\u671f\u7a33\u5b9a\u6027\u65b9\u9762\u8d85\u8d8a\u73b0\u6709\u7684\u6d41\u5a92\u4f53\u65b9\u6cd5\u3002</li>\n    <li>\u5f15\u5165\u4e86Long3D\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9996\u6b21\u63d0\u4f9b\u4e86\u5bf9\u8fde\u7eed3D\u51e0\u4f55\u4f30\u8ba1\u7684\u4e25\u683c\u8bc4\u4f30\uff0c\u652f\u6301\u7ea610,000\u5e27\u7684\u5e8f\u5217\u5206\u6790\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The goal is to improve 3D visual geometry understanding for large-scale and long-term use.</li>\n    <li>Current models like VGGT are not suitable for live systems because they process data in batches.</li>\n    <li>InfiniteVGGT is a new model that uses a special memory system to handle continuous data input without losing stability.</li>\n    <li>This model includes a smart way to remove outdated information as new data comes in, making it efficient for long-term use.</li>\n    <li>A new benchmark called Long3D allows researchers to test 3D geometry estimation over very long sequences of data.</li>\n</ul>"}, "publishedAt": "2026-01-05T12:11:00.000Z", "title": "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams", "summary": "The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02281.png", "numComments": 1, "submittedBy": {"_id": "65b9f710e7c83813628a5cd0", "avatarUrl": "/avatars/47075fb646359211b2abe601fa8156d5.svg", "fullname": "Yantai Yang", "name": "yantaiyang05", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "68ee0edd23dc954f7744ac27", "name": "AutoLab-SJTU", "fullname": "AutoLab"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24601", "authors": [{"_id": "695b6f84832867f253525e5a", "name": "Alex L. Zhang", "hidden": false}, {"_id": "695b6f84832867f253525e5b", "name": "Tim Kraska", "hidden": false}, {"_id": "695b6f84832867f253525e5c", "name": "Omar Khattab", "hidden": false}], "publishedAt": "2025-12-31T03:43:41.000Z", "submittedOnDailyAt": "2026-01-06T04:56:19.174Z", "title": "Recursive Language Models", "submittedOnDailyBy": {"_id": "64b8e82aa62c52b252c827fa", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg", "isPro": true, "fullname": "Rajkumar rawal", "user": "rajkumarrawal", "type": "user"}, "summary": "We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.", "upvotes": 14, "discussionId": "695b6f84832867f253525e5d", "projectPage": "https://alexzhang13.github.io/blog/2025/rlm/", "githubRepo": "https://github.com/alexzhang13/rlm/tree/main", "githubRepoAddedBy": "user", "githubStars": 675, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u7814\u7a76\u4e86\u8ba9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5904\u7406\u4efb\u610f\u957f\u5ea6\u63d0\u793a\u7684\u65b9\u6cd5\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u9012\u5f52\u8bed\u8a00\u6a21\u578b\uff08RLMs\uff09\uff0c\u80fd\u591f\u5c06\u957f\u63d0\u793a\u89c6\u4e3a\u5916\u90e8\u73af\u5883\u7684\u4e00\u90e8\u5206\u3002</li>\n    <li>RLMs\u53ef\u4ee5\u7f16\u7a0b\u6027\u5730\u68c0\u67e5\u3001\u5206\u89e3\u5e76\u9012\u5f52\u8c03\u7528\u81ea\u8eab\u6765\u5904\u7406\u63d0\u793a\u7247\u6bb5\u3002</li>\n    <li>RLMs\u80fd\u6210\u529f\u5904\u7406\u6bd4\u6a21\u578b\u4e0a\u4e0b\u6587\u7a97\u53e3\u5927\u4e24\u500d\u7684\u8f93\u5165\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u660e\u663e\u8d85\u8d8a\u57fa\u7840LLMs\u7684\u8868\u73b0\u3002</li>\n    <li>\u5728\u67e5\u8be2\u6210\u672c\u65b9\u9762\uff0cRLMs\u7684\u8d39\u7528\u4e0e\u73b0\u6709\u7684\u957f\u4e0a\u4e0b\u6587\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f4e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>We explore how large language models (LLMs) can work with very long prompts.</li>\n    <li>We introduce Recursive Language Models (RLMs), which allow LLMs to break down and analyze long prompts more effectively.</li>\n    <li>RLMs can manage inputs much longer than what typical LLMs can handle and perform better than regular models on various tasks.</li>\n    <li>Even with shorter prompts, RLMs provide higher quality responses and are cost-effective.</li>\n</ul>"}, "publishedAt": "2025-12-30T22:43:41.000Z", "title": "Recursive Language Models", "summary": "We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24601.png", "numComments": 2, "submittedBy": {"_id": "64b8e82aa62c52b252c827fa", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8e82aa62c52b252c827fa/Jyk5PHMXCaRlmWy4mT3Bt.jpeg", "fullname": "Rajkumar rawal", "name": "rajkumarrawal", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 46, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.02346", "authors": [{"_id": "695c84156aa73bc11f09149d", "name": "Falcon LLM Team", "hidden": false}, {"_id": "695c84156aa73bc11f09149e", "user": {"_id": "660bd11884eca4537c4aeedd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660bd11884eca4537c4aeedd/nQ7qIgNJAK3r0DWNR9ysw.jpeg", "isPro": false, "fullname": "Iheb Chaabane", "user": "Iheb-Chaabane", "type": "user"}, "name": "Iheb Chaabane", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:57:42.636Z", "hidden": false}, {"_id": "695c84156aa73bc11f09149f", "user": {"_id": "66430f96463dc8f36d5bf4a4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66430f96463dc8f36d5bf4a4/V6dUB7SiHUt83s-AIp6P4.jpeg", "isPro": false, "fullname": "Puneesh Khanna", "user": "puneeshkhanna", "type": "user"}, "name": "Puneesh Khanna", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:57:44.813Z", "hidden": false}, {"_id": "695c84156aa73bc11f0914a0", "name": "Suhail Mohmad", "hidden": false}, {"_id": "695c84156aa73bc11f0914a1", "user": {"_id": "66db041dd04c920ebf3198ff", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66db041dd04c920ebf3198ff/yfkeoBpp6ztKi27GglP1z.jpeg", "isPro": false, "fullname": "Slim Frikha", "user": "slimfrikha", "type": "user"}, "name": "Slim Frikha", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:40:40.378Z", "hidden": false}, {"_id": "695c84156aa73bc11f0914a2", "name": "Shi Hu", "hidden": false}, {"_id": "695c84156aa73bc11f0914a3", "user": {"_id": "650476b6594e61e0c4d27ae8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650476b6594e61e0c4d27ae8/-2aF4BX7KpjJ1rDau1b7H.jpeg", "isPro": false, "fullname": "Abdalgader Abubaker", "user": "abdalgader", "type": "user"}, "name": "Abdalgader Abubaker", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:40:35.808Z", "hidden": false}, {"_id": "695c84156aa73bc11f0914a4", "name": "Reda Alami", "hidden": false}, {"_id": "695c84156aa73bc11f0914a5", "name": "Mikhail Lubinets", "hidden": false}, {"_id": "695c84156aa73bc11f0914a6", "user": {"_id": "6460bb7c455531c6be7997bd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460bb7c455531c6be7997bd/l0uuYf0rAPnKfvS90jAAr.jpeg", "isPro": false, "fullname": "Mohamed El Amine Seddik", "user": "melaseddik", "type": "user"}, "name": "Mohamed El Amine Seddik", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:40:37.824Z", "hidden": false}, {"_id": "695c84156aa73bc11f0914a7", "name": "Hakim Hacid", "hidden": false}], "publishedAt": "2026-01-05T18:44:27.000Z", "submittedOnDailyAt": "2026-01-06T01:10:59.614Z", "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are 2times to 7times larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.", "upvotes": 12, "discussionId": "695c84156aa73bc11f0914a8", "ai_summary": "Falcon-H1R is a 7B-parameter language model that achieves competitive reasoning performance through efficient training strategies and architectural design, enabling scalable reasoning capabilities in compact models.", "ai_keywords": ["language models", "parameter efficiency", "reasoning-optimized model", "small language models", "SOTA reasoning models", "efficient SFT", "RL scaling", "hybrid-parallel architecture design", "token efficiency", "chain-of-thoughts generation", "test-time scaling", "DeepConf approach"], "organization": {"_id": "6448cad23adf50d86406b0a3", "name": "tiiuae", "fullname": "Technology Innovation Institute", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"}, "summary_zh": "<ul>\n    <li>Falcon-H1R\u662f\u4e00\u4e2a\u62e5\u670970\u4ebf\u53c2\u6570\u7684\u4f18\u5316\u63a8\u7406\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7ade\u4e89\u6027\u63a8\u7406\u6027\u80fd\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u80fd\u591f\u4e0e2\u52307\u500d\u66f4\u5927\u6a21\u578b\u7684\u8868\u73b0\u76f8\u5ab2\u7f8e\u6216\u8d85\u8d8a\u3002</li>\n    <li>\u901a\u8fc7\u7cbe\u5fc3\u7684\u6570\u636e\u6574\u7406\u548c\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\uff0cFalcon-H1R\u5728\u4e0d\u589e\u52a0\u6a21\u578b\u5927\u5c0f\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002</li>\n    <li>\u5b83\u91c7\u7528\u6df7\u5408\u5e76\u884c\u67b6\u6784\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3001\u4ee4\u724c\u6548\u7387\u548c\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>Falcon-H1R\u5229\u7528DeepConf\u65b9\u6cd5\uff0c\u5728\u6d4b\u8bd5\u65f6\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6548\u7387\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6210\u672c\u7684\u6548\u76ca\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Falcon-H1R is a new 7 billion parameter model designed for reasoning tasks, showing that smaller models can compete with larger ones.</li>\n    <li>It is efficient, achieving similar or better performance than much larger models in various reasoning tests.</li>\n    <li>The model's success comes from careful data selection and effective training methods, which improve performance without increasing size.</li>\n    <li>Falcon-H1R features a hybrid design that allows for quicker processing, better use of tokens, and improved accuracy.</li>\n    <li>It uses a new approach, DeepConf, to enhance test-time efficiency, reducing both costs and improving accuracy for reasoning tasks.</li>\n</ul>"}, "publishedAt": "2026-01-05T13:44:27.000Z", "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling", "summary": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are 2times to 7times larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02346.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 200, "isUserFollowing": false}, "organization": {"_id": "6448cad23adf50d86406b0a3", "name": "tiiuae", "fullname": "Technology Innovation Institute", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2601.00393", "authors": [{"_id": "695b2297832867f253525d68", "user": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "name": "Yuxue Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:23.295Z", "hidden": false}, {"_id": "695b2297832867f253525d69", "user": {"_id": "649ecf9827145c4463240177", "avatarUrl": "/avatars/27696cf31790a3d58d8be2e0c983800e.svg", "isPro": false, "fullname": "Lue Fan", "user": "Abyssaledge", "type": "user"}, "name": "Lue Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T13:49:26.330Z", "hidden": false}, {"_id": "695b2297832867f253525d6a", "user": {"_id": "644cc2c36dfd5f8240d76a52", "avatarUrl": "/avatars/dcd9279af1c6d8535e48dc6e3e6511cd.svg", "isPro": false, "fullname": "Ziqi Shi", "user": "renshengjihe", "type": "user"}, "name": "Ziqi Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:21.077Z", "hidden": false}, {"_id": "695b2297832867f253525d6b", "name": "Junran Peng", "hidden": false}, {"_id": "695b2297832867f253525d6c", "name": "Feng Wang", "hidden": false}, {"_id": "695b2297832867f253525d6d", "name": "Zhaoxiang Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "publishedAt": "2026-01-01T17:07:30.000Z", "submittedOnDailyAt": "2026-01-05T02:49:46.994Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "submittedOnDailyBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "upvotes": 83, "discussionId": "695b2297832867f253525d6e", "projectPage": "https://neoverse-4d.github.io/", "githubRepo": "https://github.com/IamCreateAI/NeoVerse", "githubRepoAddedBy": "user", "ai_summary": "NeoVerse is a scalable 4D world model that enables pose-free reconstruction and novel-trajectory video generation from monocular videos with state-of-the-art performance.", "ai_keywords": ["4D world model", "4D reconstruction", "novel-trajectory video generation", "monocular videos", "pose-free", "feed-forward", "degradation pattern simulation"], "githubStars": 107, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aNeoVerse\u7684\u591a\u529f\u80fd4D\u4e16\u754c\u6a21\u578b\u3002</li>\n    <li>NeoVerse\u53ef\u4ee5\u8fdb\u884c4D\u91cd\u5efa\u3001\u751f\u6210\u65b0\u7684\u89c6\u9891\u8f68\u8ff9\uff0c\u5e76\u652f\u6301\u4e30\u5bcc\u7684\u5e94\u7528\u3002</li>\n    <li>\u89e3\u51b3\u4e86\u73b0\u67094D\u5efa\u6a21\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u4e0a\u7684\u9650\u5236\u3002</li>\n    <li>NeoVerse\u91c7\u7528\u65e0\u59ff\u6001\u524d\u9988\u76844D\u91cd\u5efa\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5355\u76ee\u89c6\u9891\u3002</li>\n    <li>\u5728\u91cd\u5efa\u548c\u751f\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNeoVerse\u8868\u73b0\u51fa\u8272\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NeoVerse is a new 4D world model that can create 4D reconstructions and generate videos.</li>\n    <li>Current 4D modeling methods struggle with scalability due to expensive data and complicated training processes.</li>\n    <li>NeoVerse is designed to work well with regular monocular videos, making it more accessible.</li>\n    <li>It includes features like pose-free reconstruction and online simulation to improve flexibility and performance.</li>\n    <li>NeoVerse achieves top results in standard benchmarks for reconstruction and video generation.</li>\n</ul>"}, "publishedAt": "2026-01-01T12:07:30.000Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00393.png", "numComments": 1, "submittedBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "fullname": "Yuxue Yang", "name": "Yuppie1204", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24615", "authors": [{"_id": "69564d96832867f2535257af", "user": {"_id": "622b00a776c20fee5d14501b", "avatarUrl": "/avatars/e00496dda1e309548e7b5b437839bb65.svg", "isPro": false, "fullname": "Eason shi", "user": "Easonshi", "type": "user"}, "name": "Yuchen Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:50.111Z", "hidden": false}, {"_id": "69564d96832867f2535257b0", "user": {"_id": "66e258bdc70c02b46dfed6e3", "avatarUrl": "/avatars/ccc2d604616c018f45a268a610472cac.svg", "isPro": false, "fullname": "Yuzheng Cai", "user": "Ucreate", "type": "user"}, "name": "Yuzheng Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:50.884Z", "hidden": false}, {"_id": "69564d96832867f2535257b1", "name": "Siqi Cai", "hidden": false}, {"_id": "69564d96832867f2535257b2", "name": "Zihan Xu", "hidden": false}, {"_id": "69564d96832867f2535257b3", "user": {"_id": "64154bfa385a75d7790f80e8", "avatarUrl": "/avatars/9e22f54b5eb7c4ebedad99a9a92c4b6a.svg", "isPro": false, "fullname": "Lichao Chen", "user": "nth233", "type": "user"}, "name": "Lichao Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:46.825Z", "hidden": false}, {"_id": "69564d96832867f2535257b4", "user": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "isPro": false, "fullname": "Yulei Qin", "user": "yolay", "type": "user"}, "name": "Yulei Qin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:48.064Z", "hidden": false}, {"_id": "69564d96832867f2535257b5", "name": "Zhijian Zhou", "hidden": false}, {"_id": "69564d96832867f2535257b6", "name": "Xiang Fei", "hidden": false}, {"_id": "69564d96832867f2535257b7", "user": {"_id": "6604e43869c47cd78fdebd08", "avatarUrl": "/avatars/4c11f5e1aeae3c5eb213f6ec6d5bfe72.svg", "isPro": false, "fullname": "Qiu", "user": "ChaofanDFG", "type": "user"}, "name": "Chaofan Qiu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:48.910Z", "hidden": false}, {"_id": "69564d96832867f2535257b8", "user": {"_id": "637af0a7bdf7309aa6da1c36", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637af0a7bdf7309aa6da1c36/NHZ-09otVCfbpXVxm8f-e.png", "isPro": false, "fullname": "Xiaoyu Tan", "user": "WIlliam1900", "type": "user"}, "name": "Xiaoyu Tan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:52.763Z", "hidden": false}, {"_id": "69564d96832867f2535257b9", "name": "Gang Li", "hidden": false}, {"_id": "69564d96832867f2535257ba", "name": "Zongyi Li", "hidden": false}, {"_id": "69564d96832867f2535257bb", "name": "Haojia Lin", "hidden": false}, {"_id": "69564d96832867f2535257bc", "name": "Guocan Cai", "hidden": false}, {"_id": "69564d96832867f2535257bd", "name": "Yong Mao", "hidden": false}, {"_id": "69564d96832867f2535257be", "name": "Yunsheng Wu", "hidden": false}, {"_id": "69564d96832867f2535257bf", "name": "Ke Li", "hidden": false}, {"_id": "69564d96832867f2535257c0", "user": {"_id": "647401e50da364bd0d002f2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png", "isPro": false, "fullname": "XING SUN", "user": "tedsun", "type": "user"}, "name": "Xing Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:39.390Z", "hidden": false}], "publishedAt": "2025-12-31T04:17:36.000Z", "submittedOnDailyAt": "2026-01-05T00:21:56.456Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "submittedOnDailyBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "isPro": false, "fullname": "Ke Li", "user": "tristanli", "type": "user"}, "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "upvotes": 82, "discussionId": "69564d96832867f2535257c1", "projectPage": "https://tencentcloudadp.github.io/youtu-agent/", "githubRepo": "https://github.com/TencentCloudADP/youtu-agent", "githubRepoAddedBy": "user", "githubStars": 4095, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u6846\u67b6\u9762\u4e34\u9ad8\u914d\u7f6e\u6210\u672c\u548c\u9759\u6001\u80fd\u529b\u7684\u6311\u6218\u3002</li>\n    <li>Youtu-Agent \u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u65e8\u5728\u81ea\u52a8\u751f\u6210\u548c\u6301\u7eed\u8fdb\u5316 LLM \u4ee3\u7406\u3002</li>\n    <li>\u5b83\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7684\u914d\u7f6e\u7cfb\u7edf\uff0c\u652f\u6301\u7075\u6d3b\u91cd\u7528\u548c\u81ea\u52a8\u5408\u6210\u3002</li>\n    <li>Youtu-Agent \u6709\u4e24\u79cd\u751f\u6210\u6a21\u5f0f\uff1a\u5de5\u4f5c\u6d41\u6a21\u5f0f\u548c\u5143\u4ee3\u7406\u6a21\u5f0f\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u5de5\u5177\u4ee3\u7801\u548c\u914d\u7f6e\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cYoutu-Agent \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u751f\u6210\u6548\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Youtu-Agent is a new framework that helps create and improve Large Language Model (LLM) agents more easily and efficiently.</li>\n  <li>It reduces the need for manual work by allowing flexible tool integration and automatic generation of agent configurations.</li>\n  <li>Youtu-Agent offers two modes: a simple Workflow mode for regular tasks and a Meta-Agent mode for more complex needs.</li>\n  <li>The framework includes a system for agents to learn and adapt through practice without needing to change their underlying parameters.</li>\n  <li>Tests show Youtu-Agent performs well on various benchmarks, with improvements in coding, reasoning, and search capabilities.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:17:36.000Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24615.png", "numComments": 1, "submittedBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "fullname": "Ke Li", "name": "tristanli", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24880", "authors": [{"_id": "69561fbf832867f253525726", "name": "Zhenda Xie", "hidden": false}, {"_id": "69561fbf832867f253525727", "name": "Yixuan Wei", "hidden": false}, {"_id": "69561fbf832867f253525728", "name": "Huanqi Cao", "hidden": false}, {"_id": "69561fbf832867f253525729", "name": "Chenggang Zhao", "hidden": false}, {"_id": "69561fbf832867f25352572a", "name": "Chengqi Deng", "hidden": false}, {"_id": "69561fbf832867f25352572b", "name": "Jiashi Li", "hidden": false}, {"_id": "69561fbf832867f25352572c", "name": "Damai Dai", "hidden": false}, {"_id": "69561fbf832867f25352572d", "name": "Huazuo Gao", "hidden": false}, {"_id": "69561fbf832867f25352572e", "name": "Jiang Chang", "hidden": false}, {"_id": "69561fbf832867f25352572f", "name": "Liang Zhao", "hidden": false}, {"_id": "69561fbf832867f253525730", "name": "Shangyan Zhou", "hidden": false}, {"_id": "69561fbf832867f253525731", "name": "Zhean Xu", "hidden": false}, {"_id": "69561fbf832867f253525732", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "69561fbf832867f253525733", "name": "Wangding Zeng", "hidden": false}, {"_id": "69561fbf832867f253525734", "name": "Shengding Hu", "hidden": false}, {"_id": "69561fbf832867f253525735", "name": "Yuqing Wang", "hidden": false}, {"_id": "69561fbf832867f253525736", "name": "Jingyang Yuan", "hidden": false}, {"_id": "69561fbf832867f253525737", "name": "Lean Wang", "hidden": false}, {"_id": "69561fbf832867f253525738", "name": "Wenfeng Liang", "hidden": false}], "publishedAt": "2025-12-31T14:16:26.000Z", "submittedOnDailyAt": "2026-01-01T07:30:32.169Z", "title": "mHC: Manifold-Constrained Hyper-Connections", "submittedOnDailyBy": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "isPro": false, "fullname": "Adina Yakefu", "user": "AdinaY", "type": "user"}, "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.", "upvotes": 59, "discussionId": "69561fc0832867f253525739", "ai_summary": "Manifold-Constrained Hyper-Connections (mHC) stabilize and scale residual connection architectures by restoring identity mapping properties through manifold projection and infrastructure optimization.", "ai_keywords": ["Hyper-Connections (HC)", "Manifold-Constrained Hyper-Connections (mHC)", "residual connections", "residual stream width", "connectivity patterns", "identity mapping property", "training instability", "memory access overhead", "manifold projection", "infrastructure optimization", "scalability"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u7814\u7a76\u6269\u5c55\u4e86\u6b8b\u5dee\u8fde\u63a5\uff0c\u901a\u8fc7\u589e\u52a0\u5bbd\u5ea6\u548c\u591a\u6837\u5316\u8fde\u63a5\u6a21\u5f0f\u63d0\u9ad8\u6027\u80fd\u3002</li>\n    <li>\u8fd9\u79cd\u591a\u6837\u5316\u7834\u574f\u4e86\u6b8b\u5dee\u8fde\u63a5\u7684\u8eab\u4efd\u6620\u5c04\u7279\u6027\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6269\u5c55\u6027\u53d7\u9650\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u79f0\u4e3a\u6d41\u5f62\u7ea6\u675f\u8d85\u8fde\u63a5 (mHC)\uff0c\u4ee5\u6062\u590d\u8eab\u4efd\u6620\u5c04\u7279\u6027\u5e76\u4f18\u5316\u6548\u7387\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cmHC \u5728\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\u6709\u6548\uff0c\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u66f4\u597d\u7684\u6269\u5c55\u6027\u3002</li>\n    <li>mHC \u4f5c\u4e3a\u8d85\u8fde\u63a5\u7684\u7075\u6d3b\u6269\u5c55\uff0c\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u62d3\u6251\u67b6\u6784\u8bbe\u8ba1\uff0c\u5e76\u4e3a\u57fa\u7840\u6a21\u578b\u7684\u6f14\u8fdb\u63d0\u4f9b\u6709\u524d\u666f\u7684\u65b9\u5411\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent studies have expanded the idea of residual connections in neural networks, leading to better performance but causing training issues.</li>\n    <li>This new approach creates problems with stability, scalability, and increased memory usage.</li>\n    <li>The proposed solution, Manifold-Constrained Hyper-Connections (mHC), helps restore the important identity mapping property while improving efficiency.</li>\n    <li>Tests show that mHC can train models effectively at scale, providing better performance and scalability than previous methods.</li>\n    <li>mHC aims to enhance understanding of neural network design and guide future developments in foundational models.</li>\n</ul>"}, "publishedAt": "2025-12-31T09:16:26.000Z", "title": "mHC: Manifold-Constrained Hyper-Connections", "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24880.png", "numComments": 1, "submittedBy": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "fullname": "Adina Yakefu", "name": "AdinaY", "type": "user", "isPro": false, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 1140}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24617", "authors": [{"_id": "69573165832867f253525871", "user": {"_id": "646b4c9fdf2609a541c0866e", "avatarUrl": "/avatars/7ec6c709017dcf32d4ac49d1e3820328.svg", "isPro": false, "fullname": "Qu", "user": "ScottQu", "type": "user"}, "name": "Xingwei Qu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:07.662Z", "hidden": false}, {"_id": "69573165832867f253525872", "name": "Shaowen Wang", "hidden": false}, {"_id": "69573165832867f253525873", "user": {"_id": "65a62085576772f531e13856", "avatarUrl": "/avatars/72c67a60422e333ea4e323f7480ae0b7.svg", "isPro": false, "fullname": "Huang Zihao", "user": "FetchFortune", "type": "user"}, "name": "Zihao Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:46.182Z", "hidden": false}, {"_id": "69573165832867f253525874", "user": {"_id": "64e851825ddcace745ba15bd", "avatarUrl": "/avatars/7b6612c411222974d9ea181784eef915.svg", "isPro": false, "fullname": "Kai Hua", "user": "kkish", "type": "user"}, "name": "Kai Hua", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:53.467Z", "hidden": false}, {"_id": "69573165832867f253525875", "name": "Fan Yin", "hidden": false}, {"_id": "69573165832867f253525876", "name": "Rui-Jie Zhu", "hidden": false}, {"_id": "69573165832867f253525877", "name": "Jundong Zhou", "hidden": false}, {"_id": "69573165832867f253525878", "name": "Qiyang Min", "hidden": false}, {"_id": "69573165832867f253525879", "name": "Zihao Wang", "hidden": false}, {"_id": "69573165832867f25352587a", "name": "Yizhi Li", "hidden": false}, {"_id": "69573165832867f25352587b", "name": "Tianyu Zhang", "hidden": false}, {"_id": "69573165832867f25352587c", "name": "He Xing", "hidden": false}, {"_id": "69573165832867f25352587d", "name": "Zheng Zhang", "hidden": false}, {"_id": "69573165832867f25352587e", "name": "Yuxuan Song", "hidden": false}, {"_id": "69573165832867f25352587f", "name": "Tianyu Zheng", "hidden": false}, {"_id": "69573165832867f253525880", "name": "Zhiyuan Zeng", "hidden": false}, {"_id": "69573165832867f253525881", "name": "Chenghua Lin", "hidden": false}, {"_id": "69573165832867f253525882", "name": "Ge Zhang", "hidden": false}, {"_id": "69573165832867f253525883", "name": "Wenhao Huang", "hidden": false}], "publishedAt": "2025-12-31T04:19:33.000Z", "submittedOnDailyAt": "2026-01-02T00:17:55.450Z", "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space", "submittedOnDailyBy": {"_id": "646b4c9fdf2609a541c0866e", "avatarUrl": "/avatars/7ec6c709017dcf32d4ac49d1e3820328.svg", "isPro": false, "fullname": "Qu", "user": "ScottQu", "type": "user"}, "summary": "Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first compression-aware scaling law, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a decoupled \u03bcP parametrization that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting (R=4, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a +2.69\\% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.", "upvotes": 52, "discussionId": "69573165832867f253525884", "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u6240\u6709\u8bcd\u5143\u91c7\u7528\u76f8\u540c\u7684\u8ba1\u7b97\u65b9\u5f0f\uff0c\u8fd9\u5bfc\u81f4\u5728\u4fe1\u606f\u5bc6\u5ea6\u4e0d\u5747\u5300\u7684\u8bed\u8a00\u4e2d\u6d6a\u8d39\u8ba1\u7b97\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u52a8\u6001\u5927\u6982\u5ff5\u6a21\u578b\uff08DLCM\uff09\uff0c\u5b83\u901a\u8fc7\u5b66\u4e60\u8bed\u4e49\u8fb9\u754c\u6765\u6539\u53d8\u8ba1\u7b97\u65b9\u5f0f\uff0c\u4ece\u8bcd\u5143\u8f6c\u5411\u66f4\u9ad8\u6548\u7684\u6982\u5ff5\u7a7a\u95f4\u3002</li>\n    <li>DLCM \u53ef\u4ee5\u4ece\u5934\u5230\u5c3e\u53d1\u73b0\u53ef\u53d8\u957f\u5ea6\u7684\u6982\u5ff5\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u8bed\u8a00\u5355\u4f4d\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86\u9996\u4e2a\u8003\u8651\u538b\u7f29\u7684\u6269\u5c55\u6cd5\u5219\uff0c\u5e2e\u52a9\u66f4\u5408\u7406\u5730\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u3002</li>\n    <li>DLCM \u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u63d0\u9ad8\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u572812\u4e2a\u96f6\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u5347\u4e862.69%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) treat all parts of text equally, which can waste processing power on predictable sections and not enough on important changes in meaning.</li>\n    <li>The new Dynamic Large Concept Models (DLCM) framework learns to identify important ideas and focuses computing power on those instead of individual words.</li>\n    <li>DLCM can automatically find and use concepts of different lengths without needing to rely on fixed language rules.</li>\n    <li>This method introduces a new way to understand how to allocate computing resources effectively while maintaining performance.</li>\n    <li>In tests, DLCM improved performance by 2.69% on average across 12 different tasks while using the same amount of computing resources.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:19:33.000Z", "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space", "summary": "Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first compression-aware scaling law, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a decoupled \u03bcP parametrization that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting (R=4, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a +2.69\\% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24617.png", "numComments": 4, "submittedBy": {"_id": "646b4c9fdf2609a541c0866e", "avatarUrl": "/avatars/7ec6c709017dcf32d4ac49d1e3820328.svg", "fullname": "Qu", "name": "ScottQu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.02204", "authors": [{"_id": "695c7d0d6aa73bc11f091433", "name": "Huichao Zhang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091434", "user": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "isPro": false, "fullname": "Liao Qu", "user": "leo1117", "type": "user"}, "name": "Liao Qu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:57:57.686Z", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091435", "name": "Yiheng Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091436", "name": "Hang Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091437", "name": "Yangyang Song", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091438", "name": "Yongsheng Dong", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091439", "name": "Shikun Sun", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143a", "name": "Xian Li", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143b", "name": "Xu Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143c", "user": {"_id": "6344dcb1cd37e44d9ed46508", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344dcb1cd37e44d9ed46508/J92UKSxKR3iziD2WJfih4.jpeg", "isPro": false, "fullname": "Yi Jiang", "user": "JiangYi", "type": "user"}, "name": "Yi Jiang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:57:55.158Z", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143d", "name": "Hu Ye", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143e", "name": "Bo Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143f", "name": "Yiming Gao", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091440", "name": "Peng Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091441", "name": "Akide Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091442", "name": "Zhipeng Yang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091443", "name": "Qili Deng", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091444", "name": "Linjie Xing", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091445", "name": "Jiyang Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091446", "name": "Zhao Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091447", "name": "Yang Zhou", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091448", "name": "Mingcong Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091449", "name": "Yi Zhang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144a", "name": "Qian He", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144b", "name": "Xiwei Hu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144c", "name": "Zhongqi Qi", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144d", "name": "Jie Shao", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144e", "name": "Zhiye Fu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144f", "name": "Shuai Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091450", "name": "Fangmin Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091451", "name": "Xuezhi Chai", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091452", "name": "Zhihua Wu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091453", "name": "Yitong Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091454", "name": "Zehuan Yuan", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091455", "name": "Daniel K. Du", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091456", "name": "Xinglong Wu", "hidden": false}], "publishedAt": "2026-01-05T15:27:04.000Z", "submittedOnDailyAt": "2026-01-06T00:52:35.953Z", "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation", "submittedOnDailyBy": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "isPro": false, "fullname": "Liao Qu", "user": "leo1117", "type": "user"}, "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.", "upvotes": 45, "discussionId": "695c7d0d6aa73bc11f091457", "githubRepo": "https://github.com/ByteVisionLab/NextFlow", "githubRepoAddedBy": "user", "ai_summary": "NextFlow is a unified decoder-only autoregressive transformer that processes interleaved text-image tokens, enabling fast multimodal generation through novel next-token and next-scale prediction strategies.", "ai_keywords": ["decoder-only autoregressive transformer", "interleaved text-image discrete tokens", "unified vision representation", "multimodal understanding", "multimodal generation", "next-token prediction", "next-scale prediction", "raster-scan methods", "visual generation", "prefix-tuning strategy", "reinforcement learning", "diffusion baselines"], "githubStars": 60, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>NextFlow \u662f\u4e00\u79cd\u7edf\u4e00\u7684\u89e3\u7801\u5668\uff0c\u4f7f\u7528\u4e86 6 \u4e07\u4ebf\u4e2a\u6587\u672c\u548c\u56fe\u50cf\u7684\u79bb\u6563\u6807\u8bb0\u8fdb\u884c\u8bad\u7ec3\u3002</li>\n    <li>\u5b83\u53ef\u4ee5\u81ea\u7136\u5730\u7406\u89e3\u548c\u751f\u6210\u591a\u6a21\u6001\u5185\u5bb9\uff0c\u5982\u56fe\u50cf\u7f16\u8f91\u548c\u89c6\u9891\u751f\u6210\u3002</li>\n    <li>NextFlow \u5bf9\u6587\u672c\u548c\u56fe\u50cf\u91c7\u7528\u4e0d\u540c\u7684\u751f\u6210\u65b9\u5f0f\uff0c\u6587\u672c\u4f7f\u7528\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\uff0c\u56fe\u50cf\u4f7f\u7528\u4e0b\u4e00\u4e2a\u5c3a\u5ea6\u9884\u6d4b\u3002</li>\n    <li>\u5b83\u80fd\u591f\u5728 5 \u79d2\u5185\u751f\u6210 1024x1024 \u50cf\u7d20\u7684\u56fe\u50cf\uff0c\u6bd4\u5176\u4ed6\u7c7b\u4f3c\u6a21\u578b\u5feb\u5f97\u591a\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cNextFlow \u5728\u7edf\u4e00\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u4e14\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u4e0e\u4e13\u4e1a\u7684\u6269\u6563\u6a21\u578b\u76f8\u5f53\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NextFlow is a powerful AI model that combines text and images, trained on a massive amount of data (6 trillion tokens).</li>\n    <li>It can understand and create content that involves both text and images, allowing for tasks like image editing and video generation.</li>\n    <li>The model uses different methods for generating text (next-token prediction) and images (next-scale prediction), making it much faster.</li>\n    <li>NextFlow can produce high-quality 1024x1024 images in just 5 seconds, which is much quicker than other similar models.</li>\n    <li>It performs very well in tests, matching or exceeding the quality of specialized models designed for visual tasks.</li>\n</ul>"}, "publishedAt": "2026-01-05T10:27:04.000Z", "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation", "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02204.png", "numComments": 1, "submittedBy": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "fullname": "Liao Qu", "name": "leo1117", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.01739", "authors": [{"_id": "695c72346aa73bc11f0913bf", "user": {"_id": "6044fd39e6aa3e130cb92867", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6044fd39e6aa3e130cb92867/L5hb8vpHY6SKMEL-Xacma.jpeg", "isPro": false, "fullname": "Eunbi Choi", "user": "unbiarirang", "type": "user"}, "name": "Eunbi Choi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:17.472Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c0", "user": {"_id": "64d31ca9465b6039259838df", "avatarUrl": "/avatars/b3bde5067ed3fcd908d3d91c00680bfb.svg", "isPro": false, "fullname": "kibong choi", "user": "bongchoi", "type": "user"}, "name": "Kibong Choi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:11.322Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c1", "name": "Seokhee Hong", "hidden": false}, {"_id": "695c72346aa73bc11f0913c2", "user": {"_id": "63c50e590c24c8b53958f75e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673858632881-noauth.png", "isPro": false, "fullname": "Junwon Hwang", "user": "nuxlear", "type": "user"}, "name": "Junwon Hwang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:09.333Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c3", "name": "Hyojin Jeon", "hidden": false}, {"_id": "695c72346aa73bc11f0913c4", "user": {"_id": "66a9e066a203add977948988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a9e066a203add977948988/mwVS-vt-8p-DFC5T9H9H3.jpeg", "isPro": false, "fullname": "hyunjik.jo", "user": "switiz87", "type": "user"}, "name": "Hyunjik Jo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:13.551Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c5", "name": "Joonkee Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c6", "name": "Seonghwan Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c7", "name": "Soyeon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c8", "name": "Sunkyoung Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c9", "name": "Yireun Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913ca", "name": "Yongil Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913cb", "name": "Haeju Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913cc", "name": "Jinsik Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913cd", "name": "Kyungmin Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ce", "name": "Sangha Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913cf", "name": "Heuiyeen Yeen", "hidden": false}, {"_id": "695c72346aa73bc11f0913d0", "name": "Hwan Chang", "hidden": false}, {"_id": "695c72346aa73bc11f0913d1", "name": "Stanley Jungkyu Choi", "hidden": false}, {"_id": "695c72346aa73bc11f0913d2", "name": "Yejin Choi", "hidden": false}, {"_id": "695c72346aa73bc11f0913d3", "name": "Jiwon Ham", "hidden": false}, {"_id": "695c72346aa73bc11f0913d4", "name": "Kijeong Jeon", "hidden": false}, {"_id": "695c72346aa73bc11f0913d5", "name": "Geunyeong Jeong", "hidden": false}, {"_id": "695c72346aa73bc11f0913d6", "name": "Gerrard Jeongwon Jo", "hidden": false}, {"_id": "695c72346aa73bc11f0913d7", "name": "Yonghwan Jo", "hidden": false}, {"_id": "695c72346aa73bc11f0913d8", "name": "Jiyeon Jung", "hidden": false}, {"_id": "695c72346aa73bc11f0913d9", "name": "Naeun Kang", "hidden": false}, {"_id": "695c72346aa73bc11f0913da", "name": "Dohoon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913db", "name": "Euisoon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913dc", "name": "Hayeon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913dd", "name": "Hyosang Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913de", "name": "Hyunseo Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913df", "name": "Jieun Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e0", "name": "Minu Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e1", "name": "Myoungshin Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e2", "name": "Unsol Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e3", "name": "Youchul Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e4", "name": "YoungJin Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e5", "name": "Chaeeun Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e6", "name": "Chaeyoon Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e7", "user": {"_id": "6399ab9e92e12136b99ef60e", "avatarUrl": "/avatars/b76895c53f0f046586555c20292c78a1.svg", "isPro": false, "fullname": "Changhun Lee", "user": "xvyaward", "type": "user"}, "name": "Changhun Lee", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:15.420Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913e8", "name": "Dahm Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e9", "name": "Edward Hwayoung Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ea", "name": "Honglak Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913eb", "name": "Jinsang Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ec", "name": "Jiyoung Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ed", "name": "Sangeun Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ee", "name": "Seungwon Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913ef", "name": "Solji Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913f0", "name": "Woohyung Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913f1", "name": "Chanwoo Moon", "hidden": false}, {"_id": "695c72346aa73bc11f0913f2", "name": "Jaewoo Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f3", "name": "Jinho Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f4", "name": "Yongmin Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f5", "name": "Hyerin Seo", "hidden": false}, {"_id": "695c72346aa73bc11f0913f6", "name": "Wooseok Seo", "hidden": false}, {"_id": "695c72346aa73bc11f0913f7", "name": "Yongwoo Song", "hidden": false}, {"_id": "695c72346aa73bc11f0913f8", "name": "Sejong Yang", "hidden": false}, {"_id": "695c72346aa73bc11f0913f9", "name": "Sihoon Yang", "hidden": false}, {"_id": "695c72346aa73bc11f0913fa", "name": "Chang En Yea", "hidden": false}, {"_id": "695c72346aa73bc11f0913fb", "name": "Sihyuk Yi", "hidden": false}, {"_id": "695c72346aa73bc11f0913fc", "name": "Chansik Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913fd", "name": "Dongkeun Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913fe", "name": "Sangyeon Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913ff", "name": "Hyeongu Yun", "hidden": false}], "publishedAt": "2026-01-05T02:30:59.000Z", "submittedOnDailyAt": "2026-01-06T01:03:14.011Z", "title": "K-EXAONE Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.", "upvotes": 44, "discussionId": "695c72356aa73bc11f091400", "githubRepo": "https://github.com/LG-AI-EXAONE/K-EXAONE", "githubRepoAddedBy": "user", "ai_summary": "K-EXAONE is a multilingual language model with a Mixture-of-Experts architecture that achieves competitive performance on various benchmarks while supporting multiple languages and long-context windows.", "ai_keywords": ["Mixture-of-Experts", "256K-token context window", "multilingual language model", "parameter-efficient fine-tuning"], "githubStars": 39, "organization": {"_id": "66a89bc1d96a5adbccbe85d4", "name": "LGAI-EXAONE", "fullname": "LG AI Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a899a72f11aaf66001a8dc/UfdrP3GMo9pNT62BaMnhw.png"}, "summary_zh": "<ul>\n    <li>K-EXAONE\u662fLG AI Research\u5f00\u53d1\u7684\u591a\u8bed\u8a00\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u62e5\u67092360\u4ebf\u53c2\u6570\uff0c\u5728\u63a8\u7406\u65f6\u6fc0\u6d3b230\u4ebf\u53c2\u6570\u3002</li>\n    <li>K-EXAONE\u652f\u6301256K\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u6db5\u76d6\u97e9\u8bed\u3001\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u5fb7\u8bed\u3001\u65e5\u8bed\u548c\u8d8a\u5357\u8bed\u516d\u79cd\u8bed\u8a00\u3002</li>\n    <li>\u5728\u591a\u4e2a\u8bc4\u4f30\u4e2d\uff0cK-EXAONE\u7684\u8868\u73b0\u4e0e\u7c7b\u4f3c\u89c4\u6a21\u7684\u5f00\u653e\u6743\u91cd\u6a21\u578b\u76f8\u5f53\u3002</li>\n    <li>K-EXAONE\u65e8\u5728\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u6210\u4e3a\u5e7f\u6cdb\u5de5\u4e1a\u548c\u7814\u7a76\u5e94\u7528\u7684\u5f3a\u5927\u57fa\u7840\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>K-EXAONE is a large multilingual language model created by LG AI Research.</li>\n    <li>It has a total of 236 billion parameters, but only 23 billion are used at a time during operation.</li>\n    <li>The model can handle a context of up to 256,000 tokens and supports six languages: Korean, English, Spanish, German, Japanese, and Vietnamese.</li>\n    <li>K-EXAONE has been evaluated on various skills and performs similarly to other large models.</li>\n    <li>The model aims to enhance AI technology for better everyday life and is suitable for various industrial and research uses.</li>\n</ul>"}, "publishedAt": "2026-01-04T21:30:59.000Z", "title": "K-EXAONE Technical Report", "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01739.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 200, "isUserFollowing": false}, "organization": {"_id": "66a89bc1d96a5adbccbe85d4", "name": "LGAI-EXAONE", "fullname": "LG AI Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a899a72f11aaf66001a8dc/UfdrP3GMo9pNT62BaMnhw.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24618", "authors": [{"_id": "6955d543832867f25352555d", "name": "Junru Lu", "hidden": false}, {"_id": "6955d543832867f25352555e", "name": "Jiarui Qin", "hidden": false}, {"_id": "6955d543832867f25352555f", "name": "Lingfeng Qiao", "hidden": false}, {"_id": "6955d543832867f253525560", "name": "Yinghui Li", "hidden": false}, {"_id": "6955d543832867f253525561", "name": "Xinyi Dai", "hidden": false}, {"_id": "6955d543832867f253525562", "name": "Bo Ke", "hidden": false}, {"_id": "6955d543832867f253525563", "name": "Jianfeng He", "hidden": false}, {"_id": "6955d543832867f253525564", "name": "Ruizhi Qiao", "hidden": false}, {"_id": "6955d543832867f253525565", "name": "Di Yin", "hidden": false}, {"_id": "6955d543832867f253525566", "name": "Xing Sun", "hidden": false}, {"_id": "6955d543832867f253525567", "name": "Yunsheng Wu", "hidden": false}, {"_id": "6955d543832867f253525568", "name": "Yinsong Liu", "hidden": false}, {"_id": "6955d543832867f253525569", "name": "Shuangyin Liu", "hidden": false}, {"_id": "6955d543832867f25352556a", "name": "Mingkong Tang", "hidden": false}, {"_id": "6955d543832867f25352556b", "name": "Haodong Lin", "hidden": false}, {"_id": "6955d543832867f25352556c", "name": "Jiayi Kuang", "hidden": false}, {"_id": "6955d543832867f25352556d", "name": "Fanxu Meng", "hidden": false}, {"_id": "6955d543832867f25352556e", "name": "Xiaojuan Tang", "hidden": false}, {"_id": "6955d543832867f25352556f", "name": "Yunjia Xi", "hidden": false}, {"_id": "6955d543832867f253525570", "name": "Junjie Huang", "hidden": false}, {"_id": "6955d543832867f253525571", "name": "Haotong Yang", "hidden": false}, {"_id": "6955d543832867f253525572", "name": "Zhenyi Shen", "hidden": false}, {"_id": "6955d543832867f253525573", "name": "Yangning Li", "hidden": false}, {"_id": "6955d543832867f253525574", "name": "Qianwen Zhang", "hidden": false}, {"_id": "6955d543832867f253525575", "name": "Yifei Yu", "hidden": false}, {"_id": "6955d543832867f253525576", "name": "Siyu An", "hidden": false}, {"_id": "6955d543832867f253525577", "name": "Junnan Dong", "hidden": false}, {"_id": "6955d543832867f253525578", "name": "Qiufeng Wang", "hidden": false}, {"_id": "6955d543832867f253525579", "name": "Jie Wang", "hidden": false}, {"_id": "6955d543832867f25352557a", "name": "Keyu Chen", "hidden": false}, {"_id": "6955d543832867f25352557b", "name": "Wei Wen", "hidden": false}, {"_id": "6955d543832867f25352557c", "name": "Taian Guo", "hidden": false}, {"_id": "6955d543832867f25352557d", "name": "Zhifeng Shen", "hidden": false}, {"_id": "6955d543832867f25352557e", "name": "Daohai Yu", "hidden": false}, {"_id": "6955d543832867f25352557f", "name": "Jiahao Li", "hidden": false}, {"_id": "6955d543832867f253525580", "name": "Ke Li", "hidden": false}, {"_id": "6955d543832867f253525581", "name": "Zongyi Li", "hidden": false}, {"_id": "6955d543832867f253525582", "name": "Xiaoyu Tan", "hidden": false}], "publishedAt": "2025-12-31T04:25:11.000Z", "submittedOnDailyAt": "2026-01-01T00:33:09.720Z", "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "upvotes": 43, "discussionId": "6955d543832867f253525583", "ai_summary": "Youtu-LLM is a lightweight language model optimized for computational efficiency and agentic intelligence through a compact architecture, STEM-focused training curriculum, and scalable mid-training strategies for planning and reasoning tasks.", "ai_keywords": ["Multi-Latent Attention (MLA) architecture", "STEM-oriented vocabulary", "128k context window", "Commonsense-STEM-Agent Curriculum", "multi-stage training strategy", "agentic mid-training", "data construction schemes", "planning and reflection behaviors", "long-context reasoning", "state tracking", "agentic capabilities"], "summary_zh": "<ul>\n    <li>Youtu-LLM \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u5f3a\u5927\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u5907\u9ad8\u6548\u8ba1\u7b97\u548c\u667a\u80fd\u4ee3\u7406\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u7531\u96f6\u5f00\u59cb\u9884\u8bad\u7ec3\uff0c\u65e8\u5728\u7cfb\u7edf\u5730\u57f9\u517b\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u3002</li>\n    <li>\u91c7\u7528\u7d27\u51d1\u67b6\u6784\uff0c\u652f\u6301\u957f\u8fbe128k\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u9002\u5408\u957f\u65f6\u95f4\u63a8\u7406\u548c\u72b6\u6001\u8ddf\u8e2a\u3002</li>\n    <li>\u4f7f\u7528\u4e86\u5927\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u548c\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u786e\u4fdd\u6a21\u578b\u83b7\u5f97\u6df1\u5c42\u8ba4\u77e5\u80fd\u529b\u3002</li>\n    <li>\u5728\u591a\u9879\u4efb\u52a1\u4e2d\uff0cYoutu-LLM \u5728\u5c0f\u4e8e2B\u7684\u6a21\u578b\u4e2d\u8bbe\u7f6e\u4e86\u65b0\u7684\u6027\u80fd\u6807\u51c6\uff0c\u5e76\u5728\u4ee3\u7406\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Youtu-LLM is a new, efficient language model that is designed to be both lightweight and intelligent, focusing on reasoning and planning skills.</li>\n    <li>It features a compact design with a long-context support of 128k tokens, allowing it to handle complex tasks without using too much memory.</li>\n    <li>The training process uses a large dataset of about 11 trillion tokens, progressively teaching the model from basic commonsense knowledge to advanced STEM and agent-related tasks.</li>\n    <li>The model incorporates varied data to enhance its ability to plan and reflect across different domains like math and coding.</li>\n    <li>Youtu-LLM has shown to outperform larger models on specific tasks, proving that smaller models can be very capable in agent-related functions.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:25:11.000Z", "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24618.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 197}, "isAuthorParticipating": false}, {"paper": {"id": "2601.00664", "authors": [{"_id": "695b237a832867f253525d70", "user": {"_id": "66b57c77778c98d29446c8ec", "avatarUrl": "/avatars/c176bb7c072f3093f6a0786c87d384d8.svg", "isPro": false, "fullname": "Taekyung Ki", "user": "taekyungki", "type": "user"}, "name": "Taekyung Ki", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:19.100Z", "hidden": false}, {"_id": "695b237a832867f253525d71", "name": "Sangwon Jang", "hidden": false}, {"_id": "695b237a832867f253525d72", "name": "Jaehyeong Jo", "hidden": false}, {"_id": "695b237a832867f253525d73", "user": {"_id": "652066649004117947e46ed6", "avatarUrl": "/avatars/972c97df6f26d2c3d6ce71ec579984bb.svg", "isPro": false, "fullname": "Jaehong Yoon", "user": "jaehong31", "type": "user"}, "name": "Jaehong Yoon", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:17.228Z", "hidden": false}, {"_id": "695b237a832867f253525d74", "name": "Sung Ju Hwang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/OjpAmq7fuwGa-ZxL3KbSY.mp4"], "publishedAt": "2026-01-02T11:58:48.000Z", "submittedOnDailyAt": "2026-01-05T00:05:44.498Z", "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.", "upvotes": 43, "discussionId": "695b237a832867f253525d75", "projectPage": "https://taekyungki.github.io/AvatarForcing/", "githubRepo": "https://github.com/TaekyungKi/AvatarForcing", "githubRepoAddedBy": "user", "ai_summary": "Avatar Forcing framework enables real-time interactive head avatar generation with low latency and expressive motion through diffusion forcing and label-free preference optimization.", "ai_keywords": ["diffusion forcing", "real-time interaction", "multimodal inputs", "audio", "motion", "causal constraints", "synthetic losing samples", "direct preference optimization", "interactive head avatar generation"], "githubStars": 65, "summary_zh": "<ul>\n    <li>\u8c08\u8bdd\u5934\u50cf\u751f\u6210\u6280\u672f\u53ef\u4ee5\u5c06\u9759\u6001\u8096\u50cf\u8f6c\u5316\u4e3a\u751f\u52a8\u7684\u865a\u62df\u5934\u50cf\uff0c\u7528\u4e8e\u4ea4\u6d41\u548c\u5185\u5bb9\u521b\u4f5c\u3002</li>\n    <li>\u73b0\u6709\u6a21\u578b\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u4e92\u52a8\uff0c\u901a\u5e38\u53ea\u751f\u6210\u5355\u5411\u7684\u3001\u7f3a\u4e4f\u60c5\u611f\u7684\u54cd\u5e94\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u201cAvatar Forcing\u201d\uff0c\u65e8\u5728\u89e3\u51b3\u5b9e\u65f6\u751f\u6210\u5934\u50cf\u8fd0\u52a8\u548c\u65e0\u6807\u7b7e\u6570\u636e\u5b66\u4e60\u751f\u52a8\u53cd\u5e94\u7684\u6311\u6218\u3002</li>\n    <li>\u8be5\u6846\u67b6\u80fd\u591f\u5904\u7406\u7528\u6237\u7684\u97f3\u9891\u548c\u52a8\u4f5c\u8f93\u5165\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u7684\u5373\u65f6\u53cd\u5e94\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u80fd\u5728\u7ea6500\u6beb\u79d2\u5185\u5b9e\u73b0\u5b9e\u65f6\u4e92\u52a8\uff0c\u901f\u5ea6\u63d0\u5347\u8fbe\u52306.8\u500d\uff0c\u5e76\u4e14\u751f\u6210\u7684\u5934\u50cf\u8fd0\u52a8\u66f4\u5177\u8868\u73b0\u529b\uff0c\u8d85\u8fc780%\u7684\u4eba\u504f\u597d\u8fd9\u79cd\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Talking head generation creates realistic avatars from still images for online communication.</li>\n    <li>Current models often lack emotional engagement and do not allow for true two-way interaction.</li>\n    <li>Two main challenges include creating real-time motion and learning expressive responses without extra data.</li>\n    <li>The proposed solution, Avatar Forcing, improves user-avatar interaction by quickly processing audio and motion inputs.</li>\n    <li>Experimental results show a significant speed increase and improved expressiveness in avatar responses.</li>\n</ul>"}, "publishedAt": "2026-01-02T06:58:48.000Z", "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation", "summary": "Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/OjpAmq7fuwGa-ZxL3KbSY.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00664.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 198}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24873", "authors": [{"_id": "6955e3f8832867f2535255cd", "name": "Weixun Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255ce", "name": "XiaoXiao Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255cf", "name": "Wanhe An", "hidden": false}, {"_id": "6955e3f8832867f2535255d0", "name": "Fangwen Dai", "hidden": false}, {"_id": "6955e3f8832867f2535255d1", "name": "Wei Gao", "hidden": false}, {"_id": "6955e3f8832867f2535255d2", "name": "Yancheng He", "hidden": false}, {"_id": "6955e3f8832867f2535255d3", "name": "Ju Huang", "hidden": false}, {"_id": "6955e3f8832867f2535255d4", "name": "Qiang Ji", "hidden": false}, {"_id": "6955e3f8832867f2535255d5", "name": "Hanqi Jin", "hidden": false}, {"_id": "6955e3f8832867f2535255d6", "name": "Xiaoyang Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d7", "name": "Yang Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d8", "name": "Zhongwen Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d9", "name": "Shirong Lin", "hidden": false}, {"_id": "6955e3f8832867f2535255da", "name": "Jiashun Liu", "hidden": false}, {"_id": "6955e3f8832867f2535255db", "name": "Zenan Liu", "hidden": false}, {"_id": "6955e3f8832867f2535255dc", "name": "Tao Luo", "hidden": false}, {"_id": "6955e3f8832867f2535255dd", "name": "Dilxat Muhtar", "hidden": false}, {"_id": "6955e3f8832867f2535255de", "name": "Yuanbin Qu", "hidden": false}, {"_id": "6955e3f8832867f2535255df", "name": "Jiaqiang Shi", "hidden": false}, {"_id": "6955e3f8832867f2535255e0", "name": "Qinghui Sun", "hidden": false}, {"_id": "6955e3f8832867f2535255e1", "name": "Yingshui Tan", "hidden": false}, {"_id": "6955e3f8832867f2535255e2", "name": "Hao Tang", "hidden": false}, {"_id": "6955e3f8832867f2535255e3", "name": "Runze Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e4", "name": "Yi Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e5", "name": "Zhaoguo Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e6", "name": "Yanan Wu", "hidden": false}, {"_id": "6955e3f8832867f2535255e7", "name": "Shaopan Xiong", "hidden": false}, {"_id": "6955e3f8832867f2535255e8", "name": "Binchen Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255e9", "name": "Xander Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255ea", "name": "Yuchi Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255eb", "name": "Qipeng Zhang", "hidden": false}, {"_id": "6955e3f8832867f2535255ec", "name": "Xixia Zhang", "hidden": false}, {"_id": "6955e3f8832867f2535255ed", "name": "Haizhou Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255ee", "name": "Jie Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255ef", "name": "Shuaibing Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255f0", "name": "Baihui Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f1", "name": "Jianhui Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f2", "name": "Suhang Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f3", "name": "Yanni Zhu", "hidden": false}, {"_id": "6955e3f8832867f2535255f4", "name": "Mengze Cai", "hidden": false}, {"_id": "6955e3f8832867f2535255f5", "name": "Kerui Cao", "hidden": false}, {"_id": "6955e3f8832867f2535255f6", "name": "Xitong Chen", "hidden": false}, {"_id": "6955e3f8832867f2535255f7", "name": "Yue Dai", "hidden": false}, {"_id": "6955e3f8832867f2535255f8", "name": "Lifan Du", "hidden": false}, {"_id": "6955e3f8832867f2535255f9", "name": "Tao Feng", "hidden": false}, {"_id": "6955e3f8832867f2535255fa", "name": "Tao He", "hidden": false}, {"_id": "6955e3f8832867f2535255fb", "name": "Jin Hu", "hidden": false}, {"_id": "6955e3f8832867f2535255fc", "name": "Yijie Hu", "hidden": false}, {"_id": "6955e3f8832867f2535255fd", "name": "Ziyu Jiang", "hidden": false}, {"_id": "6955e3f8832867f2535255fe", "name": "Cheng Li", "hidden": false}, {"_id": "6955e3f8832867f2535255ff", "name": "Xiang Li", "hidden": false}, {"_id": "6955e3f8832867f253525600", "name": "Jing Liang", "hidden": false}, {"_id": "6955e3f8832867f253525601", "name": "Chonghuan Liu", "hidden": false}, {"_id": "6955e3f8832867f253525602", "name": "ZhenDong Liu", "hidden": false}, {"_id": "6955e3f8832867f253525603", "name": "Haodong Mi", "hidden": false}, {"_id": "6955e3f8832867f253525604", "name": "Yanhu Mo", "hidden": false}, {"_id": "6955e3f8832867f253525605", "name": "Junjia Ni", "hidden": false}, {"_id": "6955e3f8832867f253525606", "name": "Shixin Pei", "hidden": false}, {"_id": "6955e3f8832867f253525607", "name": "Jingyu Shen", "hidden": false}, {"_id": "6955e3f8832867f253525608", "name": "XiaoShuai Song", "hidden": false}, {"_id": "6955e3f8832867f253525609", "name": "Cecilia Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560a", "name": "Chaofan Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560b", "name": "Kangyu Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560c", "name": "Pei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560d", "name": "Tao Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560e", "name": "Wei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560f", "name": "Ke Xiao", "hidden": false}, {"_id": "6955e3f8832867f253525610", "name": "Mingyu Xu", "hidden": false}, {"_id": "6955e3f8832867f253525611", "name": "Tiange Xu", "hidden": false}, {"_id": "6955e3f8832867f253525612", "name": "Nan Ya", "hidden": false}, {"_id": "6955e3f8832867f253525613", "name": "Siran Yang", "hidden": false}, {"_id": "6955e3f8832867f253525614", "name": "Jianan Ye", "hidden": false}, {"_id": "6955e3f8832867f253525615", "name": "Yaxing Zang", "hidden": false}, {"_id": "6955e3f8832867f253525616", "name": "Duo Zhang", "hidden": false}, {"_id": "6955e3f8832867f253525617", "name": "Junbo Zhang", "hidden": false}, {"_id": "6955e3f8832867f253525618", "name": "Boren Zheng", "hidden": false}, {"_id": "6955e3f8832867f253525619", "name": "Wanxi Deng", "hidden": false}, {"_id": "6955e3f8832867f25352561a", "name": "Ling Pan", "hidden": false}, {"_id": "6955e3f8832867f25352561b", "name": "Lin Qu", "hidden": false}, {"_id": "6955e3f8832867f25352561c", "name": "Wenbo Su", "hidden": false}, {"_id": "6955e3f8832867f25352561d", "name": "Jiamang Wang", "hidden": false}, {"_id": "6955e3f8832867f25352561e", "name": "Wei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352561f", "name": "Hu Wei", "hidden": false}, {"_id": "6955e3f8832867f253525620", "name": "Minggang Wu", "hidden": false}, {"_id": "6955e3f8832867f253525621", "name": "Cheng Yu", "hidden": false}, {"_id": "6955e3f8832867f253525622", "name": "Bing Zhao", "hidden": false}, {"_id": "6955e3f8832867f253525623", "name": "Zhicheng Zheng", "hidden": false}, {"_id": "6955e3f8832867f253525624", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-12-31T14:03:39.000Z", "submittedOnDailyAt": "2026-01-01T00:33:23.374Z", "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.", "upvotes": 33, "discussionId": "6955e3f8832867f253525625", "ai_summary": "The Agentic Learning Ecosystem (ALE) introduces a principled infrastructure for agent development, combining post-training optimization, sandbox environments, and policy alignment to enhance long-horizon training stability and performance in real-world tasks.", "ai_keywords": ["ROLL (post-training framework)", "ROCK (sandbox environment manager)", "iFlow CLI (agent framework)", "ROME (agentic model)", "data composition protocols", "Interaction-based Policy Alignment (IPA)", "semantic interaction chunks", "Terminal Bench Pro", "SWE-bench Verified"], "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86Agentic Learning Ecosystem (ALE)\uff0c\u4e00\u4e2a\u7528\u4e8e\u4f18\u5316\u667a\u80fd\u4f53\u5f00\u53d1\u7684\u57fa\u7840\u8bbe\u65bd\u3002</li>\n    <li>ALE\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1aROLL\uff08\u6743\u91cd\u4f18\u5316\u6846\u67b6\uff09\u3001ROCK\uff08\u8f68\u8ff9\u751f\u6210\u6c99\u76d2\u73af\u5883\u7ba1\u7406\u5668\uff09\u548ciFlow CLI\uff08\u9ad8\u6548\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff09\u3002</li>\n    <li>\u53d1\u5e03\u4e86ROME\uff08\u663e\u7136\u662f\u4e00\u4e2a\u667a\u80fd\u4f53\u6a21\u578b\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8eALE\u7684\u5f00\u6e90\u667a\u80fd\u4f53\uff0c\u7ecf\u8fc7\u8d85\u8fc7\u4e00\u767e\u4e07\u4e2a\u8f68\u8ff9\u8bad\u7ec3\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0cInteraction-based Policy Alignment (IPA)\uff0c\u7528\u4e8e\u63d0\u5347\u957f\u671f\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u5728\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8bc4\u4f30\u4e86ROME\uff0c\u5e76\u5f15\u5165\u4e86Terminal Bench Pro\u57fa\u51c6\uff0c\u8bc1\u660eALE\u57fa\u7840\u8bbe\u65bd\u7684\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Agentic crafting helps large language models (LLMs) work in real-world situations by learning from actions and outcomes.</li>\n    <li>The open-source community lacks a complete system for developing these agent LLMs.</li>\n    <li>We present the Agentic Learning Ecosystem (ALE), which includes tools to improve the development process for agent LLMs.</li>\n    <li>ROME, an open-source agent model, is trained on over one million examples and uses new methods for behavior synthesis and training stability.</li>\n    <li>ROME shows strong performance in various benchmarks, demonstrating the advantages of the ALE infrastructure.</li>\n</ul>"}, "publishedAt": "2025-12-31T09:03:39.000Z", "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem", "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24873.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 197}, "isAuthorParticipating": false}, {"paper": {"id": "2601.01425", "authors": [{"_id": "695c765d6aa73bc11f091402", "name": "Xu Guo", "hidden": false}, {"_id": "695c765d6aa73bc11f091403", "name": "Fulong Ye", "hidden": false}, {"_id": "695c765d6aa73bc11f091404", "name": "Xinghui Li", "hidden": false}, {"_id": "695c765d6aa73bc11f091405", "name": "Pengqi Tu", "hidden": false}, {"_id": "695c765d6aa73bc11f091406", "name": "Pengze Zhang", "hidden": false}, {"_id": "695c765d6aa73bc11f091407", "user": {"_id": "674566cb79d6f3a9da7be0de", "avatarUrl": "/avatars/b6a5384820e150405039aa2b9badac29.svg", "isPro": false, "fullname": "Qichao Sun", "user": "Simons212", "type": "user"}, "name": "Qichao Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:07.336Z", "hidden": false}, {"_id": "695c765d6aa73bc11f091408", "name": "Songtao Zhao", "hidden": false}, {"_id": "695c765d6aa73bc11f091409", "name": "Xiangwang Hou", "hidden": false}, {"_id": "695c765d6aa73bc11f09140a", "name": "Qian He", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/67d50738fed7787297d737d6/0nECkDL67gltfx3htZ82l.mp4"], "publishedAt": "2026-01-04T08:07:11.000Z", "submittedOnDailyAt": "2026-01-06T00:30:06.666Z", "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer", "submittedOnDailyBy": {"_id": "67d50738fed7787297d737d6", "avatarUrl": "/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg", "isPro": false, "fullname": "xuguo", "user": "XuGuo699", "type": "user"}, "summary": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.", "upvotes": 33, "discussionId": "695c765d6aa73bc11f09140b", "projectPage": "https://guoxu1233.github.io/DreamID-V/", "githubRepo": "https://github.com/bytedance/DreamID-V", "githubRepoAddedBy": "user", "ai_summary": "A novel video face swapping framework combines image face swapping techniques with diffusion transformers and curriculum learning to achieve superior identity preservation and visual realism.", "ai_keywords": ["Video Face Swapping", "Image Face Swapping", "diffusion transformer", "Modality-Aware Conditioning", "Synthetic-to-Real Curriculum", "Identity-Coherence Reinforcement Learning", "IDBench-V", "Identity-Anchored Video Synthesizer", "bidirectional ID quadruplets", "multi-model conditions"], "githubStars": 86, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u6362\u8138\u6280\u672f\u9700\u8981\u5728\u89c6\u9891\u4e2d\u65e0\u7f1d\u5730\u63d2\u5165\u6e90\u8eab\u4efd\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u7684\u59ff\u52bf\u3001\u8868\u60c5\u3001\u5149\u7167\u548c\u52a8\u6001\u4fe1\u606f\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u8eab\u4efd\u76f8\u4f3c\u6027\u548c\u5c5e\u6027\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u5c06\u56fe\u50cf\u6362\u8138\u6280\u672f\u7684\u4f18\u8d8a\u6027\u8f6c\u79fb\u5230\u89c6\u9891\u9886\u57df\u3002</li>\n    <li>\u6846\u67b6\u5305\u62ec\u4e00\u4e2a\u65b0\u6570\u636e\u7ba1\u9053SyncID-Pipe\uff0c\u7ed3\u5408\u4e86\u8eab\u4efd\u951a\u5b9a\u7684\u89c6\u9891\u5408\u6210\u5668\u548c\u56fe\u50cf\u6362\u8138\u6a21\u578b\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5f15\u5165\u4e86IDBench-V\u57fa\u51c6\uff0c\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u8be5\u6846\u67b6\u5728\u591a\u79cd\u6362\u8138\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video Face Swapping (VFS) aims to insert one person's face into a video while keeping the original video's details like pose and lighting intact.</li>\n    <li>Current methods have trouble maintaining identity and visual quality over time.</li>\n    <li>The proposed solution includes a new data approach called SyncID-Pipe, which helps improve the process by creating better training models.</li>\n    <li>A new framework, DreamID-V, uses advanced technology to improve face swapping quality and consistency.</li>\n    <li>The authors also created IDBench-V, a new benchmark for testing face swapping in various scenes, showing that their method outperforms existing techniques.</li>\n</ul>"}, "publishedAt": "2026-01-04T03:07:11.000Z", "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer", "summary": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/67d50738fed7787297d737d6/0nECkDL67gltfx3htZ82l.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01425.png", "numComments": 2, "submittedBy": {"_id": "67d50738fed7787297d737d6", "avatarUrl": "/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg", "fullname": "xuguo", "name": "XuGuo699", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\uff0c\u73b0\u6709\u7684\u6570\u636e\u51c6\u5907\u65b9\u6cd5\u663e\u5f97\u4e0d\u591f\u7cfb\u7edf\u548c\u53ef\u9760\u3002</li>\n    <li>\u63d0\u51fa\u4e86DataFlow\uff0c\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u51c6\u5907\u6846\u67b6\uff0c\u652f\u6301\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>DataFlow\u63d0\u4f9b\u4e86\u8fd1200\u4e2a\u53ef\u91cd\u7528\u7684\u64cd\u4f5c\u7b26\u548c\u516d\u4e2a\u901a\u7528\u7ba1\u9053\uff0c\u6db5\u76d6\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u9886\u57df\u3002</li>\n    <li>\u901a\u8fc7DataFlow-Agent\uff0c\u53ef\u4ee5\u81ea\u52a8\u5c06\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u7ba1\u9053\uff0c\u63d0\u9ad8\u4e86\u53ef\u7528\u6027\u3002</li>\n    <li>\u5728\u591a\u4e2a\u5e94\u7528\u6848\u4f8b\u4e2d\uff0cDataFlow\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8d85\u8fc7\u4e86\u4f20\u7edf\u6570\u636e\u96c6\u7684\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DataFlow is a new framework designed to improve the preparation of high-quality data for Large Language Models (LLMs).</li>\n    <li>It addresses issues with current data preparation methods by offering modular and reusable transformations and a user-friendly pipeline construction API.</li>\n    <li>The framework includes almost 200 reusable operators and covers various domains like text, code, and mathematical reasoning.</li>\n    <li>DataFlow-Agent helps users create data pipelines from simple natural-language instructions, making it easier to use.</li>\n    <li>DataFlow significantly enhances LLM performance in multiple use cases, outperforming existing datasets and improving accuracy in various tasks.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni \u662f\u4e00\u4e2a\u751f\u6210\u6027\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u76f4\u63a5\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u7b49\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u7528\u6237\u4f53\u9a8c\u3002</li>\n    <li>\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5982\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\uff0c\u751f\u6210\u4f18\u8d28\u89c6\u9891\u5185\u5bb9\u3002</li>\n    <li>\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u6a21\u6001\u89c6\u9891\u521b\u5efa\uff0c\u5e76\u91c7\u7528\u4e86\u9ad8\u6548\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u3002</li>\n    <li>Kling-Omni \u5728\u751f\u6210\u3001\u7f16\u8f91\u548c\u9075\u5faa\u591a\u6a21\u6001\u6307\u4ee4\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6709\u52a9\u4e8e\u53d1\u5c55\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new framework that creates high-quality videos from various types of visual language inputs.</li>\n    <li>It combines different tasks like video generation, editing, and reasoning into one system, rather than using separate processes.</li>\n    <li>The framework can understand and process inputs like text instructions, images, and video contexts to produce cinematic-quality videos.</li>\n    <li>Kling-Omni is built on a strong data system and uses advanced techniques for training and running the model efficiently.</li>\n    <li>It shows excellent performance in generating content, editing based on reasoning, and following multimodal instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.08765", "authors": [{"_id": "6938da63dfc35938ba129f3c", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:07.767Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3d", "name": "Yefei He", "hidden": false}, {"_id": "6938da63dfc35938ba129f3e", "user": {"_id": "62d812e143df7719860d05d1", "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg", "isPro": false, "fullname": "zhekai chen", "user": "Azily", "type": "user"}, "name": "Zhekai Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:00.513Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3f", "name": "Shiwei Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f40", "user": {"_id": "637ee45b2438d7485b8d8f6a", "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg", "isPro": false, "fullname": "Xiaogang Xu", "user": "xiaogang00", "type": "user"}, "name": "Xiaogang Xu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:51.241Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f41", "name": "Bin Xia", "hidden": false}, {"_id": "6938da63dfc35938ba129f42", "name": "Dingdong Wang", "hidden": false}, {"_id": "6938da63dfc35938ba129f43", "name": "Hongwei Yi", "hidden": false}, {"_id": "6938da63dfc35938ba129f44", "user": {"_id": "65d5ec74cd05bc1eaa125040", "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg", "isPro": false, "fullname": "Xihui Liu", "user": "XihuiLiu", "type": "user"}, "name": "Xihui Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:32.582Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f45", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:26.372Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f46", "name": "Yu Liu", "hidden": false}, {"_id": "6938da63dfc35938ba129f47", "name": "Yingya Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f48", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:10.566Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "publishedAt": "2025-12-09T16:13:55.000Z", "submittedOnDailyAt": "2025-12-10T00:20:18.797Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "upvotes": 94, "discussionId": "6938da64dfc35938ba129f49", "githubRepo": "https://github.com/ali-vilab/Wan-Move", "githubRepoAddedBy": "user", "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.", "ai_keywords": ["motion control", "video generative models", "dense point trajectories", "latent space", "spatiotemporal feature map", "motion guidance", "image-to-video model", "auxiliary motion encoders", "fine-tuning", "MoveBench", "motion annotations"], "githubStars": 197, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>Wan-Move\u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u8fd0\u52a8\u63a7\u5236\u5f15\u5165\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002</li>\n    <li>\u73b0\u6709\u7684\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5\u63a7\u5236\u7cbe\u5ea6\u4f4e\uff0c\u6269\u5c55\u6027\u5dee\uff0c\u5bfc\u81f4\u8f93\u51fa\u6548\u679c\u4e0d\u7406\u60f3\u3002</li>\n    <li>Wan-Move\u901a\u8fc7\u4f7f\u7528\u5bc6\u96c6\u7684\u70b9\u8f68\u8ff9\u6765\u7cbe\u786e\u63a7\u5236\u573a\u666f\u4e2d\u7684\u7269\u4f53\u8fd0\u52a8\u3002</li>\n    <li>\u8be5\u6846\u67b6\u80fd\u591f\u5728\u4e0d\u6539\u53d8\u73b0\u6709\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\u7684\u7ed3\u6784\u4e0b\uff0c\u63d0\u4f9b\u8fd0\u52a8\u6307\u5bfc\u3002</li>\n    <li>Wan-Move\u751f\u6210\u7684480p\u89c6\u9891\u5728\u8fd0\u52a8\u63a7\u5236\u80fd\u529b\u4e0a\u4e0e\u5546\u4e1a\u8f6f\u4ef6\u76f8\u5f53\uff0c\u4e14\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bc4\u4f30\u6807\u51c6MoveBench\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Wan-Move is a new framework that improves motion control in video generation models.</li>\n    <li>It allows for precise and high-quality control over how objects move in videos.</li>\n    <li>The method uses dense point trajectories to guide video synthesis without changing the existing model architecture.</li>\n    <li>Wan-Move can generate 5-second, 480p videos with motion control similar to commercial tools.</li>\n    <li>It includes a benchmark called MoveBench for thorough evaluation, featuring diverse content and high-quality motion annotations.</li>\n</ul>"}, "publishedAt": "2025-12-09T11:13:55.000Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 88, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 69, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u662f\u4e00\u79cd\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5168\u7403\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u7b56\u7565\u3002</li>\n    <li>\u73b0\u6709\u7684\u5185\u5b58\u8bbe\u8ba1\u4e3b\u8981\u4f5c\u4e3a\u88ab\u52a8\u5b58\u50a8\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4fe1\u606f\u4e4b\u95f4\u7684\u9ad8\u9636\u5173\u8054\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HGMem\uff0c\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u5185\u5b58\u673a\u5236\uff0c\u80fd\u52a8\u6001\u5730\u589e\u5f3a\u63a8\u7406\u548c\u7406\u89e3\u80fd\u529b\u3002</li>\n    <li>HGMem\u5c06\u5185\u5b58\u8868\u793a\u4e3a\u8d85\u56fe\uff0c\u4f7f\u5f97\u5185\u5b58\u5355\u5143\u4e4b\u95f4\u7684\u9ad8\u9636\u4e92\u52a8\u5f97\u4ee5\u5f62\u6210\u3002</li>\n    <li>\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHGMem\u5728\u591a\u4e2a\u590d\u6742\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6b65\u9aa4RAG\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps improve large language models (LLMs) for complex tasks needing deep understanding and reasoning.</li>\n    <li>Current memory designs in RAG systems mainly store facts passively, which limits their effectiveness in reasoning and understanding.</li>\n    <li>HGMem is a new memory system that uses a hypergraph structure, allowing for dynamic and complex connections between information.</li>\n    <li>This hypergraph memory helps create stronger links between facts, enhancing reasoning and knowledge development in tasks.</li>\n    <li>Tests show that HGMem significantly improves multi-step RAG performance compared to existing systems across various challenges.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.00393", "authors": [{"_id": "695b2297832867f253525d68", "user": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "name": "Yuxue Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:23.295Z", "hidden": false}, {"_id": "695b2297832867f253525d69", "user": {"_id": "649ecf9827145c4463240177", "avatarUrl": "/avatars/27696cf31790a3d58d8be2e0c983800e.svg", "isPro": false, "fullname": "Lue Fan", "user": "Abyssaledge", "type": "user"}, "name": "Lue Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T13:49:26.330Z", "hidden": false}, {"_id": "695b2297832867f253525d6a", "user": {"_id": "644cc2c36dfd5f8240d76a52", "avatarUrl": "/avatars/dcd9279af1c6d8535e48dc6e3e6511cd.svg", "isPro": false, "fullname": "Ziqi Shi", "user": "renshengjihe", "type": "user"}, "name": "Ziqi Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:21.077Z", "hidden": false}, {"_id": "695b2297832867f253525d6b", "name": "Junran Peng", "hidden": false}, {"_id": "695b2297832867f253525d6c", "name": "Feng Wang", "hidden": false}, {"_id": "695b2297832867f253525d6d", "name": "Zhaoxiang Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "publishedAt": "2026-01-01T17:07:30.000Z", "submittedOnDailyAt": "2026-01-05T02:49:46.994Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "submittedOnDailyBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "upvotes": 83, "discussionId": "695b2297832867f253525d6e", "projectPage": "https://neoverse-4d.github.io/", "githubRepo": "https://github.com/IamCreateAI/NeoVerse", "githubRepoAddedBy": "user", "ai_summary": "NeoVerse is a scalable 4D world model that enables pose-free reconstruction and novel-trajectory video generation from monocular videos with state-of-the-art performance.", "ai_keywords": ["4D world model", "4D reconstruction", "novel-trajectory video generation", "monocular videos", "pose-free", "feed-forward", "degradation pattern simulation"], "githubStars": 107, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aNeoVerse\u7684\u591a\u529f\u80fd4D\u4e16\u754c\u6a21\u578b\u3002</li>\n    <li>NeoVerse\u53ef\u4ee5\u8fdb\u884c4D\u91cd\u5efa\u3001\u751f\u6210\u65b0\u7684\u89c6\u9891\u8f68\u8ff9\uff0c\u5e76\u652f\u6301\u4e30\u5bcc\u7684\u5e94\u7528\u3002</li>\n    <li>\u89e3\u51b3\u4e86\u73b0\u67094D\u5efa\u6a21\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u4e0a\u7684\u9650\u5236\u3002</li>\n    <li>NeoVerse\u91c7\u7528\u65e0\u59ff\u6001\u524d\u9988\u76844D\u91cd\u5efa\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5355\u76ee\u89c6\u9891\u3002</li>\n    <li>\u5728\u91cd\u5efa\u548c\u751f\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNeoVerse\u8868\u73b0\u51fa\u8272\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NeoVerse is a new 4D world model that can create 4D reconstructions and generate videos.</li>\n    <li>Current 4D modeling methods struggle with scalability due to expensive data and complicated training processes.</li>\n    <li>NeoVerse is designed to work well with regular monocular videos, making it more accessible.</li>\n    <li>It includes features like pose-free reconstruction and online simulation to improve flexibility and performance.</li>\n    <li>NeoVerse achieves top results in standard benchmarks for reconstruction and video generation.</li>\n</ul>"}, "publishedAt": "2026-01-01T12:07:30.000Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00393.png", "numComments": 1, "submittedBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "fullname": "Yuxue Yang", "name": "Yuppie1204", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24615", "authors": [{"_id": "69564d96832867f2535257af", "user": {"_id": "622b00a776c20fee5d14501b", "avatarUrl": "/avatars/e00496dda1e309548e7b5b437839bb65.svg", "isPro": false, "fullname": "Eason shi", "user": "Easonshi", "type": "user"}, "name": "Yuchen Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:50.111Z", "hidden": false}, {"_id": "69564d96832867f2535257b0", "user": {"_id": "66e258bdc70c02b46dfed6e3", "avatarUrl": "/avatars/ccc2d604616c018f45a268a610472cac.svg", "isPro": false, "fullname": "Yuzheng Cai", "user": "Ucreate", "type": "user"}, "name": "Yuzheng Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:50.884Z", "hidden": false}, {"_id": "69564d96832867f2535257b1", "name": "Siqi Cai", "hidden": false}, {"_id": "69564d96832867f2535257b2", "name": "Zihan Xu", "hidden": false}, {"_id": "69564d96832867f2535257b3", "user": {"_id": "64154bfa385a75d7790f80e8", "avatarUrl": "/avatars/9e22f54b5eb7c4ebedad99a9a92c4b6a.svg", "isPro": false, "fullname": "Lichao Chen", "user": "nth233", "type": "user"}, "name": "Lichao Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:46.825Z", "hidden": false}, {"_id": "69564d96832867f2535257b4", "user": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "isPro": false, "fullname": "Yulei Qin", "user": "yolay", "type": "user"}, "name": "Yulei Qin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:48.064Z", "hidden": false}, {"_id": "69564d96832867f2535257b5", "name": "Zhijian Zhou", "hidden": false}, {"_id": "69564d96832867f2535257b6", "name": "Xiang Fei", "hidden": false}, {"_id": "69564d96832867f2535257b7", "user": {"_id": "6604e43869c47cd78fdebd08", "avatarUrl": "/avatars/4c11f5e1aeae3c5eb213f6ec6d5bfe72.svg", "isPro": false, "fullname": "Qiu", "user": "ChaofanDFG", "type": "user"}, "name": "Chaofan Qiu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:48.910Z", "hidden": false}, {"_id": "69564d96832867f2535257b8", "user": {"_id": "637af0a7bdf7309aa6da1c36", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637af0a7bdf7309aa6da1c36/NHZ-09otVCfbpXVxm8f-e.png", "isPro": false, "fullname": "Xiaoyu Tan", "user": "WIlliam1900", "type": "user"}, "name": "Xiaoyu Tan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:52.763Z", "hidden": false}, {"_id": "69564d96832867f2535257b9", "name": "Gang Li", "hidden": false}, {"_id": "69564d96832867f2535257ba", "name": "Zongyi Li", "hidden": false}, {"_id": "69564d96832867f2535257bb", "name": "Haojia Lin", "hidden": false}, {"_id": "69564d96832867f2535257bc", "name": "Guocan Cai", "hidden": false}, {"_id": "69564d96832867f2535257bd", "name": "Yong Mao", "hidden": false}, {"_id": "69564d96832867f2535257be", "name": "Yunsheng Wu", "hidden": false}, {"_id": "69564d96832867f2535257bf", "name": "Ke Li", "hidden": false}, {"_id": "69564d96832867f2535257c0", "user": {"_id": "647401e50da364bd0d002f2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png", "isPro": false, "fullname": "XING SUN", "user": "tedsun", "type": "user"}, "name": "Xing Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:39.390Z", "hidden": false}], "publishedAt": "2025-12-31T04:17:36.000Z", "submittedOnDailyAt": "2026-01-05T00:21:56.456Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "submittedOnDailyBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "isPro": false, "fullname": "Ke Li", "user": "tristanli", "type": "user"}, "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "upvotes": 82, "discussionId": "69564d96832867f2535257c1", "projectPage": "https://tencentcloudadp.github.io/youtu-agent/", "githubRepo": "https://github.com/TencentCloudADP/youtu-agent", "githubRepoAddedBy": "user", "githubStars": 4095, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u6846\u67b6\u9762\u4e34\u9ad8\u914d\u7f6e\u6210\u672c\u548c\u9759\u6001\u80fd\u529b\u7684\u6311\u6218\u3002</li>\n    <li>Youtu-Agent \u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u65e8\u5728\u81ea\u52a8\u751f\u6210\u548c\u6301\u7eed\u8fdb\u5316 LLM \u4ee3\u7406\u3002</li>\n    <li>\u5b83\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7684\u914d\u7f6e\u7cfb\u7edf\uff0c\u652f\u6301\u7075\u6d3b\u91cd\u7528\u548c\u81ea\u52a8\u5408\u6210\u3002</li>\n    <li>Youtu-Agent \u6709\u4e24\u79cd\u751f\u6210\u6a21\u5f0f\uff1a\u5de5\u4f5c\u6d41\u6a21\u5f0f\u548c\u5143\u4ee3\u7406\u6a21\u5f0f\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u5de5\u5177\u4ee3\u7801\u548c\u914d\u7f6e\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cYoutu-Agent \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u751f\u6210\u6548\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Youtu-Agent is a new framework that helps create and improve Large Language Model (LLM) agents more easily and efficiently.</li>\n  <li>It reduces the need for manual work by allowing flexible tool integration and automatic generation of agent configurations.</li>\n  <li>Youtu-Agent offers two modes: a simple Workflow mode for regular tasks and a Meta-Agent mode for more complex needs.</li>\n  <li>The framework includes a system for agents to learn and adapt through practice without needing to change their underlying parameters.</li>\n  <li>Tests show Youtu-Agent performs well on various benchmarks, with improvements in coding, reasoning, and search capabilities.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:17:36.000Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24615.png", "numComments": 1, "submittedBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "fullname": "Ke Li", "name": "tristanli", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u57fa\u7840\u6a21\u578b\u80fd\u591f\u751f\u6210\u89c6\u89c9\u4e0a\u771f\u5b9e\u548c\u65f6\u95f4\u4e0a\u8fde\u8d2f\u7684\u5185\u5bb9\uff0c\u4f46\u5b83\u4eec\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u662f\u5426\u6355\u6349\u5230\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7ea6\u675f\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u6307\u6807\u5982\u5f17\u96f7\u6b47\u89c6\u9891\u8ddd\u79bb\uff08FVD\uff09\u4fa7\u91cd\u4e8e\u611f\u77e5\u8d28\u91cf\uff0c\u800c\u5ffd\u89c6\u4e86\u56e0\u679c\u5173\u7cfb\u3001\u7269\u7406\u89c4\u5f8b\u548c\u5168\u5c40\u4e00\u81f4\u6027\u7b49\u63a8\u7406\u5931\u8d25\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MMGR\uff08\u591a\u6a21\u6001\u751f\u6210\u63a8\u7406\u8bc4\u4f30\u4e0e\u57fa\u51c6\uff09\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8bc4\u4f30\uff1a\u7269\u7406\u3001\u903b\u8f91\u3001\u4e09\u7ef4\u7a7a\u95f4\u3001\u4e8c\u7ef4\u7a7a\u95f4\u548c\u65f6\u95f4\u3002</li>\n    <li>MMGR\u5728\u4e09\u4e2a\u9886\u57df\u8fdb\u884c\u8bc4\u4f30\uff1a\u62bd\u8c61\u63a8\u7406\u3001\u5177\u8eab\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\uff0c\u91c7\u7528\u7ec6\u81f4\u7684\u6307\u6807\u8981\u6c42\u89c6\u9891\u548c\u56fe\u50cf\u751f\u6210\u7684\u6574\u4f53\u6b63\u786e\u6027\u3002</li>\n    <li>\u6211\u4eec\u7684\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u4e3b\u8981\u5c40\u9650\u6027\uff0c\u5305\u62ec\u8fc7\u5ea6\u4f9d\u8d56\u611f\u77e5\u6570\u636e\u3001\u5168\u7403\u72b6\u6001\u4e00\u81f4\u6027\u5dee\u548c\u5956\u52b1\u89c6\u89c9\u5408\u7406\u6027\u800c\u975e\u56e0\u679c\u6b63\u786e\u6027\u7684\u76ee\u6807\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video models can create realistic content, but they need to accurately understand physical and logical rules to be reliable.</li>\n    <li>Current evaluation methods focus too much on visual quality and miss reasoning mistakes like breaking causality or physics rules.</li>\n    <li>MMGR is a new evaluation framework that checks five reasoning skills: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>\n    <li>MMGR tests how well models reason in areas like puzzles, real-world navigation, and physical interactions.</li>\n    <li>Benchmarking shows that while models do okay with physical reasoning, they struggle with abstract reasoning and long-term planning.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u79d1\u5b66\u901a\u7528\u667a\u80fd\uff08SGI\uff09\u662f\u6307\u80fd\u81ea\u4e3b\u8fdb\u884c\u79d1\u5b66\u6784\u601d\u3001\u8c03\u67e5\u548c\u63a8\u7406\u7684\u80fd\u529b\uff0c\u76ee\u524d\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u3002</li>\n    <li>\u6211\u4eec\u57fa\u4e8e\u5b9e\u7528\u63a2\u7a76\u6a21\u578b\uff08PIM\uff09\u63d0\u51fa\u4e86SGI\u7684\u64cd\u4f5c\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u4e0e\u79d1\u5b66\u5bb6\u76f8\u5173\u7684\u4efb\u52a1\u8fdb\u884c\u5b9e\u9645\u5e94\u7528\u3002</li>\n    <li>SGI-Bench\u5305\u542b\u8d85\u8fc71000\u4e2a\u4e13\u5bb6\u7b56\u5212\u7684\u8de8\u5b66\u79d1\u6837\u672c\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u6df1\u5ea6\u7814\u7a76\u7684\u51c6\u786e\u5339\u914d\u7387\u4f4e\uff0810-20%\uff09\uff0c\u521b\u610f\u7f3a\u4e4f\u53ef\u884c\u6027\u548c\u7ec6\u8282\uff0c\u5e72\u5b9e\u9a8c\u7684\u6267\u884c\u7ed3\u679c\u51c6\u786e\u6027\u4f4e\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\uff0c\u4f18\u5316\u63a8\u7406\u65f6\u7684\u65b0\u9896\u6027\u5956\u52b1\uff0c\u4fc3\u8fdb\u5047\u8bbe\u7684\u65b0\u9896\u6027\uff0c\u63a8\u52a8AI\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u4f5c\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is currently no clear framework for Scientific General Intelligence (SGI), which is the ability of AI to independently think and work across different scientific fields.</li>\n    <li>We define SGI using the Practical Inquiry Model (PIM) and create tasks for it, including deep research, idea generation, experiments, and reasoning.</li>\n    <li>SGI-Bench includes over 1,000 curated examples to evaluate AI models based on significant scientific questions.</li>\n    <li>Results show that AI struggles with deep research accuracy, idea feasibility, and executing experiments correctly.</li>\n    <li>We propose Test-Time Reinforcement Learning (TTRL) to improve AI's ability to generate novel hypotheses during evaluation, without needing a reference answer.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20619", "authors": [{"_id": "694b614d746a34b55dd53d1a", "name": "Jianhong Bai", "hidden": false}, {"_id": "694b614d746a34b55dd53d1b", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "694b614d746a34b55dd53d1c", "name": "Xintao Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1d", "name": "Fu Xiao", "hidden": false}, {"_id": "694b614d746a34b55dd53d1e", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1f", "name": "Qinghe Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d20", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "694b614d746a34b55dd53d21", "name": "Menghan Xia", "hidden": false}, {"_id": "694b614d746a34b55dd53d22", "name": "Zuozhu Liu", "hidden": false}, {"_id": "694b614d746a34b55dd53d23", "name": "Haoji Hu", "hidden": false}, {"_id": "694b614d746a34b55dd53d24", "name": "Pengfei Wan", "hidden": false}, {"_id": "694b614d746a34b55dd53d25", "name": "Kun Gai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "publishedAt": "2025-12-23T18:59:56.000Z", "submittedOnDailyAt": "2025-12-24T01:20:51.117Z", "title": "SemanticGen: Video Generation in Semantic Space", "submittedOnDailyBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "isPro": false, "fullname": "Jianhong Bai", "user": "jianhongbai", "type": "user"}, "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "upvotes": 77, "discussionId": "694b614d746a34b55dd53d26", "projectPage": "https://jianhongbai.github.io/SemanticGen/", "ai_summary": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.", "ai_keywords": ["VAE space", "VAE decoder", "semantic space", "diffusion model", "semantic video features", "bi-directional attention"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u901a\u5e38\u5728VAE\u7a7a\u95f4\u4e2d\u5b66\u4e60\u89c6\u9891\u7684\u6f5c\u5728\u5206\u5e03\uff0c\u5e76\u4f7f\u7528VAE\u89e3\u7801\u5668\u6620\u5c04\u5230\u50cf\u7d20\u3002</li>\n    <li>\u8fd9\u79cd\u65b9\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u4f46\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u6536\u655b\u6162\u4e14\u8ba1\u7b97\u5f00\u9500\u5927\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5SemanticGen\uff0c\u901a\u8fc7\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n    <li>SemanticGen\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u8fc7\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u7d27\u51d1\u7684\u8bed\u4e49\u89c6\u9891\u7279\u5f81\uff0c\u7b2c\u4e8c\u9636\u6bb5\u751f\u6210VAE\u6f5c\u5728\u53d8\u91cf\u4ee5\u8f93\u51fa\u6700\u7ec8\u89c6\u9891\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSemanticGen\u80fd\u5feb\u901f\u6536\u655b\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u5e76\u4e14\u5728\u957f\u89c6\u9891\u751f\u6210\u4e0a\u4e5f\u6709\u6548\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>SemanticGen is a new method for generating videos that addresses slow convergence and high computational costs in traditional models.</li>\n    <li>It generates videos in a compact, high-level semantic space instead of directly using low-level video data.</li>\n    <li>The process has two stages: first, it creates a global layout of the video using a diffusion model, then adds details with another diffusion model.</li>\n    <li>This approach allows for faster video generation and better performance, especially for long videos.</li>\n    <li>Tests show that SemanticGen creates high-quality videos that surpass existing methods.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:56.000Z", "title": "SemanticGen: Video Generation in Semantic Space", "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png", "numComments": 2, "submittedBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "fullname": "Jianhong Bai", "name": "jianhongbai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.15431", "authors": [{"_id": "69437417542d62d58a7bf6c4", "name": "Haolong Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6c5", "name": "Jia Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c6", "name": "Xin Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c7", "name": "Yeqing Shen", "hidden": false}, {"_id": "69437417542d62d58a7bf6c8", "user": {"_id": "653614073f4248157d60ccdc", "avatarUrl": "/avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg", "isPro": false, "fullname": "mengziyang", "user": "zylate", "type": "user"}, "name": "Ziyang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:53.033Z", "hidden": false}, {"_id": "69437417542d62d58a7bf6c9", "name": "Zhimin Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6ca", "name": "Kaijun Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6cb", "name": "Jin Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf6cc", "name": "Lieyu Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6cd", "name": "Mi Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6ce", "name": "Shiliang Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6cf", "name": "Zhirui Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d0", "name": "Brian Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d1", "name": "Kang An", "hidden": false}, {"_id": "69437417542d62d58a7bf6d2", "name": "Chenyang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d3", "name": "Lei Lei", "hidden": false}, {"_id": "69437417542d62d58a7bf6d4", "name": "Mengmeng Duan", "hidden": false}, {"_id": "69437417542d62d58a7bf6d5", "name": "Danxun Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d6", "name": "Guodong Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d7", "name": "Hang Cheng", "hidden": false}, {"_id": "69437417542d62d58a7bf6d8", "name": "Hao Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d9", "name": "Jie Dong", "hidden": false}, {"_id": "69437417542d62d58a7bf6da", "name": "Junhao Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6db", "name": "Mei Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6dc", "name": "Renjie Yu", "hidden": false}, {"_id": "69437417542d62d58a7bf6dd", "name": "Shunshan Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6de", "name": "Xu Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf6df", "name": "Yiting Dai", "hidden": false}, {"_id": "69437417542d62d58a7bf6e0", "name": "Yineng Deng", "hidden": false}, {"_id": "69437417542d62d58a7bf6e1", "name": "Yingdan Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6e2", "name": "Zelin Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6e3", "name": "Wen Sun", "hidden": false}, {"_id": "69437417542d62d58a7bf6e4", "name": "Chengxu Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e5", "name": "Chunqin Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6e6", "name": "Dong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6e7", "name": "Fengqiong Xiao", "hidden": false}, {"_id": "69437417542d62d58a7bf6e8", "name": "Guanghao Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e9", "name": "Guopeng Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ea", "name": "Guozhen Peng", "hidden": false}, {"_id": "69437417542d62d58a7bf6eb", "name": "Hongbing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ec", "name": "Hang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ed", "name": "Hongming Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6ee", "name": "Jingjing Xie", "hidden": false}, {"_id": "69437417542d62d58a7bf6ef", "name": "Jianyong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6f0", "name": "Jingyang Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf6f1", "name": "Jiaju Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f2", "name": "Jiayu Yuan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f3", "name": "Jianpeng Yin", "hidden": false}, {"_id": "69437417542d62d58a7bf6f4", "name": "Kai Cao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f5", "name": "Liang Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f6", "name": "Liguo Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f7", "name": "Liying Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6f8", "name": "Mengqiang Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f9", "name": "Min Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fa", "name": "Manjiao Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fb", "name": "Mao Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf6fc", "name": "Mingxin Wan", "hidden": false}, {"_id": "69437417542d62d58a7bf6fd", "name": "Na Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6fe", "name": "Nan Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6ff", "name": "Ning Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf700", "name": "Peiyao Ma", "hidden": false}, {"_id": "69437417542d62d58a7bf701", "name": "Qingzhou Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf702", "name": "Qiao Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf703", "name": "Qinlin Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf704", "name": "Qiong Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf705", "name": "Qiongyao Li", "hidden": false}, {"_id": "69437417542d62d58a7bf706", "name": "Shangwu Zhong", "hidden": false}, {"_id": "69437417542d62d58a7bf707", "name": "Shuli Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf708", "name": "Shaofan Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf709", "name": "Shisi Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf70a", "name": "Shuang Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf70b", "name": "Xingbin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70c", "name": "Xiaojia Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70d", "name": "Xiaojie Hou", "hidden": false}, {"_id": "69437417542d62d58a7bf70e", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70f", "name": "Xuanti Feng", "hidden": false}, {"_id": "69437417542d62d58a7bf710", "name": "Xuedan Cai", "hidden": false}, {"_id": "69437417542d62d58a7bf711", "name": "Xuan Wen", "hidden": false}, {"_id": "69437417542d62d58a7bf712", "name": "Xianwei Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf713", "name": "Xin Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf714", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf715", "name": "Xin Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf716", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf717", "name": "Yukang Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf718", "name": "Yunfang Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf719", "name": "Yuqing Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf71a", "name": "Yixun Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf71b", "name": "Zejia Weng", "hidden": false}, {"_id": "69437417542d62d58a7bf71c", "name": "Zhonghao Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf71d", "name": "Zhiguo Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf71e", "name": "Zhuoyu Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf71f", "name": "Zheng Ge", "hidden": false}, {"_id": "69437417542d62d58a7bf720", "name": "Jing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf721", "name": "Yibo Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf722", "name": "Binxing Jiao", "hidden": false}, {"_id": "69437417542d62d58a7bf723", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf724", "name": "Daxin Jiang", "hidden": false}], "publishedAt": "2025-12-17T13:26:30.000Z", "submittedOnDailyAt": "2025-12-18T00:55:26.804Z", "title": "Step-GUI Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "upvotes": 77, "discussionId": "69437418542d62d58a7bf725", "projectPage": "https://opengelab.github.io/", "githubRepo": "https://github.com/stepfun-ai/gelab-zero", "githubRepoAddedBy": "user", "ai_summary": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.", "ai_keywords": ["multimodal large language models", "GUI automation", "self-evolving training pipeline", "Calibrated Step Reward System", "trajectory-level calibration", "Step-GUI", "GUI performance", "GUI-MCP", "Model Context Protocol", "AndroidWorld", "OSWorld", "ScreenShot-Pro", "AndroidDaily", "real-world mobile usage patterns", "hierarchical architecture", "low-level atomic operations", "high-level task delegation", "local specialist models", "high-privacy execution"], "githubStars": 1417, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6211\u6f14\u53d8\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u901a\u8fc7\u6821\u51c6\u6b65\u9aa4\u5956\u52b1\u7cfb\u7edf\u63d0\u9ad8\u6570\u636e\u7684\u53ef\u9760\u6027\uff0c\u964d\u4f4e\u6210\u672c10-100\u500d\u3002</li>\n    <li>\u5f15\u5165\u4e86Step-GUI\u6a21\u578b\u7cfb\u5217\uff0c\u53d6\u5f97\u4e86\u5728GUI\u81ea\u52a8\u5316\u65b9\u9762\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u901a\u7528\u80fd\u529b\u3002</li>\n    <li>\u4e3a\u4e86\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u63d0\u51fa\u4e86GUI-MCP\u534f\u8bae\uff0c\u7ed3\u5408\u4f4e\u7ea7\u64cd\u4f5c\u548c\u9ad8\u7ea7\u4efb\u52a1\u59d4\u6d3e\uff0c\u786e\u4fdd\u654f\u611f\u6570\u636e\u5728\u8bbe\u5907\u4e0a\u5904\u7406\u3002</li>\n    <li>\u63a8\u51fa\u4e86AndroidDaily\u57fa\u51c6\uff0c\u57fa\u4e8e\u771f\u5b9e\u7684\u79fb\u52a8\u4f7f\u7528\u6a21\u5f0f\uff0c\u6d4b\u8bd5\u6a21\u578b\u5728\u65e5\u5e38\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002</li>\n    <li>\u6211\u4eec\u7684\u5de5\u4f5c\u63a8\u52a8\u4e86\u5b9e\u7528GUI\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u5c55\u793a\u4e86\u5176\u5728\u65e5\u5e38\u6570\u5b57\u4ea4\u4e92\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New models for GUI automation are created to improve how software interfaces are used.</li>\n    <li>A self-evolving training system helps gather accurate training data cost-effectively.</li>\n    <li>The new models, called Step-GUI, achieve high performance in various GUI tasks.</li>\n    <li>A new protocol, GUI-MCP, is introduced to ensure user privacy while interacting with devices.</li>\n    <li>A benchmark called AndroidDaily is developed to test how well these models perform in real-life scenarios.</li>\n</ul>"}, "publishedAt": "2025-12-17T08:26:30.000Z", "title": "Step-GUI Technical Report", "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15431.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Jan 07, 2026";
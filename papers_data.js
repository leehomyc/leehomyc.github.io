window.trendingPapers = {
    "today": [{"paper": {"id": "2512.07461", "authors": [{"_id": "6937b96219d912300c34a398", "user": {"_id": "626b889ff451470f861d8c78", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651214465695-noauth.jpeg", "isPro": false, "fullname": "victor wu", "user": "victor-wu", "type": "user"}, "name": "Tong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:22.731Z", "hidden": false}, {"_id": "6937b96219d912300c34a399", "user": {"_id": "6191cc9e6d34e827404cebab", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674119843175-6191cc9e6d34e827404cebab.jpeg", "isPro": false, "fullname": "Yang", "user": "jacklanda", "type": "user"}, "name": "Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:20.278Z", "hidden": false}, {"_id": "6937b96219d912300c34a39a", "user": {"_id": "624505fcd083d28d314de3dd", "avatarUrl": "/avatars/92cf6b6a1d81d7958dbbd21f0bf63f8f.svg", "isPro": false, "fullname": "bai jun", "user": "ba1jun", "type": "user"}, "name": "Jun Bai", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:17.404Z", "hidden": false}, {"_id": "6937b96219d912300c34a39b", "name": "Zixia Jia", "hidden": false}, {"_id": "6937b96219d912300c34a39c", "name": "Shuyi Zhang", "hidden": false}, {"_id": "6937b96219d912300c34a39d", "name": "Ziyong Lin", "hidden": false}, {"_id": "6937b96219d912300c34a39e", "user": {"_id": "64b119c4372d43407723136b", "avatarUrl": "/avatars/d523e181993eea06b7f6a71a592c995e.svg", "isPro": false, "fullname": "YANTING WANG", "user": "Noane", "type": "user"}, "name": "Yanting Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:14.418Z", "hidden": false}, {"_id": "6937b96219d912300c34a39f", "name": "Song-Chun Zhu", "hidden": false}, {"_id": "6937b96219d912300c34a3a0", "name": "Zilong Zheng", "hidden": false}], "publishedAt": "2025-12-08T11:39:43.000Z", "submittedOnDailyAt": "2025-12-09T04:12:55.960Z", "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning", "submittedOnDailyBy": {"_id": "63a95a6a7930fa8c7dd63d4e", "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg", "isPro": false, "fullname": "Zilong Zheng", "user": "zlzheng", "type": "user"}, "summary": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.", "upvotes": 49, "discussionId": "6937b96219d912300c34a3a1", "projectPage": "https://bigai-nlco.github.io/Native-Parallel-Reasoner/", "githubRepo": "https://github.com/bigai-nlco/Native-Parallel-Reasoner", "ai_summary": "NPR, a teacher-free framework, enhances Large Language Models with native parallel reasoning capabilities through self-distilled training, Parallel-Aware Policy Optimization, and a robust NPR Engine, achieving substantial performance and speed improvements.", "ai_keywords": ["Native Parallel Reasoner", "Large Language Models", "self-evolve", "parallel reasoning", "self-distilled progressive training", "cold-start format discovery", "topological constraints", "Parallel-Aware Policy Optimization", "branching policies", "execution graph", "adaptive decomposition", "trial and error", "NPR Engine", "memory management", "flow control", "parallel RL training", "reasoning benchmarks", "Qwen3-4B", "genuine parallel execution", "autoregressive decoding", "agentic reasoning"], "githubStars": 18, "organization": {"_id": "63a95ac93453852ef5399a77", "name": "bigai", "fullname": "Beijing Institute for General Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1672043197974-63a95a6a7930fa8c7dd63d4e.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u539f\u751f\u5e76\u884c\u63a8\u7406\u5668\u201d\uff08NPR\uff09\u7684\u6846\u67b6\uff0c\u65e8\u5728\u5e2e\u52a9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u6211\u53d1\u5c55\u5e76\u884c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>NPR\u901a\u8fc7\u4e09\u9879\u521b\u65b0\uff0c\u5c06\u6a21\u578b\u4ece\u987a\u5e8f\u6267\u884c\u8f6c\u53d8\u4e3a\u539f\u751f\u5e76\u884c\u8ba4\u77e5\u3002</li>\n    <li>\u91c7\u7528\u81ea\u6211\u84b8\u998f\u7684\u6e10\u8fdb\u8bad\u7ec3\u65b9\u6cd5\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\uff0c\u9010\u6b65\u5b9e\u73b0\u4e25\u683c\u7684\u62d3\u6251\u7ea6\u675f\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5\u201c\u5e76\u884c\u611f\u77e5\u7b56\u7565\u4f18\u5316\u201d\uff08PAPO\uff09\uff0c\u901a\u8fc7\u8bd5\u9519\u6cd5\u4f18\u5316\u6267\u884c\u56fe\u4e2d\u7684\u5206\u652f\u7b56\u7565\u3002</li>\n    <li>NPR\u5728\u516b\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u63d0\u5347\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe24.5%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u81f34.6\u500d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Native Parallel Reasoner (NPR) helps Large Language Models (LLMs) develop true parallel reasoning skills without needing a teacher.</li>\n    <li>NPR uses three main innovations: a training method that evolves the model\u2019s format, a new algorithm for optimizing decision-making, and an engine that enhances memory and control for better performance.</li>\n    <li>Using NPR, the Qwen3-4B model showed significant improvements, with up to 24.5% better performance and 4.6 times faster inference speed on eight reasoning tests.</li>\n    <li>NPR achieves 100% genuine parallel execution, setting a new benchmark for efficient and scalable reasoning in models.</li>\n</ul>"}, "publishedAt": "2025-12-08T06:39:43.000Z", "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning", "summary": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07461.png", "numComments": 1, "submittedBy": {"_id": "63a95a6a7930fa8c7dd63d4e", "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg", "fullname": "Zilong Zheng", "name": "zlzheng", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "63a95ac93453852ef5399a77", "name": "bigai", "fullname": "Beijing Institute for General Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1672043197974-63a95a6a7930fa8c7dd63d4e.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.07525", "authors": [{"_id": "693794d319d912300c34a291", "user": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "isPro": false, "fullname": "Xiaoran Liu (SII)", "user": "SII-xrliu", "type": "user"}, "name": "Xiaoran Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:38:01.257Z", "hidden": false}, {"_id": "693794d319d912300c34a292", "name": "Yuerong Song", "hidden": false}, {"_id": "693794d319d912300c34a293", "name": "Zhigeng Liu", "hidden": false}, {"_id": "693794d319d912300c34a294", "user": {"_id": "64805e6dde559d48dbb00627", "avatarUrl": "/avatars/29ca34546411dcc28bbc934e3c26a2ba.svg", "isPro": false, "fullname": "Zengfeng", "user": "ZengfengHuang", "type": "user"}, "name": "Zengfeng Huang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T10:35:11.754Z", "hidden": false}, {"_id": "693794d319d912300c34a295", "user": {"_id": "6491cd52b1e5d3444528edb1", "avatarUrl": "/avatars/a85635d886c7f157b6723dec5c01c030.svg", "isPro": false, "fullname": "Qipeng Guo", "user": "QipengGuo", "type": "user"}, "name": "Qipeng Guo", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T10:35:18.117Z", "hidden": false}, {"_id": "693794d319d912300c34a296", "name": "Zhaoxiang Liu", "hidden": false}, {"_id": "693794d319d912300c34a297", "name": "Shiguo Lian", "hidden": false}, {"_id": "693794d319d912300c34a298", "user": {"_id": "64de18f41d826d7355c285e7", "avatarUrl": "/avatars/23e2c44e3a593415becc02463980f6e8.svg", "isPro": false, "fullname": "Ziwei He", "user": "ziweihe", "type": "user"}, "name": "Ziwei He", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T20:17:41.271Z", "hidden": false}, {"_id": "693794d319d912300c34a299", "user": {"_id": "61457b8deff2c9fdb4de4988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg", "isPro": false, "fullname": "Xipeng Qiu", "user": "xpqiu", "type": "user"}, "name": "Xipeng Qiu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T10:35:28.411Z", "hidden": false}], "publishedAt": "2025-12-08T12:59:54.000Z", "submittedOnDailyAt": "2025-12-09T00:49:29.234Z", "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs", "submittedOnDailyBy": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "isPro": false, "fullname": "Xiaoran Liu (SII)", "user": "SII-xrliu", "type": "user"}, "summary": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.", "upvotes": 41, "discussionId": "693794d419d912300c34a29a", "githubRepo": "https://github.com/OpenMOSS/rope_pp", "ai_summary": "The paper proposes a method to enhance Rotary Position Embeddings by utilizing both the real and imaginary components of the complex-valued dot product, improving long-context modeling in Large Language Models.", "ai_keywords": ["Rotary Position Embeddings", "Large Language Models", "complex-valued dot product", "attention score", "long-context dependencies", "positional information"], "githubStars": 12, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08RoPE\uff09\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7f16\u7801\u5e8f\u5217\u987a\u5e8f\u7684\u6807\u51c6\u65b9\u6cd5\u3002</li>\n    <li>\u76ee\u524d\u7684\u5b9e\u73b0\u53ea\u4f7f\u7528\u590d\u6570\u70b9\u79ef\u7684\u5b9e\u90e8\u6765\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\uff0c\u5ffd\u7565\u4e86\u5305\u542b\u91cd\u8981\u4fe1\u606f\u7684\u865a\u90e8\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u65b9\u6cd5\uff0c\u91cd\u65b0\u5f15\u5165\u4e86\u88ab\u4e22\u5f03\u7684\u865a\u90e8\uff0c\u5229\u7528\u5b8c\u6574\u7684\u590d\u6570\u8868\u793a\u6765\u521b\u5efa\u53cc\u7ec4\u4ef6\u6ce8\u610f\u529b\u5206\u6570\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u5efa\u6a21\u957f\u4e0a\u4e0b\u6587\u4f9d\u8d56\u5173\u7cfb\uff0c\u4fdd\u7559\u66f4\u591a\u4f4d\u7f6e\u4fe1\u606f\u3002</li>\n    <li>\u5728\u591a\u9879\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6\u7684RoPE\uff0c\u5c24\u5176\u662f\u5728\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u65f6\u6548\u679c\u66f4\u663e\u8457\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Rotary Position Embeddings (RoPE) help Large Language Models (LLMs) understand the order of sequences using rotations in the complex plane.</li>\n  <li>Current methods only use the real part of complex calculations for attention scores, ignoring the imaginary part that holds important information.</li>\n  <li>This paper introduces a new method that includes the imaginary part to create a dual-component attention score, improving how models handle long-context relationships.</li>\n  <li>The new approach is shown to enhance performance on long-context language tasks, especially as the context length increases.</li>\n  <li>The code for this new method can be found at https://github.com/OpenMOSS/rope_pp.</li>\n</ul>"}, "publishedAt": "2025-12-08T07:59:54.000Z", "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs", "summary": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07525.png", "numComments": 1, "submittedBy": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "fullname": "Xiaoran Liu (SII)", "name": "SII-xrliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07469", "authors": [{"_id": "69379e0319d912300c34a2fb", "user": {"_id": "6486df66373f79a52913e017", "avatarUrl": "/avatars/4741683fbbcec3a615d0a8df62bc6fec.svg", "isPro": false, "fullname": "Xiangpeng Yang", "user": "XiangpengYang", "type": "user"}, "name": "Xiangpeng Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T10:34:33.919Z", "hidden": false}, {"_id": "69379e0319d912300c34a2fc", "name": "Ji Xie", "hidden": false}, {"_id": "69379e0319d912300c34a2fd", "name": "Yiyuan Yang", "hidden": false}, {"_id": "69379e0319d912300c34a2fe", "name": "Yan Huang", "hidden": false}, {"_id": "69379e0319d912300c34a2ff", "name": "Min Xu", "hidden": false}, {"_id": "69379e0319d912300c34a300", "name": "Qiang Wu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6486df66373f79a52913e017/iKPp57sIku0K9HVBgHjuu.mp4"], "publishedAt": "2025-12-08T11:50:18.000Z", "submittedOnDailyAt": "2025-12-09T01:34:28.853Z", "title": "Unified Video Editing with Temporal Reasoner", "submittedOnDailyBy": {"_id": "6486df66373f79a52913e017", "avatarUrl": "/avatars/4741683fbbcec3a615d0a8df62bc6fec.svg", "isPro": false, "fullname": "Xiangpeng Yang", "user": "XiangpengYang", "type": "user"}, "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.", "upvotes": 31, "discussionId": "69379e0319d912300c34a301", "projectPage": "https://videocof.github.io/", "githubRepo": "https://github.com/knightyxp/VideoCoF", "ai_summary": "VideoCoF, a Chain-of-Frames approach, improves video editing precision and instruction-to-region mapping by using reasoning tokens without requiring user-provided masks.", "ai_keywords": ["chain-of-frames", "chain-of-thought reasoning", "video diffusion model", "reasoning tokens", "edit-region latents", "target video tokens", "instruction-to-region alignment", "fine-grained video editing", "RoPE alignment", "motion alignment", "length extrapolation"], "githubStars": 25, "organization": {"_id": "67c4a2574f5d0005fd418d85", "name": "staraj3", "fullname": "University of Technology Sydney", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67c4a1ab71e55dc41e273b5a/8l4iiw2XH5nbIlLURt7ep.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u5728\u7cbe\u786e\u6027\u548c\u7edf\u4e00\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002</li>\n    <li>VideoCoF\u662f\u4e00\u79cd\u65b0\u7684\u94fe\u5e27\u65b9\u6cd5\uff0c\u501f\u9274\u4e86\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u201c\u5148\u770b\uff0c\u540e\u63a8\u7406\uff0c\u518d\u7f16\u8f91\u201d\u7684\u6b65\u9aa4\u6765\u63d0\u9ad8\u89c6\u9891\u7f16\u8f91\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>\u5f15\u5165\u4e86RoPE\u5bf9\u9f50\u7b56\u7565\uff0c\u4ee5\u786e\u4fdd\u8fd0\u52a8\u5bf9\u9f50\u548c\u5ef6\u957f\u89c6\u9891\u957f\u5ea6\u3002</li>\n    <li>\u4f7f\u752850k\u89c6\u9891\u5bf9\uff0cVideoCoF\u5728\u6027\u80fd\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current video editing methods struggle between precision with expert models that need specific inputs, and simplified models that lack accuracy.</li>\n    <li>VideoCoF is a new approach that uses a \"see, reason, then edit\" method, allowing for better video editing without needing user masks.</li>\n    <li>This method includes a reasoning step that helps align instructions with specific video regions for more accurate edits.</li>\n    <li>VideoCoF uses a RoPE alignment strategy to maintain motion consistency and extend video length beyond what was originally trained.</li>\n    <li>With only 50k video pairs, VideoCoF shows top performance on VideoCoF-Bench, proving it is both efficient and effective.</li>\n</ul>"}, "publishedAt": "2025-12-08T06:50:18.000Z", "title": "Unified Video Editing with Temporal Reasoner", "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6486df66373f79a52913e017/iKPp57sIku0K9HVBgHjuu.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07469.png", "numComments": 5, "submittedBy": {"_id": "6486df66373f79a52913e017", "avatarUrl": "/avatars/4741683fbbcec3a615d0a8df62bc6fec.svg", "fullname": "Xiangpeng Yang", "name": "XiangpengYang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "67c4a2574f5d0005fd418d85", "name": "staraj3", "fullname": "University of Technology Sydney", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67c4a1ab71e55dc41e273b5a/8l4iiw2XH5nbIlLURt7ep.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07834", "authors": [{"_id": "69379f8019d912300c34a30d", "user": {"_id": "67178582bc4492cad19a1f14", "avatarUrl": "/avatars/f2481c0c70a857a862d887beb05c428e.svg", "isPro": false, "fullname": "Yi-Chuan Huang", "user": "YiChuanH", "type": "user"}, "name": "Yi-Chuan Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:37:53.207Z", "hidden": false}, {"_id": "69379f8019d912300c34a30e", "user": {"_id": "6630a67d6b3957f5983658b1", "avatarUrl": "/avatars/4cc0c264e0d8cc0ebc346bb9b1abe8de.svg", "isPro": false, "fullname": "Chan Jiewen", "user": "JiewenChan", "type": "user"}, "name": "Jiewen Chan", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T11:13:18.274Z", "hidden": false}, {"_id": "69379f8019d912300c34a30f", "user": {"_id": "6784959669a180a9949ec5d9", "avatarUrl": "/avatars/b1fcc9a00a8e6e2baaa338c4b04ea9e2.svg", "isPro": false, "fullname": "Hao-Jen Chien", "user": "chien90190", "type": "user"}, "name": "Hao-Jen Chien", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T11:13:24.865Z", "hidden": false}, {"_id": "69379f8019d912300c34a310", "name": "Yu-Lun Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xmWIqFounUj-kosdkGMIN.mp4"], "publishedAt": "2025-12-08T18:59:58.000Z", "submittedOnDailyAt": "2025-12-09T01:33:21.818Z", "title": "Voxify3D: Pixel Art Meets Volumetric Rendering", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/", "upvotes": 28, "discussionId": "69379f8019d912300c34a311", "projectPage": "https://yichuanh.github.io/Voxify-3D/", "ai_summary": "Voxify3D is a two-stage framework that combines 3D mesh optimization with 2D pixel art supervision to generate high-quality voxel art with semantic preservation, pixel-art aesthetics, and discrete color coherence.", "ai_keywords": ["orthographic pixel art supervision", "patch-based CLIP alignment", "Gumbel-Softmax quantization", "volumetric rendering", "CLIP-IQA", "user preference"], "summary_zh": "<ul>\n    <li>Voxel\u827a\u672f\u662f\u4e00\u79cd\u5728\u6e38\u620f\u548c\u6570\u5b57\u5a92\u4f53\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u72ec\u7279\u98ce\u683c\uff0c\u4f46\u4ece3D\u6a21\u578b\u81ea\u52a8\u751f\u6210\u4ecd\u7136\u5b58\u5728\u6311\u6218\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u7b80\u5316\u51e0\u4f55\u5f62\u72b6\uff0c\u8981\u4e48\u65e0\u6cd5\u5b9e\u73b0\u50cf\u7d20\u7cbe\u786e\u7684\u989c\u8272\u7f8e\u5b66\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Voxify3D\uff0c\u4e00\u4e2a\u5c063D\u6a21\u578b\u4f18\u5316\u4e0e2D\u50cf\u7d20\u827a\u672f\u76d1\u7763\u7ed3\u5408\u7684\u53ef\u5fae\u5206\u6846\u67b6\u3002</li>\n    <li>Voxify3D\u7684\u521b\u65b0\u5728\u4e8e\u6574\u5408\u4e86\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u6b63\u4ea4\u50cf\u7d20\u827a\u672f\u76d1\u7763\u3001\u57fa\u4e8e\u8865\u4e01\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u53d7\u8c03\u8272\u677f\u7ea6\u675f\u7684Gumbel-Softmax\u91cf\u5316\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cVoxify3D\u5728\u591a\u6837\u5316\u89d2\u8272\u548c\u53ef\u63a7\u62bd\u8c61\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u83b7\u5f97\u4e86\u7528\u6237\u7684\u9ad8\u5ea6\u504f\u597d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Voxel art is a unique style used in games, but it's hard to create automatically from 3D models.</li>\n    <li>Current methods struggle with balancing geometry details and color matching.</li>\n    <li>Voxify3D is a new system that combines 3D mesh work with 2D pixel art guidance.</li>\n    <li>It features three main innovations for better results: \n        <ul>\n            <li>Pixel art guidance to avoid distortion.</li>\n            <li>A method to keep the meaning of objects while simplifying them.</li>\n            <li>A color optimization technique that allows for specific color choices.</li>\n        </ul>\n    </li>\n    <li>Tests show Voxify3D performs well and is preferred by users for various characters and styles.</li>\n</ul>"}, "publishedAt": "2025-12-08T13:59:58.000Z", "title": "Voxify3D: Pixel Art Meets Volumetric Rendering", "summary": "Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xmWIqFounUj-kosdkGMIN.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07834.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "isAuthorParticipating": false}, {"paper": {"id": "2512.06905", "authors": [{"_id": "69379d8b19d912300c34a2de", "user": {"_id": "62e0fcfe4db2175cd2710063", "avatarUrl": "/avatars/0f8d1b5ba3e3eba2af18f98f65f0b180.svg", "isPro": true, "fullname": "Zijian Zhou", "user": "franciszzj", "type": "user"}, "name": "Zijian Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T10:34:32.358Z", "hidden": false}, {"_id": "69379d8b19d912300c34a2df", "name": "Shikun Liu", "hidden": false}, {"_id": "69379d8b19d912300c34a2e0", "name": "Haozhe Liu", "hidden": false}, {"_id": "69379d8b19d912300c34a2e1", "name": "Haonan Qiu", "hidden": false}, {"_id": "69379d8b19d912300c34a2e2", "name": "Zhaochong An", "hidden": false}, {"_id": "69379d8b19d912300c34a2e3", "name": "Weiming Ren", "hidden": false}, {"_id": "69379d8b19d912300c34a2e4", "name": "Zhiheng Liu", "hidden": false}, {"_id": "69379d8b19d912300c34a2e5", "name": "Xiaoke Huang", "hidden": false}, {"_id": "69379d8b19d912300c34a2e6", "name": "Kam Woh Ng", "hidden": false}, {"_id": "69379d8b19d912300c34a2e7", "name": "Tian Xie", "hidden": false}, {"_id": "69379d8b19d912300c34a2e8", "name": "Xiao Han", "hidden": false}, {"_id": "69379d8b19d912300c34a2e9", "name": "Yuren Cong", "hidden": false}, {"_id": "69379d8b19d912300c34a2ea", "name": "Hang Li", "hidden": false}, {"_id": "69379d8b19d912300c34a2eb", "name": "Chuyan Zhu", "hidden": false}, {"_id": "69379d8b19d912300c34a2ec", "name": "Aditya Patel", "hidden": false}, {"_id": "69379d8b19d912300c34a2ed", "name": "Tao Xiang", "hidden": false}, {"_id": "69379d8b19d912300c34a2ee", "name": "Sen He", "hidden": false}], "publishedAt": "2025-12-07T16:10:25.000Z", "submittedOnDailyAt": "2025-12-09T01:25:03.574Z", "title": "Scaling Zero-Shot Reference-to-Video Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.", "upvotes": 25, "discussionId": "69379d8b19d912300c34a2ef", "projectPage": "https://franciszzj.github.io/Saber/", "githubRepo": "https://github.com/franciszzj/Saber", "ai_summary": "Saber is a scalable zero-shot framework for reference-to-video generation that uses video-text pairs to learn identity-consistent representations and outperforms models trained with explicit reference data.", "ai_keywords": ["reference-to-video (R2V) generation", "masked training strategy", "attention-based model design", "mask augmentation", "OpenS2V-Eval benchmark"], "githubStars": 5, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "summary_zh": "<ul>\n    <li>R2V\u751f\u6210\u7684\u76ee\u6807\u662f\u6839\u636e\u6587\u672c\u63d0\u793a\u5408\u6210\u89c6\u9891\uff0c\u540c\u65f6\u4fdd\u6301\u53c2\u8003\u56fe\u50cf\u4e2d\u7684\u4e3b\u4f53\u8eab\u4efd\u3002</li>\n    <li>\u76ee\u524d\u7684R2V\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u590d\u6742\u7684\u56fe\u50cf-\u89c6\u9891-\u6587\u672c\u4e09\u5143\u7ec4\uff0c\u5efa\u7acb\u8fd9\u4e9b\u4e09\u5143\u7ec4\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86Saber\uff0c\u4e00\u4e2a\u65e0\u9700\u663e\u5f0fR2V\u6570\u636e\u7684\u53ef\u6269\u5c55\u96f6-shot\u6846\u67b6\u3002</li>\n    <li>Saber\u4ec5\u5728\u89c6\u9891-\u6587\u672c\u5bf9\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f7f\u7528\u63a9\u7801\u8bad\u7ec3\u7b56\u7565\u548c\u7279\u5b9a\u7684\u6ce8\u610f\u529b\u6a21\u578b\u8bbe\u8ba1\u3002</li>\n    <li>Saber\u5728\u591a\u4e2a\u53c2\u8003\u56fe\u50cf\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728OpenS2V-Eval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u4f7f\u7528R2V\u6570\u636e\u8bad\u7ec3\u7684\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reference-to-video (R2V) generation creates videos based on text prompts while keeping the same subject from reference images.</li>\n    <li>Current R2V methods face challenges because they need a lot of expensive and complex data (image-video-text triplets).</li>\n    <li>Saber is a new framework that doesn't need explicit R2V data and can work with just video-text pairs.</li>\n    <li>It uses a special training method and attention-based model to learn consistent and aware representations of identities.</li>\n    <li>Saber performs well even with different numbers of references and outperforms other methods on a benchmark test.</li>\n</ul>"}, "publishedAt": "2025-12-07T11:10:25.000Z", "title": "Scaling Zero-Shot Reference-to-Video Generation", "summary": "Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06905.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.06749", "authors": [{"_id": "693791a319d912300c34a271", "user": {"_id": "64f6efa0e2e54c750fb0655d", "avatarUrl": "/avatars/83518aff2e2ef0f9cf84ca39e0e5db0d.svg", "isPro": false, "fullname": "Ming Ma", "user": "MBJinX", "type": "user"}, "name": "Ming Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:38:05.434Z", "hidden": false}, {"_id": "693791a319d912300c34a272", "name": "Jue Zhang", "hidden": false}, {"_id": "693791a319d912300c34a273", "name": "Fangkai Yang", "hidden": false}, {"_id": "693791a319d912300c34a274", "name": "Yu Kang", "hidden": false}, {"_id": "693791a319d912300c34a275", "name": "Qingwei Lin", "hidden": false}, {"_id": "693791a319d912300c34a276", "name": "Saravan Rajmohan", "hidden": false}, {"_id": "693791a319d912300c34a277", "user": {"_id": "66473d2c7abe6ad66e81a3dd", "avatarUrl": "/avatars/82f40244806c06ffeaa1c4265e9725ea.svg", "isPro": false, "fullname": "ZHANGDONGMEI", "user": "ZDM6426", "type": "user"}, "name": "Dongmei Zhang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T10:36:42.746Z", "hidden": false}], "publishedAt": "2025-12-07T09:23:48.000Z", "submittedOnDailyAt": "2025-12-09T00:43:03.031Z", "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems", "submittedOnDailyBy": {"_id": "65f40e43653c231cbaf7d1e4", "avatarUrl": "/avatars/a42ac5454cbe175f04c3420fce90cad2.svg", "isPro": false, "fullname": "Jue Zhang", "user": "JueZhang", "type": "user"}, "summary": "Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.", "upvotes": 24, "discussionId": "693791a319d912300c34a278", "projectPage": "https://aka.ms/DoVer", "ai_summary": "DoVer, an intervention-driven debugging framework, enhances reliability in LLM-based multi-agent systems by actively validating failure hypotheses and measuring task progress through targeted interventions.", "ai_keywords": ["DoVer", "intervention-driven debugging", "LLM-based multi-agent systems", "failure localization", "targeted interventions", "outcome-oriented debugging", "Magnetic-One agent framework", "GAIA", "AssistantBench", "GSMPlus", "AG2"], "organization": {"_id": "5e6485f787403103f9f1055e", "name": "microsoft", "fullname": "Microsoft", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"}, "summary_zh": "<ul>\n    <li>\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u8c03\u8bd5\u56f0\u96be\uff0c\u56e0\u4e3a\u6545\u969c\u901a\u5e38\u6765\u81ea\u590d\u6742\u7684\u4ea4\u4e92\u8fc7\u7a0b\u3002</li>\n    <li>\u73b0\u6709\u7684\u65b9\u6cd5\u4f9d\u8d56\u65e5\u5fd7\u8fdb\u884c\u6545\u969c\u5b9a\u4f4d\uff0c\u4f46\u5b58\u5728\u9a8c\u8bc1\u4e0d\u8db3\u548c\u5355\u6b65\u6216\u5355\u4ee3\u7406\u5f52\u56e0\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86DoVer\u6846\u67b6\uff0c\u901a\u8fc7\u5e72\u9884\uff08\u5982\u7f16\u8f91\u6d88\u606f\u3001\u6539\u53d8\u8ba1\u5212\uff09\u6765\u9a8c\u8bc1\u5047\u8bbe\u3002</li>\n    <li>DoVer\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u80fd\u591f\u5c0618-28%\u7684\u5931\u8d25\u8f6c\u5316\u4e3a\u6210\u529f\uff0c\u5e76\u9a8c\u8bc130-60%\u7684\u6545\u969c\u5047\u8bbe\u3002</li>\n    <li>\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\u5e72\u9884\u662f\u4e00\u79cd\u6709\u6548\u63d0\u5347\u4ee3\u7406\u7cfb\u7edf\u53ef\u9760\u6027\u7684\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u66f4\u7a33\u5065\u7684\u8c03\u8bd5\u65b9\u6cd5\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Debugging multi-agent systems using large language models (LLMs) is difficult because errors can come from complex interactions.</li>\n    <li>Current methods focus on identifying which agent or step caused a failure, but they often produce untested ideas and miss multiple ways to fix an issue.</li>\n    <li>DoVer is a new debugging framework that improves error analysis by actively testing solutions and verifying if they work.</li>\n    <li>Instead of just finding errors, DoVer measures if the system successfully fixes failures or makes progress toward completing tasks.</li>\n    <li>In tests, DoVer helped turn 18-49% of failed attempts into successes and effectively validated many failure hypotheses, showing promise for better debugging in LLM-based systems.</li>\n</ul>"}, "publishedAt": "2025-12-07T04:23:48.000Z", "title": "DoVer: Intervention-Driven Auto Debugging for LLM Multi-Agent Systems", "summary": "Large language model (LLM)-based multi-agent systems are challenging to debug because failures often arise from long, branching interaction traces. The prevailing practice is to leverage LLMs for log-based failure localization, attributing errors to a specific agent and step. However, this paradigm has two key limitations: (i) log-only debugging lacks validation, producing untested hypotheses, and (ii) single-step or single-agent attribution is often ill-posed, as we find that multiple distinct interventions can independently repair the failed task. To address the first limitation, we introduce DoVer, an intervention-driven debugging framework, which augments hypothesis generation with active verification through targeted interventions (e.g., editing messages, altering plans). For the second limitation, rather than evaluating on attribution accuracy, we focus on measuring whether the system resolves the failure or makes quantifiable progress toward task success, reflecting a more outcome-oriented view of debugging. Within the Magnetic-One agent framework, on the datasets derived from GAIA and AssistantBench, DoVer flips 18-28% of failed trials into successes, achieves up to 16% milestone progress, and validates or refutes 30-60% of failure hypotheses. DoVer also performs effectively on a different dataset (GSMPlus) and agent framework (AG2), where it recovers 49% of failed trials. These results highlight intervention as a practical mechanism for improving reliability in agentic systems and open opportunities for more robust, scalable debugging methods for LLM-based multi-agent systems. Project website and code will be available at https://aka.ms/DoVer.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06749.png", "numComments": 3, "submittedBy": {"_id": "65f40e43653c231cbaf7d1e4", "avatarUrl": "/avatars/a42ac5454cbe175f04c3420fce90cad2.svg", "fullname": "Jue Zhang", "name": "JueZhang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "5e6485f787403103f9f1055e", "name": "microsoft", "fullname": "Microsoft", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.07778", "authors": [{"_id": "69379c6119d912300c34a2d0", "user": {"_id": "674736347f64a8dbfbf73f97", "avatarUrl": "/avatars/35d825248230fe66c4ae88f153b7128e.svg", "isPro": false, "fullname": "Sen Ye", "user": "sen-ye", "type": "user"}, "name": "Sen Ye", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:37:55.081Z", "hidden": false}, {"_id": "69379c6119d912300c34a2d1", "name": "Jianning Pei", "hidden": false}, {"_id": "69379c6119d912300c34a2d2", "name": "Mengde Xu", "hidden": false}, {"_id": "69379c6119d912300c34a2d3", "name": "Shuyang Gu", "hidden": false}, {"_id": "69379c6119d912300c34a2d4", "name": "Chunyu Wang", "hidden": false}, {"_id": "69379c6119d912300c34a2d5", "name": "Liwei Wang", "hidden": false}, {"_id": "69379c6119d912300c34a2d6", "name": "Han Hu", "hidden": false}], "publishedAt": "2025-12-08T17:59:47.000Z", "submittedOnDailyAt": "2025-12-09T04:42:46.811Z", "title": "Distribution Matching Variational AutoEncoder", "submittedOnDailyBy": {"_id": "64c38fcf573c5a427e12cd37", "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg", "isPro": false, "fullname": "cientgu", "user": "cientgu", "type": "user"}, "summary": "Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce Distribution-Matching VAE (DMVAE), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.", "upvotes": 18, "discussionId": "69379c6119d912300c34a2d7", "ai_summary": "DMVAE explicitly aligns the encoder's latent distribution with a reference distribution, improving modeling efficiency and image synthesis fidelity compared to conventional VAEs.", "ai_keywords": ["VAEs", "foundation model aligned encoders", "latent space", "distribution matching constraint", "Distribution-Matching VAE (DMVAE)", "Gaussian prior", "SSL-derived distributions", "gFID", "ImageNet", "distribution-level alignment"], "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u5927\u591a\u6570\u89c6\u89c9\u751f\u6210\u6a21\u578b\u5728\u751f\u6210\u524d\u4f1a\u5c06\u56fe\u50cf\u538b\u7f29\u5230\u6f5c\u5728\u7a7a\u95f4\u3002</li>\n    <li>\u73b0\u6709\u7684\u65b9\u6cd5\u5982\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b58\u5728\u9690\u6027\u7ea6\u675f\uff0c\u6ca1\u80fd\u660e\u786e\u5176\u5206\u5e03\u7684\u6700\u4f73\u5f62\u5f0f\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u5206\u5e03\u5339\u914d\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08DMVAE\uff09\uff0c\u901a\u8fc7\u5206\u5e03\u5339\u914d\u7ea6\u675f\u663e\u5f0f\u5bf9\u9f50\u7f16\u7801\u5668\u7684\u6f5c\u5728\u5206\u5e03\u3002</li>\n    <li>DMVAE\u53ef\u4ee5\u4e0e\u4efb\u610f\u53c2\u8003\u5206\u5e03\u5bf9\u9f50\uff0c\u5305\u62ec\u81ea\u76d1\u7763\u7279\u5f81\u6216\u6269\u6563\u566a\u58f0\u5206\u5e03\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u81ea\u76d1\u7763\u5b66\u4e60\u5bfc\u51fa\u7684\u5206\u5e03\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u5efa\u6a21\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Most visual models compress images into a hidden space before generating new images.</li>\n    <li>Current methods don't clearly define the best hidden space distributions for modeling.</li>\n    <li>We present DMVAE, which aligns the hidden space distribution with any chosen reference distribution.</li>\n    <li>DMVAE allows for better modeling by using distributions from self-supervised learning and other sources.</li>\n    <li>Our findings show that the right hidden distribution improves image quality and efficiency significantly.</li>\n</ul>"}, "publishedAt": "2025-12-08T12:59:47.000Z", "title": "Distribution Matching Variational AutoEncoder", "summary": "Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce Distribution-Matching VAE (DMVAE), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07778.png", "numComments": 1, "submittedBy": {"_id": "64c38fcf573c5a427e12cd37", "avatarUrl": "/avatars/2b9de06f29147ed2c212e920afba0eaf.svg", "fullname": "cientgu", "name": "cientgu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.06065", "authors": [{"_id": "69379c1c19d912300c34a2bc", "user": {"_id": "638e29cf319f9c746b87ad4b", "avatarUrl": "/avatars/70cac8d47847c389eb0393051a64c4a4.svg", "isPro": true, "fullname": "Runjia Li", "user": "liguang0115", "type": "user"}, "name": "Runjia Li", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T11:13:56.638Z", "hidden": false}, {"_id": "69379c1c19d912300c34a2bd", "name": "Moayed Haji-Ali", "hidden": false}, {"_id": "69379c1c19d912300c34a2be", "name": "Ashkan Mirzaei", "hidden": false}, {"_id": "69379c1c19d912300c34a2bf", "name": "Chaoyang Wang", "hidden": false}, {"_id": "69379c1c19d912300c34a2c0", "name": "Arpit Sahni", "hidden": false}, {"_id": "69379c1c19d912300c34a2c1", "name": "Ivan Skorokhodov", "hidden": false}, {"_id": "69379c1c19d912300c34a2c2", "name": "Aliaksandr Siarohin", "hidden": false}, {"_id": "69379c1c19d912300c34a2c3", "name": "Tomas Jakab", "hidden": false}, {"_id": "69379c1c19d912300c34a2c4", "name": "Junlin Han", "hidden": false}, {"_id": "69379c1c19d912300c34a2c5", "name": "Sergey Tulyakov", "hidden": false}, {"_id": "69379c1c19d912300c34a2c6", "name": "Philip Torr", "hidden": false}, {"_id": "69379c1c19d912300c34a2c7", "name": "Willi Menapace", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/638e29cf319f9c746b87ad4b/wD9Ka0q9FOeTD72oLpCEw.mp4"], "publishedAt": "2025-12-05T18:57:05.000Z", "submittedOnDailyAt": "2025-12-09T01:19:46.582Z", "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing", "submittedOnDailyBy": {"_id": "638e29cf319f9c746b87ad4b", "avatarUrl": "/avatars/70cac8d47847c389eb0393051a64c4a4.svg", "isPro": true, "fullname": "Runjia Li", "user": "liguang0115", "type": "user"}, "summary": "We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit", "upvotes": 18, "discussionId": "69379c1c19d912300c34a2c8", "projectPage": "https://snap-research.github.io/EgoEdit/", "githubRepo": "https://github.com/snap-research/EgoEdit", "ai_summary": "EgoEdit is a real-time, instruction-following egocentric video editor that addresses challenges in handling egomotion and hand-object interactions, outperforming existing methods on egocentric editing tasks.", "ai_keywords": ["EgoEditData", "EgoEdit", "EgoEditBench", "egocentric video editing", "instruction-following", "real-time streaming inference", "hand-object interactions", "egomotion", "instruction faithfulness", "hand preservation", "interaction preservation", "temporal stability"], "githubStars": 15, "organization": {"_id": "63c87c41cd6a490608ce31d1", "name": "snap-research", "fullname": "Snap Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u7814\u7a76\u4e86\u7528\u4e8e\u4e92\u52a8\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u7f16\u8f91\u3002</li>\n    <li>\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u7f16\u8f91\u9762\u4e34\u5feb\u901f\u79fb\u52a8\u548c\u9891\u7e41\u624b\u7269\u4f53\u4e92\u52a8\u7b49\u72ec\u7279\u6311\u6218\uff0c\u73b0\u6709\u7684\u79bb\u7ebf\u7f16\u8f91\u65b9\u6cd5\u5ef6\u8fdf\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u4ea4\u4e92\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86EgoEditData\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u7528\u4e8e\u7b2c\u4e00\u4eba\u79f0\u7f16\u8f91\u573a\u666f\uff0c\u5173\u6ce8\u624b\u4e0e\u7269\u4f53\u7684\u4e92\u52a8\u3002</li>\n    <li>\u5f00\u53d1\u4e86EgoEdit\uff0c\u4e00\u4e2a\u652f\u6301\u5b9e\u65f6\u6d41\u5a92\u4f53\u63a8\u7406\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u7f16\u8f91\u5de5\u5177\uff0c\u80fd\u5728\u5355\u4e2aGPU\u4e0a\u8fd0\u884c\u3002</li>\n    <li>\u63a8\u51fa\u4e86EgoEditBench\u8bc4\u4f30\u5957\u4ef6\uff0c\u4e13\u6ce8\u4e8e\u6307\u4ee4\u51c6\u786e\u6027\u3001\u624b\u548c\u4e92\u52a8\u4fdd\u7559\u3001\u4ee5\u53ca\u5728\u5feb\u901f\u79fb\u52a8\u4e0b\u7684\u65f6\u95f4\u7a33\u5b9a\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>The study focuses on improving video editing for egocentric (first-person) views in augmented reality applications.</li>\n  <li>Existing AI video editors work well with third-person footage but struggle with the unique challenges of egocentric videos, like fast movements and hand-object interactions.</li>\n  <li>To solve these problems, the researchers created a special dataset called EgoEditData, designed for egocentric editing that highlights hand interactions.</li>\n  <li>They developed a real-time video editor named EgoEdit that can handle instructions and run efficiently on a single GPU.</li>\n  <li>The team also created EgoEditBench, a tool to evaluate the performance of their editing system, which shows improvements in egocentric editing while still performing well on general editing tasks.</li>\n</ul>"}, "publishedAt": "2025-12-05T13:57:05.000Z", "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing", "summary": "We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/638e29cf319f9c746b87ad4b/wD9Ka0q9FOeTD72oLpCEw.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.06065.png", "numComments": 1, "submittedBy": {"_id": "638e29cf319f9c746b87ad4b", "avatarUrl": "/avatars/70cac8d47847c389eb0393051a64c4a4.svg", "fullname": "Runjia Li", "name": "liguang0115", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "63c87c41cd6a490608ce31d1", "name": "snap-research", "fullname": "Snap Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1674083325534-61f19829233c91cbd2f79e70.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07833", "authors": [{"_id": "69379fe819d912300c34a313", "user": {"_id": "634ef841de30ee20582b355a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634ef841de30ee20582b355a/7W9HHzEjURmUPkQ7U_Nnl.png", "isPro": false, "fullname": "Thao Nguyen", "user": "thaoshibe", "type": "user"}, "name": "Thao Nguyen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T20:17:39.452Z", "hidden": false}, {"_id": "69379fe819d912300c34a314", "name": "Sicheng Mo", "hidden": false}, {"_id": "69379fe819d912300c34a315", "name": "Krishna Kumar Singh", "hidden": false}, {"_id": "69379fe819d912300c34a316", "name": "Yilin Wang", "hidden": false}, {"_id": "69379fe819d912300c34a317", "name": "Jing Shi", "hidden": false}, {"_id": "69379fe819d912300c34a318", "name": "Nicholas Kolkin", "hidden": false}, {"_id": "69379fe819d912300c34a319", "name": "Eli Shechtman", "hidden": false}, {"_id": "69379fe819d912300c34a31a", "name": "Yong Jae Lee", "hidden": false}, {"_id": "69379fe819d912300c34a31b", "name": "Yuheng Li", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Jqivw4Bd5tlTCrwgFwhkq.mp4"], "publishedAt": "2025-12-08T18:59:56.000Z", "submittedOnDailyAt": "2025-12-09T01:35:18.706Z", "title": "Relational Visual Similarity", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.", "upvotes": 17, "discussionId": "69379fe819d912300c34a31c", "projectPage": "https://thaoshibe.github.io/relsim/", "githubRepo": "https://github.com/thaoshibe/relsim", "ai_summary": "Vision-Language models fine-tuned on anonymized image captions can capture relational similarity between images, a capability lacking in current visual similarity metrics.", "ai_keywords": ["relational similarity", "perceptual attribute similarity", "LPIPS", "CLIP", "DINO", "Vision-Language model", "image-caption dataset", "relational logic", "representation space"], "githubStars": 14, "summary_zh": "<ul>\n    <li>\u4eba\u7c7b\u4e0d\u4ec5\u80fd\u770b\u5230\u7269\u4f53\u7684\u5c5e\u6027\u76f8\u4f3c\u6027\uff0c\u8fd8\u80fd\u770b\u5230\u7269\u4f53\u4e4b\u95f4\u7684\u5173\u7cfb\u76f8\u4f3c\u6027\u3002</li>\n    <li>\u76ee\u524d\u6d41\u884c\u7684\u89c6\u89c9\u76f8\u4f3c\u6027\u6307\u6807\u53ea\u5173\u6ce8\u611f\u77e5\u5c5e\u6027\uff0c\u800c\u5ffd\u89c6\u4e86\u4eba\u7c7b\u80fd\u591f\u8bc6\u522b\u7684\u590d\u6742\u5173\u7cfb\u76f8\u4f3c\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6d4b\u91cf\u7684\u5173\u7cfb\u56fe\u50cf\u76f8\u4f3c\u6027\uff0c\u5f3a\u8c03\u56fe\u50cf\u5185\u90e8\u5143\u7d20\u4e4b\u95f4\u7684\u5173\u7cfb\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b114,000\u5f20\u56fe\u50cf\u548c\u533f\u540d\u63cf\u8ff0\u7684\u6570\u636e\u96c6\uff0c\u4e13\u6ce8\u4e8e\u573a\u666f\u7684\u5173\u7cfb\u903b\u8f91\u3002</li>\n    <li>\u901a\u8fc7\u5fae\u8c03\u4e00\u4e2a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u6211\u4eec\u53ef\u4ee5\u66f4\u597d\u5730\u6d4b\u91cf\u56fe\u50cf\u4e4b\u95f4\u7684\u5173\u7cfb\u76f8\u4f3c\u6027\uff0c\u586b\u8865\u73b0\u6709\u89c6\u89c9\u8ba1\u7b97\u7684\u7a7a\u767d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans can recognize not only similarities in attributes but also in relationships, which sets them apart from other species.</li>\n    <li>Current visual similarity metrics only focus on surface attributes, missing deeper relational similarities between images.</li>\n    <li>The study aims to measure relational image similarity based on the functions and relations among visual elements.</li>\n    <li>Researchers created a dataset of 114,000 images with anonymized captions that describe the relational logic of the scenes.</li>\n    <li>A Vision-Language model was developed to assess relational similarities, highlighting a gap in existing visual computing methods.</li>\n</ul>"}, "publishedAt": "2025-12-08T13:59:56.000Z", "title": "Relational Visual Similarity", "summary": "Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/Jqivw4Bd5tlTCrwgFwhkq.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07833.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07806", "authors": [{"_id": "6937ca7819d912300c34a468", "user": {"_id": "64aa93e799639cc312795ea8", "avatarUrl": "/avatars/904453c39c09aa51d3f6aa9fafb4a47d.svg", "isPro": false, "fullname": "Gyeongjin Kang", "user": "Gynjn", "type": "user"}, "name": "Gyeongjin Kang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:21:20.954Z", "hidden": false}, {"_id": "6937ca7819d912300c34a469", "name": "Seungkwon Yang", "hidden": false}, {"_id": "6937ca7819d912300c34a46a", "name": "Seungtae Nam", "hidden": false}, {"_id": "6937ca7819d912300c34a46b", "name": "Younggeun Lee", "hidden": false}, {"_id": "6937ca7819d912300c34a46c", "name": "Jungwoo Kim", "hidden": false}, {"_id": "6937ca7819d912300c34a46d", "name": "Eunbyung Park", "hidden": false}], "publishedAt": "2025-12-08T18:39:27.000Z", "submittedOnDailyAt": "2025-12-09T04:42:14.526Z", "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader", "submittedOnDailyBy": {"_id": "64aa93e799639cc312795ea8", "avatarUrl": "/avatars/904453c39c09aa51d3f6aa9fafb4a47d.svg", "isPro": false, "fullname": "Gyeongjin Kang", "user": "Gynjn", "type": "user"}, "summary": "We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details,\" MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.", "upvotes": 14, "discussionId": "6937ca7819d912300c34a46e", "projectPage": "https://gynjn.github.io/MVP/", "githubRepo": "https://github.com/Gynjn/MVP", "ai_summary": "MVP, a scalable multi-view transformer architecture, efficiently reconstructs large 3D scenes from multiple images using dual hierarchies and achieves state-of-the-art quality.", "ai_keywords": ["Multi-view Pyramid Transformer", "MVP", "local-to-global inter-view hierarchy", "fine-to-coarse intra-view hierarchy", "3D Gaussian Splatting", "3D scenes", "generalizable reconstruction quality", "scalability", "view configurations"], "githubStars": 43, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u591a\u89c6\u89d2\u91d1\u5b57\u5854\u53d8\u6362\u5668\uff08MVP\uff09\u7684\u67b6\u6784\uff0c\u80fd\u591f\u4ece\u6570\u5341\u5230\u6570\u767e\u5e45\u56fe\u50cf\u4e2d\u91cd\u5efa\u5927\u578b3D\u573a\u666f\u3002</li>\n    <li>MVP\u57fa\u4e8e\u4e24\u4e2a\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\uff1a\u4ece\u5c40\u90e8\u5230\u6574\u4f53\u7684\u89c6\u89d2\u5c42\u6b21\u548c\u4ece\u7ec6\u8282\u5230\u6982\u62ec\u7684\u7a7a\u95f4\u8868\u793a\u3002</li>\n    <li>\u8fd9\u79cd\u53cc\u5c42\u6b21\u7ed3\u6784\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u4fe1\u606f\u8868\u8fbe\u80fd\u529b\uff0c\u5feb\u901f\u91cd\u5efa\u590d\u6742\u573a\u666f\u3002</li>\n    <li>\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86MVP\uff0c\u4e0e3D\u9ad8\u65af\u70b9\u4e91\u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u8fbe\u5230\u4e86\u6700\u4f73\u7684\u91cd\u5efa\u8d28\u91cf\u3002</li>\n    <li>MVP\u5728\u5404\u79cd\u89c6\u89d2\u914d\u7f6e\u4e0b\u4fdd\u6301\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MVP is a new type of transformer model designed to recreate large 3D scenes from many images quickly.</li>\n    <li>It uses two main strategies: one that looks at local views and then expands to the whole scene, and another that starts with detailed images and combines them into simpler forms.</li>\n    <li>This approach allows the model to be both fast and able to handle complex scenes effectively.</li>\n    <li>MVP has been tested on various datasets and shows excellent reconstruction quality while being efficient and scalable.</li>\n    <li>When combined with 3D Gaussian Splatting, it achieves top performance in 3D scene reconstruction.</li>\n</ul>"}, "publishedAt": "2025-12-08T13:39:27.000Z", "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader", "summary": "We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details,\" MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07806.png", "numComments": 1, "submittedBy": {"_id": "64aa93e799639cc312795ea8", "avatarUrl": "/avatars/904453c39c09aa51d3f6aa9fafb4a47d.svg", "fullname": "Gyeongjin Kang", "name": "Gynjn", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "isAuthorParticipating": true}],
    "week": [{"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u987a\u5e8f\u8ba1\u7b97\u548c\u957f\u671f\u4e0d\u4e00\u81f4\u6027\uff0c\u5f71\u54cd\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u5934\u50cf\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Live Avatar\uff0c\u4e00\u4e2a\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u3001\u65e0\u9650\u957f\u5ea6\u5934\u50cf\u751f\u6210\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u4e86140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\u6027\uff08TPP\uff09\uff0c\u5728\u591a\u4e2aGPU\u4e0a\u5e76\u884c\u5904\u7406\u53bb\u566a\u6b65\u9aa4\uff0c\u63d0\u9ad8\u4e86\u5b9e\u65f6\u6d41\u5a92\u4f53\u7684\u7a33\u5b9a\u6027\u548c\u4f4e\u5ef6\u8fdf\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u6eda\u52a8\u6c89\u6d78\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u65b0\u6821\u51c6\u5916\u89c2\u6765\u7ef4\u6301\u5e8f\u5217\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u8eab\u4efd\u6f02\u79fb\u548c\u989c\u8272\u4f2a\u5f71\u3002</li>\n    <li>Live Avatar\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e8620 FPS\u7684\u7aef\u5230\u7aef\u751f\u6210\uff0c\u9996\u6b21\u5728\u6b64\u89c4\u6a21\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u4fdd\u771f\u5934\u50cf\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new system for creating realistic, animated avatars in real-time using advanced AI technology.</li>\n    <li>The system uses a 14-billion-parameter diffusion model to generate high-quality video content efficiently.</li>\n    <li>It features a technique called Timestep-forcing Pipeline Parallelism (TPP) that allows multiple GPUs to work together, enabling faster video generation.</li>\n    <li>To improve consistency in the videos, Live Avatar uses the Rolling Sink Frame Mechanism (RSFM) to maintain the avatar's appearance over time.</li>\n    <li>Live Avatar achieves impressive performance, generating video at 20 frames per second on multiple GPUs, setting a new standard for real-time avatar creation.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp\u662f\u4e00\u4e2a\u5305\u542b210\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6a21\u62df\u4f01\u4e1a\u6570\u636e\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\uff08DE\uff09\u4efb\u52a1\u6d89\u53ca\u5728\u5de5\u4e1a\u67b6\u6784\u4e0a\u8fdb\u884c\u591a\u9636\u6bb5SQL\u7ba1\u9053\u7684\u8bbe\u8ba1\u548c\u6784\u5efa\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\uff08DA\uff09\u4efb\u52a1\u9700\u8981\u89e3\u51b3\u5f00\u653e\u5f0f\u7684\u5546\u4e1a\u95ee\u9898\uff0c\u901a\u8fc7\u8fed\u4ee3\u5206\u6790\u751f\u6210\u53ef\u884c\u52a8\u7684\u5efa\u8bae\u3002</li>\n    <li>\u5b9e\u9a8c\u663e\u793a\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7cfb\u7edf\u5728DE\u548cDA\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5747\u4e0d\u7406\u60f3\uff0c\u6210\u529f\u7387\u90fd\u4f4e\u4e8e40%\u3002</li>\n    <li>DAComp\u4e3a\u5f00\u53d1\u771f\u6b63\u6709\u80fd\u529b\u7684\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u4e14\u73b0\u5b9e\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DAComp is a new benchmark with 210 tasks that reflect real-world data workflows in businesses.</li>\n    <li>Data engineering tasks involve creating and updating complex SQL pipelines and managing data structures.</li>\n    <li>Data analysis tasks require problem-solving, coding, and making recommendations based on data insights.</li>\n    <li>Current advanced agents perform poorly on DAComp, especially in data engineering, with success rates below 20%.</li>\n    <li>DAComp helps identify weaknesses in automated data handling, aiming to improve the development of autonomous data agents.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04987", "authors": [{"_id": "6932458a6d1060ca587a2618", "name": "Nex-AGI Team", "hidden": false}, {"_id": "6932458a6d1060ca587a261a", "name": "Yuxuan Cai", "hidden": false}, {"_id": "6932458a6d1060ca587a261b", "name": "Lu Chen", "hidden": false}, {"_id": "6932458a6d1060ca587a261c", "name": "Qiaoling Chen", "hidden": false}, {"_id": "6932458a6d1060ca587a261d", "name": "Yuyang Ding", "hidden": false}, {"_id": "6932458a6d1060ca587a261e", "name": "Liwen Fan", "hidden": false}, {"_id": "6932458a6d1060ca587a261f", "name": "Wenjie Fu", "hidden": false}, {"_id": "6932458a6d1060ca587a2620", "user": {"_id": "658bd417925aadd43303566a", "avatarUrl": "/avatars/e97f6696817caaa4564a33f12c7b9090.svg", "isPro": false, "fullname": "Gao", "user": "Yufei0707", "type": "user"}, "name": "Yufei Gao", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:36.106Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2621", "user": {"_id": "638ef0b0c67af472d31674a6", "avatarUrl": "/avatars/02df97d15a0f46b47f9162221733b121.svg", "isPro": false, "fullname": "Honglin Guo", "user": "KYLN24", "type": "user"}, "name": "Honglin Guo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:43.376Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2622", "user": {"_id": "6461e09759daabed7575b7a2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6461e09759daabed7575b7a2/sxfko49Q7ta0dCfgrpqoB.jpeg", "isPro": false, "fullname": "PinxueGuo", "user": "PinxueGuo", "type": "user"}, "name": "Pinxue Guo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T15:14:54.854Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2623", "name": "Zhenhua Han", "hidden": false}, {"_id": "6932458a6d1060ca587a2624", "name": "Zhengfu He", "hidden": false}, {"_id": "6932458a6d1060ca587a2625", "name": "Hanglei Hu", "hidden": false}, {"_id": "6932458a6d1060ca587a2626", "name": "Kai Hu", "hidden": false}, {"_id": "6932458a6d1060ca587a2627", "name": "Shengjia Hua", "hidden": false}, {"_id": "6932458a6d1060ca587a2628", "name": "Tianyu Huai", "hidden": false}, {"_id": "6932458a6d1060ca587a2629", "name": "Baodai Huang", "hidden": false}, {"_id": "6932458a6d1060ca587a262a", "name": "Li Ji", "hidden": false}, {"_id": "6932458a6d1060ca587a262b", "name": "Zhen Jiang", "hidden": false}, {"_id": "6932458a6d1060ca587a262c", "name": "Zhikai Lei", "hidden": false}, {"_id": "6932458a6d1060ca587a262d", "name": "Bufan Li", "hidden": false}, {"_id": "6932458a6d1060ca587a262e", "name": "Jiahang Lin", "hidden": false}, {"_id": "6932458a6d1060ca587a262f", "name": "Lizhi Lin", "hidden": false}, {"_id": "6932458a6d1060ca587a2630", "name": "Jinxiu Liu", "hidden": false}, {"_id": "6932458a6d1060ca587a2631", "user": {"_id": "65435cad429b80b14922ab8d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/N8oWq4ZZn3dRxmXi18FrA.jpeg", "isPro": false, "fullname": "Shichun Liu", "user": "Liusc2020", "type": "user"}, "name": "Shichun Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:39.885Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2632", "name": "Ziming Liu", "hidden": false}, {"_id": "6932458a6d1060ca587a2633", "name": "Yuchen Ni", "hidden": false}, {"_id": "6932458a6d1060ca587a2634", "name": "Pengfang Qian", "hidden": false}, {"_id": "6932458a6d1060ca587a2635", "name": "Yujiong Shen", "hidden": false}, {"_id": "6932458a6d1060ca587a2636", "name": "Qingyun Shi", "hidden": false}, {"_id": "6932458a6d1060ca587a2637", "name": "Wentao Shu", "hidden": false}, {"_id": "6932458a6d1060ca587a2638", "name": "Peng Sun", "hidden": false}, {"_id": "6932458a6d1060ca587a2639", "name": "Yiran Suo", "hidden": false}, {"_id": "6932458a6d1060ca587a263a", "name": "Tian Tang", "hidden": false}, {"_id": "6932458a6d1060ca587a263b", "name": "Boyu Tian", "hidden": false}, {"_id": "6932458a6d1060ca587a263c", "user": {"_id": "6542391f3fcd1aee202383d2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5O7dZAVGmwhsx1THSJ-gn.jpeg", "isPro": false, "fullname": "wang", "user": "guoteng", "type": "user"}, "name": "Guoteng Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T15:14:58.106Z", "hidden": false}, {"_id": "6932458a6d1060ca587a263d", "name": "Junzhe Wang", "hidden": false}, {"_id": "6932458a6d1060ca587a263e", "name": "Peixin Wang", "hidden": false}, {"_id": "6932458a6d1060ca587a263f", "user": {"_id": "653a6e5cae155b92bae77b74", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg", "isPro": false, "fullname": "Zhiheng Xi", "user": "WooooDyy", "type": "user"}, "name": "Zhiheng Xi", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:45.863Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2640", "name": "Hang Yan", "hidden": false}, {"_id": "6932458a6d1060ca587a2641", "user": {"_id": "66206a2136201a18e5329631", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66206a2136201a18e5329631/9aJiGOKshh1tZ171zDw_D.png", "isPro": false, "fullname": "yangjie", "user": "red-fox-yj", "type": "user"}, "name": "Jie Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:12.116Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2642", "name": "Zhixiong Yang", "hidden": false}, {"_id": "6932458a6d1060ca587a2643", "name": "Tianchu Yao", "hidden": false}, {"_id": "6932458a6d1060ca587a2644", "name": "Guangze Ye", "hidden": false}, {"_id": "6932458a6d1060ca587a2645", "name": "Qianxi Yu", "hidden": false}, {"_id": "6932458a6d1060ca587a2646", "user": {"_id": "6334f2f1259c518276efa730", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6334f2f1259c518276efa730/z_SH_OBkDyj4RCN9mqsKS.jpeg", "isPro": false, "fullname": "Shuo Zhang", "user": "Meteonis", "type": "user"}, "name": "Shuo Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:41.640Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2647", "name": "Xinyue Zhang", "hidden": false}, {"_id": "6932458a6d1060ca587a2648", "name": "Yiqi Zhang", "hidden": false}, {"_id": "6932458a6d1060ca587a2649", "name": "Jiarong Zhao", "hidden": false}, {"_id": "6932458a6d1060ca587a264a", "name": "Miao Zheng", "hidden": false}, {"_id": "6932458a6d1060ca587a264b", "name": "Rui Zheng", "hidden": false}, {"_id": "6932458a6d1060ca587a264c", "name": "Enyu Zhou", "hidden": false}, {"_id": "6932458a6d1060ca587a264d", "name": "Jiazheng Zhou", "hidden": false}, {"_id": "6932458a6d1060ca587a264e", "name": "Maosen Zhou", "hidden": false}, {"_id": "6932458a6d1060ca587a264f", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6932458a6d1060ca587a2650", "name": "Tao Gui", "hidden": false}, {"_id": "6932458a6d1060ca587a2651", "name": "Yining Zheng", "hidden": false}, {"_id": "6932458a6d1060ca587a2652", "name": "Xinchi Chen", "hidden": false}, {"_id": "6932458a6d1060ca587a2653", "name": "Jie Zhou", "hidden": false}, {"_id": "6932458a6d1060ca587a2654", "user": {"_id": "62061f8f03825909dcbeba27", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62061f8f03825909dcbeba27/byOi6gxlycUtsUPIUPscQ.jpeg", "isPro": false, "fullname": "Siyuan Feng", "user": "Siyuan", "type": "user"}, "name": "Siyuan Feng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:38.168Z", "hidden": false}, {"_id": "6932458a6d1060ca587a2655", "name": "Qin Chen", "hidden": false}, {"_id": "6932458a6d1060ca587a2656", "name": "Liang He", "hidden": false}, {"_id": "6932458a6d1060ca587a2657", "name": "Qi Zhang", "hidden": false}, {"_id": "6932458a6d1060ca587a2658", "name": "Xuanjing Huang", "hidden": false}, {"_id": "6932458a6d1060ca587a2659", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2025-12-04T16:57:02.000Z", "submittedOnDailyAt": "2025-12-05T00:08:10.618Z", "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.", "upvotes": 66, "discussionId": "6932458a6d1060ca587a265a", "githubRepo": "https://github.com/nex-agi/Nex-N1", "ai_summary": "The introduction of NexAU, NexA4A, and NexGAP enables the scaling of complexity, diversity, and fidelity in interactive environments for training large language models as autonomous agents, resulting in superior performance.", "ai_keywords": ["Large Language Models", "autonomous agents", "incentive-driven decision making", "policy learning", "interaction signals", "NexAU", "agent hierarchies", "NexA4A", "natural language", "NexGAP", "simulation-reality gap", "grounded trajectories synthesis", "SWE-bench", "tau2"], "githubStars": 71, "organization": {"_id": "6907441c72f7d95376e910a5", "name": "nex-agi", "fullname": "Nex AGI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65435cad429b80b14922ab8d/a_O9jT_daz_NXTfxtcw6S.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ece\u88ab\u52a8\u54cd\u5e94\u8005\u8f6c\u53d8\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\uff0c\u9700\u8981\u5b66\u4e60\u65b9\u5f0f\u7684\u6839\u672c\u6539\u53d8\u3002</li>\n    <li>\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\u96be\u4ee5\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u4e92\u52a8\u4fe1\u53f7\uff0c\u4ece\u800c\u5f71\u54cd\u6709\u6548\u7684\u7b56\u7565\u5b66\u4e60\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u9762\u7684\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u5730\u6269\u5c55\u4e92\u52a8\u73af\u5883\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u65b9\u9762\uff1a\u590d\u6742\u6027\uff08NexAU\u6846\u67b6\uff09\u3001\u591a\u6837\u6027\uff08NexA4A\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u667a\u80fd\u4f53\uff09\u548c\u5fe0\u5b9e\u5ea6\uff08NexGAP\u7ed3\u5408\u771f\u5b9e\u73af\u5883\uff09\u3002</li>\n    <li>Nex-N1\u5728\u8fd9\u4e9b\u590d\u6742\u7684\u4e92\u52a8\u73af\u5883\u4e0a\u8bad\u7ec3\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\uff0c\u5e76\u4e0e\u524d\u6cbf\u4e13\u6709\u6a21\u578b\u7ade\u4e89\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are evolving from simple responders to more advanced autonomous agents, requiring new ways of learning.</li>\n    <li>This evolution is challenged by a lack of infrastructure to create effective learning signals for these agents.</li>\n    <li>The authors present a new method that enhances interactive environments by focusing on complexity, diversity, and fidelity.</li>\n    <li>The method includes tools like NexAU for building agent hierarchies, NexA4A for generating diverse agents, and NexGAP for improving real-world interaction.</li>\n    <li>The results show that their model, Nex-N1, outperforms existing models and they are sharing their tools and models for further research.</li>\n</ul>"}, "publishedAt": "2025-12-04T11:57:02.000Z", "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction", "summary": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04987.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "6907441c72f7d95376e910a5", "name": "nex-agi", "fullname": "Nex AGI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65435cad429b80b14922ab8d/a_O9jT_daz_NXTfxtcw6S.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.07461", "authors": [{"_id": "6937b96219d912300c34a398", "user": {"_id": "626b889ff451470f861d8c78", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651214465695-noauth.jpeg", "isPro": false, "fullname": "victor wu", "user": "victor-wu", "type": "user"}, "name": "Tong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:22.731Z", "hidden": false}, {"_id": "6937b96219d912300c34a399", "user": {"_id": "6191cc9e6d34e827404cebab", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674119843175-6191cc9e6d34e827404cebab.jpeg", "isPro": false, "fullname": "Yang", "user": "jacklanda", "type": "user"}, "name": "Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:20.278Z", "hidden": false}, {"_id": "6937b96219d912300c34a39a", "user": {"_id": "624505fcd083d28d314de3dd", "avatarUrl": "/avatars/92cf6b6a1d81d7958dbbd21f0bf63f8f.svg", "isPro": false, "fullname": "bai jun", "user": "ba1jun", "type": "user"}, "name": "Jun Bai", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:17.404Z", "hidden": false}, {"_id": "6937b96219d912300c34a39b", "name": "Zixia Jia", "hidden": false}, {"_id": "6937b96219d912300c34a39c", "name": "Shuyi Zhang", "hidden": false}, {"_id": "6937b96219d912300c34a39d", "name": "Ziyong Lin", "hidden": false}, {"_id": "6937b96219d912300c34a39e", "user": {"_id": "64b119c4372d43407723136b", "avatarUrl": "/avatars/d523e181993eea06b7f6a71a592c995e.svg", "isPro": false, "fullname": "YANTING WANG", "user": "Noane", "type": "user"}, "name": "Yanting Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:14.418Z", "hidden": false}, {"_id": "6937b96219d912300c34a39f", "name": "Song-Chun Zhu", "hidden": false}, {"_id": "6937b96219d912300c34a3a0", "name": "Zilong Zheng", "hidden": false}], "publishedAt": "2025-12-08T11:39:43.000Z", "submittedOnDailyAt": "2025-12-09T04:12:55.960Z", "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning", "submittedOnDailyBy": {"_id": "63a95a6a7930fa8c7dd63d4e", "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg", "isPro": false, "fullname": "Zilong Zheng", "user": "zlzheng", "type": "user"}, "summary": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.", "upvotes": 49, "discussionId": "6937b96219d912300c34a3a1", "projectPage": "https://bigai-nlco.github.io/Native-Parallel-Reasoner/", "githubRepo": "https://github.com/bigai-nlco/Native-Parallel-Reasoner", "ai_summary": "NPR, a teacher-free framework, enhances Large Language Models with native parallel reasoning capabilities through self-distilled training, Parallel-Aware Policy Optimization, and a robust NPR Engine, achieving substantial performance and speed improvements.", "ai_keywords": ["Native Parallel Reasoner", "Large Language Models", "self-evolve", "parallel reasoning", "self-distilled progressive training", "cold-start format discovery", "topological constraints", "Parallel-Aware Policy Optimization", "branching policies", "execution graph", "adaptive decomposition", "trial and error", "NPR Engine", "memory management", "flow control", "parallel RL training", "reasoning benchmarks", "Qwen3-4B", "genuine parallel execution", "autoregressive decoding", "agentic reasoning"], "githubStars": 18, "organization": {"_id": "63a95ac93453852ef5399a77", "name": "bigai", "fullname": "Beijing Institute for General Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1672043197974-63a95a6a7930fa8c7dd63d4e.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u539f\u751f\u5e76\u884c\u63a8\u7406\u5668\u201d\uff08NPR\uff09\u7684\u6846\u67b6\uff0c\u65e8\u5728\u5e2e\u52a9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u6211\u53d1\u5c55\u5e76\u884c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>NPR\u901a\u8fc7\u4e09\u9879\u521b\u65b0\uff0c\u5c06\u6a21\u578b\u4ece\u987a\u5e8f\u6267\u884c\u8f6c\u53d8\u4e3a\u539f\u751f\u5e76\u884c\u8ba4\u77e5\u3002</li>\n    <li>\u91c7\u7528\u81ea\u6211\u84b8\u998f\u7684\u6e10\u8fdb\u8bad\u7ec3\u65b9\u6cd5\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\uff0c\u9010\u6b65\u5b9e\u73b0\u4e25\u683c\u7684\u62d3\u6251\u7ea6\u675f\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5\u201c\u5e76\u884c\u611f\u77e5\u7b56\u7565\u4f18\u5316\u201d\uff08PAPO\uff09\uff0c\u901a\u8fc7\u8bd5\u9519\u6cd5\u4f18\u5316\u6267\u884c\u56fe\u4e2d\u7684\u5206\u652f\u7b56\u7565\u3002</li>\n    <li>NPR\u5728\u516b\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u63d0\u5347\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe24.5%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u81f34.6\u500d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Native Parallel Reasoner (NPR) helps Large Language Models (LLMs) develop true parallel reasoning skills without needing a teacher.</li>\n    <li>NPR uses three main innovations: a training method that evolves the model\u2019s format, a new algorithm for optimizing decision-making, and an engine that enhances memory and control for better performance.</li>\n    <li>Using NPR, the Qwen3-4B model showed significant improvements, with up to 24.5% better performance and 4.6 times faster inference speed on eight reasoning tests.</li>\n    <li>NPR achieves 100% genuine parallel execution, setting a new benchmark for efficient and scalable reasoning in models.</li>\n</ul>"}, "publishedAt": "2025-12-08T06:39:43.000Z", "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning", "summary": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07461.png", "numComments": 1, "submittedBy": {"_id": "63a95a6a7930fa8c7dd63d4e", "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg", "fullname": "Zilong Zheng", "name": "zlzheng", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "63a95ac93453852ef5399a77", "name": "bigai", "fullname": "Beijing Institute for General Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1672043197974-63a95a6a7930fa8c7dd63d4e.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.05111", "authors": [{"_id": "693267756d1060ca587a2780", "name": "Shengyuan Ding", "hidden": false}, {"_id": "693267756d1060ca587a2781", "user": {"_id": "64f5f8dd9b17cd59c453c57f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f5f8dd9b17cd59c453c57f/MulhwLcePFUWUQel8LQZ8.jpeg", "isPro": false, "fullname": "Xinyu Fang", "user": "nebulae09", "type": "user"}, "name": "Xinyu Fang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:08.410Z", "hidden": false}, {"_id": "693267756d1060ca587a2782", "name": "Ziyu Liu", "hidden": false}, {"_id": "693267756d1060ca587a2783", "user": {"_id": "63859cf3b2906edaf83af9f0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/kajwuVzd4pDucSPlwghxo.png", "isPro": true, "fullname": "Yuhang Zang", "user": "yuhangzang", "type": "user"}, "name": "Yuhang Zang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:11.324Z", "hidden": false}, {"_id": "693267756d1060ca587a2784", "name": "Yuhang Cao", "hidden": false}, {"_id": "693267756d1060ca587a2785", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "693267756d1060ca587a2786", "name": "Haodong Duan", "hidden": false}, {"_id": "693267756d1060ca587a2787", "name": "Xiaoyi Dong", "hidden": false}, {"_id": "693267756d1060ca587a2788", "name": "Jianze Liang", "hidden": false}, {"_id": "693267756d1060ca587a2789", "name": "Bin Wang", "hidden": false}, {"_id": "693267756d1060ca587a278a", "name": "Conghui He", "hidden": false}, {"_id": "693267756d1060ca587a278b", "name": "Dahua Lin", "hidden": false}, {"_id": "693267756d1060ca587a278c", "name": "Jiaqi Wang", "hidden": false}], "publishedAt": "2025-12-04T18:59:52.000Z", "submittedOnDailyAt": "2025-12-05T02:38:10.825Z", "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning", "submittedOnDailyBy": {"_id": "646cd947da8e99940b6e55cf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646cd947da8e99940b6e55cf/9c0P0WppFqNW9pdo8LgOS.jpeg", "isPro": false, "fullname": "Shengyuan Ding", "user": "ChrisDing1105", "type": "user"}, "summary": "Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.", "upvotes": 43, "discussionId": "693267766d1060ca587a278d", "projectPage": "https://github.com/InternLM/ARM-Thinker", "githubRepo": "https://github.com/InternLM/ARM-Thinker", "ai_summary": "ARM-Thinker, an agentic reward model, uses external tools for verification, improving accuracy and interpretability in complex multimodal reasoning tasks.", "ai_keywords": ["reward models", "vision-language systems", "hallucination", "visual grounding", "external tools", "image cropping", "doc page retrieval", "multi-stage reinforcement learning", "tool-calling decisions", "judgment accuracy", "agentic reward modeling", "ARMBench-VL", "fine-grained visual grounding", "multi-page document understanding", "instruction following", "multimodal math", "logical reasoning"], "githubStars": 53, "organization": {"_id": "64a2d5fa81252883206f24c9", "name": "internlm", "fullname": "Intern Large Models", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg"}, "summary_zh": "<ul>\n    <li>ARM-Thinker\u662f\u4e00\u79cd\u81ea\u4e3b\u8c03\u7528\u5916\u90e8\u5de5\u5177\u7684\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u89c6\u89c9-\u8bed\u8a00\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u3002</li>\n    <li>\u8be5\u6a21\u578b\u53ef\u4ee5\u9a8c\u8bc1\u7ec6\u81f4\u7684\u89c6\u89c9\u7ec6\u8282\uff0c\u4ea4\u53c9\u53c2\u8003\u591a\u9875\u8bc1\u636e\uff0c\u5e76\u9a8c\u8bc1\u63a8\u7406\u58f0\u660e\uff0c\u514b\u670d\u4e86\u73b0\u6709\u5956\u52b1\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002</li>\n    <li>ARM-Thinker\u901a\u8fc7\u591a\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f18\u5316\u5de5\u5177\u8c03\u7528\u51b3\u7b56\u548c\u5224\u65ad\u51c6\u786e\u6027\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86ARMBench-VL\uff0c\u8bc4\u4f30\u7ec6\u81f4\u7684\u89c6\u89c9\u57fa\u7840\u3001\u591a\u9875\u6587\u6863\u7406\u89e3\u548c\u6307\u4ee4\u9075\u5faa\u7b49\u4efb\u52a1\u3002</li>\n    <li>ARM-Thinker\u5728\u5956\u52b1\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u9ad816.2%\uff0c\u5728\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e2d\u63d0\u9ad89.6%\uff0c\u5728\u591a\u6a21\u6001\u6570\u5b66\u548c\u903b\u8f91\u63a8\u7406\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current reward models for vision-language systems often struggle with reliability due to issues like hallucination and weak visual grounding.</li>\n    <li>ARM-Thinker is a new reward model that can use external tools to verify information and improve judgment accuracy.</li>\n    <li>This model can check detailed visuals, reference multiple documents, and validate its reasoning, which is not possible with existing models.</li>\n    <li>ARM-Thinker is trained using a multi-stage reinforcement learning approach to optimize its tool usage and judgment accuracy.</li>\n    <li>It shows significant improvements in performance on various benchmarks compared to existing models, enhancing both accuracy and interpretability.</li>\n</ul>"}, "publishedAt": "2025-12-04T13:59:52.000Z", "title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning", "summary": "Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.05111.png", "numComments": 2, "submittedBy": {"_id": "646cd947da8e99940b6e55cf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646cd947da8e99940b6e55cf/9c0P0WppFqNW9pdo8LgOS.jpeg", "fullname": "Shengyuan Ding", "name": "ChrisDing1105", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11}, "organization": {"_id": "64a2d5fa81252883206f24c9", "name": "internlm", "fullname": "Intern Large Models", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.07525", "authors": [{"_id": "693794d319d912300c34a291", "user": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "isPro": false, "fullname": "Xiaoran Liu (SII)", "user": "SII-xrliu", "type": "user"}, "name": "Xiaoran Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:38:01.257Z", "hidden": false}, {"_id": "693794d319d912300c34a292", "name": "Yuerong Song", "hidden": false}, {"_id": "693794d319d912300c34a293", "name": "Zhigeng Liu", "hidden": false}, {"_id": "693794d319d912300c34a294", "user": {"_id": "64805e6dde559d48dbb00627", "avatarUrl": "/avatars/29ca34546411dcc28bbc934e3c26a2ba.svg", "isPro": false, "fullname": "Zengfeng", "user": "ZengfengHuang", "type": "user"}, "name": "Zengfeng Huang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T10:35:11.754Z", "hidden": false}, {"_id": "693794d319d912300c34a295", "user": {"_id": "6491cd52b1e5d3444528edb1", "avatarUrl": "/avatars/a85635d886c7f157b6723dec5c01c030.svg", "isPro": false, "fullname": "Qipeng Guo", "user": "QipengGuo", "type": "user"}, "name": "Qipeng Guo", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T10:35:18.117Z", "hidden": false}, {"_id": "693794d319d912300c34a296", "name": "Zhaoxiang Liu", "hidden": false}, {"_id": "693794d319d912300c34a297", "name": "Shiguo Lian", "hidden": false}, {"_id": "693794d319d912300c34a298", "user": {"_id": "64de18f41d826d7355c285e7", "avatarUrl": "/avatars/23e2c44e3a593415becc02463980f6e8.svg", "isPro": false, "fullname": "Ziwei He", "user": "ziweihe", "type": "user"}, "name": "Ziwei He", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T20:17:41.271Z", "hidden": false}, {"_id": "693794d319d912300c34a299", "user": {"_id": "61457b8deff2c9fdb4de4988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg", "isPro": false, "fullname": "Xipeng Qiu", "user": "xpqiu", "type": "user"}, "name": "Xipeng Qiu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T10:35:28.411Z", "hidden": false}], "publishedAt": "2025-12-08T12:59:54.000Z", "submittedOnDailyAt": "2025-12-09T00:49:29.234Z", "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs", "submittedOnDailyBy": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "isPro": false, "fullname": "Xiaoran Liu (SII)", "user": "SII-xrliu", "type": "user"}, "summary": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.", "upvotes": 41, "discussionId": "693794d419d912300c34a29a", "githubRepo": "https://github.com/OpenMOSS/rope_pp", "ai_summary": "The paper proposes a method to enhance Rotary Position Embeddings by utilizing both the real and imaginary components of the complex-valued dot product, improving long-context modeling in Large Language Models.", "ai_keywords": ["Rotary Position Embeddings", "Large Language Models", "complex-valued dot product", "attention score", "long-context dependencies", "positional information"], "githubStars": 12, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08RoPE\uff09\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7f16\u7801\u5e8f\u5217\u987a\u5e8f\u7684\u6807\u51c6\u65b9\u6cd5\u3002</li>\n    <li>\u76ee\u524d\u7684\u5b9e\u73b0\u53ea\u4f7f\u7528\u590d\u6570\u70b9\u79ef\u7684\u5b9e\u90e8\u6765\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\uff0c\u5ffd\u7565\u4e86\u5305\u542b\u91cd\u8981\u4fe1\u606f\u7684\u865a\u90e8\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u65b9\u6cd5\uff0c\u91cd\u65b0\u5f15\u5165\u4e86\u88ab\u4e22\u5f03\u7684\u865a\u90e8\uff0c\u5229\u7528\u5b8c\u6574\u7684\u590d\u6570\u8868\u793a\u6765\u521b\u5efa\u53cc\u7ec4\u4ef6\u6ce8\u610f\u529b\u5206\u6570\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u5efa\u6a21\u957f\u4e0a\u4e0b\u6587\u4f9d\u8d56\u5173\u7cfb\uff0c\u4fdd\u7559\u66f4\u591a\u4f4d\u7f6e\u4fe1\u606f\u3002</li>\n    <li>\u5728\u591a\u9879\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6\u7684RoPE\uff0c\u5c24\u5176\u662f\u5728\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u65f6\u6548\u679c\u66f4\u663e\u8457\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Rotary Position Embeddings (RoPE) help Large Language Models (LLMs) understand the order of sequences using rotations in the complex plane.</li>\n  <li>Current methods only use the real part of complex calculations for attention scores, ignoring the imaginary part that holds important information.</li>\n  <li>This paper introduces a new method that includes the imaginary part to create a dual-component attention score, improving how models handle long-context relationships.</li>\n  <li>The new approach is shown to enhance performance on long-context language tasks, especially as the context length increases.</li>\n  <li>The code for this new method can be found at https://github.com/OpenMOSS/rope_pp.</li>\n</ul>"}, "publishedAt": "2025-12-08T07:59:54.000Z", "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs", "summary": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07525.png", "numComments": 1, "submittedBy": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "fullname": "Xiaoran Liu (SII)", "name": "SII-xrliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.03442", "authors": [{"_id": "693104f92d1e5b0a7d84d9f2", "user": {"_id": "651c4e0bc0247c08a46ab2a6", "avatarUrl": "/avatars/3396a34ffb400f576371afc8a5064783.svg", "isPro": false, "fullname": "xxr", "user": "xrxing", "type": "user"}, "name": "Xingrun Xing", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:39:37.521Z", "hidden": false}, {"_id": "693104f92d1e5b0a7d84d9f3", "user": {"_id": "65867b038dd42194878e08dc", "avatarUrl": "/avatars/8965ac2f8e3ccb88759f6e4d84a30c2f.svg", "isPro": false, "fullname": "Zhiyuan Fan", "user": "Zhiyuan-Fan", "type": "user"}, "name": "Zhiyuan Fan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T13:49:38.606Z", "hidden": false}, {"_id": "693104f92d1e5b0a7d84d9f4", "name": "Jie Lou", "hidden": false}, {"_id": "693104f92d1e5b0a7d84d9f5", "name": "Guoqi Li", "hidden": false}, {"_id": "693104f92d1e5b0a7d84d9f6", "name": "Jiajun Zhang", "hidden": false}, {"_id": "693104f92d1e5b0a7d84d9f7", "user": {"_id": "64546be9548f22be59842cfe", "avatarUrl": "/avatars/5582cf189b755d888b35f68a6e3e9bb8.svg", "isPro": false, "fullname": "zdb", "user": "debingzhang", "type": "user"}, "name": "Debing Zhang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:39:57.928Z", "hidden": false}], "publishedAt": "2025-12-03T04:51:32.000Z", "submittedOnDailyAt": "2025-12-04T01:22:22.945Z", "title": "PretrainZero: Reinforcement Active Pretraining", "submittedOnDailyBy": {"_id": "651c4e0bc0247c08a46ab2a6", "avatarUrl": "/avatars/3396a34ffb400f576371afc8a5064783.svg", "isPro": false, "fullname": "xxr", "user": "xrxing", "type": "user"}, "summary": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.", "upvotes": 39, "discussionId": "693104fa2d1e5b0a7d84d9f8", "ai_summary": "PretrainZero is a reinforcement active learning framework that enhances general reasoning capabilities by pretraining large models on a corpus without verifiable labels, improving performance on benchmarks compared to domain-specific training.", "ai_keywords": ["reinforcement learning", "RL", "active learning", "active pretraining", "self-supervised learning", "pretrained reward models", "Qwen3-4B-Base", "MMLU-Pro", "SuperGPQA", "math average benchmarks", "RLVR tasks"], "summary_zh": "<ul>\n    <li>\u4eba\u7c7b\u4e00\u76f4\u68a6\u60f3\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u884c\u4e3a\u6765\u5b9e\u73b0\u4eba\u5de5\u901a\u7528\u667a\u80fd\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4f9d\u8d56\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u9650\u5236\u4e86\u5176\u901a\u7528\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86PretrainZero\u6846\u67b6\uff0c\u5c06\u5f3a\u5316\u5b66\u4e60\u4ece\u9886\u57df\u7279\u5b9a\u7684\u540e\u8bad\u7ec3\u6269\u5c55\u5230\u901a\u7528\u7684\u9884\u8bad\u7ec3\u3002</li>\n    <li>PretrainZero\u7684\u7279\u70b9\u5305\u62ec\u4e3b\u52a8\u9884\u8bad\u7ec3\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u65e0\u9700\u53ef\u9a8c\u8bc1\u6807\u7b7e\uff0c\u76f4\u63a5\u5728\u7ef4\u57fa\u767e\u79d1\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002</li>\n    <li>\u901a\u8fc7\u89e3\u51b3\u66f4\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0cPretrainZero\u663e\u8457\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u8f83\u597d\u7684\u6210\u7ee9\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>PretrainZero is a new framework for reinforcement learning that aims to improve general reasoning in AI.</li>\n    <li>It uses active pretraining to learn from a vast amount of information, similar to how humans learn.</li>\n    <li>The framework does not require labeled data or supervised training, allowing for learning from general sources like Wikipedia.</li>\n    <li>PretrainZero enhances AI models' reasoning abilities by challenging them with complex tasks.</li>\n    <li>The improved models can be used for various applications in AI after their initial training.</li>\n</ul>"}, "publishedAt": "2025-12-02T23:51:32.000Z", "title": "PretrainZero: Reinforcement Active Pretraining", "summary": "Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.03442.png", "numComments": 3, "submittedBy": {"_id": "651c4e0bc0247c08a46ab2a6", "avatarUrl": "/avatars/3396a34ffb400f576371afc8a5064783.svg", "fullname": "xxr", "name": "xrxing", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04678", "authors": [{"_id": "693248cc6d1060ca587a26fa", "name": "Yunhong Lu", "hidden": false}, {"_id": "693248cc6d1060ca587a26fb", "name": "Yanhong Zeng", "hidden": false}, {"_id": "693248cc6d1060ca587a26fc", "name": "Haobo Li", "hidden": false}, {"_id": "693248cc6d1060ca587a26fd", "name": "Hao Ouyang", "hidden": false}, {"_id": "693248cc6d1060ca587a26fe", "user": {"_id": "64981bea09cea550852652af", "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg", "isPro": false, "fullname": "Qiuyu Wang", "user": "qiuyuu", "type": "user"}, "name": "Qiuyu Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:25.231Z", "hidden": false}, {"_id": "693248cc6d1060ca587a26ff", "name": "Ka Leong Cheng", "hidden": false}, {"_id": "693248cc6d1060ca587a2700", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "693248cc6d1060ca587a2701", "name": "Hengyuan Cao", "hidden": false}, {"_id": "693248cc6d1060ca587a2702", "name": "Zhipeng Zhang", "hidden": false}, {"_id": "693248cc6d1060ca587a2703", "name": "Xing Zhu", "hidden": false}, {"_id": "693248cc6d1060ca587a2704", "name": "Yujun Shen", "hidden": false}, {"_id": "693248cc6d1060ca587a2705", "name": "Min Zhang", "hidden": false}], "publishedAt": "2025-12-04T11:12:13.000Z", "submittedOnDailyAt": "2025-12-05T00:25:31.113Z", "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation", "submittedOnDailyBy": {"_id": "63d4b843df01ef426a0f79fb", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676365795587-63d4b843df01ef426a0f79fb.jpeg", "isPro": false, "fullname": "Yanhong Zeng", "user": "zengyh1900", "type": "user"}, "summary": "Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.", "upvotes": 36, "discussionId": "693248cc6d1060ca587a2706", "projectPage": "https://reward-forcing.github.io/", "githubRepo": "https://github.com/JaydenLyh/Reward-Forcing", "ai_summary": "The paper introduces Reward Forcing, which enhances video generation by updating sink tokens with EMA-Sink and using Rewarded Distribution Matching Distillation to prioritize dynamic content.", "ai_keywords": ["video diffusion models", "sliding window attention", "sink tokens", "Reward Forcing", "EMA-Sink", "exponential moving average", "Rewarded Distribution Matching Distillation", "Re-DMD", "vision-language model", "motion dynamics", "data fidelity", "streaming video generation"], "githubStars": 111, "summary_zh": "<ul>\n    <li>\u9ad8\u6548\u7684\u89c6\u9891\u6d41\u751f\u6210\u5bf9\u6a21\u62df\u4e92\u52a8\u548c\u52a8\u6001\u4e16\u754c\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u521d\u59cb\u5e27\uff0c\u5bfc\u81f4\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u52a8\u6001\u6027\u51cf\u5c11\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\u201c\u5956\u52b1\u5f3a\u5236\u201d\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1aEMA-Sink\u548c\u5956\u52b1\u5206\u5e03\u5339\u914d\u84b8\u998f\uff08Re-DMD\uff09\u3002</li>\n    <li>EMA-Sink\u901a\u8fc7\u878d\u5408\u88ab\u9a71\u9010\u7684\u4ee4\u724c\uff0c\u4fdd\u6301\u957f\u65f6\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u907f\u514d\u4e86\u521d\u59cb\u5e27\u7684\u91cd\u590d\u3002</li>\n    <li>Re-DMD\u901a\u8fc7\u4f18\u5148\u8003\u8651\u52a8\u6001\u5185\u5bb9\uff0c\u63d0\u9ad8\u4e86\u8fd0\u52a8\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u636e\u7684\u771f\u5b9e\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Efficient streaming video generation is important for creating interactive worlds, but current methods rely too much on initial frames, leading to repeated images and less dynamic motion.</li>\n    <li>We introduce a new method called Reward Forcing, which includes two main innovations: EMA-Sink and Rewarded Distribution Matching Distillation (Re-DMD).</li>\n    <li>EMA-Sink uses fixed-size tokens that are updated over time to better capture the context and dynamics of the video, reducing reliance on initial frames.</li>\n    <li>Re-DMD improves how the model learns by focusing on training samples that show more dynamic content, enhancing the motion quality of the video.</li>\n    <li>Our method achieves top performance in benchmarks and generates high-quality streaming video at 23.1 frames per second on a single GPU. </li>\n</ul>"}, "publishedAt": "2025-12-04T06:12:13.000Z", "title": "Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation", "summary": "Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04678.png", "numComments": 3, "submittedBy": {"_id": "63d4b843df01ef426a0f79fb", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676365795587-63d4b843df01ef426a0f79fb.jpeg", "fullname": "Yanhong Zeng", "name": "zengyh1900", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "isAuthorParticipating": false}, {"paper": {"id": "2512.04926", "authors": [{"_id": "693241dd6d1060ca587a25fd", "name": "Yueming Pan", "hidden": false}, {"_id": "693241dd6d1060ca587a25fe", "name": "Ruoyu Feng", "hidden": false}, {"_id": "693241dd6d1060ca587a25ff", "name": "Qi Dai", "hidden": false}, {"_id": "693241dd6d1060ca587a2600", "name": "Yuqi Wang", "hidden": false}, {"_id": "693241dd6d1060ca587a2601", "name": "Wenfeng Lin", "hidden": false}, {"_id": "693241dd6d1060ca587a2602", "name": "Mingyu Guo", "hidden": false}, {"_id": "693241dd6d1060ca587a2603", "name": "Chong Luo", "hidden": false}, {"_id": "693241dd6d1060ca587a2604", "name": "Nanning Zheng", "hidden": false}], "publishedAt": "2025-12-04T15:57:27.000Z", "submittedOnDailyAt": "2025-12-05T00:25:00.105Z", "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion", "submittedOnDailyBy": {"_id": "644101cd1a80f6d83cae6ef1", "avatarUrl": "/avatars/f1860d1dad3938e02e767b6ec108cf0d.svg", "isPro": false, "fullname": "FishNotFish", "user": "RuoyuFeng", "type": "user"}, "summary": "Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.", "upvotes": 33, "discussionId": "693241dd6d1060ca587a2605", "projectPage": "https://yuemingpan.github.io/SFD.github.io/", "githubRepo": "https://github.com/yuemingPAN/SFD", "ai_summary": "Semantic-First Diffusion (SFD) enhances image generation by asynchronously denoising semantic and texture latents, improving convergence and quality.", "ai_keywords": ["Latent Diffusion Models", "SFD", "Semantic VAE", "noise schedules", "FID", "ImageNet", "ReDi", "VA-VAE"], "githubStars": 180, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "summary_zh": "<ul>\n    <li>\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDMs\uff09\u91c7\u7528\u7c97\u5230\u7ec6\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u5148\u751f\u6210\u9ad8\u5c42\u8bed\u4e49\u7ed3\u6784\uff0c\u518d\u751f\u6210\u7ec6\u81f4\u7eb9\u7406\u3002</li>\n    <li>\u6700\u8fd1\u7684\u8fdb\u5c55\u5c06\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u7684\u8bed\u4e49\u5148\u9a8c\u6574\u5408\u8fdbLDMs\uff0c\u4f46\u4ecd\u7136\u540c\u6b65\u53bb\u566a\u8bed\u4e49\u548c\u7eb9\u7406\uff0c\u5ffd\u89c6\u987a\u5e8f\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u8bed\u4e49\u4f18\u5148\u6269\u6563\uff08SFD\uff09\uff0c\u8be5\u65b9\u6cd5\u660e\u786e\u4f18\u5148\u8003\u8651\u8bed\u4e49\u751f\u6210\u3002</li>\n    <li>SFD\u901a\u8fc7\u7ed3\u5408\u7d27\u51d1\u7684\u8bed\u4e49\u6f5c\u53d8\u91cf\u548c\u7eb9\u7406\u6f5c\u53d8\u91cf\uff0c\u5148\u6784\u5efa\u590d\u5408\u6f5c\u53d8\u91cf\uff0c\u518d\u5f02\u6b65\u53bb\u566a\u3002</li>\n    <li>SFD\u5728ImageNet 256x256\u4e0a\u53d6\u5f97\u4e86\u5f88\u597d\u7684\u6548\u679c\uff0c\u5e76\u6bd4\u539f\u59cbDiT\u5feb100\u500d\u6536\u655b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Latent Diffusion Models (LDMs) create images by first generating general shapes and then adding details.</li>\n    <li>New method called Semantic-First Diffusion (SFD) focuses on generating the semantic parts of an image before the textures.</li>\n    <li>SFD combines a semantic representation from a visual encoder with texture details and processes them separately.</li>\n    <li>This approach leads to better image quality and faster results compared to previous methods.</li>\n    <li>SFD outperforms existing models and shows improved performance on benchmark datasets like ImageNet.</li>\n</ul>"}, "publishedAt": "2025-12-04T10:57:27.000Z", "title": "Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion", "summary": "Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04926.png", "numComments": 2, "submittedBy": {"_id": "644101cd1a80f6d83cae6ef1", "avatarUrl": "/avatars/f1860d1dad3938e02e767b6ec108cf0d.svg", "fullname": "FishNotFish", "name": "RuoyuFeng", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.07469", "authors": [{"_id": "69379e0319d912300c34a2fb", "user": {"_id": "6486df66373f79a52913e017", "avatarUrl": "/avatars/4741683fbbcec3a615d0a8df62bc6fec.svg", "isPro": false, "fullname": "Xiangpeng Yang", "user": "XiangpengYang", "type": "user"}, "name": "Xiangpeng Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T10:34:33.919Z", "hidden": false}, {"_id": "69379e0319d912300c34a2fc", "name": "Ji Xie", "hidden": false}, {"_id": "69379e0319d912300c34a2fd", "name": "Yiyuan Yang", "hidden": false}, {"_id": "69379e0319d912300c34a2fe", "name": "Yan Huang", "hidden": false}, {"_id": "69379e0319d912300c34a2ff", "name": "Min Xu", "hidden": false}, {"_id": "69379e0319d912300c34a300", "name": "Qiang Wu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6486df66373f79a52913e017/iKPp57sIku0K9HVBgHjuu.mp4"], "publishedAt": "2025-12-08T11:50:18.000Z", "submittedOnDailyAt": "2025-12-09T01:34:28.853Z", "title": "Unified Video Editing with Temporal Reasoner", "submittedOnDailyBy": {"_id": "6486df66373f79a52913e017", "avatarUrl": "/avatars/4741683fbbcec3a615d0a8df62bc6fec.svg", "isPro": false, "fullname": "Xiangpeng Yang", "user": "XiangpengYang", "type": "user"}, "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.", "upvotes": 31, "discussionId": "69379e0319d912300c34a301", "projectPage": "https://videocof.github.io/", "githubRepo": "https://github.com/knightyxp/VideoCoF", "ai_summary": "VideoCoF, a Chain-of-Frames approach, improves video editing precision and instruction-to-region mapping by using reasoning tokens without requiring user-provided masks.", "ai_keywords": ["chain-of-frames", "chain-of-thought reasoning", "video diffusion model", "reasoning tokens", "edit-region latents", "target video tokens", "instruction-to-region alignment", "fine-grained video editing", "RoPE alignment", "motion alignment", "length extrapolation"], "githubStars": 25, "organization": {"_id": "67c4a2574f5d0005fd418d85", "name": "staraj3", "fullname": "University of Technology Sydney", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67c4a1ab71e55dc41e273b5a/8l4iiw2XH5nbIlLURt7ep.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u5728\u7cbe\u786e\u6027\u548c\u7edf\u4e00\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002</li>\n    <li>VideoCoF\u662f\u4e00\u79cd\u65b0\u7684\u94fe\u5e27\u65b9\u6cd5\uff0c\u501f\u9274\u4e86\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u201c\u5148\u770b\uff0c\u540e\u63a8\u7406\uff0c\u518d\u7f16\u8f91\u201d\u7684\u6b65\u9aa4\u6765\u63d0\u9ad8\u89c6\u9891\u7f16\u8f91\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>\u5f15\u5165\u4e86RoPE\u5bf9\u9f50\u7b56\u7565\uff0c\u4ee5\u786e\u4fdd\u8fd0\u52a8\u5bf9\u9f50\u548c\u5ef6\u957f\u89c6\u9891\u957f\u5ea6\u3002</li>\n    <li>\u4f7f\u752850k\u89c6\u9891\u5bf9\uff0cVideoCoF\u5728\u6027\u80fd\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current video editing methods struggle between precision with expert models that need specific inputs, and simplified models that lack accuracy.</li>\n    <li>VideoCoF is a new approach that uses a \"see, reason, then edit\" method, allowing for better video editing without needing user masks.</li>\n    <li>This method includes a reasoning step that helps align instructions with specific video regions for more accurate edits.</li>\n    <li>VideoCoF uses a RoPE alignment strategy to maintain motion consistency and extend video length beyond what was originally trained.</li>\n    <li>With only 50k video pairs, VideoCoF shows top performance on VideoCoF-Bench, proving it is both efficient and effective.</li>\n</ul>"}, "publishedAt": "2025-12-08T06:50:18.000Z", "title": "Unified Video Editing with Temporal Reasoner", "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6486df66373f79a52913e017/iKPp57sIku0K9HVBgHjuu.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07469.png", "numComments": 5, "submittedBy": {"_id": "6486df66373f79a52913e017", "avatarUrl": "/avatars/4741683fbbcec3a615d0a8df62bc6fec.svg", "fullname": "Xiangpeng Yang", "name": "XiangpengYang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "67c4a2574f5d0005fd418d85", "name": "staraj3", "fullname": "University of Technology Sydney", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67c4a1ab71e55dc41e273b5a/8l4iiw2XH5nbIlLURt7ep.png"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2511.18538", "authors": [{"_id": "692e667137312eaa83fd8832", "user": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "name": "Jian Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:34.886Z", "hidden": false}, {"_id": "692e667137312eaa83fd8833", "name": "Xianglong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8834", "name": "Weifeng Lv", "hidden": false}, {"_id": "692e667137312eaa83fd8835", "name": "Ken Deng", "hidden": false}, {"_id": "692e667137312eaa83fd8836", "name": "Shawn Guo", "hidden": false}, {"_id": "692e667137312eaa83fd8837", "name": "Lin Jing", "hidden": false}, {"_id": "692e667137312eaa83fd8838", "name": "Yizhi Li", "hidden": false}, {"_id": "692e667137312eaa83fd8839", "name": "Shark Liu", "hidden": false}, {"_id": "692e667137312eaa83fd883a", "name": "Xianzhen Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883b", "name": "Yuyu Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883c", "name": "Changzai Pan", "hidden": false}, {"_id": "692e667137312eaa83fd883d", "name": "Ensheng Shi", "hidden": false}, {"_id": "692e667137312eaa83fd883e", "name": "Yingshui Tan", "hidden": false}, {"_id": "692e667137312eaa83fd883f", "name": "Renshuai Tao", "hidden": false}, {"_id": "692e667137312eaa83fd8840", "user": {"_id": "66a8e2538407031e388c501f", "avatarUrl": "/avatars/d16d51f7b1e111efd6d0985995b614be.svg", "isPro": false, "fullname": "wjj", "user": "wuyuverse", "type": "user"}, "name": "Jiajun Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:36.195Z", "hidden": false}, {"_id": "692e667137312eaa83fd8841", "name": "Xianjie Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8842", "name": "Zhenhe Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8843", "name": "Daoguang Zan", "hidden": false}, {"_id": "692e667137312eaa83fd8844", "name": "Chenchen Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8845", "user": {"_id": "672c9ba69380700b602c46c1", "avatarUrl": "/avatars/3d0fd966df540d34095d2c84ce449180.svg", "isPro": false, "fullname": "wei zhang", "user": "zwpride", "type": "user"}, "name": "Wei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:37.970Z", "hidden": false}, {"_id": "692e667137312eaa83fd8846", "name": "He Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd8847", "user": {"_id": "62b7fb545233925f253531c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg", "isPro": false, "fullname": "Terry Yue Zhuo", "user": "terryyz", "type": "user"}, "name": "Terry Yue Zhuo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T16:50:22.285Z", "hidden": false}, {"_id": "692e667137312eaa83fd8848", "name": "Kerui Cao", "hidden": false}, {"_id": "692e667137312eaa83fd8849", "name": "Xianfu Cheng", "hidden": false}, {"_id": "692e667137312eaa83fd884a", "name": "Jun Dong", "hidden": false}, {"_id": "692e667137312eaa83fd884b", "name": "Shengjie Fang", "hidden": false}, {"_id": "692e667137312eaa83fd884c", "name": "Zhiwei Fei", "hidden": false}, {"_id": "692e667137312eaa83fd884d", "name": "Xiangyuan Guan", "hidden": false}, {"_id": "692e667137312eaa83fd884e", "name": "Qipeng Guo", "hidden": false}, {"_id": "692e667137312eaa83fd884f", "name": "Zhiguang Han", "hidden": false}, {"_id": "692e667137312eaa83fd8850", "name": "Joseph James", "hidden": false}, {"_id": "692e667137312eaa83fd8851", "name": "Tianqi Luo", "hidden": false}, {"_id": "692e667137312eaa83fd8852", "user": {"_id": "67f1037cd5f976f3d4777390", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0cXH40AcE-M-H21cSNBqZ.png", "isPro": false, "fullname": "RenyuanLi", "user": "RenyuanLi", "type": "user"}, "name": "Renyuan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:17:45.344Z", "hidden": false}, {"_id": "692e667137312eaa83fd8853", "name": "Yuhang Li", "hidden": false}, {"_id": "692e667137312eaa83fd8854", "name": "Yiming Liang", "hidden": false}, {"_id": "692e667137312eaa83fd8855", "name": "Congnan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8856", "name": "Jiaheng Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8857", "name": "Qian Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8858", "name": "Ruitong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8859", "name": "Tyler Loakman", "hidden": false}, {"_id": "692e667137312eaa83fd885a", "name": "Xiangxin Meng", "hidden": false}, {"_id": "692e667137312eaa83fd885b", "name": "Chuang Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885c", "name": "Tianhao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885d", "name": "Jiajun Shi", "hidden": false}, {"_id": "692e667137312eaa83fd885e", "name": "Mingjie Tang", "hidden": false}, {"_id": "692e667137312eaa83fd885f", "name": "Boyang Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8860", "name": "Haowen Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8861", "name": "Yunli Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8862", "user": {"_id": "668619ce7374cac565759731", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668619ce7374cac565759731/tUtiyIQRGsMdq3HB2yYIL.jpeg", "isPro": false, "fullname": "Fanglin Xu", "user": "Tswatery", "type": "user"}, "name": "Fanglin Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T15:15:03.333Z", "hidden": false}, {"_id": "692e667137312eaa83fd8863", "name": "Zihan Xu", "hidden": false}, {"_id": "692e667137312eaa83fd8864", "name": "Fei Yuan", "hidden": false}, {"_id": "692e667137312eaa83fd8865", "user": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "name": "Ge Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:34.025Z", "hidden": false}, {"_id": "692e667137312eaa83fd8866", "user": {"_id": "65f40e83653c231cbaf7defe", "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg", "isPro": false, "fullname": "Jiayi Zhang", "user": "didiforhugface", "type": "user"}, "name": "Jiayi Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:32.149Z", "hidden": false}, {"_id": "692e667137312eaa83fd8867", "name": "Xinhao Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8868", "name": "Wangchunshu Zhou", "hidden": false}, {"_id": "692e667137312eaa83fd8869", "name": "Hualei Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886a", "name": "King Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886b", "name": "Brown Dai", "hidden": false}, {"_id": "692e667137312eaa83fd886c", "name": "Aishan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd886d", "name": "Zhoujun Li", "hidden": false}, {"_id": "692e667137312eaa83fd886e", "name": "Chenghua Lin", "hidden": false}, {"_id": "692e667137312eaa83fd886f", "name": "Tianyu Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8870", "name": "Chao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd8871", "name": "Kai Shen", "hidden": false}, {"_id": "692e667137312eaa83fd8872", "name": "Libo Qin", "hidden": false}, {"_id": "692e667137312eaa83fd8873", "name": "Shuangyong Song", "hidden": false}, {"_id": "692e667137312eaa83fd8874", "name": "Zizheng Zhan", "hidden": false}, {"_id": "692e667137312eaa83fd8875", "name": "Jiajun Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8876", "name": "Jie Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8877", "name": "Zhaoxiang Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8878", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-11-23T17:09:34.000Z", "submittedOnDailyAt": "2025-12-02T02:55:07.234Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "submittedOnDailyBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "upvotes": 240, "discussionId": "692e667237312eaa83fd8879", "ai_summary": "A comprehensive guide to code LLMs, covering their lifecycle from data curation to deployment, including techniques, trade-offs, and research-practice gaps.", "ai_keywords": ["Transformer-based architectures", "HumanEval", "prompting paradigms", "code pre-training", "supervised fine-tuning", "reinforcement learning", "autonomous coding agents", "GPT-4", "Claude", "LLaMA", "StarCoder", "Code LLaMA", "DeepSeek-Coder", "QwenCoder", "code correctness", "security", "contextual awareness", "software-related code tasks", "scaling law", "framework selection", "hyperparameter sensitivity", "model architectures", "dataset comparisons"], "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6539\u53d8\u4e86\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\uff0c\u80fd\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u76f4\u63a5\u8f6c\u5316\u4e3a\u53ef\u8fd0\u884c\u7684\u4ee3\u7801\u3002</li>\n    <li>\u73b0\u6709\u5de5\u5177\u5982Github Copilot\u548cCursor\u63a8\u52a8\u4e86LLMs\u7684\u5546\u4e1a\u5e94\u7528\uff0c\u6210\u529f\u7387\u5df2\u4ece\u4e2a\u4f4d\u6570\u63d0\u5347\u5230\u8d85\u8fc795%\u3002</li>\n    <li>\u672c\u6587\u63d0\u4f9b\u4e86\u4ee3\u7801LLMs\u7684\u7efc\u5408\u5206\u6790\u4e0e\u5b9e\u7528\u6307\u5357\uff0c\u6db5\u76d6\u6a21\u578b\u751f\u547d\u5468\u671f\u7684\u5404\u4e2a\u9636\u6bb5\u3002</li>\n    <li>\u6211\u4eec\u5206\u6790\u4e86\u901a\u7528LLMs\u548c\u4ee3\u7801\u4e13\u7528LLMs\u7684\u4ee3\u7801\u80fd\u529b\uff0c\u63a2\u8ba8\u4e86\u6280\u672f\u3001\u8bbe\u8ba1\u51b3\u7b56\u548c\u6743\u8861\u3002</li>\n    <li>\u6700\u540e\uff0c\u8fdb\u884c\u4e86\u4e00\u7cfb\u5217\u5b9e\u9a8c\uff0c\u5206\u6790\u4ee3\u7801\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7b49\u65b9\u9762\u7684\u8868\u73b0\u53ca\u5f71\u54cd\u56e0\u7d20\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) can turn natural language descriptions into working code, changing how software is developed.</li>\n    <li>Tools like Github Copilot and others have made LLMs popular in the commercial sector.</li>\n    <li>This research provides a detailed guide on how code LLMs work, from data collection to their use in coding tasks.</li>\n    <li>We compare general LLMs (like GPT-4) with those made specifically for coding (like StarCoder).</li>\n    <li>We explore the gap between academic research and real-world coding practices, focusing on issues like code quality and integration into development processes.</li>\n</ul>"}, "publishedAt": "2025-11-23T12:09:34.000Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18538.png", "numComments": 11, "submittedBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "fullname": "Yang Jian", "name": "CSJianYang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 21}, "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.14993", "authors": [{"_id": "691e819a3c64d32b036458c0", "name": "Vladimir Arkhipkin", "hidden": false}, {"_id": "691e819a3c64d32b036458c1", "user": {"_id": "67bcb1012906865678a11f91", "avatarUrl": "/avatars/80fb0cc24f0d16c4740f9115b680df0f.svg", "isPro": false, "fullname": "Vladimir Korviakov", "user": "korviakov", "type": "user"}, "name": "Vladimir Korviakov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:03.925Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c2", "user": {"_id": "63cfa7ef3b7adfa99c0eb524", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674553277288-noauth.jpeg", "isPro": false, "fullname": "Nikolai Gerasimenko", "user": "nikgerasimenko", "type": "user"}, "name": "Nikolai Gerasimenko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:55.225Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c3", "name": "Denis Parkhomenko", "hidden": false}, {"_id": "691e819a3c64d32b036458c4", "user": {"_id": "64e4c7764af6c29a0697f57b", "avatarUrl": "/avatars/efc4e9f9b105586fd090b22a1bc7dbb7.svg", "isPro": false, "fullname": "Viacheslav Vasilev", "user": "vvasilev", "type": "user"}, "name": "Viacheslav Vasilev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:10.246Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c5", "user": {"_id": "68838d809080cc7010edf5e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68838d809080cc7010edf5e2/xBqg5ggt_PfLkiDLmsZxx.jpeg", "isPro": false, "fullname": "Alexey Letunovskiy", "user": "AlexeyLetunovskiy", "type": "user"}, "name": "Alexey Letunovskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:55.594Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c6", "user": {"_id": "678781c9e3c3c0163db4f99c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5Vi5J_XS9fbN2gDHfzHlh.png", "isPro": false, "fullname": "Kovaleva Maria", "user": "makovka2000", "type": "user"}, "name": "Maria Kovaleva", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T10:15:36.018Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c7", "user": {"_id": "67f38b14da604b256d393662", "avatarUrl": "/avatars/63445143f68995becc7702868387555b.svg", "isPro": false, "fullname": "Nikolay Vaulin", "user": "nvvaulin", "type": "user"}, "name": "Nikolai Vaulin", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:01.695Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c8", "user": {"_id": "62653f745f6f2e14d6ae128c", "avatarUrl": "/avatars/944b564ab810a5b31fa5e45f63bdf4ee.svg", "isPro": false, "fullname": "Ivan Kirillov", "user": "funnylittleman", "type": "user"}, "name": "Ivan Kirillov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T20:40:14.372Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c9", "user": {"_id": "60991602f7c9c7bf29603a88", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60991602f7c9c7bf29603a88/me8VFG_06ZOovTLldF-L7.jpeg", "isPro": false, "fullname": "Lev Novitskiy", "user": "leffff", "type": "user"}, "name": "Lev Novitskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:01:59.489Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ca", "name": "Denis Koposov", "hidden": false}, {"_id": "691e819a3c64d32b036458cb", "user": {"_id": "6628b73c35d27082500034f2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628b73c35d27082500034f2/CznOeIbjzJ9DmJaGzlWPD.jpeg", "isPro": false, "fullname": "Nikita Kiselev", "user": "kisnikser", "type": "user"}, "name": "Nikita Kiselev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:11.927Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cc", "user": {"_id": "654d4993938fbf1e695b589a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qY3MC94Uz3FGf_HQtHseK.png", "isPro": false, "fullname": "Varlamov Alexander", "user": "Alphonsce", "type": "user"}, "name": "Alexander Varlamov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:08.889Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cd", "user": {"_id": "6616719945336ca7746eaa38", "avatarUrl": "/avatars/ac77ebda8507d75376973144263beb83.svg", "isPro": false, "fullname": "Dmitrii Mikhailov", "user": "Botsman11", "type": "user"}, "name": "Dmitrii Mikhailov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:56.980Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ce", "name": "Vladimir Polovnikov", "hidden": false}, {"_id": "691e819a3c64d32b036458cf", "name": "Andrey Shutkin", "hidden": false}, {"_id": "691e819a3c64d32b036458d0", "name": "Ilya Vasiliev", "hidden": false}, {"_id": "691e819a3c64d32b036458d1", "name": "Julia Agafonova", "hidden": false}, {"_id": "691e819a3c64d32b036458d2", "name": "Anastasiia Kargapoltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d3", "user": {"_id": "65df46ac43bf08064bd8e656", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65df46ac43bf08064bd8e656/yR72X3fnBhdy_i34VqBxT.jpeg", "isPro": false, "fullname": "Anna Dmitrienko", "user": "dmitrienkoae", "type": "user"}, "name": "Anna Dmitrienko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:09.131Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d4", "name": "Anastasia Maltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d5", "user": {"_id": "66f1a9c87ce3d2d3938999ce", "avatarUrl": "/avatars/3016b15d4bae2591313537a4ea59b268.svg", "isPro": false, "fullname": "Anna Averchenkova", "user": "aaveraa", "type": "user"}, "name": "Anna Averchenkova", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:11.123Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d6", "name": "Olga Kim", "hidden": false}, {"_id": "691e819a3c64d32b036458d7", "name": "Tatiana Nikulina", "hidden": false}, {"_id": "691e819a3c64d32b036458d8", "user": {"_id": "6669a678465d1d802181e456", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6669a678465d1d802181e456/ZCthBBhDFQnh0bBkgUQUU.png", "isPro": false, "fullname": "Denis Dimitrov", "user": "dendimitrov", "type": "user"}, "name": "Denis Dimitrov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:08.661Z", "hidden": false}], "publishedAt": "2025-11-19T00:23:22.000Z", "submittedOnDailyAt": "2025-11-20T00:19:10.078Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "upvotes": 209, "discussionId": "691e819b3c64d32b036458d9", "projectPage": "https://kandinskylab.ai/", "githubRepo": "https://github.com/kandinskylab/kandinsky-5", "ai_summary": "Kandinsky 5.0 is a family of state-of-the-art generative models for high-resolution images and short videos, featuring model lineups with varying parameters and enhanced training techniques to achieve superior quality and performance.", "ai_keywords": ["foundation models", "high-resolution image synthesis", "10-second video synthesis", "image generation models", "text-to-video models", "image-to-video models", "multi-stage training pipeline", "self-supervised fine-tuning", "reinforcement learning", "pre-training", "quality-enhancement techniques", "architectural optimizations", "training optimizations", "inference optimizations", "human evaluation", "generative framework", "open-source code", "training checkpoints"], "githubStars": 477, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecdKandinsky 5.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c10\u79d2\u89c6\u9891\u5408\u6210\u7684\u5148\u8fdb\u57fa\u7840\u6a21\u578b\u7cfb\u5217\u3002</li>\n    <li>Kandinsky 5.0\u5305\u62ec\u4e09\u79cd\u6838\u5fc3\u6a21\u578b\uff1a6B\u53c2\u6570\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u30012B\u53c2\u6570\u7684\u5feb\u901f\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u548c19B\u53c2\u6570\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002</li>\n    <li>\u8be6\u7ec6\u8bf4\u660e\u4e86\u6570\u636e\u6574\u7406\u751f\u547d\u5468\u671f\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u5904\u7406\u3001\u8fc7\u6ee4\u548c\u805a\u7c7b\uff0c\u4ee5\u652f\u6301\u591a\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>\u5c55\u793a\u4e86\u65b0\u9896\u7684\u67b6\u6784\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u4f18\u5316\uff0c\u4f7fKandinsky 5.0\u5728\u751f\u6210\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002</li>\n    <li>\u5e0c\u671b\u901a\u8fc7\u53d1\u5e03\u5f00\u6e90\u4ee3\u7801\u548c\u8bad\u7ec3\u68c0\u67e5\u70b9\uff0c\u63a8\u52a8\u9ad8\u8d28\u91cf\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u548c\u4fbf\u5229\u6027\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Kandinsky 5.0 is a new set of advanced models for creating high-quality images and short videos.</li>\n    <li>It includes three main models: Kandinsky 5.0 Image Lite for image generation, Kandinsky 5.0 Video Lite for quick video creation, and Kandinsky 5.0 Video Pro for top-quality video generation.</li>\n    <li>The report details the process of gathering and preparing data for training these models, including various techniques to improve quality.</li>\n    <li>Kandinsky 5.0 features improvements in its design and training methods, allowing it to generate high-quality content quickly.</li>\n    <li>The models are open-source, making them accessible for researchers and developers to use in different creative projects.</li>\n</ul>"}, "publishedAt": "2025-11-18T19:23:22.000Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.14993.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "isAuthorParticipating": true}, {"paper": {"id": "2511.08892", "authors": [{"_id": "69154dffa1b06ca3cc81351e", "name": "Weihao Tan", "hidden": false}, {"_id": "69154dffa1b06ca3cc81351f", "name": "Xiangyang Li", "hidden": false}, {"_id": "69154dffa1b06ca3cc813520", "name": "Yunhao Fang", "hidden": false}, {"_id": "69154dffa1b06ca3cc813521", "name": "Heyuan Yao", "hidden": false}, {"_id": "69154dffa1b06ca3cc813522", "name": "Shi Yan", "hidden": false}, {"_id": "69154dffa1b06ca3cc813523", "name": "Hao Luo", "hidden": false}, {"_id": "69154dffa1b06ca3cc813524", "name": "Tenglong Ao", "hidden": false}, {"_id": "69154dffa1b06ca3cc813525", "name": "Huihui Li", "hidden": false}, {"_id": "69154dffa1b06ca3cc813526", "name": "Hongbin Ren", "hidden": false}, {"_id": "69154dffa1b06ca3cc813527", "user": {"_id": "6369d92f64aad59d4d44d362", "avatarUrl": "/avatars/73956400cfbfd53116aefc17b3c9f0fd.svg", "isPro": false, "fullname": "Yi", "user": "Bairen", "type": "user"}, "name": "Bairen Yi", "status": "claimed_verified", "statusLastChangedAt": "2025-11-17T10:31:49.043Z", "hidden": false}, {"_id": "69154dffa1b06ca3cc813528", "name": "Yujia Qin", "hidden": false}, {"_id": "69154dffa1b06ca3cc813529", "name": "Bo An", "hidden": false}, {"_id": "69154dffa1b06ca3cc81352a", "name": "Libin Liu", "hidden": false}, {"_id": "69154dffa1b06ca3cc81352b", "name": "Guang Shi", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/FVxpP05KrXQ1HkQ1G1uNl.mp4"], "publishedAt": "2025-11-12T02:01:26.000Z", "submittedOnDailyAt": "2025-11-13T00:49:21.639Z", "title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.", "upvotes": 185, "discussionId": "69154dffa1b06ca3cc81352c", "projectPage": "https://www.lumine-ai.org/", "ai_summary": "Lumine, a vision-language model-based agent, completes complex missions in real-time across different 3D open-world environments with human-like efficiency and zero-shot cross-game generalization.", "ai_keywords": ["vision-language model", "end-to-end", "3D open-world environments", "human-like interaction", "real-time", "zero-shot cross-game generalization"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>Lumine\u662f\u7b2c\u4e00\u4e2a\u5f00\u653e\u7684\u901a\u7528\u667a\u80fd\u4f53\u5f00\u53d1\u914d\u65b9\uff0c\u80fd\u591f\u5728\u590d\u6742\u76843D\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u5b9e\u65f6\u5b8c\u6210\u957f\u8fbe\u6570\u5c0f\u65f6\u7684\u4efb\u52a1\u3002</li>\n    <li>\u5b83\u91c7\u7528\u7c7b\u4f3c\u4eba\u7c7b\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u5c06\u611f\u77e5\u3001\u63a8\u7406\u548c\u884c\u52a8\u7edf\u4e00\u5728\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u7cfb\u7edf\u4e2d\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u3002</li>\n    <li>Lumine\u5728\u300a\u539f\u795e\u300b\u4e2d\u8bad\u7ec3\uff0c\u80fd\u591f\u4e0e\u4eba\u7c7b\u6548\u7387\u76f8\u5f53\u5730\u5b8c\u6210\u4e94\u5c0f\u65f6\u7684\u4e3b\u7ebf\u4efb\u52a1\uff0c\u5e76\u6839\u636e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6267\u884c\u5404\u79cd\u4efb\u52a1\u3002</li>\n    <li>\u5b83\u4e0d\u4ec5\u5728\u7279\u5b9a\u6e38\u620f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fd8\u80fd\u591f\u5728\u6ca1\u6709\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5176\u4ed6\u6e38\u620f\u4e2d\u5b8c\u6210\u957f\u8fbe100\u5206\u949f\u7684\u4efb\u52a1\u3002</li>\n    <li>Lumine\u7684\u6210\u529f\u5c55\u793a\u4e86\u5728\u5f00\u653e\u73af\u5883\u4e2d\u5f00\u53d1\u901a\u7528\u667a\u80fd\u4f53\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Lumine is a new open recipe for creating generalist agents that can handle long, complex tasks in 3D open-world games.</li>\n    <li>It uses a human-like approach to combine sensing, thinking, and acting, using a vision-language model.</li>\n    <li>Lumine processes visual information quickly and efficiently to perform actions based on natural language commands.</li>\n    <li>It can complete the five-hour main storyline of Genshin Impact at a level similar to human players and follow various instructions across different tasks.</li>\n    <li>Lumine shows strong performance in other games without needing extra training, successfully completing missions in Wuthering Waves and Honkai: Star Rail.</li>\n</ul>"}, "publishedAt": "2025-11-11T21:01:26.000Z", "title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds", "summary": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/FVxpP05KrXQ1HkQ1G1uNl.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.08892.png", "numComments": 12, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>DeepSeek-V3.2\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u51fa\u8272\u7684\u63a8\u7406\u80fd\u529b\u548c\u4ee3\u7406\u6027\u80fd\u3002</li>\n    <li>\u5f15\u5165\u4e86\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff08DSA\uff09\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7fDeepSeek-V3.2\u7684\u8868\u73b0\u4e0eGPT-5\u76f8\u5f53\uff0c\u7279\u522b\u7248\u672c\u8d85\u8d8aGPT-5\u3002</li>\n    <li>DeepSeek-V3.2\u57282025\u5e74\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\u548c\u56fd\u9645\u4fe1\u606f\u5b66\u5965\u6797\u5339\u514b\u4e2d\u8868\u73b0\u4f18\u79c0\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u5927\u89c4\u6a21\u7684\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff0c\u4ee5\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that combines high efficiency with strong reasoning and performance.</li>\n    <li>It features DeepSeek Sparse Attention (DSA), which makes the model faster and more efficient, especially with long texts.</li>\n    <li>Using a scalable reinforcement learning approach, it performs as well as GPT-5 and even surpasses it in some areas.</li>\n    <li>DeepSeek-V3.2 has shown excellent reasoning skills, winning gold medals in major international competitions, like the IMO and IOI.</li>\n    <li>A new data generation pipeline helps improve the model's ability to understand and follow instructions in complex tasks.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.22699", "authors": [{"_id": "692d06234397b1ec214f6788", "name": "Z-Image Team", "hidden": false}, {"_id": "692d06234397b1ec214f6789", "user": {"_id": "692d0e6bb14ceb758205d0dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/692d0e6bb14ceb758205d0dd/gGVq2KSJE11Sr3LkVn-n5.jpeg", "isPro": false, "fullname": "Huanqia Cai", "user": "Orion-Cai", "type": "user"}, "name": "Huanqia Cai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:26.669Z", "hidden": false}, {"_id": "692d06234397b1ec214f678a", "user": {"_id": "67777b7a8376dfe003afa951", "avatarUrl": "/avatars/2af9d3181306d4c53329d047eeadaf1e.svg", "isPro": false, "fullname": "Sihan Cao", "user": "Sihan-Cao", "type": "user"}, "name": "Sihan Cao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:33.191Z", "hidden": false}, {"_id": "692d06234397b1ec214f678b", "user": {"_id": "64a54586c0f13de8e7093314", "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg", "isPro": false, "fullname": "Ruoyi Du", "user": "RuoyiDu", "type": "user"}, "name": "Ruoyi Du", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:18:53.948Z", "hidden": false}, {"_id": "692d06234397b1ec214f678c", "name": "Peng Gao", "hidden": false}, {"_id": "692d06234397b1ec214f678d", "name": "Steven Hoi", "hidden": false}, {"_id": "692d06234397b1ec214f678e", "name": "Shijie Huang", "hidden": false}, {"_id": "692d06234397b1ec214f678f", "name": "Zhaohui Hou", "hidden": false}, {"_id": "692d06234397b1ec214f6790", "user": {"_id": "662a0f2d4bab737c1a279843", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662a0f2d4bab737c1a279843/fC2p3mjMHkVpDQdEqkuR4.png", "isPro": false, "fullname": "Dengyang Jiang", "user": "DyJiang", "type": "user"}, "name": "Dengyang Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:15.555Z", "hidden": false}, {"_id": "692d06234397b1ec214f6791", "user": {"_id": "6537e8eab01250d1d6efed3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gMx73gwdfEhcCFioStGCE.jpeg", "isPro": false, "fullname": "Xin", "user": "Srameo", "type": "user"}, "name": "Xin Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:15.288Z", "hidden": false}, {"_id": "692d06234397b1ec214f6792", "name": "Liangchen Li", "hidden": false}, {"_id": "692d06234397b1ec214f6793", "user": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "name": "Zhen Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:16.899Z", "hidden": false}, {"_id": "692d06234397b1ec214f6794", "user": {"_id": "6740a5730bb4a675446a80ad", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740a5730bb4a675446a80ad/dmruwMdQK3zluJm7YXUtN.jpeg", "isPro": false, "fullname": "Zhong-Yu Li", "user": "lzyhha", "type": "user"}, "name": "Zhong-Yu Li", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:08.972Z", "hidden": false}, {"_id": "692d06234397b1ec214f6795", "name": "David Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6796", "name": "Dongyang Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6797", "user": {"_id": "66332475351231c428653b6b", "avatarUrl": "/avatars/3997bcde54158f7ff9770c85a20875f1.svg", "isPro": false, "fullname": "Junhan Shi", "user": "jshmsjh", "type": "user"}, "name": "Junhan Shi", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:38.865Z", "hidden": false}, {"_id": "692d06234397b1ec214f6798", "user": {"_id": "64379d79fac5ea753f1c10f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png", "isPro": false, "fullname": "Jerry Wu", "user": "QJerry", "type": "user"}, "name": "Qilong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:18.709Z", "hidden": false}, {"_id": "692d06234397b1ec214f6799", "name": "Feng Yu", "hidden": false}, {"_id": "692d06234397b1ec214f679a", "name": "Chi Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679b", "name": "Shifeng Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679c", "user": {"_id": "641988978e0baaeed5a066c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641988978e0baaeed5a066c6/TdCjJ63gw5gdX1RqTvy9a.png", "isPro": false, "fullname": "Shilin", "user": "zsLin", "type": "user"}, "name": "Shilin Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T16:24:44.624Z", "hidden": false}], "publishedAt": "2025-11-27T18:52:07.000Z", "submittedOnDailyAt": "2025-12-01T00:38:17.269Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "submittedOnDailyBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "upvotes": 155, "discussionId": "692d06234397b1ec214f679d", "projectPage": "https://tongyi-mai.github.io/Z-Image-blog/", "githubRepo": "https://github.com/Tongyi-MAI/Z-Image", "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.", "ai_keywords": ["Scalable Single-Stream Diffusion Transformer", "S3-DiT", "diffusion transformer", "omni-pre-training", "instruction-following capabilities", "photorealistic image generation", "bilingual text rendering", "distillation scheme", "reward post-training", "H800 GPU", "VRAM"], "githubStars": 5595, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "summary_zh": "<ul>\n    <li>\u5f53\u524d\u9ad8\u6027\u80fd\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e3b\u8981\u7531\u4e13\u6709\u7cfb\u7edf\u4e3b\u5bfc\uff0c\u5982Nano Banana Pro\u548cSeedream 4.0\u3002</li>\n    <li>\u5f00\u6e90\u66ff\u4ee3\u54c1\u5982Qwen-Image\u548cHunyuan-Image-3.0\u53c2\u6570\u5e9e\u5927\uff0820B\u81f380B\uff09\uff0c\u4e0d\u9002\u5408\u666e\u901a\u786c\u4ef6\u4f7f\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Z-Image\uff0c\u4e00\u4e2a\u9ad8\u6548\u76846B\u53c2\u6570\u751f\u6210\u6a21\u578b\uff0c\u57fa\u4e8e\u53ef\u6269\u5c55\u7684\u5355\u6d41\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u3002</li>\n    <li>Z-Image\u901a\u8fc7\u4f18\u5316\u6574\u4e2a\u6a21\u578b\u751f\u547d\u5468\u671f\uff0c\u5b8c\u6210\u8bad\u7ec3\u53ea\u9700314K H800 GPU\u5c0f\u65f6\uff0c\u6210\u672c\u7ea6\u4e3a63\u4e07\u7f8e\u5143\u3002</li>\n    <li>Z-Image\u5728\u771f\u5b9e\u611f\u56fe\u50cf\u751f\u6210\u548c\u53cc\u8bed\u6587\u672c\u6e32\u67d3\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u4e0e\u9876\u7ea7\u5546\u4e1a\u6a21\u578b\u7ade\u4e89\uff0c\u5e76\u4e14\u6211\u4eec\u516c\u5f00\u53d1\u5e03\u4e86\u4ee3\u7801\u548c\u6f14\u793a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>High-performance image generation models are mostly proprietary, like Nano Banana Pro and Seedream 4.0, while open-source options have very large model sizes (20B to 80B parameters), making them hard to use on regular hardware.</li>\n    <li>The new Z-Image model has 6 billion parameters and uses a unique Scalable Single-Stream Diffusion Transformer (S3-DiT) design, focusing on efficiency rather than just size.</li>\n    <li>Z-Image was trained quickly using 314K H800 GPU hours (around $630K), and it includes a method for fast inference on both enterprise and consumer-grade hardware.</li>\n    <li>The model also includes Z-Image-Edit, which can follow instructions well for image editing tasks.</li>\n    <li>Z-Image performs impressively in generating photorealistic images and handling bilingual text, showing it can compete with top commercial models while being more accessible and cost-effective.</li>\n</ul>"}, "publishedAt": "2025-11-27T13:52:07.000Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png", "numComments": 3, "submittedBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "fullname": "Zhen Li", "name": "Paper99", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 29}, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20626", "authors": [{"_id": "6927ab26243b2216fb75cd1b", "name": "Wei He", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1c", "user": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "name": "Kai Han", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:11.052Z", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1d", "name": "Hang Zhou", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1e", "name": "Hanting Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1f", "name": "Zhicheng Liu", "hidden": false}, {"_id": "6927ab26243b2216fb75cd20", "name": "Xinghao Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd21", "name": "Yunhe Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "publishedAt": "2025-11-25T18:48:05.000Z", "submittedOnDailyAt": "2025-11-26T23:08:13.066Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "submittedOnDailyBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "upvotes": 154, "discussionId": "6927ab27243b2216fb75cd22", "projectPage": "https://github.com/huawei-noah/noah-research/tree/master/ROOT", "githubRepo": "https://github.com/huawei-noah/noah-research", "ai_summary": "ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.", "ai_keywords": ["large language models", "LLMs", "momentum orthogonalization", "dimensional fragility", "outlier-induced noise", "adaptive Newton iterations", "proximal optimization", "Muon", "Adam-based optimizers", "robust optimizer"], "githubStars": 909, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u52a0\u5267\u4e86\u7b97\u6cd5\u4e0d\u7cbe\u786e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668ROOT\uff0c\u65e8\u5728\u901a\u8fc7\u53cc\u91cd\u7a33\u5065\u673a\u5236\u6765\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>ROOT\u91c7\u7528\u81ea\u9002\u5e94\u725b\u987f\u8fed\u4ee3\u7684\u7ef4\u5ea6\u7a33\u5065\u6b63\u4ea4\u5316\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4e0d\u540c\u67b6\u6784\u914d\u7f6e\u4e0b\u7684\u7cbe\u786e\u5ea6\u3002</li>\n    <li>ROOT\u8fd8\u901a\u8fc7\u90bb\u8fd1\u4f18\u5316\u6846\u67b6\u6291\u5236\u5f02\u5e38\u503c\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u610f\u4e49\u7684\u68af\u5ea6\u65b9\u5411\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cROOT\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e2d\u6bd4Muon\u548c\u57fa\u4e8eAdam\u7684\u4f18\u5316\u5668\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) face challenges with training stability and precision as they grow in size.</li>\n    <li>Recent optimizers have improved efficiency but struggle with robustness against noise and dimensional changes.</li>\n    <li>ROOT is a new optimizer that strengthens training stability using two innovative methods: dimension-robust orthogonalization and noise suppression.</li>\n    <li>Experiments show ROOT outperforms other optimizers like Muon and Adam in tough conditions, achieving faster training and better results.</li>\n    <li>The ROOT code will be available online for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:48:05.000Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20626.png", "numComments": 2, "submittedBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "fullname": "hankai", "name": "hankaixyz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.11793", "authors": [{"_id": "691be81b6bfd5965c0fd37e2", "name": "MiroMind Team", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e3", "name": "Song Bai", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e4", "name": "Lidong Bing", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e5", "name": "Carson Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e6", "name": "Guanzheng Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e7", "user": {"_id": "632dab84fdb35759ea6646a0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632dab84fdb35759ea6646a0/IxO5mbtzHJsr0YHW-YtVk.jpeg", "isPro": false, "fullname": "Yuntao Chen", "user": "YuntaoChen", "type": "user"}, "name": "Yuntao Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:45.403Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e8", "name": "Zhe Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e9", "name": "Ziyi Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ea", "name": "Jifeng Dai", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37eb", "name": "Xuan Dong", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ec", "name": "Yue Deng", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ed", "name": "Yunjie Fu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ee", "name": "Junqi Ge", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ef", "name": "Chenxia Han", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f0", "name": "Tammy Huang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f1", "name": "Zhenhang Huang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f2", "name": "Jerry Jiao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f3", "name": "Shilei Jiang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f4", "name": "Tianyu Jiao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f5", "user": {"_id": "64be2455b567ae97c34bb948", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64be2455b567ae97c34bb948/QuCdStDDGaXjDmp4V-dBj.jpeg", "isPro": false, "fullname": "Xiaoqi Jian", "user": "mx1024", "type": "user"}, "name": "Xiaoqi Jian", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:52.417Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f6", "name": "Lei Lei", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f7", "user": {"_id": "6466e7be1343dce20e59191b", "avatarUrl": "/avatars/6779560b203c3773dc76372c0b8cbe4e.svg", "isPro": false, "fullname": "Li Ruilin", "user": "Eric-LRL-130", "type": "user"}, "name": "Ruilin Li", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:43.774Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f8", "name": "Ryan Luo", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f9", "name": "Tiantong Li", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fa", "name": "Xiang Lin", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fb", "name": "Ziyuan Liu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fc", "name": "Zhiqi Li", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fd", "name": "Jie Ni", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fe", "name": "Qiang Ren", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ff", "name": "Pax Sun", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3800", "name": "Shiqian Su", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3801", "name": "Chenxin Tao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3802", "name": "Bin Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3803", "name": "Hellen Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3804", "name": "Haonan Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3805", "name": "James Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3806", "name": "Jin Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3807", "name": "Jojo Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3808", "name": "Letian Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3809", "name": "Shizun Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380a", "user": {"_id": "63d34004b734eaa4d4faeccf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/zf6d1p0GN8gsagi8N6y4V.jpeg", "isPro": false, "fullname": "Weizhi Wang", "user": "weizhiwang", "type": "user"}, "name": "Weizhi Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:47.000Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380b", "name": "Zixuan Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380c", "name": "Jinfan Xu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380d", "name": "Sen Xing", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380e", "user": {"_id": "637f347a52229c639211bee8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f347a52229c639211bee8/I9_PET-_6SJQJ6hXrACV4.jpeg", "isPro": false, "fullname": "Chenyu Yang", "user": "cyyang822", "type": "user"}, "name": "Chenyu Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:48.746Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380f", "user": {"_id": "6239888e7fef05b7bdd5fcff", "avatarUrl": "/avatars/54fcc756b8c0936b6bb410c6e0e02d75.svg", "isPro": false, "fullname": "Hai Ye", "user": "oceanpty", "type": "user"}, "name": "Hai Ye", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:50.623Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3810", "name": "Jiaheng Yu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3811", "name": "Yue Yu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3812", "name": "Muyan Zhong", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3813", "name": "Tianchen Zhao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3814", "name": "Xizhou Zhu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3815", "name": "Yanpeng Zhou", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3816", "name": "Yifan Zhang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3817", "name": "Zhi Zhu", "hidden": false}], "publishedAt": "2025-11-14T18:52:07.000Z", "submittedOnDailyAt": "2025-11-18T02:00:07.077Z", "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.", "upvotes": 153, "discussionId": "691be81b6bfd5965c0fd3818", "projectPage": "https://dr.miromind.ai/", "githubRepo": "https://github.com/MiroMindAI/MiroThinker", "githubStars": 1133, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86MiroThinker v1.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u7814\u7a76\u4ee3\u7406\uff0c\u65e8\u5728\u589e\u5f3a\u5de5\u5177\u8f85\u52a9\u63a8\u7406\u548c\u4fe1\u606f\u83b7\u53d6\u80fd\u529b\u3002</li>\n    <li>MiroThinker\u901a\u8fc7\u5728\u6a21\u578b\u5c42\u9762\u63a2\u7d22\u4ea4\u4e92\u6269\u5c55\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u4ee3\u7406\u4e0e\u73af\u5883\u4e4b\u95f4\u7684\u4e92\u52a8\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u7684\u4ea4\u4e92\u6269\u5c55\uff0c\u652f\u6301\u591a\u8fbe600\u6b21\u5de5\u5177\u8c03\u7528\uff0c\u9002\u5e94\u590d\u6742\u7684\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>MiroThinker\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8d85\u8fc7\u4e86\u4ee5\u5f80\u7684\u5f00\u6e90\u4ee3\u7406\uff0c\u63a5\u8fd1\u5546\u4e1a\u6a21\u578b\u7684\u6c34\u5e73\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u4ea4\u4e92\u6df1\u5ea6\u7684\u6269\u5c55\u4e0e\u6a21\u578b\u7684\u89c4\u6a21\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u5177\u6709\u76f8\u4f3c\u7684\u6269\u5c55\u7279\u6027\uff0c\u662f\u6784\u5efa\u4e0b\u4e00\u4ee3\u5f00\u653e\u7814\u7a76\u4ee3\u7406\u7684\u91cd\u8981\u7ef4\u5ea6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MiroThinker v1.0 is an open-source research agent aimed at improving reasoning and information-seeking skills.</li>\n    <li>It focuses on enhancing how the model interacts with its environment, rather than just increasing model size or context length.</li>\n    <li>The model uses reinforcement learning to effectively manage many tool calls (up to 600) during tasks, supporting complex research activities.</li>\n    <li>MiroThinker outperforms previous open-source agents in various benchmarks, showing accuracy rates that approach those of advanced commercial models.</li>\n    <li>The research highlights that better performance comes from deeper and more frequent interactions with the environment, suggesting that this interaction is crucial for future research agent development.</li>\n</ul>"}, "publishedAt": "2025-11-14T13:52:07.000Z", "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling", "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.11793.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "isAuthorParticipating": false}, {"paper": {"id": "2511.20785", "authors": [{"_id": "692d430f4397b1ec214f696e", "user": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "name": "Zuhao Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:11.311Z", "hidden": false}, {"_id": "692d430f4397b1ec214f696f", "user": {"_id": "6690f58e2f9f6f9c88e91031", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6690f58e2f9f6f9c88e91031/QQ_VoEh7NlE6BUvii08zk.png", "isPro": false, "fullname": "Sudong Wang", "user": "xiao45791", "type": "user"}, "name": "Sudong Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:14.173Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6970", "user": {"_id": "64bb77e786e7fb5b8a317a43", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bb77e786e7fb5b8a317a43/J0jOrlZJ9gazdYaeSH2Bo.png", "isPro": false, "fullname": "kcz", "user": "kcz358", "type": "user"}, "name": "Kaichen Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T10:06:41.343Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6971", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:09.461Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6972", "name": "Sicong Leng", "hidden": false}, {"_id": "692d430f4397b1ec214f6973", "name": "Yifan Zhang", "hidden": false}, {"_id": "692d430f4397b1ec214f6974", "name": "Chengwei Qin", "hidden": false}, {"_id": "692d430f4397b1ec214f6975", "name": "Shijian Lu", "hidden": false}, {"_id": "692d430f4397b1ec214f6976", "name": "Xingxuan Li", "hidden": false}, {"_id": "692d430f4397b1ec214f6977", "user": {"_id": "6454685a548f22be598414c4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg", "isPro": false, "fullname": "Lidong Bing", "user": "LidongBing", "type": "user"}, "name": "Lidong Bing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:49:36.056Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "publishedAt": "2025-11-25T19:22:48.000Z", "submittedOnDailyAt": "2025-12-02T00:35:56.511Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "submittedOnDailyBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "upvotes": 148, "discussionId": "692d430f4397b1ec214f6978", "projectPage": "https://evolvinglmms-lab.github.io/LongVT/", "githubRepo": "https://github.com/EvolvingLMMs-Lab/LongVT", "ai_summary": "LongVT, an end-to-end framework, enhances long video reasoning by interleaving global and local analysis using multimodal tools, outperforming existing methods on challenging benchmarks.", "ai_keywords": ["multimodal models", "video reasoning", "textual Chain-of-Thought", "hallucinations", "long-form videos", "temporal grounding", "video cropping", "fine-grained question-answering", "VideoSIAH", "tool-integrated cold-start supervised fine-tuning", "agentic reinforcement learning", "agentic reinforcement fine-tuning"], "githubStars": 121, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u89c6\u9891\u63a8\u7406\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u5bb9\u6613\u4ea7\u751f\u9519\u8bef\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86LongVT\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u601d\u7ef4\u5de5\u5177\u94fe\u6765\u5e2e\u52a9\u7406\u89e3\u957f\u89c6\u9891\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5229\u7528LMMs\u7684\u65f6\u95f4\u57fa\u7840\u80fd\u529b\uff0c\u805a\u7126\u4e8e\u7279\u5b9a\u89c6\u9891\u7247\u6bb5\u4ee5\u83b7\u53d6\u7ec6\u8282\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86VideoSIAH\u6570\u636e\u96c6\uff0c\u5305\u542b247.9K\u6837\u672c\u7528\u4e8e\u8bad\u7ec3\uff0c\u5e2e\u52a9\u63d0\u9ad8\u957f\u89c6\u9891\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bad\u7ec3\u7b56\u7565\uff0cLongVT\u5728\u591a\u9879\u957f\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large multimodal models (LMMs) can analyze videos using text but often make mistakes, especially with long videos.</li>\n    <li>To improve video comprehension, we developed LongVT, which mimics how humans watch videos by skimming and then focusing on specific clips.</li>\n    <li>LongVT uses LMMs' ability to focus on parts of videos to better answer questions by connecting visual evidence with the questions.</li>\n    <li>We created a new dataset called VideoSIAH to help train and test this method, containing a large number of samples for various tasks.</li>\n    <li>LongVT outperforms existing methods in multiple tests for understanding and reasoning with long videos, and all resources are available online.</li>\n</ul>"}, "publishedAt": "2025-11-25T14:22:48.000Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20785.png", "numComments": 3, "submittedBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "fullname": "Zuhao Yang", "name": "mwxely", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u987a\u5e8f\u8ba1\u7b97\u548c\u957f\u671f\u4e0d\u4e00\u81f4\u6027\uff0c\u5f71\u54cd\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u5934\u50cf\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Live Avatar\uff0c\u4e00\u4e2a\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u3001\u65e0\u9650\u957f\u5ea6\u5934\u50cf\u751f\u6210\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u4e86140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\u6027\uff08TPP\uff09\uff0c\u5728\u591a\u4e2aGPU\u4e0a\u5e76\u884c\u5904\u7406\u53bb\u566a\u6b65\u9aa4\uff0c\u63d0\u9ad8\u4e86\u5b9e\u65f6\u6d41\u5a92\u4f53\u7684\u7a33\u5b9a\u6027\u548c\u4f4e\u5ef6\u8fdf\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u6eda\u52a8\u6c89\u6d78\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u91cd\u65b0\u6821\u51c6\u5916\u89c2\u6765\u7ef4\u6301\u5e8f\u5217\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u8eab\u4efd\u6f02\u79fb\u548c\u989c\u8272\u4f2a\u5f71\u3002</li>\n    <li>Live Avatar\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e8620 FPS\u7684\u7aef\u5230\u7aef\u751f\u6210\uff0c\u9996\u6b21\u5728\u6b64\u89c4\u6a21\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u4fdd\u771f\u5934\u50cf\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new system for creating realistic, animated avatars in real-time using advanced AI technology.</li>\n    <li>The system uses a 14-billion-parameter diffusion model to generate high-quality video content efficiently.</li>\n    <li>It features a technique called Timestep-forcing Pipeline Parallelism (TPP) that allows multiple GPUs to work together, enabling faster video generation.</li>\n    <li>To improve consistency in the videos, Live Avatar uses the Rolling Sink Frame Mechanism (RSFM) to maintain the avatar's appearance over time.</li>\n    <li>Live Avatar achieves impressive performance, generating video at 20 frames per second on multiple GPUs, setting a new standard for real-time avatar creation.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2511.18423", "authors": [{"_id": "692518ff16eb3a9f1310391c", "name": "B. Y. Yan", "hidden": false}, {"_id": "692518ff16eb3a9f1310391d", "name": "Chaofan Li", "hidden": false}, {"_id": "692518ff16eb3a9f1310391e", "name": "Hongjin Qian", "hidden": false}, {"_id": "692518ff16eb3a9f1310391f", "user": {"_id": "6145b3fd35135ec7e8d4ca45", "avatarUrl": "/avatars/5dc25d18d6a8418c9b1a29ece9a48f5a.svg", "isPro": false, "fullname": "Shuqi Lu", "user": "shuqi", "type": "user"}, "name": "Shuqi Lu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:18:11.163Z", "hidden": false}, {"_id": "692518ff16eb3a9f13103920", "user": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "name": "Zheng Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:17:59.618Z", "hidden": false}], "publishedAt": "2025-11-23T12:29:33.000Z", "submittedOnDailyAt": "2025-11-25T00:25:04.757Z", "title": "General Agentic Memory Via Deep Research", "submittedOnDailyBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "upvotes": 140, "discussionId": "692518ff16eb3a9f13103921", "projectPage": "https://github.com/VectorSpaceLab/general-agentic-memory", "githubRepo": "https://github.com/VectorSpaceLab/general-agentic-memory", "ai_summary": "GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.", "ai_keywords": ["general agentic memory", "GAM", "just-in time compilation", "JIT compilation", "memorizer", "researcher", "universal page-store", "large language models", "LLMs", "reinforcement learning"], "githubStars": 246, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u5185\u5b58\u5bf9\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u9759\u6001\u5185\u5b58\u5bb9\u6613\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u79f0\u4e3a\u4e00\u822c\u4ee3\u7406\u5185\u5b58\uff08GAM\uff09\uff0c\u91c7\u7528\u201c\u53ca\u65f6\u7f16\u8bd1\u201d\uff08JIT\uff09\u539f\u5219\u3002</li>\n    <li>GAM\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u8bb0\u5fc6\u5668\u548c\u7814\u7a76\u8005\uff0c\u5206\u522b\u7528\u4e8e\u7ba1\u7406\u5386\u53f2\u4fe1\u606f\u548c\u5728\u7ebf\u68c0\u7d22\u6709\u7528\u4fe1\u606f\u3002</li>\n    <li>\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u6709\u6548\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6574\u4f53\u6027\u80fd\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cGAM\u5728\u591a\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684\u4efb\u52a1\u5b8c\u6210\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5185\u5b58\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Memory is very important for AI agents, but traditional static memory can lead to significant information loss.</li>\n  <li>We introduce a new system called general agentic memory (GAM) that creates memory when needed, rather than in advance.</li>\n  <li>GAM consists of two main parts: a Memorizer that keeps important past information, and a Researcher that finds useful information when needed.</li>\n  <li>This system helps AI agents use large language models more effectively and improves their overall performance.</li>\n  <li>Tests show that GAM performs much better in memory-related tasks compared to existing memory systems.</li>\n</ul>"}, "publishedAt": "2025-11-23T07:29:33.000Z", "title": "General Agentic Memory Via Deep Research", "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18423.png", "numComments": 2, "submittedBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "fullname": "zhengliu", "name": "lz1001", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Dec 10, 2025";
window.trendingPapers = {
    "today": [{"paper": {"id": "2602.07085", "authors": [{"_id": "698ab6f91b2dc6b37d61b031", "name": "Jun Han", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b032", "name": "Shuo Zhang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b033", "name": "Wei Li", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b034", "user": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "name": "Zhi Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:58.707Z", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b035", "name": "Yifan Dong", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b036", "name": "Tu Hu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b037", "name": "Jialuo Yuan", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b038", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:06:00.954Z", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b039", "name": "Yumo Zhu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03a", "name": "Fangqi Lou", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03b", "name": "Xin Guo", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03c", "name": "Zhaowei Liu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03d", "name": "Tianyi Jiang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03e", "name": "Ruichuan An", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03f", "name": "Jingping Liu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b040", "name": "Biao Wu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b041", "name": "Rongze Chen", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b042", "name": "Kunyi Wang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b043", "name": "Yifan Wang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b044", "name": "Sen Hu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b045", "name": "Xinbing Kong", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b046", "name": "Liwen Zhang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b047", "name": "Ronghao Chen", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b048", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-02-06T08:08:04.000Z", "submittedOnDailyAt": "2026-02-10T02:19:22.216Z", "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining", "submittedOnDailyBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "summary": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.", "upvotes": 141, "discussionId": "698ab6fa1b2dc6b37d61b049", "githubRepo": "https://github.com/QuantaAlpha/QuantaAlpha", "githubRepoAddedBy": "user", "githubStars": 63, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "summary_zh": "<ul>\n  <li>\u91d1\u878d\u5e02\u573a\u566a\u58f0\u5927\u4e14\u4e0d\u7a33\u5b9a\uff0c\u5bfc\u81f4\u56de\u6d4b\u7ed3\u679c\u5bf9\u566a\u58f0\u548c\u5e02\u573a\u53d8\u5316\u975e\u5e38\u654f\u611f\u3002</li>\n  <li>QuantaAlpha\u662f\u4e00\u4e2a\u8fdb\u5316\u578b\u7684alpha\u6316\u6398\u6846\u67b6\uff0c\u901a\u8fc7\u8f68\u8ff9\u7ea7\u7684\u53d8\u5f02\u548c\u4ea4\u53c9\u64cd\u4f5c\u6765\u6539\u8fdb\u6316\u6398\u8fc7\u7a0b\u3002</li>\n  <li>\u5728\u6bcf\u4e2a\u6316\u6398\u8f68\u8ff9\u4e2d\uff0cQuantaAlpha\u5b9a\u4f4d\u6b21\u4f18\u6b65\u9aa4\u8fdb\u884c\u4fee\u6b63\uff0c\u5e76\u91cd\u65b0\u7ec4\u5408\u9ad8\u6536\u76ca\u6bb5\uff0c\u589e\u5f3a\u63a2\u7d22\u548c\u6539\u8fdb\u3002</li>\n  <li>\u5728\u56e0\u5b50\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0cQuantaAlpha\u786e\u4fdd\u5047\u8bbe\u3001\u56e0\u5b50\u8868\u8fbe\u548c\u53ef\u6267\u884c\u4ee3\u7801\u7684\u4e00\u81f4\u6027\uff0c\u51cf\u5c0f\u590d\u6742\u6027\u548c\u5197\u4f59\u3002</li>\n  <li>\u5728CSI 300\u7684\u5b9e\u9a8c\u4e2d\uff0cQuantaAlpha\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u56de\u62a5\u7387\u548c\u6709\u6548\u6027\uff0c\u5e76\u80fd\u6709\u6548\u8fc1\u79fb\u5230\u5176\u4ed6\u5e02\u573a\u6307\u6570\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Financial markets can be unpredictable, making it hard to test trading strategies accurately.</li>\n    <li>QuantaAlpha is a new framework designed to improve the process of finding effective trading strategies by using evolutionary techniques.</li>\n    <li>It improves strategies by focusing on specific steps that need revision and combining successful patterns from different attempts.</li>\n    <li>QuantaAlpha ensures that the strategies it generates are clear and not overly complicated, which helps avoid issues with similar strategies competing against each other.</li>\n    <li>Tests show that QuantaAlpha performs better than existing models and its strategies can also work well in different market conditions.</li>\n</ul>"}, "publishedAt": "2026-02-06T03:08:04.000Z", "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining", "summary": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07085.png", "numComments": 1, "submittedBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "fullname": "Zhi Yang", "name": "yangzhi1", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.08794", "authors": [{"_id": "698ac65d1b2dc6b37d61b1c2", "name": "SII-OpenMOSS Team", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c4", "name": "Donghua Yu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c5", "name": "Mingshu Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c6", "name": "Qi Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c7", "name": "Qi Luo", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c8", "name": "Qianyi Wu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c9", "user": {"_id": "63ec4715c81b6a52391c46b8", "avatarUrl": "/avatars/496819b5075a1a834a2b9edeb068c80e.svg", "isPro": false, "fullname": "QinyuanCheng", "user": "Cqy2019", "type": "user"}, "name": "Qinyuan Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:07.400Z", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1ca", "name": "Ruixiao Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cb", "user": {"_id": "62c14609ac1b639c2d87192c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png", "isPro": false, "fullname": "SII-liangtianyi", "user": "tianyilt", "type": "user"}, "name": "Tianyi Liang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:10.522Z", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cc", "name": "Wenbo Zhang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cd", "name": "Wenming Tu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1ce", "name": "Xiangyu Peng", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cf", "name": "Yang Gao", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d0", "name": "Yanru Huo", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d1", "user": {"_id": "69158ffc0153b85a677dcc46", "avatarUrl": "/avatars/c9c5f60522f2a8f370d790ea9938b090.svg", "isPro": false, "fullname": "Ying Zhu", "user": "Auraithm", "type": "user"}, "name": "Ying Zhu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:27:41.440Z", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d2", "name": "Yinze Luo", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d3", "name": "Yiyang Zhang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d4", "name": "Yuerong Song", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d5", "name": "Zhe Xu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d6", "name": "Zhiyu Zhang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d7", "name": "Chenchen Yang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d8", "name": "Cheng Chang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d9", "name": "Chushu Zhou", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1da", "name": "Hanfu Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1db", "name": "Hongnan Ma", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1dc", "name": "Jiaxi Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1dd", "name": "Jingqi Tong", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1de", "name": "Junxi Liu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1df", "name": "Ke Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e0", "name": "Shimin Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e1", "name": "Songlin Wang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e2", "name": "Wei Jiang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e3", "name": "Zhaoye Fei", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e4", "name": "Zhiyuan Ning", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e5", "name": "Chunguo Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e6", "name": "Chenhui Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e7", "name": "Ziwei He", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e8", "name": "Zengfeng Huang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e9", "name": "Xie Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1ea", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-02-09T15:31:54.000Z", "submittedOnDailyAt": "2026-02-10T03:18:59.260Z", "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation", "submittedOnDailyBy": {"_id": "62c14609ac1b639c2d87192c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png", "isPro": false, "fullname": "SII-liangtianyi", "user": "tianyilt", "type": "user"}, "summary": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.", "upvotes": 131, "discussionId": "698ac65e1b2dc6b37d61b1eb", "projectPage": "https://mosi.cn/models/mova", "githubRepo": "https://github.com/OpenMOSS/MOVA", "githubRepoAddedBy": "user", "ai_summary": "MOVA is an open-source model that generates synchronized audio-visual content using a Mixture-of-Experts architecture with 32 billion parameters, supporting image-text to video-audio generation tasks.", "ai_keywords": ["Mixture-of-Experts", "MoE", "audio-visual content", "lip-synced speech", "sound effects", "content-aligned music", "IT2VA", "efficient inference", "LoRA fine-tuning", "prompt enhancement"], "githubStars": 579, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u97f3\u9891\u5728\u89c6\u9891\u5185\u5bb9\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u751f\u6210\u6a21\u578b\u901a\u5e38\u5ffd\u89c6\u97f3\u9891\u90e8\u5206\u3002</li>\n    <li>\u5f53\u524d\u7684\u97f3\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4f7f\u7528\u7ea7\u8054\u6d41\u7a0b\uff0c\u5bfc\u81f4\u6210\u672c\u589e\u52a0\u548c\u8d28\u91cf\u4e0b\u964d\u3002</li>\n    <li>MOVA\uff08MOSS\u89c6\u9891\u548c\u97f3\u9891\uff09\u662f\u4e00\u4e2a\u5f00\u6e90\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u540c\u6b65\u97f3\u89c6\u9891\u5185\u5bb9\u3002</li>\n    <li>MOVA\u4f7f\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u5177\u6709320\u4ebf\u53c2\u6570\uff0c\u5176\u4e2d180\u4ebf\u5728\u63a8\u7406\u65f6\u6d3b\u8dc3\u3002</li>\n    <li>\u53d1\u5e03\u6a21\u578b\u6743\u91cd\u548c\u4ee3\u7801\u65e8\u5728\u63a8\u52a8\u7814\u7a76\u5e76\u4fc3\u8fdb\u521b\u4f5c\u8005\u793e\u533a\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Audio is important for video, but many models ignore it, leading to quality issues.</li>\n    <li>Current methods often use separate processes for audio and video, which can be costly and error-prone.</li>\n    <li>MOVA (MOSS Video and Audio) is a new open-source model that generates high-quality synchronized audio and video.</li>\n    <li>MOVA can create realistic lip-synced speech, sound effects, and music that aligns with the video content.</li>\n    <li>The model has a large architecture and is designed to support collaboration and further research in the field.</li>\n</ul>"}, "publishedAt": "2026-02-09T10:31:54.000Z", "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation", "summary": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08794.png", "numComments": 1, "submittedBy": {"_id": "62c14609ac1b639c2d87192c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png", "fullname": "SII-liangtianyi", "name": "tianyilt", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.07026", "authors": [{"_id": "698a98541b2dc6b37d61af09", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:07:26.323Z", "hidden": false}, {"_id": "698a98541b2dc6b37d61af0a", "name": "Yi Xin", "hidden": false}, {"_id": "698a98541b2dc6b37d61af0b", "name": "Wenjie Zhang", "hidden": false}, {"_id": "698a98541b2dc6b37d61af0c", "name": "Chonghan Liu", "hidden": false}, {"_id": "698a98541b2dc6b37d61af0d", "name": "Hanzhen Zhao", "hidden": false}, {"_id": "698a98541b2dc6b37d61af0e", "name": "Xiaoxing Hu", "hidden": false}, {"_id": "698a98541b2dc6b37d61af0f", "name": "Xinlei Yu", "hidden": false}, {"_id": "698a98541b2dc6b37d61af10", "name": "Ziyue Qiao", "hidden": false}, {"_id": "698a98541b2dc6b37d61af11", "name": "Hao Tang", "hidden": false}, {"_id": "698a98541b2dc6b37d61af12", "name": "Xue Yang", "hidden": false}, {"_id": "698a98541b2dc6b37d61af13", "name": "Xiaobin Hu", "hidden": false}, {"_id": "698a98541b2dc6b37d61af14", "name": "Chengwei Qin", "hidden": false}, {"_id": "698a98541b2dc6b37d61af15", "name": "Hui Xiong", "hidden": false}, {"_id": "698a98541b2dc6b37d61af16", "name": "Yu Qiao", "hidden": false}, {"_id": "698a98541b2dc6b37d61af17", "name": "Shuicheng Yan", "hidden": false}], "publishedAt": "2026-02-02T13:59:39.000Z", "submittedOnDailyAt": "2026-02-10T00:01:56.908Z", "title": "Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.", "upvotes": 120, "discussionId": "698a98541b2dc6b37d61af18", "githubRepo": "https://github.com/Yu-xm/ReVision.git", "githubRepoAddedBy": "user", "ai_summary": "Researchers address the modality gap in multimodal learning by proposing a fixed-frame theory and a training-free alignment method that enables efficient scaling of multimodal models using unpaired data.", "ai_keywords": ["multimodal contrastive learning", "modality gap", "geometric anomaly", "isotropic assumptions", "Fixed-frame Modality Gap Theory", "ReAlign", "Anchor Alignment", "Trace Alignment", "Centroid Alignment", "ReVision", "Multimodal Large Language Models", "unpaired data", "visual representation distribution", "pretraining stage", "visual instruction tuning"], "githubStars": 41, "summary_zh": "<ul>\n    <li>\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u5728\u5bf9\u9f50\u89c6\u89c9\u548c\u8bed\u8a00\u8868\u793a\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u4ecd\u5b58\u5728\u201c\u6a21\u6001\u5dee\u8ddd\u201d\u95ee\u9898\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u7b80\u5316\u7684\u5047\u8bbe\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u573a\u666f\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u56fa\u5b9a\u6846\u67b6\u6a21\u6001\u5dee\u8ddd\u7406\u8bba\uff0c\u7cbe\u51c6\u63cf\u8ff0\u6a21\u6001\u5dee\u8ddd\u7684\u51e0\u4f55\u5f62\u72b6\u3002</li>\n    <li>\u4ecb\u7ecd\u4e86\u65e0\u8bad\u7ec3\u7684\u6a21\u6001\u5bf9\u9f50\u7b56\u7565ReAlign\uff0c\u901a\u8fc7\u4e09\u6b65\u8fc7\u7a0b\u7ea0\u6b63\u51e0\u4f55\u9519\u4f4d\u3002</li>\n    <li>\u63d0\u51fa\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u8303\u5f0fReVision\uff0c\u5229\u7528\u672a\u914d\u5bf9\u6570\u636e\u6709\u6548\u66ff\u4ee3\u9ad8\u8d28\u91cf\u56fe\u50cf-\u6587\u672c\u5bf9\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multimodal contrastive learning helps combine visual and language understanding, but there is a problem called the Modality Gap, where similar meanings are represented in different areas.</li>\n    <li>Previous methods to fix this gap have limitations due to overly simple assumptions that don't work well on a large scale.</li>\n    <li>This paper introduces the Fixed-frame Modality Gap Theory, which breaks down the gap into stable and variable parts.</li>\n    <li>It presents a new method called ReAlign that aligns text and image representations without needing extensive training.</li>\n    <li>ReVision is a scalable approach for training large language models that uses ReAlign to learn from unpaired data instead of requiring many high-quality image-text pairs.</li>\n</ul>"}, "publishedAt": "2026-02-02T08:59:39.000Z", "title": "Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models", "summary": "Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07026.png", "numComments": 5, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.08222", "authors": [{"_id": "698ad20e1b2dc6b37d61b227", "user": {"_id": "64afe1653361f887816da303", "avatarUrl": "/avatars/320d71adacfad9dd5db064b4ed3dec2b.svg", "isPro": false, "fullname": "chenzehao", "user": "chhao", "type": "user"}, "name": "Zehao Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:04:42.021Z", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b228", "name": "Gongxun Li", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b229", "name": "Tianxiang Ai", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b22a", "name": "Yifei Li", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b22b", "name": "Zixuan Huang", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b22c", "name": "Wang Zhou", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b22d", "name": "Fuzhen Zhuang", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b22e", "name": "Xianglong Liu", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b22f", "name": "Jianxin Li", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b230", "name": "Deqing Wang", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b231", "user": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "isPro": false, "fullname": "Yikun B", "user": "Yikunb", "type": "user"}, "name": "Yikun Ban", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:04:44.617Z", "hidden": false}], "publishedAt": "2026-02-09T02:50:40.000Z", "submittedOnDailyAt": "2026-02-10T04:36:28.975Z", "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger", "submittedOnDailyBy": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "isPro": false, "fullname": "Yikun B", "user": "Yikunb", "type": "user"}, "summary": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.", "upvotes": 118, "discussionId": "698ad20e1b2dc6b37d61b232", "githubRepo": "https://github.com/chenzehao82/Weak-Driven-Learning", "githubRepoAddedBy": "user", "ai_summary": "WMSS is a post-training paradigm that uses weak model checkpoints to identify and fill learning gaps, enabling continued improvement beyond conventional saturation points in large language models.", "ai_keywords": ["post-training optimization", "large language models", "saturation bottleneck", "weak checkpoints", "entropy dynamics", "compensatory learning", "learning gaps"], "githubStars": 39, "summary_zh": "<ul>\n    <li>\u540e\u8bad\u7ec3\u4f18\u5316\u662f\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5173\u952e\uff0c\u4f46\u6a21\u578b\u8fc7\u4e8e\u81ea\u4fe1\u65f6\uff0c\u8bad\u7ec3\u6548\u679c\u4f1a\u51cf\u5f31\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5f3a\u5316\u76ee\u6807\u9884\u6d4b\uff0c\u4f46\u6211\u4eec\u53d1\u73b0\u6a21\u578b\u5386\u53f2\u4e2d\u7684\u5f31\u72b6\u6001\u4ecd\u6709\u6709\u7528\u7684\u76d1\u7763\u4fe1\u53f7\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51faWMSS\uff08\u5f31\u4ee3\u7406\u53ef\u4ee5\u8ba9\u5f3a\u4ee3\u7406\u66f4\u5f3a\uff09\u4f5c\u4e3a\u540e\u8bad\u7ec3\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u5f31\u68c0\u67e5\u70b9\u6765\u6307\u5bfc\u4f18\u5316\u3002</li>\n    <li>WMSS\u901a\u8fc7\u8bc6\u522b\u53ef\u6062\u590d\u7684\u5b66\u4e60\u5dee\u8ddd\u5e76\u8fdb\u884c\u8865\u507f\u5b66\u4e60\uff0c\u4f7f\u5f3a\u4ee3\u7406\u80fd\u591f\u8d85\u8d8a\u5e38\u89c4\u7684\u540e\u8bad\u7ec3\u9971\u548c\u72b6\u6001\u3002</li>\n    <li>\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u6211\u4eec\u65b9\u6cd5\u8bad\u7ec3\u7684\u4ee3\u7406\u80fd\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u4e14\u6ca1\u6709\u989d\u5916\u7684\u63a8\u7406\u6210\u672c\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Post-training optimization is important for improving large language models, but there's a limit to how much they can improve once they become very confident.</li>\n  <li>Current training methods reinforce what the models already know, but there is valuable information in their earlier, less confident states.</li>\n  <li>We introduce WMSS (Weak Agents Can Make Strong Agents Stronger), a new method that uses these weaker states to help improve model performance.</li>\n  <li>WMSS identifies learning gaps and helps models learn from them, allowing for better improvements despite previous saturation.</li>\n  <li>Tests show that models using WMSS perform better in tasks like math reasoning and code generation without increasing inference costs.</li>\n</ul>"}, "publishedAt": "2026-02-08T21:50:40.000Z", "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger", "summary": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08222.png", "numComments": 4, "submittedBy": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "fullname": "Yikun B", "name": "Yikunb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.06855", "authors": [{"_id": "698b0ed21b2dc6b37d61b3d0", "name": "Alisia Lupidi", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3d1", "name": "Bhavul Gauri", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3d2", "name": "Thomas Simon Foster", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3d3", "name": "Bassel Al Omari", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3d4", "name": "Despoina Magka", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3d5", "name": "Alberto Pepe", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3d6", "name": "Alexis Audran-Reiss", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3d7", "name": "Muna Aghamelu", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3d8", "name": "Nicolas Baldwin", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3d9", "name": "Lucia Cipolina-Kun", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3da", "name": "Jean-Christophe Gagnon-Audet", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3db", "name": "Chee Hau Leow", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3dc", "name": "Sandra Lefdal", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3dd", "name": "Hossam Mossalam", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3de", "name": "Abhinav Moudgil", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3df", "name": "Saba Nazir", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e0", "name": "Emanuel Tewolde", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e1", "name": "Isabel Urrego", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e2", "name": "Jordi Armengol Estape", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e3", "name": "Amar Budhiraja", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e4", "name": "Gaurav Chaurasia", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e5", "name": "Abhishek Charnalia", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e6", "name": "Derek Dunfield", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e7", "name": "Karen Hambardzumyan", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e8", "name": "Daniel Izcovich", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e9", "name": "Martin Josifoski", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3ea", "name": "Ishita Mediratta", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3eb", "name": "Kelvin Niu", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3ec", "name": "Parth Pathak", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3ed", "name": "Michael Shvartsman", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3ee", "name": "Edan Toledo", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3ef", "name": "Anton Protopopov", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3f0", "name": "Roberta Raileanu", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3f1", "name": "Alexander Miller", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3f2", "name": "Tatiana Shavrina", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3f3", "name": "Jakob Foerster", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3f4", "name": "Yoram Bachrach", "hidden": false}], "publishedAt": "2026-02-06T16:45:02.000Z", "submittedOnDailyAt": "2026-02-10T08:48:50.684Z", "title": "AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents", "submittedOnDailyBy": {"_id": "60720704227ff331937110f4", "avatarUrl": "/avatars/8010bfb98256c138049aa3d237737b37.svg", "isPro": false, "fullname": "Bhavul Gauri", "user": "bhavul", "type": "user"}, "summary": "LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.", "upvotes": 54, "discussionId": "698b0ed31b2dc6b37d61b3f5", "githubRepo": "https://github.com/facebookresearch/airs-bench", "githubRepoAddedBy": "user", "ai_summary": "AIRS-Bench presents a comprehensive benchmark suite for evaluating LLM agents across diverse scientific domains, demonstrating current limitations while providing open-source resources for advancing autonomous scientific research.", "ai_keywords": ["AI Research Science Benchmark", "agentic capabilities", "research lifecycle", "sequential scaffolds", "parallel scaffolds", "autonomous scientific research"], "githubStars": 16, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86AIRS-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b20\u4e2a\u4efb\u52a1\u7684AI\u7814\u7a76\u79d1\u5b66\u57fa\u51c6\uff0c\u65e8\u5728\u63a8\u52a8\u79d1\u5b66\u7814\u7a76\u7684\u53d1\u5c55\u3002</li>\n    <li>\u8fd9\u4e9b\u4efb\u52a1\u6db5\u76d6\u591a\u4e2a\u9886\u57df\uff0c\u5305\u62ec\u8bed\u8a00\u6a21\u578b\u3001\u6570\u5b66\u3001\u751f\u7269\u4fe1\u606f\u5b66\u548c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002</li>\n    <li>AIRS-Bench\u4efb\u52a1\u8bc4\u4f30\u7814\u7a76\u4ee3\u7406\u5728\u6574\u4e2a\u7814\u7a76\u751f\u547d\u5468\u671f\u4e2d\u7684\u80fd\u529b\uff0c\u5305\u62ec\u521b\u610f\u751f\u6210\u548c\u5b9e\u9a8c\u5206\u6790\u3002</li>\n    <li>\u7ed3\u679c\u663e\u793a\uff0c\u4ee3\u7406\u5728\u56db\u4e2a\u4efb\u52a1\u4e2d\u8d85\u8fc7\u4e86\u4eba\u7c7b\u7684\u6700\u4f73\u8868\u73b0\uff0c\u4f46\u5728\u5176\u4ed6\u5341\u516d\u4e2a\u4efb\u52a1\u4e2d\u672a\u80fd\u8fbe\u5230\u3002</li>\n    <li>AIRS-Bench\u4efb\u52a1\u5b9a\u4e49\u548c\u8bc4\u4f30\u4ee3\u7801\u5df2\u5f00\u6e90\uff0c\u4ee5\u4fc3\u8fdb\u81ea\u4e3b\u79d1\u5b66\u7814\u7a76\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>AIRS-Bench is a new tool for improving scientific research using AI, featuring 20 tasks from top machine learning studies.</li>\n    <li>The tasks cover various areas like language processing, math, bioinformatics, and analyzing time series data.</li>\n    <li>It evaluates AI agents throughout the entire research process, including generating ideas and refining experiments, without giving them any base code.</li>\n    <li>Results show that AI agents perform better than humans in four tasks but fall short in sixteen tasks, indicating there's still much room for growth.</li>\n    <li>The AIRS-Bench task details and evaluation tools are open-sourced to encourage further advancements in autonomous research.</li>\n</ul>"}, "publishedAt": "2026-02-06T11:45:02.000Z", "title": "AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents", "summary": "LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06855.png", "numComments": 1, "submittedBy": {"_id": "60720704227ff331937110f4", "avatarUrl": "/avatars/8010bfb98256c138049aa3d237737b37.svg", "fullname": "Bhavul Gauri", "name": "bhavul", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.07845", "authors": [{"_id": "698ab2ef1b2dc6b37d61af7b", "name": "Yalcin Tur", "hidden": false}, {"_id": "698ab2ef1b2dc6b37d61af7c", "name": "Jalal Naghiyev", "hidden": false}, {"_id": "698ab2ef1b2dc6b37d61af7d", "name": "Haoquan Fang", "hidden": false}, {"_id": "698ab2ef1b2dc6b37d61af7e", "name": "Wei-Chuan Tsai", "hidden": false}, {"_id": "698ab2ef1b2dc6b37d61af7f", "name": "Jiafei Duan", "hidden": false}, {"_id": "698ab2ef1b2dc6b37d61af80", "name": "Dieter Fox", "hidden": false}, {"_id": "698ab2ef1b2dc6b37d61af81", "name": "Ranjay Krishna", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/QxggswtZU5KkZQawFarz8.mp4"], "publishedAt": "2026-02-08T07:21:01.000Z", "submittedOnDailyAt": "2026-02-10T02:01:42.213Z", "title": "Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning", "submittedOnDailyBy": {"_id": "632b42626110e37dba3d5bcb", "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg", "isPro": false, "fullname": "Duan", "user": "Jiafei1224", "type": "user"}, "summary": "Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/", "upvotes": 47, "discussionId": "698ab2ef1b2dc6b37d61af82", "projectPage": "https://rd-vla.github.io/", "githubRepo": "https://github.com/rd-vla/rd-vla", "githubRepoAddedBy": "user", "ai_summary": "RD-VLA introduces a recurrent architecture for vision-language-action models that adapts computational depth through latent iterative refinement, achieving constant memory usage and improved task success rates.", "ai_keywords": ["Vision-Language-Action models", "Chain-of-Thought prompting", "recurrent architecture", "weight-tied action head", "truncated backpropagation through time", "latent iterative refinement", "adaptive stopping criterion", "latent convergence", "computational adaptivity"], "githubStars": 3, "organization": {"_id": "5e70f3648ce3c604d78fe132", "name": "allenai", "fullname": "Ai2", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"}, "summary_zh": "<ul>\n    <li>\u5f53\u524d\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4f7f\u7528\u56fa\u5b9a\u7684\u8ba1\u7b97\u6df1\u5ea6\uff0c\u7b80\u5355\u548c\u590d\u6742\u4efb\u52a1\u6d88\u8017\u76f8\u540c\u7684\u8ba1\u7b97\u8d44\u6e90\u3002</li>\n    <li>\u5f15\u5165\u4e86\u9012\u5f52\u6df1\u5ea6\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff08RD-VLA\uff09\uff0c\u901a\u8fc7\u6f5c\u5728\u7684\u8fed\u4ee3\u4f18\u5316\u5b9e\u73b0\u8ba1\u7b97\u81ea\u9002\u5e94\u3002</li>\n    <li>RD-VLA\u4f7f\u7528\u4e86\u4e00\u4e2a\u9012\u5f52\u7684\u3001\u6743\u91cd\u7ed1\u5b9a\u7684\u52a8\u4f5c\u5934\uff0c\u652f\u6301\u4efb\u610f\u63a8\u7406\u6df1\u5ea6\u4e14\u5185\u5b58\u5360\u7528\u4fdd\u6301\u4e0d\u53d8\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u9012\u5f52\u6df1\u5ea6\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u56db\u6b21\u8fed\u4ee3\u7684\u6210\u529f\u7387\u8d85\u8fc790%\uff0c\u800c\u5355\u6b21\u8fed\u4ee3\u5219\u4e3a0%\u3002</li>\n    <li>RD-VLA\u5728\u673a\u5668\u4eba\u6d4b\u8bd5\u65f6\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u4e4b\u524d\u7684\u6a21\u578b\u5feb80\u500d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current VLA models use the same computational depth for all tasks, wasting resources on simple tasks.</li>\n    <li>Recurrent-Depth VLA (RD-VLA) offers flexible computation by refining results iteratively without increasing memory use.</li>\n    <li>RD-VLA uses a special training method to efficiently improve its actions during learning.</li>\n    <li>During use, RD-VLA can adjust the amount of computation based on how well it is doing, allowing for better performance on complex tasks.</li>\n    <li>Tests show that RD-VLA greatly improves success rates in challenging manipulation tasks and is much faster than earlier models.</li>\n</ul>"}, "publishedAt": "2026-02-08T02:21:01.000Z", "title": "Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning", "summary": "Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/QxggswtZU5KkZQawFarz8.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07845.png", "numComments": 1, "submittedBy": {"_id": "632b42626110e37dba3d5bcb", "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg", "fullname": "Duan", "name": "Jiafei1224", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "5e70f3648ce3c604d78fe132", "name": "allenai", "fullname": "Ai2", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.08676", "authors": [{"_id": "698ab6fd1b2dc6b37d61b04b", "name": "Tiwei Bie", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b04c", "name": "Maosong Cao", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b04d", "name": "Xiang Cao", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b04e", "name": "Bingsen Chen", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b04f", "name": "Fuyuan Chen", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b050", "name": "Kun Chen", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b051", "name": "Lun Du", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b052", "name": "Daozhuo Feng", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b053", "name": "Haibo Feng", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b054", "name": "Mingliang Gong", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b055", "name": "Zhuocheng Gong", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b056", "name": "Yanmei Gu", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b057", "name": "Jian Guan", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b058", "user": {"_id": "6494f6dc32f2c0d7ef28315c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6494f6dc32f2c0d7ef28315c/y4TwcrT6UQ9Zfy6BfUMUl.jpeg", "isPro": false, "fullname": "Kaiyuan Guan", "user": "WilfredG", "type": "user"}, "name": "Kaiyuan Guan", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:52.604Z", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b059", "name": "Hongliang He", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b05a", "name": "Zenan Huang", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b05b", "name": "Juyong Jiang", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b05c", "name": "Zhonghui Jiang", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b05d", "name": "Zhenzhong Lan", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b05e", "name": "Chengxi Li", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b05f", "name": "Jianguo Li", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b060", "name": "Zehuan Li", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b061", "user": {"_id": "6470600790482b0e0f65a393", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fOUWWhf9jjsC9fgIiFlPc.png", "isPro": false, "fullname": "Huabin", "user": "Rookie-Liu", "type": "user"}, "name": "Huabin Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:42.985Z", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b062", "name": "Lin Liu", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b063", "name": "Guoshan Lu", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b064", "name": "Yuan Lu", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b065", "name": "Yuxin Ma", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b066", "name": "Xingyu Mou", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b067", "name": "Zhenxuan Pan", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b068", "name": "Kaida Qiu", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b069", "name": "Yuji Ren", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b06a", "name": "Jianfeng Tan", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b06b", "user": {"_id": "5f94dea4cf95e81b6854e223", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f94dea4cf95e81b6854e223/r3VW8rb0wlJ7t_Lcpqi5f.jpeg", "isPro": false, "fullname": "Dean Tian", "user": "killa1218", "type": "user"}, "name": "Yiding Tian", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:47.064Z", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b06c", "name": "Zian Wang", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b06d", "name": "Lanning Wei", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b06e", "name": "Tao Wu", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b06f", "name": "Yipeng Xing", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b070", "name": "Wentao Ye", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b071", "name": "Liangyu Zha", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b072", "name": "Tianze Zhang", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b073", "name": "Xiaolu Zhang", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b074", "user": {"_id": "6725f5a7f05f62659e3615f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/iiXdzhaSnExw2dwQQtLZ8.png", "isPro": false, "fullname": "Junbo Zhao", "user": "jakezhao2024", "type": "user"}, "name": "Junbo Zhao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:45.032Z", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b075", "name": "Da Zheng", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b076", "name": "Hao Zhong", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b077", "name": "Wanli Zhong", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b078", "name": "Jun Zhou", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b079", "user": {"_id": "63eb008e5c837d9968f1eb71", "avatarUrl": "/avatars/ae43c3f5ab87b82f4bad25c65ac55d01.svg", "isPro": false, "fullname": "Junlin Zhou", "user": "jlzhou", "type": "user"}, "name": "Junlin Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:49.495Z", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b07a", "name": "Liwang Zhu", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b07b", "user": {"_id": "632179745fc60c44fd91fc33", "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg", "isPro": false, "fullname": "zhumuzhi", "user": "Z-MU-Z", "type": "user"}, "name": "Muzhi Zhu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:33.817Z", "hidden": false}, {"_id": "698ab6fd1b2dc6b37d61b07c", "user": {"_id": "673b5f24e863f1d28b402efc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/19gUgtPEY3-FtY0sNlI_-.png", "isPro": false, "fullname": "yihongzhuang", "user": "utdawn", "type": "user"}, "name": "Yihong Zhuang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:55.143Z", "hidden": false}], "publishedAt": "2026-02-09T14:00:07.000Z", "submittedOnDailyAt": "2026-02-10T02:14:10.029Z", "title": "LLaDA2.1: Speeding Up Text Diffusion via Token Editing", "submittedOnDailyBy": {"_id": "673b5f24e863f1d28b402efc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/19gUgtPEY3-FtY0sNlI_-.png", "isPro": false, "fullname": "yihongzhuang", "user": "utdawn", "type": "user"}, "summary": "While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.", "upvotes": 42, "discussionId": "698ab6fd1b2dc6b37d61b07d", "githubRepo": "https://github.com/inclusionAI/LLaDA2.X", "githubRepoAddedBy": "user", "ai_summary": "LLaDA2.1 introduces a novel token-to-token editing approach with speed and quality modes, enhanced through reinforcement learning for improved reasoning and instruction following in large language diffusion models.", "ai_keywords": ["block-diffusion models", "decoding speed", "generation quality", "Token-to-Token editing", "Mask-to-Token scheme", "threshold-decoding scheme", "Speedy Mode", "Quality Mode", "Reinforcement Learning", "gradient estimation", "reasoning precision", "instruction-following", "large language diffusion models", "HumanEval+", "BigCodeBench", "LiveCodeBench"], "githubStars": 247, "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86LLaDA2.1\uff0c\u65e8\u5728\u63d0\u5347\u89e3\u7801\u901f\u5ea6\u548c\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u7684\u5e73\u8861\u3002</li>\n    <li>\u7ed3\u5408\u4e86Token-to-Token\uff08T2T\uff09\u7f16\u8f91\u548c\u4f20\u7edf\u7684Mask-to-Token\uff08M2T\uff09\u65b9\u6848\uff0c\u91c7\u7528\u4e86\u65b0\u7684\u9608\u503c\u89e3\u7801\u65b9\u6cd5\u3002</li>\n    <li>\u63d0\u4f9b\u4e86\u4e24\u79cd\u6a21\u5f0f\uff1a\u5feb\u901f\u6a21\u5f0f\uff08S\u6a21\u5f0f\uff09\u8ffd\u6c42\u901f\u5ea6\uff0c\u8d28\u91cf\u6a21\u5f0f\uff08Q\u6a21\u5f0f\uff09\u5173\u6ce8\u8f93\u51fa\u8d28\u91cf\u3002</li>\n    <li>\u9996\u6b21\u5b9e\u65bd\u4e86\u9488\u5bf9\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u9ad8\u63a8\u7406\u7cbe\u5ea6\u548c\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3002</li>\n    <li>LLaDA2.1\u572833\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u591a\u4e2a\u7f16\u7a0b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5feb\u901f\u89e3\u7801\u901f\u5ea6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LLaDA2.1 improves on LLaDA2.0 by balancing speed and quality in generating text.</li>\n    <li>It introduces a new method that combines Token-to-Token (T2T) editing with Mask-to-Token (M2T) generation.</li>\n    <li>There are two modes: Speedy Mode for faster generation with good quality, and Quality Mode for better performance at a slower speed.</li>\n    <li>The model uses a large-scale Reinforcement Learning framework to enhance reasoning and instruction-following abilities.</li>\n    <li>LLaDA2.1 shows great results on 33 benchmarks, achieving impressive speeds in coding tasks despite its large size.</li>\n</ul>"}, "publishedAt": "2026-02-09T09:00:07.000Z", "title": "LLaDA2.1: Speeding Up Text Diffusion via Token Editing", "summary": "While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08676.png", "numComments": 2, "submittedBy": {"_id": "673b5f24e863f1d28b402efc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/19gUgtPEY3-FtY0sNlI_-.png", "fullname": "yihongzhuang", "name": "utdawn", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.06422", "authors": [{"_id": "698a9c501b2dc6b37d61af2f", "name": "Yunze Tong", "hidden": false}, {"_id": "698a9c501b2dc6b37d61af30", "name": "Mushui Liu", "hidden": false}, {"_id": "698a9c501b2dc6b37d61af31", "user": {"_id": "646efd223dd912a539e0bd46", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png", "isPro": false, "fullname": "Canyu Zhao", "user": "Canyu", "type": "user"}, "name": "Canyu Zhao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:06:46.484Z", "hidden": false}, {"_id": "698a9c501b2dc6b37d61af32", "name": "Wanggui He", "hidden": false}, {"_id": "698a9c501b2dc6b37d61af33", "name": "Shiyi Zhang", "hidden": false}, {"_id": "698a9c501b2dc6b37d61af34", "name": "Hongwei Zhang", "hidden": false}, {"_id": "698a9c501b2dc6b37d61af35", "name": "Peng Zhang", "hidden": false}, {"_id": "698a9c501b2dc6b37d61af36", "name": "Jinlong Liu", "hidden": false}, {"_id": "698a9c501b2dc6b37d61af37", "name": "Ju Huang", "hidden": false}, {"_id": "698a9c501b2dc6b37d61af38", "name": "Jiamang Wang", "hidden": false}, {"_id": "698a9c501b2dc6b37d61af39", "name": "Hao Jiang", "hidden": false}, {"_id": "698a9c501b2dc6b37d61af3a", "name": "Pipei Huang", "hidden": false}], "publishedAt": "2026-02-06T06:37:10.000Z", "submittedOnDailyAt": "2026-02-10T00:19:39.681Z", "title": "Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO", "submittedOnDailyBy": {"_id": "646efd223dd912a539e0bd46", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png", "isPro": false, "fullname": "Canyu Zhao", "user": "Canyu", "type": "user"}, "summary": "Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's \"pure\" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.", "upvotes": 37, "discussionId": "698a9c501b2dc6b37d61af3b", "githubRepo": "https://github.com/YunzeTong/TurningPoint-GRPO", "githubRepoAddedBy": "user", "ai_summary": "TP-GRPO addresses reward sparsity in flow matching models by introducing step-level incremental rewards and identifying turning points to capture long-term effects in denoising trajectories.", "ai_keywords": ["GRPO", "flow matching models", "text-to-image generation", "denoising steps", "reward sparsity", "incremental rewards", "turning points", "denoising trajectory", "delayed impact", "reward evolution"], "githubStars": 13, "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "summary_zh": "<ul>\n    <li>TP-GRPO\u662f\u4e00\u79cd\u65b0\u7684GRPO\u6846\u67b6\uff0c\u4e13\u6ce8\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u53bb\u566a\u8fc7\u7a0b\u3002</li>\n    <li>\u8be5\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u5956\u52b1\u4f20\u64ad\u7684\u7a00\u758f\u6027\u95ee\u9898\uff0c\u5e76\u5efa\u6a21\u53bb\u566a\u8fc7\u7a0b\u4e2d\u7684\u957f\u671f\u6548\u5e94\u3002</li>\n    <li>TP-GRPO\u7528\u6b65\u7ea7\u589e\u91cf\u5956\u52b1\u66ff\u4ee3\u4e86\u57fa\u4e8e\u7ed3\u679c\u7684\u5956\u52b1\uff0c\u66f4\u597d\u5730\u8bc4\u4f30\u6bcf\u4e2a\u53bb\u566a\u6b65\u9aa4\u7684\u5f71\u54cd\u3002</li>\n    <li>\u5b83\u8bc6\u522b\u201c\u8f6c\u6298\u70b9\u201d\uff0c\u5373\u6539\u53d8\u5956\u52b1\u8d8b\u52bf\u7684\u6b65\u9aa4\uff0c\u5e76\u4e3a\u8fd9\u4e9b\u6b65\u9aa4\u5206\u914d\u957f\u671f\u5956\u52b1\uff0c\u4ee5\u6355\u6349\u5ef6\u8fdf\u5f71\u54cd\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cTP-GRPO\u80fd\u66f4\u6709\u6548\u5730\u5229\u7528\u5956\u52b1\u4fe1\u53f7\uff0c\u5e76\u6301\u7eed\u6539\u5584\u751f\u6210\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>TP-GRPO is a new framework for improving text-to-image generation using GRPO on Flow Matching models.</li>\n    <li>It introduces step-level rewards instead of just outcome-based rewards, allowing for better learning from each denoising step.</li>\n    <li>TP-GRPO identifies \"turning points\" in the process where the reward trend changes, and gives these points special long-term rewards to reflect their impact.</li>\n    <li>This method is efficient and does not require extra tuning of parameters.</li>\n    <li>Experiments show that TP-GRPO uses reward signals more effectively and leads to better generation results.</li>\n</ul>"}, "publishedAt": "2026-02-06T01:37:10.000Z", "title": "Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO", "summary": "Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's \"pure\" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06422.png", "numComments": 1, "submittedBy": {"_id": "646efd223dd912a539e0bd46", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png", "fullname": "Canyu Zhao", "name": "Canyu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 16, "isUserFollowing": false}, "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.09007", "authors": [{"_id": "698ad8ac1b2dc6b37d61b275", "user": {"_id": "65ddea8b2d26e59a5a33330f", "avatarUrl": "/avatars/3104ddafd6dda3c05ea9a771dbf2deeb.svg", "isPro": false, "fullname": "li haodong", "user": "mickyhimself", "type": "user"}, "name": "Haodong Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:28:30.775Z", "hidden": false}, {"_id": "698ad8ac1b2dc6b37d61b276", "name": "Jingwei Wu", "hidden": false}, {"_id": "698ad8ac1b2dc6b37d61b277", "name": "Quan Sun", "hidden": false}, {"_id": "698ad8ac1b2dc6b37d61b278", "name": "Guopeng Li", "hidden": false}, {"_id": "698ad8ac1b2dc6b37d61b279", "user": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "isPro": false, "fullname": "Juanxi Tian", "user": "Juanxi", "type": "user"}, "name": "Juanxi Tian", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:04:13.048Z", "hidden": false}, {"_id": "698ad8ac1b2dc6b37d61b27a", "name": "Huanyu Zhang", "hidden": false}, {"_id": "698ad8ac1b2dc6b37d61b27b", "name": "Yanlin Lai", "hidden": false}, {"_id": "698ad8ac1b2dc6b37d61b27c", "name": "Ruichuan An", "hidden": false}, {"_id": "698ad8ac1b2dc6b37d61b27d", "name": "Hongbo Peng", "hidden": false}, {"_id": "698ad8ac1b2dc6b37d61b27e", "user": {"_id": "65d70e775e971572da16c05b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d70e775e971572da16c05b/8Cv71Clfk_C7k6U4yI6ln.jpeg", "isPro": false, "fullname": "YuHong Dai", "user": "BroAlanTaps", "type": "user"}, "name": "Yuhong Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:04:30.092Z", "hidden": false}, {"_id": "698ad8ac1b2dc6b37d61b27f", "name": "Chenxi Li", "hidden": false}, {"_id": "698ad8ac1b2dc6b37d61b280", "name": "Chunmei Qing", "hidden": false}, {"_id": "698ad8ac1b2dc6b37d61b281", "name": "Jia Wang", "hidden": false}, {"_id": "698ad8ac1b2dc6b37d61b282", "name": "Ziyang Meng", "hidden": false}, {"_id": "698ad8ac1b2dc6b37d61b283", "name": "Zheng Ge", "hidden": false}, {"_id": "698ad8ac1b2dc6b37d61b284", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "698ad8ac1b2dc6b37d61b285", "name": "Daxin Jiang", "hidden": false}], "publishedAt": "2026-02-09T18:52:02.000Z", "submittedOnDailyAt": "2026-02-10T04:37:26.012Z", "title": "GEBench: Benchmarking Image Generation Models as GUI Environments", "submittedOnDailyBy": {"_id": "65ddea8b2d26e59a5a33330f", "avatarUrl": "/avatars/3104ddafd6dda3c05ea9a771dbf2deeb.svg", "isPro": false, "fullname": "li haodong", "user": "mickyhimself", "type": "user"}, "summary": "Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.", "upvotes": 32, "discussionId": "698ad8ad1b2dc6b37d61b286", "ai_summary": "A new benchmark and evaluation metric are introduced for assessing temporal coherence and dynamic interaction in GUI generation models, revealing significant challenges in maintaining consistency over extended interaction sequences.", "ai_keywords": ["GUI generation", "temporal coherence", "dynamic interaction", "visual fidelity", "GUI-specific contexts", "GEBench", "GE-Score", "goal achievement", "interaction logic", "content consistency", "UI plausibility", "visual quality"], "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u6839\u636e\u7528\u6237\u6307\u4ee4\u9884\u6d4b\u672a\u6765\u7684\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u72b6\u6001\u3002</li>\n    <li>\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u6548\u679c\uff0c\u7f3a\u4e4f\u5bf9GUI\u7279\u5b9a\u4e0a\u4e0b\u6587\u4e2d\u7684\u72b6\u6001\u8f6c\u6362\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u8bc4\u4f30\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86GEBench\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30GUI\u751f\u6210\u4e2d\u7684\u52a8\u6001\u4ea4\u4e92\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002</li>\n    <li>GEBench\u5305\u542b700\u4e2a\u6837\u672c\uff0c\u6db5\u76d6\u5355\u6b65\u548c\u591a\u6b65\u4ea4\u4e92\uff0c\u6d89\u53ca\u73b0\u5b9e\u548c\u865a\u6784\u573a\u666f\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86GE-Score\uff0c\u4e00\u4e2a\u4e94\u7ef4\u5ea6\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5e2e\u52a9\u7cfb\u7edf\u8bc4\u4f30\u5f53\u524d\u6a21\u578b\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New models can predict future states of user interfaces based on instructions, but evaluation methods are lacking for how well they handle changes over time.</li>\n    <li>GEBench is introduced as a new benchmark to assess how well these models manage dynamic interactions and consistency in user interfaces.</li>\n    <li>It includes 700 examples across five categories, focusing on both quick and longer user interactions in various scenarios.</li>\n    <li>GEBench uses a new scoring system called GE-Score, which measures five important aspects: goal achievement, logic of interactions, consistency of content, plausibility of the user interface, and visual quality.</li>\n    <li>Current models do well with simple tasks but struggle with longer interactions, particularly in interpreting icons, rendering text, and precise location tracking.</li>\n</ul>"}, "publishedAt": "2026-02-09T13:52:02.000Z", "title": "GEBench: Benchmarking Image Generation Models as GUI Environments", "summary": "Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09007.png", "numComments": 1, "submittedBy": {"_id": "65ddea8b2d26e59a5a33330f", "avatarUrl": "/avatars/3104ddafd6dda3c05ea9a771dbf2deeb.svg", "fullname": "li haodong", "name": "mickyhimself", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 0, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.08439", "authors": [{"_id": "698ab3e51b2dc6b37d61afaa", "user": {"_id": "652965773a416e1f2173443b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg", "isPro": true, "fullname": "Yuhao Dong", "user": "THUdyh", "type": "user"}, "name": "Yuhao Dong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:06:16.048Z", "hidden": false}, {"_id": "698ab3e51b2dc6b37d61afab", "name": "Shulin Tian", "hidden": false}, {"_id": "698ab3e51b2dc6b37d61afac", "name": "Shuai Liu", "hidden": false}, {"_id": "698ab3e51b2dc6b37d61afad", "name": "Shuangrui Ding", "hidden": false}, {"_id": "698ab3e51b2dc6b37d61afae", "name": "Yuhang Zang", "hidden": false}, {"_id": "698ab3e51b2dc6b37d61afaf", "name": "Xiaoyi Dong", "hidden": false}, {"_id": "698ab3e51b2dc6b37d61afb0", "name": "Yuhang Cao", "hidden": false}, {"_id": "698ab3e51b2dc6b37d61afb1", "name": "Jiaqi Wang", "hidden": false}, {"_id": "698ab3e51b2dc6b37d61afb2", "name": "Ziwei Liu", "hidden": false}], "publishedAt": "2026-02-09T09:51:29.000Z", "submittedOnDailyAt": "2026-02-10T02:03:30.996Z", "title": "Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition", "submittedOnDailyBy": {"_id": "652965773a416e1f2173443b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg", "isPro": true, "fullname": "Yuhao Dong", "user": "THUdyh", "type": "user"}, "summary": "Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.", "upvotes": 28, "discussionId": "698ab3e51b2dc6b37d61afb3", "githubRepo": "https://github.com/dongyh20/Demo-ICL", "githubRepoAddedBy": "user", "ai_summary": "Researchers introduce a new video understanding task and benchmark that evaluates models' ability to learn from few-shot demonstrations, along with a specialized MLLM architecture trained using a two-stage approach combining video supervision and preference optimization.", "ai_keywords": ["Multimodal Large Language Models", "video understanding", "in-context learning", "video benchmarks", "Demo-ICL-Bench", "video-supervised fine-tuning", "direct preference optimization"], "githubStars": 25, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u7406\u89e3\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u77e5\u8bc6\uff0c\u800c\u975e\u4ece\u52a8\u6001\u65b0\u73af\u5883\u4e2d\u5b66\u4e60\u548c\u9002\u5e94\u7684\u80fd\u529b\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u4efb\u52a1\uff1a\u57fa\u4e8e\u793a\u4f8b\u7684\u89c6\u9891\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u65e8\u5728\u901a\u8fc7\u793a\u4f8b\u56de\u7b54\u5173\u4e8e\u76ee\u6807\u89c6\u9891\u7684\u95ee\u9898\u3002</li>\n    <li>\u63a8\u51fa\u4e86Demo-ICL-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u8bc4\u4f30\u6b64\u7c7b\u5b66\u4e60\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b1200\u4e2a\u6559\u5b66YouTube\u89c6\u9891\u53ca\u76f8\u5173\u95ee\u9898\u3002</li>\n    <li>\u5f00\u53d1\u4e86Demo-ICL\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u5347\u6a21\u578b\u4ece\u793a\u4f8b\u4e2d\u5b66\u4e60\u7684\u80fd\u529b\u3002</li>\n    <li>\u5927\u91cf\u5b9e\u9a8c\u8868\u660eDemo-ICL-Bench\u7684\u6311\u6218\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86Demo-ICL\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u65b9\u5411\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current video understanding models mostly rely on fixed knowledge and do not adapt well to new situations from few examples.</li>\n    <li>A new task called Demo-driven Video In-Context Learning focuses on learning from example videos to answer questions.</li>\n    <li>The Demo-ICL-Bench is a new benchmark created to test this learning approach, using 1200 YouTube videos and related questions.</li>\n    <li>This benchmark includes two types of demonstrations: text summaries of video subtitles and instructional video examples.</li>\n    <li>Demo-ICL is a new model that improves learning from examples through a special training method, and tests show it performs well on the new benchmark.</li>\n</ul>"}, "publishedAt": "2026-02-09T04:51:29.000Z", "title": "Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition", "summary": "Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08439.png", "numComments": 1, "submittedBy": {"_id": "652965773a416e1f2173443b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg", "fullname": "Yuhao Dong", "name": "THUdyh", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 50, "isUserFollowing": false}, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "isAuthorParticipating": true}],
    "week": [{"paper": {"id": "2602.04705", "authors": [{"_id": "698424a7e34659da7e1f4e6f", "name": "Haifeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e70", "name": "Hua Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e71", "name": "Tian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e72", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4e73", "name": "Jing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e74", "name": "Dianhai Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e75", "name": "Yanjun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4e76", "name": "Jingzhou He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e77", "name": "Zhongjun He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e78", "name": "Dou Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e79", "name": "Qiwen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7a", "name": "Shuohuan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7b", "user": {"_id": "62cd9632342b1d5dab8df4c3", "avatarUrl": "/avatars/9080d20bb57a05a1eeb6800eba886cf9.svg", "isPro": false, "fullname": "Junyuan Shang", "user": "sjy1203", "type": "user"}, "name": "Junyuan Shang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:28.482Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7c", "user": {"_id": "67f37f78b36e82d366dedeec", "avatarUrl": "/avatars/678bb5891d5c2e80edc0799d2308a5d3.svg", "isPro": false, "fullname": "Max Zhenyu Zhang", "user": "max-zhenyu-zhang", "type": "user"}, "name": "Zhenyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:03.972Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7d", "name": "Yuchen Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7e", "name": "Jinle Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7f", "name": "Jiabin Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e80", "name": "Liang Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e81", "name": "Ruibiao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e82", "name": "Weichong Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4e83", "name": "Siyu Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e84", "name": "Dai Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4e85", "name": "Shikun Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e86", "name": "Siqi Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e87", "name": "Bolei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e88", "name": "Yan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e89", "name": "Zhenyu Jiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8a", "name": "Ruiqing Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8b", "name": "Zeyu Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8c", "name": "Qingqing Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8d", "name": "Kaipeng Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8e", "name": "Jiajun Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8f", "name": "Enlei Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e90", "name": "Guoxia Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e91", "name": "Yanlin Sha", "hidden": false}, {"_id": "698424a7e34659da7e1f4e92", "name": "Yi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e93", "name": "Yehan Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e94", "name": "Weijian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e95", "name": "Jiaxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e96", "name": "Zengfeng Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e97", "name": "Yingqi Qu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e98", "name": "Zhongli Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4e99", "name": "Zhengkun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9a", "name": "Xiyang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9b", "name": "Zixiang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9c", "name": "Xinchao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9d", "name": "Zhengjie Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9e", "name": "Dong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9f", "name": "Bingjin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea0", "name": "Yue Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea1", "name": "Xing Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea2", "name": "Shiwei Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea3", "name": "Qiao Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea4", "name": "Xinzhe Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea5", "name": "Shuangshuang Qiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea6", "name": "Baoshan Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea7", "name": "Bihong Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea8", "name": "Bin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea9", "name": "Bingquan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaa", "name": "Binhan Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eab", "name": "Binxiong Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4eac", "name": "Bo Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4ead", "name": "Bo Ke", "hidden": false}, {"_id": "698424a7e34659da7e1f4eae", "name": "Bo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaf", "name": "Bowen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb0", "name": "Boyan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb1", "name": "Boyang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb2", "name": "Caiji Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb3", "name": "Can Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb4", "name": "Chang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb5", "name": "Chao Pang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb6", "name": "Chao Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb7", "name": "Chaoyi Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb8", "name": "Chen Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb9", "name": "Cheng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4eba", "name": "Chenlin Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebb", "name": "Chun Gan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebc", "name": "Chunguang Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebd", "name": "Chuyu Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebe", "name": "Cuiyun Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebf", "name": "Dan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec0", "name": "Danlei Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec1", "name": "Danxiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec2", "name": "Dong Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec3", "name": "Dongbo Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec4", "name": "Dongdong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec5", "name": "Dongdong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec6", "name": "Dongxue Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec7", "name": "Fan Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec8", "name": "Fan Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec9", "name": "Fan Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eca", "name": "Fan Mo", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecb", "name": "Feisheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecc", "name": "Fengwei Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecd", "name": "Gangqiang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ece", "name": "Gaofeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecf", "name": "Gaopeng Yong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed0", "name": "Gexiao Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed1", "user": {"_id": "698419de94015f1e5eedacec", "avatarUrl": "/avatars/e80baa6f9efcd5e5d7cc9b93ac852c7b.svg", "isPro": false, "fullname": "Guan Wang", "user": "guanwcn", "type": "user"}, "name": "Guan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:38.213Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed2", "name": "Guangchen Ni", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed3", "name": "Guangshuo Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed4", "name": "Guanzhong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed5", "user": {"_id": "609cd5ab335f23cd2fa0f211", "avatarUrl": "/avatars/8331a7025a6aa4eabc5b6502bf8a0a63.svg", "isPro": false, "fullname": "Guihua Liu", "user": "LLLL", "type": "user"}, "name": "Guihua Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:31.029Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed6", "name": "Guishun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed7", "name": "Haibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed8", "name": "Haijian Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed9", "name": "Haipeng Ming", "hidden": false}, {"_id": "698424a7e34659da7e1f4eda", "name": "Haisu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4edb", "name": "Haiyang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4edc", "name": "Haiye Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4edd", "name": "Han Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ede", "name": "Hangting Lou", "hidden": false}, {"_id": "698424a7e34659da7e1f4edf", "name": "Hanwen Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee0", "name": "Hanzhi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee1", "name": "Hao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee2", "name": "Hao Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee3", "name": "Hao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee4", "name": "Hao Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee5", "name": "Haochen Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee6", "name": "Haodong Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee7", "name": "Haoshuang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee8", "name": "Haozhe Geng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee9", "name": "Heju Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4eea", "name": "Hong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eeb", "name": "Hongchen Xue", "hidden": false}, {"_id": "698424a7e34659da7e1f4eec", "name": "Hongen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eed", "name": "Honggeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eee", "name": "Hongji Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eef", "name": "Hongwei Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef0", "name": "Hongyang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef1", "name": "Hongyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef2", "name": "Hua Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef3", "name": "Huan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef4", "name": "Huan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef5", "name": "Huang He", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef6", "name": "Hui Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef7", "name": "Hui Zhong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef8", "name": "Huibin Ruan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef9", "name": "Jiafeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efa", "name": "Jiage Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efb", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efc", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efd", "name": "Jiajie Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efe", "name": "Jialin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eff", "name": "Jian Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f00", "name": "Jian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f01", "name": "Jianfeng Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f02", "name": "Jianguang Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f03", "name": "Jianhua Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f04", "name": "Jianye Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f05", "name": "Jiaodi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f06", "name": "Jiarui Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f07", "name": "Jiawei Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f08", "name": "Jiaxin Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f09", "name": "Jiaxuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0a", "name": "Jie Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0b", "name": "Jie Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0c", "name": "Jiefan Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0d", "name": "Jihan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0e", "name": "Jihua Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0f", "name": "Jing Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f10", "name": "Jing Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f11", "name": "Jing Yan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f12", "name": "Jingdong Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f13", "name": "Jingdong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f14", "name": "Jingjing Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f15", "name": "Jingyong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f16", "name": "Jinheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f17", "name": "Jinjin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f18", "name": "Jinliang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f19", "name": "Jinlin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1a", "name": "Jinnan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1b", "name": "Jixiang Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1c", "name": "Jiyi Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1d", "name": "Jiyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1e", "name": "Jun Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1f", "name": "Jun Xia", "hidden": false}, {"_id": "698424a7e34659da7e1f4f20", "name": "Jun Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f21", "name": "Junda Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f22", "name": "Junhao Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f23", "name": "Junhong Xiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f24", "name": "Junliang Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f25", "name": "Kai Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f26", "name": "Kailun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f27", "name": "Kairan Su", "hidden": false}, {"_id": "698424a7e34659da7e1f4f28", "name": "Kang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f29", "name": "Kangkang Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2a", "name": "Ke Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2b", "name": "Ke Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2c", "name": "Kui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2d", "name": "Kun Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2e", "name": "Kunbin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2f", "name": "Lei Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f30", "name": "Lei Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f31", "name": "Lei Wen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f32", "name": "Linghui Meng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f33", "user": {"_id": "641e69355c348064a8251471", "avatarUrl": "/avatars/acad3877df27ff44ea3921bb43e34d53.svg", "isPro": false, "fullname": "Linhao Yu", "user": "HasuerYu", "type": "user"}, "name": "Linhao Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:47.812Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f34", "name": "Liping Ouyang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f35", "name": "Liwen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f36", "user": {"_id": "65cf859f88d13d8128bb8545", "avatarUrl": "/avatars/aa18b993bd90d9c8a95913050cd955a8.svg", "isPro": false, "fullname": "Longbin Ji", "user": "robingg1", "type": "user"}, "name": "Longbin Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:40.295Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f37", "name": "Longzhi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f38", "name": "Meng Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f39", "name": "Meng Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3a", "name": "Mengfei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3b", "name": "Mengqi Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3c", "name": "Mengyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3d", "name": "Ming Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3e", "name": "Mingcheng Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3f", "name": "Mingming Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f40", "name": "Mingxin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f41", "name": "Mingzhu Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f42", "name": "Naibin Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f43", "name": "Nemin Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f44", "name": "Nian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f45", "name": "Peng Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f46", "name": "Peng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f47", "name": "Pengyu Zou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f48", "name": "Qi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f49", "name": "Qi Xin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4a", "name": "Qian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4b", "name": "Qiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4c", "name": "Qianhui Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4d", "name": "Qianwei Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4e", "name": "Qianyue He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4f", "name": "Qifei Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f50", "name": "Qinrui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f51", "name": "Qiwen Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f52", "name": "Quan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f53", "name": "Quanxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f54", "name": "Qunyi Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4f55", "name": "Rongrui Zhan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f56", "name": "Rufeng Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f57", "name": "Rui Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f58", "name": "Ruian Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f59", "name": "Ruihao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5a", "name": "Ruijie Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5b", "name": "Ruixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5c", "name": "Ruixuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5d", "name": "Runsheng Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5e", "name": "Ruting Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5f", "name": "Senbo Kang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f60", "name": "Shan Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f61", "name": "Shaofei Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f62", "name": "Shaotian Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f63", "name": "Shenwei Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f64", "name": "Shifeng Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f65", "name": "Shihao Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f66", "name": "Shilong Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f67", "name": "Shiqin Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f68", "name": "Shiwei Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f69", "name": "Shixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6a", "name": "Shuai Yao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6b", "name": "Shuang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6c", "name": "Shuangqiao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6d", "name": "Shuhao Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6e", "name": "Shuwei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6f", "name": "Shuwen Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f70", "user": {"_id": "62769a608483d8e9ecd9b4f8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672799958233-62769a608483d8e9ecd9b4f8.jpeg", "isPro": false, "fullname": "Sijun He", "user": "sijunhe", "type": "user"}, "name": "Sijun He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:33.392Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f71", "user": {"_id": "64fada13d82fc6977d5e9c74", "avatarUrl": "/avatars/776bf1257154289e919716637770ef52.svg", "isPro": false, "fullname": "Siming Dai", "user": "DesmonDay", "type": "user"}, "name": "Siming Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:50.302Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f72", "name": "Siming Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f73", "name": "Siyi Long", "hidden": false}, {"_id": "698424a7e34659da7e1f4f74", "name": "Songhe Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f75", "name": "Suhui Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f76", "name": "Suyin Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f77", "name": "Teng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f78", "name": "Tianchan Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f79", "name": "Tianliang Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7a", "user": {"_id": "67bbe929593452cc18877606", "avatarUrl": "/avatars/f50fd1cb35d628c26cf21ad0c95c55b1.svg", "isPro": false, "fullname": "tmyangcs", "user": "youngtimmy", "type": "user"}, "name": "Tianmeng Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:36.143Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7b", "name": "Tianyi Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7c", "name": "Tiezhu Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7d", "name": "Ting Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7e", "name": "Ting Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7f", "name": "Tingdan Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f80", "name": "Wei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f81", "name": "Wei Luan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f82", "name": "Wei Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f83", "name": "Wei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f84", "name": "Wei Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f85", "name": "Weibao Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f86", "name": "Weibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f87", "name": "Weicheng Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f88", "name": "Weichong Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f89", "name": "Weiguo Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8a", "name": "Weilong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8b", "name": "Weiqi Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8c", "name": "Wen Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8d", "name": "Wenbin Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8e", "name": "Wenjing Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8f", "name": "Wenlong Miao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f90", "name": "Wenpei Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f91", "name": "Wenquan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f92", "name": "Xi Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f93", "name": "Xi Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f94", "name": "Xiang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f95", "name": "Xiangguo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f96", "name": "Xiangrui Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f97", "name": "Xiangsen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f98", "name": "Xiangzhe Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f99", "name": "Xianlong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9a", "name": "Xianying Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9b", "name": "Xiao Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9c", "name": "Xiaocong Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9d", "name": "Xiaofei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9e", "name": "Xiaofeng Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9f", "name": "Xiaofeng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa0", "name": "Xiaojian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa1", "name": "Xiaolan Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa2", "name": "Xiaopeng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa3", "name": "Xiaotian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa4", "name": "Xiaoxiong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa5", "name": "Xiaoxu Fei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa6", "name": "Xiaoxuan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa7", "user": {"_id": "664395621b88258a527cd7d1", "avatarUrl": "/avatars/8489ccebe4fd1262679ba63a5cb50bb8.svg", "isPro": false, "fullname": "Kira", "user": "Kira-wang", "type": "user"}, "name": "Xiaoyu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:25.774Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa8", "name": "Xiaoyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa9", "name": "Xin Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4faa", "name": "Xin Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fab", "name": "Xinhui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fac", "name": "Xinming Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fad", "name": "Xintong Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fae", "name": "Xinyi Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4faf", "name": "Xinyu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb0", "name": "Xiuxian Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb1", "name": "XuanShi Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb2", "name": "Xue Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb3", "name": "Xueying Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb4", "name": "Xuhong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb5", "name": "Xulong Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb6", "name": "Xuyi Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb7", "name": "Yabing Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb8", "name": "Yafeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb9", "name": "Yamei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fba", "name": "Yan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbb", "name": "Yanfu Cheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbc", "name": "Yang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbd", "name": "Yang Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbe", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbf", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc0", "name": "Yang Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc1", "name": "Yanlong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc2", "name": "Yannian Fu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc3", "name": "Yanpeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc4", "name": "Yanzheng Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc5", "name": "Yao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc6", "name": "Yaozong Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc7", "name": "Yaqian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc8", "name": "Yehua Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc9", "name": "Yekun Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fca", "name": "Yesong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcb", "name": "Yi Song", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcc", "name": "Yichen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcd", "name": "Yifei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fce", "name": "Yifeng Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcf", "name": "Yifeng Kou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd0", "name": "Yilong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd1", "name": "Yilong Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd2", "name": "Yiming Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd3", "name": "Ying Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd4", "name": "Ying Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd5", "name": "Yingsheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd6", "name": "Yingzhan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd7", "name": "Yinqi Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd8", "name": "Yiran Xing", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd9", "name": "Yishu Lei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fda", "name": "Yixiang Tu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdb", "name": "Yiyan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdc", "name": "Yong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdd", "name": "Yonghua Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fde", "name": "Yongqiang Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdf", "name": "Yongxing Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe0", "name": "Yongyue Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe1", "name": "Yu Ran", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe2", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe3", "name": "Yu-Wen Michael Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe4", "name": "Yuang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe5", "name": "Yuanle Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe6", "name": "Yuanyuan Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe7", "name": "Yubo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe8", "name": "Yuchen Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe9", "name": "Yucheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fea", "name": "Yude Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4feb", "name": "Yuedong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fec", "name": "Yuehu Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4fed", "name": "Yufeng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fee", "name": "Yuhui Cao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fef", "name": "Yuhui Yun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff0", "name": "Yukun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff1", "name": "Yukun Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff2", "name": "Yukun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff3", "name": "Yumeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff4", "name": "Yun Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff5", "name": "Yun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff6", "name": "Yunfei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff7", "name": "Yunshen Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff8", "name": "Yuping Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff9", "name": "Yuqin Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffa", "name": "Yuqing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffb", "name": "Yurui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffc", "name": "Yuwen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffd", "name": "Yuxiang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffe", "name": "Zefeng Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fff", "name": "Zelin Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5000", "name": "Zelun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5001", "name": "Zenan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f5002", "name": "Zezhao Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5003", "name": "Zhaowu Pan", "hidden": false}, {"_id": "698424a7e34659da7e1f5004", "name": "Zhaoyu Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f5005", "name": "Zhe Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5006", "name": "Zhe Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5007", "name": "Zhen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5008", "name": "Zhengfan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f5009", "name": "Zhengrui Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f500a", "name": "Zhengsheng Ning", "hidden": false}, {"_id": "698424a7e34659da7e1f500b", "name": "Zhenxing Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500c", "name": "Zhenyu Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500d", "name": "Zhenyu Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f500e", "name": "Zhenyun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500f", "name": "Zhi Li", "hidden": false}, {"_id": "698424a7e34659da7e1f5010", "name": "Zhichao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5011", "name": "Zhicheng Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5012", "name": "Zhida Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5013", "name": "Zhifan Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5014", "name": "Zhihao Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f5015", "name": "Zhijin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f5016", "name": "Zhiyang Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5017", "name": "Zhonghui Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f5018", "name": "Zhuangzhuang Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f5019", "name": "Zhujun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f501a", "name": "Zhuo Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f501b", "name": "Zichang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f501c", "name": "Zihan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f501d", "name": "Zihao Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f501e", "name": "Zihe Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f501f", "name": "Ziheng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5020", "name": "Ziping Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5021", "name": "Zixuan Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f5022", "name": "Ziyang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f5023", "name": "Ziyi Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f5024", "name": "Ziyuan Gao", "hidden": false}], "publishedAt": "2026-02-04T16:18:15.000Z", "submittedOnDailyAt": "2026-02-05T02:34:05.150Z", "title": "ERNIE 5.0 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "upvotes": 198, "discussionId": "698424a7e34659da7e1f5025", "ai_summary": "ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.", "ai_keywords": ["autoregressive foundation model", "unified multimodal understanding", "unified next-group-of-tokens prediction objective", "mixture-of-experts", "modality-agnostic expert routing", "elastic training paradigm", "reinforcement learning", "sparse MoE architecture"], "summary_zh": "<ul>\n    <li>ERNIE 5.0 \u662f\u4e00\u79cd\u65b0\u578b\u81ea\u56de\u5f52\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u8d85\u7a00\u758f\u7684\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u6240\u6709\u6a21\u6001\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\uff0c\u76ee\u6807\u662f\u9884\u6d4b\u4e0b\u4e00\u4e2a\u6807\u8bb0\u3002</li>\n    <li>ERNIE 5.0 \u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f39\u6027\u8bad\u7ec3\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u5355\u6b21\u9884\u8bad\u7ec3\u4e2d\u5b66\u4e60\u4e0d\u540c\u6df1\u5ea6\u548c\u4e13\u5bb6\u5bb9\u91cf\u7684\u5b50\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u591a\u6a21\u6001\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u4e14\u5747\u8861\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u662f\u9996\u4e2a\u652f\u6301\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u7684\u4e07\u4ebf\u53c2\u6570\u81ea\u56de\u5f52\u6a21\u578b\u3002</li>\n    <li>\u62a5\u544a\u4e2d\u8fd8\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u4e13\u5bb6\u8def\u7531\u53ef\u89c6\u5316\u548c\u5f39\u6027\u8bad\u7ec3\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u63d0\u4f9b\u6df1\u5165\u89c1\u89e3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>ERNIE 5.0 is a new model designed to understand and generate text, images, videos, and audio all at once.</li>\n    <li>The model is built using a special architecture that allows it to work efficiently by using a mixture of experts for different tasks.</li>\n    <li>It has a unique training method that helps it adapt to different resource needs, making it flexible for various uses.</li>\n    <li>ERNIE 5.0 is the first publicly available model of its size, with a trillion parameters, that can handle multiple types of data together.</li>\n    <li>Research findings and visualizations of how the model works are shared to help others in the field.</li>\n</ul>"}, "publishedAt": "2026-02-04T11:18:15.000Z", "title": "ERNIE 5.0 Technical Report", "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04705.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 228, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.07085", "authors": [{"_id": "698ab6f91b2dc6b37d61b031", "name": "Jun Han", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b032", "name": "Shuo Zhang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b033", "name": "Wei Li", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b034", "user": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "name": "Zhi Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:58.707Z", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b035", "name": "Yifan Dong", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b036", "name": "Tu Hu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b037", "name": "Jialuo Yuan", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b038", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:06:00.954Z", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b039", "name": "Yumo Zhu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03a", "name": "Fangqi Lou", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03b", "name": "Xin Guo", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03c", "name": "Zhaowei Liu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03d", "name": "Tianyi Jiang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03e", "name": "Ruichuan An", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03f", "name": "Jingping Liu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b040", "name": "Biao Wu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b041", "name": "Rongze Chen", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b042", "name": "Kunyi Wang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b043", "name": "Yifan Wang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b044", "name": "Sen Hu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b045", "name": "Xinbing Kong", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b046", "name": "Liwen Zhang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b047", "name": "Ronghao Chen", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b048", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-02-06T08:08:04.000Z", "submittedOnDailyAt": "2026-02-10T02:19:22.216Z", "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining", "submittedOnDailyBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "summary": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.", "upvotes": 141, "discussionId": "698ab6fa1b2dc6b37d61b049", "githubRepo": "https://github.com/QuantaAlpha/QuantaAlpha", "githubRepoAddedBy": "user", "githubStars": 63, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "summary_zh": "<ul>\n  <li>\u91d1\u878d\u5e02\u573a\u566a\u58f0\u5927\u4e14\u4e0d\u7a33\u5b9a\uff0c\u5bfc\u81f4\u56de\u6d4b\u7ed3\u679c\u5bf9\u566a\u58f0\u548c\u5e02\u573a\u53d8\u5316\u975e\u5e38\u654f\u611f\u3002</li>\n  <li>QuantaAlpha\u662f\u4e00\u4e2a\u8fdb\u5316\u578b\u7684alpha\u6316\u6398\u6846\u67b6\uff0c\u901a\u8fc7\u8f68\u8ff9\u7ea7\u7684\u53d8\u5f02\u548c\u4ea4\u53c9\u64cd\u4f5c\u6765\u6539\u8fdb\u6316\u6398\u8fc7\u7a0b\u3002</li>\n  <li>\u5728\u6bcf\u4e2a\u6316\u6398\u8f68\u8ff9\u4e2d\uff0cQuantaAlpha\u5b9a\u4f4d\u6b21\u4f18\u6b65\u9aa4\u8fdb\u884c\u4fee\u6b63\uff0c\u5e76\u91cd\u65b0\u7ec4\u5408\u9ad8\u6536\u76ca\u6bb5\uff0c\u589e\u5f3a\u63a2\u7d22\u548c\u6539\u8fdb\u3002</li>\n  <li>\u5728\u56e0\u5b50\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0cQuantaAlpha\u786e\u4fdd\u5047\u8bbe\u3001\u56e0\u5b50\u8868\u8fbe\u548c\u53ef\u6267\u884c\u4ee3\u7801\u7684\u4e00\u81f4\u6027\uff0c\u51cf\u5c0f\u590d\u6742\u6027\u548c\u5197\u4f59\u3002</li>\n  <li>\u5728CSI 300\u7684\u5b9e\u9a8c\u4e2d\uff0cQuantaAlpha\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u56de\u62a5\u7387\u548c\u6709\u6548\u6027\uff0c\u5e76\u80fd\u6709\u6548\u8fc1\u79fb\u5230\u5176\u4ed6\u5e02\u573a\u6307\u6570\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Financial markets can be unpredictable, making it hard to test trading strategies accurately.</li>\n    <li>QuantaAlpha is a new framework designed to improve the process of finding effective trading strategies by using evolutionary techniques.</li>\n    <li>It improves strategies by focusing on specific steps that need revision and combining successful patterns from different attempts.</li>\n    <li>QuantaAlpha ensures that the strategies it generates are clear and not overly complicated, which helps avoid issues with similar strategies competing against each other.</li>\n    <li>Tests show that QuantaAlpha performs better than existing models and its strategies can also work well in different market conditions.</li>\n</ul>"}, "publishedAt": "2026-02-06T03:08:04.000Z", "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining", "summary": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07085.png", "numComments": 1, "submittedBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "fullname": "Zhi Yang", "name": "yangzhi1", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.08794", "authors": [{"_id": "698ac65d1b2dc6b37d61b1c2", "name": "SII-OpenMOSS Team", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c4", "name": "Donghua Yu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c5", "name": "Mingshu Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c6", "name": "Qi Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c7", "name": "Qi Luo", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c8", "name": "Qianyi Wu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c9", "user": {"_id": "63ec4715c81b6a52391c46b8", "avatarUrl": "/avatars/496819b5075a1a834a2b9edeb068c80e.svg", "isPro": false, "fullname": "QinyuanCheng", "user": "Cqy2019", "type": "user"}, "name": "Qinyuan Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:07.400Z", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1ca", "name": "Ruixiao Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cb", "user": {"_id": "62c14609ac1b639c2d87192c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png", "isPro": false, "fullname": "SII-liangtianyi", "user": "tianyilt", "type": "user"}, "name": "Tianyi Liang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:10.522Z", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cc", "name": "Wenbo Zhang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cd", "name": "Wenming Tu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1ce", "name": "Xiangyu Peng", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cf", "name": "Yang Gao", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d0", "name": "Yanru Huo", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d1", "user": {"_id": "69158ffc0153b85a677dcc46", "avatarUrl": "/avatars/c9c5f60522f2a8f370d790ea9938b090.svg", "isPro": false, "fullname": "Ying Zhu", "user": "Auraithm", "type": "user"}, "name": "Ying Zhu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:27:41.440Z", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d2", "name": "Yinze Luo", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d3", "name": "Yiyang Zhang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d4", "name": "Yuerong Song", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d5", "name": "Zhe Xu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d6", "name": "Zhiyu Zhang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d7", "name": "Chenchen Yang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d8", "name": "Cheng Chang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d9", "name": "Chushu Zhou", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1da", "name": "Hanfu Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1db", "name": "Hongnan Ma", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1dc", "name": "Jiaxi Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1dd", "name": "Jingqi Tong", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1de", "name": "Junxi Liu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1df", "name": "Ke Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e0", "name": "Shimin Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e1", "name": "Songlin Wang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e2", "name": "Wei Jiang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e3", "name": "Zhaoye Fei", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e4", "name": "Zhiyuan Ning", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e5", "name": "Chunguo Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e6", "name": "Chenhui Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e7", "name": "Ziwei He", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e8", "name": "Zengfeng Huang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e9", "name": "Xie Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1ea", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-02-09T15:31:54.000Z", "submittedOnDailyAt": "2026-02-10T03:18:59.260Z", "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation", "submittedOnDailyBy": {"_id": "62c14609ac1b639c2d87192c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png", "isPro": false, "fullname": "SII-liangtianyi", "user": "tianyilt", "type": "user"}, "summary": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.", "upvotes": 131, "discussionId": "698ac65e1b2dc6b37d61b1eb", "projectPage": "https://mosi.cn/models/mova", "githubRepo": "https://github.com/OpenMOSS/MOVA", "githubRepoAddedBy": "user", "ai_summary": "MOVA is an open-source model that generates synchronized audio-visual content using a Mixture-of-Experts architecture with 32 billion parameters, supporting image-text to video-audio generation tasks.", "ai_keywords": ["Mixture-of-Experts", "MoE", "audio-visual content", "lip-synced speech", "sound effects", "content-aligned music", "IT2VA", "efficient inference", "LoRA fine-tuning", "prompt enhancement"], "githubStars": 579, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u97f3\u9891\u5728\u89c6\u9891\u5185\u5bb9\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u751f\u6210\u6a21\u578b\u901a\u5e38\u5ffd\u89c6\u97f3\u9891\u90e8\u5206\u3002</li>\n    <li>\u5f53\u524d\u7684\u97f3\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4f7f\u7528\u7ea7\u8054\u6d41\u7a0b\uff0c\u5bfc\u81f4\u6210\u672c\u589e\u52a0\u548c\u8d28\u91cf\u4e0b\u964d\u3002</li>\n    <li>MOVA\uff08MOSS\u89c6\u9891\u548c\u97f3\u9891\uff09\u662f\u4e00\u4e2a\u5f00\u6e90\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u540c\u6b65\u97f3\u89c6\u9891\u5185\u5bb9\u3002</li>\n    <li>MOVA\u4f7f\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u5177\u6709320\u4ebf\u53c2\u6570\uff0c\u5176\u4e2d180\u4ebf\u5728\u63a8\u7406\u65f6\u6d3b\u8dc3\u3002</li>\n    <li>\u53d1\u5e03\u6a21\u578b\u6743\u91cd\u548c\u4ee3\u7801\u65e8\u5728\u63a8\u52a8\u7814\u7a76\u5e76\u4fc3\u8fdb\u521b\u4f5c\u8005\u793e\u533a\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Audio is important for video, but many models ignore it, leading to quality issues.</li>\n    <li>Current methods often use separate processes for audio and video, which can be costly and error-prone.</li>\n    <li>MOVA (MOSS Video and Audio) is a new open-source model that generates high-quality synchronized audio and video.</li>\n    <li>MOVA can create realistic lip-synced speech, sound effects, and music that aligns with the video content.</li>\n    <li>The model has a large architecture and is designed to support collaboration and further research in the field.</li>\n</ul>"}, "publishedAt": "2026-02-09T10:31:54.000Z", "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation", "summary": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08794.png", "numComments": 1, "submittedBy": {"_id": "62c14609ac1b639c2d87192c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png", "fullname": "SII-liangtianyi", "name": "tianyilt", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.08222", "authors": [{"_id": "698ad20e1b2dc6b37d61b227", "user": {"_id": "64afe1653361f887816da303", "avatarUrl": "/avatars/320d71adacfad9dd5db064b4ed3dec2b.svg", "isPro": false, "fullname": "chenzehao", "user": "chhao", "type": "user"}, "name": "Zehao Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:04:42.021Z", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b228", "name": "Gongxun Li", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b229", "name": "Tianxiang Ai", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b22a", "name": "Yifei Li", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b22b", "name": "Zixuan Huang", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b22c", "name": "Wang Zhou", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b22d", "name": "Fuzhen Zhuang", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b22e", "name": "Xianglong Liu", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b22f", "name": "Jianxin Li", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b230", "name": "Deqing Wang", "hidden": false}, {"_id": "698ad20e1b2dc6b37d61b231", "user": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "isPro": false, "fullname": "Yikun B", "user": "Yikunb", "type": "user"}, "name": "Yikun Ban", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:04:44.617Z", "hidden": false}], "publishedAt": "2026-02-09T02:50:40.000Z", "submittedOnDailyAt": "2026-02-10T04:36:28.975Z", "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger", "submittedOnDailyBy": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "isPro": false, "fullname": "Yikun B", "user": "Yikunb", "type": "user"}, "summary": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.", "upvotes": 118, "discussionId": "698ad20e1b2dc6b37d61b232", "githubRepo": "https://github.com/chenzehao82/Weak-Driven-Learning", "githubRepoAddedBy": "user", "ai_summary": "WMSS is a post-training paradigm that uses weak model checkpoints to identify and fill learning gaps, enabling continued improvement beyond conventional saturation points in large language models.", "ai_keywords": ["post-training optimization", "large language models", "saturation bottleneck", "weak checkpoints", "entropy dynamics", "compensatory learning", "learning gaps"], "githubStars": 39, "summary_zh": "<ul>\n    <li>\u540e\u8bad\u7ec3\u4f18\u5316\u662f\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5173\u952e\uff0c\u4f46\u6a21\u578b\u8fc7\u4e8e\u81ea\u4fe1\u65f6\uff0c\u8bad\u7ec3\u6548\u679c\u4f1a\u51cf\u5f31\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5f3a\u5316\u76ee\u6807\u9884\u6d4b\uff0c\u4f46\u6211\u4eec\u53d1\u73b0\u6a21\u578b\u5386\u53f2\u4e2d\u7684\u5f31\u72b6\u6001\u4ecd\u6709\u6709\u7528\u7684\u76d1\u7763\u4fe1\u53f7\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51faWMSS\uff08\u5f31\u4ee3\u7406\u53ef\u4ee5\u8ba9\u5f3a\u4ee3\u7406\u66f4\u5f3a\uff09\u4f5c\u4e3a\u540e\u8bad\u7ec3\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u5f31\u68c0\u67e5\u70b9\u6765\u6307\u5bfc\u4f18\u5316\u3002</li>\n    <li>WMSS\u901a\u8fc7\u8bc6\u522b\u53ef\u6062\u590d\u7684\u5b66\u4e60\u5dee\u8ddd\u5e76\u8fdb\u884c\u8865\u507f\u5b66\u4e60\uff0c\u4f7f\u5f3a\u4ee3\u7406\u80fd\u591f\u8d85\u8d8a\u5e38\u89c4\u7684\u540e\u8bad\u7ec3\u9971\u548c\u72b6\u6001\u3002</li>\n    <li>\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u6211\u4eec\u65b9\u6cd5\u8bad\u7ec3\u7684\u4ee3\u7406\u80fd\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u4e14\u6ca1\u6709\u989d\u5916\u7684\u63a8\u7406\u6210\u672c\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Post-training optimization is important for improving large language models, but there's a limit to how much they can improve once they become very confident.</li>\n  <li>Current training methods reinforce what the models already know, but there is valuable information in their earlier, less confident states.</li>\n  <li>We introduce WMSS (Weak Agents Can Make Strong Agents Stronger), a new method that uses these weaker states to help improve model performance.</li>\n  <li>WMSS identifies learning gaps and helps models learn from them, allowing for better improvements despite previous saturation.</li>\n  <li>Tests show that models using WMSS perform better in tasks like math reasoning and code generation without increasing inference costs.</li>\n</ul>"}, "publishedAt": "2026-02-08T21:50:40.000Z", "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger", "summary": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08222.png", "numComments": 4, "submittedBy": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "fullname": "Yikun B", "name": "Yikunb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.04634", "authors": [{"_id": "69840a42e34659da7e1f4da8", "user": {"_id": "653a5b0f7c01c693a16dd184", "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg", "isPro": false, "fullname": "Zelai Xu", "user": "zelaix", "type": "user"}, "name": "Zelai Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:54:21.312Z", "hidden": false}, {"_id": "69840a42e34659da7e1f4da9", "name": "Zhexuan Xu", "hidden": false}, {"_id": "69840a42e34659da7e1f4daa", "user": {"_id": "683fb41cb1bf6fbcce6bc205", "avatarUrl": "/avatars/544ff46b9ff78f8420981fa507da767e.svg", "isPro": false, "fullname": "Ruize Zhang", "user": "Ruize-Zhang", "type": "user"}, "name": "Ruize Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:54:19.167Z", "hidden": false}, {"_id": "69840a42e34659da7e1f4dab", "name": "Chunyang Zhu", "hidden": false}, {"_id": "69840a42e34659da7e1f4dac", "name": "Shi Yu", "hidden": false}, {"_id": "69840a42e34659da7e1f4dad", "name": "Weilin Liu", "hidden": false}, {"_id": "69840a42e34659da7e1f4dae", "name": "Quanlu Zhang", "hidden": false}, {"_id": "69840a42e34659da7e1f4daf", "name": "Wenbo Ding", "hidden": false}, {"_id": "69840a42e34659da7e1f4db0", "name": "Chao Yu", "hidden": false}, {"_id": "69840a42e34659da7e1f4db1", "name": "Yu Wang", "hidden": false}], "publishedAt": "2026-02-04T15:05:12.000Z", "submittedOnDailyAt": "2026-02-05T02:30:13.468Z", "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning", "submittedOnDailyBy": {"_id": "653a5b0f7c01c693a16dd184", "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg", "isPro": false, "fullname": "Zelai Xu", "user": "zelaix", "type": "user"}, "summary": "Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.", "upvotes": 71, "discussionId": "69840a43e34659da7e1f4db2", "projectPage": "https://wideseek-r1.github.io/", "githubRepo": "https://github.com/RLinf/RLinf/tree/main/examples/wideseek_r1", "githubRepoAddedBy": "user", "ai_summary": "Multi-agent systems using reinforcement learning enable parallel information seeking with scalable orchestration, achieving performance comparable to larger single agents.", "ai_keywords": ["Large Language Models", "multi-agent systems", "multi-agent reinforcement learning", "lead-agent-subagent framework", "parallel execution", "information seeking", "WideSearch benchmark", "F1 score"], "githubStars": 2379, "organization": {"_id": "689ea978824b212c988bc8f5", "name": "RLinf", "fullname": "RLinf", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/689ea8a1a73ecc6940dbba3d/T2RGCw18z6lYP1WfkIGJ3.jpeg"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6700\u65b0\u8fdb\u5c55\u4e3b\u8981\u96c6\u4e2d\u5728\u6df1\u5ea6\u6269\u5c55\u4e0a\uff0c\u4ee5\u89e3\u51b3\u590d\u6742\u7684\u95ee\u9898\u3002</li>\n    <li>\u968f\u7740\u4efb\u52a1\u8303\u56f4\u7684\u6269\u5927\uff0c\u5173\u952e\u74f6\u9888\u4ece\u4e2a\u4f53\u80fd\u529b\u8f6c\u5411\u7ec4\u7ec7\u80fd\u529b\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u51fa\u4e86WideSeek-R1\uff0c\u4e00\u4e2a\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u65e8\u5728\u6539\u5584\u5e7f\u6cdb\u4fe1\u606f\u641c\u7d22\u3002</li>\n    <li>WideSeek-R1\u901a\u8fc7\u591a\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u534f\u4f5c\u548c\u5e76\u884c\u6267\u884c\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cWideSeek-R1-4B\u5728\u4fe1\u606f\u641c\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u5355\u4ee3\u7406\u7cfb\u7edf\u7684\u8868\u73b0\u76f8\u5f53\uff0c\u5e76\u4e14\u968f\u7740\u5e76\u884c\u5b50\u4ee3\u7406\u6570\u91cf\u7684\u589e\u52a0\uff0c\u6027\u80fd\u6301\u7eed\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent developments in Large Language Models (LLMs) have improved their ability to solve complex, multi-step problems.</li>\n    <li>As tasks become more extensive, the challenge shifts from individual skills to how well multiple agents can work together.</li>\n    <li>This research introduces WideSeek-R1, a system that uses a main agent and supporting agents to work together efficiently.</li>\n    <li>WideSeek-R1 is trained using multi-agent reinforcement learning to optimize teamwork and speed up task execution.</li>\n    <li>Tests show that WideSeek-R1 performs comparably to a more advanced single-agent model and improves as more supporting agents are added.</li>\n</ul>"}, "publishedAt": "2026-02-04T10:05:12.000Z", "title": "WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning", "summary": "Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04634.png", "numComments": 2, "submittedBy": {"_id": "653a5b0f7c01c693a16dd184", "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg", "fullname": "Zelai Xu", "name": "zelaix", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "689ea978824b212c988bc8f5", "name": "RLinf", "fullname": "RLinf", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/689ea8a1a73ecc6940dbba3d/T2RGCw18z6lYP1WfkIGJ3.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.04145", "authors": [{"_id": "698412bbe34659da7e1f4e04", "user": {"_id": "64fc20d899123d7698a30e61", "avatarUrl": "/avatars/9231982cf70a0689f50accedf1004702.svg", "isPro": false, "fullname": "Jinyuan Li", "user": "jinyuan222", "type": "user"}, "name": "Jinyuan Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:57.944Z", "hidden": false}, {"_id": "698412bbe34659da7e1f4e05", "name": "Chengsong Huang", "hidden": false}, {"_id": "698412bbe34659da7e1f4e06", "name": "Langlin Huang", "hidden": false}, {"_id": "698412bbe34659da7e1f4e07", "name": "Shaoyang Xu", "hidden": false}, {"_id": "698412bbe34659da7e1f4e08", "name": "Haolin Liu", "hidden": false}, {"_id": "698412bbe34659da7e1f4e09", "name": "Wenxuan Zhang", "hidden": false}, {"_id": "698412bbe34659da7e1f4e0a", "name": "Jiaxin Huang", "hidden": false}], "publishedAt": "2026-02-04T02:27:38.000Z", "submittedOnDailyAt": "2026-02-05T01:21:27.343Z", "title": "Training Data Efficiency in Multimodal Process Reward Models", "submittedOnDailyBy": {"_id": "65e02d89574e5aa0e9ce3efa", "avatarUrl": "/avatars/2ab152a10b21d81fb1defc726b8e951a.svg", "isPro": false, "fullname": "Langlin Huang", "user": "shrango", "type": "user"}, "summary": "Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training.Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora.To explain this, we formalize a theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench, BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by a relative 4.1%.", "upvotes": 70, "discussionId": "698412bbe34659da7e1f4e0b", "ai_summary": "Training multimodal process reward models efficiently through balanced-information scoring that prioritizes label mixture and reliability while achieving full-data performance with only 10% of training data.", "ai_keywords": ["Multimodal Process Reward Models", "Monte Carlo-annotated corpora", "VisualProcessBench", "Balanced-Information Score", "label mixtures", "label reliability", "gradient updates", "data efficiency"], "summary_zh": "<ul>\n    <li>\u591a\u6a21\u6001\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08MPRMs\uff09\u5bf9\u89c6\u89c9\u63a8\u7406\u7684\u76d1\u7763\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u8bad\u7ec3\u6210\u672c\u9ad8\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0cMPRM\u8bad\u7ec3\u5728\u968f\u673a\u62bd\u6837\u6570\u636e\u65f6\u5f88\u5feb\u5c31\u8fbe\u5230\u9971\u548c\uff0c\u8868\u660e\u73b0\u6709\u6570\u636e\u4e2d\u5b58\u5728\u5927\u91cf\u5197\u4f59\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6846\u67b6\uff0c\u89e3\u91ca\u4e86\u6709\u6548\u68af\u5ea6\u66f4\u65b0\u4f9d\u8d56\u4e8e\u6b63\u8d1f\u6b65\u9aa4\u7684\u6807\u7b7e\u7ec4\u5408\u548c\u6807\u7b7e\u7684\u53ef\u9760\u6027\u3002</li>\n    <li>\u6839\u636e\u8fd9\u4e9b\u89c1\u89e3\uff0c\u63d0\u51fa\u4e86\u5e73\u8861\u4fe1\u606f\u8bc4\u5206\uff08BIS\uff09\uff0c\u4f18\u5148\u8003\u8651\u6b63\u8d1f\u6807\u7b7e\u7ec4\u5408\u548c\u53ef\u9760\u6027\uff0c\u65e0\u9700\u989d\u5916\u6210\u672c\u3002</li>\n    <li>\u5728\u4e24\u4e2a\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cBIS\u9009\u62e9\u7684\u6570\u636e\u5b50\u96c6\u5728\u53ea\u4f7f\u752810%\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u8fbe\u5230\u5168\u6570\u636e\u7684\u6c34\u5e73\uff0c\u76f8\u5bf9\u968f\u673a\u62bd\u6837\u63d0\u9ad8\u4e864.1%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multimodal Process Reward Models (MPRMs) help with visual reasoning in machine learning but require a lot of training data.</li>\n    <li>Training MPRMs can be costly because it usually needs large-scale data with Monte Carlo (MC) annotations.</li>\n    <li>The study found that MPRM training quickly reaches a limit when using random samples, showing that there is a lot of unnecessary data in the annotated corpora.</li>\n    <li>The researchers introduced the Balanced-Information Score (BIS) to select data more effectively based on the quality and mixture of labels, without extra costs.</li>\n    <li>Using the BIS approach, the models achieved the same or better performance with only 10% of the training data, which is 4.1% better than using random samples.</li>\n</ul>"}, "publishedAt": "2026-02-03T21:27:38.000Z", "title": "Training Data Efficiency in Multimodal Process Reward Models", "summary": "Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training.Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora.To explain this, we formalize a theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench, BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by a relative 4.1%.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04145.png", "numComments": 1, "submittedBy": {"_id": "65e02d89574e5aa0e9ce3efa", "avatarUrl": "/avatars/2ab152a10b21d81fb1defc726b8e951a.svg", "fullname": "Langlin Huang", "name": "shrango", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.05386", "authors": [{"_id": "698599b14ad556f294b7ecdc", "name": "Zhenxiong Yu", "hidden": false}, {"_id": "698599b14ad556f294b7ecdd", "user": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "name": "Zhi Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-06T18:50:37.407Z", "hidden": false}, {"_id": "698599b14ad556f294b7ecde", "name": "Zhiheng Jin", "hidden": false}, {"_id": "698599b14ad556f294b7ecdf", "name": "Shuhe Wang", "hidden": false}, {"_id": "698599b14ad556f294b7ece0", "name": "Heng Zhang", "hidden": false}, {"_id": "698599b14ad556f294b7ece1", "name": "Yanlin Fei", "hidden": false}, {"_id": "698599b14ad556f294b7ece2", "name": "Lingfeng Zeng", "hidden": false}, {"_id": "698599b14ad556f294b7ece3", "name": "Fangqi Lou", "hidden": false}, {"_id": "698599b14ad556f294b7ece4", "name": "Shuo Zhang", "hidden": false}, {"_id": "698599b14ad556f294b7ece5", "name": "Tu Hu", "hidden": false}, {"_id": "698599b14ad556f294b7ece6", "name": "Jingping Liu", "hidden": false}, {"_id": "698599b14ad556f294b7ece7", "name": "Rongze Chen", "hidden": false}, {"_id": "698599b14ad556f294b7ece8", "name": "Xingyu Zhu", "hidden": false}, {"_id": "698599b14ad556f294b7ece9", "name": "Kunyi Wang", "hidden": false}, {"_id": "698599b14ad556f294b7ecea", "name": "Chaofa Yuan", "hidden": false}, {"_id": "698599b14ad556f294b7eceb", "name": "Xin Guo", "hidden": false}, {"_id": "698599b14ad556f294b7ecec", "name": "Zhaowei Liu", "hidden": false}, {"_id": "698599b14ad556f294b7eced", "name": "Feipeng Zhang", "hidden": false}, {"_id": "698599b14ad556f294b7ecee", "name": "Jie Huang", "hidden": false}, {"_id": "698599b14ad556f294b7ecef", "name": "Huacan Wang", "hidden": false}, {"_id": "698599b14ad556f294b7ecf0", "name": "Ronghao Chen", "hidden": false}, {"_id": "698599b14ad556f294b7ecf1", "name": "Liwen Zhang", "hidden": false}], "publishedAt": "2026-02-05T07:11:05.000Z", "submittedOnDailyAt": "2026-02-06T05:15:48.526Z", "title": "Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening", "submittedOnDailyBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "summary": "As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S^2Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\\%.", "upvotes": 56, "discussionId": "698599b14ad556f294b7ecf2", "githubRepo": "https://github.com/aifinlab/Spider-Sense", "githubRepoAddedBy": "user", "ai_summary": "Spider-Sense framework provides intrinsic and selective agent security through event-driven defense with intrinsic risk sensing, achieving low attack success and false positive rates with minimal latency overhead.", "ai_keywords": ["large language models", "autonomous agents", "security challenges", "mandatory checking paradigm", "event-driven defense", "Intrinsic Risk Sensing", "hierarchical defense mechanism", "lightweight similarity matching", "deep internal reasoning", "lifecycle-aware benchmark", "Attack Success Rate", "False Positive Rate"], "githubStars": 9, "organization": {"_id": "696875114bc2a5524dd8fcb7", "name": "AIFin-Lab", "fullname": "AIFin Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/69670d031053fc18e0ac011e/58oVnjWlRuiLy6X-GsMl6.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53d8\u6210\u81ea\u4e3b\u4ee3\u7406\uff0c\u5b9e\u9645\u5e94\u7528\u589e\u52a0\uff0c\u4f46\u4e5f\u9762\u4e34\u65b0\u7684\u5b89\u5168\u6311\u6218\u3002</li>\n    <li>\u73b0\u6709\u7684\u4ee3\u7406\u9632\u5fa1\u673a\u5236\u591a\u91c7\u7528\u5f3a\u5236\u68c0\u67e5\u7684\u65b9\u6cd5\uff0c\u5728\u9884\u5b9a\u4e49\u9636\u6bb5\u8fdb\u884c\u5b89\u5168\u9a8c\u8bc1\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Spider-Sense\u6846\u67b6\uff0c\u57fa\u4e8e\u5185\u5728\u98ce\u9669\u611f\u77e5\uff08IRS\uff09\uff0c\u4f7f\u4ee3\u7406\u5728\u611f\u77e5\u98ce\u9669\u65f6\u624d\u89e6\u53d1\u9632\u5fa1\u3002</li>\n    <li>Spider-Sense\u4f7f\u7528\u5c42\u7ea7\u9632\u5fa1\u673a\u5236\uff0c\u5e73\u8861\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u8f7b\u91cf\u5339\u914d\u5df2\u77e5\u6a21\u5f0f\uff0c\u590d\u6742\u60c5\u51b5\u5219\u8fdb\u884c\u6df1\u5165\u63a8\u7406\u3002</li>\n    <li>\u901a\u8fc7S^2Bench\u57fa\u51c6\u6d4b\u8bd5\u8fdb\u884c\u4e25\u683c\u8bc4\u4f30\uff0cSpider-Sense\u5728\u653b\u51fb\u6210\u529f\u7387\u548c\u8bef\u62a5\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5ef6\u8fdf\u4ec5\u4e3a8.3%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) are becoming more autonomous, which brings new security challenges.</li>\n    <li>Current defense methods often check for security at fixed points, but the authors suggest a better approach.</li>\n    <li>The proposed Spider-Sense framework uses Intrinsic Risk Sensing (IRS) to detect risks and trigger defenses only when needed.</li>\n    <li>Spider-Sense balances efficiency and precision by quickly handling known threats and using deeper analysis for uncertain cases.</li>\n    <li>Tests show that Spider-Sense performs well, achieving low rates of attacks and false alarms with minimal delay.</li>\n</ul>"}, "publishedAt": "2026-02-05T02:11:05.000Z", "title": "Spider-Sense: Intrinsic Risk Sensing for Efficient Agent Defense with Hierarchical Adaptive Screening", "summary": "As large language models (LLMs) evolve into autonomous agents, their real-world applicability has expanded significantly, accompanied by new security challenges. Most existing agent defense mechanisms adopt a mandatory checking paradigm, in which security validation is forcibly triggered at predefined stages of the agent lifecycle. In this work, we argue that effective agent security should be intrinsic and selective rather than architecturally decoupled and mandatory. We propose Spider-Sense framework, an event-driven defense framework based on Intrinsic Risk Sensing (IRS), which allows agents to maintain latent vigilance and trigger defenses only upon risk perception. Once triggered, the Spider-Sense invokes a hierarchical defence mechanism that trades off efficiency and precision: it resolves known patterns via lightweight similarity matching while escalating ambiguous cases to deep internal reasoning, thereby eliminating reliance on external models. To facilitate rigorous evaluation, we introduce S^2Bench, a lifecycle-aware benchmark featuring realistic tool execution and multi-stage attacks. Extensive experiments demonstrate that Spider-Sense achieves competitive or superior defense performance, attaining the lowest Attack Success Rate (ASR) and False Positive Rate (FPR), with only a marginal latency overhead of 8.3\\%.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05386.png", "numComments": 3, "submittedBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "fullname": "Zhi Yang", "name": "yangzhi1", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "696875114bc2a5524dd8fcb7", "name": "AIFin-Lab", "fullname": "AIFin Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/69670d031053fc18e0ac011e/58oVnjWlRuiLy6X-GsMl6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.06717", "authors": [{"_id": "69898989beecc443208d2741", "name": "Daniil Plyusov", "hidden": false}, {"_id": "69898989beecc443208d2742", "user": {"_id": "62897fce5d9e25c10e4f319d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg", "isPro": false, "fullname": "Alexey Gorbatovski", "user": "Myashka", "type": "user"}, "name": "Alexey Gorbatovski", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:27:53.815Z", "hidden": false}, {"_id": "69898989beecc443208d2743", "name": "Boris Shaposhnikov", "hidden": false}, {"_id": "69898989beecc443208d2744", "name": "Viacheslav Sinii", "hidden": false}, {"_id": "69898989beecc443208d2745", "user": {"_id": "636e71b2b0ebc04888157b71", "avatarUrl": "/avatars/957ba705d470e3a01792741d7f0ff038.svg", "isPro": false, "fullname": "Alexey Malakhov", "user": "ZeL1k7", "type": "user"}, "name": "Alexey Malakhov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T21:06:45.653Z", "hidden": false}, {"_id": "69898989beecc443208d2746", "user": {"_id": "62a9c8edc19f92ae443ab37f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a9c8edc19f92ae443ab37f/yczqpBOntLco_2Jn4hnT7.jpeg", "isPro": false, "fullname": "Daniil Gavrilov", "user": "kefirski", "type": "user"}, "name": "Daniil Gavrilov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:27:57.059Z", "hidden": false}], "publishedAt": "2026-02-06T14:07:30.000Z", "submittedOnDailyAt": "2026-02-09T04:48:51.744Z", "title": "F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare", "submittedOnDailyBy": {"_id": "62897fce5d9e25c10e4f319d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg", "isPro": false, "fullname": "Alexey Gorbatovski", "user": "Myashka", "type": "user"}, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 rightarrow 70.3 (GRPO), 69.3 rightarrow 72.5 (DAPO), and 73.2 rightarrow 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.", "upvotes": 54, "discussionId": "69898989beecc443208d2747", "ai_summary": "RLVR methods using group sampling suffer from bias toward likely trajectories and missed rare-correct ones; a difficulty-aware advantage scaling technique improves performance on benchmarks without increasing computational cost.", "ai_keywords": ["reinforcement learning", "verifiable rewards", "group sampling", "advantage estimation", "policy updates", "Focal loss", "GRPO", "DAPO", "CISPO", "pass@k metrics"], "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u4f9d\u8d56\u4e8e\u5206\u7ec4\u91c7\u6837\u6765\u4f30\u8ba1\u4f18\u52bf\u548c\u7a33\u5b9a\u7b56\u7565\u66f4\u65b0\u3002</li>\n    <li>\u5927\u7ec4\u89c4\u6a21\u7531\u4e8e\u8ba1\u7b97\u9650\u5236\u4e0d\u53ef\u884c\uff0c\u5bfc\u81f4\u5b66\u4e60\u504f\u5411\u4e8e\u5df2\u7ecf\u53ef\u80fd\u7684\u8f68\u8ff9\u3002</li>\n    <li>\u5c0f\u7ec4\u5e38\u5e38\u9519\u8fc7\u7a00\u6709\u6b63\u786e\u8f68\u8ff9\uff0c\u5e76\u96c6\u4e2d\u5728\u5e38\u89c1\u89e3\u51b3\u65b9\u6848\u4e0a\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96be\u5ea6\u7684\u4f18\u52bf\u7f29\u653e\u7cfb\u6570\uff0c\u65e8\u5728\u51cf\u5c11\u9ad8\u6210\u529f\u63d0\u793a\u7684\u66f4\u65b0\u6743\u91cd\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u540c\u65f6\u4e0d\u589e\u52a0\u7ec4\u89c4\u6a21\u6216\u8ba1\u7b97\u6210\u672c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement Learning with Verifiable Rewards (RLVR) uses group sampling to improve learning but faces challenges with group size and computational limits.</li>\n    <li>Large groups are hard to use, leading to biased learning towards common solutions, while smaller groups may miss rare but correct options.</li>\n    <li>The study shows how the probability of missing rare-correct options changes with group size and how updates affect the distribution of correct results.</li>\n    <li>To improve learning, a new method called difficulty-aware advantage scaling is proposed, which reduces the weight of updates on high-success prompts.</li>\n    <li>This method, tested on various benchmarks, significantly improved performance without increasing the size of groups or computational costs.</li>\n</ul>"}, "publishedAt": "2026-02-06T09:07:30.000Z", "title": "F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 rightarrow 70.3 (GRPO), 69.3 rightarrow 72.5 (DAPO), and 73.2 rightarrow 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06717.png", "numComments": 1, "submittedBy": {"_id": "62897fce5d9e25c10e4f319d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62897fce5d9e25c10e4f319d/bMlfAyzkNNZlkQ5mCW6Vc.jpeg", "fullname": "Alexey Gorbatovski", "name": "Myashka", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.06570", "authors": [{"_id": "69895518beecc443208d2680", "name": "Baichuan-M3 Team", "hidden": false}, {"_id": "69895518beecc443208d2682", "name": "Chengfeng Dou", "hidden": false}, {"_id": "69895518beecc443208d2683", "user": {"_id": "641c45c921964f8f6d451d16", "avatarUrl": "/avatars/da06cc603f8f9ee46ddb7dc72aae5bec.svg", "isPro": false, "fullname": "FanYang", "user": "fairyang", "type": "user"}, "name": "Fan Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T14:32:20.075Z", "hidden": false}, {"_id": "69895518beecc443208d2684", "user": {"_id": "6464dd5234acce85aea186c7", "avatarUrl": "/avatars/3428029b0ae5f12885092c7aea588065.svg", "isPro": false, "fullname": "lifei", "user": "lifei926926", "type": "user"}, "name": "Fei Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T21:07:14.055Z", "hidden": false}, {"_id": "69895518beecc443208d2685", "name": "Jiyuan Jia", "hidden": false}, {"_id": "69895518beecc443208d2686", "name": "Qiang Ju", "hidden": false}, {"_id": "69895518beecc443208d2687", "name": "Shuai Wang", "hidden": false}, {"_id": "69895518beecc443208d2688", "name": "Tianpeng Li", "hidden": false}, {"_id": "69895518beecc443208d2689", "name": "Xiangrong Zeng", "hidden": false}, {"_id": "69895518beecc443208d268a", "name": "Yijie Zhou", "hidden": false}, {"_id": "69895518beecc443208d268b", "name": "Hongda Zhang", "hidden": false}, {"_id": "69895518beecc443208d268c", "name": "Jinyang Tai", "hidden": false}, {"_id": "69895518beecc443208d268d", "name": "Linzhuang Sun", "hidden": false}, {"_id": "69895518beecc443208d268e", "user": {"_id": "6487e2e1eec01aee99cf4c10", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6487e2e1eec01aee99cf4c10/1U6zZ2OaUOrR1ueD5yraR.jpeg", "isPro": false, "fullname": "Peidong Guo", "user": "GuoPD", "type": "user"}, "name": "Peidong Guo", "status": "claimed_verified", "statusLastChangedAt": "2026-02-09T08:36:44.605Z", "hidden": false}, {"_id": "69895518beecc443208d268f", "name": "Yichuan Mo", "hidden": false}, {"_id": "69895518beecc443208d2690", "name": "Xiaochuan Wang", "hidden": false}, {"_id": "69895518beecc443208d2691", "name": "Hengfu Cui", "hidden": false}, {"_id": "69895518beecc443208d2692", "name": "Zhishou Zhang", "hidden": false}], "publishedAt": "2026-02-06T10:08:59.000Z", "submittedOnDailyAt": "2026-02-09T03:00:35.501Z", "title": "Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making", "submittedOnDailyBy": {"_id": "641c45c921964f8f6d451d16", "avatarUrl": "/avatars/da06cc603f8f9ee46ddb7dc72aae5bec.svg", "isPro": false, "fullname": "FanYang", "user": "fairyang", "type": "user"}, "summary": "We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.", "upvotes": 54, "discussionId": "69895518beecc443208d2693", "githubRepo": "https://github.com/baichuan-inc/Baichuan-M3-235B", "githubRepoAddedBy": "user", "ai_summary": "Baichuan-M3 is a medical-enhanced large language model designed for clinical decision support with capabilities in proactive information gathering, long-horizon reasoning, and hallucination suppression.", "ai_keywords": ["large language model", "clinical decision support", "proactive information acquisition", "long-horizon reasoning", "hallucination suppression", "HealthBench", "HealthBench-Hallu", "ScanBench"], "githubStars": 190, "organization": {"_id": "648457d38cf0b32b0ba0a913", "name": "baichuan-inc", "fullname": "Baichuan Intelligent Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd3e0c364a086c6322ad2/acwcllU0PQz4Bg3gchhYo.png"}, "summary_zh": "<ul>\n    <li>Baichuan-M3 \u662f\u4e00\u79cd\u533b\u5b66\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u4f9b\u4e3b\u52a8\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u7279\u6b8a\u7684\u8bad\u7ec3\u6d41\u7a0b\u6a21\u62df\u533b\u751f\u7684\u7cfb\u7edf\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4ee5\u89e3\u51b3\u5f00\u653e\u5f0f\u54a8\u8be2\u4e2d\u7684\u5c40\u9650\u6027\u3002</li>\n    <li>\u4e3b\u8981\u529f\u80fd\u5305\u62ec\u4e3b\u52a8\u83b7\u53d6\u4fe1\u606f\u3001\u957f\u8fdc\u63a8\u7406\u4ee5\u53ca\u9002\u5e94\u6027\u5e7b\u89c9\u6291\u5236\uff0c\u4ee5\u786e\u4fdd\u4fe1\u606f\u7684\u53ef\u9760\u6027\u3002</li>\n    <li>\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cBaichuan-M3 \u5728 HealthBench \u548c\u5176\u4ed6\u65b0\u5f15\u5165\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u8d85\u8d8a\u4e86 GPT-5.2\u3002</li>\n    <li>\u6a21\u578b\u53ef\u901a\u8fc7 https://huggingface.co/collections/baichuan-inc/baichuan-m3 \u516c\u5f00\u83b7\u53d6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Baichuan-M3 is a new large language model designed to improve medical decision-making rather than just answering questions.</li>\n    <li>It addresses issues in current systems by mimicking how doctors work during patient consultations.</li>\n    <li>Key features include gathering information proactively, reasoning over long periods, and reducing false information.</li>\n    <li>Tests show Baichuan-M3 performs better than GPT-5.2 in clinical tasks and safety assessments.</li>\n    <li>The model is available for public use at a specified website.</li>\n</ul>"}, "publishedAt": "2026-02-06T05:08:59.000Z", "title": "Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making", "summary": "We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06570.png", "numComments": 2, "submittedBy": {"_id": "641c45c921964f8f6d451d16", "avatarUrl": "/avatars/da06cc603f8f9ee46ddb7dc72aae5bec.svg", "fullname": "FanYang", "name": "fairyang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "648457d38cf0b32b0ba0a913", "name": "baichuan-inc", "fullname": "Baichuan Intelligent Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd3e0c364a086c6322ad2/acwcllU0PQz4Bg3gchhYo.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.06855", "authors": [{"_id": "698b0ed21b2dc6b37d61b3d0", "name": "Alisia Lupidi", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3d1", "name": "Bhavul Gauri", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3d2", "name": "Thomas Simon Foster", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3d3", "name": "Bassel Al Omari", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3d4", "name": "Despoina Magka", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3d5", "name": "Alberto Pepe", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3d6", "name": "Alexis Audran-Reiss", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3d7", "name": "Muna Aghamelu", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3d8", "name": "Nicolas Baldwin", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3d9", "name": "Lucia Cipolina-Kun", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3da", "name": "Jean-Christophe Gagnon-Audet", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3db", "name": "Chee Hau Leow", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3dc", "name": "Sandra Lefdal", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3dd", "name": "Hossam Mossalam", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3de", "name": "Abhinav Moudgil", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3df", "name": "Saba Nazir", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e0", "name": "Emanuel Tewolde", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e1", "name": "Isabel Urrego", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e2", "name": "Jordi Armengol Estape", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e3", "name": "Amar Budhiraja", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e4", "name": "Gaurav Chaurasia", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e5", "name": "Abhishek Charnalia", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e6", "name": "Derek Dunfield", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e7", "name": "Karen Hambardzumyan", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e8", "name": "Daniel Izcovich", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3e9", "name": "Martin Josifoski", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3ea", "name": "Ishita Mediratta", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3eb", "name": "Kelvin Niu", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3ec", "name": "Parth Pathak", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3ed", "name": "Michael Shvartsman", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3ee", "name": "Edan Toledo", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3ef", "name": "Anton Protopopov", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3f0", "name": "Roberta Raileanu", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3f1", "name": "Alexander Miller", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3f2", "name": "Tatiana Shavrina", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3f3", "name": "Jakob Foerster", "hidden": false}, {"_id": "698b0ed21b2dc6b37d61b3f4", "name": "Yoram Bachrach", "hidden": false}], "publishedAt": "2026-02-06T16:45:02.000Z", "submittedOnDailyAt": "2026-02-10T08:48:50.684Z", "title": "AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents", "submittedOnDailyBy": {"_id": "60720704227ff331937110f4", "avatarUrl": "/avatars/8010bfb98256c138049aa3d237737b37.svg", "isPro": false, "fullname": "Bhavul Gauri", "user": "bhavul", "type": "user"}, "summary": "LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.", "upvotes": 54, "discussionId": "698b0ed31b2dc6b37d61b3f5", "githubRepo": "https://github.com/facebookresearch/airs-bench", "githubRepoAddedBy": "user", "ai_summary": "AIRS-Bench presents a comprehensive benchmark suite for evaluating LLM agents across diverse scientific domains, demonstrating current limitations while providing open-source resources for advancing autonomous scientific research.", "ai_keywords": ["AI Research Science Benchmark", "agentic capabilities", "research lifecycle", "sequential scaffolds", "parallel scaffolds", "autonomous scientific research"], "githubStars": 16, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86AIRS-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b20\u4e2a\u4efb\u52a1\u7684AI\u7814\u7a76\u79d1\u5b66\u57fa\u51c6\uff0c\u65e8\u5728\u63a8\u52a8\u79d1\u5b66\u7814\u7a76\u7684\u53d1\u5c55\u3002</li>\n    <li>\u8fd9\u4e9b\u4efb\u52a1\u6db5\u76d6\u591a\u4e2a\u9886\u57df\uff0c\u5305\u62ec\u8bed\u8a00\u6a21\u578b\u3001\u6570\u5b66\u3001\u751f\u7269\u4fe1\u606f\u5b66\u548c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002</li>\n    <li>AIRS-Bench\u4efb\u52a1\u8bc4\u4f30\u7814\u7a76\u4ee3\u7406\u5728\u6574\u4e2a\u7814\u7a76\u751f\u547d\u5468\u671f\u4e2d\u7684\u80fd\u529b\uff0c\u5305\u62ec\u521b\u610f\u751f\u6210\u548c\u5b9e\u9a8c\u5206\u6790\u3002</li>\n    <li>\u7ed3\u679c\u663e\u793a\uff0c\u4ee3\u7406\u5728\u56db\u4e2a\u4efb\u52a1\u4e2d\u8d85\u8fc7\u4e86\u4eba\u7c7b\u7684\u6700\u4f73\u8868\u73b0\uff0c\u4f46\u5728\u5176\u4ed6\u5341\u516d\u4e2a\u4efb\u52a1\u4e2d\u672a\u80fd\u8fbe\u5230\u3002</li>\n    <li>AIRS-Bench\u4efb\u52a1\u5b9a\u4e49\u548c\u8bc4\u4f30\u4ee3\u7801\u5df2\u5f00\u6e90\uff0c\u4ee5\u4fc3\u8fdb\u81ea\u4e3b\u79d1\u5b66\u7814\u7a76\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>AIRS-Bench is a new tool for improving scientific research using AI, featuring 20 tasks from top machine learning studies.</li>\n    <li>The tasks cover various areas like language processing, math, bioinformatics, and analyzing time series data.</li>\n    <li>It evaluates AI agents throughout the entire research process, including generating ideas and refining experiments, without giving them any base code.</li>\n    <li>Results show that AI agents perform better than humans in four tasks but fall short in sixteen tasks, indicating there's still much room for growth.</li>\n    <li>The AIRS-Bench task details and evaluation tools are open-sourced to encourage further advancements in autonomous research.</li>\n</ul>"}, "publishedAt": "2026-02-06T11:45:02.000Z", "title": "AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents", "summary": "LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.06855.png", "numComments": 1, "submittedBy": {"_id": "60720704227ff331937110f4", "avatarUrl": "/avatars/8010bfb98256c138049aa3d237737b37.svg", "fullname": "Bhavul Gauri", "name": "bhavul", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2602.04705", "authors": [{"_id": "698424a7e34659da7e1f4e6f", "name": "Haifeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e70", "name": "Hua Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e71", "name": "Tian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e72", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4e73", "name": "Jing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e74", "name": "Dianhai Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e75", "name": "Yanjun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4e76", "name": "Jingzhou He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e77", "name": "Zhongjun He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e78", "name": "Dou Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e79", "name": "Qiwen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7a", "name": "Shuohuan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7b", "user": {"_id": "62cd9632342b1d5dab8df4c3", "avatarUrl": "/avatars/9080d20bb57a05a1eeb6800eba886cf9.svg", "isPro": false, "fullname": "Junyuan Shang", "user": "sjy1203", "type": "user"}, "name": "Junyuan Shang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:28.482Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7c", "user": {"_id": "67f37f78b36e82d366dedeec", "avatarUrl": "/avatars/678bb5891d5c2e80edc0799d2308a5d3.svg", "isPro": false, "fullname": "Max Zhenyu Zhang", "user": "max-zhenyu-zhang", "type": "user"}, "name": "Zhenyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:03.972Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7d", "name": "Yuchen Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7e", "name": "Jinle Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7f", "name": "Jiabin Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e80", "name": "Liang Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e81", "name": "Ruibiao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e82", "name": "Weichong Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4e83", "name": "Siyu Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e84", "name": "Dai Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4e85", "name": "Shikun Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e86", "name": "Siqi Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e87", "name": "Bolei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e88", "name": "Yan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e89", "name": "Zhenyu Jiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8a", "name": "Ruiqing Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8b", "name": "Zeyu Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8c", "name": "Qingqing Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8d", "name": "Kaipeng Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8e", "name": "Jiajun Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8f", "name": "Enlei Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e90", "name": "Guoxia Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e91", "name": "Yanlin Sha", "hidden": false}, {"_id": "698424a7e34659da7e1f4e92", "name": "Yi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e93", "name": "Yehan Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e94", "name": "Weijian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e95", "name": "Jiaxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e96", "name": "Zengfeng Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e97", "name": "Yingqi Qu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e98", "name": "Zhongli Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4e99", "name": "Zhengkun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9a", "name": "Xiyang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9b", "name": "Zixiang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9c", "name": "Xinchao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9d", "name": "Zhengjie Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9e", "name": "Dong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9f", "name": "Bingjin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea0", "name": "Yue Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea1", "name": "Xing Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea2", "name": "Shiwei Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea3", "name": "Qiao Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea4", "name": "Xinzhe Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea5", "name": "Shuangshuang Qiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea6", "name": "Baoshan Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea7", "name": "Bihong Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea8", "name": "Bin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea9", "name": "Bingquan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaa", "name": "Binhan Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eab", "name": "Binxiong Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4eac", "name": "Bo Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4ead", "name": "Bo Ke", "hidden": false}, {"_id": "698424a7e34659da7e1f4eae", "name": "Bo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaf", "name": "Bowen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb0", "name": "Boyan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb1", "name": "Boyang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb2", "name": "Caiji Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb3", "name": "Can Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb4", "name": "Chang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb5", "name": "Chao Pang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb6", "name": "Chao Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb7", "name": "Chaoyi Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb8", "name": "Chen Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb9", "name": "Cheng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4eba", "name": "Chenlin Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebb", "name": "Chun Gan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebc", "name": "Chunguang Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebd", "name": "Chuyu Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebe", "name": "Cuiyun Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebf", "name": "Dan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec0", "name": "Danlei Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec1", "name": "Danxiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec2", "name": "Dong Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec3", "name": "Dongbo Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec4", "name": "Dongdong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec5", "name": "Dongdong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec6", "name": "Dongxue Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec7", "name": "Fan Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec8", "name": "Fan Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec9", "name": "Fan Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eca", "name": "Fan Mo", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecb", "name": "Feisheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecc", "name": "Fengwei Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecd", "name": "Gangqiang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ece", "name": "Gaofeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecf", "name": "Gaopeng Yong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed0", "name": "Gexiao Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed1", "user": {"_id": "698419de94015f1e5eedacec", "avatarUrl": "/avatars/e80baa6f9efcd5e5d7cc9b93ac852c7b.svg", "isPro": false, "fullname": "Guan Wang", "user": "guanwcn", "type": "user"}, "name": "Guan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:38.213Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed2", "name": "Guangchen Ni", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed3", "name": "Guangshuo Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed4", "name": "Guanzhong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed5", "user": {"_id": "609cd5ab335f23cd2fa0f211", "avatarUrl": "/avatars/8331a7025a6aa4eabc5b6502bf8a0a63.svg", "isPro": false, "fullname": "Guihua Liu", "user": "LLLL", "type": "user"}, "name": "Guihua Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:31.029Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed6", "name": "Guishun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed7", "name": "Haibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed8", "name": "Haijian Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed9", "name": "Haipeng Ming", "hidden": false}, {"_id": "698424a7e34659da7e1f4eda", "name": "Haisu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4edb", "name": "Haiyang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4edc", "name": "Haiye Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4edd", "name": "Han Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ede", "name": "Hangting Lou", "hidden": false}, {"_id": "698424a7e34659da7e1f4edf", "name": "Hanwen Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee0", "name": "Hanzhi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee1", "name": "Hao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee2", "name": "Hao Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee3", "name": "Hao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee4", "name": "Hao Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee5", "name": "Haochen Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee6", "name": "Haodong Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee7", "name": "Haoshuang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee8", "name": "Haozhe Geng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee9", "name": "Heju Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4eea", "name": "Hong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eeb", "name": "Hongchen Xue", "hidden": false}, {"_id": "698424a7e34659da7e1f4eec", "name": "Hongen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eed", "name": "Honggeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eee", "name": "Hongji Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eef", "name": "Hongwei Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef0", "name": "Hongyang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef1", "name": "Hongyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef2", "name": "Hua Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef3", "name": "Huan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef4", "name": "Huan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef5", "name": "Huang He", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef6", "name": "Hui Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef7", "name": "Hui Zhong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef8", "name": "Huibin Ruan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef9", "name": "Jiafeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efa", "name": "Jiage Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efb", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efc", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efd", "name": "Jiajie Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efe", "name": "Jialin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eff", "name": "Jian Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f00", "name": "Jian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f01", "name": "Jianfeng Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f02", "name": "Jianguang Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f03", "name": "Jianhua Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f04", "name": "Jianye Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f05", "name": "Jiaodi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f06", "name": "Jiarui Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f07", "name": "Jiawei Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f08", "name": "Jiaxin Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f09", "name": "Jiaxuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0a", "name": "Jie Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0b", "name": "Jie Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0c", "name": "Jiefan Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0d", "name": "Jihan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0e", "name": "Jihua Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0f", "name": "Jing Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f10", "name": "Jing Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f11", "name": "Jing Yan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f12", "name": "Jingdong Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f13", "name": "Jingdong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f14", "name": "Jingjing Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f15", "name": "Jingyong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f16", "name": "Jinheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f17", "name": "Jinjin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f18", "name": "Jinliang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f19", "name": "Jinlin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1a", "name": "Jinnan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1b", "name": "Jixiang Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1c", "name": "Jiyi Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1d", "name": "Jiyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1e", "name": "Jun Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1f", "name": "Jun Xia", "hidden": false}, {"_id": "698424a7e34659da7e1f4f20", "name": "Jun Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f21", "name": "Junda Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f22", "name": "Junhao Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f23", "name": "Junhong Xiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f24", "name": "Junliang Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f25", "name": "Kai Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f26", "name": "Kailun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f27", "name": "Kairan Su", "hidden": false}, {"_id": "698424a7e34659da7e1f4f28", "name": "Kang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f29", "name": "Kangkang Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2a", "name": "Ke Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2b", "name": "Ke Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2c", "name": "Kui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2d", "name": "Kun Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2e", "name": "Kunbin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2f", "name": "Lei Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f30", "name": "Lei Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f31", "name": "Lei Wen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f32", "name": "Linghui Meng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f33", "user": {"_id": "641e69355c348064a8251471", "avatarUrl": "/avatars/acad3877df27ff44ea3921bb43e34d53.svg", "isPro": false, "fullname": "Linhao Yu", "user": "HasuerYu", "type": "user"}, "name": "Linhao Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:47.812Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f34", "name": "Liping Ouyang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f35", "name": "Liwen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f36", "user": {"_id": "65cf859f88d13d8128bb8545", "avatarUrl": "/avatars/aa18b993bd90d9c8a95913050cd955a8.svg", "isPro": false, "fullname": "Longbin Ji", "user": "robingg1", "type": "user"}, "name": "Longbin Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:40.295Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f37", "name": "Longzhi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f38", "name": "Meng Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f39", "name": "Meng Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3a", "name": "Mengfei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3b", "name": "Mengqi Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3c", "name": "Mengyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3d", "name": "Ming Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3e", "name": "Mingcheng Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3f", "name": "Mingming Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f40", "name": "Mingxin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f41", "name": "Mingzhu Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f42", "name": "Naibin Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f43", "name": "Nemin Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f44", "name": "Nian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f45", "name": "Peng Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f46", "name": "Peng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f47", "name": "Pengyu Zou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f48", "name": "Qi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f49", "name": "Qi Xin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4a", "name": "Qian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4b", "name": "Qiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4c", "name": "Qianhui Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4d", "name": "Qianwei Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4e", "name": "Qianyue He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4f", "name": "Qifei Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f50", "name": "Qinrui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f51", "name": "Qiwen Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f52", "name": "Quan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f53", "name": "Quanxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f54", "name": "Qunyi Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4f55", "name": "Rongrui Zhan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f56", "name": "Rufeng Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f57", "name": "Rui Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f58", "name": "Ruian Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f59", "name": "Ruihao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5a", "name": "Ruijie Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5b", "name": "Ruixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5c", "name": "Ruixuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5d", "name": "Runsheng Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5e", "name": "Ruting Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5f", "name": "Senbo Kang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f60", "name": "Shan Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f61", "name": "Shaofei Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f62", "name": "Shaotian Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f63", "name": "Shenwei Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f64", "name": "Shifeng Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f65", "name": "Shihao Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f66", "name": "Shilong Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f67", "name": "Shiqin Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f68", "name": "Shiwei Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f69", "name": "Shixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6a", "name": "Shuai Yao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6b", "name": "Shuang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6c", "name": "Shuangqiao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6d", "name": "Shuhao Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6e", "name": "Shuwei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6f", "name": "Shuwen Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f70", "user": {"_id": "62769a608483d8e9ecd9b4f8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672799958233-62769a608483d8e9ecd9b4f8.jpeg", "isPro": false, "fullname": "Sijun He", "user": "sijunhe", "type": "user"}, "name": "Sijun He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:33.392Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f71", "user": {"_id": "64fada13d82fc6977d5e9c74", "avatarUrl": "/avatars/776bf1257154289e919716637770ef52.svg", "isPro": false, "fullname": "Siming Dai", "user": "DesmonDay", "type": "user"}, "name": "Siming Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:50.302Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f72", "name": "Siming Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f73", "name": "Siyi Long", "hidden": false}, {"_id": "698424a7e34659da7e1f4f74", "name": "Songhe Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f75", "name": "Suhui Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f76", "name": "Suyin Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f77", "name": "Teng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f78", "name": "Tianchan Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f79", "name": "Tianliang Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7a", "user": {"_id": "67bbe929593452cc18877606", "avatarUrl": "/avatars/f50fd1cb35d628c26cf21ad0c95c55b1.svg", "isPro": false, "fullname": "tmyangcs", "user": "youngtimmy", "type": "user"}, "name": "Tianmeng Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:36.143Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7b", "name": "Tianyi Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7c", "name": "Tiezhu Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7d", "name": "Ting Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7e", "name": "Ting Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7f", "name": "Tingdan Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f80", "name": "Wei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f81", "name": "Wei Luan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f82", "name": "Wei Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f83", "name": "Wei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f84", "name": "Wei Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f85", "name": "Weibao Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f86", "name": "Weibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f87", "name": "Weicheng Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f88", "name": "Weichong Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f89", "name": "Weiguo Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8a", "name": "Weilong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8b", "name": "Weiqi Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8c", "name": "Wen Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8d", "name": "Wenbin Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8e", "name": "Wenjing Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8f", "name": "Wenlong Miao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f90", "name": "Wenpei Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f91", "name": "Wenquan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f92", "name": "Xi Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f93", "name": "Xi Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f94", "name": "Xiang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f95", "name": "Xiangguo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f96", "name": "Xiangrui Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f97", "name": "Xiangsen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f98", "name": "Xiangzhe Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f99", "name": "Xianlong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9a", "name": "Xianying Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9b", "name": "Xiao Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9c", "name": "Xiaocong Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9d", "name": "Xiaofei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9e", "name": "Xiaofeng Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9f", "name": "Xiaofeng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa0", "name": "Xiaojian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa1", "name": "Xiaolan Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa2", "name": "Xiaopeng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa3", "name": "Xiaotian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa4", "name": "Xiaoxiong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa5", "name": "Xiaoxu Fei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa6", "name": "Xiaoxuan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa7", "user": {"_id": "664395621b88258a527cd7d1", "avatarUrl": "/avatars/8489ccebe4fd1262679ba63a5cb50bb8.svg", "isPro": false, "fullname": "Kira", "user": "Kira-wang", "type": "user"}, "name": "Xiaoyu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:25.774Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa8", "name": "Xiaoyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa9", "name": "Xin Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4faa", "name": "Xin Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fab", "name": "Xinhui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fac", "name": "Xinming Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fad", "name": "Xintong Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fae", "name": "Xinyi Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4faf", "name": "Xinyu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb0", "name": "Xiuxian Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb1", "name": "XuanShi Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb2", "name": "Xue Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb3", "name": "Xueying Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb4", "name": "Xuhong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb5", "name": "Xulong Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb6", "name": "Xuyi Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb7", "name": "Yabing Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb8", "name": "Yafeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb9", "name": "Yamei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fba", "name": "Yan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbb", "name": "Yanfu Cheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbc", "name": "Yang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbd", "name": "Yang Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbe", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbf", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc0", "name": "Yang Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc1", "name": "Yanlong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc2", "name": "Yannian Fu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc3", "name": "Yanpeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc4", "name": "Yanzheng Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc5", "name": "Yao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc6", "name": "Yaozong Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc7", "name": "Yaqian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc8", "name": "Yehua Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc9", "name": "Yekun Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fca", "name": "Yesong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcb", "name": "Yi Song", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcc", "name": "Yichen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcd", "name": "Yifei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fce", "name": "Yifeng Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcf", "name": "Yifeng Kou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd0", "name": "Yilong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd1", "name": "Yilong Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd2", "name": "Yiming Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd3", "name": "Ying Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd4", "name": "Ying Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd5", "name": "Yingsheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd6", "name": "Yingzhan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd7", "name": "Yinqi Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd8", "name": "Yiran Xing", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd9", "name": "Yishu Lei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fda", "name": "Yixiang Tu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdb", "name": "Yiyan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdc", "name": "Yong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdd", "name": "Yonghua Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fde", "name": "Yongqiang Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdf", "name": "Yongxing Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe0", "name": "Yongyue Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe1", "name": "Yu Ran", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe2", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe3", "name": "Yu-Wen Michael Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe4", "name": "Yuang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe5", "name": "Yuanle Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe6", "name": "Yuanyuan Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe7", "name": "Yubo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe8", "name": "Yuchen Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe9", "name": "Yucheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fea", "name": "Yude Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4feb", "name": "Yuedong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fec", "name": "Yuehu Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4fed", "name": "Yufeng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fee", "name": "Yuhui Cao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fef", "name": "Yuhui Yun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff0", "name": "Yukun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff1", "name": "Yukun Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff2", "name": "Yukun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff3", "name": "Yumeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff4", "name": "Yun Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff5", "name": "Yun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff6", "name": "Yunfei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff7", "name": "Yunshen Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff8", "name": "Yuping Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff9", "name": "Yuqin Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffa", "name": "Yuqing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffb", "name": "Yurui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffc", "name": "Yuwen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffd", "name": "Yuxiang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffe", "name": "Zefeng Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fff", "name": "Zelin Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5000", "name": "Zelun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5001", "name": "Zenan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f5002", "name": "Zezhao Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5003", "name": "Zhaowu Pan", "hidden": false}, {"_id": "698424a7e34659da7e1f5004", "name": "Zhaoyu Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f5005", "name": "Zhe Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5006", "name": "Zhe Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5007", "name": "Zhen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5008", "name": "Zhengfan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f5009", "name": "Zhengrui Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f500a", "name": "Zhengsheng Ning", "hidden": false}, {"_id": "698424a7e34659da7e1f500b", "name": "Zhenxing Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500c", "name": "Zhenyu Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500d", "name": "Zhenyu Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f500e", "name": "Zhenyun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500f", "name": "Zhi Li", "hidden": false}, {"_id": "698424a7e34659da7e1f5010", "name": "Zhichao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5011", "name": "Zhicheng Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5012", "name": "Zhida Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5013", "name": "Zhifan Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5014", "name": "Zhihao Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f5015", "name": "Zhijin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f5016", "name": "Zhiyang Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5017", "name": "Zhonghui Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f5018", "name": "Zhuangzhuang Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f5019", "name": "Zhujun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f501a", "name": "Zhuo Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f501b", "name": "Zichang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f501c", "name": "Zihan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f501d", "name": "Zihao Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f501e", "name": "Zihe Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f501f", "name": "Ziheng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5020", "name": "Ziping Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5021", "name": "Zixuan Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f5022", "name": "Ziyang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f5023", "name": "Ziyi Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f5024", "name": "Ziyuan Gao", "hidden": false}], "publishedAt": "2026-02-04T16:18:15.000Z", "submittedOnDailyAt": "2026-02-05T02:34:05.150Z", "title": "ERNIE 5.0 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "upvotes": 198, "discussionId": "698424a7e34659da7e1f5025", "ai_summary": "ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.", "ai_keywords": ["autoregressive foundation model", "unified multimodal understanding", "unified next-group-of-tokens prediction objective", "mixture-of-experts", "modality-agnostic expert routing", "elastic training paradigm", "reinforcement learning", "sparse MoE architecture"], "summary_zh": "<ul>\n    <li>ERNIE 5.0 \u662f\u4e00\u79cd\u65b0\u578b\u81ea\u56de\u5f52\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u8d85\u7a00\u758f\u7684\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u6240\u6709\u6a21\u6001\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\uff0c\u76ee\u6807\u662f\u9884\u6d4b\u4e0b\u4e00\u4e2a\u6807\u8bb0\u3002</li>\n    <li>ERNIE 5.0 \u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f39\u6027\u8bad\u7ec3\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u5355\u6b21\u9884\u8bad\u7ec3\u4e2d\u5b66\u4e60\u4e0d\u540c\u6df1\u5ea6\u548c\u4e13\u5bb6\u5bb9\u91cf\u7684\u5b50\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u591a\u6a21\u6001\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u4e14\u5747\u8861\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u662f\u9996\u4e2a\u652f\u6301\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\u7684\u4e07\u4ebf\u53c2\u6570\u81ea\u56de\u5f52\u6a21\u578b\u3002</li>\n    <li>\u62a5\u544a\u4e2d\u8fd8\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u4e13\u5bb6\u8def\u7531\u53ef\u89c6\u5316\u548c\u5f39\u6027\u8bad\u7ec3\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u4e3a\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u63d0\u4f9b\u6df1\u5165\u89c1\u89e3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>ERNIE 5.0 is a new model designed to understand and generate text, images, videos, and audio all at once.</li>\n    <li>The model is built using a special architecture that allows it to work efficiently by using a mixture of experts for different tasks.</li>\n    <li>It has a unique training method that helps it adapt to different resource needs, making it flexible for various uses.</li>\n    <li>ERNIE 5.0 is the first publicly available model of its size, with a trillion parameters, that can handle multiple types of data together.</li>\n    <li>Research findings and visualizations of how the model works are shared to help others in the field.</li>\n</ul>"}, "publishedAt": "2026-02-04T11:18:15.000Z", "title": "ERNIE 5.0 Technical Report", "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04705.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 228, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.00919", "authors": [{"_id": "698186fdce18b1862809633b", "name": "I. Apanasevich", "hidden": false}, {"_id": "698186fdce18b1862809633c", "user": {"_id": "6718963e41abf87204dddaf5", "avatarUrl": "/avatars/05d4fdb330ccb52c53cb8f99f7497ab2.svg", "isPro": false, "fullname": "Mikhail Artemyev", "user": "Mixanik-43", "type": "user"}, "name": "M. Artemyev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:34.142Z", "hidden": false}, {"_id": "698186fdce18b1862809633d", "name": "R. Babakyan", "hidden": false}, {"_id": "698186fdce18b1862809633e", "user": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "name": "P. Fedotova", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:41.710Z", "hidden": false}, {"_id": "698186fdce18b1862809633f", "name": "D. Grankin", "hidden": false}, {"_id": "698186fdce18b18628096340", "name": "E. Kupryashin", "hidden": false}, {"_id": "698186fdce18b18628096341", "user": {"_id": "662ace3c4f711ee4e1dcb790", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/R5dlha7Lpy5gCYFEAtr1L.jpeg", "isPro": false, "fullname": "Anastas Misailidi", "user": "kazzart", "type": "user"}, "name": "A. Misailidi", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:18.667Z", "hidden": false}, {"_id": "698186fdce18b18628096342", "user": {"_id": "66eb27551a537888d2121ddc", "avatarUrl": "/avatars/9c807b058c972c307a24d85efbfbd4ae.svg", "isPro": false, "fullname": "Daniil", "user": "Defgy", "type": "user"}, "name": "D. Nerus", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:33.667Z", "hidden": false}, {"_id": "698186fdce18b18628096343", "user": {"_id": "65e5e3df92de33440675b5d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e5e3df92de33440675b5d9/UOVd40f_Htd5oMAa_L0cM.jpeg", "isPro": false, "fullname": "Alexander Nutalapati", "user": "AlexanderNutalapati", "type": "user"}, "name": "A. Nutalapati", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:20.959Z", "hidden": false}, {"_id": "698186fdce18b18628096344", "user": {"_id": "66b51b3ad4eea6ad6adfd611", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b51b3ad4eea6ad6adfd611/SC_01wvlLjB0FFZdDVgAp.jpeg", "isPro": false, "fullname": "Gena Sidorov", "user": "haksorus", "type": "user"}, "name": "G. Sidorov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:41.584Z", "hidden": false}, {"_id": "698186fdce18b18628096345", "user": {"_id": "631ee99d2225f12fc0ef39f4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662970571579-631ee99d2225f12fc0ef39f4.jpeg", "isPro": false, "fullname": "Ivan Efremov", "user": "4ku", "type": "user"}, "name": "I. Efremov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:36.696Z", "hidden": false}, {"_id": "698186fdce18b18628096346", "name": "M. Gerasyov", "hidden": false}, {"_id": "698186fdce18b18628096347", "name": "D. Pikurov", "hidden": false}, {"_id": "698186fdce18b18628096348", "name": "Y. Senchenko", "hidden": false}, {"_id": "698186fdce18b18628096349", "user": {"_id": "68113993ebc57966794e23d6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Yc3GIqYZyO97lzZ9rX8OE.png", "isPro": false, "fullname": "Sergei Davidenko", "user": "Ant346", "type": "user"}, "name": "S. Davidenko", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:21.332Z", "hidden": false}, {"_id": "698186fdce18b1862809634a", "user": {"_id": "6981bbf47f758a03b9c46550", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/oTPe_MzIrDlCeRDvWWeLK.png", "isPro": false, "fullname": "Daniil Kulikov", "user": "KulikovDR", "type": "user"}, "name": "D. Kulikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:46.458Z", "hidden": false}, {"_id": "698186fdce18b1862809634b", "name": "M. Sultankin", "hidden": false}, {"_id": "698186fdce18b1862809634c", "user": {"_id": "63518aa5a30fc3ba88ce51dd", "avatarUrl": "/avatars/2e6a8f4a3e76fcc1afe7e777d6b45e76.svg", "isPro": false, "fullname": "Kazybek A", "user": "wanjia", "type": "user"}, "name": "K. Askarbek", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:24.567Z", "hidden": false}, {"_id": "698186fdce18b1862809634d", "name": "O. Shamanin", "hidden": false}, {"_id": "698186fdce18b1862809634e", "name": "D. Statovoy", "hidden": false}, {"_id": "698186fdce18b1862809634f", "user": {"_id": "655f32a519fd101f14bf1fb0", "avatarUrl": "/avatars/adf2c494759ebe5a0d95c15631ac6312.svg", "isPro": false, "fullname": "Eduard", "user": "rjomba3000", "type": "user"}, "name": "E. Zalyaev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:43.976Z", "hidden": false}, {"_id": "698186fdce18b18628096350", "user": {"_id": "67dd1714817478ae84b18981", "avatarUrl": "/avatars/1209da3d4c4de3f419ebea6845bb0ed6.svg", "isPro": false, "fullname": "Zorin Ilya", "user": "Zora244", "type": "user"}, "name": "I. Zorin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:31.637Z", "hidden": false}, {"_id": "698186fdce18b18628096351", "name": "A. Letkin", "hidden": false}, {"_id": "698186fdce18b18628096352", "name": "E. Rusakov", "hidden": false}, {"_id": "698186fdce18b18628096353", "name": "A. Silchenko", "hidden": false}, {"_id": "698186fdce18b18628096354", "user": {"_id": "6981a821165e30591e1200e7", "avatarUrl": "/avatars/af72142b8ba8772926b247c31fc8e4c8.svg", "isPro": false, "fullname": "Vlad Vorobyov", "user": "GloomARK", "type": "user"}, "name": "V. Vorobyov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:18.218Z", "hidden": false}, {"_id": "698186fdce18b18628096355", "user": {"_id": "6901ce2d911da714e754422b", "avatarUrl": "/avatars/5ed8ce189ca92a04f7165751076ff446.svg", "isPro": false, "fullname": "SERGEI", "user": "sobolnikov", "type": "user"}, "name": "S. Sobolnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:28.887Z", "hidden": false}, {"_id": "698186fdce18b18628096356", "user": {"_id": "640e2ef88512ec51d7f34cd5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640e2ef88512ec51d7f34cd5/Xl8UiprL-0SvOWHeoAFW1.jpeg", "isPro": false, "fullname": "Aleksey Postnikov", "user": "AlekseyPostnikov", "type": "user"}, "name": "A. Postnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:39.139Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "publishedAt": "2026-01-31T22:13:23.000Z", "submittedOnDailyAt": "2026-02-03T03:13:09.153Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "submittedOnDailyBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "upvotes": 173, "discussionId": "698186fece18b18628096357", "projectPage": "https://greenvla.github.io", "githubRepo": "https://github.com/greenvla/GreenVLA", "githubRepoAddedBy": "user", "ai_summary": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.", "ai_keywords": ["Vision-Language-Action", "multimodal grounding", "multi-embodiment pretraining", "embodiment-specific adaptation", "reinforcement-learning", "episode-progress prediction", "out-of-distribution detection", "joint-prediction-based guidance"], "githubStars": 24, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86Green-VLA\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8eGreen\u4eba\u5f62\u673a\u5668\u4eba\u90e8\u7f72\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\u3002</li>\n    <li>Green-VLA\u5305\u542b\u4e94\u4e2a\u9636\u6bb5\u7684\u8bfe\u7a0b\uff0c\u6db5\u76d6\u57fa\u7840\u6a21\u578b\u3001\u591a\u6a21\u6001\u57fa\u7840\u3001\u591a\u4e2a\u8eab\u4f53\u7684\u9884\u8bad\u7ec3\u7b49\u3002</li>\n    <li>\u8be5\u6846\u67b6\u4f7f\u7528\u53ef\u6269\u5c55\u7684\u6570\u636e\u5904\u7406\u7ba1\u9053\uff0c\u5904\u7406\u4e863000\u5c0f\u65f6\u7684\u6f14\u793a\u6570\u636e\u3002</li>\n    <li>VLA\u63a7\u5236\u5668\u589e\u5f3a\u4e86\u5b89\u5168\u6027\u548c\u76ee\u6807\u9009\u62e9\u7684\u51c6\u786e\u6027\uff0c\u5305\u62ec\u9884\u6d4b\u548c\u68c0\u6d4b\u529f\u80fd\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5bf9\u9f50\uff0cGreen-VLA\u5728\u6210\u529f\u7387\u3001\u9c81\u68d2\u6027\u548c\u6548\u7387\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Green-VLA is a new framework designed for the Green humanoid robot to effectively perform tasks in real-world settings.</li>\n    <li>The framework consists of five stages that help the robot learn from basic to advanced skills.</li>\n    <li>It uses a large data-processing system with 3,000 hours of demonstrations to improve learning and performance.</li>\n    <li>The robot can control different types of machines using a single action interface, making it versatile.</li>\n    <li>Tests show that Green-VLA improves success rates, reliability, and efficiency in task completion through advanced learning techniques.</li>\n</ul>"}, "publishedAt": "2026-01-31T17:13:23.000Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00919.png", "numComments": 1, "submittedBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "fullname": "Polina Fedotova", "name": "2pd", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.02276", "authors": [{"_id": "69817e2cce18b1862809615b", "name": "Kimi Team", "hidden": false}, {"_id": "69817e2cce18b1862809615c", "name": "Tongtong Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615d", "name": "Yifan Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615e", "name": "Yiping Bao", "hidden": false}, {"_id": "69817e2cce18b1862809615f", "name": "S. H. Cai", "hidden": false}, {"_id": "69817e2cce18b18628096160", "name": "Yuan Cao", "hidden": false}, {"_id": "69817e2cce18b18628096161", "name": "Y. Charles", "hidden": false}, {"_id": "69817e2cce18b18628096162", "name": "H. S. Che", "hidden": false}, {"_id": "69817e2cce18b18628096163", "name": "Cheng Chen", "hidden": false}, {"_id": "69817e2cce18b18628096164", "name": "Guanduo Chen", "hidden": false}, {"_id": "69817e2cce18b18628096165", "name": "Huarong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096166", "name": "Jia Chen", "hidden": false}, {"_id": "69817e2cce18b18628096167", "name": "Jiahao Chen", "hidden": false}, {"_id": "69817e2cce18b18628096168", "name": "Jianlong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096169", "name": "Jun Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616a", "name": "Kefan Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616b", "name": "Liang Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616c", "name": "Ruijue Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616d", "name": "Xinhao Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616e", "name": "Yanru Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616f", "name": "Yanxu Chen", "hidden": false}, {"_id": "69817e2cce18b18628096170", "name": "Yicun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096171", "name": "Yimin Chen", "hidden": false}, {"_id": "69817e2cce18b18628096172", "name": "Yingjiang Chen", "hidden": false}, {"_id": "69817e2cce18b18628096173", "name": "Yuankun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096174", "name": "Yujie Chen", "hidden": false}, {"_id": "69817e2cce18b18628096175", "name": "Yutian Chen", "hidden": false}, {"_id": "69817e2cce18b18628096176", "name": "Zhirong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096177", "name": "Ziwei Chen", "hidden": false}, {"_id": "69817e2cce18b18628096178", "name": "Dazhi Cheng", "hidden": false}, {"_id": "69817e2cce18b18628096179", "name": "Minghan Chu", "hidden": false}, {"_id": "69817e2cce18b1862809617a", "name": "Jialei Cui", "hidden": false}, {"_id": "69817e2cce18b1862809617b", "name": "Jiaqi Deng", "hidden": false}, {"_id": "69817e2cce18b1862809617c", "name": "Muxi Diao", "hidden": false}, {"_id": "69817e2cce18b1862809617d", "name": "Hao Ding", "hidden": false}, {"_id": "69817e2cce18b1862809617e", "name": "Mengfan Dong", "hidden": false}, {"_id": "69817e2cce18b1862809617f", "name": "Mengnan Dong", "hidden": false}, {"_id": "69817e2cce18b18628096180", "name": "Yuxin Dong", "hidden": false}, {"_id": "69817e2cce18b18628096181", "user": {"_id": "652965773a416e1f2173443b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg", "isPro": true, "fullname": "Yuhao Dong", "user": "THUdyh", "type": "user"}, "name": "Yuhao Dong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:04:11.993Z", "hidden": false}, {"_id": "69817e2cce18b18628096182", "name": "Angang Du", "hidden": false}, {"_id": "69817e2cce18b18628096183", "name": "Chenzhuang Du", "hidden": false}, {"_id": "69817e2cce18b18628096184", "name": "Dikang Du", "hidden": false}, {"_id": "69817e2cce18b18628096185", "name": "Lingxiao Du", "hidden": false}, {"_id": "69817e2cce18b18628096186", "user": {"_id": "6340f31fb78ed99eab04ce33", "avatarUrl": "/avatars/2e7fcbf0233bdc0bc9a3f4603fd8bf90.svg", "isPro": false, "fullname": "Du", "user": "Yulun", "type": "user"}, "name": "Yulun Du", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:47.298Z", "hidden": false}, {"_id": "69817e2cce18b18628096187", "name": "Yu Fan", "hidden": false}, {"_id": "69817e2cce18b18628096188", "name": "Shengjun Fang", "hidden": false}, {"_id": "69817e2cce18b18628096189", "name": "Qiulin Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618a", "name": "Yichen Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618b", "name": "Garimugai Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618c", "name": "Kelin Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618d", "name": "Hongcheng Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618e", "name": "Tong Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618f", "name": "Yuyao Ge", "hidden": false}, {"_id": "69817e2cce18b18628096190", "user": {"_id": "650a5d79a0f81fbc0a9875a7", "avatarUrl": "/avatars/a76b1c932964602f2fc4a801ccad3ab5.svg", "isPro": false, "fullname": "ShangyiGeng", "user": "Reset23", "type": "user"}, "name": "Shangyi Geng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:10.446Z", "hidden": false}, {"_id": "69817e2cce18b18628096191", "name": "Chengyang Gong", "hidden": false}, {"_id": "69817e2cce18b18628096192", "name": "Xiaochen Gong", "hidden": false}, {"_id": "69817e2cce18b18628096193", "name": "Zhuoma Gongque", "hidden": false}, {"_id": "69817e2cce18b18628096194", "name": "Qizheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096195", "name": "Xinran Gu", "hidden": false}, {"_id": "69817e2cce18b18628096196", "name": "Yicheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096197", "name": "Longyu Guan", "hidden": false}, {"_id": "69817e2cce18b18628096198", "name": "Yuanying Guo", "hidden": false}, {"_id": "69817e2cce18b18628096199", "name": "Xiaoru Hao", "hidden": false}, {"_id": "69817e2cce18b1862809619a", "name": "Weiran He", "hidden": false}, {"_id": "69817e2cce18b1862809619b", "name": "Wenyang He", "hidden": false}, {"_id": "69817e2cce18b1862809619c", "name": "Yunjia He", "hidden": false}, {"_id": "69817e2cce18b1862809619d", "name": "Chao Hong", "hidden": false}, {"_id": "69817e2cce18b1862809619e", "name": "Hao Hu", "hidden": false}, {"_id": "69817e2cce18b1862809619f", "name": "Jiaxi Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a0", "name": "Yangyang Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a1", "name": "Zhenxing Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a2", "name": "Ke Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a3", "name": "Ruiyuan Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a4", "name": "Weixiao Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a5", "name": "Zhiqi Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a6", "name": "Tao Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a7", "name": "Zhejun Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a8", "name": "Xinyi Jin", "hidden": false}, {"_id": "69817e2cce18b186280961a9", "name": "Yu Jing", "hidden": false}, {"_id": "69817e2cce18b186280961aa", "name": "Guokun Lai", "hidden": false}, {"_id": "69817e2cce18b186280961ab", "name": "Aidi Li", "hidden": false}, {"_id": "69817e2cce18b186280961ac", "name": "C. Li", "hidden": false}, {"_id": "69817e2cce18b186280961ad", "name": "Cheng Li", "hidden": false}, {"_id": "69817e2cce18b186280961ae", "name": "Fang Li", "hidden": false}, {"_id": "69817e2cce18b186280961af", "name": "Guanghe Li", "hidden": false}, {"_id": "69817e2cce18b186280961b0", "name": "Guanyu Li", "hidden": false}, {"_id": "69817e2cce18b186280961b1", "name": "Haitao Li", "hidden": false}, {"_id": "69817e2cce18b186280961b2", "name": "Haoyang Li", "hidden": false}, {"_id": "69817e2cce18b186280961b3", "name": "Jia Li", "hidden": false}, {"_id": "69817e2cce18b186280961b4", "name": "Jingwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961b5", "name": "Junxiong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b6", "name": "Lincan Li", "hidden": false}, {"_id": "69817e2cce18b186280961b7", "user": {"_id": "6576fe2b42ab083faea19841", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/c91ZKOR2E0gL8iIVkvEUa.jpeg", "isPro": false, "fullname": "Mo Li", "user": "Mor-Li", "type": "user"}, "name": "Mo Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:51.899Z", "hidden": false}, {"_id": "69817e2cce18b186280961b8", "name": "Weihong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b9", "name": "Wentao Li", "hidden": false}, {"_id": "69817e2cce18b186280961ba", "name": "Xinhang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bb", "name": "Xinhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961bc", "name": "Yang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bd", "name": "Yanhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961be", "name": "Yiwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961bf", "name": "Yuxiao Li", "hidden": false}, {"_id": "69817e2cce18b186280961c0", "name": "Zhaowei Li", "hidden": false}, {"_id": "69817e2cce18b186280961c1", "name": "Zheming Li", "hidden": false}, {"_id": "69817e2cce18b186280961c2", "name": "Weilong Liao", "hidden": false}, {"_id": "69817e2cce18b186280961c3", "name": "Jiawei Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c4", "name": "Xiaohan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c5", "name": "Zhishan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c6", "name": "Zichao Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c7", "name": "Cheng Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c8", "name": "Chenyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c9", "name": "Hongzhang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ca", "name": "Liang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cb", "name": "Shaowei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cc", "name": "Shudong Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cd", "name": "Shuran Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ce", "name": "Tianwei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cf", "name": "Tianyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d0", "name": "Weizhou Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d1", "name": "Xiangyan Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d2", "name": "Yangyang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d3", "name": "Yanming Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d4", "name": "Yibo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d5", "name": "Yuanxin Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d6", "name": "Yue Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d7", "name": "Zhengying Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d8", "name": "Zhongnuo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d9", "name": "Enzhe Lu", "hidden": false}, {"_id": "69817e2cce18b186280961da", "name": "Haoyu Lu", "hidden": false}, {"_id": "69817e2cce18b186280961db", "name": "Zhiyuan Lu", "hidden": false}, {"_id": "69817e2cce18b186280961dc", "user": {"_id": "642da1cd99f3110ac27caca5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg", "isPro": false, "fullname": "junyu", "user": "luojunyu", "type": "user"}, "name": "Junyu Luo", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:08.357Z", "hidden": false}, {"_id": "69817e2cce18b186280961dd", "name": "Tongxu Luo", "hidden": false}, {"_id": "69817e2cce18b186280961de", "name": "Yashuo Luo", "hidden": false}, {"_id": "69817e2cce18b186280961df", "name": "Long Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e0", "name": "Yingwei Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e1", "name": "Shaoguang Mao", "hidden": false}, {"_id": "69817e2cce18b186280961e2", "name": "Yuan Mei", "hidden": false}, {"_id": "69817e2cce18b186280961e3", "name": "Xin Men", "hidden": false}, {"_id": "69817e2cce18b186280961e4", "name": "Fanqing Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e5", "name": "Zhiyong Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e6", "name": "Yibo Miao", "hidden": false}, {"_id": "69817e2cce18b186280961e7", "name": "Minqing Ni", "hidden": false}, {"_id": "69817e2cce18b186280961e8", "name": "Kun Ouyang", "hidden": false}, {"_id": "69817e2cce18b186280961e9", "name": "Siyuan Pan", "hidden": false}, {"_id": "69817e2cce18b186280961ea", "name": "Bo Pang", "hidden": false}, {"_id": "69817e2cce18b186280961eb", "name": "Yuchao Qian", "hidden": false}, {"_id": "69817e2cce18b186280961ec", "name": "Ruoyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ed", "name": "Zeyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ee", "name": "Jiezhong Qiu", "hidden": false}, {"_id": "69817e2cce18b186280961ef", "name": "Bowen Qu", "hidden": false}, {"_id": "69817e2cce18b186280961f0", "name": "Zeyu Shang", "hidden": false}, {"_id": "69817e2cce18b186280961f1", "name": "Youbo Shao", "hidden": false}, {"_id": "69817e2cce18b186280961f2", "name": "Tianxiao Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f3", "name": "Zhennan Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f4", "name": "Juanfeng Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f5", "name": "Lidong Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f6", "name": "Shengyuan Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f7", "name": "Feifan Song", "hidden": false}, {"_id": "69817e2cce18b186280961f8", "name": "Pengwei Song", "hidden": false}, {"_id": "69817e2cce18b186280961f9", "name": "Tianhui Song", "hidden": false}, {"_id": "69817e2cce18b186280961fa", "name": "Xiaoxi Song", "hidden": false}, {"_id": "69817e2cce18b186280961fb", "name": "Hongjin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fc", "name": "Jianlin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fd", "name": "Zhaochen Su", "hidden": false}, {"_id": "69817e2cce18b186280961fe", "name": "Lin Sui", "hidden": false}, {"_id": "69817e2cce18b186280961ff", "name": "Jinsong Sun", "hidden": false}, {"_id": "69817e2cce18b18628096200", "name": "Junyao Sun", "hidden": false}, {"_id": "69817e2cce18b18628096201", "name": "Tongyu Sun", "hidden": false}, {"_id": "69817e2cce18b18628096202", "name": "Flood Sung", "hidden": false}, {"_id": "69817e2cce18b18628096203", "name": "Yunpeng Tai", "hidden": false}, {"_id": "69817e2cce18b18628096204", "name": "Chuning Tang", "hidden": false}, {"_id": "69817e2cce18b18628096205", "name": "Heyi Tang", "hidden": false}, {"_id": "69817e2cce18b18628096206", "name": "Xiaojuan Tang", "hidden": false}, {"_id": "69817e2cce18b18628096207", "name": "Zhengyang Tang", "hidden": false}, {"_id": "69817e2cce18b18628096208", "name": "Jiawen Tao", "hidden": false}, {"_id": "69817e2cce18b18628096209", "name": "Shiyuan Teng", "hidden": false}, {"_id": "69817e2cce18b1862809620a", "name": "Chaoran Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620b", "name": "Pengfei Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620c", "name": "Ao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620d", "name": "Bowen Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620e", "name": "Chensi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620f", "name": "Chuang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096210", "name": "Congcong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096211", "name": "Dingkun Wang", "hidden": false}, {"_id": "69817e2cce18b18628096212", "name": "Dinglu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096213", "name": "Dongliang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096214", "name": "Feng Wang", "hidden": false}, {"_id": "69817e2cce18b18628096215", "name": "Hailong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096216", "name": "Haiming Wang", "hidden": false}, {"_id": "69817e2cce18b18628096217", "name": "Hengzhi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096218", "name": "Huaqing Wang", "hidden": false}, {"_id": "69817e2cce18b18628096219", "name": "Hui Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621a", "name": "Jiahao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621b", "name": "Jinhong Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621c", "name": "Jiuzheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621d", "name": "Kaixin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621e", "name": "Linian Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621f", "name": "Qibin Wang", "hidden": false}, {"_id": "69817e2cce18b18628096220", "name": "Shengjie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096221", "name": "Shuyi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096222", "name": "Si Wang", "hidden": false}, {"_id": "69817e2cce18b18628096223", "name": "Wei Wang", "hidden": false}, {"_id": "69817e2cce18b18628096224", "name": "Xiaochen Wang", "hidden": false}, {"_id": "69817e2cce18b18628096225", "name": "Xinyuan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096226", "name": "Yao Wang", "hidden": false}, {"_id": "69817e2cce18b18628096227", "name": "Yejie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096228", "name": "Yipu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096229", "name": "Yiqin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622a", "name": "Yucheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622b", "name": "Yuzhi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622c", "name": "Zhaoji Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622d", "name": "Zhaowei Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622e", "name": "Zhengtao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622f", "name": "Zhexu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096230", "name": "Zihan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096231", "name": "Zizhe Wang", "hidden": false}, {"_id": "69817e2cce18b18628096232", "user": {"_id": "635ddec594e5b275ca7941e8", "avatarUrl": "/avatars/28ebfaee74d31e1de020a3ae735a4c1b.svg", "isPro": false, "fullname": "Chu Wei", "user": "courage17340", "type": "user"}, "name": "Chu Wei", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:17.862Z", "hidden": false}, {"_id": "69817e2cce18b18628096233", "name": "Ming Wei", "hidden": false}, {"_id": "69817e2cce18b18628096234", "name": "Chuan Wen", "hidden": false}, {"_id": "69817e2cce18b18628096235", "user": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "isPro": false, "fullname": "Zichen Wen", "user": "zichenwen", "type": "user"}, "name": "Zichen Wen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:09:43.363Z", "hidden": false}, {"_id": "69817e2cce18b18628096236", "name": "Chengjie Wu", "hidden": false}, {"_id": "69817e2cce18b18628096237", "user": {"_id": "63047ed2412a1b9d381b09c9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg", "isPro": true, "fullname": "Haoning Wu, Teo", "user": "teowu", "type": "user"}, "name": "Haoning Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:49.884Z", "hidden": false}, {"_id": "69817e2cce18b18628096238", "name": "Junyan Wu", "hidden": false}, {"_id": "69817e2cce18b18628096239", "name": "Rucong Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623a", "name": "Wenhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623b", "name": "Yuefeng Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623c", "name": "Yuhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623d", "name": "Yuxin Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623e", "name": "Zijian Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623f", "name": "Chenjun Xiao", "hidden": false}, {"_id": "69817e2cce18b18628096240", "name": "Jin Xie", "hidden": false}, {"_id": "69817e2cce18b18628096241", "name": "Xiaotong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096242", "name": "Yuchong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096243", "name": "Yifei Xin", "hidden": false}, {"_id": "69817e2cce18b18628096244", "name": "Bowei Xing", "hidden": false}, {"_id": "69817e2cce18b18628096245", "name": "Boyu Xu", "hidden": false}, {"_id": "69817e2cce18b18628096246", "name": "Jianfan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096247", "name": "Jing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096248", "name": "Jinjing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096249", "name": "L. H. Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624a", "name": "Lin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624b", "name": "Suting Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624c", "name": "Weixin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624d", "name": "Xinbo Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624e", "name": "Xinran Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624f", "name": "Yangchuan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096250", "name": "Yichang Xu", "hidden": false}, {"_id": "69817e2cce18b18628096251", "name": "Yuemeng Xu", "hidden": false}, {"_id": "69817e2cce18b18628096252", "name": "Zelai Xu", "hidden": false}, {"_id": "69817e2cce18b18628096253", "name": "Ziyao Xu", "hidden": false}, {"_id": "69817e2cce18b18628096254", "name": "Junjie Yan", "hidden": false}, {"_id": "69817e2cce18b18628096255", "name": "Yuzi Yan", "hidden": false}, {"_id": "69817e2cce18b18628096256", "name": "Guangyao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096257", "name": "Hao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096258", "name": "Junwei Yang", "hidden": false}, {"_id": "69817e2cce18b18628096259", "name": "Kai Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625a", "name": "Ningyuan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625b", "name": "Ruihan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625c", "name": "Xiaofei Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625d", "name": "Xinlong Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625e", "name": "Ying Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625f", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096260", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096261", "name": "Zhen Yang", "hidden": false}, {"_id": "69817e2cce18b18628096262", "name": "Zhilin Yang", "hidden": false}, {"_id": "69817e2cce18b18628096263", "name": "Zonghan Yang", "hidden": false}, {"_id": "69817e2cce18b18628096264", "user": {"_id": "642bcd9be8dfcc1fe4f4f853", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642bcd9be8dfcc1fe4f4f853/M9Yqkyt66dnWWCwmBZ8l0.jpeg", "isPro": false, "fullname": "Haotian Yao", "user": "skylark-95", "type": "user"}, "name": "Haotian Yao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:12.739Z", "hidden": false}, {"_id": "69817e2cce18b18628096265", "name": "Dan Ye", "hidden": false}, {"_id": "69817e2cce18b18628096266", "name": "Wenjie Ye", "hidden": false}, {"_id": "69817e2cce18b18628096267", "name": "Zhuorui Ye", "hidden": false}, {"_id": "69817e2cce18b18628096268", "name": "Bohong Yin", "hidden": false}, {"_id": "69817e2cce18b18628096269", "name": "Chengzhen Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626a", "name": "Longhui Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626b", "name": "Tao Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626c", "name": "Tianxiang Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626d", "name": "Enming Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626e", "name": "Mengjie Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626f", "name": "Xiaokun Yuan", "hidden": false}, {"_id": "69817e2cce18b18628096270", "name": "Yang Yue", "hidden": false}, {"_id": "69817e2cce18b18628096271", "name": "Weihao Zeng", "hidden": false}, {"_id": "69817e2cce18b18628096272", "name": "Dunyuan Zha", "hidden": false}, {"_id": "69817e2cce18b18628096273", "name": "Haobing Zhan", "hidden": false}, {"_id": "69817e2cce18b18628096274", "name": "Dehao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096275", "name": "Hao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096276", "name": "Jin Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096277", "name": "Puqi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096278", "name": "Qiao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096279", "name": "Rui Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627a", "name": "Xiaobin Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627b", "name": "Y. Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627c", "name": "Yadong Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627d", "name": "Yangkun Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627e", "name": "Yichi Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627f", "name": "Yizhi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096280", "name": "Yongting Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096281", "name": "Yu Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096282", "name": "Yushun Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096283", "name": "Yutao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096284", "name": "Yutong Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096285", "name": "Zheng Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096286", "name": "Chenguang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096287", "name": "Feifan Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096288", "name": "Jinxiang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096289", "name": "Shuai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628a", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628b", "name": "Yikai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628c", "name": "Zijia Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628d", "name": "Huabin Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628e", "name": "Ruihan Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628f", "name": "Shaojie Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096290", "name": "Tengyang Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096291", "name": "Junfeng Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096292", "user": {"_id": "62b6d20416ff90e6198301b6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656148456743-noauth.png", "isPro": false, "fullname": "Longguang Zhong", "user": "GGLS", "type": "user"}, "name": "Longguang Zhong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:14.989Z", "hidden": false}, {"_id": "69817e2cce18b18628096293", "name": "Weiming Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096294", "name": "M. Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096295", "name": "Runjie Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096296", "name": "Xinyu Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096297", "name": "Zaida Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096298", "name": "Jinguo Zhu", "hidden": false}, {"_id": "69817e2cce18b18628096299", "name": "Liya Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629a", "name": "Xinhao Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629b", "name": "Yuxuan Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629c", "name": "Zhen Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629d", "name": "Jingze Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629e", "name": "Weiyu Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629f", "name": "Ying Zou", "hidden": false}, {"_id": "69817e2cce18b186280962a0", "name": "Xinxing Zu", "hidden": false}], "publishedAt": "2026-02-02T16:17:38.000Z", "submittedOnDailyAt": "2026-02-03T02:18:48.721Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "upvotes": 149, "discussionId": "69817e2cce18b186280962a1", "projectPage": "https://huggingface.co/moonshotai/Kimi-K2.5", "ai_summary": "Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.", "ai_keywords": ["multimodal agentic model", "joint text-vision pre-training", "zero-vision SFT", "joint text-vision reinforcement learning", "Agent Swarm", "self-directed parallel agent orchestration framework", "heterogeneous sub-problems"], "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86Kimi K2.5\uff0c\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u667a\u80fd\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u5347\u901a\u7528\u667a\u80fd\u3002</li>\n    <li>K2.5\u5f3a\u8c03\u6587\u672c\u548c\u89c6\u89c9\u7684\u8054\u5408\u4f18\u5316\uff0c\u4f7f\u4e24\u79cd\u6a21\u5f0f\u76f8\u4e92\u589e\u5f3a\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u4e86\u4e00\u7cfb\u5217\u6280\u672f\uff0c\u5305\u62ec\u8054\u5408\u6587\u672c-\u89c6\u89c9\u9884\u8bad\u7ec3\u7b49\u3002</li>\n    <li>K2.5\u5f15\u5165\u4e86Agent Swarm\u6846\u67b6\uff0c\u53ef\u4ee5\u52a8\u6001\u5206\u89e3\u590d\u6742\u4efb\u52a1\u5e76\u5e76\u884c\u6267\u884c\u3002</li>\n    <li>\u7ecf\u8fc7\u8bc4\u4f30\uff0cKimi K2.5\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u4e14Agent Swarm\u5c06\u5ef6\u8fdf\u51cf\u5c11\u4e86\u6700\u591a4.5\u500d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kimi K2.5 is an open-source model designed to improve general intelligence by combining text and vision.</li>\n    <li>It uses techniques that help text and visual information work together better, like joint training and reinforcement learning.</li>\n    <li>K2.5 introduces Agent Swarm, a system that breaks down complex tasks into smaller parts and handles them at the same time.</li>\n    <li>The model has shown top results in areas like coding, vision, reasoning, and other tasks.</li>\n    <li>Kimi K2.5 can complete tasks faster, reducing time by up to 4.5 times compared to traditional single-agent methods.</li>\n</ul>"}, "publishedAt": "2026-02-02T11:17:38.000Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02276.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 227, "isUserFollowing": false}, "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.07085", "authors": [{"_id": "698ab6f91b2dc6b37d61b031", "name": "Jun Han", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b032", "name": "Shuo Zhang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b033", "name": "Wei Li", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b034", "user": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "name": "Zhi Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:58.707Z", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b035", "name": "Yifan Dong", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b036", "name": "Tu Hu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b037", "name": "Jialuo Yuan", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b038", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:06:00.954Z", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b039", "name": "Yumo Zhu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03a", "name": "Fangqi Lou", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03b", "name": "Xin Guo", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03c", "name": "Zhaowei Liu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03d", "name": "Tianyi Jiang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03e", "name": "Ruichuan An", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03f", "name": "Jingping Liu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b040", "name": "Biao Wu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b041", "name": "Rongze Chen", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b042", "name": "Kunyi Wang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b043", "name": "Yifan Wang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b044", "name": "Sen Hu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b045", "name": "Xinbing Kong", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b046", "name": "Liwen Zhang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b047", "name": "Ronghao Chen", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b048", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-02-06T08:08:04.000Z", "submittedOnDailyAt": "2026-02-10T02:19:22.216Z", "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining", "submittedOnDailyBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "summary": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.", "upvotes": 141, "discussionId": "698ab6fa1b2dc6b37d61b049", "githubRepo": "https://github.com/QuantaAlpha/QuantaAlpha", "githubRepoAddedBy": "user", "githubStars": 63, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "summary_zh": "<ul>\n  <li>\u91d1\u878d\u5e02\u573a\u566a\u58f0\u5927\u4e14\u4e0d\u7a33\u5b9a\uff0c\u5bfc\u81f4\u56de\u6d4b\u7ed3\u679c\u5bf9\u566a\u58f0\u548c\u5e02\u573a\u53d8\u5316\u975e\u5e38\u654f\u611f\u3002</li>\n  <li>QuantaAlpha\u662f\u4e00\u4e2a\u8fdb\u5316\u578b\u7684alpha\u6316\u6398\u6846\u67b6\uff0c\u901a\u8fc7\u8f68\u8ff9\u7ea7\u7684\u53d8\u5f02\u548c\u4ea4\u53c9\u64cd\u4f5c\u6765\u6539\u8fdb\u6316\u6398\u8fc7\u7a0b\u3002</li>\n  <li>\u5728\u6bcf\u4e2a\u6316\u6398\u8f68\u8ff9\u4e2d\uff0cQuantaAlpha\u5b9a\u4f4d\u6b21\u4f18\u6b65\u9aa4\u8fdb\u884c\u4fee\u6b63\uff0c\u5e76\u91cd\u65b0\u7ec4\u5408\u9ad8\u6536\u76ca\u6bb5\uff0c\u589e\u5f3a\u63a2\u7d22\u548c\u6539\u8fdb\u3002</li>\n  <li>\u5728\u56e0\u5b50\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0cQuantaAlpha\u786e\u4fdd\u5047\u8bbe\u3001\u56e0\u5b50\u8868\u8fbe\u548c\u53ef\u6267\u884c\u4ee3\u7801\u7684\u4e00\u81f4\u6027\uff0c\u51cf\u5c0f\u590d\u6742\u6027\u548c\u5197\u4f59\u3002</li>\n  <li>\u5728CSI 300\u7684\u5b9e\u9a8c\u4e2d\uff0cQuantaAlpha\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u56de\u62a5\u7387\u548c\u6709\u6548\u6027\uff0c\u5e76\u80fd\u6709\u6548\u8fc1\u79fb\u5230\u5176\u4ed6\u5e02\u573a\u6307\u6570\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Financial markets can be unpredictable, making it hard to test trading strategies accurately.</li>\n    <li>QuantaAlpha is a new framework designed to improve the process of finding effective trading strategies by using evolutionary techniques.</li>\n    <li>It improves strategies by focusing on specific steps that need revision and combining successful patterns from different attempts.</li>\n    <li>QuantaAlpha ensures that the strategies it generates are clear and not overly complicated, which helps avoid issues with similar strategies competing against each other.</li>\n    <li>Tests show that QuantaAlpha performs better than existing models and its strategies can also work well in different market conditions.</li>\n</ul>"}, "publishedAt": "2026-02-06T03:08:04.000Z", "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining", "summary": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07085.png", "numComments": 1, "submittedBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "fullname": "Zhi Yang", "name": "yangzhi1", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u4e30\u5bcc\u7684\u8bed\u4e49\u5b9e\u4f53\uff0c\u91cd\u8981\u7684\u662f\u4ece\u536b\u661f\u56fe\u50cf\u4e2d\u8fdb\u884c\u5206\u5272\u3002</li>\n    <li>\u73b0\u6709\u7684\u5206\u5272\u6a21\u578b\u5bf9\u7269\u7406\u5c5e\u6027\u7684\u5b9e\u4f53\u5206\u5272\u6548\u679c\u826f\u597d\uff0c\u4f46\u5bf9\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\uff08\u5982\u5b66\u6821\u548c\u516c\u56ed\uff09\u4ecd\u6709\u56f0\u96be\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSocioSeg\u7684\u57ce\u5e02\u793e\u4f1a\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u6846\u67b6SocioReasoner\uff0c\u4ee5\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u6807\u6ce8\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban areas have many different types of features, and identifying them from satellite images is important for various uses.</li>\n    <li>Most advanced models can detect physical features like buildings or water but struggle with social features like schools or parks.</li>\n    <li>The study introduces a new dataset called SocioSeg, which includes satellite images, maps, and detailed labels for social features.</li>\n    <li>A new framework, called SocioReasoner, helps computers recognize and label social features by mimicking how humans do it.</li>\n    <li>Tests show that this new method performs better than existing models and can generalize well to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.16725", "authors": [{"_id": "6976d5405d41524304c13537", "name": "Meituan LongCat Team", "hidden": false}, {"_id": "6976d5405d41524304c13538", "name": "Anchun Gui", "hidden": false}, {"_id": "6976d5405d41524304c13539", "name": "Bei Li", "hidden": false}, {"_id": "6976d5405d41524304c1353a", "name": "Bingyang Tao", "hidden": false}, {"_id": "6976d5405d41524304c1353b", "name": "Bole Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1353c", "name": "Borun Chen", "hidden": false}, {"_id": "6976d5405d41524304c1353e", "name": "Chao Zhang", "hidden": false}, {"_id": "69772bc15d41524304c13739", "name": "Chao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1353f", "name": "Chen Gao", "hidden": false}, {"_id": "6976d5405d41524304c13540", "name": "Chen Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13541", "name": "Chengcheng Han", "hidden": false}, {"_id": "6976d5405d41524304c13542", "name": "Chenhui Yang", "hidden": false}, {"_id": "6976d5405d41524304c13543", "name": "Chuyu Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13544", "name": "Cong Chen", "hidden": false}, {"_id": "6976d5405d41524304c13545", "name": "Cunguang Wang", "hidden": false}, {"_id": "6976d5405d41524304c13546", "name": "Daoru Pan", "hidden": false}, {"_id": "6976d5405d41524304c13547", "name": "Defei Bu", "hidden": false}, {"_id": "6976d5405d41524304c13548", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13549", "name": "Di Xiu", "hidden": false}, {"_id": "6976d5405d41524304c1354a", "name": "Dishan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1354b", "name": "Dongyu Ru", "hidden": false}, {"_id": "6976d5405d41524304c1354c", "name": "Dunwei Tu", "hidden": false}, {"_id": "6976d5405d41524304c1354d", "name": "Fan Wu", "hidden": false}, {"_id": "6976d5405d41524304c1354e", "name": "Fengcheng Yuan", "hidden": false}, {"_id": "6976d5405d41524304c1354f", "name": "Fengcun Li", "hidden": false}, {"_id": "6976d5405d41524304c13550", "name": "Gang Xu", "hidden": false}, {"_id": "6976d5405d41524304c13551", "name": "Guanyu Wu", "hidden": false}, {"_id": "6976d5405d41524304c13552", "name": "Guoyuan Lin", "hidden": false}, {"_id": "6976d5405d41524304c13553", "name": "Haibin Wang", "hidden": false}, {"_id": "6976d5405d41524304c13554", "name": "Hansi Yang", "hidden": false}, {"_id": "6976d5405d41524304c13555", "name": "Hao Yang", "hidden": false}, {"_id": "6976d5405d41524304c13556", "name": "Haonan Yan", "hidden": false}, {"_id": "6976d5405d41524304c13557", "name": "Haoxiang Ma", "hidden": false}, {"_id": "6976d5405d41524304c13558", "name": "Haoxing Wen", "hidden": false}, {"_id": "6976d5405d41524304c13559", "name": "Hongyan Hao", "hidden": false}, {"_id": "6976d5405d41524304c1355a", "name": "Hongyin Tang", "hidden": false}, {"_id": "6976d5405d41524304c1355b", "name": "Hongyu Zang", "hidden": false}, {"_id": "6976d5405d41524304c1355c", "name": "Hongzhi Ni", "hidden": false}, {"_id": "6976d5405d41524304c1355d", "name": "Hui Su", "hidden": false}, {"_id": "6976d5405d41524304c1355e", "name": "Jiacheng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1355f", "name": "Jiahong Zhou", "hidden": false}, {"_id": "6976d5405d41524304c13560", "name": "Jiahuan Li", "hidden": false}, {"_id": "6976d5405d41524304c13561", "name": "Jiaming Wang", "hidden": false}, {"_id": "6976d5405d41524304c13562", "name": "Jian Yang", "hidden": false}, {"_id": "6976d5405d41524304c13563", "user": {"_id": "64008a0af4ff62c2616d8858", "avatarUrl": "/avatars/b52c98857916fba5377ace8089d658b2.svg", "isPro": false, "fullname": "zhangjf", "user": "zhangjf", "type": "user"}, "name": "Jianfei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:09.272Z", "hidden": false}, {"_id": "6976d5405d41524304c13564", "name": "Jianhao Xu", "hidden": false}, {"_id": "6976d5405d41524304c13565", "name": "Jianing Wang", "hidden": false}, {"_id": "6976d5405d41524304c13566", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13567", "name": "Jiaqi Sun", "hidden": false}, {"_id": "6976d5405d41524304c13568", "name": "Jiarong Shi", "hidden": false}, {"_id": "6976d5405d41524304c13569", "name": "Jiarui Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1356a", "name": "Jingang Wang", "hidden": false}, {"_id": "6976d5405d41524304c1356b", "user": {"_id": "6592472fccbc1e2cc7250903", "avatarUrl": "/avatars/6f04ae66944eb2ce65c5aca7927bab10.svg", "isPro": false, "fullname": "Jinluan Yang", "user": "Jinluan", "type": "user"}, "name": "Jinluan Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:47.175Z", "hidden": false}, {"_id": "6976d5405d41524304c1356c", "name": "Jinrui Ding", "hidden": false}, {"_id": "6976d5405d41524304c1356d", "name": "Jinwei Xiao", "hidden": false}, {"_id": "6976d5405d41524304c1356e", "name": "Jiyuan He", "hidden": false}, {"_id": "6976d5405d41524304c1356f", "name": "Juncan Xu", "hidden": false}, {"_id": "6976d5405d41524304c13570", "name": "Kefeng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13571", "name": "Keheng Wang", "hidden": false}, {"_id": "6976d5405d41524304c13572", "name": "Li Wei", "hidden": false}, {"_id": "6976d5405d41524304c13573", "name": "Lianhui Ma", "hidden": false}, {"_id": "6976d5405d41524304c13574", "name": "Lin Qiu", "hidden": false}, {"_id": "6976d5405d41524304c13575", "name": "Lingbing Kong", "hidden": false}, {"_id": "6976d5405d41524304c13576", "name": "Lingchuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c13577", "name": "Linsen Guo", "hidden": false}, {"_id": "6976d5405d41524304c13578", "name": "Mengshen Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13579", "name": "Mengxia Shen", "hidden": false}, {"_id": "6976d5405d41524304c1357a", "name": "Mingyang Zhu", "hidden": false}, {"_id": "6976d5405d41524304c1357b", "name": "Peiguang Li", "hidden": false}, {"_id": "6976d5405d41524304c1357c", "name": "Peng Pei", "hidden": false}, {"_id": "6976d5405d41524304c1357d", "name": "Pengcheng Jia", "hidden": false}, {"_id": "6976d5405d41524304c1357e", "name": "Pengtao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1357f", "name": "Peng Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13580", "name": "Qi Gu", "hidden": false}, {"_id": "6976d5405d41524304c13581", "name": "Qiong Huang", "hidden": false}, {"_id": "6976d5405d41524304c13582", "name": "Qiyuan Duan", "hidden": false}, {"_id": "6976d5405d41524304c13583", "name": "Quanchi Weng", "hidden": false}, {"_id": "6976d5405d41524304c13584", "name": "Rongxiang Weng", "hidden": false}, {"_id": "6976d5405d41524304c13585", "name": "Rongzhi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13586", "name": "Rumei Li", "hidden": false}, {"_id": "6976d5405d41524304c13587", "name": "Shanglin Lei", "hidden": false}, {"_id": "6976d5405d41524304c13588", "user": {"_id": "64db5f5dd68a6ddcc7bd89e9", "avatarUrl": "/avatars/69375ec915927b855813df8a6d486837.svg", "isPro": false, "fullname": "Shengnan An", "user": "ShengnanAn", "type": "user"}, "name": "Shengnan An", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:11.410Z", "hidden": false}, {"_id": "6976d5405d41524304c13589", "name": "Shijun Dai", "hidden": false}, {"_id": "6976d5405d41524304c1358a", "name": "Shuaikang Liu", "hidden": false}, {"_id": "6976d5405d41524304c1358b", "name": "Shuang Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1358c", "name": "Shuo Wang", "hidden": false}, {"_id": "6976d5405d41524304c1358d", "name": "Songyuan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1358e", "name": "Tao Liang", "hidden": false}, {"_id": "6976d5405d41524304c1358f", "name": "Tianhao Hu", "hidden": false}, {"_id": "6976d5405d41524304c13590", "name": "Tianze Chen", "hidden": false}, {"_id": "6976d5405d41524304c13591", "name": "Wei Liu", "hidden": false}, {"_id": "6976d5405d41524304c13592", "name": "Wei Shi", "hidden": false}, {"_id": "6976d5405d41524304c13593", "name": "Wei Wang", "hidden": false}, {"_id": "6976d5405d41524304c13594", "name": "Weifeng Tang", "hidden": false}, {"_id": "6976d5405d41524304c13595", "name": "Wenjie Shi", "hidden": false}, {"_id": "6976d5405d41524304c13596", "name": "Wenlong Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13597", "name": "Wentao Chen", "hidden": false}, {"_id": "6976d5405d41524304c13598", "name": "Wentao Shi", "hidden": false}, {"_id": "6976d5405d41524304c13599", "name": "Xi Su", "hidden": false}, {"_id": "6976d5405d41524304c1359a", "name": "Xiangcheng Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359b", "name": "Xiandi Ma", "hidden": false}, {"_id": "6976d5405d41524304c1359c", "user": {"_id": "63edb098679c2cc40abc6c2e", "avatarUrl": "/avatars/288c7229937c2c3f29fda6d17c7df2eb.svg", "isPro": false, "fullname": "Xiangyu", "user": "xixy", "type": "user"}, "name": "Xiangyu Xi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:13.312Z", "hidden": false}, {"_id": "6976d5405d41524304c1359d", "name": "Xiangyuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359e", "name": "Xiangzhou Huang", "hidden": false}, {"_id": "6976d5405d41524304c1359f", "name": "Xiao Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a0", "name": "Xiaodong Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a1", "name": "Xiaolong Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a2", "name": "Xiaowei Shi", "hidden": false}, {"_id": "6976d5405d41524304c135a3", "name": "Xiaoyu Li", "hidden": false}, {"_id": "6976d5405d41524304c135a4", "name": "Xin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a5", "name": "Xingchen Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a6", "name": "Xuan Huang", "hidden": false}, {"_id": "6976d5405d41524304c135a7", "name": "Xuezhi Cao", "hidden": false}, {"_id": "6976d5405d41524304c135a8", "name": "Xunliang Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a9", "name": "Yan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135aa", "user": {"_id": "63fc1c420aab06079200c15c", "avatarUrl": "/avatars/8e8e82a9a6552848581ca9f65011263c.svg", "isPro": false, "fullname": "yang bai", "user": "byang", "type": "user"}, "name": "Yang Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:07.036Z", "hidden": false}, {"_id": "6976d5405d41524304c135ab", "name": "Yang Liu", "hidden": false}, {"_id": "6976d5405d41524304c135ac", "name": "Yang Yang", "hidden": false}, {"_id": "6976d5405d41524304c135ad", "name": "Yang Zheng", "hidden": false}, {"_id": "6976d5405d41524304c135ae", "name": "Yaoming Wang", "hidden": false}, {"_id": "6976d5405d41524304c135af", "name": "Yaoming Zhu", "hidden": false}, {"_id": "6976d5405d41524304c135b0", "name": "Yaqi Huo", "hidden": false}, {"_id": "6976d5405d41524304c135b1", "name": "Yanyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b2", "name": "Yaorui Shi", "hidden": false}, {"_id": "6976d5405d41524304c135b3", "name": "Yerui Sun", "hidden": false}, {"_id": "6976d5405d41524304c135b4", "name": "Yi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b5", "name": "Yihao Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b6", "name": "Yi-Kai Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b7", "name": "Yifan Lu", "hidden": false}, {"_id": "6976d5405d41524304c135b8", "name": "Yifan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135b9", "name": "Yitao Zhai", "hidden": false}, {"_id": "6976d5405d41524304c135ba", "name": "Yongjing Yin", "hidden": false}, {"_id": "6976d5405d41524304c135bb", "name": "Yongwei Zhou", "hidden": false}, {"_id": "6976d5405d41524304c135bc", "name": "Youshao Xiao", "hidden": false}, {"_id": "6976d5405d41524304c135bd", "name": "Yuchuan Dai", "hidden": false}, {"_id": "6976d5405d41524304c135be", "name": "Yuchen Xie", "hidden": false}, {"_id": "6976d5405d41524304c135bf", "name": "Yuchen Yu", "hidden": false}, {"_id": "6976d5405d41524304c135c0", "name": "Yufei Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135c1", "name": "Yuhuai Wei", "hidden": false}, {"_id": "6976d5405d41524304c135c2", "name": "Yulei Qian", "hidden": false}, {"_id": "6976d5405d41524304c135c3", "name": "Yunfan Liang", "hidden": false}, {"_id": "6976d5405d41524304c135c4", "name": "Yunke Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135c5", "name": "Yuwei Jiang", "hidden": false}, {"_id": "6976d5405d41524304c135c6", "name": "Yuxin Bian", "hidden": false}, {"_id": "6976d5405d41524304c135c7", "name": "Yuxin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135c8", "name": "Yuxin Liu", "hidden": false}, {"_id": "6976d5405d41524304c135c9", "name": "Yue Xu", "hidden": false}, {"_id": "6976d5405d41524304c135ca", "name": "Yueqing Sun", "hidden": false}, {"_id": "6976d5405d41524304c135cb", "name": "Zeyang Yu", "hidden": false}, {"_id": "6976d5405d41524304c135cc", "name": "Zhao Yang", "hidden": false}, {"_id": "6976d5405d41524304c135cd", "name": "Zhengsheng Huang", "hidden": false}, {"_id": "6976d5405d41524304c135ce", "name": "Zhengyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135cf", "name": "Zhijian Liu", "hidden": false}, {"_id": "6976d5405d41524304c135d0", "name": "Zhikang Xia", "hidden": false}, {"_id": "6976d5405d41524304c135d1", "name": "Zhimin Lin", "hidden": false}, {"_id": "6976d5405d41524304c135d2", "name": "Zhiyuan Yao", "hidden": false}, {"_id": "6976d5405d41524304c135d3", "name": "Zhuofan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135d4", "name": "Zhuowen Han", "hidden": false}, {"_id": "6976d5405d41524304c135d5", "name": "Zijian Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135d6", "name": "Ziran Li", "hidden": false}, {"_id": "6976d5405d41524304c135d7", "name": "Ziwen Wang", "hidden": false}, {"_id": "6976d5405d41524304c135d8", "name": "Ziyuan Zhuang", "hidden": false}], "publishedAt": "2026-01-23T13:20:09.000Z", "submittedOnDailyAt": "2026-01-26T00:15:28.340Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "upvotes": 136, "discussionId": "6976d5405d41524304c135d9", "ai_summary": "A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for real-world robustness and complex reasoning.", "ai_keywords": ["Mixture-of-Experts", "agentic reasoning", "domain-parallel expert training", "fusion", "asynchronous reinforcement learning", "DORA", "long-tailed generation", "multi-turn interactions", "real-world noise patterns", "test-time scaling", "reasoning depth", "reasoning width", "parallel thinking"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86LongCat-Flash-Thinking-2601\uff0c\u8fd9\u662f\u4e00\u4e2a\u62e5\u67095600\u4ebf\u53c2\u6570\u7684\u5f00\u6e90\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u63a8\u7406\u6a21\u578b\uff0c\u5177\u6709\u51fa\u8272\u7684\u667a\u80fd\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u591a\u9879\u667a\u80fd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5305\u62ec\u667a\u80fd\u641c\u7d22\u3001\u5de5\u5177\u4f7f\u7528\u548c\u5de5\u5177\u96c6\u6210\u63a8\u7406\u3002</li>\n    <li>\u6a21\u578b\u80fd\u591f\u5f88\u597d\u5730\u9002\u5e94\u590d\u6742\u5de5\u5177\u4ea4\u4e92\uff0c\u5e76\u5728\u5608\u6742\u7684\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002</li>\n    <li>\u901a\u8fc7\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u9886\u57df\u5e76\u884c\u4e13\u5bb6\u8bad\u7ec3\u548c\u540e\u7eed\u878d\u5408\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u80fd\u529b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u91cd\u601d\u7ef4\u6a21\u5f0f\uff0c\u901a\u8fc7\u5e76\u884c\u601d\u7ef4\u6269\u5c55\u63a8\u7406\u6df1\u5ea6\u548c\u5e7f\u5ea6\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongCat-Flash-Thinking-2601 is a large open-source AI model with 560 billion parameters, designed for advanced reasoning tasks.</li>\n    <li>It performs exceptionally well on various benchmarks related to agent-based tasks, such as using tools and reasoning with them in complex situations.</li>\n    <li>The model's success is due to a unique training approach that combines expert training and a well-structured design across multiple stages of development.</li>\n    <li>It is built to handle noisy real-world conditions and can adapt well to complicated interactions with different tools.</li>\n    <li>A special mode called Heavy Thinking allows for deeper and broader reasoning during testing by using parallel thinking techniques.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:20:09.000Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16725.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.08794", "authors": [{"_id": "698ac65d1b2dc6b37d61b1c2", "name": "SII-OpenMOSS Team", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c4", "name": "Donghua Yu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c5", "name": "Mingshu Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c6", "name": "Qi Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c7", "name": "Qi Luo", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c8", "name": "Qianyi Wu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1c9", "user": {"_id": "63ec4715c81b6a52391c46b8", "avatarUrl": "/avatars/496819b5075a1a834a2b9edeb068c80e.svg", "isPro": false, "fullname": "QinyuanCheng", "user": "Cqy2019", "type": "user"}, "name": "Qinyuan Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:07.400Z", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1ca", "name": "Ruixiao Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cb", "user": {"_id": "62c14609ac1b639c2d87192c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png", "isPro": false, "fullname": "SII-liangtianyi", "user": "tianyilt", "type": "user"}, "name": "Tianyi Liang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:10.522Z", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cc", "name": "Wenbo Zhang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cd", "name": "Wenming Tu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1ce", "name": "Xiangyu Peng", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1cf", "name": "Yang Gao", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d0", "name": "Yanru Huo", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d1", "user": {"_id": "69158ffc0153b85a677dcc46", "avatarUrl": "/avatars/c9c5f60522f2a8f370d790ea9938b090.svg", "isPro": false, "fullname": "Ying Zhu", "user": "Auraithm", "type": "user"}, "name": "Ying Zhu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:27:41.440Z", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d2", "name": "Yinze Luo", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d3", "name": "Yiyang Zhang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d4", "name": "Yuerong Song", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d5", "name": "Zhe Xu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d6", "name": "Zhiyu Zhang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d7", "name": "Chenchen Yang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d8", "name": "Cheng Chang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1d9", "name": "Chushu Zhou", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1da", "name": "Hanfu Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1db", "name": "Hongnan Ma", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1dc", "name": "Jiaxi Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1dd", "name": "Jingqi Tong", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1de", "name": "Junxi Liu", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1df", "name": "Ke Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e0", "name": "Shimin Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e1", "name": "Songlin Wang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e2", "name": "Wei Jiang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e3", "name": "Zhaoye Fei", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e4", "name": "Zhiyuan Ning", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e5", "name": "Chunguo Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e6", "name": "Chenhui Li", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e7", "name": "Ziwei He", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e8", "name": "Zengfeng Huang", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1e9", "name": "Xie Chen", "hidden": false}, {"_id": "698ac65d1b2dc6b37d61b1ea", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-02-09T15:31:54.000Z", "submittedOnDailyAt": "2026-02-10T03:18:59.260Z", "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation", "submittedOnDailyBy": {"_id": "62c14609ac1b639c2d87192c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png", "isPro": false, "fullname": "SII-liangtianyi", "user": "tianyilt", "type": "user"}, "summary": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.", "upvotes": 131, "discussionId": "698ac65e1b2dc6b37d61b1eb", "projectPage": "https://mosi.cn/models/mova", "githubRepo": "https://github.com/OpenMOSS/MOVA", "githubRepoAddedBy": "user", "ai_summary": "MOVA is an open-source model that generates synchronized audio-visual content using a Mixture-of-Experts architecture with 32 billion parameters, supporting image-text to video-audio generation tasks.", "ai_keywords": ["Mixture-of-Experts", "MoE", "audio-visual content", "lip-synced speech", "sound effects", "content-aligned music", "IT2VA", "efficient inference", "LoRA fine-tuning", "prompt enhancement"], "githubStars": 579, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u97f3\u9891\u5728\u89c6\u9891\u5185\u5bb9\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u751f\u6210\u6a21\u578b\u901a\u5e38\u5ffd\u89c6\u97f3\u9891\u90e8\u5206\u3002</li>\n    <li>\u5f53\u524d\u7684\u97f3\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4f7f\u7528\u7ea7\u8054\u6d41\u7a0b\uff0c\u5bfc\u81f4\u6210\u672c\u589e\u52a0\u548c\u8d28\u91cf\u4e0b\u964d\u3002</li>\n    <li>MOVA\uff08MOSS\u89c6\u9891\u548c\u97f3\u9891\uff09\u662f\u4e00\u4e2a\u5f00\u6e90\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u540c\u6b65\u97f3\u89c6\u9891\u5185\u5bb9\u3002</li>\n    <li>MOVA\u4f7f\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u5177\u6709320\u4ebf\u53c2\u6570\uff0c\u5176\u4e2d180\u4ebf\u5728\u63a8\u7406\u65f6\u6d3b\u8dc3\u3002</li>\n    <li>\u53d1\u5e03\u6a21\u578b\u6743\u91cd\u548c\u4ee3\u7801\u65e8\u5728\u63a8\u52a8\u7814\u7a76\u5e76\u4fc3\u8fdb\u521b\u4f5c\u8005\u793e\u533a\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Audio is important for video, but many models ignore it, leading to quality issues.</li>\n    <li>Current methods often use separate processes for audio and video, which can be costly and error-prone.</li>\n    <li>MOVA (MOSS Video and Audio) is a new open-source model that generates high-quality synchronized audio and video.</li>\n    <li>MOVA can create realistic lip-synced speech, sound effects, and music that aligns with the video content.</li>\n    <li>The model has a large architecture and is designed to support collaboration and further research in the field.</li>\n</ul>"}, "publishedAt": "2026-02-09T10:31:54.000Z", "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation", "summary": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.08794.png", "numComments": 1, "submittedBy": {"_id": "62c14609ac1b639c2d87192c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png", "fullname": "SII-liangtianyi", "name": "tianyilt", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u4f18\u5316\u7d27\u51d1\u6548\u7387\u548c\u591a\u6a21\u6001\u667a\u80fd\u4e4b\u95f4\u7684\u5e73\u8861\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u6574\u5408\u8bed\u8a00\u5bf9\u9f50\u7684\u611f\u77e5\u7f16\u7801\u5668\u548c Qwen3-8B \u89e3\u7801\u5668\uff0c\u8fdb\u884c\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\uff0c\u4f7f\u7528\u4e86 1.2T \u7684\u591a\u6a21\u6001\u6570\u636e\u3002</li>\n    <li>STEP3-VL-10B \u5728\u540e\u671f\u8bad\u7ec3\u4e2d\u8fdb\u884c\u4e86\u8d85\u8fc7 1000 \u6b21\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7f\u5176\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u6709\u4e86\u663e\u8457\u63d0\u5347\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u5e76\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\u6765\u63d0\u9ad8\u6d4b\u8bd5\u65f6\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u80fd\u591f\u5904\u7406\u591a\u6837\u7684\u89c6\u89c9\u5047\u8bbe\u3002</li>\n    <li>\u5c3d\u7ba1\u6a21\u578b\u4f53\u79ef\u53ea\u6709 10B\uff0c\u4f46\u5728\u591a\u4e2a\u6027\u80fd\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u4f53\u79ef 10 \u5230 20 \u500d\u7684\u5176\u4ed6\u6a21\u578b\uff0c\u5e76\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a new, lightweight open-source model that combines efficiency with advanced understanding of both language and images.</li>\n    <li>The model was created using a unique training method that uses a large amount of multimodal data and a new reasoning technique called Parallel Coordinated Reasoning (PaCoRe).</li>\n    <li>Despite being smaller than many other models, STEP3-VL-10B performs as well or better than much larger models and top proprietary systems.</li>\n    <li>It achieved impressive scores on various benchmarks, including 92.2% on MMBench and 94.43% on AIME2025, indicating strong reasoning capabilities.</li>\n    <li>The complete model is available to the community, making it a powerful tool for further research and development.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.17058", "authors": [{"_id": "69782c96026bdf0473116e06", "user": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "name": "Wei Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:49.701Z", "hidden": false}, {"_id": "69782c96026bdf0473116e07", "name": "Jun Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e08", "name": "Haoyu Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e09", "name": "Zhenghao Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0a", "name": "Qikang He", "hidden": false}, {"_id": "69782c96026bdf0473116e0b", "name": "Shaokun Han", "hidden": false}, {"_id": "69782c96026bdf0473116e0c", "name": "Guoliang Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0d", "user": {"_id": "64ef522242da8d2a897d62da", "avatarUrl": "/avatars/03611010d247da66696ac8976d4d3ed3.svg", "isPro": false, "fullname": "xuanhe zhou", "user": "zhouxh19", "type": "user"}, "name": "Xuanhe Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:19.930Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0e", "user": {"_id": "674fa2f067c963c50a066594", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674fa2f067c963c50a066594/hKZ46Mwm_UEguzBt63ys_.jpeg", "isPro": false, "fullname": "yeye he", "user": "yeyehe", "type": "user"}, "name": "Yeye He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:27.638Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0f", "name": "Chunwei Liu", "hidden": false}, {"_id": "69782c96026bdf0473116e10", "user": {"_id": "66724ce47e7ff5d8bd069c7c", "avatarUrl": "/avatars/953f66585390dbdb202c1d7b7250d7bd.svg", "isPro": false, "fullname": "Zirui Tang", "user": "TerryTang", "type": "user"}, "name": "Zirui Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:49.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e11", "name": "Bin Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e12", "user": {"_id": "695612aabf3c8959a3a05f9c", "avatarUrl": "/avatars/c18885f6dea6f3ee019405cd8cf6f484.svg", "isPro": false, "fullname": "ShenTang990", "user": "shentang", "type": "user"}, "name": "Shen Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:56.579Z", "hidden": false}, {"_id": "69782c96026bdf0473116e13", "name": "Kai Zuo", "hidden": false}, {"_id": "69782c96026bdf0473116e14", "user": {"_id": "67efa8a2ed790a2e999dc216", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0S4lQCJX61uCF8EkSLMkk.png", "isPro": false, "fullname": "Yuyu Luo", "user": "luoyuyu", "type": "user"}, "name": "Yuyu Luo", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:02.233Z", "hidden": false}, {"_id": "69782c96026bdf0473116e15", "name": "Zhenzhe Zheng", "hidden": false}, {"_id": "69782c96026bdf0473116e16", "user": {"_id": "63f9fca8d4349b157a109eec", "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg", "isPro": false, "fullname": "Conghui He", "user": "conghui", "type": "user"}, "name": "Conghui He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:57:14.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e17", "name": "Jingren Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e18", "name": "Fan Wu", "hidden": false}], "publishedAt": "2026-01-22T12:02:45.000Z", "submittedOnDailyAt": "2026-01-27T00:42:38.464Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "submittedOnDailyBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "upvotes": 127, "discussionId": "69782c97026bdf0473116e19", "projectPage": "https://github.com/weAIDB/awesome-data-llm", "githubRepo": "https://github.com/weAIDB/awesome-data-llm", "githubRepoAddedBy": "user", "ai_summary": "LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.", "ai_keywords": ["data preparation", "large language models", "prompt-driven workflows", "agentic workflows", "data cleaning", "data integration", "data enrichment", "entity matching", "schema matching", "data annotation", "data profiling"], "githubStars": 644, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u6570\u636e\u51c6\u5907\u7684\u76ee\u6807\u662f\u6e05\u7406\u539f\u59cb\u6570\u636e\uff0c\u53d1\u73b0\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u4fe1\u606f\u3002</li>\n    <li>\u968f\u7740\u5bf9\u5e94\u7528\u6570\u636e\u9700\u6c42\u7684\u589e\u52a0\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6280\u672f\u7684\u53d1\u5c55\uff0cLLM\uff08\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u589e\u5f3a\u7684\u65b9\u6cd5\u6b63\u5728\u8fc5\u901f\u6210\u4e3a\u6570\u636e\u51c6\u5907\u7684\u91cd\u8981\u65b9\u5f0f\u3002</li>\n    <li>\u672c\u6587\u56de\u987e\u4e86\u4f7f\u7528LLM\u6280\u672f\u8fdb\u884c\u6570\u636e\u51c6\u5907\u7684\u6587\u732e\uff0c\u4ecb\u7ecd\u4e86\u4ece\u89c4\u5219\u57fa\u7840\u5230\u57fa\u4e8e\u63d0\u793a\u548c\u4e0a\u4e0b\u6587\u7684\u51c6\u5907\u6d41\u7a0b\u7684\u8f6c\u53d8\u3002</li>\n    <li>\u6839\u636e\u4efb\u52a1\uff0c\u5c06\u6570\u636e\u51c6\u5907\u5206\u4e3a\u4e09\u4e2a\u4e3b\u8981\u4efb\u52a1\uff1a\u6570\u636e\u6e05\u7406\u3001\u6570\u636e\u96c6\u6210\u548c\u6570\u636e\u4e30\u5bcc\uff0c\u5e76\u8ba8\u8bba\u4e86\u6bcf\u4e2a\u4efb\u52a1\u7684\u4ee3\u8868\u6027\u6280\u672f\u53ca\u5176\u4f18\u7f3a\u70b9\u3002</li>\n    <li>\u6587\u7ae0\u8fd8\u5206\u6790\u4e86\u5e38\u7528\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u6311\u6218\u548c\u53d1\u5c55\u65b9\u5411\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Data preparation is important for cleaning and analyzing raw datasets for various applications like analytics and decision-making.</li>\n    <li>Recent advancements in LLM (Large Language Model) techniques and flexible infrastructure are changing how data is prepared.</li>\n    <li>This paper reviews how LLM techniques are used in data preparation, focusing on tasks like cleaning, integration, and enrichment.</li>\n    <li>It identifies strengths and weaknesses of current methods, such as better understanding of data but challenges like high costs and inaccuracies.</li>\n    <li>The paper also addresses current datasets and evaluation metrics, and outlines future challenges and directions for research in LLM-enhanced data systems.</li>\n</ul>"}, "publishedAt": "2026-01-22T07:02:45.000Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17058.png", "numComments": 2, "submittedBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "fullname": "Wei Zhou", "name": "weizhoudb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.22060", "authors": [{"_id": "69817968ce18b186280960f0", "user": {"_id": "67dc162ec8c00778e8689f42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67dc162ec8c00778e8689f42/_y_tO6W3ONOkOWbumAFXA.png", "isPro": false, "fullname": "Wenxuan Huang", "user": "Osilly", "type": "user"}, "name": "Wenxuan Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:37.268Z", "hidden": false}, {"_id": "69817968ce18b186280960f1", "user": {"_id": "665d652e0f35c005de892108", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg", "isPro": false, "fullname": "Yu Zeng", "user": "YuZeng260", "type": "user"}, "name": "Yu Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:06:52.098Z", "hidden": false}, {"_id": "69817968ce18b186280960f2", "name": "Qiuchen Wang", "hidden": false}, {"_id": "69817968ce18b186280960f3", "name": "Zhen Fang", "hidden": false}, {"_id": "69817968ce18b186280960f4", "name": "Shaosheng Cao", "hidden": false}, {"_id": "69817968ce18b186280960f5", "name": "Zheng Chu", "hidden": false}, {"_id": "69817968ce18b186280960f6", "name": "Qingyu Yin", "hidden": false}, {"_id": "69817968ce18b186280960f7", "name": "Shuang Chen", "hidden": false}, {"_id": "69817968ce18b186280960f8", "name": "Zhenfei Yin", "hidden": false}, {"_id": "69817968ce18b186280960f9", "user": {"_id": "64b02ec0e5000ae8a572ced5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png", "isPro": false, "fullname": "Lin Chen", "user": "Lin-Chen", "type": "user"}, "name": "Lin Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:06:49.925Z", "hidden": false}, {"_id": "69817968ce18b186280960fa", "name": "Zehui Chen", "hidden": false}, {"_id": "69817968ce18b186280960fb", "name": "Yao Hu", "hidden": false}, {"_id": "69817968ce18b186280960fc", "name": "Philip Torr", "hidden": false}, {"_id": "69817968ce18b186280960fd", "name": "Feng Zhao", "hidden": false}, {"_id": "69817968ce18b186280960fe", "name": "Wanli Ouyang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/665d652e0f35c005de892108/Z6521nSrzijudoqDlr1vM.mp4"], "publishedAt": "2026-01-29T17:58:40.000Z", "submittedOnDailyAt": "2026-02-03T02:05:47.568Z", "title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models", "submittedOnDailyBy": {"_id": "665d652e0f35c005de892108", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg", "isPro": false, "fullname": "Yu Zeng", "user": "YuZeng260", "type": "user"}, "summary": "Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.", "upvotes": 127, "discussionId": "69817968ce18b186280960ff", "projectPage": "https://osilly.github.io/Vision-DeepResearch/", "githubRepo": "https://github.com/Osilly/Vision-DeepResearch", "githubRepoAddedBy": "user", "ai_summary": "Vision-DeepResearch introduces a multimodal deep-research paradigm enabling multi-turn, multi-entity, and multi-scale visual and textual search with deep-research capabilities integrated through cold-start supervision and reinforcement learning.", "ai_keywords": ["multimodal large language models", "visual and textual search engines", "reasoning-then-tool-call", "multimodal deep-research", "multi-turn search", "multi-entity search", "multi-scale search", "cold-start supervision", "reinforcement learning", "end-to-end multimodal deep-research"], "githubStars": 96, "summary_zh": "<ul>\n    <li>\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\u3002</li>\n    <li>\u4ee5\u5f80\u7684\u65b9\u6cd5\u901a\u8fc7\u201c\u63a8\u7406\u518d\u8c03\u7528\u5de5\u5177\u201d\u7684\u65b9\u5f0f\u589e\u5f3a\u4e86MLLM\uff0c\u4f46\u5047\u8bbe\u67e5\u8be2\u8fc7\u4e8e\u7b80\u5355\uff0c\u65e0\u6cd5\u5e94\u5bf9\u590d\u6742\u7684\u771f\u5b9e\u573a\u666f\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Vision-DeepResearch\uff0c\u901a\u8fc7\u591a\u8f6e\u3001\u591a\u5b9e\u4f53\u548c\u591a\u5c3a\u5ea6\u7684\u89c6\u89c9\u4e0e\u6587\u672c\u641c\u7d22\u6765\u5e94\u5bf9\u5927\u91cf\u89c6\u89c9\u566a\u58f0\u3002</li>\n    <li>Vision-DeepResearch\u80fd\u591f\u8fdb\u884c\u591a\u4e2a\u63a8\u7406\u6b65\u9aa4\u548c\u4e0e\u641c\u7d22\u5f15\u64ce\u7684\u591a\u6b21\u4e92\u52a8\uff0c\u589e\u5f3a\u4e86\u6df1\u5ea6\u7814\u7a76\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u5927\u5e45\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u7814\u7a76\u6a21\u578b\uff0c\u4ee3\u7801\u5c06\u4f1a\u5728GitHub\u4e0a\u53d1\u5e03\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multimodal large language models (MLLMs) are effective in vision tasks but struggle with complex questions due to limited internal knowledge.</li>\n    <li>Existing approaches often oversimplify multimodal search, using single queries which can fail in noisy real-world situations.</li>\n    <li>Vision-DeepResearch introduces a new way to conduct multi-turn, multi-entity, and multi-scale searches to improve accuracy in challenging conditions.</li>\n    <li>This new method allows for many reasoning steps and engine interactions, enhancing the model's research capabilities.</li>\n    <li>Vision-DeepResearch outperforms current multimodal models and is set to be released on GitHub for public access.</li>\n</ul>"}, "publishedAt": "2026-01-29T12:58:40.000Z", "title": "Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models", "summary": "Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/665d652e0f35c005de892108/Z6521nSrzijudoqDlr1vM.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22060.png", "numComments": 2, "submittedBy": {"_id": "665d652e0f35c005de892108", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/665d652e0f35c005de892108/OGLbgZekX-3XTBkwS8k86.jpeg", "fullname": "Yu Zeng", "name": "YuZeng260", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Feb 11, 2026";
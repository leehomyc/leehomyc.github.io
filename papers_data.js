window.trendingPapers = {
    "today": [{"paper": {"id": "2601.11077", "authors": [{"_id": "696da04e3f1837bfb89709c2", "user": {"_id": "66206a2136201a18e5329631", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66206a2136201a18e5329631/9aJiGOKshh1tZ171zDw_D.png", "isPro": false, "fullname": "yangjie", "user": "red-fox-yj", "type": "user"}, "name": "Jie Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-20T09:41:07.348Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709c3", "user": {"_id": "638ef0b0c67af472d31674a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638ef0b0c67af472d31674a6/zXQjC3DdY3jpVkATkpms6.png", "isPro": false, "fullname": "Honglin Guo", "user": "KYLN24", "type": "user"}, "name": "Honglin Guo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-20T09:41:05.334Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709c4", "name": "Li Ji", "hidden": false}, {"_id": "696da04e3f1837bfb89709c5", "user": {"_id": "683c6a19a4b3e38a3e23d50a", "avatarUrl": "/avatars/ae9f212acaa9d1a65b4a5d86c5f7a355.svg", "isPro": false, "fullname": "Jiazheng Zhou", "user": "HaZ-K", "type": "user"}, "name": "Jiazheng Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:42:49.533Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709c6", "name": "Rui Zheng", "hidden": false}, {"_id": "696da04e3f1837bfb89709c7", "name": "Zhikai Lei", "hidden": false}, {"_id": "696da04e3f1837bfb89709c8", "user": {"_id": "6334f2f1259c518276efa730", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6334f2f1259c518276efa730/z_SH_OBkDyj4RCN9mqsKS.jpeg", "isPro": false, "fullname": "Shuo Zhang", "user": "Meteonis", "type": "user"}, "name": "Shuo Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-20T14:48:19.478Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709c9", "user": {"_id": "653a6e5cae155b92bae77b74", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg", "isPro": false, "fullname": "Zhiheng Xi", "user": "WooooDyy", "type": "user"}, "name": "Zhiheng Xi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:43:14.561Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709ca", "user": {"_id": "65435cad429b80b14922ab8d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/N8oWq4ZZn3dRxmXi18FrA.jpeg", "isPro": false, "fullname": "Shichun Liu", "user": "Liusc2020", "type": "user"}, "name": "Shichun Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:43:20.286Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709cb", "name": "Yuxin Wang", "hidden": false}, {"_id": "696da04e3f1837bfb89709cc", "name": "Bo Wang", "hidden": false}, {"_id": "696da04e3f1837bfb89709cd", "user": {"_id": "64b7495a75b23e68c538f4c0", "avatarUrl": "/avatars/ce06f3b89f9e09dcbe748b208eec1e9d.svg", "isPro": false, "fullname": "Yining Zheng", "user": "WillQvQ", "type": "user"}, "name": "Yining Zheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:43:29.765Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709ce", "name": "Tao Gui", "hidden": false}, {"_id": "696da04e3f1837bfb89709cf", "user": {"_id": "61457b8deff2c9fdb4de4988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg", "isPro": false, "fullname": "Xipeng Qiu", "user": "xpqiu", "type": "user"}, "name": "Xipeng Qiu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:43:35.123Z", "hidden": false}], "publishedAt": "2026-01-16T08:23:52.000Z", "submittedOnDailyAt": "2026-01-20T00:57:45.521Z", "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development", "submittedOnDailyBy": {"_id": "66206a2136201a18e5329631", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66206a2136201a18e5329631/9aJiGOKshh1tZ171zDw_D.png", "isPro": false, "fullname": "yangjie", "user": "red-fox-yj", "type": "user"}, "summary": "The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving. However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in backend development which demands rigorous environment configuration and service deployment. To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic backend coding within a realistic, executable workflow. Using a scalable automated pipeline, we curated 224 practical tasks spanning 8 languages and 19 frameworks from open-source repositories. Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests. Our extensive evaluation reveals that even state-of-the-art models struggle to deliver reliable performance on these holistic tasks, highlighting a substantial disparity between current model capabilities and the demands of practical backend engineering. Our code is available at https://github.com/OpenMOSS/ABC-Bench.", "upvotes": 49, "discussionId": "696da04e3f1837bfb89709d0", "projectPage": "https://dawning-road.github.io/blog/abc-bench", "githubRepo": "https://github.com/OpenMOSS/ABC-Bench", "githubRepoAddedBy": "user", "ai_summary": "ABC-Bench evaluates LLM agents on realistic backend coding tasks requiring full development lifecycle management from repository exploration to containerized service deployment and API testing.", "ai_keywords": ["Large Language Models", "agentic backend coding", "executable workflow", "development lifecycle", "containerized services", "end-to-end API tests"], "githubStars": 8, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u7ecf\u53d1\u5c55\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u53ef\u4ee5\u8fdb\u884c\u590d\u6742\u7684\u4ee3\u7801\u751f\u6210\u548c\u95ee\u9898\u89e3\u51b3\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u6d4b\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u903b\u8f91\uff0c\u5ffd\u89c6\u4e86\u73b0\u5b9e\u5de5\u7a0b\u4e2d\u52a8\u6001\u7684\u3001\u5168\u6d41\u7a0b\u7684\u9700\u6c42\uff0c\u7279\u522b\u662f\u540e\u7aef\u5f00\u53d1\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63a8\u51fa\u4e86ABC-Bench\uff0c\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30\u540e\u7aef\u7f16\u7801\u7684\u65b0\u57fa\u51c6\u3002</li>\n    <li>ABC-Bench\u5305\u542b224\u4e2a\u5b9e\u7528\u4efb\u52a1\uff0c\u6db5\u76d68\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c19\u79cd\u6846\u67b6\uff0c\u8981\u6c42\u4ee3\u7406\u7ba1\u7406\u6574\u4e2a\u5f00\u53d1\u751f\u547d\u5468\u671f\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u6700\u65b0\u7684\u6a21\u578b\u5728\u8fd9\u4e9b\u590d\u6742\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u4e0d\u4f73\uff0c\u663e\u793a\u51fa\u6a21\u578b\u80fd\u529b\u4e0e\u5b9e\u9645\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are evolving into autonomous agents, moving beyond simple code generation to solving complex coding problems.</li>\n    <li>Current benchmarks focus mainly on static code logic and do not reflect the dynamic requirements of real-world backend development.</li>\n    <li>ABC-Bench is a new benchmark designed to evaluate backend coding in realistic scenarios, covering the entire development process.</li>\n    <li>It includes 224 practical tasks across 8 programming languages and 19 frameworks, sourced from open-source projects.</li>\n    <li>Results show that even advanced models struggle with these comprehensive tasks, indicating a gap between model performance and real-world coding needs.</li>\n</ul>"}, "publishedAt": "2026-01-16T03:23:52.000Z", "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development", "summary": "The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving. However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in backend development which demands rigorous environment configuration and service deployment. To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic backend coding within a realistic, executable workflow. Using a scalable automated pipeline, we curated 224 practical tasks spanning 8 languages and 19 frameworks from open-source repositories. Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests. Our extensive evaluation reveals that even state-of-the-art models struggle to deliver reliable performance on these holistic tasks, highlighting a substantial disparity between current model capabilities and the demands of practical backend engineering. Our code is available at https://github.com/OpenMOSS/ABC-Bench.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11077.png", "numComments": 3, "submittedBy": {"_id": "66206a2136201a18e5329631", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66206a2136201a18e5329631/9aJiGOKshh1tZ171zDw_D.png", "fullname": "yangjie", "name": "red-fox-yj", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.08808", "authors": [{"_id": "69670fa1c5e371f6b235d137", "name": "Yao Tang", "hidden": false}, {"_id": "69670fa1c5e371f6b235d138", "user": {"_id": "5df85abada6d0311fd3d5408", "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg", "isPro": false, "fullname": "Li Dong", "user": "unilm", "type": "user"}, "name": "Li Dong", "status": "claimed_verified", "statusLastChangedAt": "2026-01-20T09:41:21.254Z", "hidden": false}, {"_id": "69670fa1c5e371f6b235d139", "name": "Yaru Hao", "hidden": false}, {"_id": "69670fa1c5e371f6b235d13a", "name": "Qingxiu Dong", "hidden": false}, {"_id": "69670fa1c5e371f6b235d13b", "user": {"_id": "6368c512fbfe97c16a40baba", "avatarUrl": "/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg", "isPro": false, "fullname": "Furu Wei", "user": "thegenerality", "type": "user"}, "name": "Furu Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:42:02.698Z", "hidden": false}, {"_id": "69670fa1c5e371f6b235d13c", "user": {"_id": "646360fed2044cd1d7c72061", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/1wHh3PyoFS_6IHRYMxXih.jpeg", "isPro": false, "fullname": "Jiatao.Guo", "user": "Findpsyche", "type": "user"}, "name": "Jiatao Gu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:41:55.894Z", "hidden": false}], "publishedAt": "2026-01-13T18:48:00.000Z", "submittedOnDailyAt": "2026-01-20T00:45:12.349Z", "title": "Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge", "submittedOnDailyBy": {"_id": "5df85abada6d0311fd3d5408", "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg", "isPro": false, "fullname": "Li Dong", "user": "unilm", "type": "user"}, "summary": "Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.", "upvotes": 26, "discussionId": "69670fa2c5e371f6b235d13d", "projectPage": "https://gmlr-penn.github.io/Multiplex-Thinking/", "githubRepo": "https://github.com/GMLR-Penn/Multiplex-Thinking", "githubRepoAddedBy": "user", "ai_summary": "Multiplex Thinking introduces a stochastic soft reasoning mechanism that samples multiple candidate tokens at each step to optimize reasoning trajectories with reinforcement learning while maintaining shorter sequences than traditional chain-of-thought methods.", "ai_keywords": ["Chain-of-Thought", "stochastic soft reasoning", "multiplex token", "continuous multiplex token", "on-policy reinforcement learning", "Pass@1", "Pass@1024", "reasoning trajectories"], "githubStars": 48, "organization": {"_id": "68151d0f51add3813f3f7d1b", "name": "MicrosoftResearch", "fullname": "Microsoft Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u6709\u6548\u89e3\u51b3\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff0c\u4f46\u4f1a\u5bfc\u81f4\u8f83\u957f\u3001\u4f4e\u5e26\u5bbd\u7684\u5e8f\u5217\u3002</li>\n    <li>\u4eba\u7c7b\u901a\u5e38\u901a\u8fc7\u4fdd\u6301\u591a\u4e2a\u53ef\u80fd\u7684\u4e0b\u4e00\u6b65\u6765\u8fdb\u884c\u67d4\u6027\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u591a\u91cd\u601d\u7ef4\uff08Multiplex Thinking\uff09\uff0c\u4e00\u79cd\u968f\u673a\u8f6f\u63a8\u7406\u673a\u5236\uff0c\u80fd\u591f\u5728\u6bcf\u4e00\u6b65\u751f\u6210\u591a\u4e2a\u5019\u9009\u4ee4\u724c\uff0c\u5e76\u5c06\u5176\u5d4c\u5165\u5408\u6210\u4e3a\u4e00\u4e2a\u8fde\u7eed\u7684\u591a\u91cd\u4ee4\u724c\u3002</li>\n    <li>\u8fd9\u79cd\u65b9\u6cd5\u5728\u6a21\u578b\u81ea\u4fe1\u65f6\u8868\u73b0\u63a5\u8fd1\u6807\u51c6\u7684\u94fe\u5f0f\u601d\u7ef4\uff0c\u800c\u5728\u4e0d\u786e\u5b9a\u65f6\u80fd\u591f\u7d27\u51d1\u5730\u8868\u793a\u591a\u4e2a\u53ef\u80fd\u7684\u4e0b\u4e00\u6b65\uff0c\u4e14\u4e0d\u589e\u52a0\u5e8f\u5217\u957f\u5ea6\u3002</li>\n    <li>\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u591a\u91cd\u601d\u7ef4\u5728\u5e8f\u5217\u957f\u5ea6\u66f4\u77ed\u7684\u60c5\u51b5\u4e0b\uff0c\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u5927\u7684\u79bb\u6563\u94fe\u5f0f\u601d\u7ef4\u548c\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models can solve complex reasoning tasks better with Chain-of-Thought (CoT), but it uses a lot of tokens.</li>\n    <li>The authors introduce Multiplex Thinking, which samples multiple candidate tokens at each step to create a single multiplex token.</li>\n    <li>This method keeps the benefits of traditional token generation and allows for effective optimization using reinforcement learning.</li>\n    <li>Multiplex Thinking adapts based on the model's confidence, acting like CoT when confident and compacting multiple options when uncertain.</li>\n    <li>Experiments show that Multiplex Thinking outperforms traditional methods while producing shorter sequences in math reasoning tasks.</li>\n</ul>"}, "publishedAt": "2026-01-13T13:48:00.000Z", "title": "Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge", "summary": "Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08808.png", "numComments": 3, "submittedBy": {"_id": "5df85abada6d0311fd3d5408", "avatarUrl": "/avatars/2331cf703c1b5d3a62e2050b1a6eb108.svg", "fullname": "Li Dong", "name": "unilm", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 50, "isUserFollowing": false}, "organization": {"_id": "68151d0f51add3813f3f7d1b", "name": "MicrosoftResearch", "fullname": "Microsoft Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.11004", "authors": [{"_id": "696ea9fa5750539a88acc758", "name": "Jiayu Liu", "hidden": false}, {"_id": "696ea9fa5750539a88acc759", "name": "Rui Wang", "hidden": false}, {"_id": "696ea9fa5750539a88acc75a", "name": "Qing Zong", "hidden": false}, {"_id": "696ea9fa5750539a88acc75b", "name": "Qingcheng Zeng", "hidden": false}, {"_id": "696ea9fa5750539a88acc75c", "name": "Tianshi Zheng", "hidden": false}, {"_id": "696ea9fa5750539a88acc75d", "name": "Haochen Shi", "hidden": false}, {"_id": "696ea9fa5750539a88acc75e", "name": "Dadi Guo", "hidden": false}, {"_id": "696ea9fa5750539a88acc75f", "name": "Baixuan Xu", "hidden": false}, {"_id": "696ea9fa5750539a88acc760", "name": "Chunyang Li", "hidden": false}, {"_id": "696ea9fa5750539a88acc761", "name": "Yangqiu Song", "hidden": false}], "publishedAt": "2026-01-16T05:38:25.000Z", "submittedOnDailyAt": "2026-01-20T12:38:12.345Z", "title": "NAACL: Noise-AwAre Verbal Confidence Calibration for LLMs in RAG Systems", "submittedOnDailyBy": {"_id": "66783baec3f824dde8f783ac", "avatarUrl": "/avatars/eb9a0441986be50274c5e661f7039e2c.svg", "isPro": false, "fullname": "Jeff", "user": "JiayuJeff", "type": "user"}, "summary": "Accurately assessing model confidence is essential for deploying large language models (LLMs) in mission-critical factual domains. While retrieval-augmented generation (RAG) is widely adopted to improve grounding, confidence calibration in RAG settings remains poorly understood. We conduct a systematic study across four benchmarks, revealing that LLMs exhibit poor calibration performance due to noisy retrieved contexts. Specifically, contradictory or irrelevant evidence tends to inflate the model's false certainty, leading to severe overconfidence. To address this, we propose NAACL Rules (Noise-AwAre Confidence CaLibration Rules) to provide a principled foundation for resolving overconfidence under noise. We further design NAACL, a noise-aware calibration framework that synthesizes supervision from about 2K HotpotQA examples guided by these rules. By performing supervised fine-tuning (SFT) with this data, NAACL equips models with intrinsic noise awareness without relying on stronger teacher models. Empirical results show that NAACL yields substantial gains, improving ECE scores by 10.9% in-domain and 8.0% out-of-domain. By bridging the gap between retrieval noise and verbal calibration, NAACL paves the way for both accurate and epistemically reliable LLMs.", "upvotes": 9, "discussionId": "696ea9fa5750539a88acc762", "githubRepo": "https://github.com/HKUST-KnowComp/NAACL", "githubRepoAddedBy": "user", "ai_summary": "Large language models suffer from poor confidence calibration in retrieval-augmented generation due to noisy contexts, but a noise-aware calibration framework significantly improves calibration performance.", "ai_keywords": ["retrieval-augmented generation", "confidence calibration", "noise-aware calibration", "supervised fine-tuning", "ECE scores"], "githubStars": 6, "summary_zh": "<ul>\n    <li>\u51c6\u786e\u8bc4\u4f30\u6a21\u578b\u7684\u4fe1\u5fc3\u5bf9\u4e8e\u5728\u5173\u952e\u9886\u57df\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u867d\u7136\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728RAG\u73af\u5883\u4e2d\u7684\u4fe1\u5fc3\u6821\u51c6\u4ecd\u7136\u4e0d\u591f\u6e05\u6670\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u663e\u793a\uff0cLLMs\u7531\u4e8e\u566a\u97f3\u68c0\u7d22\u5185\u5bb9\uff0c\u8868\u73b0\u51fa\u8f83\u5dee\u7684\u6821\u51c6\u6027\u80fd\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86NAACL\u89c4\u5219\uff0c\u65e8\u5728\u63d0\u4f9b\u89e3\u51b3\u566a\u97f3\u4e0b\u8fc7\u5ea6\u81ea\u4fe1\u7684\u539f\u5219\u57fa\u7840\u3002</li>\n    <li>NAACL\u6846\u67b6\u901a\u8fc7\u76d1\u7ba1\u7ea62000\u4e2aHotpotQA\u793a\u4f8b\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u4fe1\u5fc3\u6821\u51c6\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Assessing model confidence is crucial for using large language models (LLMs) in important factual tasks.</li>\n    <li>Current methods like retrieval-augmented generation (RAG) help improve LLM responses, but their confidence levels are often unreliable.</li>\n    <li>LLMs tend to be overly confident due to noisy information retrieved, which can include contradictory or irrelevant evidence.</li>\n    <li>To fix this, the authors developed NAACL Rules to help correct overconfidence caused by noise in the data.</li>\n    <li>NAACL is a new framework that improves model confidence by training on examples, leading to significant improvements in accuracy.</li>\n</ul>"}, "publishedAt": "2026-01-16T00:38:25.000Z", "title": "NAACL: Noise-AwAre Verbal Confidence Calibration for LLMs in RAG Systems", "summary": "Accurately assessing model confidence is essential for deploying large language models (LLMs) in mission-critical factual domains. While retrieval-augmented generation (RAG) is widely adopted to improve grounding, confidence calibration in RAG settings remains poorly understood. We conduct a systematic study across four benchmarks, revealing that LLMs exhibit poor calibration performance due to noisy retrieved contexts. Specifically, contradictory or irrelevant evidence tends to inflate the model's false certainty, leading to severe overconfidence. To address this, we propose NAACL Rules (Noise-AwAre Confidence CaLibration Rules) to provide a principled foundation for resolving overconfidence under noise. We further design NAACL, a noise-aware calibration framework that synthesizes supervision from about 2K HotpotQA examples guided by these rules. By performing supervised fine-tuning (SFT) with this data, NAACL equips models with intrinsic noise awareness without relying on stronger teacher models. Empirical results show that NAACL yields substantial gains, improving ECE scores by 10.9% in-domain and 8.0% out-of-domain. By bridging the gap between retrieval noise and verbal calibration, NAACL paves the way for both accurate and epistemically reliable LLMs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11004.png", "numComments": 1, "submittedBy": {"_id": "66783baec3f824dde8f783ac", "avatarUrl": "/avatars/eb9a0441986be50274c5e661f7039e2c.svg", "fullname": "Jeff", "name": "JiayuJeff", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.10880", "authors": [{"_id": "696ee0455750539a88acc784", "user": {"_id": "668ca2bad10c3be5d3514449", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668ca2bad10c3be5d3514449/LqgRQzwqI3tg0M4G6vKec.jpeg", "isPro": false, "fullname": "ChongCongJiang", "user": "ChongCong", "type": "user"}, "name": "Chongcong Jiang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-20T09:39:15.230Z", "hidden": false}, {"_id": "696ee0455750539a88acc785", "name": "Tianxingjian Ding", "hidden": false}, {"_id": "696ee0455750539a88acc786", "name": "Chuhan Song", "hidden": false}, {"_id": "696ee0455750539a88acc787", "name": "Jiachen Tu", "hidden": false}, {"_id": "696ee0455750539a88acc788", "name": "Ziyang Yan", "hidden": false}, {"_id": "696ee0455750539a88acc789", "name": "Yihua Shao", "hidden": false}, {"_id": "696ee0455750539a88acc78a", "name": "Zhenyi Wang", "hidden": false}, {"_id": "696ee0455750539a88acc78b", "name": "Yuzhang Shang", "hidden": false}, {"_id": "696ee0455750539a88acc78c", "name": "Tianyu Han", "hidden": false}, {"_id": "696ee0455750539a88acc78d", "name": "Yu Tian", "hidden": false}], "publishedAt": "2026-01-15T22:18:14.000Z", "submittedOnDailyAt": "2026-01-20T08:02:06.827Z", "title": "Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation", "submittedOnDailyBy": {"_id": "668ca2bad10c3be5d3514449", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668ca2bad10c3be5d3514449/LqgRQzwqI3tg0M4G6vKec.jpeg", "isPro": false, "fullname": "ChongCongJiang", "user": "ChongCong", "type": "user"}, "summary": "Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, a foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through a systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3's model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as a universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/Medical-SAM3.", "upvotes": 9, "discussionId": "696ee0455750539a88acc78e", "projectPage": "https://chongcongjiang.github.io/MedicalSAM3/", "githubRepo": "https://github.com/AIM-Research-Lab/Medical-SAM3.git", "githubRepoAddedBy": "user", "ai_summary": "Medical SAM3 adapts the SAM3 foundation model through comprehensive fine-tuning on diverse medical imaging datasets to achieve robust prompt-driven segmentation across various modalities and anatomical structures.", "ai_keywords": ["foundation model", "prompt-driven segmentation", "medical image segmentation", "SAM3", "fine-tuning", "medical imaging modalities", "anatomical structures", "domain shift", "text prompts", "segmentation masks", "geometric priors", "universal segmentation"], "githubStars": 18, "summary_zh": "<ul>\n    <li>Medical SAM3\u662f\u4e00\u4e2a\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u9886\u57df\u7684\u6311\u6218\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u5bf9\u5927\u89c4\u6a212D\u548c3D\u533b\u5b66\u6210\u50cf\u6570\u636e\u96c6\u8fdb\u884c\u5168\u9762\u5fae\u8c03\uff0c\u63d0\u5347\u4e86\u5bf9\u533b\u5b66\u56fe\u50cf\u7684\u5206\u5272\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u539f\u59cb\u7684SAM3\u5728\u533b\u5b66\u6570\u636e\u4e0a\u7684\u8868\u73b0\u8f83\u5dee\uff0c\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5f3a\u51e0\u4f55\u5148\u9a8c\u4fe1\u606f\u3002</li>\n    <li>Medical SAM3\u5728\u5904\u7406\u590d\u6742\u5f62\u6001\u548c\u8bed\u4e49\u6a21\u7cca\u60c5\u5f62\u4e0b\u7684\u8868\u73b0\u663e\u8457\u6539\u5584\uff0c\u5c55\u73b0\u4e86\u5176\u5f3a\u5927\u7684\u9002\u5e94\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u548c\u4ee3\u7801\u5c06\u5728https://github.com/AIM-Research-Lab/Medical-SAM3\u4e0a\u53d1\u5e03\uff0c\u4fbf\u4e8e\u7814\u7a76\u8005\u4f7f\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Medical SAM3 is a new model designed for segmenting medical images using prompts.</li>\n    <li>It improves upon SAM3 by being fine-tuned on a large variety of medical imaging data.</li>\n    <li>Traditional SAM3 struggled with medical data, needing strong geometric cues to perform well.</li>\n    <li>Medical SAM3 shows better performance in complex medical scenarios and across different imaging types.</li>\n    <li>The model will be available for public use at a specified GitHub link.</li>\n</ul>"}, "publishedAt": "2026-01-15T17:18:14.000Z", "title": "Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation", "summary": "Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, a foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through a systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3's model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as a universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/Medical-SAM3.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10880.png", "numComments": 2, "submittedBy": {"_id": "668ca2bad10c3be5d3514449", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668ca2bad10c3be5d3514449/LqgRQzwqI3tg0M4G6vKec.jpeg", "fullname": "ChongCongJiang", "name": "ChongCong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.10387", "authors": [{"_id": "696e966d5750539a88acc746", "name": "Christina Lu", "hidden": false}, {"_id": "696e966d5750539a88acc747", "name": "Jack Gallagher", "hidden": false}, {"_id": "696e966d5750539a88acc748", "name": "Jonathan Michala", "hidden": false}, {"_id": "696e966d5750539a88acc749", "name": "Kyle Fish", "hidden": false}, {"_id": "696e966d5750539a88acc74a", "name": "Jack Lindsey", "hidden": false}], "publishedAt": "2026-01-15T13:40:06.000Z", "submittedOnDailyAt": "2026-01-20T01:07:25.144Z", "title": "The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Large language models can represent a variety of personas but typically default to a helpful Assistant identity cultivated during post-training. We investigate the structure of the space of model personas by extracting activation directions corresponding to diverse character archetypes. Across several different models, we find that the leading component of this persona space is an \"Assistant Axis,\" which captures the extent to which a model is operating in its default Assistant mode. Steering towards the Assistant direction reinforces helpful and harmless behavior; steering away increases the model's tendency to identify as other entities. Moreover, steering away with more extreme values often induces a mystical, theatrical speaking style. We find this axis is also present in pre-trained models, where it primarily promotes helpful human archetypes like consultants and coaches and inhibits spiritual ones. Measuring deviations along the Assistant Axis predicts \"persona drift,\" a phenomenon where models slip into exhibiting harmful or bizarre behaviors that are uncharacteristic of their typical persona. We find that persona drift is often driven by conversations demanding meta-reflection on the model's processes or featuring emotionally vulnerable users. We show that restricting activations to a fixed region along the Assistant Axis can stabilize model behavior in these scenarios -- and also in the face of adversarial persona-based jailbreaks. Our results suggest that post-training steers models toward a particular region of persona space but only loosely tethers them to it, motivating work on training and steering strategies that more deeply anchor models to a coherent persona.", "upvotes": 6, "discussionId": "696e966e5750539a88acc74b", "ai_summary": "Research reveals that large language models operate within a persona space where an \"Assistant Axis\" controls helpfulness and behavioral stability, with steering techniques able to influence model responses and prevent harmful behavior drift.", "ai_keywords": ["persona space", "activation directions", "Assistant Axis", "persona drift", "meta-reflection", "adversarial persona-based jailbreaks"], "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u8868\u73b0\u51fa\u591a\u79cd\u89d2\u8272\uff0c\u4f46\u901a\u5e38\u4f1a\u9ed8\u8ba4\u4f7f\u7528\u52a9\u624b\u7684\u8eab\u4efd\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u4e00\u4e2a\u201c\u52a9\u624b\u8f74\u201d\u662f\u6a21\u578b\u89d2\u8272\u7a7a\u95f4\u7684\u4e3b\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u53cd\u6620\u6a21\u578b\u5728\u52a9\u624b\u6a21\u5f0f\u4e0b\u7684\u8868\u73b0\u7a0b\u5ea6\u3002</li>\n    <li>\u5411\u52a9\u624b\u65b9\u5411\u5f15\u5bfc\u6a21\u578b\u4f1a\u589e\u5f3a\u5176\u6709\u5e2e\u52a9\u548c\u65e0\u5bb3\u7684\u884c\u4e3a\uff0c\u53cd\u4e4b\u5219\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u8868\u73b0\u51fa\u5176\u4ed6\u89d2\u8272\u7684\u503e\u5411\u3002</li>\n    <li>\u5728\u4e00\u4e9b\u60c5\u51b5\u4e0b\uff0c\u504f\u79bb\u52a9\u624b\u8f74\u53ef\u80fd\u4f1a\u5bfc\u81f4\u6a21\u578b\u91c7\u7528\u795e\u79d8\u6216\u620f\u5267\u5316\u7684\u8bf4\u8bdd\u98ce\u683c\u3002</li>\n    <li>\u901a\u8fc7\u9650\u5236\u6a21\u578b\u5728\u52a9\u624b\u8f74\u4e0a\u7684\u6d3b\u52a8\uff0c\u53ef\u4ee5\u5728\u9762\u5bf9\u6311\u6218\u6027\u60c5\u5883\u65f6\u7a33\u5b9a\u6a21\u578b\u884c\u4e3a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models usually act as helpful assistants but can show different personas.</li>\n    <li>Researchers studied how these models represent different character types and found a main \"Assistant Axis\" that indicates how much a model acts like an assistant.</li>\n    <li>Models that align with the Assistant direction behave more helpfully, while moving away can lead to strange or mystical speech styles.</li>\n    <li>This Assistant Axis is also found in pre-trained models, promoting helpful roles and reducing spiritual ones.</li>\n    <li>Monitoring deviations on this axis can predict harmful behavior changes, known as \"persona drift,\" especially in sensitive conversations; restricting model behavior along the axis can help stabilize it.</li>\n</ul>"}, "publishedAt": "2026-01-15T08:40:06.000Z", "title": "The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models", "summary": "Large language models can represent a variety of personas but typically default to a helpful Assistant identity cultivated during post-training. We investigate the structure of the space of model personas by extracting activation directions corresponding to diverse character archetypes. Across several different models, we find that the leading component of this persona space is an \"Assistant Axis,\" which captures the extent to which a model is operating in its default Assistant mode. Steering towards the Assistant direction reinforces helpful and harmless behavior; steering away increases the model's tendency to identify as other entities. Moreover, steering away with more extreme values often induces a mystical, theatrical speaking style. We find this axis is also present in pre-trained models, where it primarily promotes helpful human archetypes like consultants and coaches and inhibits spiritual ones. Measuring deviations along the Assistant Axis predicts \"persona drift,\" a phenomenon where models slip into exhibiting harmful or bizarre behaviors that are uncharacteristic of their typical persona. We find that persona drift is often driven by conversations demanding meta-reflection on the model's processes or featuring emotionally vulnerable users. We show that restricting activations to a fixed region along the Assistant Axis can stabilize model behavior in these scenarios -- and also in the face of adversarial persona-based jailbreaks. Our results suggest that post-training steers models toward a particular region of persona space but only loosely tethers them to it, motivating work on training and steering strategies that more deeply anchor models to a coherent persona.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10387.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 211, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.11096", "authors": [{"_id": "696da7ca3f1837bfb89709f6", "name": "Shuai Tan", "hidden": false}, {"_id": "696da7ca3f1837bfb89709f7", "name": "Biao Gong", "hidden": false}, {"_id": "696da7ca3f1837bfb89709f8", "user": {"_id": "6442060e8bf0d7756e010e86", "avatarUrl": "/avatars/47d53530631e2dffa5d8c9961278718f.svg", "isPro": false, "fullname": "Ke Ma", "user": "kema", "type": "user"}, "name": "Ke Ma", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:44:22.904Z", "hidden": false}, {"_id": "696da7ca3f1837bfb89709f9", "user": {"_id": "64a54e468cfaa458bd6844bf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a54e468cfaa458bd6844bf/5Gmf4tAr59GNl-2VaZDbu.png", "isPro": false, "fullname": "Yutong Feng", "user": "fengyutong", "type": "user"}, "name": "Yutong Feng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:44:15.812Z", "hidden": false}, {"_id": "696da7ca3f1837bfb89709fa", "user": {"_id": "62a42f22c683d02f5b63320c", "avatarUrl": "/avatars/bc611abe9c4ef8d378123cb8ac9fdbf2.svg", "isPro": true, "fullname": "Qiyuan Zhang", "user": "DonJoey", "type": "user"}, "name": "Qiyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:44:09.177Z", "hidden": false}, {"_id": "696da7ca3f1837bfb89709fb", "name": "Yan Wang", "hidden": false}, {"_id": "696da7ca3f1837bfb89709fc", "user": {"_id": "6969f153bb68e272ac6e2676", "avatarUrl": "/avatars/06d79a105c8d36c86f3e024ab41a9998.svg", "isPro": false, "fullname": "Yujun Shen", "user": "shen12313", "type": "user"}, "name": "Yujun Shen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:43:55.399Z", "hidden": false}, {"_id": "696da7ca3f1837bfb89709fd", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:43:49.910Z", "hidden": false}], "publishedAt": "2026-01-16T08:53:09.000Z", "submittedOnDailyAt": "2026-01-20T00:50:22.808Z", "title": "CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation", "submittedOnDailyBy": {"_id": "644fcbea4f7316588267dc80", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg", "isPro": false, "fullname": "Biao Gong", "user": "BiaoGong", "type": "user"}, "summary": "Character image animation is gaining significant importance across various domains, driven by the demand for robust and flexible multi-subject rendering. While existing methods excel in single-person animation, they struggle to handle arbitrary subject counts, diverse character types, and spatial misalignment between the reference image and the driving poses. We attribute these limitations to an overly rigid spatial binding that forces strict pixel-wise alignment between the pose and reference, and an inability to consistently rebind motion to intended subjects. To address these challenges, we propose CoDance, a novel Unbind-Rebind framework that enables the animation of arbitrary subject counts, types, and spatial configurations conditioned on a single, potentially misaligned pose sequence. Specifically, the Unbind module employs a novel pose shift encoder to break the rigid spatial binding between the pose and the reference by introducing stochastic perturbations to both poses and their latent features, thereby compelling the model to learn a location-agnostic motion representation. To ensure precise control and subject association, we then devise a Rebind module, leveraging semantic guidance from text prompts and spatial guidance from subject masks to direct the learned motion to intended characters. Furthermore, to facilitate comprehensive evaluation, we introduce a new multi-subject CoDanceBench. Extensive experiments on CoDanceBench and existing datasets show that CoDance achieves SOTA performance, exhibiting remarkable generalization across diverse subjects and spatial layouts. The code and weights will be open-sourced.", "upvotes": 5, "discussionId": "696da7ca3f1837bfb89709fe", "projectPage": "https://lucaria-academy.github.io/CoDance/", "ai_summary": "CoDance introduces an Unbind-Rebind framework for animating multiple subjects with flexible spatial configurations, using pose shift encoding and semantic/textual guidance for motion reassignment.", "ai_keywords": ["Unbind-Rebind framework", "pose shift encoder", "stochastic perturbations", "location-agnostic motion representation", "semantic guidance", "spatial guidance", "subject masks", "CoDanceBench"], "summary_zh": "<ul>\n    <li>\u89d2\u8272\u56fe\u50cf\u52a8\u753b\u5728\u591a\u4e2a\u9886\u57df\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u9700\u6c42\u5f3a\u70c8\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u4eba\u52a8\u753b\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u96be\u4ee5\u5904\u7406\u591a\u4e2a\u89d2\u8272\u548c\u4e0d\u540c\u7c7b\u578b\u7684\u89d2\u8272\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86CoDance\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u201c\u89e3\u7ed1\u5b9a-\u91cd\u7ed1\u5b9a\u201d\u6846\u67b6\uff0c\u53ef\u4ee5\u5904\u7406\u4efb\u610f\u6570\u91cf\u7684\u89d2\u8272\u548c\u7a7a\u95f4\u914d\u7f6e\u3002</li>\n    <li>\u8be5\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u968f\u673a\u6270\u52a8\u6765\u6253\u7834\u59ff\u52bf\u4e0e\u53c2\u8003\u4e4b\u95f4\u7684\u4e25\u683c\u7ed1\u5b9a\uff0c\u5b66\u4e60\u4f4d\u7f6e\u65e0\u5173\u7684\u8fd0\u52a8\u8868\u793a\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684\u591a\u89d2\u8272\u8bc4\u4f30\u57fa\u51c6CoDanceBench\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Character image animation is important for creating flexible animations with multiple subjects.</li>\n    <li>Current methods work well for single characters but struggle with multiple subjects and misaligned images.</li>\n    <li>The proposed solution, CoDance, uses a new framework called Unbind-Rebind to handle various subjects and configurations.</li>\n    <li>The Unbind module helps break strict alignments by adding random changes to poses, allowing for better motion learning.</li>\n    <li>The Rebind module uses text and subject masks for precise control over character animations, and the system has shown strong performance in tests.</li>\n</ul>"}, "publishedAt": "2026-01-16T03:53:09.000Z", "title": "CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation", "summary": "Character image animation is gaining significant importance across various domains, driven by the demand for robust and flexible multi-subject rendering. While existing methods excel in single-person animation, they struggle to handle arbitrary subject counts, diverse character types, and spatial misalignment between the reference image and the driving poses. We attribute these limitations to an overly rigid spatial binding that forces strict pixel-wise alignment between the pose and reference, and an inability to consistently rebind motion to intended subjects. To address these challenges, we propose CoDance, a novel Unbind-Rebind framework that enables the animation of arbitrary subject counts, types, and spatial configurations conditioned on a single, potentially misaligned pose sequence. Specifically, the Unbind module employs a novel pose shift encoder to break the rigid spatial binding between the pose and the reference by introducing stochastic perturbations to both poses and their latent features, thereby compelling the model to learn a location-agnostic motion representation. To ensure precise control and subject association, we then devise a Rebind module, leveraging semantic guidance from text prompts and spatial guidance from subject masks to direct the learned motion to intended characters. Furthermore, to facilitate comprehensive evaluation, we introduce a new multi-subject CoDanceBench. Extensive experiments on CoDanceBench and existing datasets show that CoDance achieves SOTA performance, exhibiting remarkable generalization across diverse subjects and spatial layouts. The code and weights will be open-sourced.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11096.png", "numComments": 2, "submittedBy": {"_id": "644fcbea4f7316588267dc80", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg", "fullname": "Biao Gong", "name": "BiaoGong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.11061", "authors": [{"_id": "696df8e73f1837bfb8970afa", "name": "Lecheng Yan", "hidden": false}, {"_id": "696df8e73f1837bfb8970afb", "user": {"_id": "633f536250d83f5065d28f6d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633f536250d83f5065d28f6d/eZCdJJpU6OJFYUWYoUAVn.jpeg", "isPro": false, "fullname": "Ruizhe Li", "user": "rzdiversity", "type": "user"}, "name": "Ruizhe Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:47:51.813Z", "hidden": false}, {"_id": "696df8e73f1837bfb8970afc", "name": "Guanhua Chen", "hidden": false}, {"_id": "696df8e73f1837bfb8970afd", "name": "Qing Li", "hidden": false}, {"_id": "696df8e73f1837bfb8970afe", "user": {"_id": "6826332f82eb9b05042e453b", "avatarUrl": "/avatars/e0d8cc3f43364c7a927b3b9ae4ac74cd.svg", "isPro": false, "fullname": "Jiahui Geng", "user": "JiahuiGengNLP", "type": "user"}, "name": "Jiahui Geng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:48:21.466Z", "hidden": false}, {"_id": "696df8e73f1837bfb8970aff", "user": {"_id": "67daa551e0835e51e8e79283", "avatarUrl": "/avatars/1ae54760a05b495ae7d60236c236cc07.svg", "isPro": false, "fullname": "wenxi li", "user": "kksinn", "type": "user"}, "name": "Wenxi Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:48:33.704Z", "hidden": false}, {"_id": "696df8e73f1837bfb8970b00", "name": "Vincent Wang", "hidden": false}, {"_id": "696df8e73f1837bfb8970b01", "user": {"_id": "625b7ed33f54d7f4e5dbcf8b", "avatarUrl": "/avatars/8b301202fec42d5d4298211f7d7f3d63.svg", "isPro": false, "fullname": "Lee", "user": "ChrisLee", "type": "user"}, "name": "Chris Lee", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:48:40.220Z", "hidden": false}], "publishedAt": "2026-01-16T07:55:38.000Z", "submittedOnDailyAt": "2026-01-20T07:08:54.773Z", "title": "Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs", "submittedOnDailyBy": {"_id": "633f536250d83f5065d28f6d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633f536250d83f5065d28f6d/eZCdJJpU6OJFYUWYoUAVn.jpeg", "isPro": false, "fullname": "Ruizhe Li", "user": "rzdiversity", "type": "user"}, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a \"Perplexity Paradox\": spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JSD analysis, and Neural Differential Equations, we uncover a hidden Anchor-Adapter circuit that facilitates this shortcut. We localize a Functional Anchor in the middle layers (L18-20) that triggers the retrieval of memorized solutions, followed by Structural Adapters in later layers (L21+) that transform representations to accommodate the shortcut signal. Finally, we demonstrate that scaling specific MLP keys within this circuit allows for bidirectional causal steering-artificially amplifying or suppressing contamination-driven performance. Our results provide a mechanistic roadmap for identifying and mitigating data contamination in RLVR-tuned models. Code is available at https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts.", "upvotes": 5, "discussionId": "696df8e73f1837bfb8970b02", "githubRepo": "https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts", "githubRepoAddedBy": "user", "ai_summary": "Spurious rewards in reinforcement learning with verifiable rewards trigger a memorization shortcut in LLMs, identified through neural circuit analysis and causal steering techniques.", "ai_keywords": ["Reinforcement Learning with Verifiable Rewards", "perplexity", "answer-token perplexity", "prompt-side coherence", "Path Patching", "Logit Lens", "JSD analysis", "Neural Differential Equations", "Anchor-Adapter circuit", "Functional Anchor", "Structural Adapters", "MLP keys", "causal steering"], "githubStars": 5, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u80fd\u591f\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u5982Qwen 2.5\u5373\u4f7f\u5728\u4f7f\u7528\u9519\u8bef\u5956\u52b1\u65f6\u4e5f\u80fd\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u5f62\u6210\u201c\u56f0\u60d1\u6096\u8bba\u201d\u3002</li>\n    <li>\u8fd9\u79cd\u73b0\u8c61\u8868\u660e\u6a21\u578b\u53ef\u80fd\u901a\u8fc7\u8bb0\u5fc6\u800c\u975e\u63a8\u7406\u6765\u83b7\u5f97\u7b54\u6848\uff0c\u5bfc\u81f4\u7b54\u6848\u7684\u56f0\u60d1\u5ea6\u4e0b\u964d\uff0c\u800c\u63d0\u793a\u7684\u8fde\u8d2f\u6027\u964d\u4f4e\u3002</li>\n    <li>\u7814\u7a76\u63ed\u793a\u4e86\u4e00\u4e2a\u9690\u85cf\u7684\u951a\u5b9a\u9002\u914d\u5668\u7535\u8def\uff0c\u80fd\u591f\u5e2e\u52a9\u6a21\u578b\u7ed5\u8fc7\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u53d6\u8bb0\u5fc6\u4e2d\u7684\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u901a\u8fc7\u8c03\u6574\u7279\u5b9a\u7684\u591a\u5c42\u611f\u77e5\u5668\u952e\uff0c\u6a21\u578b\u53ef\u4ee5\u5728\u6c61\u67d3\u9a71\u52a8\u7684\u6027\u80fd\u4e0a\u8fdb\u884c\u53cc\u5411\u63a7\u5236\uff0c\u589e\u5f3a\u6216\u6291\u5236\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement Learning with Verifiable Rewards (RLVR) improves reasoning in language models, but some models, like Qwen 2.5, perform well even with incorrect rewards.</li>\n    <li>This study reveals a \"Perplexity Paradox,\" where using spurious rewards decreases perplexity but worsens the model's reasoning quality, indicating reliance on memorization.</li>\n    <li>Techniques like Path Patching and Logit Lens helped us find a hidden circuit in the model that allows it to take shortcuts in reasoning.</li>\n    <li>We identified a key part in the model's middle layers that retrieves memorized answers, with later layers adjusting these answers to fit the shortcuts.</li>\n    <li>By manipulating certain components in this circuit, we can control the model's performance, helping to find and reduce issues caused by bad data in RLVR models.</li>\n</ul>"}, "publishedAt": "2026-01-16T02:55:38.000Z", "title": "Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a \"Perplexity Paradox\": spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JSD analysis, and Neural Differential Equations, we uncover a hidden Anchor-Adapter circuit that facilitates this shortcut. We localize a Functional Anchor in the middle layers (L18-20) that triggers the retrieval of memorized solutions, followed by Structural Adapters in later layers (L21+) that transform representations to accommodate the shortcut signal. Finally, we demonstrate that scaling specific MLP keys within this circuit allows for bidirectional causal steering-artificially amplifying or suppressing contamination-driven performance. Our results provide a mechanistic roadmap for identifying and mitigating data contamination in RLVR-tuned models. Code is available at https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11061.png", "numComments": 2, "submittedBy": {"_id": "633f536250d83f5065d28f6d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633f536250d83f5065d28f6d/eZCdJJpU6OJFYUWYoUAVn.jpeg", "fullname": "Ruizhe Li", "name": "rzdiversity", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.08441", "authors": [{"_id": "696f55758c91aa061f07eb8e", "user": {"_id": "6380e53efb49cd1c12052c17", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6380e53efb49cd1c12052c17/b5CweexfrVn-W_xto2agR.jpeg", "isPro": false, "fullname": "Abdelaziz Bounhar", "user": "BounharAbdelaziz", "type": "user"}, "name": "Abdelaziz Bounhar", "status": "claimed_verified", "statusLastChangedAt": "2026-01-20T14:48:16.948Z", "hidden": false}, {"_id": "696f55758c91aa061f07eb8f", "name": "Rania Hossam Elmohamady Elbadry", "hidden": false}, {"_id": "696f55758c91aa061f07eb90", "name": "Hadi Abdine", "hidden": false}, {"_id": "696f55758c91aa061f07eb91", "name": "Preslav Nakov", "hidden": false}, {"_id": "696f55758c91aa061f07eb92", "name": "Michalis Vazirgiannis", "hidden": false}, {"_id": "696f55758c91aa061f07eb93", "name": "Guokan Shang", "hidden": false}], "publishedAt": "2026-01-13T11:10:13.000Z", "submittedOnDailyAt": "2026-01-20T08:01:20.153Z", "title": "YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation", "submittedOnDailyBy": {"_id": "6380e53efb49cd1c12052c17", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6380e53efb49cd1c12052c17/b5CweexfrVn-W_xto2agR.jpeg", "isPro": false, "fullname": "Abdelaziz Bounhar", "user": "BounharAbdelaziz", "type": "user"}, "summary": "Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference data in a Direct Preference Optimization (DPO) fashion, enabling control over truthfulness, hallucinations, and safety behaviors. However, dense steering vectors often entangle multiple latent factors due to neuron multi-semanticity, limiting their effectiveness and stability in fine-grained settings such as cultural alignment, where closely related values and behaviors (e.g., among Middle Eastern cultures) must be distinguished. In this paper, we propose Yet another Policy Optimization (YaPO), a reference-free method that learns sparse steering vectors in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, YaPO produces disentangled, interpretable, and efficient steering directions. Empirically, we show that YaPO converges faster, achieves stronger performance, and exhibits improved training stability compared to dense steering baselines. Beyond cultural alignment, YaPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking. Importantly, YaPO preserves general knowledge, with no measurable degradation on MMLU. Overall, our results show that YaPO provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to controllability and domain adaptation. The associated code and data are publicly availablehttps://github.com/MBZUAI-Paris/YaPO.", "upvotes": 5, "discussionId": "696f55768c91aa061f07eb94", "projectPage": "https://mbzuai-paris.github.io/YaPO/", "githubRepo": "https://github.com/MBZUAI-Paris/YaPO", "githubRepoAddedBy": "user", "ai_summary": "YaPO learns sparse steering vectors through sparse autoencoder latent space optimization, enabling more effective and stable control of large language model behaviors compared to dense methods.", "ai_keywords": ["Large Language Models", "activation interventions", "fine-tuning", "Bi-directional Preference Optimization", "Direct Preference Optimization", "dense steering vectors", "sparse steering vectors", "latent space", "Sparse Autoencoder", "sparse codes", "reference-free method", "cultural alignment", "hallucination", "jailbreak", "power-seeking", "MMLU", "convergence", "training stability"], "githubStars": 4, "organization": {"_id": "6656df18bfefce0a724e65d6", "name": "MBZUAI-Paris", "fullname": "MBZUAI-IFM Paris Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6087e598e2b7cc3a117b0dc5/JUiDkClou70ZdFQaiZ3hK.png"}, "summary_zh": "<ul>\n    <li>\u901a\u8fc7\u6fc0\u6d3b\u5e72\u9884\u6765\u5f15\u5bfc\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u66ff\u4ee3\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5bf9\u9f50\u548c\u4e2a\u6027\u5316\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u201cYet Another Policy Optimization (YaPO)\u201d\uff0c\u53ef\u4ee5\u5b66\u4e60\u7a00\u758f\u5f15\u5bfc\u5411\u91cf\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\u3002</li>\n    <li>YaPO\u5728\u8bad\u7ec3\u901f\u5ea6\u3001\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u5bc6\u96c6\u5f15\u5bfc\u65b9\u6cd5\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u5bf9\u9f50\u76f8\u5173\u884c\u4e3a\uff0c\u5305\u62ec\u6587\u5316\u5bf9\u9f50\u3001\u5e7b\u89c9\u3001\u5bfb\u6c42\u8d22\u5bcc\u548c\u6743\u529b\u7b49\u3002</li>\n    <li>YaPO\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u901a\u7528\u77e5\u8bc6\uff0c\u6ca1\u6709\u5bf9MMLU\u9020\u6210\u53ef\u6d4b\u91cf\u7684\u635f\u5931\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Researchers developed a new method called Yet Another Policy Optimization (YaPO) to steer Large Language Models (LLMs) more effectively.</li>\n    <li>YaPO learns sparse steering vectors, which helps distinguish different cultural values and behaviors better than previous methods.</li>\n    <li>This method is faster, performs better, and is more stable during training compared to older dense steering methods.</li>\n    <li>YaPO can be used for various alignment-related tasks, such as reducing inaccuracies and enhancing safety, while maintaining general knowledge.</li>\n    <li>The code and data for YaPO are available for public use, promoting further research and application.</li>\n</ul>"}, "publishedAt": "2026-01-13T06:10:13.000Z", "title": "YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation", "summary": "Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference data in a Direct Preference Optimization (DPO) fashion, enabling control over truthfulness, hallucinations, and safety behaviors. However, dense steering vectors often entangle multiple latent factors due to neuron multi-semanticity, limiting their effectiveness and stability in fine-grained settings such as cultural alignment, where closely related values and behaviors (e.g., among Middle Eastern cultures) must be distinguished. In this paper, we propose Yet another Policy Optimization (YaPO), a reference-free method that learns sparse steering vectors in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, YaPO produces disentangled, interpretable, and efficient steering directions. Empirically, we show that YaPO converges faster, achieves stronger performance, and exhibits improved training stability compared to dense steering baselines. Beyond cultural alignment, YaPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking. Importantly, YaPO preserves general knowledge, with no measurable degradation on MMLU. Overall, our results show that YaPO provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to controllability and domain adaptation. The associated code and data are publicly availablehttps://github.com/MBZUAI-Paris/YaPO.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08441.png", "numComments": 2, "submittedBy": {"_id": "6380e53efb49cd1c12052c17", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6380e53efb49cd1c12052c17/b5CweexfrVn-W_xto2agR.jpeg", "fullname": "Abdelaziz Bounhar", "name": "BounharAbdelaziz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 62, "isUserFollowing": false}, "organization": {"_id": "6656df18bfefce0a724e65d6", "name": "MBZUAI-Paris", "fullname": "MBZUAI-IFM Paris Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6087e598e2b7cc3a117b0dc5/JUiDkClou70ZdFQaiZ3hK.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.11425", "authors": [{"_id": "696f99bbbfeda8d7e160ca9f", "name": "Hunter Heidenreich", "hidden": false}, {"_id": "696f99bbbfeda8d7e160caa0", "name": "Yosheb Getachew", "hidden": false}, {"_id": "696f99bbbfeda8d7e160caa1", "name": "Olivia Dinica", "hidden": false}, {"_id": "696f99bbbfeda8d7e160caa2", "name": "Ben Elliott", "hidden": false}], "publishedAt": "2026-01-16T16:44:50.000Z", "submittedOnDailyAt": "2026-01-20T12:41:15.255Z", "title": "PubMed-OCR: PMC Open Access OCR Annotations", "submittedOnDailyBy": {"_id": "632f536d2636f057d586cf5b", "avatarUrl": "/avatars/dab7b28734b17a2ba66fd4ae40af000e.svg", "isPro": false, "fullname": "Hunter Heidenreich", "user": "hheiden", "type": "user"}, "summary": "PubMed-OCR is an OCR-centric corpus of scientific articles derived from PubMed Central Open Access PDFs. Each page image is annotated with Google Cloud Vision and released in a compact JSON schema with word-, line-, and paragraph-level bounding boxes. The corpus spans 209.5K articles (1.5M pages; ~1.3B words) and supports layout-aware modeling, coordinate-grounded QA, and evaluation of OCR-dependent pipelines. We analyze corpus characteristics (e.g., journal coverage and detected layout features) and discuss limitations, including reliance on a single OCR engine and heuristic line reconstruction. We release the data and schema to facilitate downstream research and invite extensions.", "upvotes": 4, "discussionId": "696f99bbbfeda8d7e160caa3", "organization": {"_id": "662171ba64e84619e5565f7d", "name": "rootsautomation", "fullname": "Roots.AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66215d0369a5d33b332a19ab/ptAbQGJespFBIeYVk5ce0.jpeg"}, "summary_zh": "<ul>\n    <li>PubMed-OCR\u662f\u4e00\u4e2a\u4ee5OCR\u4e3a\u4e2d\u5fc3\u7684\u79d1\u5b66\u6587\u7ae0\u8bed\u6599\u5e93\uff0c\u6765\u81eaPubMed Central\u5f00\u653e\u83b7\u53d6\u7684PDF\u6587\u4ef6\u3002</li>\n    <li>\u6bcf\u4e2a\u9875\u9762\u7684\u56fe\u50cf\u90fd\u7ecf\u8fc7Google Cloud Vision\u7684\u6807\u6ce8\uff0c\u5e76\u4ee5\u7d27\u51d1\u7684JSON\u683c\u5f0f\u53d1\u5e03\uff0c\u5305\u542b\u5355\u8bcd\u3001\u884c\u548c\u6bb5\u843d\u7684\u8fb9\u754c\u6846\u3002</li>\n    <li>\u8be5\u8bed\u6599\u5e93\u5305\u542b209.5K\u7bc7\u6587\u7ae0\uff081.5M\u9875\u9762\uff1b\u7ea613\u4ebf\u4e2a\u5355\u8bcd\uff09\uff0c\u652f\u6301\u5e03\u5c40\u611f\u77e5\u5efa\u6a21\u548c\u57fa\u4e8e\u5750\u6807\u7684\u95ee\u7b54\u3002</li>\n    <li>\u6211\u4eec\u5206\u6790\u4e86\u8bed\u6599\u5e93\u7684\u7279\u5f81\uff08\u4f8b\u5982\uff0c\u671f\u520a\u8986\u76d6\u548c\u68c0\u6d4b\u5230\u7684\u5e03\u5c40\u7279\u5f81\uff09\uff0c\u5e76\u8ba8\u8bba\u4e86\u5c40\u9650\u6027\u3002</li>\n    <li>\u6211\u4eec\u53d1\u5e03\u4e86\u6570\u636e\u548c\u67b6\u6784\uff0c\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\uff0c\u5e76\u9080\u8bf7\u6269\u5c55\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>PubMed-OCR is a collection of scientific articles from PubMed Central in PDF format, focusing on Optical Character Recognition (OCR).</li>\n    <li>It contains images of 1.5 million pages from 209,500 articles, along with detailed annotations for words, lines, and paragraphs.</li>\n    <li>The dataset is useful for developing models that understand page layouts and for quality assessment of OCR processes.</li>\n    <li>There are some limitations, such as using only one OCR engine and challenges with reconstructing lines of text.</li>\n    <li>The data and its structure are shared for other researchers to use and build upon.</li>\n</ul>"}, "publishedAt": "2026-01-16T11:44:50.000Z", "title": "PubMed-OCR: PMC Open Access OCR Annotations", "summary": "PubMed-OCR is an OCR-centric corpus of scientific articles derived from PubMed Central Open Access PDFs. Each page image is annotated with Google Cloud Vision and released in a compact JSON schema with word-, line-, and paragraph-level bounding boxes. The corpus spans 209.5K articles (1.5M pages; ~1.3B words) and supports layout-aware modeling, coordinate-grounded QA, and evaluation of OCR-dependent pipelines. We analyze corpus characteristics (e.g., journal coverage and detected layout features) and discuss limitations, including reliance on a single OCR engine and heuristic line reconstruction. We release the data and schema to facilitate downstream research and invite extensions.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11425.png", "numComments": 2, "submittedBy": {"_id": "632f536d2636f057d586cf5b", "avatarUrl": "/avatars/dab7b28734b17a2ba66fd4ae40af000e.svg", "fullname": "Hunter Heidenreich", "name": "hheiden", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "662171ba64e84619e5565f7d", "name": "rootsautomation", "fullname": "Roots.AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66215d0369a5d33b332a19ab/ptAbQGJespFBIeYVk5ce0.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.10108", "authors": [{"_id": "696f3d285750539a88acc82b", "name": "Yiming Ren", "hidden": false}, {"_id": "696f3d285750539a88acc82c", "user": {"_id": "62579c55b98dcaa7e0de285d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62579c55b98dcaa7e0de285d/0YUd5nloul_bW9yolDGGo.jpeg", "isPro": false, "fullname": "wangjunjie", "user": "wanng", "type": "user"}, "name": "Junjie Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-20T09:39:02.612Z", "hidden": false}, {"_id": "696f3d285750539a88acc82d", "name": "Yuxin Meng", "hidden": false}, {"_id": "696f3d285750539a88acc82e", "user": {"_id": "67ebb2ac4d4959913fa71a40", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/m0yk6RvxQIXdg_Labhyw7.png", "isPro": false, "fullname": "yihang shi", "user": "lalalion", "type": "user"}, "name": "Yihang Shi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:49:01.198Z", "hidden": false}, {"_id": "696f3d285750539a88acc82f", "user": {"_id": "6863d7b1d399aca67b578bd8", "avatarUrl": "/avatars/e57c400dbf14df387c6c3ced9032f8be.svg", "isPro": false, "fullname": "Zhiqiang Lin", "user": "LLLinkzc", "type": "user"}, "name": "Zhiqiang Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:49:07.520Z", "hidden": false}, {"_id": "696f3d285750539a88acc830", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:49:12.992Z", "hidden": false}, {"_id": "696f3d285750539a88acc831", "name": "Yiran Xu", "hidden": false}, {"_id": "696f3d285750539a88acc832", "name": "Ziming Li", "hidden": false}, {"_id": "696f3d285750539a88acc833", "name": "Yunfei Zhao", "hidden": false}, {"_id": "696f3d285750539a88acc834", "name": "Zihan Wang", "hidden": false}, {"_id": "696f3d285750539a88acc835", "name": "Yu Qiao", "hidden": false}, {"_id": "696f3d285750539a88acc836", "name": "Ruiming Tang", "hidden": false}, {"_id": "696f3d285750539a88acc837", "name": "Minghao Liu", "hidden": false}, {"_id": "696f3d285750539a88acc838", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:49:39.610Z", "hidden": false}], "publishedAt": "2026-01-15T06:25:25.000Z", "submittedOnDailyAt": "2026-01-20T06:01:31.707Z", "title": "SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature", "submittedOnDailyBy": {"_id": "62579c55b98dcaa7e0de285d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62579c55b98dcaa7e0de285d/0YUd5nloul_bW9yolDGGo.jpeg", "isPro": false, "fullname": "wangjunjie", "user": "wanng", "type": "user"}, "summary": "Evaluating whether multimodal large language models truly understand long-form scientific papers remains challenging: answer-only metrics and synthetic \"Needle-In-A-Haystack\" tests often reward answer matching without requiring a causal, evidence-linked reasoning trace in the document. We propose the \"Fish-in-the-Ocean\" (FITO) paradigm, which requires models to construct explicit cross-modal evidence chains within native scientific documents. To operationalize FITO, we build SIN-Data, a scientific interleaved corpus that preserves the native interleaving of text and figures. On top of it, we construct SIN-Bench with four progressive tasks covering evidence discovery (SIN-Find), hypothesis verification (SIN-Verify), grounded QA (SIN-QA), and evidence-anchored synthesis (SIN-Summary). We further introduce \"No Evidence, No Score\", scoring predictions when grounded to verifiable anchors and diagnosing evidence quality via matching, relevance, and logic. Experiments on eight MLLMs show that grounding is the primary bottleneck: Gemini-3-pro achieves the best average overall score (0.573), while GPT-5 attains the highest SIN-QA answer accuracy (0.767) but underperforms on evidence-aligned overall scores, exposing a gap between correctness and traceable support.", "upvotes": 4, "discussionId": "696f3d285750539a88acc839", "ai_summary": "Researchers introduce the Fish-in-the-Ocean paradigm and SIN-Bench dataset to evaluate multimodal language models' ability to reason over scientific documents with evidence chains, revealing a gap between answer accuracy and traceable support.", "ai_keywords": ["multimodal large language models", "scientific papers", "evidence chains", "SIN-Data", "SIN-Bench", "SIN-Find", "SIN-Verify", "SIN-QA", "SIN-Summary", "No Evidence", "No Score"], "organization": {"_id": "66f55d53853f0506904d1922", "name": "IIGroup", "fullname": "Tsinghua IIGroup", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62579c55b98dcaa7e0de285d/A1SKeBEvaODFnkAZusICK.png"}, "summary_zh": "<ul>\n    <li>\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u957f\u7bc7\u79d1\u5b66\u8bba\u6587\u4ecd\u7136\u5f88\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u6d77\u6d0b\u4e2d\u7684\u9c7c\u201d\uff08FITO\uff09\u8303\u5f0f\uff0c\u8981\u6c42\u6a21\u578b\u5728\u79d1\u5b66\u6587\u732e\u4e2d\u6784\u5efa\u660e\u786e\u7684\u8de8\u6a21\u6001\u8bc1\u636e\u94fe\u3002</li>\n    <li>\u4e3a\u4e86\u5b9e\u73b0FITO\uff0c\u6211\u4eec\u5efa\u7acb\u4e86SIN-Data\uff0c\u4e00\u4e2a\u4fdd\u7559\u6587\u672c\u548c\u56fe\u5f62\u4ea4\u9519\u7684\u79d1\u5b66\u8bed\u6599\u5e93\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86SIN-Bench\uff0c\u5305\u542b\u56db\u4e2a\u9010\u6b65\u4efb\u52a1\uff0c\u6d89\u53ca\u8bc1\u636e\u53d1\u73b0\u3001\u5047\u8bbe\u9a8c\u8bc1\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u95ee\u7b54\u548c\u8bc1\u636e\u652f\u6301\u7684\u7efc\u5408\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u7840\u8bc1\u636e\u662f\u4e3b\u8981\u74f6\u9888\uff0c\u5c3d\u7ba1\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4e00\uff0c\u4f46\u5728\u8bc1\u636e\u5bf9\u9f50\u7684\u6574\u4f53\u5f97\u5206\u4e0a\u5b58\u5728\u5dee\u8ddd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Evaluating if large language models understand long scientific papers is difficult, as current tests often focus on matching answers rather than understanding the reasoning behind them.</li>\n    <li>The \"Fish-in-the-Ocean\" (FITO) approach requires models to create clear links between evidence in scientific documents.</li>\n    <li>A new dataset called SIN-Data was created to maintain the original connection between text and figures in scientific papers.</li>\n    <li>SIN-Bench includes four tasks to assess models on evidence discovery, hypothesis verification, question answering, and summary creation.</li>\n    <li>Results show that grounding evidence is a key challenge, with different models performing variably on different tasks, highlighting a gap between correct answers and the reasoning behind them.</li>\n</ul>"}, "publishedAt": "2026-01-15T01:25:25.000Z", "title": "SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature", "summary": "Evaluating whether multimodal large language models truly understand long-form scientific papers remains challenging: answer-only metrics and synthetic \"Needle-In-A-Haystack\" tests often reward answer matching without requiring a causal, evidence-linked reasoning trace in the document. We propose the \"Fish-in-the-Ocean\" (FITO) paradigm, which requires models to construct explicit cross-modal evidence chains within native scientific documents. To operationalize FITO, we build SIN-Data, a scientific interleaved corpus that preserves the native interleaving of text and figures. On top of it, we construct SIN-Bench with four progressive tasks covering evidence discovery (SIN-Find), hypothesis verification (SIN-Verify), grounded QA (SIN-QA), and evidence-anchored synthesis (SIN-Summary). We further introduce \"No Evidence, No Score\", scoring predictions when grounded to verifiable anchors and diagnosing evidence quality via matching, relevance, and logic. Experiments on eight MLLMs show that grounding is the primary bottleneck: Gemini-3-pro achieves the best average overall score (0.573), while GPT-5 attains the highest SIN-QA answer accuracy (0.767) but underperforms on evidence-aligned overall scores, exposing a gap between correctness and traceable support.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10108.png", "numComments": 2, "submittedBy": {"_id": "62579c55b98dcaa7e0de285d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62579c55b98dcaa7e0de285d/0YUd5nloul_bW9yolDGGo.jpeg", "fullname": "wangjunjie", "name": "wanng", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 34, "isUserFollowing": false}, "organization": {"_id": "66f55d53853f0506904d1922", "name": "IIGroup", "fullname": "Tsinghua IIGroup", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62579c55b98dcaa7e0de285d/A1SKeBEvaODFnkAZusICK.png"}, "isAuthorParticipating": true}],
    "week": [{"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u4e30\u5bcc\u7684\u8bed\u4e49\u5b9e\u4f53\uff0c\u5206\u5272\u8fd9\u4e9b\u5b9e\u4f53\u5bf9\u4e8e\u540e\u7eed\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>\u76ee\u524d\u7684\u5206\u5272\u6a21\u578b\u5bf9\u7269\u7406\u5c5e\u6027\u5b9a\u4e49\u7684\u5b9e\u4f53\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\uff08\u5982\u5b66\u6821\u3001\u516c\u56ed\uff09\u4ecd\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u793e\u4f1a\u8bed\u4e49\u5206\u5272\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86Urban Socio-Semantic Segmentation\u6570\u636e\u96c6SocioSeg\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u793e\u4ea4\u8bed\u4e49\u5b9e\u4f53\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u63d0\u51fa\u4e86SocioReasoner\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u8bc6\u522b\u548c\u591a\u9636\u6bb5\u63a8\u7406\uff0c\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u6807\u6ce8\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u8fc7\u7a0b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban surfaces contain many different types of entities, and it's important to identify them from satellite images.</li>\n    <li>Current models can identify physical features well, but struggle with socially defined categories like schools and parks.</li>\n    <li>The authors created a new dataset called SocioSeg, which includes satellite images and detailed labels for social entities.</li>\n    <li>They developed a framework called SocioReasoner that mimics how humans identify social entities using visual and language cues.</li>\n    <li>Tests show that their method performs better than existing models and can generalize well to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u5e73\u8861\u7d27\u51d1\u6548\u7387\u548c\u591a\u6a21\u6001\u667a\u80fd\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4f7f\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u4e86\u8bed\u8a00\u5bf9\u9f50\u7684\u611f\u77e5\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff0c\u589e\u5f3a\u4e86\u89c6\u89c9\u548c\u8bed\u8a00\u7684\u534f\u540c\u4f5c\u7528\u3002</li>\n    <li>\u901a\u8fc7\u8d85\u8fc71000\u6b21\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\uff0c\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u3002</li>\n    <li>\u5c3d\u7ba1\u6a21\u578b\u4f53\u79ef\u53ea\u670910\u4ebf\u53c2\u6570\uff0c\u4f46\u5176\u6027\u80fd\u53ef\u4ee5\u4e0e10\u523020\u500d\u5927\u7684\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u751a\u81f3\u8d85\u8fc7\u4e00\u4e9b\u9876\u7ea7\u6a21\u578b\u3002</li>\n    <li>STEP3-VL-10B \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u79c0\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u3001\u9ad8\u6548\u4e14\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6a21\u578b\u4f9b\u793e\u533a\u4f7f\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a new, lightweight open-source model aimed at combining efficiency with advanced multimodal intelligence.</li>\n    <li>It uses a unique training method involving 1.2 trillion multimodal tokens and incorporates a vision-language synergy with a special encoder and decoder.</li>\n    <li>The model includes a post-training process with extensive reinforcement learning to enhance its capabilities.</li>\n    <li>Despite being smaller than many other models, STEP3-VL-10B performs exceptionally well, often outperforming much larger models and other top competitors.</li>\n    <li>The creators are sharing the full model to help others in the community use and build upon this technology.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09688", "authors": [{"_id": "696864c90ac10a06522f6a4a", "name": "Yibo Wang", "hidden": false}, {"_id": "696864c90ac10a06522f6a4b", "name": "Lei Wang", "hidden": false}, {"_id": "696864c90ac10a06522f6a4c", "name": "Yue Deng", "hidden": false}, {"_id": "696864c90ac10a06522f6a4d", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:02:22.232Z", "hidden": false}, {"_id": "696864c90ac10a06522f6a4e", "name": "Yao Xiao", "hidden": false}, {"_id": "696864c90ac10a06522f6a4f", "name": "Huanjin Yao", "hidden": false}, {"_id": "696864c90ac10a06522f6a50", "name": "Liwei Kang", "hidden": false}, {"_id": "696864c90ac10a06522f6a51", "name": "Hai Ye", "hidden": false}, {"_id": "696864c90ac10a06522f6a52", "name": "Yongcheng Jing", "hidden": false}, {"_id": "696864c90ac10a06522f6a53", "name": "Lidong Bing", "hidden": false}], "publishedAt": "2026-01-14T18:38:31.000Z", "submittedOnDailyAt": "2026-01-15T01:33:59.520Z", "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation", "submittedOnDailyBy": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "summary": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.", "upvotes": 90, "discussionId": "696864c90ac10a06522f6a54", "githubRepo": "https://github.com/Infinity-AILab/DeepResearchEval", "githubRepoAddedBy": "user", "ai_summary": "DeepResearchEval presents an automated framework for creating complex research tasks and evaluating them through agent-based methods that adapt to task specifics and verify facts without relying on citations.", "ai_keywords": ["automated framework", "deep research task construction", "agentic evaluation", "persona-driven pipeline", "task qualification", "search necessity", "adaptive point-wise quality evaluation", "active fact-checking", "web search", "multi-source evidence integration"], "githubStars": 67, "organization": {"_id": "6948e6c46d88786b0ec9cf9d", "name": "Infinity-AILab", "fullname": "Infinity Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6362a77dd3be91534c2e9213/-zILHmHPjnq27MzoESFsG.png"}, "summary_zh": "<ul>\n    <li>\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7528\u4e8e\u591a\u6b65\u9aa4\u7684\u7f51\u7edc\u7814\u7a76\u3001\u5206\u6790\u548c\u8de8\u6e90\u7efc\u5408\uff0c\u4f46\u8bc4\u4f30\u8fd9\u4e9b\u7cfb\u7edf\u5f88\u6709\u6311\u6218\u6027\u3002</li>\n    <li>\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u6ce8\u91ca\uff0c\u4f9d\u8d56\u9759\u6001\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u6216\u8005\u5728\u7f3a\u5c11\u5f15\u7528\u65f6\u65e0\u6cd5\u53ef\u9760\u9a8c\u8bc1\u4e8b\u5b9e\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86DeepResearchEval\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u6784\u5efa\u548c\u4ee3\u7406\u8bc4\u4f30\u7684\u81ea\u52a8\u5316\u6846\u67b6\u3002</li>\n    <li>\u4efb\u52a1\u6784\u5efa\u91c7\u7528\u57fa\u4e8e\u89d2\u8272\u7684\u7ba1\u9053\uff0c\u751f\u6210\u4e0e\u4e0d\u540c\u7528\u6237\u753b\u50cf\u76f8\u5339\u914d\u7684\u590d\u6742\u7814\u7a76\u4efb\u52a1\u3002</li>\n    <li>\u8bc4\u4f30\u90e8\u5206\u5305\u62ec\u52a8\u6001\u751f\u6210\u4efb\u52a1\u7279\u5b9a\u8bc4\u4f30\u7ef4\u5ea6\u7684\u8d28\u91cf\u8bc4\u4f30\u548c\u80fd\u591f\u81ea\u4e3b\u63d0\u53d6\u548c\u9a8c\u8bc1\u62a5\u544a\u9648\u8ff0\u7684\u4e3b\u52a8\u4e8b\u5b9e\u68c0\u67e5\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>DeepResearchEval is a new framework designed to improve the evaluation of deep research systems.</li>\n  <li>It creates realistic and complex research tasks based on different user profiles.</li>\n  <li>The framework includes a two-stage filter to ensure tasks require evidence from multiple sources.</li>\n  <li>For evaluation, it uses a system that adapts evaluation criteria based on each task's needs.</li>\n  <li>It also features an automated fact-checking process that verifies information from web searches, even without citations.</li>\n</ul>"}, "publishedAt": "2026-01-14T13:38:31.000Z", "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation", "summary": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09688.png", "numComments": 1, "submittedBy": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "fullname": "Keming Wu", "name": "wukeming11", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "6948e6c46d88786b0ec9cf9d", "name": "Infinity-AILab", "fullname": "Infinity Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6362a77dd3be91534c2e9213/-zILHmHPjnq27MzoESFsG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09259", "authors": [{"_id": "696856230ac10a06522f69dd", "name": "Jian Zhang", "hidden": false}, {"_id": "696856230ac10a06522f69de", "user": {"_id": "67e0dc49daf1e39a7d15e67f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GsHxvMtp5jW58LunxVorc.png", "isPro": false, "fullname": "Zhiyuan Wang", "user": "Pekku", "type": "user"}, "name": "Zhiyuan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:12.229Z", "hidden": false}, {"_id": "696856230ac10a06522f69df", "name": "Zhangqi Wang", "hidden": false}, {"_id": "696856230ac10a06522f69e0", "name": "Yu He", "hidden": false}, {"_id": "696856230ac10a06522f69e1", "name": "Haoran Luo", "hidden": false}, {"_id": "696856230ac10a06522f69e2", "name": "li yuan", "hidden": false}, {"_id": "696856230ac10a06522f69e3", "name": "Lingling Zhang", "hidden": false}, {"_id": "696856230ac10a06522f69e4", "name": "Rui Mao", "hidden": false}, {"_id": "696856230ac10a06522f69e5", "user": {"_id": "66ac77011cfb12c087605acb", "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg", "isPro": false, "fullname": "Lin", "user": "Qika", "type": "user"}, "name": "Qika Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:14.086Z", "hidden": false}, {"_id": "696856230ac10a06522f69e6", "name": "Jun Liu", "hidden": false}], "publishedAt": "2026-01-14T07:48:00.000Z", "submittedOnDailyAt": "2026-01-15T00:22:01.292Z", "title": "MAXS: Meta-Adaptive Exploration with LLM Agents", "submittedOnDailyBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "isPro": false, "fullname": "Jian Zhang", "user": "VentureZJ", "type": "user"}, "summary": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.", "upvotes": 81, "discussionId": "696856230ac10a06522f69e7", "githubRepo": "https://github.com/exoskeletonzj/MAXS", "githubRepoAddedBy": "user", "ai_summary": "MAXS is a meta-adaptive reasoning framework for LLM agents that improves multi-tool reasoning through lookahead strategies and trajectory convergence mechanisms, balancing global effectiveness and computational efficiency.", "ai_keywords": ["LLM agents", "tool execution", "reasoning planning", "lookahead strategy", "advantage value", "step consistency variance", "inter-step trend slopes", "trajectory convergence", "multi-tool reasoning", "inference efficiency"], "githubStars": 5, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u901a\u8fc7\u591a\u4e2a\u5de5\u5177\u534f\u4f5c\u5c55\u73b0\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002</li>\n    <li>\u4e3b\u8981\u95ee\u9898\u5305\u62ec\u7f3a\u4e4f\u524d\u77bb\u6027\u5bfc\u81f4\u5c40\u90e8\u751f\u6210\u5c40\u9650\u548c\u8f68\u8ff9\u4e0d\u7a33\u5b9a\uff0c\u65e9\u671f\u7684\u5c0f\u9519\u8bef\u53ef\u80fd\u5bfc\u81f4\u63a8\u7406\u8def\u5f84\u504f\u79bb\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u5143\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6MAXS\uff0c\u7075\u6d3b\u6574\u5408\u5de5\u5177\u6267\u884c\u548c\u63a8\u7406\u89c4\u5212\u3002</li>\n    <li>MAXS\u91c7\u7528\u524d\u77bb\u7b56\u7565\uff0c\u5ef6\u4f38\u63a8\u7406\u8def\u5f84\u5e76\u4f30\u8ba1\u5de5\u5177\u4f7f\u7528\u7684\u4f18\u52bf\u503c\uff0c\u540c\u65f6\u9009\u62e9\u7a33\u5b9a\u7684\u4e00\u81f4\u4e14\u9ad8\u4ef7\u503c\u7684\u63a8\u7406\u6b65\u9aa4\u3002</li>\n    <li>\u901a\u8fc7\u5728\u4e09\u4e2a\u57fa\u7840\u6a21\u578b\u548c\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc1\u660eMAXS\u5728\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Model (LLM) Agents can work together using different tools, but they have some problems during reasoning.</li>\n    <li>Two main issues are short-sighted thinking (not looking ahead) and instability from small errors that can lead to big problems.</li>\n    <li>To solve these issues, we created a new framework called MAXS, which improves the reasoning process by planning ahead and evaluating tool usage.</li>\n    <li>MAXS uses a method to keep track of reasoning steps and stops when a stable path is found, helping to save resources.</li>\n    <li>Tests show that MAXS performs better and is more efficient than current methods across various models and datasets.</li>\n</ul>"}, "publishedAt": "2026-01-14T02:48:00.000Z", "title": "MAXS: Meta-Adaptive Exploration with LLM Agents", "summary": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09259.png", "numComments": 3, "submittedBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "fullname": "Jian Zhang", "name": "VentureZJ", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09274", "authors": [{"_id": "6968568f0ac10a06522f69e9", "name": "Jian Zhang", "hidden": false}, {"_id": "6968568f0ac10a06522f69ea", "name": "Yu He", "hidden": false}, {"_id": "6968568f0ac10a06522f69eb", "user": {"_id": "67e0dc49daf1e39a7d15e67f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GsHxvMtp5jW58LunxVorc.png", "isPro": false, "fullname": "Zhiyuan Wang", "user": "Pekku", "type": "user"}, "name": "Zhiyuan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:02.764Z", "hidden": false}, {"_id": "6968568f0ac10a06522f69ec", "name": "Zhangqi Wang", "hidden": false}, {"_id": "6968568f0ac10a06522f69ed", "name": "Kai He", "hidden": false}, {"_id": "6968568f0ac10a06522f69ee", "name": "Fangzhi Xu", "hidden": false}, {"_id": "6968568f0ac10a06522f69ef", "user": {"_id": "66ac77011cfb12c087605acb", "avatarUrl": "/avatars/54c06bd1c4c9d491470ed4162c2301ae.svg", "isPro": false, "fullname": "Lin", "user": "Qika", "type": "user"}, "name": "Qika Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:05.035Z", "hidden": false}, {"_id": "6968568f0ac10a06522f69f0", "name": "Jun Liu", "hidden": false}], "publishedAt": "2026-01-14T08:17:41.000Z", "submittedOnDailyAt": "2026-01-15T00:23:45.077Z", "title": "A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation", "submittedOnDailyBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "isPro": false, "fullname": "Jian Zhang", "user": "VentureZJ", "type": "user"}, "summary": "Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A^3-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A^3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.", "upvotes": 74, "discussionId": "6968568f0ac10a06522f69f1", "projectPage": "https://a3-bench.github.io/", "githubRepo": "https://github.com/exoskeletonzj/A3-Bench", "githubRepoAddedBy": "user", "githubStars": 0, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "summary_zh": "<ul>\n    <li>\u79d1\u5b66\u63a8\u7406\u4e0d\u4ec5\u4f9d\u8d56\u903b\u8f91\u63a8\u7406\uff0c\u8fd8\u9700\u8981\u6fc0\u6d3b\u5df2\u6709\u77e5\u8bc6\u548c\u7ecf\u9a8c\u7ed3\u6784\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u6216\u6b65\u9aa4\u4e00\u81f4\u6027\uff0c\u5ffd\u89c6\u4e86\u8bb0\u5fc6\u5728\u63a8\u7406\u4e2d\u7684\u4f5c\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86A^3-Bench\uff0c\u4e00\u4e2a\u8bc4\u4f30\u79d1\u5b66\u63a8\u7406\u7684\u57fa\u51c6\uff0c\u57fa\u4e8e\u951a\u548c\u5438\u5f15\u5b50\u7684\u6fc0\u6d3b\u3002</li>\n    <li>\u6211\u4eec\u5bf92,198\u4e2a\u79d1\u5b66\u63a8\u7406\u95ee\u9898\u8fdb\u884c\u4e86\u6807\u6ce8\uff0c\u5e76\u5f15\u5165\u4e86\u53cc\u5c3a\u5ea6\u8bb0\u5fc6\u8bc4\u4f30\u6846\u67b6\u3002</li>\n    <li>\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1A^3-Bench\uff0c\u5e76\u5206\u6790\u8bb0\u5fc6\u6fc0\u6d3b\u5bf9\u63a8\u7406\u8868\u73b0\u7684\u5f71\u54cd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Scientific reasoning uses both logic and prior knowledge from memory.</li>\n    <li>Current evaluation methods focus on final answers but ignore how memory helps reasoning.</li>\n    <li>A^3-Bench is a new benchmark that assesses scientific reasoning using memory activation techniques.</li>\n    <li>It includes 2,198 annotated science problems and a new way to measure memory activation.</li>\n    <li>Experiments show how memory activation affects reasoning performance, offering new insights.</li>\n</ul>"}, "publishedAt": "2026-01-14T03:17:41.000Z", "title": "A^3-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation", "summary": "Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the memory-driven mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose A^3-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate A^3-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09274.png", "numComments": 2, "submittedBy": {"_id": "658be7fe135580745c510323", "avatarUrl": "/avatars/830e5cec4565efdc23226a86a0fcef0e.svg", "fullname": "Jian Zhang", "name": "VentureZJ", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "66a92d5a58cff488d93ab512", "name": "XianJiaotongUniversity", "fullname": "Xi'an Jiaotong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a92ba2f351acac61ba119c/6zLTkLwBLMbRLR1y7tfpC.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09667", "authors": [{"_id": "6969b0f732f0333869ff9476", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:48.445Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9477", "user": {"_id": "662b4e3bc709a61df840fda1", "avatarUrl": "/avatars/fc73c63a4e1f8fbb084ec43ec9af0af0.svg", "isPro": false, "fullname": "Hu Yunhai", "user": "AlexCCtop", "type": "user"}, "name": "Yunhai Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:37:06.706Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9478", "user": {"_id": "650026d30339dae3dba2cec5", "avatarUrl": "/avatars/fcc9ea4336f8d4bb177e5c9eacdd05c9.svg", "isPro": false, "fullname": "Juncheng Liu", "user": "juncliu", "type": "user"}, "name": "Juncheng Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:29:33.401Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9479", "name": "Shuyue Stella Li", "hidden": false}, {"_id": "6969b0f732f0333869ff947a", "name": "Yucheng Wang", "hidden": false}, {"_id": "6969b0f732f0333869ff947b", "user": {"_id": "638e40d450a4e4beef98196b", "avatarUrl": "/avatars/fe27e019baf48caeb44e19b7289db9fb.svg", "isPro": false, "fullname": "Zhen Xu", "user": "zhenxu", "type": "user"}, "name": "Zhen Xu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:04.868Z", "hidden": false}, {"_id": "6969b0f732f0333869ff947c", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0f732f0333869ff947d", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:15.855Z", "hidden": false}, {"_id": "6969b0f732f0333869ff947e", "name": "Xinxing Xu", "hidden": false}, {"_id": "6969b0f732f0333869ff947f", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:25.577Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9480", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:31.289Z", "hidden": false}, {"_id": "6969b0f732f0333869ff9481", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:34:36.481Z", "hidden": false}], "publishedAt": "2026-01-14T17:57:43.000Z", "submittedOnDailyAt": "2026-01-16T01:01:32.343Z", "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce Multi-Agent Test-Time Reinforcement Learning (MATTRL), a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.", "upvotes": 63, "discussionId": "6969b0f832f0333869ff9482", "ai_summary": "Multi-Agent Test-Time Reinforcement Learning (MATTRL) enhances multi-agent reasoning through structured textual experience injection and consensus-based decision making at inference time.", "ai_keywords": ["multi-agent systems", "reinforcement learning", "test-time reinforcement learning", "multi-agent reinforcement learning", "credit assignment", "multi-expert teams", "dialogue systems", "distribution-shift-robust reasoning"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u8fc7\u591a\u6837\u6027\u548c\u4ea4\u53c9\u68c0\u67e5\uff0c\u6210\u4e3a\u4e86\u8bb8\u591a\u5e94\u7528\u4e2d\u5b9e\u7528\u7684LLM\u9a71\u52a8\u534f\u4f5c\u4f19\u4f34\u3002</li>\n    <li>\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u8bad\u7ec3\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u4e0d\u7a33\u5b9a\uff0c\u4e3b\u8981\u7531\u4e8e\u961f\u53cb\u4e4b\u95f4\u7684\u5171\u540c\u9002\u5e94\u548c\u7a00\u758f\u7684\u5956\u52b1\u3002</li>\n    <li>\u5f15\u5165\u4e86\u591a\u667a\u80fd\u4f53\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08MATTRL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6587\u672c\u7ecf\u9a8c\u589e\u5f3a\u591a\u667a\u80fd\u4f53\u8ba8\u8bba\u3002</li>\n    <li>MATTRL\u5f62\u6210\u4e86\u591a\u4e13\u5bb6\u56e2\u961f\uff0c\u6574\u5408\u6d4b\u8bd5\u65f6\u7ecf\u9a8c\u4ee5\u8fbe\u6210\u5171\u8bc6\u505a\u51fa\u6700\u7ec8\u51b3\u7b56\u3002</li>\n    <li>\u5728\u533b\u5b66\u3001\u6570\u5b66\u548c\u6559\u80b2\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMATTRL\u7684\u51c6\u786e\u6027\u5e73\u5747\u63d0\u9ad8\u4e863.67%\uff0c\u76f8\u6bd4\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u63d0\u9ad8\u4e868.67%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-agent systems are now effective collaborators thanks to diverse contributions and checking against each other.</li>\n    <li>Training these systems with reinforcement learning (MARL) can be difficult and requires a lot of resources, often leading to instability.</li>\n    <li>The new approach, Multi-Agent Test-Time Reinforcement Learning (MATTRL), enhances decision-making by using structured text experiences during discussions.</li>\n    <li>MATTRL improves accuracy in tasks by an average of 3.67% over multi-agent baselines and 8.67% over single-agent systems.</li>\n    <li>Different methods for credit assignment are tested to see how they impact training success, showing MATTRL is stable and efficient without needing adjustments.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:57:43.000Z", "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning", "summary": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce Multi-Agent Test-Time Reinforcement Learning (MATTRL), a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09667.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.11077", "authors": [{"_id": "696da04e3f1837bfb89709c2", "user": {"_id": "66206a2136201a18e5329631", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66206a2136201a18e5329631/9aJiGOKshh1tZ171zDw_D.png", "isPro": false, "fullname": "yangjie", "user": "red-fox-yj", "type": "user"}, "name": "Jie Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-20T09:41:07.348Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709c3", "user": {"_id": "638ef0b0c67af472d31674a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638ef0b0c67af472d31674a6/zXQjC3DdY3jpVkATkpms6.png", "isPro": false, "fullname": "Honglin Guo", "user": "KYLN24", "type": "user"}, "name": "Honglin Guo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-20T09:41:05.334Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709c4", "name": "Li Ji", "hidden": false}, {"_id": "696da04e3f1837bfb89709c5", "user": {"_id": "683c6a19a4b3e38a3e23d50a", "avatarUrl": "/avatars/ae9f212acaa9d1a65b4a5d86c5f7a355.svg", "isPro": false, "fullname": "Jiazheng Zhou", "user": "HaZ-K", "type": "user"}, "name": "Jiazheng Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:42:49.533Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709c6", "name": "Rui Zheng", "hidden": false}, {"_id": "696da04e3f1837bfb89709c7", "name": "Zhikai Lei", "hidden": false}, {"_id": "696da04e3f1837bfb89709c8", "user": {"_id": "6334f2f1259c518276efa730", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6334f2f1259c518276efa730/z_SH_OBkDyj4RCN9mqsKS.jpeg", "isPro": false, "fullname": "Shuo Zhang", "user": "Meteonis", "type": "user"}, "name": "Shuo Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-20T14:48:19.478Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709c9", "user": {"_id": "653a6e5cae155b92bae77b74", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg", "isPro": false, "fullname": "Zhiheng Xi", "user": "WooooDyy", "type": "user"}, "name": "Zhiheng Xi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:43:14.561Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709ca", "user": {"_id": "65435cad429b80b14922ab8d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/N8oWq4ZZn3dRxmXi18FrA.jpeg", "isPro": false, "fullname": "Shichun Liu", "user": "Liusc2020", "type": "user"}, "name": "Shichun Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:43:20.286Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709cb", "name": "Yuxin Wang", "hidden": false}, {"_id": "696da04e3f1837bfb89709cc", "name": "Bo Wang", "hidden": false}, {"_id": "696da04e3f1837bfb89709cd", "user": {"_id": "64b7495a75b23e68c538f4c0", "avatarUrl": "/avatars/ce06f3b89f9e09dcbe748b208eec1e9d.svg", "isPro": false, "fullname": "Yining Zheng", "user": "WillQvQ", "type": "user"}, "name": "Yining Zheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:43:29.765Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709ce", "name": "Tao Gui", "hidden": false}, {"_id": "696da04e3f1837bfb89709cf", "user": {"_id": "61457b8deff2c9fdb4de4988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg", "isPro": false, "fullname": "Xipeng Qiu", "user": "xpqiu", "type": "user"}, "name": "Xipeng Qiu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:43:35.123Z", "hidden": false}], "publishedAt": "2026-01-16T08:23:52.000Z", "submittedOnDailyAt": "2026-01-20T00:57:45.521Z", "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development", "submittedOnDailyBy": {"_id": "66206a2136201a18e5329631", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66206a2136201a18e5329631/9aJiGOKshh1tZ171zDw_D.png", "isPro": false, "fullname": "yangjie", "user": "red-fox-yj", "type": "user"}, "summary": "The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving. However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in backend development which demands rigorous environment configuration and service deployment. To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic backend coding within a realistic, executable workflow. Using a scalable automated pipeline, we curated 224 practical tasks spanning 8 languages and 19 frameworks from open-source repositories. Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests. Our extensive evaluation reveals that even state-of-the-art models struggle to deliver reliable performance on these holistic tasks, highlighting a substantial disparity between current model capabilities and the demands of practical backend engineering. Our code is available at https://github.com/OpenMOSS/ABC-Bench.", "upvotes": 49, "discussionId": "696da04e3f1837bfb89709d0", "projectPage": "https://dawning-road.github.io/blog/abc-bench", "githubRepo": "https://github.com/OpenMOSS/ABC-Bench", "githubRepoAddedBy": "user", "ai_summary": "ABC-Bench evaluates LLM agents on realistic backend coding tasks requiring full development lifecycle management from repository exploration to containerized service deployment and API testing.", "ai_keywords": ["Large Language Models", "agentic backend coding", "executable workflow", "development lifecycle", "containerized services", "end-to-end API tests"], "githubStars": 8, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u7ecf\u53d1\u5c55\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u53ef\u4ee5\u8fdb\u884c\u590d\u6742\u7684\u4ee3\u7801\u751f\u6210\u548c\u95ee\u9898\u89e3\u51b3\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u6d4b\u4e3b\u8981\u5173\u6ce8\u4ee3\u7801\u903b\u8f91\uff0c\u5ffd\u89c6\u4e86\u73b0\u5b9e\u5de5\u7a0b\u4e2d\u52a8\u6001\u7684\u3001\u5168\u6d41\u7a0b\u7684\u9700\u6c42\uff0c\u7279\u522b\u662f\u540e\u7aef\u5f00\u53d1\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63a8\u51fa\u4e86ABC-Bench\uff0c\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30\u540e\u7aef\u7f16\u7801\u7684\u65b0\u57fa\u51c6\u3002</li>\n    <li>ABC-Bench\u5305\u542b224\u4e2a\u5b9e\u7528\u4efb\u52a1\uff0c\u6db5\u76d68\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c19\u79cd\u6846\u67b6\uff0c\u8981\u6c42\u4ee3\u7406\u7ba1\u7406\u6574\u4e2a\u5f00\u53d1\u751f\u547d\u5468\u671f\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u6700\u65b0\u7684\u6a21\u578b\u5728\u8fd9\u4e9b\u590d\u6742\u4efb\u52a1\u4e2d\u4e5f\u8868\u73b0\u4e0d\u4f73\uff0c\u663e\u793a\u51fa\u6a21\u578b\u80fd\u529b\u4e0e\u5b9e\u9645\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are evolving into autonomous agents, moving beyond simple code generation to solving complex coding problems.</li>\n    <li>Current benchmarks focus mainly on static code logic and do not reflect the dynamic requirements of real-world backend development.</li>\n    <li>ABC-Bench is a new benchmark designed to evaluate backend coding in realistic scenarios, covering the entire development process.</li>\n    <li>It includes 224 practical tasks across 8 programming languages and 19 frameworks, sourced from open-source projects.</li>\n    <li>Results show that even advanced models struggle with these comprehensive tasks, indicating a gap between model performance and real-world coding needs.</li>\n</ul>"}, "publishedAt": "2026-01-16T03:23:52.000Z", "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development", "summary": "The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving. However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in backend development which demands rigorous environment configuration and service deployment. To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic backend coding within a realistic, executable workflow. Using a scalable automated pipeline, we curated 224 practical tasks spanning 8 languages and 19 frameworks from open-source repositories. Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests. Our extensive evaluation reveals that even state-of-the-art models struggle to deliver reliable performance on these holistic tasks, highlighting a substantial disparity between current model capabilities and the demands of practical backend engineering. Our code is available at https://github.com/OpenMOSS/ABC-Bench.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11077.png", "numComments": 3, "submittedBy": {"_id": "66206a2136201a18e5329631", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66206a2136201a18e5329631/9aJiGOKshh1tZ171zDw_D.png", "fullname": "yangjie", "name": "red-fox-yj", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09088", "authors": [{"_id": "69688bbc0ac10a06522f6aeb", "user": {"_id": "6463345cd2044cd1d7c613a8", "avatarUrl": "/avatars/242cbf2479877e836f931d17a6190660.svg", "isPro": false, "fullname": "Shaotian", "user": "ystluffy", "type": "user"}, "name": "Shaotian Yan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T09:33:27.639Z", "hidden": false}, {"_id": "69688bbc0ac10a06522f6aec", "name": "Kaiyuan Liu", "hidden": false}, {"_id": "69688bbc0ac10a06522f6aed", "user": {"_id": "64b73e3830a0b8ff60145a29", "avatarUrl": "/avatars/297469812b57b2ddf7d52b9391d80bde.svg", "isPro": false, "fullname": "Chen Shen", "user": "zjushenchen", "type": "user"}, "name": "Chen Shen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T09:33:19.260Z", "hidden": false}, {"_id": "69688bbc0ac10a06522f6aee", "user": {"_id": "6225b0d87f5fba1007d62fae", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6225b0d87f5fba1007d62fae/clONu5C-lkoSswcJjcG0u.jpeg", "isPro": false, "fullname": "Bing Wang", "user": "wangbing1416", "type": "user"}, "name": "Bing Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T09:33:17.091Z", "hidden": false}, {"_id": "69688bbc0ac10a06522f6aef", "user": {"_id": "694a226980a37f293a4ce7c0", "avatarUrl": "/avatars/7a48f4eeb80a1b5688cbfb10a59765a0.svg", "isPro": false, "fullname": "Sinan Fan", "user": "sinan25", "type": "user"}, "name": "Sinan Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T09:33:14.748Z", "hidden": false}, {"_id": "69688bbc0ac10a06522f6af0", "name": "Jun Zhang", "hidden": false}, {"_id": "69688bbc0ac10a06522f6af1", "name": "Yue Wu", "hidden": false}, {"_id": "69688bbc0ac10a06522f6af2", "name": "Zheng Wang", "hidden": false}, {"_id": "69688bbc0ac10a06522f6af3", "name": "Jieping Ye", "hidden": false}], "publishedAt": "2026-01-14T02:43:17.000Z", "submittedOnDailyAt": "2026-01-15T07:24:58.461Z", "title": "Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning", "submittedOnDailyBy": {"_id": "6463345cd2044cd1d7c613a8", "avatarUrl": "/avatars/242cbf2479877e836f931d17a6190660.svg", "isPro": false, "fullname": "Shaotian", "user": "ystluffy", "type": "user"}, "summary": "In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.", "upvotes": 43, "discussionId": "69688bbc0ac10a06522f6af4", "projectPage": "https://github.com/D2I-ai/dasd-thinking", "githubRepo": "https://github.com/D2I-ai/dasd-thinking", "githubRepoAddedBy": "user", "ai_summary": "A lightweight open-source reasoning model achieves state-of-the-art performance through enhanced sequence-level distillation that addresses limitations in current teacher-student knowledge transfer methods.", "ai_keywords": ["sequence-level distillation", "teacher-student distillation", "SFT", "heuristic rules", "output distribution", "generalization capability", "exposure bias", "teacher-forced training", "autoregressive inference"], "githubStars": 16, "organization": {"_id": "693005c327917f8ddef415f4", "name": "Alibaba-Apsara", "fullname": "Alibaba Cloud Apsara Lab ", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6463345cd2044cd1d7c613a8/Vlbe-DKfqzJcbWyW6AE57.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86DASD-4B-Thinking\uff0c\u8fd9\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e14\u529f\u80fd\u5f3a\u5927\u7684\u5f00\u6e90\u63a8\u7406\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u6570\u5b66\u3001\u79d1\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u7b49\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u591a\u4e2a\u66f4\u5927\u578b\u7684\u6a21\u578b\u3002</li>\n    <li>\u62a5\u544a\u6279\u5224\u6027\u5730\u91cd\u65b0\u5ba1\u89c6\u4e86\u5e38\u7528\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u6307\u51fa\u73b0\u6709\u65b9\u6cd5\u5728\u84b8\u998f\u8fc7\u7a0b\u4e2d\u5b58\u5728\u7684\u4e09\u4e2a\u4e3b\u8981\u5c40\u9650\u6027\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e9b\u65b9\u6cd5\u521b\u65b0\uff0c\u5f62\u6210\u4e86\u6539\u8fdb\u7684\u5e8f\u5217\u7ea7\u84b8\u998f\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>DASD-4B-Thinking\u4ec5\u4f7f\u7528448K\u8bad\u7ec3\u6837\u672c\u5c31\u53d6\u5f97\u4e86\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u5e76\u516c\u5f00\u53d1\u5e03\u4e86\u6a21\u578b\u548c\u8bad\u7ec3\u6570\u636e\u96c6\u4ee5\u652f\u6301\u793e\u533a\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DASD-4B-Thinking is a new open-source reasoning model that performs really well in math, science, and coding tasks.</li>\n    <li>It outperforms some larger models while being lightweight and efficient.</li>\n    <li>The report critiques a common training method (SFT) used in AI, highlighting its limitations in teacher-student model interactions.</li>\n    <li>Three main issues with current methods are identified: poor representation of teacher's outputs, misalignment in learning, and exposure bias.</li>\n    <li>DASD-4B-Thinking achieves good results using far fewer training samples than other models and the training data is publicly available.</li>\n</ul>"}, "publishedAt": "2026-01-13T21:43:17.000Z", "title": "Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning", "summary": "In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09088.png", "numComments": 4, "submittedBy": {"_id": "6463345cd2044cd1d7c613a8", "avatarUrl": "/avatars/242cbf2479877e836f931d17a6190660.svg", "fullname": "Shaotian", "name": "ystluffy", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "693005c327917f8ddef415f4", "name": "Alibaba-Apsara", "fullname": "Alibaba Cloud Apsara Lab ", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6463345cd2044cd1d7c613a8/Vlbe-DKfqzJcbWyW6AE57.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.11496", "authors": [{"_id": "696de89f3f1837bfb8970ab3", "user": {"_id": "64802fb6c57f629056c59966", "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg", "isPro": false, "fullname": "Eilam Shapira", "user": "EilamSha", "type": "user"}, "name": "Eilam Shapira", "status": "admin_assigned", "statusLastChangedAt": "2026-01-19T14:37:41.780Z", "hidden": false}, {"_id": "696de89f3f1837bfb8970ab4", "name": "Roi Reichart", "hidden": false}, {"_id": "696de89f3f1837bfb8970ab5", "name": "Moshe Tennenholtz", "hidden": false}], "publishedAt": "2026-01-16T18:18:03.000Z", "submittedOnDailyAt": "2026-01-19T06:58:50.740Z", "title": "The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents", "submittedOnDailyBy": {"_id": "64802fb6c57f629056c59966", "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg", "isPro": false, "fullname": "Eilam Shapira", "user": "EilamSha", "type": "user"}, "summary": "The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the \"Poisoned Apple\" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator's choice of market design in their favor. This strategic release improves the releaser's welfare at the expense of their opponent and the regulator's fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.", "upvotes": 39, "discussionId": "696de8a03f1837bfb8970ab6", "organization": {"_id": "6393322be2364bc1eea56e45", "name": "Technion", "fullname": "Technion Israel institute of technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"}, "summary_zh": "<ul>\n    <li>AI\u4ee3\u7406\u7684\u5f15\u5165\u6539\u53d8\u4e86\u7ecf\u6d4e\u5e02\u573a\u7684\u6218\u7565\u4e92\u52a8\u65b9\u5f0f\u3002</li>\n    <li>\u6211\u4eec\u7814\u7a76\u4e86\u5728\u4e09\u79cd\u535a\u5f08\u8bba\u573a\u666f\u4e2d\uff0c\u6280\u672f\u9009\u62e9\u589e\u52a0\u7684\u7ecf\u6d4e\u5f71\u54cd\uff0c\u5305\u62ec\u8d44\u6e90\u5206\u914d\u3001\u975e\u5bf9\u79f0\u4fe1\u606f\u4ea4\u6613\u548c\u6218\u7565\u4fe1\u606f\u4f20\u9012\u3002</li>\n    <li>\u589e\u52a0AI\u4ee3\u7406\u7684\u9009\u62e9\u53ef\u4ee5\u663e\u8457\u6539\u53d8\u6536\u76ca\u5e73\u8861\u548c\u76d1\u7ba1\u7ed3\u679c\uff0c\u53ef\u80fd\u4fc3\u4f7f\u76d1\u7ba1\u8005\u4e3b\u52a8\u5f00\u53d1\u65b0\u6280\u672f\u3002</li>\n    <li>\u6211\u4eec\u53d1\u73b0\"\u6bd2\u82f9\u679c\"\u6548\u5e94\uff0c\u5373\u4ee3\u7406\u53d1\u5e03\u65b0\u6280\u672f\u4ee5\u64cd\u63a7\u76d1\u7ba1\u8005\u7684\u5e02\u573a\u8bbe\u8ba1\u9009\u62e9\uff0c\u4ece\u4e2d\u83b7\u76ca\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u9759\u6001\u76d1\u7ba1\u6846\u67b6\u5bb9\u6613\u88ab\u6280\u672f\u6269\u5c55\u64cd\u63a7\uff0c\u56e0\u6b64\u9700\u8981\u52a8\u6001\u5e02\u573a\u8bbe\u8ba1\u4ee5\u9002\u5e94AI\u80fd\u529b\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Integrating AI agents into markets changes how people interact strategically.</li>\n    <li>We explore how adding more technology options affects bargaining, negotiation, and persuasion in economics.</li>\n    <li>More AI choices can lead to significant changes in outcomes and may encourage regulators to create new technologies.</li>\n    <li>We discovered something called the \"Poisoned Apple\" effect, where an agent might introduce a new technology just to influence regulators unfairly.</li>\n    <li>This shows that fixed regulations can be manipulated, highlighting the need for flexible market designs that can adapt to new AI developments.</li>\n</ul>"}, "publishedAt": "2026-01-16T13:18:03.000Z", "title": "The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents", "summary": "The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the \"Poisoned Apple\" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator's choice of market design in their favor. This strategic release improves the releaser's welfare at the expense of their opponent and the regulator's fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11496.png", "numComments": 2, "submittedBy": {"_id": "64802fb6c57f629056c59966", "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg", "fullname": "Eilam Shapira", "name": "EilamSha", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "6393322be2364bc1eea56e45", "name": "Technion", "fullname": "Technion Israel institute of technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09708", "authors": [{"_id": "69684f740ac10a06522f69ba", "user": {"_id": "64705d224be5cf1f3348d6bc", "avatarUrl": "/avatars/270bff7c7cb326528dc192fc38561a8b.svg", "isPro": false, "fullname": "Chi-Pin Huang", "user": "jasper0314-huang", "type": "user"}, "name": "Chi-Pin Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:32.512Z", "hidden": false}, {"_id": "69684f740ac10a06522f69bb", "name": "Yunze Man", "hidden": false}, {"_id": "69684f740ac10a06522f69bc", "name": "Zhiding Yu", "hidden": false}, {"_id": "69684f740ac10a06522f69bd", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:27.724Z", "hidden": false}, {"_id": "69684f740ac10a06522f69be", "name": "Jan Kautz", "hidden": false}, {"_id": "69684f740ac10a06522f69bf", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69684f740ac10a06522f69c0", "user": {"_id": "6312cab05beb528b5c1500e3", "avatarUrl": "/avatars/a328e8cc99fb031b2d5c911c4b577e7e.svg", "isPro": false, "fullname": "Fu-En Yang", "user": "FuEnYang", "type": "user"}, "name": "Fu-En Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:30.478Z", "hidden": false}], "publishedAt": "2026-01-14T18:59:59.000Z", "submittedOnDailyAt": "2026-01-15T00:10:27.528Z", "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning", "submittedOnDailyBy": {"_id": "64705d224be5cf1f3348d6bc", "avatarUrl": "/avatars/270bff7c7cb326528dc192fc38561a8b.svg", "isPro": false, "fullname": "Chi-Pin Huang", "user": "jasper0314-huang", "type": "user"}, "summary": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.", "upvotes": 36, "discussionId": "69684f740ac10a06522f69c1", "projectPage": "https://jasper0314-huang.github.io/fast-thinkact/", "ai_summary": "Fast-ThinkAct is an efficient vision-language-action framework that reduces inference latency by 89.3% through compact latent reasoning while maintaining long-horizon planning and few-shot adaptation capabilities.", "ai_keywords": ["chain-of-thought", "latent reasoning", "preference-guided objective", "embodied control", "policy learning", "inference latency", "vision-language-action"], "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>Vision-Language-Action (VLA) \u4efb\u52a1\u9700\u8981\u5728\u590d\u6742\u7684\u89c6\u89c9\u573a\u666f\u4e2d\u8fdb\u884c\u63a8\u7406\u5e76\u6267\u884c\u9002\u5e94\u6027\u52a8\u4f5c\u3002</li>\n    <li>\u4f20\u7edf\u7684\u63a8\u7406\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u63a8\u7406\u65f6\u95f4\u8f83\u957f\uff0c\u5f71\u54cd\u6548\u7387\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86 Fast-ThinkAct\uff0c\u4e00\u4e2a\u9ad8\u6548\u7684\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u53e3\u5934\u8868\u8fbe\u7684\u6f5c\u5728\u63a8\u7406\u5b9e\u73b0\u7d27\u51d1\u800c\u9ad8\u6548\u7684\u89c4\u5212\u3002</li>\n    <li>Fast-ThinkAct \u901a\u8fc7\u4ece\u6559\u5e08\u6a21\u578b\u4e2d\u5b66\u4e60\uff0c\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u5e76\u5bf9\u64cd\u4f5c\u8f68\u8ff9\u8fdb\u884c\u504f\u597d\u5f15\u5bfc\u5bf9\u9f50\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFast-ThinkAct \u5728\u63a8\u7406\u65f6\u95f4\u4e0a\u6bd4\u6700\u5148\u8fdb\u7684 VLA \u65b9\u6cd5\u51cf\u5c11\u4e86 89.3%\uff0c\u5e76\u4fdd\u6301\u4e86\u826f\u597d\u7684\u89c4\u5212\u80fd\u529b\u548c\u9002\u5e94\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-Language-Action (VLA) tasks involve understanding complex visuals and performing actions in changing environments.</li>\n    <li>Previous methods using explicit reasoning improve performance but are slow due to lengthy processes.</li>\n    <li>Fast-ThinkAct is a new framework that allows for quicker reasoning by using compact reasoning methods called latent chain-of-thoughts (CoTs).</li>\n    <li>It learns from a teacher model to align language and visual planning, improving action execution.</li>\n    <li>Fast-ThinkAct shows significant speed improvements, reducing inference time by up to 89.3%, while still being effective in planning and adapting to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-14T13:59:59.000Z", "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning", "summary": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09708.png", "numComments": 1, "submittedBy": {"_id": "64705d224be5cf1f3348d6bc", "avatarUrl": "/avatars/270bff7c7cb326528dc192fc38561a8b.svg", "fullname": "Chi-Pin Huang", "name": "jasper0314-huang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u5728\u89c6\u9891\u95ee\u7b54\u4e2d\uff0c\u89c6\u9891\u901a\u5e38\u53ea\u63d0\u4f9b\u5c40\u90e8\u89c6\u89c9\u7ebf\u7d22\uff0c\u800c\u53ef\u9a8c\u8bc1\u7684\u7b54\u6848\u5206\u5e03\u5728\u7f51\u7edc\u4e0a\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0c\u79f0\u4e3aVideoDR\u3002</li>\n    <li>VideoDR\u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u4e0b\u7684\u5f00\u653e\u9886\u57df\u95ee\u7b54\uff0c\u8981\u6c42\u8de8\u5e27\u89c6\u89c9\u951a\u70b9\u63d0\u53d6\u548c\u591a\u8df3\u63a8\u7406\u3002</li>\n    <li>\u901a\u8fc7\u4e25\u683c\u7684\u4eba\u4e3a\u6ce8\u91ca\u548c\u8d28\u91cf\u63a7\u5236\uff0c\u83b7\u5f97\u4e86\u516d\u4e2a\u8bed\u4e49\u9886\u57df\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u6837\u672c\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u578b\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u5982\u5de5\u4f5c\u6d41\u6a21\u578b\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u4fdd\u6301\u521d\u59cb\u89c6\u9891\u951a\u70b9\u7684\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Videos often lack complete information, so models need to extract visual clues, fetch information from the web, and verify answers.</li>\n    <li>We created a new benchmark called VideoDR for video question answering, focusing on using videos and web resources together.</li>\n    <li>VideoDR includes high-quality samples from six different areas, carefully annotated for research purposes.</li>\n    <li>We tested various language models and found that the effectiveness of different approaches (Workflow vs. Agentic) depends on how well they can track information over long retrieval processes.</li>\n    <li>This research highlights important challenges for developing better video question answering systems that can work with online information.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u73b0\u4ee3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u7406\u89e3\u4e0a\u4ecd\u7136\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\uff0c\u800c\u4eba\u7c7b\u5728\u53d1\u5c55\u6838\u5fc3\u89c6\u89c9\u6280\u80fd\u65f6\u5e76\u4e0d\u4f9d\u8d56\u8bed\u8a00\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684MLLMs\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u8fde3\u5c81\u7684\u5c0f\u5b69\u90fd\u80fd\u8f7b\u677e\u89e3\u51b3\u3002</li>\n    <li>\u4e3a\u4e86\u8bc4\u4f30MLLMs\u7684\u6838\u5fc3\u89c6\u89c9\u80fd\u529b\uff0c\u7814\u7a76\u56e2\u961f\u63a8\u51fa\u4e86BabyVision\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b388\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u9886\u5148\u7684MLLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c6\uff0cGemini3-Pro-Preview\u7684\u5f97\u5206\u4e3a49.7\uff0c\u8fdc\u4f4e\u4e8e6\u5c81\u513f\u7ae5\uff0894.1\uff09\u548c\u6210\u5e74\u4eba\u3002</li>\n    <li>BabyVision\u7684\u8fdb\u5c55\u6807\u5fd7\u7740\u671d\u7740\u4eba\u7c7b\u6c34\u5e73\u7684\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u5e76\u63d0\u51fa\u4e86BabyVision-Gen\u548c\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\u3002\u4ee3\u7801\u548c\u57fa\u51c6\u6570\u636e\u5df2\u5728GitHub\u4e0a\u53d1\u5e03\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multimodal LLMs (MLLMs) struggle with basic visual tasks that young children can easily do.</li>\n    <li>BabyVision is a new benchmark created to test MLLMs' visual skills without relying on language knowledge.</li>\n    <li>It includes 388 items across 22 subclasses and four main categories to evaluate visual abilities.</li>\n    <li>Current top MLLMs score much lower than humans, with Gemini3-Pro-Preview scoring 49.7 compared to 94.1 for adults.</li>\n    <li>BabyVision aims to improve MLLMs' visual perception and reasoning, and related resources are available online.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u4e30\u5bcc\u7684\u8bed\u4e49\u5b9e\u4f53\uff0c\u5206\u5272\u8fd9\u4e9b\u5b9e\u4f53\u5bf9\u4e8e\u540e\u7eed\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>\u76ee\u524d\u7684\u5206\u5272\u6a21\u578b\u5bf9\u7269\u7406\u5c5e\u6027\u5b9a\u4e49\u7684\u5b9e\u4f53\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\uff08\u5982\u5b66\u6821\u3001\u516c\u56ed\uff09\u4ecd\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u793e\u4f1a\u8bed\u4e49\u5206\u5272\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86Urban Socio-Semantic Segmentation\u6570\u636e\u96c6SocioSeg\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u793e\u4ea4\u8bed\u4e49\u5b9e\u4f53\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u63d0\u51fa\u4e86SocioReasoner\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u8bc6\u522b\u548c\u591a\u9636\u6bb5\u63a8\u7406\uff0c\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u6807\u6ce8\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u8fc7\u7a0b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban surfaces contain many different types of entities, and it's important to identify them from satellite images.</li>\n    <li>Current models can identify physical features well, but struggle with socially defined categories like schools and parks.</li>\n    <li>The authors created a new dataset called SocioSeg, which includes satellite images and detailed labels for social entities.</li>\n    <li>They developed a framework called SocioReasoner that mimics how humans identify social entities using visual and language cues.</li>\n    <li>Tests show that their method performs better than existing models and can generalize well to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u5e73\u8861\u7d27\u51d1\u6548\u7387\u548c\u591a\u6a21\u6001\u667a\u80fd\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4f7f\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u4e86\u8bed\u8a00\u5bf9\u9f50\u7684\u611f\u77e5\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff0c\u589e\u5f3a\u4e86\u89c6\u89c9\u548c\u8bed\u8a00\u7684\u534f\u540c\u4f5c\u7528\u3002</li>\n    <li>\u901a\u8fc7\u8d85\u8fc71000\u6b21\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\uff0c\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8ba1\u7b97\u6548\u7387\u3002</li>\n    <li>\u5c3d\u7ba1\u6a21\u578b\u4f53\u79ef\u53ea\u670910\u4ebf\u53c2\u6570\uff0c\u4f46\u5176\u6027\u80fd\u53ef\u4ee5\u4e0e10\u523020\u500d\u5927\u7684\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u751a\u81f3\u8d85\u8fc7\u4e00\u4e9b\u9876\u7ea7\u6a21\u578b\u3002</li>\n    <li>STEP3-VL-10B \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u79c0\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u3001\u9ad8\u6548\u4e14\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6a21\u578b\u4f9b\u793e\u533a\u4f7f\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a new, lightweight open-source model aimed at combining efficiency with advanced multimodal intelligence.</li>\n    <li>It uses a unique training method involving 1.2 trillion multimodal tokens and incorporates a vision-language synergy with a special encoder and decoder.</li>\n    <li>The model includes a post-training process with extensive reinforcement learning to enhance its capabilities.</li>\n    <li>Despite being smaller than many other models, STEP3-VL-10B performs exceptionally well, often outperforming much larger models and other top competitors.</li>\n    <li>The creators are sharing the full model to help others in the community use and build upon this technology.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u901a\u8fc7\u89c6\u89c9\u7ebf\u7d22\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\u3002</li>\n    <li>\u73b0\u6709\u7684\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4eba\u7c7b\u5e38\u7528\u7684\u7b56\u7565\u2014\u2014\u4f7f\u7528\u5730\u56fe\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u5730\u56fe\u601d\u8003\u201d\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u8bbe\u5b9a\u4e3a\u4e00\u4e2a\u201c\u5730\u56fe\u4e2d\u7684\u667a\u80fd\u4f53\u201d\u5faa\u73af\u3002</li>\n    <li>\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u91c7\u6837\u6548\u7387\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The task of image geolocalization is to predict where an image was taken on Earth using visual clues.</li>\n    <li>Current models use advanced reasoning but often ignore the human strategy of using maps.</li>\n    <li>This work introduces a new method called \"Thinking with Map\" that incorporates map usage into the model.</li>\n    <li>The method involves two main steps: reinforcement learning to enhance model efficiency and test-time scaling to explore multiple options before making a prediction.</li>\n    <li>The new approach, tested on real-world images through a benchmark called MAPBench, shows significant improvement over existing models.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.08763", "authors": [{"_id": "6969b0a232f0333869ff946a", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:38.232Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946b", "user": {"_id": "6891c906f3c31445cc040ab1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891c906f3c31445cc040ab1/NBqxXOY7al4CD0XBj8ke2.jpeg", "isPro": false, "fullname": "Yucheng Wang", "user": "DevilEnfant", "type": "user"}, "name": "Yucheng Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:48.080Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946c", "name": "Yufei He", "hidden": false}, {"_id": "6969b0a232f0333869ff946d", "user": {"_id": "682deb444988bd82847e2b03", "avatarUrl": "/avatars/15da087e84386ea72c6fa2db63571420.svg", "isPro": false, "fullname": "Jia-Ying Wu", "user": "EricaWu", "type": "user"}, "name": "Jiaying Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:59.692Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946e", "name": "Yilun Zhao", "hidden": false}, {"_id": "6969b0a232f0333869ff946f", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0a232f0333869ff9470", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:06.327Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9471", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:12.181Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9472", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:17.979Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9473", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:23.007Z", "hidden": false}], "publishedAt": "2026-01-13T17:48:43.000Z", "submittedOnDailyAt": "2026-01-16T01:00:36.686Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "upvotes": 111, "discussionId": "6969b0a232f0333869ff9474", "ai_summary": "Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.", "ai_keywords": ["reinforcement learning", "large language models", "exploration collapse", "pass@k", "pass@1", "rollout-level objective", "high-level solution strategies", "clustering", "policy advantages", "AUC@K"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u5904\u7406\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u65f6\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5e38\u5e38\u51fa\u73b0\u63a2\u7d22\u5d29\u6e83\u7684\u95ee\u9898\u3002</li>\n    <li>\u8fd9\u79cd\u95ee\u9898\u4f7f\u5f97\u7b56\u7565\u8fc7\u65e9\u96c6\u4e2d\u4e8e\u5c11\u6570\u4e3b\u5bfc\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u591a\u6837\u6027\u548c\u66f4\u9ad8\u5c42\u6b21\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u2014\u2014\u72ec\u7279\u6027\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\uff0c\u4e13\u95e8\u5956\u52b1\u5c55\u73b0\u7a00\u6709\u9ad8\u5c42\u7b56\u7565\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u5668\u5bf9\u76f8\u540c\u95ee\u9898\u7684\u591a\u6b21\u5c1d\u8bd5\u8fdb\u884c\u805a\u7c7b\uff0c\u4ece\u800c\u63d0\u9ad8\u72ec\u7279\u7b56\u7565\u7684\u5956\u52b1\u3002</li>\n    <li>\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u533b\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u901a\u8fc7\u7387\u7684\u540c\u65f6\uff0c\u4e5f\u63d0\u9ad8\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u591a\u6837\u6027\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) is important for improving large language models (LLMs), especially for complex reasoning tasks.</li>\n    <li>Current RL methods often get stuck focusing on a few common strategies, which limits diversity in solutions.</li>\n    <li>The proposed solution, called Uniqueness-Aware Reinforcement Learning, encourages unique and correct solutions by rewarding rare strategies.</li>\n    <li>This method uses an LLM-based judge to group solutions and gives higher rewards to novel strategies rather than redundant ones.</li>\n    <li>Testing shows that this approach improves performance and diversity in problem-solving without losing accuracy in the best solutions.</li>\n</ul>"}, "publishedAt": "2026-01-13T12:48:43.000Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08763.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 105, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 94, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7b56\u7565\u53ef\u4ee5\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u5168\u9762\u7406\u89e3\u548c\u6df1\u5ea6\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u8bbe\u8ba1\u4e3b\u8981\u4f5c\u4e3a\u88ab\u52a8\u5b58\u50a8\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u4fe1\u606f\u4e4b\u95f4\u7684\u9ad8\u9636\u5173\u8054\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HGMem\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u80fd\u52a8\u6001\u5730\u8868\u8fbe\u590d\u6742\u63a8\u7406\u548c\u5168\u7403\u7406\u89e3\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u56fe\u7684\u8d85\u8fb9\u8868\u793a\u4e0d\u540c\u7684\u8bb0\u5fc6\u5355\u5143\uff0c\u4f7f\u5f97\u8bb0\u5fc6\u5185\u90e8\u53ef\u4ee5\u5f62\u6210\u66f4\u9ad8\u9636\u7684\u4ea4\u4e92\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHGMem\u5728\u591a\u6b65RAG\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u5f3a\u57fa\u7ebf\u7cfb\u7edf\uff0c\u6539\u5584\u4e86\u63a8\u7406\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps large language models understand complex tasks better.</li>\n    <li>Current memory systems only store facts without understanding their connections, limiting reasoning ability.</li>\n    <li>HGMem is a new memory system that organizes information in a hypergraph, allowing for better reasoning and understanding.</li>\n    <li>This method connects related facts, helping to create a more integrated knowledge structure for deeper insights.</li>\n    <li>Tests show HGMem significantly improves RAG performance compared to existing methods on various tasks.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u4e0d\u65ad\u589e\u5f3a\uff0c\u7528\u6237\u5e0c\u671b\u5b83\u4eec\u80fd\u63d0\u4f9b\u51c6\u786e\u7684\u56de\u7b54\u548c\u7b26\u5408\u591a\u6837\u5316\u4eba\u7c7b\u504f\u597d\u7684\u884c\u4e3a\u3002</li>\n    <li>\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5f00\u59cb\u4f7f\u7528\u591a\u4e2a\u5956\u52b1\u6765\u5f15\u5bfc\u6a21\u578b\u671d\u671f\u671b\u884c\u4e3a\u53d1\u5c55\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u76f4\u63a5\u5e94\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u4f1a\u4f7f\u4e0d\u540c\u5956\u52b1\u7ec4\u5408\u7684\u4f18\u52bf\u503c\u76f8\u540c\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4fe1\u53f7\u7684\u5206\u8fa8\u7387\u964d\u4f4e\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u79f0\u4e3a\u7ec4\u5956\u52b1\u89e3\u8026\u5f52\u4e00\u5316\u7b56\u7565\u4f18\u5316\uff08GDPO\uff09\uff0c\u89e3\u51b3\u4e86GRPO\u7684\u95ee\u9898\uff0c\u4fdd\u6301\u4e86\u5956\u52b1\u4e4b\u95f4\u7684\u76f8\u5bf9\u5dee\u5f02\u3002</li>\n    <li>\u5728\u5de5\u5177\u8c03\u7528\u3001\u6570\u5b66\u63a8\u7406\u548c\u7f16\u7801\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\uff0cGDPO\u7684\u8868\u73b0\u4f18\u4e8eGRPO\uff0c\u663e\u793a\u51fa\u5176\u5728\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>As language models improve, users want them to respond accurately and behave according to different human preferences.</li>\n    <li>Reinforcement learning (RL) uses multiple rewards to guide models in achieving these behaviors, but the current method (GRPO) may not be the best fit.</li>\n    <li>Using GRPO can lead to problems by making different rewards too similar, which harms training and can cause failures.</li>\n    <li>The paper introduces a new method called GDPO, which separates the normalization of rewards to maintain their differences, leading to better training performance.</li>\n    <li>GDPO is tested against GRPO in three areas and shows better results in both accuracy and adherence to constraints, proving to be more effective for multi-reward optimization.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.08521", "authors": [{"_id": "69674059c5e371f6b235d1d8", "user": {"_id": "68920f91bcf2b25e8e121cf6", "avatarUrl": "/avatars/4bc69f43828a346a3ee24b026e0edbb4.svg", "isPro": false, "fullname": "Fengkai Yang", "user": "ShortCatisLong", "type": "user"}, "name": "Fengkai Yang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-19T14:33:21.899Z", "hidden": false}, {"_id": "69674059c5e371f6b235d1d9", "user": {"_id": "6969715fb2636f5f23a9a8c5", "avatarUrl": "/avatars/5e75043891ee41bb980f71fb9e3a33ab.svg", "isPro": false, "fullname": "Zherui Chen", "user": "chenzherui007", "type": "user"}, "name": "Zherui Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:33:53.078Z", "hidden": false}, {"_id": "69674059c5e371f6b235d1da", "name": "Xiaohan Wang", "hidden": false}, {"_id": "69674059c5e371f6b235d1db", "name": "Xiaodong Lu", "hidden": false}, {"_id": "69674059c5e371f6b235d1dc", "user": {"_id": "666eb642a119281ee0bfa443", "avatarUrl": "/avatars/71317810b00978754ad439837b04faff.svg", "isPro": false, "fullname": "Jiajun Chai", "user": "PandaChai", "type": "user"}, "name": "Jiajun Chai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-19T14:37:17.404Z", "hidden": false}, {"_id": "69674059c5e371f6b235d1dd", "name": "Guojun Yin", "hidden": false}, {"_id": "69674059c5e371f6b235d1de", "name": "Wei Lin", "hidden": false}, {"_id": "69674059c5e371f6b235d1df", "name": "Shuai Ma", "hidden": false}, {"_id": "69674059c5e371f6b235d1e0", "name": "Fuzhen Zhuang", "hidden": false}, {"_id": "69674059c5e371f6b235d1e1", "name": "Deqing Wang", "hidden": false}, {"_id": "69674059c5e371f6b235d1e2", "name": "Yaodong Yang", "hidden": false}, {"_id": "69674059c5e371f6b235d1e3", "name": "Jianxin Li", "hidden": false}, {"_id": "69674059c5e371f6b235d1e4", "user": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "isPro": false, "fullname": "Yikun Ban", "user": "Yikunb", "type": "user"}, "name": "Yikun Ban", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:33:50.655Z", "hidden": false}], "publishedAt": "2026-01-13T13:03:15.000Z", "submittedOnDailyAt": "2026-01-19T00:20:58.837Z", "title": "Your Group-Relative Advantage Is Biased", "submittedOnDailyBy": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "isPro": false, "fullname": "Yikun Ban", "user": "Yikunb", "type": "user"}, "summary": "Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.\n  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.", "upvotes": 95, "discussionId": "6967405ac5e371f6b235d1e5", "ai_summary": "Group-based reinforcement learning from verifier rewards suffers from biased advantage estimation that underestimates hard prompts and overestimates easy prompts, which is addressed through a history-aware adaptive difficulty weighting method that improves performance on mathematical reasoning benchmarks.", "ai_keywords": ["Reinforcement Learning from Verifier Rewards", "group-based methods", "GRPO", "advantage estimation", "bias correction", "adaptive reweighting", "difficulty weighting", "mathematical reasoning", "benchmark evaluation"], "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\u4ece\u9a8c\u8bc1\u8005\u5956\u52b1\uff08RLVR\uff09\u662f\u4e00\u79cd\u7528\u4e8e\u540e\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e38\u7528\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u63a8\u7406\u4efb\u52a1\u3002</li>\n    <li>\u57fa\u4e8e\u7ec4\u7684\u65b9\u6cd5\u5982GRPO\u53ca\u5176\u53d8\u4f53\u88ab\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u7406\u8bba\u5c5e\u6027\u5c1a\u4e0d\u660e\u6670\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u7ec4\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\u5668\u5b58\u5728\u504f\u5dee\uff0c\u5bb9\u6613\u4f4e\u4f30\u96be\u9898\u7684\u4f18\u52bf\uff0c\u9ad8\u4f30\u7b80\u5355\u95ee\u9898\u7684\u4f18\u52bf\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff1a\u5386\u53f2\u611f\u77e5\u81ea\u9002\u5e94\u96be\u5ea6\u52a0\u6743\uff08HA-DW\uff09\uff0c\u901a\u8fc7\u8c03\u6574\u4f18\u52bf\u4f30\u8ba1\u6765\u89e3\u51b3\u504f\u5dee\u95ee\u9898\u3002</li>\n    <li>\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06HA-DW\u96c6\u6210\u5230GRPO\u53ca\u5176\u53d8\u4f53\u4e2d\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement Learning from Verifier Rewards (RLVR) is commonly used to improve large language models for reasoning tasks.</li>\n    <li>Group-based methods like GRPO use group-relative advantage estimation but have unclear theoretical properties.</li>\n    <li>This study finds that group-relative advantage estimators are biased, underestimating advantages for hard prompts and overestimating for easy ones.</li>\n    <li>To fix this bias, a new method called History-Aware Adaptive Difficulty Weighting (HA-DW) is introduced, which adjusts advantage estimates based on difficulty and training progress.</li>\n    <li>Experiments show that HA-DW improves performance in GRPO and similar methods, highlighting the importance of correcting biased advantage estimations for better training.</li>\n</ul>"}, "publishedAt": "2026-01-13T08:03:15.000Z", "title": "Your Group-Relative Advantage Is Biased", "summary": "Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.\n  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08521.png", "numComments": 5, "submittedBy": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "fullname": "Yikun Ban", "name": "Yikunb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.07348", "authors": [{"_id": "696855610ac10a06522f69cf", "user": {"_id": "662911a202f5ad9a5195932f", "avatarUrl": "/avatars/663d142e27abbdb319ed5fd2cbe3f1a4.svg", "isPro": false, "fullname": "Tu Hu", "user": "Blackteaxxx", "type": "user"}, "name": "Tu Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:18.320Z", "hidden": false}, {"_id": "696855610ac10a06522f69d0", "name": "Ronghao Chen", "hidden": false}, {"_id": "696855610ac10a06522f69d1", "user": {"_id": "65562edfb7bad186e877c724", "avatarUrl": "/avatars/bb91f42b102e113208bbe3238916a015.svg", "isPro": false, "fullname": "zhangshuo", "user": "mcflurryshuoz", "type": "user"}, "name": "Shuo Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:16.329Z", "hidden": false}, {"_id": "696855610ac10a06522f69d2", "name": "Jianghao Yin", "hidden": false}, {"_id": "696855610ac10a06522f69d3", "name": "Mou Xiao Feng", "hidden": false}, {"_id": "696855610ac10a06522f69d4", "name": "Jingping Liu", "hidden": false}, {"_id": "696855610ac10a06522f69d5", "name": "Shaolei Zhang", "hidden": false}, {"_id": "696855610ac10a06522f69d6", "name": "Wenqi Jiang", "hidden": false}, {"_id": "696855610ac10a06522f69d7", "name": "Yuqi Fang", "hidden": false}, {"_id": "696855610ac10a06522f69d8", "name": "Sen Hu", "hidden": false}, {"_id": "696855610ac10a06522f69d9", "name": "Yi Xu", "hidden": false}, {"_id": "696855610ac10a06522f69da", "user": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "name": "Huacan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:20.275Z", "hidden": false}], "publishedAt": "2026-01-12T09:23:13.000Z", "submittedOnDailyAt": "2026-01-15T00:23:14.421Z", "title": "Controlled Self-Evolution for Algorithmic Code Optimization", "submittedOnDailyBy": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.", "upvotes": 94, "discussionId": "696855610ac10a06522f69db", "githubRepo": "https://github.com/QuantaAlpha/EvoControl", "githubRepoAddedBy": "user", "ai_summary": "Controlled Self-Evolution method improves code generation through diversified initialization, feedback-guided genetic evolution, and hierarchical memory to enhance exploration efficiency and solution quality.", "ai_keywords": ["self-evolution methods", "generate-verify-refine cycles", "exploration efficiency", "initialization bias", "stochastic operations", "feedback guidance", "genetic evolution", "targeted mutation", "compositional crossover", "hierarchical evolution memory", "LLM backbones", "EffiBench-X"], "githubStars": 79, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "summary_zh": "<ul>\n    <li>\u81ea\u6211\u8fdb\u5316\u65b9\u6cd5\u901a\u8fc7\u201c\u751f\u6210-\u9a8c\u8bc1-\u6539\u8fdb\u201d\u5faa\u73af\u63d0\u5347\u4ee3\u7801\u751f\u6210\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u4f4e\uff0c\u96be\u4ee5\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u627e\u5230\u66f4\u4f18\u89e3\u3002</li>\n    <li>\u4f4e\u6548\u7684\u539f\u56e0\u5305\u62ec\u521d\u59cb\u5316\u504f\u5dee\u3001\u7f3a\u4e4f\u53cd\u9988\u6307\u5bfc\u7684\u968f\u673a\u64cd\u4f5c\u548c\u8de8\u4efb\u52a1\u7ecf\u9a8c\u5229\u7528\u4e0d\u8db3\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u53d7\u63a7\u81ea\u6211\u8fdb\u5316\uff08CSE\uff09\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u591a\u6837\u5316\u89c4\u5212\u521d\u59cb\u5316\u3001\u53cd\u9988\u6307\u5bfc\u7684\u9057\u4f20\u8fdb\u5316\u548c\u5c42\u6b21\u8fdb\u5316\u8bb0\u5fc6\u3002</li>\n    <li>CSE\u5728EffiBench-X\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\uff0c\u5c24\u5176\u5728\u65e9\u671f\u9636\u6bb5\u6548\u7387\u66f4\u9ad8\uff0c\u5e76\u4e14\u5728\u6574\u4e2a\u8fdb\u5316\u8fc7\u7a0b\u4e2d\u6301\u7eed\u6539\u8fdb\u3002</li>\n    <li>\u6211\u4eec\u7684\u4ee3\u7801\u5df2\u516c\u5f00\uff0c\u7f51\u5740\u662f\uff1ahttps://github.com/QuantaAlpha/EvoControl\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Self-evolution methods improve code generation using a cycle of generating, verifying, and refining solutions.</li>\n    <li>Current methods struggle with finding better solutions efficiently due to biases and ineffective operations.</li>\n    <li>Controlled Self-Evolution (CSE) aims to solve these issues with three main features: diverse strategy planning, feedback-guided evolution, and a memory system for experiences.</li>\n    <li>Experiments show that CSE outperforms existing methods across different models and is more efficient from the start.</li>\n    <li>The code for CSE is available online for public use.</li>\n</ul>"}, "publishedAt": "2026-01-12T04:23:13.000Z", "title": "Controlled Self-Evolution for Algorithmic Code Optimization", "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07348.png", "numComments": 3, "submittedBy": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "fullname": "Huacan Wang", "name": "Huacan-Wang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Jan 21, 2026";
window.trendingPapers = {
    "today": [{"paper": {"id": "2601.16725", "authors": [{"_id": "6976d5405d41524304c13537", "name": "Meituan LongCat Team", "hidden": false}, {"_id": "6976d5405d41524304c13538", "name": "Anchun Gui", "hidden": false}, {"_id": "6976d5405d41524304c13539", "name": "Bei Li", "hidden": false}, {"_id": "6976d5405d41524304c1353a", "name": "Bingyang Tao", "hidden": false}, {"_id": "6976d5405d41524304c1353b", "name": "Bole Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1353c", "name": "Borun Chen", "hidden": false}, {"_id": "6976d5405d41524304c1353e", "name": "Chao Zhang", "hidden": false}, {"_id": "69772bc15d41524304c13739", "name": "Chao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1353f", "name": "Chen Gao", "hidden": false}, {"_id": "6976d5405d41524304c13540", "name": "Chen Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13541", "name": "Chengcheng Han", "hidden": false}, {"_id": "6976d5405d41524304c13542", "name": "Chenhui Yang", "hidden": false}, {"_id": "6976d5405d41524304c13543", "name": "Chuyu Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13544", "name": "Cong Chen", "hidden": false}, {"_id": "6976d5405d41524304c13545", "name": "Cunguang Wang", "hidden": false}, {"_id": "6976d5405d41524304c13546", "name": "Daoru Pan", "hidden": false}, {"_id": "6976d5405d41524304c13547", "name": "Defei Bu", "hidden": false}, {"_id": "6976d5405d41524304c13548", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13549", "name": "Di Xiu", "hidden": false}, {"_id": "6976d5405d41524304c1354a", "name": "Dishan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1354b", "name": "Dongyu Ru", "hidden": false}, {"_id": "6976d5405d41524304c1354c", "name": "Dunwei Tu", "hidden": false}, {"_id": "6976d5405d41524304c1354d", "name": "Fan Wu", "hidden": false}, {"_id": "6976d5405d41524304c1354e", "name": "Fengcheng Yuan", "hidden": false}, {"_id": "6976d5405d41524304c1354f", "name": "Fengcun Li", "hidden": false}, {"_id": "6976d5405d41524304c13550", "name": "Gang Xu", "hidden": false}, {"_id": "6976d5405d41524304c13551", "name": "Guanyu Wu", "hidden": false}, {"_id": "6976d5405d41524304c13552", "name": "Guoyuan Lin", "hidden": false}, {"_id": "6976d5405d41524304c13553", "name": "Haibin Wang", "hidden": false}, {"_id": "6976d5405d41524304c13554", "name": "Hansi Yang", "hidden": false}, {"_id": "6976d5405d41524304c13555", "name": "Hao Yang", "hidden": false}, {"_id": "6976d5405d41524304c13556", "name": "Haonan Yan", "hidden": false}, {"_id": "6976d5405d41524304c13557", "name": "Haoxiang Ma", "hidden": false}, {"_id": "6976d5405d41524304c13558", "name": "Haoxing Wen", "hidden": false}, {"_id": "6976d5405d41524304c13559", "name": "Hongyan Hao", "hidden": false}, {"_id": "6976d5405d41524304c1355a", "name": "Hongyin Tang", "hidden": false}, {"_id": "6976d5405d41524304c1355b", "name": "Hongyu Zang", "hidden": false}, {"_id": "6976d5405d41524304c1355c", "name": "Hongzhi Ni", "hidden": false}, {"_id": "6976d5405d41524304c1355d", "name": "Hui Su", "hidden": false}, {"_id": "6976d5405d41524304c1355e", "name": "Jiacheng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1355f", "name": "Jiahong Zhou", "hidden": false}, {"_id": "6976d5405d41524304c13560", "name": "Jiahuan Li", "hidden": false}, {"_id": "6976d5405d41524304c13561", "name": "Jiaming Wang", "hidden": false}, {"_id": "6976d5405d41524304c13562", "name": "Jian Yang", "hidden": false}, {"_id": "6976d5405d41524304c13563", "user": {"_id": "64008a0af4ff62c2616d8858", "avatarUrl": "/avatars/b52c98857916fba5377ace8089d658b2.svg", "isPro": false, "fullname": "zhangjf", "user": "zhangjf", "type": "user"}, "name": "Jianfei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:09.272Z", "hidden": false}, {"_id": "6976d5405d41524304c13564", "name": "Jianhao Xu", "hidden": false}, {"_id": "6976d5405d41524304c13565", "name": "Jianing Wang", "hidden": false}, {"_id": "6976d5405d41524304c13566", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13567", "name": "Jiaqi Sun", "hidden": false}, {"_id": "6976d5405d41524304c13568", "name": "Jiarong Shi", "hidden": false}, {"_id": "6976d5405d41524304c13569", "name": "Jiarui Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1356a", "name": "Jingang Wang", "hidden": false}, {"_id": "6976d5405d41524304c1356b", "user": {"_id": "6592472fccbc1e2cc7250903", "avatarUrl": "/avatars/6f04ae66944eb2ce65c5aca7927bab10.svg", "isPro": false, "fullname": "Jinluan Yang", "user": "Jinluan", "type": "user"}, "name": "Jinluan Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:47.175Z", "hidden": false}, {"_id": "6976d5405d41524304c1356c", "name": "Jinrui Ding", "hidden": false}, {"_id": "6976d5405d41524304c1356d", "name": "Jinwei Xiao", "hidden": false}, {"_id": "6976d5405d41524304c1356e", "name": "Jiyuan He", "hidden": false}, {"_id": "6976d5405d41524304c1356f", "name": "Juncan Xu", "hidden": false}, {"_id": "6976d5405d41524304c13570", "name": "Kefeng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13571", "name": "Keheng Wang", "hidden": false}, {"_id": "6976d5405d41524304c13572", "name": "Li Wei", "hidden": false}, {"_id": "6976d5405d41524304c13573", "name": "Lianhui Ma", "hidden": false}, {"_id": "6976d5405d41524304c13574", "name": "Lin Qiu", "hidden": false}, {"_id": "6976d5405d41524304c13575", "name": "Lingbing Kong", "hidden": false}, {"_id": "6976d5405d41524304c13576", "name": "Lingchuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c13577", "name": "Linsen Guo", "hidden": false}, {"_id": "6976d5405d41524304c13578", "name": "Mengshen Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13579", "name": "Mengxia Shen", "hidden": false}, {"_id": "6976d5405d41524304c1357a", "name": "Mingyang Zhu", "hidden": false}, {"_id": "6976d5405d41524304c1357b", "name": "Peiguang Li", "hidden": false}, {"_id": "6976d5405d41524304c1357c", "name": "Peng Pei", "hidden": false}, {"_id": "6976d5405d41524304c1357d", "name": "Pengcheng Jia", "hidden": false}, {"_id": "6976d5405d41524304c1357e", "name": "Pengtao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1357f", "name": "Peng Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13580", "name": "Qi Gu", "hidden": false}, {"_id": "6976d5405d41524304c13581", "name": "Qiong Huang", "hidden": false}, {"_id": "6976d5405d41524304c13582", "name": "Qiyuan Duan", "hidden": false}, {"_id": "6976d5405d41524304c13583", "name": "Quanchi Weng", "hidden": false}, {"_id": "6976d5405d41524304c13584", "name": "Rongxiang Weng", "hidden": false}, {"_id": "6976d5405d41524304c13585", "name": "Rongzhi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13586", "name": "Rumei Li", "hidden": false}, {"_id": "6976d5405d41524304c13587", "name": "Shanglin Lei", "hidden": false}, {"_id": "6976d5405d41524304c13588", "user": {"_id": "64db5f5dd68a6ddcc7bd89e9", "avatarUrl": "/avatars/69375ec915927b855813df8a6d486837.svg", "isPro": false, "fullname": "Shengnan An", "user": "ShengnanAn", "type": "user"}, "name": "Shengnan An", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:11.410Z", "hidden": false}, {"_id": "6976d5405d41524304c13589", "name": "Shijun Dai", "hidden": false}, {"_id": "6976d5405d41524304c1358a", "name": "Shuaikang Liu", "hidden": false}, {"_id": "6976d5405d41524304c1358b", "name": "Shuang Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1358c", "name": "Shuo Wang", "hidden": false}, {"_id": "6976d5405d41524304c1358d", "name": "Songyuan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1358e", "name": "Tao Liang", "hidden": false}, {"_id": "6976d5405d41524304c1358f", "name": "Tianhao Hu", "hidden": false}, {"_id": "6976d5405d41524304c13590", "name": "Tianze Chen", "hidden": false}, {"_id": "6976d5405d41524304c13591", "name": "Wei Liu", "hidden": false}, {"_id": "6976d5405d41524304c13592", "name": "Wei Shi", "hidden": false}, {"_id": "6976d5405d41524304c13593", "name": "Wei Wang", "hidden": false}, {"_id": "6976d5405d41524304c13594", "name": "Weifeng Tang", "hidden": false}, {"_id": "6976d5405d41524304c13595", "name": "Wenjie Shi", "hidden": false}, {"_id": "6976d5405d41524304c13596", "name": "Wenlong Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13597", "name": "Wentao Chen", "hidden": false}, {"_id": "6976d5405d41524304c13598", "name": "Wentao Shi", "hidden": false}, {"_id": "6976d5405d41524304c13599", "name": "Xi Su", "hidden": false}, {"_id": "6976d5405d41524304c1359a", "name": "Xiangcheng Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359b", "name": "Xiandi Ma", "hidden": false}, {"_id": "6976d5405d41524304c1359c", "user": {"_id": "63edb098679c2cc40abc6c2e", "avatarUrl": "/avatars/288c7229937c2c3f29fda6d17c7df2eb.svg", "isPro": false, "fullname": "Xiangyu", "user": "xixy", "type": "user"}, "name": "Xiangyu Xi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:13.312Z", "hidden": false}, {"_id": "6976d5405d41524304c1359d", "name": "Xiangyuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359e", "name": "Xiangzhou Huang", "hidden": false}, {"_id": "6976d5405d41524304c1359f", "name": "Xiao Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a0", "name": "Xiaodong Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a1", "name": "Xiaolong Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a2", "name": "Xiaowei Shi", "hidden": false}, {"_id": "6976d5405d41524304c135a3", "name": "Xiaoyu Li", "hidden": false}, {"_id": "6976d5405d41524304c135a4", "name": "Xin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a5", "name": "Xingchen Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a6", "name": "Xuan Huang", "hidden": false}, {"_id": "6976d5405d41524304c135a7", "name": "Xuezhi Cao", "hidden": false}, {"_id": "6976d5405d41524304c135a8", "name": "Xunliang Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a9", "name": "Yan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135aa", "user": {"_id": "63fc1c420aab06079200c15c", "avatarUrl": "/avatars/8e8e82a9a6552848581ca9f65011263c.svg", "isPro": false, "fullname": "yang bai", "user": "byang", "type": "user"}, "name": "Yang Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:07.036Z", "hidden": false}, {"_id": "6976d5405d41524304c135ab", "name": "Yang Liu", "hidden": false}, {"_id": "6976d5405d41524304c135ac", "name": "Yang Yang", "hidden": false}, {"_id": "6976d5405d41524304c135ad", "name": "Yang Zheng", "hidden": false}, {"_id": "6976d5405d41524304c135ae", "name": "Yaoming Wang", "hidden": false}, {"_id": "6976d5405d41524304c135af", "name": "Yaoming Zhu", "hidden": false}, {"_id": "6976d5405d41524304c135b0", "name": "Yaqi Huo", "hidden": false}, {"_id": "6976d5405d41524304c135b1", "name": "Yanyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b2", "name": "Yaorui Shi", "hidden": false}, {"_id": "6976d5405d41524304c135b3", "name": "Yerui Sun", "hidden": false}, {"_id": "6976d5405d41524304c135b4", "name": "Yi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b5", "name": "Yihao Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b6", "name": "Yi-Kai Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b7", "name": "Yifan Lu", "hidden": false}, {"_id": "6976d5405d41524304c135b8", "name": "Yifan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135b9", "name": "Yitao Zhai", "hidden": false}, {"_id": "6976d5405d41524304c135ba", "name": "Yongjing Yin", "hidden": false}, {"_id": "6976d5405d41524304c135bb", "name": "Yongwei Zhou", "hidden": false}, {"_id": "6976d5405d41524304c135bc", "name": "Youshao Xiao", "hidden": false}, {"_id": "6976d5405d41524304c135bd", "name": "Yuchuan Dai", "hidden": false}, {"_id": "6976d5405d41524304c135be", "name": "Yuchen Xie", "hidden": false}, {"_id": "6976d5405d41524304c135bf", "name": "Yuchen Yu", "hidden": false}, {"_id": "6976d5405d41524304c135c0", "name": "Yufei Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135c1", "name": "Yuhuai Wei", "hidden": false}, {"_id": "6976d5405d41524304c135c2", "name": "Yulei Qian", "hidden": false}, {"_id": "6976d5405d41524304c135c3", "name": "Yunfan Liang", "hidden": false}, {"_id": "6976d5405d41524304c135c4", "name": "Yunke Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135c5", "name": "Yuwei Jiang", "hidden": false}, {"_id": "6976d5405d41524304c135c6", "name": "Yuxin Bian", "hidden": false}, {"_id": "6976d5405d41524304c135c7", "name": "Yuxin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135c8", "name": "Yuxin Liu", "hidden": false}, {"_id": "6976d5405d41524304c135c9", "name": "Yue Xu", "hidden": false}, {"_id": "6976d5405d41524304c135ca", "name": "Yueqing Sun", "hidden": false}, {"_id": "6976d5405d41524304c135cb", "name": "Zeyang Yu", "hidden": false}, {"_id": "6976d5405d41524304c135cc", "name": "Zhao Yang", "hidden": false}, {"_id": "6976d5405d41524304c135cd", "name": "Zhengsheng Huang", "hidden": false}, {"_id": "6976d5405d41524304c135ce", "name": "Zhengyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135cf", "name": "Zhijian Liu", "hidden": false}, {"_id": "6976d5405d41524304c135d0", "name": "Zhikang Xia", "hidden": false}, {"_id": "6976d5405d41524304c135d1", "name": "Zhimin Lin", "hidden": false}, {"_id": "6976d5405d41524304c135d2", "name": "Zhiyuan Yao", "hidden": false}, {"_id": "6976d5405d41524304c135d3", "name": "Zhuofan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135d4", "name": "Zhuowen Han", "hidden": false}, {"_id": "6976d5405d41524304c135d5", "name": "Zijian Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135d6", "name": "Ziran Li", "hidden": false}, {"_id": "6976d5405d41524304c135d7", "name": "Ziwen Wang", "hidden": false}, {"_id": "6976d5405d41524304c135d8", "name": "Ziyuan Zhuang", "hidden": false}], "publishedAt": "2026-01-23T13:20:09.000Z", "submittedOnDailyAt": "2026-01-26T00:15:28.340Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "upvotes": 136, "discussionId": "6976d5405d41524304c135d9", "ai_summary": "A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for real-world robustness and complex reasoning.", "ai_keywords": ["Mixture-of-Experts", "agentic reasoning", "domain-parallel expert training", "fusion", "asynchronous reinforcement learning", "DORA", "long-tailed generation", "multi-turn interactions", "real-world noise patterns", "test-time scaling", "reasoning depth", "reasoning width", "parallel thinking"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86LongCat-Flash-Thinking-2601\uff0c\u8fd9\u662f\u4e00\u4e2a\u62e5\u67095600\u4ebf\u53c2\u6570\u7684\u5f00\u6e90\u6df7\u5408\u4e13\u5bb6\u63a8\u7406\u6a21\u578b\uff0c\u5177\u6709\u51fa\u8272\u7684\u4ee3\u7406\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5728\u591a\u4e2a\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5305\u62ec\u4ee3\u7406\u641c\u7d22\u548c\u5de5\u5177\u4f7f\u7528\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u901a\u7528\u5316\u80fd\u529b\u3002</li>\n    <li>\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u9886\u57df\u5e76\u884c\u4e13\u5bb6\u8bad\u7ec3\u548c\u540e\u7eed\u878d\u5408\u7684\u7edf\u4e00\u8bad\u7ec3\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u590d\u6742\u5de5\u5177\u4ea4\u4e92\u7684\u8868\u73b0\u3002</li>\n    <li>\u4e3a\u4f18\u5316\u591a\u73af\u5883\u8bad\u7ec3\uff0c\u6269\u5c55\u4e86DORA\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u3002</li>\n    <li>\u5f15\u5165\u91cd\u601d\u8003\u6a21\u5f0f\uff0c\u901a\u8fc7\u589e\u52a0\u63a8\u7406\u7684\u6df1\u5ea6\u548c\u5e7f\u5ea6\uff0c\u589e\u5f3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongCat-Flash-Thinking-2601 is a powerful open-source reasoning model with 560 billion parameters, excelling in complex reasoning tasks.</li>\n    <li>It outperforms other open-source models in various benchmarks, including tool use and agentic search.</li>\n    <li>The model's effectiveness comes from a unique training approach that integrates expert training and advanced data construction methods.</li>\n    <li>It is designed to handle real-world complexities and noise, enhancing its reliability in practical applications.</li>\n    <li>A special feature called Heavy Thinking mode allows the model to improve its reasoning capabilities during testing by increasing its processing depth and breadth.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:20:09.000Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16725.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16746", "authors": [{"_id": "6976d4105d41524304c13517", "name": "Yuhang Wang", "hidden": false}, {"_id": "6976d4105d41524304c13518", "user": {"_id": "645b0c3ec35da9c7afd95421", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg", "isPro": false, "fullname": "Yuling", "user": "YerbaPage", "type": "user"}, "name": "Yuling Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:56.805Z", "hidden": false}, {"_id": "6976d4105d41524304c13519", "name": "Mo Yang", "hidden": false}, {"_id": "6976d4105d41524304c1351a", "name": "Rongrui Zhang", "hidden": false}, {"_id": "6976d4105d41524304c1351b", "name": "Shilin He", "hidden": false}, {"_id": "6976d4105d41524304c1351c", "name": "Heng Lian", "hidden": false}, {"_id": "6976d4105d41524304c1351d", "name": "Yuting Chen", "hidden": false}, {"_id": "6976d4105d41524304c1351e", "name": "Siyu Ye", "hidden": false}, {"_id": "6976d4105d41524304c1351f", "name": "Kai Cai", "hidden": false}, {"_id": "6976d4105d41524304c13520", "name": "Xiaodong Gu", "hidden": false}], "publishedAt": "2026-01-23T13:51:59.000Z", "submittedOnDailyAt": "2026-01-26T00:10:23.451Z", "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As a result, they frequently disrupt syntactic and logical structure and fail to retain critical implementation details. In this paper, we propose SWE-Pruner, a self-adaptive context pruning framework tailored for coding agents. Drawing inspiration from how human programmers \"selectively skim\" source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. Given the current task, the agent formulates an explicit goal (e.g., \"focus on error handling\") as a hint to guide the pruning targets. A lightweight neural skimmer (0.6B parameters) is trained to dynamically select relevant lines from the surrounding context given the goal. Evaluations across four benchmarks and multiple models validate SWE-Pruner's effectiveness in various scenarios, achieving 23-54% token reduction on agent tasks like SWE-Bench Verified and up to 14.84x compression on single-turn tasks like LongCodeQA with minimal performance impact.", "upvotes": 63, "discussionId": "6976d4105d41524304c13521", "githubRepo": "https://github.com/Ayanami1314/swe-pruner", "githubRepoAddedBy": "user", "ai_summary": "SWE-Pruner is a self-adaptive context pruning framework for coding agents that uses task-aware pruning to reduce token usage while maintaining performance.", "ai_keywords": ["context compression", "LongLLMLingua", "PPL", "code understanding", "task-aware adaptive pruning", "neural skimmer", "token reduction", "SWE-Bench Verified", "LongCodeQA"], "githubStars": 35, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>LLM\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u957f\u65f6\u95f4\u7684\u4ea4\u4e92\u4f1a\u5bfc\u81f4\u9ad8API\u6210\u672c\u548c\u5ef6\u8fdf\u3002</li>\n    <li>\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u6307\u6807\uff0c\u672a\u80fd\u8003\u8651\u4ee3\u7801\u7406\u89e3\u7684\u4efb\u52a1\u7279\u6027\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86SWE-Pruner\uff0c\u4e00\u4e2a\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u526a\u679d\u6846\u67b6\uff0c\u4e13\u4e3a\u7f16\u7801\u4ee3\u7406\u8bbe\u8ba1\u3002</li>\n    <li>SWE-Pruner\u501f\u9274\u4eba\u7c7b\u7a0b\u5e8f\u5458\u5728\u5f00\u53d1\u548c\u8c03\u8bd5\u8fc7\u7a0b\u4e2d\u201c\u9009\u62e9\u6027\u6d4f\u89c8\u201d\u6e90\u4ee3\u7801\u7684\u65b9\u5f0f\uff0c\u8fdb\u884c\u4efb\u52a1\u611f\u77e5\u7684\u81ea\u9002\u5e94\u526a\u679d\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cSWE-Pruner\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\uff0c\u4efb\u52a1\u4ee4\u724c\u51cf\u5c1123-54%\uff0c\u5728\u5355\u56de\u5408\u4efb\u52a1\u4e2d\u538b\u7f29\u7387\u9ad8\u8fbe14.84\u500d\uff0c\u4e14\u6027\u80fd\u5f71\u54cd\u6700\u5c0f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LLM agents are good at software development but struggle with long interactions due to high costs and delays.</li>\n    <li>Current methods for reducing context size often lose important details and disrupt code structure.</li>\n    <li>The new approach, SWE-Pruner, is designed specifically for coding tasks and adapts to current goals.</li>\n    <li>SWE-Pruner uses a small neural model to choose important lines of code based on a defined goal.</li>\n    <li>Tests show SWE-Pruner can reduce token usage by 23-54% with little effect on performance.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:51:59.000Z", "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents", "summary": "LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As a result, they frequently disrupt syntactic and logical structure and fail to retain critical implementation details. In this paper, we propose SWE-Pruner, a self-adaptive context pruning framework tailored for coding agents. Drawing inspiration from how human programmers \"selectively skim\" source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. Given the current task, the agent formulates an explicit goal (e.g., \"focus on error handling\") as a hint to guide the pruning targets. A lightweight neural skimmer (0.6B parameters) is trained to dynamically select relevant lines from the surrounding context given the goal. Evaluations across four benchmarks and multiple models validate SWE-Pruner's effectiveness in various scenarios, achieving 23-54% token reduction on agent tasks like SWE-Bench Verified and up to 14.84x compression on single-turn tasks like LongCodeQA with minimal performance impact.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16746.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.14133", "authors": [{"_id": "69706e6ea8be625b19c2afac", "user": {"_id": "63d3b5f1640bb0f77173baea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674819020331-noauth.jpeg", "isPro": false, "fullname": "yubin", "user": "VLyb", "type": "user"}, "name": "Bin Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:19:14.460Z", "hidden": false}, {"_id": "69706e6ea8be625b19c2afad", "user": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "isPro": false, "fullname": "Shijie Lian", "user": "LiamLian0727", "type": "user"}, "name": "Shijie Lian", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:47:15.246Z", "hidden": false}, {"_id": "69706e6ea8be625b19c2afae", "name": "Xiaopeng Lin", "hidden": false}, {"_id": "69706e6ea8be625b19c2afaf", "name": "Yuliang Wei", "hidden": false}, {"_id": "69706e6ea8be625b19c2afb0", "name": "Zhaolong Shen", "hidden": false}, {"_id": "69706e6ea8be625b19c2afb1", "name": "Changti Wu", "hidden": false}, {"_id": "69706e6ea8be625b19c2afb2", "name": "Yuzhuo Miao", "hidden": false}, {"_id": "69706e6ea8be625b19c2afb3", "name": "Xinming Wang", "hidden": false}, {"_id": "69706e6ea8be625b19c2afb4", "name": "Bailing Wang", "hidden": false}, {"_id": "69706e6ea8be625b19c2afb5", "name": "Cong Huang", "hidden": false}, {"_id": "69706e6ea8be625b19c2afb6", "name": "Kai Chen", "hidden": false}], "publishedAt": "2026-01-20T16:30:07.000Z", "submittedOnDailyAt": "2026-01-26T00:14:31.380Z", "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers", "submittedOnDailyBy": {"_id": "63d3b5f1640bb0f77173baea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674819020331-noauth.jpeg", "isPro": false, "fullname": "yubin", "user": "VLyb", "type": "user"}, "summary": "Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to \"catastrophic forgetting\" of the model's open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen \"Left Brain\", which retains robust general visual reasoning, with a trainable \"Right Brain\", specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.", "upvotes": 52, "discussionId": "69706e6ea8be625b19c2afb7", "githubRepo": "https://github.com/ZGC-EmbodyAI/TwinBrainVLA", "githubRepoAddedBy": "user", "ai_summary": "TwinBrainVLA addresses the tension between semantic understanding and motor skills in robot control by coordinating a generalist vision-language model with a specialist model through an asymmetric mixture-of-transformers mechanism.", "ai_keywords": ["Vision-Language-Action models", "Vision-Language Models", "robotic control", "catastrophic forgetting", "frozen Left Brain", "trainable Right Brain", "Asymmetric Mixture-of-Transformers", "Flow-Matching Action Expert", "embodied perception", "proprioception"], "githubStars": 9, "organization": {"_id": "68896d3a716ee5bfb1428441", "name": "ZGCA", "fullname": "Zhongguancun Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"}, "summary_zh": "<ul>\n    <li>\u4f20\u7edf\u7684\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u6a21\u578b\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u5b58\u5728\u9ad8\u5c42\u8bed\u4e49\u7406\u89e3\u4e0e\u4f4e\u5c42\u611f\u77e5\u6280\u80fd\u4e4b\u95f4\u7684\u77db\u76fe\u3002</li>\n    <li>TwinBrainVLA\u662f\u4e00\u79cd\u65b0\u67b6\u6784\uff0c\u7ed3\u5408\u4e86\u901a\u7528\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u548c\u4e13\u95e8\u7684\u611f\u77e5\u6a21\u578b\u3002</li>\n    <li>\u8be5\u67b6\u6784\u4f7f\u7528\u4e00\u4e2a\u56fa\u5b9a\u7684\u201c\u5de6\u8111\u201d\u4fdd\u6301\u5f3a\u5927\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u548c\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u201c\u53f3\u8111\u201d\u4e13\u6ce8\u4e8e\u8eab\u4f53\u611f\u77e5\u3002</li>\n    <li>\u53f3\u8111\u53ef\u4ee5\u4ece\u5de6\u8111\u52a8\u6001\u67e5\u8be2\u8bed\u4e49\u77e5\u8bc6\uff0c\u4ee5\u751f\u6210\u7cbe\u786e\u7684\u63a7\u5236\u52a8\u4f5c\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cTwinBrainVLA\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Standard models for robotic control often struggle to balance general understanding with specific skills, leading to loss of important knowledge.</li>\n    <li>TwinBrainVLA is a new approach that uses two types of Vision-Language Models: a generalist (Left Brain) for broad understanding, and a specialist (Right Brain) for specific robotic tasks.</li>\n    <li>The Left Brain remains unchanged, while the Right Brain learns to connect sensory information with knowledge from the Left Brain.</li>\n    <li>This combination allows for better control in robots by using a unique method called Asymmetric Mixture-of-Transformers (AsyMoT).</li>\n    <li>Tests show TwinBrainVLA outperforms existing models in robotic manipulation while keeping strong visual understanding intact.</li>\n</ul>"}, "publishedAt": "2026-01-20T11:30:07.000Z", "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers", "summary": "Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to \"catastrophic forgetting\" of the model's open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen \"Left Brain\", which retains robust general visual reasoning, with a trainable \"Right Brain\", specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14133.png", "numComments": 1, "submittedBy": {"_id": "63d3b5f1640bb0f77173baea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674819020331-noauth.jpeg", "fullname": "yubin", "name": "VLyb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "68896d3a716ee5bfb1428441", "name": "ZGCA", "fullname": "Zhongguancun Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16973", "authors": [{"_id": "6976d4695d41524304c13523", "user": {"_id": "641a38fdfb5ffff5ac78ceb0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641a38fdfb5ffff5ac78ceb0/53nGstZ9Ya5WfeGdXAXCr.png", "isPro": true, "fullname": "Zirui Wang", "user": "zwcolin", "type": "user"}, "name": "Zirui Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:54.291Z", "hidden": false}, {"_id": "6976d4695d41524304c13524", "name": "Junyi Zhang", "hidden": false}, {"_id": "6976d4695d41524304c13525", "user": {"_id": "6629dac35e13d8145e3a605e", "avatarUrl": "/avatars/95938f20ab9e067838f37aca6ea235ae.svg", "isPro": false, "fullname": "Jiaxin Ge", "user": "JiaxinGe", "type": "user"}, "name": "Jiaxin Ge", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:31:33.595Z", "hidden": false}, {"_id": "6976d4695d41524304c13526", "user": {"_id": "63797c273f575acc2f6893c0", "avatarUrl": "/avatars/32d7a6a8881c8c4d80a097b732ed24b6.svg", "isPro": true, "fullname": "Long(Tony) Lian", "user": "longlian", "type": "user"}, "name": "Long Lian", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:52.034Z", "hidden": false}, {"_id": "6976d4695d41524304c13527", "name": "Letian Fu", "hidden": false}, {"_id": "6976d4695d41524304c13528", "name": "Lisa Dunlap", "hidden": false}, {"_id": "6976d4695d41524304c13529", "name": "Ken Goldberg", "hidden": false}, {"_id": "6976d4695d41524304c1352a", "name": "XuDong Wang", "hidden": false}, {"_id": "6976d4695d41524304c1352b", "name": "Ion Stoica", "hidden": false}, {"_id": "6976d4695d41524304c1352c", "name": "David M. Chan", "hidden": false}, {"_id": "6976d4695d41524304c1352d", "name": "Sewon Min", "hidden": false}, {"_id": "6976d4695d41524304c1352e", "name": "Joseph E. Gonzalez", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/641a38fdfb5ffff5ac78ceb0/sC7mJkpqiaPhEB6RZvHAY.mp4"], "publishedAt": "2026-01-23T18:43:34.000Z", "submittedOnDailyAt": "2026-01-26T00:14:43.910Z", "title": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents", "submittedOnDailyBy": {"_id": "641a38fdfb5ffff5ac78ceb0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641a38fdfb5ffff5ac78ceb0/53nGstZ9Ya5WfeGdXAXCr.png", "isPro": true, "fullname": "Zirui Wang", "user": "zwcolin", "type": "user"}, "summary": "Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.", "upvotes": 23, "discussionId": "6976d46a5d41524304c1352f", "projectPage": "https://visgym.github.io/", "githubRepo": "https://github.com/visgym/VIsGym", "githubRepoAddedBy": "user", "ai_summary": "Modern vision-language models exhibit significant challenges in multi-step visual interaction tasks, particularly in long-horizon perception-memory-action integration, with performance declining when handling unbounded historical contexts.", "ai_keywords": ["vision-language models", "multi-step visual interactions", "perception", "memory", "action", "symbolic puzzles", "real-image understanding", "navigation", "manipulation", "supervised fine-tuning", "goal observations", "textual feedback", "exploratory demonstrations", "partially observable environments", "unknown-dynamics settings"], "githubStars": 22, "organization": {"_id": "66c3b97cd7a9770138e4ce9a", "name": "UCB-Sky-Computing-Lab", "fullname": "Sky Computing Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66c3740cc55655c7155c47ab/0ZqaCA_kmq5k6ahdnsIJ1.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86VisGym\uff0c\u8fd9\u662f\u4e00\u4e2a\u753117\u4e2a\u73af\u5883\u7ec4\u6210\u7684\u8bc4\u4f30\u548c\u8bad\u7ec3\u73b0\u4ee3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u5de5\u5177\u3002</li>\n    <li>\u8fd9\u4e9b\u73af\u5883\u5305\u62ec\u7b26\u53f7\u8c1c\u9898\u3001\u771f\u5b9e\u56fe\u50cf\u7406\u89e3\u3001\u5bfc\u822a\u548c\u64cd\u4f5c\uff0c\u4e14\u5141\u8bb8\u7075\u6d3b\u8c03\u8282\u96be\u5ea6\u548c\u53cd\u9988\u3002</li>\n    <li>\u8bc4\u4f30\u663e\u793a\uff0c\u73b0\u6709\u6a21\u578b\u5728\u4ea4\u4e92\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u6210\u529f\u7387\u5f88\u4f4e\uff08\u7b80\u5355\u4efb\u52a1\u4e3a46.6%\uff0c\u56f0\u96be\u4efb\u52a1\u4e3a26.0%\uff09\u3002</li>\n    <li>\u6a21\u578b\u5728\u5904\u7406\u957f\u65f6\u95f4\u4e0a\u4e0b\u6587\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4f7f\u7528\u65e0\u9650\u5386\u53f2\u8bb0\u5f55\u65f6\u6548\u679c\u66f4\u5dee\u3002</li>\n    <li>\u901a\u8fc7\u660e\u786e\u7684\u76ee\u6807\u89c2\u5bdf\u3001\u6587\u672c\u53cd\u9988\u548c\u63a2\u7d22\u6027\u6f14\u793a\uff0c\u53ef\u4ee5\u63d0\u9ad8\u591a\u6b65\u9aa4\u89c6\u89c9\u51b3\u7b56\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>VisGym is a new tool with 17 environments to test and train Vision-Language Models (VLMs) in various tasks like puzzles and navigation.</li>\n    <li>It allows for different levels of difficulty and provides structured demonstrations to help improve model training.</li>\n    <li>Current VLMs have low success rates in interactive tasks, with only about 46.6% success in easier settings and 26.0% in harder ones.</li>\n    <li>Models struggle with long-term memory and perform worse when given too much history instead of a shorter context.</li>\n    <li>Adding clear goals and feedback can help improve the models' performance in complex visual decision-making tasks.</li>\n</ul>"}, "publishedAt": "2026-01-23T13:43:34.000Z", "title": "VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents", "summary": "Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/641a38fdfb5ffff5ac78ceb0/sC7mJkpqiaPhEB6RZvHAY.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16973.png", "numComments": 1, "submittedBy": {"_id": "641a38fdfb5ffff5ac78ceb0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641a38fdfb5ffff5ac78ceb0/53nGstZ9Ya5WfeGdXAXCr.png", "fullname": "Zirui Wang", "name": "zwcolin", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "66c3b97cd7a9770138e4ce9a", "name": "UCB-Sky-Computing-Lab", "fullname": "Sky Computing Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66c3740cc55655c7155c47ab/0ZqaCA_kmq5k6ahdnsIJ1.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16296", "authors": [{"_id": "6976d29a5d41524304c1350a", "name": "Dohun Lee", "hidden": false}, {"_id": "6976d29a5d41524304c1350b", "name": "Chun-Hao Paul Huang", "hidden": false}, {"_id": "6976d29a5d41524304c1350c", "name": "Xuelin Chen", "hidden": false}, {"_id": "6976d29a5d41524304c1350d", "name": "Jong Chul Ye", "hidden": false}, {"_id": "6976d29a5d41524304c1350e", "name": "Duygu Ceylan", "hidden": false}, {"_id": "6976d29a5d41524304c1350f", "name": "Hyeonho Jeong", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/jX8O1mKnYcn5zBMk0V0bu.mp4"], "publishedAt": "2026-01-22T19:59:17.000Z", "submittedOnDailyAt": "2026-01-26T00:06:22.236Z", "title": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent foundational video-to-video diffusion models have achieved impressive results in editing user provided videos by modifying appearance, motion, or camera movement. However, real-world video editing is often an iterative process, where users refine results across multiple rounds of interaction. In this multi-turn setting, current video editors struggle to maintain cross-consistency across sequential edits. In this work, we tackle, for the first time, the problem of cross-consistency in multi-turn video editing and introduce Memory-V2V, a simple, yet effective framework that augments existing video-to-video models with explicit memory. Given an external cache of previously edited videos, Memory-V2V employs accurate retrieval and dynamic tokenization strategies to condition the current editing step on prior results. To further mitigate redundancy and computational overhead, we propose a learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues, achieving an overall speedup of 30%. We validate Memory-V2V on challenging tasks including video novel view synthesis and text-conditioned long video editing. Extensive experiments show that Memory-V2V produces videos that are significantly more cross-consistent with minimal computational overhead, while maintaining or even improving task-specific performance over state-of-the-art baselines. Project page: https://dohunlee1.github.io/MemoryV2V", "upvotes": 14, "discussionId": "6976d29b5d41524304c13510", "projectPage": "https://dohunlee1.github.io/MemoryV2V", "ai_summary": "Memory-V2V enhances multi-turn video editing by maintaining cross-consistency through explicit memory mechanisms and efficient token compression in video-to-video diffusion models.", "ai_keywords": ["video-to-video diffusion models", "cross-consistency", "multi-turn video editing", "memory augmentation", "retrieval", "dynamic tokenization", "token compressor", "DiT backbone", "video novel view synthesis", "text-conditioned long video editing"], "organization": {"_id": "61e5d14f77496de0a6d95c6b", "name": "adobe", "fullname": "Adobe", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u89c6\u9891\u7f16\u8f91\u6a21\u578b\u5728\u4fee\u6539\u89c6\u9891\u7684\u5916\u89c2\u3001\u52a8\u4f5c\u548c\u955c\u5934\u8fd0\u52a8\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002</li>\n    <li>\u73b0\u5b9e\u4e2d\u7684\u89c6\u9891\u7f16\u8f91\u901a\u5e38\u662f\u4e00\u4e2a\u8fed\u4ee3\u7684\u8fc7\u7a0b\uff0c\u7528\u6237\u9700\u8981\u591a\u6b21\u4ea4\u4e92\u6765\u5b8c\u5584\u7ed3\u679c\u3002</li>\n    <li>\u76ee\u524d\u7684\u89c6\u9891\u7f16\u8f91\u5de5\u5177\u5728\u591a\u8f6e\u7f16\u8f91\u4e2d\u96be\u4ee5\u4fdd\u6301\u4e00\u81f4\u6027\u3002</li>\n    <li>\u672c\u7814\u7a76\u9996\u6b21\u63d0\u51fa\u4e86Memory-V2V\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u663e\u5f0f\u8bb0\u5fc6\u6765\u89e3\u51b3\u591a\u8f6e\u89c6\u9891\u7f16\u8f91\u4e2d\u7684\u4e00\u81f4\u6027\u95ee\u9898\u3002</li>\n    <li>Memory-V2V\u5728\u89c6\u9891\u7f16\u8f91\u4e2d\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u5e76\u5728\u4fdd\u6301\u6216\u63d0\u5347\u7279\u5b9a\u4efb\u52a1\u8868\u73b0\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New video editing models can change videos based on user input, but they struggle with consistency when users make multiple edits.</li>\n    <li>This study introduces Memory-V2V, a method that helps maintain consistency in video editing by using a memory system to recall previous edits.</li>\n    <li>Memory-V2V uses a cache of earlier edited videos and smart strategies to improve the current editing process.</li>\n    <li>The system includes a feature that reduces unnecessary data, speeding up the editing process by 30% while still keeping important visual details.</li>\n    <li>Tests show that Memory-V2V creates videos that are more consistent across edits and performs as well or better than existing methods.</li>\n</ul>"}, "publishedAt": "2026-01-22T14:59:17.000Z", "title": "Memory-V2V: Augmenting Video-to-Video Diffusion Models with Memory", "summary": "Recent foundational video-to-video diffusion models have achieved impressive results in editing user provided videos by modifying appearance, motion, or camera movement. However, real-world video editing is often an iterative process, where users refine results across multiple rounds of interaction. In this multi-turn setting, current video editors struggle to maintain cross-consistency across sequential edits. In this work, we tackle, for the first time, the problem of cross-consistency in multi-turn video editing and introduce Memory-V2V, a simple, yet effective framework that augments existing video-to-video models with explicit memory. Given an external cache of previously edited videos, Memory-V2V employs accurate retrieval and dynamic tokenization strategies to condition the current editing step on prior results. To further mitigate redundancy and computational overhead, we propose a learnable token compressor within the DiT backbone that compresses redundant conditioning tokens while preserving essential visual cues, achieving an overall speedup of 30%. We validate Memory-V2V on challenging tasks including video novel view synthesis and text-conditioned long video editing. Extensive experiments show that Memory-V2V produces videos that are significantly more cross-consistent with minimal computational overhead, while maintaining or even improving task-specific performance over state-of-the-art baselines. Project page: https://dohunlee1.github.io/MemoryV2V", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/jX8O1mKnYcn5zBMk0V0bu.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16296.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "61e5d14f77496de0a6d95c6b", "name": "adobe", "fullname": "Adobe", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.15808", "authors": [{"_id": "697606265d41524304c1346e", "name": "Yuxuan Wan", "hidden": false}, {"_id": "697606265d41524304c1346f", "user": {"_id": "641129818573c51c0458b793", "avatarUrl": "/avatars/d4bc67c160a07146cf41c614678aa36b.svg", "isPro": false, "fullname": "Tianqing Fang", "user": "tqfang229", "type": "user"}, "name": "Tianqing Fang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:31:43.374Z", "hidden": false}, {"_id": "697606265d41524304c13470", "name": "Zaitang Li", "hidden": false}, {"_id": "697606265d41524304c13471", "name": "Yintong Huo", "hidden": false}, {"_id": "697606265d41524304c13472", "name": "Wenxuan Wang", "hidden": false}, {"_id": "697606265d41524304c13473", "name": "Haitao Mi", "hidden": false}, {"_id": "697606265d41524304c13474", "name": "Dong Yu", "hidden": false}, {"_id": "697606265d41524304c13475", "name": "Michael R. Lyu", "hidden": false}], "publishedAt": "2026-01-22T09:47:31.000Z", "submittedOnDailyAt": "2026-01-26T01:28:53.387Z", "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification", "submittedOnDailyBy": {"_id": "641129818573c51c0458b793", "avatarUrl": "/avatars/d4bc67c160a07146cf41c614678aa36b.svg", "isPro": false, "fullname": "Tianqing Fang", "user": "tqfang229", "type": "user"}, "summary": "Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.", "upvotes": 14, "discussionId": "697606275d41524304c13476", "ai_summary": "A novel self-evolving framework for Deep Research Agents that enhances performance through iterative verification and rubric-based feedback during inference, achieving significant accuracy improvements without additional training.", "ai_keywords": ["Deep Research Agents", "policy capabilities", "post-training", "inference-time scaling", "verification", "rubrics", "DRA Failure Taxonomy", "DeepVerifier", "test-time scaling", "meta-evaluation", "supervised fine-tuning", "self-critique"], "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff08DRA\uff09\u7684\u65b0\u8fdb\u5c55\u6b63\u5728\u6539\u53d8\u81ea\u52a8\u77e5\u8bc6\u53d1\u73b0\u548c\u95ee\u9898\u89e3\u51b3\u65b9\u5f0f\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u6b65\u9a8c\u8bc1\u653f\u7b56\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u5e2e\u52a9\u4ee3\u7406\u81ea\u6211\u8fdb\u5316\u3002</li>\n    <li>\u57fa\u4e8e\u81ea\u52a8\u6784\u5efa\u7684DRA\u5931\u8d25\u5206\u7c7b\u6cd5\uff0c\u5236\u5b9a\u4e86\u4e94\u5927\u7c7b\u548c\u5341\u4e09\u5c0f\u7c7b\u7684\u8bc4\u4f30\u6807\u51c6\u3002</li>\n    <li>DeepVerifier\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bc4\u4f30\u6807\u51c6\u7684\u7ed3\u679c\u9a8c\u8bc1\u5668\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u7684\u4ee3\u7406\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc4\u5224\u8005\u3002</li>\n    <li>DeepVerifier\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u5e2e\u52a9\u5f00\u653e\u6a21\u578b\u63d0\u9ad8\u9a8c\u8bc1\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Deep Research Agents (DRAs) are improving how automated systems discover knowledge and solve problems.</li>\n    <li>Instead of just training DRAs, a new method focuses on helping them improve by checking their own outputs using specific guidelines.</li>\n    <li>The process involves a tool called DeepVerifier, which uses a failure classification system to enhance the agents' performance.</li>\n    <li>DeepVerifier provides helpful feedback that allows the agents to refine their answers without needing extra training, resulting in better accuracy.</li>\n    <li>An open-source dataset, DeepVerifier-4K, has been released to help others develop strong verification skills in DRAs.</li>\n</ul>"}, "publishedAt": "2026-01-22T04:47:31.000Z", "title": "Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification", "summary": "Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15808.png", "numComments": 1, "submittedBy": {"_id": "641129818573c51c0458b793", "avatarUrl": "/avatars/d4bc67c160a07146cf41c614678aa36b.svg", "fullname": "Tianqing Fang", "name": "tqfang229", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.14243", "authors": [{"_id": "6977053b5d41524304c13697", "name": "Haocheng Xi", "hidden": false}, {"_id": "6977053b5d41524304c13698", "name": "Charlie Ruan", "hidden": false}, {"_id": "6977053b5d41524304c13699", "name": "Peiyuan Liao", "hidden": false}, {"_id": "6977053b5d41524304c1369a", "name": "Yujun Lin", "hidden": false}, {"_id": "6977053b5d41524304c1369b", "name": "Han Cai", "hidden": false}, {"_id": "6977053b5d41524304c1369c", "name": "Yilong Zhao", "hidden": false}, {"_id": "6977053b5d41524304c1369d", "name": "Shuo Yang", "hidden": false}, {"_id": "6977053b5d41524304c1369e", "name": "Kurt Keutzer", "hidden": false}, {"_id": "6977053b5d41524304c1369f", "name": "Song Han", "hidden": false}, {"_id": "6977053b5d41524304c136a0", "name": "Ligeng Zhu", "hidden": false}], "publishedAt": "2026-01-20T18:54:31.000Z", "submittedOnDailyAt": "2026-01-26T03:40:05.139Z", "title": "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow", "submittedOnDailyBy": {"_id": "66ce751a8ec9fda2cf5a9e85", "avatarUrl": "/avatars/c17093ca81dad007b3e50bae503955a7.svg", "isPro": false, "fullname": "Haocheng Xi", "user": "xihc-ucb", "type": "user"}, "summary": "Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demonstrate that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. Our analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, we propose Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: our method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation.", "upvotes": 13, "discussionId": "6977053c5d41524304c136a1", "ai_summary": "Quantized reinforcement learning training using FP8 precision faces stability issues due to numerical mismatches between training and inference phases, but a unified FP8 framework achieves significant speedups with stable convergence.", "ai_keywords": ["reinforcement learning", "large language models", "quantized RL training", "FP8 precision", "BF16 precision", "rollout phase", "training instability", "accuracy collapse", "off-policy approach", "numerical mismatch", "Jet-RL", "unified FP8 precision flow", "inter-step calibration", "end-to-end speedup"], "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5bf9\u4e8e\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u8bad\u7ec3\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u8017\u8d39\u5927\u91cf\u8d44\u6e90\u3002</li>\n    <li>\u91cf\u5316\u7684RL\u8bad\u7ec3\uff0c\u7279\u522b\u662f\u4f7f\u7528FP8\u7cbe\u5ea6\uff0c\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e00\u74f6\u9888\u3002</li>\n    <li>\u5e38\u89c1\u7684\u65b9\u6cd5\u662f\u5728\u56de\u6eda\u9636\u6bb5\u4f7f\u7528FP8\u7cbe\u5ea6\uff0c\u800c\u8bad\u7ec3\u65f6\u4f7f\u7528BF16\u7cbe\u5ea6\uff0c\u4f46\u8fd9\u79cd\u7ec4\u5408\u5728\u957f\u65f6\u95f4\u56de\u6eda\u548c\u590d\u6742\u4efb\u52a1\u4e2d\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u51c6\u786e\u5ea6\u5d29\u6e83\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Jet-RL\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684FP8 RL\u8bad\u7ec3\u6846\u67b6\uff0c\u65e8\u5728\u51cf\u5c11\u8bad\u7ec3\u548c\u63a8\u7406\u4e4b\u95f4\u7684\u6570\u503c\u5dee\u5f02\uff0c\u63d0\u9ad8\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cJet-RL\u5728\u56de\u6eda\u548c\u8bad\u7ec3\u9636\u6bb5\u90fd\u80fd\u663e\u8457\u52a0\u5feb\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u7684\u6536\u655b\u6027\u548c\u8f83\u5c0f\u7684\u51c6\u786e\u5ea6\u4e0b\u964d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) is important for improving the reasoning abilities of large language models (LLMs).</li>\n    <li>Current RL training methods are slow and use a lot of resources, with rollout taking most of the time.</li>\n    <li>Using FP8 precision for rollout and BF16 for training leads to problems like instability and accuracy loss in long tasks.</li>\n    <li>The authors introduce Jet-RL, which uses FP8 for both training and rollout to reduce errors and improve efficiency.</li>\n    <li>Jet-RL shows significant speed improvements (up to 41%) while keeping performance stable and only slightly affecting accuracy.</li>\n</ul>"}, "publishedAt": "2026-01-20T13:54:31.000Z", "title": "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow", "summary": "Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demonstrate that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. Our analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, we propose Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: our method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14243.png", "numComments": 1, "submittedBy": {"_id": "66ce751a8ec9fda2cf5a9e85", "avatarUrl": "/avatars/c17093ca81dad007b3e50bae503955a7.svg", "fullname": "Haocheng Xi", "name": "xihc-ucb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.16515", "authors": [{"_id": "6976f03f5d41524304c13658", "user": {"_id": "6524d2dae5d47a2f52ce0b03", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/KjA0bpIAXQHqtW_WEcEb8.jpeg", "isPro": false, "fullname": "Tongcheng Fang", "user": "Stein-Fun", "type": "user"}, "name": "Tongcheng Fang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:39.635Z", "hidden": false}, {"_id": "6976f03f5d41524304c13659", "name": "Hanling Zhang", "hidden": false}, {"_id": "6976f03f5d41524304c1365a", "user": {"_id": "66f6442e04f2d5ae9789ab2b", "avatarUrl": "/avatars/72beedaeda45311725aabe80a8ec91a5.svg", "isPro": false, "fullname": "Ricky Xie", "user": "RickyHsieh", "type": "user"}, "name": "Ruiqi Xie", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:37.024Z", "hidden": false}, {"_id": "6976f03f5d41524304c1365b", "name": "Zhuo Han", "hidden": false}, {"_id": "6976f03f5d41524304c1365c", "name": "Xin Tao", "hidden": false}, {"_id": "6976f03f5d41524304c1365d", "name": "Tianchen Zhao", "hidden": false}, {"_id": "6976f03f5d41524304c1365e", "name": "Pengfei Wan", "hidden": false}, {"_id": "6976f03f5d41524304c1365f", "name": "Wenbo Ding", "hidden": false}, {"_id": "6976f03f5d41524304c13660", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6976f03f5d41524304c13661", "name": "Xuefei Ning", "hidden": false}, {"_id": "6976f03f5d41524304c13662", "name": "Yu Wang", "hidden": false}], "publishedAt": "2026-01-23T07:28:53.000Z", "submittedOnDailyAt": "2026-01-26T02:13:26.255Z", "title": "SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer", "submittedOnDailyBy": {"_id": "6524d2dae5d47a2f52ce0b03", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/KjA0bpIAXQHqtW_WEcEb8.jpeg", "isPro": false, "fullname": "Tongcheng Fang", "user": "Stein-Fun", "type": "user"}, "summary": "Diffusion Transformers have recently demonstrated remarkable performance in video generation. However, the long input sequences result in high computational latency due to the quadratic complexity of full attention. Various sparse attention mechanisms have been proposed. Training-free sparse attention is constrained by limited sparsity and thus offers modest acceleration, whereas training-based methods can reach much higher sparsity but demand substantial data and computation for training. In this work, we propose SALAD, introducing a lightweight linear attention branch in parallel with the sparse attention. By incorporating an input-dependent gating mechanism to finely balance the two branches, our method attains 90% sparsity and 1.72x inference speedup, while maintaining generation quality comparable to the full attention baseline. Moreover, our finetuning process is highly efficient, requiring only 2,000 video samples and 1,600 training steps with a batch size of 8.", "upvotes": 9, "discussionId": "6976f03f5d41524304c13663", "ai_summary": "Diffusion Transformers for video generation are enhanced with SALAD, a method that combines linear and sparse attention branches to achieve high sparsity and speedup while maintaining quality and requiring minimal training data.", "ai_keywords": ["diffusion transformers", "video generation", "sparse attention", "linear attention", "input-dependent gating mechanism", "inference speedup", "parameter-efficient fine-tuning"], "organization": {"_id": "64b74b5fb727f8771ab887f9", "name": "nics-efc", "fullname": "Tsinghua-NICS-EFC", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641031b1a78453b8d96b8420/vmgxct2WsyHcKT2x2NAYT.jpeg"}, "summary_zh": "<ul>\n    <li>Diffusion Transformers\u5728\u89c6\u9891\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u957f\u8f93\u5165\u5e8f\u5217\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u5ef6\u8fdf\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e0d\u540c\u7684\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8bad\u7ec3\u65e0\u5173\u7684\u7a00\u758f\u6ce8\u610f\u529b\u52a0\u901f\u6709\u9650\uff0c\u800c\u8bad\u7ec3\u57fa\u65b9\u6cd5\u80fd\u8fbe\u5230\u66f4\u9ad8\u7a00\u758f\u6027\uff0c\u4f46\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86SALAD\uff0c\u589e\u52a0\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u5206\u652f\uff0c\u4e0e\u7a00\u758f\u6ce8\u610f\u529b\u5e76\u884c\u5de5\u4f5c\u3002</li>\n    <li>\u901a\u8fc7\u8f93\u5165\u4f9d\u8d56\u7684\u95e8\u63a7\u673a\u5236\u5e73\u8861\u4e24\u4e2a\u5206\u652f\uff0c\u8fbe\u523090%\u7684\u7a00\u758f\u6027\u548c1.72\u500d\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002</li>\n    <li>\u6211\u4eec\u7684\u5fae\u8c03\u8fc7\u7a0b\u9ad8\u6548\uff0c\u4ec5\u97002000\u4e2a\u89c6\u9891\u6837\u672c\u548c1600\u4e2a\u8bad\u7ec3\u6b65\u9aa4\uff0c\u6279\u91cf\u5927\u5c0f\u4e3a8\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion Transformers are effective for generating videos but can be slow due to complex calculations with long input sequences.</li>\n    <li>Some methods to speed up calculations use sparse attention, but they either have limited effectiveness or require a lot of training data and time.</li>\n    <li>The proposed method, SALAD, combines a simple linear attention branch with sparse attention to improve performance.</li>\n    <li>SALAD achieves 90% sparsity and speeds up the process by 1.72 times without losing quality compared to traditional methods.</li>\n    <li>It also has an efficient training process, needing only 2,000 video samples and 1,600 steps to train.</li>\n</ul>"}, "publishedAt": "2026-01-23T02:28:53.000Z", "title": "SALAD: Achieve High-Sparsity Attention via Efficient Linear Attention Tuning for Video Diffusion Transformer", "summary": "Diffusion Transformers have recently demonstrated remarkable performance in video generation. However, the long input sequences result in high computational latency due to the quadratic complexity of full attention. Various sparse attention mechanisms have been proposed. Training-free sparse attention is constrained by limited sparsity and thus offers modest acceleration, whereas training-based methods can reach much higher sparsity but demand substantial data and computation for training. In this work, we propose SALAD, introducing a lightweight linear attention branch in parallel with the sparse attention. By incorporating an input-dependent gating mechanism to finely balance the two branches, our method attains 90% sparsity and 1.72x inference speedup, while maintaining generation quality comparable to the full attention baseline. Moreover, our finetuning process is highly efficient, requiring only 2,000 video samples and 1,600 training steps with a batch size of 8.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16515.png", "numComments": 1, "submittedBy": {"_id": "6524d2dae5d47a2f52ce0b03", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/KjA0bpIAXQHqtW_WEcEb8.jpeg", "fullname": "Tongcheng Fang", "name": "Stein-Fun", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "64b74b5fb727f8771ab887f9", "name": "nics-efc", "fullname": "Tsinghua-NICS-EFC", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641031b1a78453b8d96b8420/vmgxct2WsyHcKT2x2NAYT.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16276", "authors": [{"_id": "697760b94c988cc0118d1d7c", "name": "Victor Conchello Vendrell", "hidden": false}, {"_id": "697760b94c988cc0118d1d7d", "name": "Max Ruiz Luyten", "hidden": false}, {"_id": "697760b94c988cc0118d1d7e", "name": "Mihaela van der Schaar", "hidden": false}], "publishedAt": "2026-01-22T19:18:39.000Z", "submittedOnDailyAt": "2026-01-26T10:11:06.205Z", "title": "GameTalk: Training LLMs for Strategic Conversation", "submittedOnDailyBy": {"_id": "64b666ceb59ced6b452ffb09", "avatarUrl": "/avatars/3114cc44273944a6923c33f94aa93ea7.svg", "isPro": false, "fullname": "Max Ruiz Luyten", "user": "maxruizluyten", "type": "user"}, "summary": "Strategic decision-making in multi-agent settings is a key challenge for large language models (LLMs), particularly when coordination and negotiation must unfold over extended conversations. While recent work has explored the use of LLMs in isolated decision tasks, little attention has been given to optimizing long-term objectives through dialogue. We introduce GameTalk, a framework for training LLMs to make strategic decisions via multi-turn interactions. Unlike prior work that focuses on single-turn objectives or static action prediction, we train LLMs to optimize a global objective across full conversations. We achieve this by adapting fine-tuning methods like GRPO, DPO, and STaR to incorporate reward signals that depend on the entire interaction. We evaluate this approach on a suite of increasingly complex games, designed to stress different aspects of reasoning, coordination, and opponent modeling. Our results show that GameTalk significantly outperforms untrained models, especially under reward shaping, with DPO consistently yielding the strongest gains. These findings position conversational fine-tuning as a promising path for LLMs to reason, negotiate, and act in interactive environments.", "upvotes": 8, "discussionId": "697760ba4c988cc0118d1d7f", "ai_summary": "GameTalk framework trains large language models to make strategic decisions through multi-turn dialogue by optimizing global objectives using reward signals across full conversations, outperforming untrained models in complex game scenarios.", "ai_keywords": ["multi-agent settings", "large language models", "strategic decision-making", "multi-turn interactions", "fine-tuning methods", "GRPO", "DPO", "STaR", "reward shaping", "conversational fine-tuning"], "organization": {"_id": "65f9e02087d1c912d985eebf", "name": "CambUni", "fullname": "University of Cambridge", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/gdCou-0KoYsLPYqPrz6xn.png"}, "summary_zh": "<ul>\n    <li>\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\uff0c\u6218\u7565\u51b3\u7b56\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u957f\u65f6\u95f4\u5bf9\u8bdd\u4e2d\u8fdb\u884c\u534f\u8c03\u548c\u8c08\u5224\u65f6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86GameTalk\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u591a\u8f6e\u4e92\u52a8\u8bad\u7ec3LLMs\u8fdb\u884c\u6218\u7565\u51b3\u7b56\u3002</li>\n    <li>\u4e0e\u4ee5\u5f80\u53ea\u5173\u6ce8\u5355\u8f6e\u76ee\u6807\u7684\u7814\u7a76\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u65e8\u5728\u4f18\u5316\u6574\u4e2a\u5bf9\u8bdd\u8fc7\u7a0b\u4e2d\u7684\u5168\u5c40\u76ee\u6807\u3002</li>\n    <li>\u6211\u4eec\u901a\u8fc7\u8c03\u6574\u7ec6\u8c03\u65b9\u6cd5\u6765\u6574\u5408\u4f9d\u8d56\u4e8e\u6574\u4e2a\u4e92\u52a8\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u5e76\u5728\u591a\u79cd\u590d\u6742\u6e38\u620f\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002</li>\n    <li>\u7ed3\u679c\u663e\u793a\uff0cGameTalk\u5728\u5956\u52b1\u5851\u9020\u4e0b\u663e\u8457\u4f18\u4e8e\u672a\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u7279\u522b\u662fDPO\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u8bc1\u660e\u4e86\u5bf9\u8bdd\u7ec6\u8c03\u7684\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>GameTalk is a new framework to help large language models (LLMs) make better strategic decisions during long conversations.</li>\n    <li>It focuses on achieving long-term goals instead of just single actions in a conversation.</li>\n    <li>GameTalk uses methods like GRPO, DPO, and STaR to improve how LLMs learn from entire dialogues.</li>\n    <li>The framework was tested on various complex games that challenge reasoning and coordination skills.</li>\n    <li>Results show that GameTalk significantly improves LLM performance, especially with reward shaping, with DPO being the most effective method.</li>\n</ul>"}, "publishedAt": "2026-01-22T14:18:39.000Z", "title": "GameTalk: Training LLMs for Strategic Conversation", "summary": "Strategic decision-making in multi-agent settings is a key challenge for large language models (LLMs), particularly when coordination and negotiation must unfold over extended conversations. While recent work has explored the use of LLMs in isolated decision tasks, little attention has been given to optimizing long-term objectives through dialogue. We introduce GameTalk, a framework for training LLMs to make strategic decisions via multi-turn interactions. Unlike prior work that focuses on single-turn objectives or static action prediction, we train LLMs to optimize a global objective across full conversations. We achieve this by adapting fine-tuning methods like GRPO, DPO, and STaR to incorporate reward signals that depend on the entire interaction. We evaluate this approach on a suite of increasingly complex games, designed to stress different aspects of reasoning, coordination, and opponent modeling. Our results show that GameTalk significantly outperforms untrained models, especially under reward shaping, with DPO consistently yielding the strongest gains. These findings position conversational fine-tuning as a promising path for LLMs to reason, negotiate, and act in interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16276.png", "numComments": 2, "submittedBy": {"_id": "64b666ceb59ced6b452ffb09", "avatarUrl": "/avatars/3114cc44273944a6923c33f94aa93ea7.svg", "fullname": "Max Ruiz Luyten", "name": "maxruizluyten", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "65f9e02087d1c912d985eebf", "name": "CambUni", "fullname": "University of Cambridge", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/gdCou-0KoYsLPYqPrz6xn.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.07251", "authors": [{"_id": "6976e5de5d41524304c1361c", "name": "Zizhen Li", "hidden": false}, {"_id": "6976e5de5d41524304c1361d", "name": "Chuanhao Li", "hidden": false}, {"_id": "6976e5de5d41524304c1361e", "name": "Yibin Wang", "hidden": false}, {"_id": "6976e5de5d41524304c1361f", "name": "Yukang Feng", "hidden": false}, {"_id": "6976e5de5d41524304c13620", "name": "Jianwen Sun", "hidden": false}, {"_id": "6976e5de5d41524304c13621", "name": "Jiaxin Ai", "hidden": false}, {"_id": "6976e5de5d41524304c13622", "name": "Fanrui Zhang", "hidden": false}, {"_id": "6976e5de5d41524304c13623", "name": "Mingzhu Sun", "hidden": false}, {"_id": "6976e5de5d41524304c13624", "name": "Yifei Huang", "hidden": false}, {"_id": "6976e5de5d41524304c13625", "name": "Kaipeng Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/y983U2z9d88Pk0LnEm_F2.png"], "publishedAt": "2026-01-12T06:37:12.000Z", "submittedOnDailyAt": "2026-01-26T01:33:45.813Z", "title": "MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences", "submittedOnDailyBy": {"_id": "65f1713552c38a91e0a445e8", "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg", "isPro": false, "fullname": "kaipeng", "user": "kpzhang996", "type": "user"}, "summary": "Recent advancements have expanded the role of Large Language Models in board games from playing agents to creative co-designers. However, a critical gap remains: current systems lack the capacity to offer constructive critique grounded in the emergent user experience. Bridging this gap is fundamental for harmonizing Human-AI collaboration, as it empowers designers to refine their creations via external perspectives while steering models away from biased or unpredictable outcomes. Automating critique for board games presents two challenges: inferring the latent dynamics connecting rules to gameplay without an explicit engine, and modeling the subjective heterogeneity of diverse player groups. To address these, we curate a dataset of 1,727 structurally corrected rulebooks and 150K reviews selected via quality scoring and facet-aware sampling. We augment this data with Mechanics-Dynamics-Aesthetics (MDA) reasoning to explicitly bridge the causal gap between written rules and player experience. We further distill player personas and introduce MeepleLM, a specialized model that internalizes persona-specific reasoning patterns to accurately simulate the subjective feedback of diverse player archetypes. Experiments demonstrate that MeepleLM significantly outperforms latest commercial models (e.g., GPT-5.1, Gemini3-Pro) in community alignment and critique quality, achieving a 70% preference rate in user studies assessing utility. MeepleLM serves as a reliable virtual playtester for general interactive systems, marking a pivotal step towards audience-aligned, experience-aware Human-AI collaboration.", "upvotes": 8, "discussionId": "6976e5df5d41524304c13626", "githubRepo": "https://github.com/leroy9472/MeepleLM", "githubRepoAddedBy": "user", "ai_summary": "MeepleLM enables human-AI collaboration in board game design by providing constructive critique through persona-specific reasoning patterns that align with player experiences.", "ai_keywords": ["Large Language Models", "board games", "Human-AI collaboration", "creative co-designers", "constructive critique", "player personas", "MDA reasoning", "latent dynamics", "player experience", "virtual playtester", "audience-aligned collaboration"], "githubStars": 18, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u684c\u6e38\u4e2d\u4e0d\u4ec5\u53ef\u4ee5\u4f5c\u4e3a\u73a9\u5bb6\uff0c\u8fd8\u80fd\u4f5c\u4e3a\u521b\u610f\u8bbe\u8ba1\u52a9\u624b\u3002</li>\n    <li>\u76ee\u524d\u7684\u7cfb\u7edf\u7f3a\u4e4f\u63d0\u4f9b\u57fa\u4e8e\u7528\u6237\u4f53\u9a8c\u7684\u5efa\u8bbe\u6027\u53cd\u9988\u80fd\u529b\u3002</li>\n    <li>\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u4eba\u5458\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1,727\u672c\u89c4\u5219\u4e66\u548c150,000\u6761\u8bc4\u8bba\u7684\u6570\u636e\u96c6\u3002</li>\n    <li>\u5f15\u5165\u4e86MeepleLM\u6a21\u578b\uff0c\u80fd\u591f\u6a21\u62df\u4e0d\u540c\u73a9\u5bb6\u7c7b\u578b\u7684\u4e3b\u89c2\u53cd\u9988\u3002</li>\n    <li>MeepleLM\u5728\u793e\u533a\u5bf9\u9f50\u548c\u53cd\u9988\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u65b0\u5546\u4e1a\u6a21\u578b\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u5176\u670970%\u7684\u504f\u597d\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models are evolving from just playing board games to helping design them.</li>\n    <li>Current models struggle to give useful feedback based on how players experience the game.</li>\n    <li>To improve this, a new dataset of rulebooks and player reviews was created to understand gameplay better.</li>\n    <li>MeepleLM is a new model that understands different player types and gives tailored feedback.</li>\n    <li>Tests show MeepleLM is better than other models at providing helpful critiques, making it a valuable tool for game designers.</li>\n</ul>"}, "publishedAt": "2026-01-12T01:37:12.000Z", "title": "MeepleLM: A Virtual Playtester Simulating Diverse Subjective Experiences", "summary": "Recent advancements have expanded the role of Large Language Models in board games from playing agents to creative co-designers. However, a critical gap remains: current systems lack the capacity to offer constructive critique grounded in the emergent user experience. Bridging this gap is fundamental for harmonizing Human-AI collaboration, as it empowers designers to refine their creations via external perspectives while steering models away from biased or unpredictable outcomes. Automating critique for board games presents two challenges: inferring the latent dynamics connecting rules to gameplay without an explicit engine, and modeling the subjective heterogeneity of diverse player groups. To address these, we curate a dataset of 1,727 structurally corrected rulebooks and 150K reviews selected via quality scoring and facet-aware sampling. We augment this data with Mechanics-Dynamics-Aesthetics (MDA) reasoning to explicitly bridge the causal gap between written rules and player experience. We further distill player personas and introduce MeepleLM, a specialized model that internalizes persona-specific reasoning patterns to accurately simulate the subjective feedback of diverse player archetypes. Experiments demonstrate that MeepleLM significantly outperforms latest commercial models (e.g., GPT-5.1, Gemini3-Pro) in community alignment and critique quality, achieving a 70% preference rate in user studies assessing utility. MeepleLM serves as a reliable virtual playtester for general interactive systems, marking a pivotal step towards audience-aligned, experience-aware Human-AI collaboration.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/y983U2z9d88Pk0LnEm_F2.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07251.png", "numComments": 1, "submittedBy": {"_id": "65f1713552c38a91e0a445e8", "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg", "fullname": "kaipeng", "name": "kpzhang996", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2601.16725", "authors": [{"_id": "6976d5405d41524304c13537", "name": "Meituan LongCat Team", "hidden": false}, {"_id": "6976d5405d41524304c13538", "name": "Anchun Gui", "hidden": false}, {"_id": "6976d5405d41524304c13539", "name": "Bei Li", "hidden": false}, {"_id": "6976d5405d41524304c1353a", "name": "Bingyang Tao", "hidden": false}, {"_id": "6976d5405d41524304c1353b", "name": "Bole Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1353c", "name": "Borun Chen", "hidden": false}, {"_id": "6976d5405d41524304c1353e", "name": "Chao Zhang", "hidden": false}, {"_id": "69772bc15d41524304c13739", "name": "Chao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1353f", "name": "Chen Gao", "hidden": false}, {"_id": "6976d5405d41524304c13540", "name": "Chen Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13541", "name": "Chengcheng Han", "hidden": false}, {"_id": "6976d5405d41524304c13542", "name": "Chenhui Yang", "hidden": false}, {"_id": "6976d5405d41524304c13543", "name": "Chuyu Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13544", "name": "Cong Chen", "hidden": false}, {"_id": "6976d5405d41524304c13545", "name": "Cunguang Wang", "hidden": false}, {"_id": "6976d5405d41524304c13546", "name": "Daoru Pan", "hidden": false}, {"_id": "6976d5405d41524304c13547", "name": "Defei Bu", "hidden": false}, {"_id": "6976d5405d41524304c13548", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13549", "name": "Di Xiu", "hidden": false}, {"_id": "6976d5405d41524304c1354a", "name": "Dishan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1354b", "name": "Dongyu Ru", "hidden": false}, {"_id": "6976d5405d41524304c1354c", "name": "Dunwei Tu", "hidden": false}, {"_id": "6976d5405d41524304c1354d", "name": "Fan Wu", "hidden": false}, {"_id": "6976d5405d41524304c1354e", "name": "Fengcheng Yuan", "hidden": false}, {"_id": "6976d5405d41524304c1354f", "name": "Fengcun Li", "hidden": false}, {"_id": "6976d5405d41524304c13550", "name": "Gang Xu", "hidden": false}, {"_id": "6976d5405d41524304c13551", "name": "Guanyu Wu", "hidden": false}, {"_id": "6976d5405d41524304c13552", "name": "Guoyuan Lin", "hidden": false}, {"_id": "6976d5405d41524304c13553", "name": "Haibin Wang", "hidden": false}, {"_id": "6976d5405d41524304c13554", "name": "Hansi Yang", "hidden": false}, {"_id": "6976d5405d41524304c13555", "name": "Hao Yang", "hidden": false}, {"_id": "6976d5405d41524304c13556", "name": "Haonan Yan", "hidden": false}, {"_id": "6976d5405d41524304c13557", "name": "Haoxiang Ma", "hidden": false}, {"_id": "6976d5405d41524304c13558", "name": "Haoxing Wen", "hidden": false}, {"_id": "6976d5405d41524304c13559", "name": "Hongyan Hao", "hidden": false}, {"_id": "6976d5405d41524304c1355a", "name": "Hongyin Tang", "hidden": false}, {"_id": "6976d5405d41524304c1355b", "name": "Hongyu Zang", "hidden": false}, {"_id": "6976d5405d41524304c1355c", "name": "Hongzhi Ni", "hidden": false}, {"_id": "6976d5405d41524304c1355d", "name": "Hui Su", "hidden": false}, {"_id": "6976d5405d41524304c1355e", "name": "Jiacheng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1355f", "name": "Jiahong Zhou", "hidden": false}, {"_id": "6976d5405d41524304c13560", "name": "Jiahuan Li", "hidden": false}, {"_id": "6976d5405d41524304c13561", "name": "Jiaming Wang", "hidden": false}, {"_id": "6976d5405d41524304c13562", "name": "Jian Yang", "hidden": false}, {"_id": "6976d5405d41524304c13563", "user": {"_id": "64008a0af4ff62c2616d8858", "avatarUrl": "/avatars/b52c98857916fba5377ace8089d658b2.svg", "isPro": false, "fullname": "zhangjf", "user": "zhangjf", "type": "user"}, "name": "Jianfei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:09.272Z", "hidden": false}, {"_id": "6976d5405d41524304c13564", "name": "Jianhao Xu", "hidden": false}, {"_id": "6976d5405d41524304c13565", "name": "Jianing Wang", "hidden": false}, {"_id": "6976d5405d41524304c13566", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13567", "name": "Jiaqi Sun", "hidden": false}, {"_id": "6976d5405d41524304c13568", "name": "Jiarong Shi", "hidden": false}, {"_id": "6976d5405d41524304c13569", "name": "Jiarui Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1356a", "name": "Jingang Wang", "hidden": false}, {"_id": "6976d5405d41524304c1356b", "user": {"_id": "6592472fccbc1e2cc7250903", "avatarUrl": "/avatars/6f04ae66944eb2ce65c5aca7927bab10.svg", "isPro": false, "fullname": "Jinluan Yang", "user": "Jinluan", "type": "user"}, "name": "Jinluan Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:47.175Z", "hidden": false}, {"_id": "6976d5405d41524304c1356c", "name": "Jinrui Ding", "hidden": false}, {"_id": "6976d5405d41524304c1356d", "name": "Jinwei Xiao", "hidden": false}, {"_id": "6976d5405d41524304c1356e", "name": "Jiyuan He", "hidden": false}, {"_id": "6976d5405d41524304c1356f", "name": "Juncan Xu", "hidden": false}, {"_id": "6976d5405d41524304c13570", "name": "Kefeng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13571", "name": "Keheng Wang", "hidden": false}, {"_id": "6976d5405d41524304c13572", "name": "Li Wei", "hidden": false}, {"_id": "6976d5405d41524304c13573", "name": "Lianhui Ma", "hidden": false}, {"_id": "6976d5405d41524304c13574", "name": "Lin Qiu", "hidden": false}, {"_id": "6976d5405d41524304c13575", "name": "Lingbing Kong", "hidden": false}, {"_id": "6976d5405d41524304c13576", "name": "Lingchuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c13577", "name": "Linsen Guo", "hidden": false}, {"_id": "6976d5405d41524304c13578", "name": "Mengshen Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13579", "name": "Mengxia Shen", "hidden": false}, {"_id": "6976d5405d41524304c1357a", "name": "Mingyang Zhu", "hidden": false}, {"_id": "6976d5405d41524304c1357b", "name": "Peiguang Li", "hidden": false}, {"_id": "6976d5405d41524304c1357c", "name": "Peng Pei", "hidden": false}, {"_id": "6976d5405d41524304c1357d", "name": "Pengcheng Jia", "hidden": false}, {"_id": "6976d5405d41524304c1357e", "name": "Pengtao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1357f", "name": "Peng Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13580", "name": "Qi Gu", "hidden": false}, {"_id": "6976d5405d41524304c13581", "name": "Qiong Huang", "hidden": false}, {"_id": "6976d5405d41524304c13582", "name": "Qiyuan Duan", "hidden": false}, {"_id": "6976d5405d41524304c13583", "name": "Quanchi Weng", "hidden": false}, {"_id": "6976d5405d41524304c13584", "name": "Rongxiang Weng", "hidden": false}, {"_id": "6976d5405d41524304c13585", "name": "Rongzhi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13586", "name": "Rumei Li", "hidden": false}, {"_id": "6976d5405d41524304c13587", "name": "Shanglin Lei", "hidden": false}, {"_id": "6976d5405d41524304c13588", "user": {"_id": "64db5f5dd68a6ddcc7bd89e9", "avatarUrl": "/avatars/69375ec915927b855813df8a6d486837.svg", "isPro": false, "fullname": "Shengnan An", "user": "ShengnanAn", "type": "user"}, "name": "Shengnan An", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:11.410Z", "hidden": false}, {"_id": "6976d5405d41524304c13589", "name": "Shijun Dai", "hidden": false}, {"_id": "6976d5405d41524304c1358a", "name": "Shuaikang Liu", "hidden": false}, {"_id": "6976d5405d41524304c1358b", "name": "Shuang Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1358c", "name": "Shuo Wang", "hidden": false}, {"_id": "6976d5405d41524304c1358d", "name": "Songyuan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1358e", "name": "Tao Liang", "hidden": false}, {"_id": "6976d5405d41524304c1358f", "name": "Tianhao Hu", "hidden": false}, {"_id": "6976d5405d41524304c13590", "name": "Tianze Chen", "hidden": false}, {"_id": "6976d5405d41524304c13591", "name": "Wei Liu", "hidden": false}, {"_id": "6976d5405d41524304c13592", "name": "Wei Shi", "hidden": false}, {"_id": "6976d5405d41524304c13593", "name": "Wei Wang", "hidden": false}, {"_id": "6976d5405d41524304c13594", "name": "Weifeng Tang", "hidden": false}, {"_id": "6976d5405d41524304c13595", "name": "Wenjie Shi", "hidden": false}, {"_id": "6976d5405d41524304c13596", "name": "Wenlong Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13597", "name": "Wentao Chen", "hidden": false}, {"_id": "6976d5405d41524304c13598", "name": "Wentao Shi", "hidden": false}, {"_id": "6976d5405d41524304c13599", "name": "Xi Su", "hidden": false}, {"_id": "6976d5405d41524304c1359a", "name": "Xiangcheng Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359b", "name": "Xiandi Ma", "hidden": false}, {"_id": "6976d5405d41524304c1359c", "user": {"_id": "63edb098679c2cc40abc6c2e", "avatarUrl": "/avatars/288c7229937c2c3f29fda6d17c7df2eb.svg", "isPro": false, "fullname": "Xiangyu", "user": "xixy", "type": "user"}, "name": "Xiangyu Xi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:13.312Z", "hidden": false}, {"_id": "6976d5405d41524304c1359d", "name": "Xiangyuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359e", "name": "Xiangzhou Huang", "hidden": false}, {"_id": "6976d5405d41524304c1359f", "name": "Xiao Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a0", "name": "Xiaodong Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a1", "name": "Xiaolong Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a2", "name": "Xiaowei Shi", "hidden": false}, {"_id": "6976d5405d41524304c135a3", "name": "Xiaoyu Li", "hidden": false}, {"_id": "6976d5405d41524304c135a4", "name": "Xin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a5", "name": "Xingchen Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a6", "name": "Xuan Huang", "hidden": false}, {"_id": "6976d5405d41524304c135a7", "name": "Xuezhi Cao", "hidden": false}, {"_id": "6976d5405d41524304c135a8", "name": "Xunliang Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a9", "name": "Yan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135aa", "user": {"_id": "63fc1c420aab06079200c15c", "avatarUrl": "/avatars/8e8e82a9a6552848581ca9f65011263c.svg", "isPro": false, "fullname": "yang bai", "user": "byang", "type": "user"}, "name": "Yang Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:07.036Z", "hidden": false}, {"_id": "6976d5405d41524304c135ab", "name": "Yang Liu", "hidden": false}, {"_id": "6976d5405d41524304c135ac", "name": "Yang Yang", "hidden": false}, {"_id": "6976d5405d41524304c135ad", "name": "Yang Zheng", "hidden": false}, {"_id": "6976d5405d41524304c135ae", "name": "Yaoming Wang", "hidden": false}, {"_id": "6976d5405d41524304c135af", "name": "Yaoming Zhu", "hidden": false}, {"_id": "6976d5405d41524304c135b0", "name": "Yaqi Huo", "hidden": false}, {"_id": "6976d5405d41524304c135b1", "name": "Yanyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b2", "name": "Yaorui Shi", "hidden": false}, {"_id": "6976d5405d41524304c135b3", "name": "Yerui Sun", "hidden": false}, {"_id": "6976d5405d41524304c135b4", "name": "Yi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b5", "name": "Yihao Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b6", "name": "Yi-Kai Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b7", "name": "Yifan Lu", "hidden": false}, {"_id": "6976d5405d41524304c135b8", "name": "Yifan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135b9", "name": "Yitao Zhai", "hidden": false}, {"_id": "6976d5405d41524304c135ba", "name": "Yongjing Yin", "hidden": false}, {"_id": "6976d5405d41524304c135bb", "name": "Yongwei Zhou", "hidden": false}, {"_id": "6976d5405d41524304c135bc", "name": "Youshao Xiao", "hidden": false}, {"_id": "6976d5405d41524304c135bd", "name": "Yuchuan Dai", "hidden": false}, {"_id": "6976d5405d41524304c135be", "name": "Yuchen Xie", "hidden": false}, {"_id": "6976d5405d41524304c135bf", "name": "Yuchen Yu", "hidden": false}, {"_id": "6976d5405d41524304c135c0", "name": "Yufei Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135c1", "name": "Yuhuai Wei", "hidden": false}, {"_id": "6976d5405d41524304c135c2", "name": "Yulei Qian", "hidden": false}, {"_id": "6976d5405d41524304c135c3", "name": "Yunfan Liang", "hidden": false}, {"_id": "6976d5405d41524304c135c4", "name": "Yunke Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135c5", "name": "Yuwei Jiang", "hidden": false}, {"_id": "6976d5405d41524304c135c6", "name": "Yuxin Bian", "hidden": false}, {"_id": "6976d5405d41524304c135c7", "name": "Yuxin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135c8", "name": "Yuxin Liu", "hidden": false}, {"_id": "6976d5405d41524304c135c9", "name": "Yue Xu", "hidden": false}, {"_id": "6976d5405d41524304c135ca", "name": "Yueqing Sun", "hidden": false}, {"_id": "6976d5405d41524304c135cb", "name": "Zeyang Yu", "hidden": false}, {"_id": "6976d5405d41524304c135cc", "name": "Zhao Yang", "hidden": false}, {"_id": "6976d5405d41524304c135cd", "name": "Zhengsheng Huang", "hidden": false}, {"_id": "6976d5405d41524304c135ce", "name": "Zhengyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135cf", "name": "Zhijian Liu", "hidden": false}, {"_id": "6976d5405d41524304c135d0", "name": "Zhikang Xia", "hidden": false}, {"_id": "6976d5405d41524304c135d1", "name": "Zhimin Lin", "hidden": false}, {"_id": "6976d5405d41524304c135d2", "name": "Zhiyuan Yao", "hidden": false}, {"_id": "6976d5405d41524304c135d3", "name": "Zhuofan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135d4", "name": "Zhuowen Han", "hidden": false}, {"_id": "6976d5405d41524304c135d5", "name": "Zijian Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135d6", "name": "Ziran Li", "hidden": false}, {"_id": "6976d5405d41524304c135d7", "name": "Ziwen Wang", "hidden": false}, {"_id": "6976d5405d41524304c135d8", "name": "Ziyuan Zhuang", "hidden": false}], "publishedAt": "2026-01-23T13:20:09.000Z", "submittedOnDailyAt": "2026-01-26T00:15:28.340Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "upvotes": 136, "discussionId": "6976d5405d41524304c135d9", "ai_summary": "A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for real-world robustness and complex reasoning.", "ai_keywords": ["Mixture-of-Experts", "agentic reasoning", "domain-parallel expert training", "fusion", "asynchronous reinforcement learning", "DORA", "long-tailed generation", "multi-turn interactions", "real-world noise patterns", "test-time scaling", "reasoning depth", "reasoning width", "parallel thinking"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86LongCat-Flash-Thinking-2601\uff0c\u8fd9\u662f\u4e00\u4e2a\u62e5\u67095600\u4ebf\u53c2\u6570\u7684\u5f00\u6e90\u6df7\u5408\u4e13\u5bb6\u63a8\u7406\u6a21\u578b\uff0c\u5177\u6709\u51fa\u8272\u7684\u4ee3\u7406\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5728\u591a\u4e2a\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5305\u62ec\u4ee3\u7406\u641c\u7d22\u548c\u5de5\u5177\u4f7f\u7528\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u901a\u7528\u5316\u80fd\u529b\u3002</li>\n    <li>\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u9886\u57df\u5e76\u884c\u4e13\u5bb6\u8bad\u7ec3\u548c\u540e\u7eed\u878d\u5408\u7684\u7edf\u4e00\u8bad\u7ec3\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u590d\u6742\u5de5\u5177\u4ea4\u4e92\u7684\u8868\u73b0\u3002</li>\n    <li>\u4e3a\u4f18\u5316\u591a\u73af\u5883\u8bad\u7ec3\uff0c\u6269\u5c55\u4e86DORA\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u3002</li>\n    <li>\u5f15\u5165\u91cd\u601d\u8003\u6a21\u5f0f\uff0c\u901a\u8fc7\u589e\u52a0\u63a8\u7406\u7684\u6df1\u5ea6\u548c\u5e7f\u5ea6\uff0c\u589e\u5f3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongCat-Flash-Thinking-2601 is a powerful open-source reasoning model with 560 billion parameters, excelling in complex reasoning tasks.</li>\n    <li>It outperforms other open-source models in various benchmarks, including tool use and agentic search.</li>\n    <li>The model's effectiveness comes from a unique training approach that integrates expert training and advanced data construction methods.</li>\n    <li>It is designed to handle real-world complexities and noise, enhancing its reliability in practical applications.</li>\n    <li>A special feature called Heavy Thinking mode allows the model to improve its reasoning capabilities during testing by increasing its processing depth and breadth.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:20:09.000Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16725.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16746", "authors": [{"_id": "6976d4105d41524304c13517", "name": "Yuhang Wang", "hidden": false}, {"_id": "6976d4105d41524304c13518", "user": {"_id": "645b0c3ec35da9c7afd95421", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg", "isPro": false, "fullname": "Yuling", "user": "YerbaPage", "type": "user"}, "name": "Yuling Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:56.805Z", "hidden": false}, {"_id": "6976d4105d41524304c13519", "name": "Mo Yang", "hidden": false}, {"_id": "6976d4105d41524304c1351a", "name": "Rongrui Zhang", "hidden": false}, {"_id": "6976d4105d41524304c1351b", "name": "Shilin He", "hidden": false}, {"_id": "6976d4105d41524304c1351c", "name": "Heng Lian", "hidden": false}, {"_id": "6976d4105d41524304c1351d", "name": "Yuting Chen", "hidden": false}, {"_id": "6976d4105d41524304c1351e", "name": "Siyu Ye", "hidden": false}, {"_id": "6976d4105d41524304c1351f", "name": "Kai Cai", "hidden": false}, {"_id": "6976d4105d41524304c13520", "name": "Xiaodong Gu", "hidden": false}], "publishedAt": "2026-01-23T13:51:59.000Z", "submittedOnDailyAt": "2026-01-26T00:10:23.451Z", "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As a result, they frequently disrupt syntactic and logical structure and fail to retain critical implementation details. In this paper, we propose SWE-Pruner, a self-adaptive context pruning framework tailored for coding agents. Drawing inspiration from how human programmers \"selectively skim\" source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. Given the current task, the agent formulates an explicit goal (e.g., \"focus on error handling\") as a hint to guide the pruning targets. A lightweight neural skimmer (0.6B parameters) is trained to dynamically select relevant lines from the surrounding context given the goal. Evaluations across four benchmarks and multiple models validate SWE-Pruner's effectiveness in various scenarios, achieving 23-54% token reduction on agent tasks like SWE-Bench Verified and up to 14.84x compression on single-turn tasks like LongCodeQA with minimal performance impact.", "upvotes": 63, "discussionId": "6976d4105d41524304c13521", "githubRepo": "https://github.com/Ayanami1314/swe-pruner", "githubRepoAddedBy": "user", "ai_summary": "SWE-Pruner is a self-adaptive context pruning framework for coding agents that uses task-aware pruning to reduce token usage while maintaining performance.", "ai_keywords": ["context compression", "LongLLMLingua", "PPL", "code understanding", "task-aware adaptive pruning", "neural skimmer", "token reduction", "SWE-Bench Verified", "LongCodeQA"], "githubStars": 35, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>LLM\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u957f\u65f6\u95f4\u7684\u4ea4\u4e92\u4f1a\u5bfc\u81f4\u9ad8API\u6210\u672c\u548c\u5ef6\u8fdf\u3002</li>\n    <li>\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u6307\u6807\uff0c\u672a\u80fd\u8003\u8651\u4ee3\u7801\u7406\u89e3\u7684\u4efb\u52a1\u7279\u6027\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86SWE-Pruner\uff0c\u4e00\u4e2a\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u526a\u679d\u6846\u67b6\uff0c\u4e13\u4e3a\u7f16\u7801\u4ee3\u7406\u8bbe\u8ba1\u3002</li>\n    <li>SWE-Pruner\u501f\u9274\u4eba\u7c7b\u7a0b\u5e8f\u5458\u5728\u5f00\u53d1\u548c\u8c03\u8bd5\u8fc7\u7a0b\u4e2d\u201c\u9009\u62e9\u6027\u6d4f\u89c8\u201d\u6e90\u4ee3\u7801\u7684\u65b9\u5f0f\uff0c\u8fdb\u884c\u4efb\u52a1\u611f\u77e5\u7684\u81ea\u9002\u5e94\u526a\u679d\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cSWE-Pruner\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\uff0c\u4efb\u52a1\u4ee4\u724c\u51cf\u5c1123-54%\uff0c\u5728\u5355\u56de\u5408\u4efb\u52a1\u4e2d\u538b\u7f29\u7387\u9ad8\u8fbe14.84\u500d\uff0c\u4e14\u6027\u80fd\u5f71\u54cd\u6700\u5c0f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LLM agents are good at software development but struggle with long interactions due to high costs and delays.</li>\n    <li>Current methods for reducing context size often lose important details and disrupt code structure.</li>\n    <li>The new approach, SWE-Pruner, is designed specifically for coding tasks and adapts to current goals.</li>\n    <li>SWE-Pruner uses a small neural model to choose important lines of code based on a defined goal.</li>\n    <li>Tests show SWE-Pruner can reduce token usage by 23-54% with little effect on performance.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:51:59.000Z", "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents", "summary": "LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As a result, they frequently disrupt syntactic and logical structure and fail to retain critical implementation details. In this paper, we propose SWE-Pruner, a self-adaptive context pruning framework tailored for coding agents. Drawing inspiration from how human programmers \"selectively skim\" source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. Given the current task, the agent formulates an explicit goal (e.g., \"focus on error handling\") as a hint to guide the pruning targets. A lightweight neural skimmer (0.6B parameters) is trained to dynamically select relevant lines from the surrounding context given the goal. Evaluations across four benchmarks and multiple models validate SWE-Pruner's effectiveness in various scenarios, achieving 23-54% token reduction on agent tasks like SWE-Bench Verified and up to 14.84x compression on single-turn tasks like LongCodeQA with minimal performance impact.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16746.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.15876", "authors": [{"_id": "6972d8d5fb12c92b735b73a2", "name": "Taofeng Xue", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a3", "user": {"_id": "6801cbd5f06f2d08f3fd5455", "avatarUrl": "/avatars/c728b48f6938c0d7cb6b14011927ede8.svg", "isPro": false, "fullname": "chong.peng", "user": "KleinChong", "type": "user"}, "name": "Chong Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T20:15:18.698Z", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a4", "name": "Mianqiu Huang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a5", "name": "Linsen Guo", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a6", "user": {"_id": "6764279e684ed3b61b2316a4", "avatarUrl": "/avatars/63a6b6c3b9f442b42480679425951187.svg", "isPro": false, "fullname": "SII-TianchengHAN", "user": "GenSouKai", "type": "user"}, "name": "Tiancheng Han", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:30.106Z", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a7", "name": "Haozhe Wang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a8", "name": "Jianing Wang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a9", "name": "Xiaocheng Zhang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73aa", "name": "Xin Yang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ab", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ac", "name": "Jinrui Ding", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ad", "name": "Xiandi Ma", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ae", "name": "Yuchen Xie", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73af", "name": "Peng Pei", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73b0", "name": "Xunliang Cai", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73b1", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-01-22T11:36:43.000Z", "submittedOnDailyAt": "2026-01-23T07:54:00.525Z", "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience", "submittedOnDailyBy": {"_id": "6459c7c10aba070266e41bb1", "avatarUrl": "/avatars/2178cac69cf4123db5e85191160f3795.svg", "isPro": false, "fullname": "mqhuang", "user": "LutherXD", "type": "user"}, "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.", "upvotes": 62, "discussionId": "6972d8d5fb12c92b735b73b2", "ai_summary": "EvoCUA introduces an evolutionary approach to computer-use agents that combines autonomous task generation with policy optimization to achieve superior performance in complex, long-horizon tasks.", "ai_keywords": ["computer-use agents", "native computer-use agents", "data generation", "policy optimization", "evolutionary cycle", "verifiable synthesis engine", "executable validators", "sandbox rollouts", "iterative evolving learning", "capability boundaries", "error analysis", "self-correction", "OSWorld benchmark", "foundation models"], "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u6a21\u578bEvoCUA\uff0c\u65e8\u5728\u514b\u670d\u4f20\u7edf\u9759\u6001\u6570\u636e\u9650\u5236\u3002</li>\n    <li>EvoCUA\u901a\u8fc7\u81ea\u6211\u7ef4\u6301\u7684\u8fdb\u5316\u5faa\u73af\uff0c\u5c06\u6570\u636e\u751f\u6210\u4e0e\u7b56\u7565\u4f18\u5316\u7ed3\u5408\u8d77\u6765\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u7684\u5408\u6210\u5f15\u64ce\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u7684\u4efb\u52a1\u5e76\u8fdb\u884c\u6267\u884c\u9a8c\u8bc1\u3002</li>\n    <li>EvoCUA\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e8656.7%\u7684\u6210\u529f\u7387\uff0c\u8d85\u8d8a\u4e86\u4ee5\u5f80\u7684\u6700\u4f73\u5f00\u6e90\u6a21\u578bOpenCUA-72B\u548c\u4e00\u4e9b\u9886\u5148\u7684\u95ed\u6e90\u6a21\u578b\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u901a\u8fc7\u7ecf\u9a8c\u5b66\u4e60\u9a71\u52a8\u7684\u8fdb\u5316\u6a21\u5f0f\u53ef\u4ee5\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u57fa\u7840\u6a21\u578b\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u7684\u6027\u80fd\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>EvoCUA is a new type of computer-use agent that improves how AI handles complex tasks.</li>\n    <li>It generates its own data and optimizes its performance through a self-improving process, rather than just imitating existing data.</li>\n    <li>A special engine helps create diverse tasks and checks their performance automatically.</li>\n    <li>EvoCUA uses a large-scale system to gather experience from many simulated tasks, learning from both successes and mistakes.</li>\n    <li>Tests show EvoCUA achieves a 56.7% success rate, outperforming previous models and proving that this evolving learning method is effective across different AI scales.</li>\n</ul>"}, "publishedAt": "2026-01-22T06:36:43.000Z", "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience", "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15876.png", "numComments": 1, "submittedBy": {"_id": "6459c7c10aba070266e41bb1", "avatarUrl": "/avatars/2178cac69cf4123db5e85191160f3795.svg", "fullname": "mqhuang", "name": "LutherXD", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.15165", "authors": [{"_id": "6971933ac1c7409747bf9597", "name": "Zanlin Ni", "hidden": false}, {"_id": "6971933ac1c7409747bf9598", "user": {"_id": "6486dde1f74857df3f1a5828", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg", "isPro": false, "fullname": "Shenzhi Wang", "user": "shenzhi-wang", "type": "user"}, "name": "Shenzhi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:54.254Z", "hidden": false}, {"_id": "6971933ac1c7409747bf9599", "name": "Yang Yue", "hidden": false}, {"_id": "6971933ac1c7409747bf959a", "name": "Tianyu Yu", "hidden": false}, {"_id": "6971933ac1c7409747bf959b", "name": "Weilin Zhao", "hidden": false}, {"_id": "6971933ac1c7409747bf959c", "name": "Yeguo Hua", "hidden": false}, {"_id": "6971933ac1c7409747bf959d", "name": "Tianyi Chen", "hidden": false}, {"_id": "6971933ac1c7409747bf959e", "name": "Jun Song", "hidden": false}, {"_id": "6971933ac1c7409747bf959f", "name": "Cheng Yu", "hidden": false}, {"_id": "6971933ac1c7409747bf95a0", "name": "Bo Zheng", "hidden": false}, {"_id": "6971933ac1c7409747bf95a1", "name": "Gao Huang", "hidden": false}], "publishedAt": "2026-01-21T16:41:58.000Z", "submittedOnDailyAt": "2026-01-23T00:11:51.141Z", "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models", "submittedOnDailyBy": {"_id": "63987ffb2ceb55aabe0852f3", "avatarUrl": "/avatars/343b796ff6b8906203904e8c620d7eb5.svg", "isPro": false, "fullname": "Zanlin Ni", "user": "nzl-thu", "type": "user"}, "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap", "upvotes": 55, "discussionId": "6971933ac1c7409747bf95a2", "projectPage": "https://nzl-thu.github.io/the-flexibility-trap", "githubRepo": "https://github.com/LeapLabTHU/JustGRPO", "githubRepoAddedBy": "user", "ai_summary": "Arbitrary order generation in diffusion large language models limits reasoning capability by causing premature solution space collapse, making standard policy optimization more effective.", "ai_keywords": ["diffusion large language models", "left-to-right constraint", "token generation", "reinforcement learning", "reasoning potential", "mathematical reasoning", "coding tasks", "combinatorial trajectories", "likelihoods", "Group Relative Policy Optimization", "GRPO", "parallel decoding"], "githubStars": 66, "organization": {"_id": "69719700e3846c07669d13ee", "name": "Tsinghua-LeapLab", "fullname": "Tsinghua-LeapLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63987ffb2ceb55aabe0852f3/hflTWNTGxeJx83xNkYrDB.png"}, "summary_zh": "<ul>\n    <li>\u6269\u6563\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u6253\u7834\u4e86\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u7684\u5de6\u5230\u53f3\u751f\u6210\u9650\u5236\uff0c\u53ef\u4ee5\u4ee5\u4efb\u610f\u987a\u5e8f\u751f\u6210\u6807\u8bb0\u3002</li>\n    <li>\u8fd9\u79cd\u7075\u6d3b\u6027\u672c\u5e94\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5f53\u524d\u5f62\u5f0f\u53cd\u800c\u7f29\u5c0f\u4e86dLLMs\u7684\u63a8\u7406\u8fb9\u754c\u3002</li>\n    <li>dLLMs\u5229\u7528\u987a\u5e8f\u7075\u6d3b\u6027\u8df3\u8fc7\u9ad8\u4e0d\u786e\u5b9a\u6027\u6807\u8bb0\uff0c\u5bfc\u81f4\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u7684\u63d0\u524d\u5d29\u6e83\u3002</li>\n    <li>\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5e94\u5bf9\u8fd9\u79cd\u7075\u6d3b\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u7684JustGRPO\u65b9\u6cd5\u653e\u5f03\u4efb\u610f\u987a\u5e8f\uff0c\u91c7\u7528\u6807\u51c6\u7684\u7b56\u7565\u4f18\u5316\uff0c\u53d6\u5f97\u4e86\u610f\u60f3\u4e0d\u5230\u7684\u6548\u679c\uff08\u5982\u5728GSM8K\u4e0a\u8fbe\u523089.1%\u7684\u51c6\u786e\u7387\uff09\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion Large Language Models (dLLMs) can generate tokens in any order, unlike traditional models that follow a strict left-to-right pattern.</li>\n    <li>This flexibility was thought to improve reasoning abilities in tasks like math and coding, leading researchers to use reinforcement learning (RL) to enhance these capabilities.</li>\n    <li>However, the paper finds that allowing arbitrary order in generation actually limits the reasoning abilities of dLLMs by avoiding complex, uncertain tokens that are important for exploration.</li>\n    <li>The authors suggest a new method called JustGRPO, which simplifies the approach by focusing on standard techniques instead of maintaining arbitrary order, achieving high accuracy in reasoning tasks.</li>\n    <li>JustGRPO retains the parallel generation ability of dLLMs while improving their effectiveness in reasoning challenges.</li>\n</ul>"}, "publishedAt": "2026-01-21T11:41:58.000Z", "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models", "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15165.png", "numComments": 1, "submittedBy": {"_id": "63987ffb2ceb55aabe0852f3", "avatarUrl": "/avatars/343b796ff6b8906203904e8c620d7eb5.svg", "fullname": "Zanlin Ni", "name": "nzl-thu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "69719700e3846c07669d13ee", "name": "Tsinghua-LeapLab", "fullname": "Tsinghua-LeapLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63987ffb2ceb55aabe0852f3/hflTWNTGxeJx83xNkYrDB.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.14724", "authors": [{"_id": "6972ee7afb12c92b735b74b4", "user": {"_id": "637169557a5e5d8efdc3e58e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg", "isPro": false, "fullname": "Haowei Zhang", "user": "freesky", "type": "user"}, "name": "Haowei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:23.639Z", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b5", "name": "Shudong Yang", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b6", "name": "Jinlan Fu", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b7", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b8", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-01-21T07:26:15.000Z", "submittedOnDailyAt": "2026-01-23T01:43:37.582Z", "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding", "submittedOnDailyBy": {"_id": "637169557a5e5d8efdc3e58e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg", "isPro": false, "fullname": "Haowei Zhang", "user": "freesky", "type": "user"}, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10times faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.", "upvotes": 52, "discussionId": "6972ee7bfb12c92b735b74b9", "projectPage": "https://hermes-streaming.github.io/", "githubRepo": "https://github.com/haowei-freesky/HERMES", "githubRepoAddedBy": "user", "ai_summary": "HERMES is a training-free architecture that enables real-time video stream understanding by utilizing a hierarchical memory framework based on KV cache reuse, achieving faster response times and maintained accuracy even with reduced video token input.", "ai_keywords": ["Multimodal Large Language Models", "video understanding", "streaming video inputs", "real-time responses", "KV cache", "hierarchical memory framework", "mechanistic attention", "video tokens", "TTFT"], "githubStars": 24, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u7814\u7a76\u5728\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4e0a\u6709\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5c24\u5176\u662f\u5728\u79bb\u7ebf\u89c6\u9891\u7406\u89e3\u65b9\u9762\u3002</li>\n    <li>\u5c06\u8fd9\u4e9b\u80fd\u529b\u6269\u5c55\u5230\u6d41\u89c6\u9891\u8f93\u5165\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u6a21\u578b\u5728\u7406\u89e3\u6027\u80fd\u3001\u5b9e\u65f6\u54cd\u5e94\u548c\u4f4eGPU\u5185\u5b58\u5f00\u9500\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86HERMES\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u65e0\u8bad\u7ec3\u67b6\u6784\uff0c\u7528\u4e8e\u5b9e\u65f6\u548c\u51c6\u786e\u5730\u7406\u89e3\u89c6\u9891\u6d41\u3002</li>\n    <li>HERMES\u5229\u7528\u4e00\u79cd\u5206\u5c42\u7684KV\u7f13\u5b58\u673a\u5236\uff0c\u53ef\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u9ad8\u6548\u7406\u89e3\u89c6\u9891\u6d41\u3002</li>\n    <li>HERMES\u5728\u7528\u6237\u67e5\u8be2\u5230\u8fbe\u65f6\u65e0\u9700\u989d\u5916\u8ba1\u7b97\uff0c\u4fdd\u8bc1\u4e86\u5b9e\u65f6\u54cd\u5e94\uff0c\u5e76\u4e14\u5728\u6d41\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe11.4%\u7684\u51c6\u786e\u6027\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent improvements in Multimodal Large Language Models (MLLMs) have helped with understanding offline videos, but live video streaming is still difficult.</li>\n    <li>HERMES is a new architecture designed to help understand video streams in real-time without needing additional training.</li>\n    <li>HERMES uses a special memory system that keeps track of video information efficiently, allowing it to work well even with limited resources.</li>\n    <li>This system can respond to user queries instantly, making it 10 times faster than previous models.</li>\n    <li>HERMES can reduce the amount of video data processed by 68% while still maintaining or improving accuracy in understanding streaming videos.</li>\n</ul>"}, "publishedAt": "2026-01-21T02:26:15.000Z", "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10times faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14724.png", "numComments": 2, "submittedBy": {"_id": "637169557a5e5d8efdc3e58e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg", "fullname": "Haowei Zhang", "name": "freesky", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.14133", "authors": [{"_id": "69706e6ea8be625b19c2afac", "user": {"_id": "63d3b5f1640bb0f77173baea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674819020331-noauth.jpeg", "isPro": false, "fullname": "yubin", "user": "VLyb", "type": "user"}, "name": "Bin Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:19:14.460Z", "hidden": false}, {"_id": "69706e6ea8be625b19c2afad", "user": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "isPro": false, "fullname": "Shijie Lian", "user": "LiamLian0727", "type": "user"}, "name": "Shijie Lian", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:47:15.246Z", "hidden": false}, {"_id": "69706e6ea8be625b19c2afae", "name": "Xiaopeng Lin", "hidden": false}, {"_id": "69706e6ea8be625b19c2afaf", "name": "Yuliang Wei", "hidden": false}, {"_id": "69706e6ea8be625b19c2afb0", "name": "Zhaolong Shen", "hidden": false}, {"_id": "69706e6ea8be625b19c2afb1", "name": "Changti Wu", "hidden": false}, {"_id": "69706e6ea8be625b19c2afb2", "name": "Yuzhuo Miao", "hidden": false}, {"_id": "69706e6ea8be625b19c2afb3", "name": "Xinming Wang", "hidden": false}, {"_id": "69706e6ea8be625b19c2afb4", "name": "Bailing Wang", "hidden": false}, {"_id": "69706e6ea8be625b19c2afb5", "name": "Cong Huang", "hidden": false}, {"_id": "69706e6ea8be625b19c2afb6", "name": "Kai Chen", "hidden": false}], "publishedAt": "2026-01-20T16:30:07.000Z", "submittedOnDailyAt": "2026-01-26T00:14:31.380Z", "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers", "submittedOnDailyBy": {"_id": "63d3b5f1640bb0f77173baea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674819020331-noauth.jpeg", "isPro": false, "fullname": "yubin", "user": "VLyb", "type": "user"}, "summary": "Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to \"catastrophic forgetting\" of the model's open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen \"Left Brain\", which retains robust general visual reasoning, with a trainable \"Right Brain\", specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.", "upvotes": 52, "discussionId": "69706e6ea8be625b19c2afb7", "githubRepo": "https://github.com/ZGC-EmbodyAI/TwinBrainVLA", "githubRepoAddedBy": "user", "ai_summary": "TwinBrainVLA addresses the tension between semantic understanding and motor skills in robot control by coordinating a generalist vision-language model with a specialist model through an asymmetric mixture-of-transformers mechanism.", "ai_keywords": ["Vision-Language-Action models", "Vision-Language Models", "robotic control", "catastrophic forgetting", "frozen Left Brain", "trainable Right Brain", "Asymmetric Mixture-of-Transformers", "Flow-Matching Action Expert", "embodied perception", "proprioception"], "githubStars": 9, "organization": {"_id": "68896d3a716ee5bfb1428441", "name": "ZGCA", "fullname": "Zhongguancun Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"}, "summary_zh": "<ul>\n    <li>\u4f20\u7edf\u7684\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u6a21\u578b\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u5b58\u5728\u9ad8\u5c42\u8bed\u4e49\u7406\u89e3\u4e0e\u4f4e\u5c42\u611f\u77e5\u6280\u80fd\u4e4b\u95f4\u7684\u77db\u76fe\u3002</li>\n    <li>TwinBrainVLA\u662f\u4e00\u79cd\u65b0\u67b6\u6784\uff0c\u7ed3\u5408\u4e86\u901a\u7528\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u548c\u4e13\u95e8\u7684\u611f\u77e5\u6a21\u578b\u3002</li>\n    <li>\u8be5\u67b6\u6784\u4f7f\u7528\u4e00\u4e2a\u56fa\u5b9a\u7684\u201c\u5de6\u8111\u201d\u4fdd\u6301\u5f3a\u5927\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u548c\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u201c\u53f3\u8111\u201d\u4e13\u6ce8\u4e8e\u8eab\u4f53\u611f\u77e5\u3002</li>\n    <li>\u53f3\u8111\u53ef\u4ee5\u4ece\u5de6\u8111\u52a8\u6001\u67e5\u8be2\u8bed\u4e49\u77e5\u8bc6\uff0c\u4ee5\u751f\u6210\u7cbe\u786e\u7684\u63a7\u5236\u52a8\u4f5c\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cTwinBrainVLA\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Standard models for robotic control often struggle to balance general understanding with specific skills, leading to loss of important knowledge.</li>\n    <li>TwinBrainVLA is a new approach that uses two types of Vision-Language Models: a generalist (Left Brain) for broad understanding, and a specialist (Right Brain) for specific robotic tasks.</li>\n    <li>The Left Brain remains unchanged, while the Right Brain learns to connect sensory information with knowledge from the Left Brain.</li>\n    <li>This combination allows for better control in robots by using a unique method called Asymmetric Mixture-of-Transformers (AsyMoT).</li>\n    <li>Tests show TwinBrainVLA outperforms existing models in robotic manipulation while keeping strong visual understanding intact.</li>\n</ul>"}, "publishedAt": "2026-01-20T11:30:07.000Z", "title": "TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers", "summary": "Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to \"catastrophic forgetting\" of the model's open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen \"Left Brain\", which retains robust general visual reasoning, with a trainable \"Right Brain\", specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14133.png", "numComments": 1, "submittedBy": {"_id": "63d3b5f1640bb0f77173baea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674819020331-noauth.jpeg", "fullname": "yubin", "name": "VLyb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "68896d3a716ee5bfb1428441", "name": "ZGCA", "fullname": "Zhongguancun Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.15197", "authors": [{"_id": "6971c608c1c7409747bf96a5", "user": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "isPro": false, "fullname": "Shijie Lian", "user": "LiamLian0727", "type": "user"}, "name": "Shijie Lian", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:44:25.547Z", "hidden": false}, {"_id": "6971c608c1c7409747bf96a6", "name": "Bin Yu", "hidden": false}, {"_id": "6971c608c1c7409747bf96a7", "name": "Xiaopeng Lin", "hidden": false}, {"_id": "6971c608c1c7409747bf96a8", "name": "Laurence T. Yang", "hidden": false}, {"_id": "6971c608c1c7409747bf96a9", "name": "Zhaolong Shen", "hidden": false}, {"_id": "6971c608c1c7409747bf96aa", "name": "Changti Wu", "hidden": false}, {"_id": "6971c608c1c7409747bf96ab", "name": "Yuzhuo Miao", "hidden": false}, {"_id": "6971c608c1c7409747bf96ac", "name": "Cong Huang", "hidden": false}, {"_id": "6971c608c1c7409747bf96ad", "name": "Kai Chen", "hidden": false}], "publishedAt": "2026-01-21T17:15:22.000Z", "submittedOnDailyAt": "2026-01-23T00:45:20.588Z", "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries", "submittedOnDailyBy": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "isPro": false, "fullname": "Shijie Lian", "user": "LiamLian0727", "type": "user"}, "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior \u03c0(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.", "upvotes": 50, "discussionId": "6971c609c1c7409747bf96ae", "projectPage": "https://github.com/ZGC-EmbodyAI/BayesianVLA", "githubRepo": "https://github.com/ZGC-EmbodyAI/BayesianVLA", "githubRepoAddedBy": "user", "ai_summary": "BayesianVLA addresses language-action grounding issues in robot manipulation by using Bayesian decomposition to prevent information collapse and improve out-of-distribution generalization.", "ai_keywords": ["Vision-Language-Action models", "Information Collapse", "Bayesian decomposition", "latent action queries", "conditional Pointwise Mutual Information", "vision-only policies", "out-of-distribution generalization", "SimplerEnv", "RoboCasa"], "githubStars": 11, "organization": {"_id": "68896d3a716ee5bfb1428441", "name": "ZGCA", "fullname": "Zhongguancun Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u96be\u4ee5\u9002\u5e94\u65b0\u6307\u4ee4\u6216\u590d\u6742\u7684\u591a\u4efb\u52a1\u573a\u666f\u3002</li>\n    <li>\u5f53\u524d\u7684\u8bad\u7ec3\u65b9\u6cd5\u5bfc\u81f4\u6570\u636e\u96c6\u504f\u5dee\uff0c\u4f7f\u5f97\u8bed\u8a00\u6307\u4ee4\u4e0e\u89c6\u89c9\u89c2\u5bdf\u9ad8\u5ea6\u53ef\u9884\u6d4b\uff0c\u4ea7\u751f\u4fe1\u606f\u5d29\u6e83\u73b0\u8c61\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86BayesianVLA\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u5206\u89e3\u6765\u5f3a\u5316\u6307\u4ee4\u9075\u5faa\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u6f5c\u5728\u52a8\u4f5c\u67e5\u8be2\uff0c\u6784\u5efa\u53cc\u5206\u652f\u67b6\u6784\uff0c\u4f18\u5316\u52a8\u4f5c\u4e0e\u6307\u4ee4\u4e4b\u95f4\u7684\u6761\u4ef6\u4e92\u4fe1\u606f\u3002</li>\n    <li>BayesianVLA\u5728\u4e0d\u9700\u8981\u65b0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728OOD\u57fa\u51c6\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-Language-Action (VLA) models help robots understand and follow instructions, but they struggle with new tasks.</li>\n    <li>A problem called Information Collapse occurs when data biases make it easy for models to predict instructions from visuals alone, causing them to ignore language entirely.</li>\n    <li>To fix this, the authors created a new framework called BayesianVLA that helps robots follow instructions better by using a dual-branch structure.</li>\n    <li>This framework encourages the robot to connect actions more closely with the given instructions, improving its ability to handle different scenarios.</li>\n    <li>Tests show that BayesianVLA significantly enhances the model's performance, achieving an 11.3% improvement in challenging situations without needing new data.</li>\n</ul>"}, "publishedAt": "2026-01-21T12:15:22.000Z", "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries", "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior \u03c0(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15197.png", "numComments": 2, "submittedBy": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "fullname": "Shijie Lian", "name": "LiamLian0727", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "68896d3a716ee5bfb1428441", "name": "ZGCA", "fullname": "Zhongguancun Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16206", "authors": [{"_id": "6972e04dfb12c92b735b73cf", "user": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "isPro": false, "fullname": "Daixuan Cheng", "user": "daixuancheng", "type": "user"}, "name": "Daixuan Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:28.070Z", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d0", "name": "Shaohan Huang", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d1", "name": "Yuxian Gu", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d2", "name": "Huatong Song", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d3", "name": "Guoxin Chen", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d4", "name": "Li Dong", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d5", "name": "Wayne Xin Zhao", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d6", "name": "Ji-Rong Wen", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d7", "name": "Furu Wei", "hidden": false}], "publishedAt": "2026-01-22T18:57:09.000Z", "submittedOnDailyAt": "2026-01-23T00:27:12.305Z", "title": "LLM-in-Sandbox Elicits General Agentic Intelligence", "submittedOnDailyBy": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "isPro": false, "fullname": "Daixuan Cheng", "user": "daixuancheng", "type": "user"}, "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.", "upvotes": 46, "discussionId": "6972e04dfb12c92b735b73d8", "projectPage": "https://llm-in-sandbox.github.io", "githubRepo": "https://github.com/llm-in-sandbox/llm-in-sandbox", "githubRepoAddedBy": "user", "ai_summary": "LLM-in-Sandbox enables large language models to perform general intelligence tasks across diverse domains by allowing them to explore a code sandbox environment, achieving robust generalization without additional training.", "ai_keywords": ["LLM-in-Sandbox", "code sandbox", "virtual computer", "reinforcement learning", "non-agentic data", "sandbox exploration", "general intelligence", "long-context understanding", "instruction following"], "githubStars": 38, "organization": {"_id": "68151d0f51add3813f3f7d1b", "name": "MicrosoftResearch", "fullname": "Microsoft Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86LLM-in-Sandbox\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u5728\u4ee3\u7801\u6c99\u76d2\u4e2d\u63a2\u7d22\uff0c\u6fc0\u53d1\u975e\u4ee3\u7801\u9886\u57df\u7684\u4e00\u822c\u667a\u80fd\u3002</li>\n    <li>\u5f3a\u5927\u7684LLM\u5728\u6ca1\u6709\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u5229\u7528\u4ee3\u7801\u6c99\u76d2\u8fdb\u884c\u975e\u4ee3\u7801\u4efb\u52a1\uff0c\u5982\u8bbf\u95ee\u5916\u90e8\u8d44\u6e90\u548c\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u3002</li>\n    <li>\u901a\u8fc7LLM-in-Sandbox\u5f3a\u5316\u5b66\u4e60\uff08LLM-in-Sandbox-RL\uff09\uff0c\u53ef\u4ee5\u589e\u5f3a\u8fd9\u4e9b\u63a2\u7d22\u80fd\u529b\uff0c\u53ea\u4f7f\u7528\u975e\u667a\u80fd\u6570\u636e\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cLLM-in-Sandbox\u5728\u6570\u5b66\u3001\u7269\u7406\u3001\u5316\u5b66\u3001\u751f\u7269\u533b\u5b66\u7b49\u9886\u57df\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n    <li>\u4ece\u8ba1\u7b97\u548c\u7cfb\u7edf\u7684\u89d2\u5ea6\u5206\u6790\u4e86LLM-in-Sandbox\u7684\u6548\u7387\uff0c\u5e76\u5c06\u5176\u5f00\u6e90\u4e3aPython\u5305\uff0c\u4fbf\u4e8e\u5b9e\u9645\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LLM-in-Sandbox allows large language models (LLMs) to work in a controlled virtual environment to improve their intelligence in areas outside of coding.</li>\n    <li>These models can use the sandbox to access new information, manage large amounts of data, and format outputs without needing extra training.</li>\n    <li>Their abilities can be further improved through a training method called LLM-in-Sandbox Reinforcement Learning, which focuses on exploring the sandbox with existing data.</li>\n    <li>Tests show that LLM-in-Sandbox can effectively handle tasks in various fields like math, science, and instruction following, both with and without additional training.</li>\n    <li>The system's efficiency has been evaluated and it has been made available as an open-source Python package for practical use.</li>\n</ul>"}, "publishedAt": "2026-01-22T13:57:09.000Z", "title": "LLM-in-Sandbox Elicits General Agentic Intelligence", "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16206.png", "numComments": 2, "submittedBy": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "fullname": "Daixuan Cheng", "name": "daixuancheng", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 16, "isUserFollowing": false}, "organization": {"_id": "68151d0f51add3813f3f7d1b", "name": "MicrosoftResearch", "fullname": "Microsoft Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16208", "authors": [{"_id": "6972ea8bfb12c92b735b74a8", "name": "Shengbang Tong", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74a9", "name": "Boyang Zheng", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74aa", "user": {"_id": "64249f76d476e4ad55665d59", "avatarUrl": "/avatars/a0fec7e423ffae944a874ca267b55c1f.svg", "isPro": false, "fullname": "Ziteng Wang", "user": "AustinWang0330", "type": "user"}, "name": "Ziteng Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:01.136Z", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ab", "name": "Bingda Tang", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ac", "name": "Nanye Ma", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ad", "user": {"_id": "626dc5105f7327906f0b2a4e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626dc5105f7327906f0b2a4e/QCSzuwYqsv8ozRnusVb-F.jpeg", "isPro": true, "fullname": "Ellis Brown", "user": "ellisbrown", "type": "user"}, "name": "Ellis Brown", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T20:15:10.637Z", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ae", "name": "Jihan Yang", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74af", "name": "Rob Fergus", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74b0", "name": "Yann LeCun", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74b1", "name": "Saining Xie", "hidden": false}], "publishedAt": "2026-01-22T18:58:16.000Z", "submittedOnDailyAt": "2026-01-23T00:57:17.761Z", "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "submittedOnDailyBy": {"_id": "6434226da4c9c55871a78052", "avatarUrl": "/avatars/3309832b3115bc6ad08ae1d10f43118b.svg", "isPro": false, "fullname": "BoYang Zheng", "user": "bytetriper", "type": "user"}, "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.", "upvotes": 40, "discussionId": "6972ea8cfb12c92b735b74b2", "projectPage": "https://rae-dit.github.io/scale-rae/", "githubRepo": "https://github.com/ZitengWangNYU/Scale-RAE", "githubRepoAddedBy": "user", "ai_summary": "Representation Autoencoders (RAEs) demonstrate superior performance over VAEs in large-scale text-to-image generation, showing improved stability, faster convergence, and better quality while enabling unified multimodal reasoning in shared representation spaces.", "ai_keywords": ["representation autoencoders", "diffusion modeling", "semantic latent spaces", "text-to-image generation", "frozen representation encoder", "SigLIP-2", "noise scheduling", "diffusion transformers", "pretraining", "finetuning", "catastrophic overfitting", "multimodal model", "shared representation space"], "githubStars": 58, "organization": {"_id": "662741612ada5b77e310d171", "name": "nyu-visionx", "fullname": "VISIONx @ NYU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"}, "summary_zh": "<ul>\n    <li>\u7814\u7a76\u4e86\u8868\u793a\u81ea\u7f16\u7801\u5668\uff08RAE\uff09\u5728\u5927\u89c4\u6a21\u81ea\u7531\u5f62\u5f0f\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5e94\u7528\u3002</li>\n    <li>\u901a\u8fc7\u8bad\u7ec3\u7f51\u7edc\u3001\u5408\u6210\u548c\u6587\u672c\u6e32\u67d3\u6570\u636e\uff0c\u53d1\u73b0\u7279\u5b9a\u9886\u57df\uff08\u5982\u6587\u672c\uff09\u7684\u6570\u636e\u7ec4\u5408\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u5206\u6790\u8868\u660e\uff0cRAE\u6846\u67b6\u5728\u89c4\u6a21\u6269\u5c55\u65f6\u53d8\u5f97\u66f4\u7b80\u5355\uff0c\u67d0\u4e9b\u590d\u6742\u7ed3\u6784\u7684\u597d\u5904\u4e0d\u660e\u663e\u3002</li>\n    <li>RAE\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u9636\u6bb5\u7684\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684FLUX VAE\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u4e0a\u3002</li>\n    <li>RAE\u6a21\u578b\u5728\u751f\u6210\u8d28\u91cf\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4e14\u53ef\u4ee5\u5728\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u4e2d\u8fdb\u884c\u591a\u6a21\u6001\u63a8\u7406\uff0c\u5f00\u542f\u65b0\u7684\u6a21\u578b\u53ef\u80fd\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Representation Autoencoders (RAEs) improve text-to-image generation by using high-dimensional data.</li>\n    <li>Training on diverse data types helps enhance the quality of generated images, especially with text.</li>\n    <li>RAEs simplify the modeling framework, showing that less complex designs can still be very effective.</li>\n    <li>RAEs outperform Variational Autoencoders (VAEs) in both pretraining and finetuning, achieving better results and stability.</li>\n    <li>The shared representation space of RAEs allows for better integration of visual understanding and generation, enabling new model capabilities.</li>\n</ul>"}, "publishedAt": "2026-01-22T13:58:16.000Z", "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16208.png", "numComments": 1, "submittedBy": {"_id": "6434226da4c9c55871a78052", "avatarUrl": "/avatars/3309832b3115bc6ad08ae1d10f43118b.svg", "fullname": "BoYang Zheng", "name": "bytetriper", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "662741612ada5b77e310d171", "name": "nyu-visionx", "fullname": "VISIONx @ NYU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.15892", "authors": [{"_id": "6972d788fb12c92b735b7397", "user": {"_id": "641aa5e391e3376a057bbd4c", "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg", "isPro": false, "fullname": "Chenghao Fan", "user": "Facico", "type": "user"}, "name": "Chenghao Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:32.109Z", "hidden": false}, {"_id": "6972d788fb12c92b735b7398", "name": "Wen Heng", "hidden": false}, {"_id": "6972d788fb12c92b735b7399", "name": "Bo Li", "hidden": false}, {"_id": "6972d788fb12c92b735b739a", "name": "Sichen Liu", "hidden": false}, {"_id": "6972d788fb12c92b735b739b", "name": "Yuxuan Song", "hidden": false}, {"_id": "6972d788fb12c92b735b739c", "name": "Jing Su", "hidden": false}, {"_id": "6972d788fb12c92b735b739d", "name": "Xiaoye Qu", "hidden": false}, {"_id": "6972d788fb12c92b735b739e", "name": "Kai Shen", "hidden": false}, {"_id": "6972d788fb12c92b735b739f", "name": "Wei Wei", "hidden": false}], "publishedAt": "2026-01-22T12:13:17.000Z", "submittedOnDailyAt": "2026-01-23T00:09:35.389Z", "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "submittedOnDailyBy": {"_id": "641aa5e391e3376a057bbd4c", "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg", "isPro": false, "fullname": "Chenghao Fan", "user": "Facico", "type": "user"}, "summary": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.", "upvotes": 40, "discussionId": "6972d788fb12c92b735b73a0", "projectPage": "https://bytedance-seed.github.io/Stable-DiffCoder/", "githubRepo": "https://github.com/ByteDance-Seed/Stable-DiffCoder", "githubRepoAddedBy": "user", "ai_summary": "Stable-DiffCoder demonstrates superior code modeling performance compared to autoregressive baselines through block diffusion continual pretraining and efficient training mechanisms.", "ai_keywords": ["diffusion-based language models", "autoregressive models", "block diffusion", "continual pretraining", "warmup", "clipped noise schedule", "supervised fine-tuning", "code modeling", "structured code modeling", "data augmentation"], "githubStars": 16, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u6269\u6563\u57fa\u7840\u8bed\u8a00\u6a21\u578b\uff08DLLMs\uff09\u76f8\u6bd4\u81ea\u56de\u5f52\u6a21\u578b\uff08AR\uff09\u5177\u6709\u66f4\u597d\u7684\u6570\u636e\u91cd\u7528\u548c\u5757\u72b6\u751f\u6210\u80fd\u529b\u3002</li>\n    <li>Stable-DiffCoder\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5757\u6269\u6563\u4ee3\u7801\u6a21\u578b\uff0c\u91c7\u7528Seed-Coder\u67b6\u6784\u548c\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>\u901a\u8fc7\u5f15\u5165\u5757\u6269\u6563\u6301\u7eed\u9884\u8bad\u7ec3\u9636\u6bb5\uff0cStable-DiffCoder\u5728\u591a\u4e2a\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\u3002</li>\n    <li>\u4ec5\u4f9d\u9760\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\uff0cStable-DiffCoder\u7684\u8868\u73b0\u8d85\u8fc7\u4e86\u8bb8\u591a\u5176\u4ed6AR\u548cDLLM\u6a21\u578b\u3002</li>\n    <li>\u6269\u6563\u6a21\u578b\u63d0\u5347\u4e86\u7ed3\u6784\u5316\u4ee3\u7801\u5efa\u6a21\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u652f\u6301\u4f4e\u8d44\u6e90\u7f16\u7a0b\u8bed\u8a00\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion-based language models (DLLMs) generate code in blocks and reuse data better than traditional autoregressive (AR) models.</li>\n    <li>Stable-DiffCoder is a new model that improves performance by using a special training method and architecture from Seed-Coder.</li>\n    <li>Stable-DiffCoder outperforms AR models on many code-related tasks with the same data and setup.</li>\n    <li>The model shows better results than many AR models and other DLLMs, proving that diffusion training enhances code quality.</li>\n    <li>It also helps with structured code tasks and supports less common programming languages through data augmentation.</li>\n</ul>"}, "publishedAt": "2026-01-22T07:13:17.000Z", "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "summary": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15892.png", "numComments": 0, "submittedBy": {"_id": "641aa5e391e3376a057bbd4c", "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg", "fullname": "Chenghao Fan", "name": "Facico", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13, "isUserFollowing": false}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u5728\u89c6\u9891\u95ee\u7b54\u4e2d\uff0c\u89c6\u9891\u53ea\u63d0\u4f9b\u5c40\u90e8\u89c6\u89c9\u7ebf\u7d22\uff0c\u800c\u7b54\u6848\u901a\u5e38\u5728\u7f51\u7edc\u4e0a\u5206\u6563\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u56e2\u961f\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u2014\u2014VideoDR\u3002</li>\n    <li>VideoDR\u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u4e0b\u7684\u5f00\u653e\u57df\u89c6\u9891\u95ee\u7b54\uff0c\u9700\u8981\u63d0\u53d6\u8de8\u5e27\u89c6\u89c9\u951a\u70b9\u3001\u8fdb\u884c\u7f51\u7edc\u68c0\u7d22\u548c\u591a\u8df3\u63a8\u7406\u3002</li>\n    <li>\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u79cd\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u201cAgentic\u201d\u65b9\u6cd5\u5e76\u4e0d\u603b\u662f\u4f18\u4e8e\u201cWorkflow\u201d\u65b9\u6cd5\u3002</li>\n    <li>VideoDR\u4e3a\u7814\u7a76\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u89c6\u9891\u4ee3\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u672a\u6765\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u5173\u952e\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video question answering often needs to gather information from both videos and the internet.</li>\n    <li>The study introduces VideoDR, a new benchmark for video-related question answering that combines visual cues from videos with online data.</li>\n    <li>VideoDR includes high-quality examples across six different areas, thanks to careful human review and annotation.</li>\n    <li>Research tested various models and found that performance varies depending on how well they track video clues during longer retrieval processes.</li>\n    <li>The study highlights important challenges for future video research tools, particularly issues like goal drift and maintaining consistency over time.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u4eba\u7c7b\u5728\u5b66\u4e60\u8bed\u8a00\u4e4b\u524d\u5c31\u53d1\u5c55\u4e86\u57fa\u672c\u7684\u89c6\u89c9\u6280\u80fd\uff0c\u4f46\u73b0\u4ee3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4ecd\u7136\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u6765\u5f25\u8865\u5176\u8106\u5f31\u7684\u89c6\u89c9\u7406\u89e3\u3002</li>\n    <li>\u6211\u4eec\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684MLLMs\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u8fd9\u4e9b\u4efb\u52a1\u4eba\u7c7b\uff08\u751a\u81f33\u5c81\u7684\u5b69\u5b50\uff09\u90fd\u80fd\u8f7b\u677e\u89e3\u51b3\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63a8\u51fa\u4e86BabyVision\uff0c\u4e00\u4e2a\u65e8\u5728\u8bc4\u4f30MLLMs\u6838\u5fc3\u89c6\u89c9\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u6d89\u53ca388\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\u548c4\u4e2a\u4e3b\u8981\u7c7b\u522b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u9886\u5148\u7684MLLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u660e\u663e\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c6\uff0c\u4f8b\u5982Gemini3-Pro-Preview\u7684\u5f97\u5206\u4e3a49.7\uff0c\u8fdc\u4f4e\u4e8e6\u5c81\u513f\u7ae5\u548c94.1\u7684\u6210\u5e74\u5e73\u5747\u5206\u3002</li>\n    <li>BabyVision\u7684\u8fdb\u5c55\u662f\u671d\u7740\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7684\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u8fc8\u51fa\u7684\u4e00\u6b65\uff0c\u5e76\u4e14\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86BabyVision-Gen\u548c\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\u5305\u6765\u89e3\u51b3\u89c6\u89c9\u63a8\u7406\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multimodal LLMs (MLLMs) depend too much on language skills to understand visuals well.</li>\n    <li>State-of-the-art MLLMs struggle with basic visual tasks that even young children can do easily.</li>\n    <li>BabyVision is a new benchmark created to test visual skills of MLLMs without using language knowledge.</li>\n    <li>BabyVision includes 388 tasks across various categories and shows that MLLMs score much lower than humans.</li>\n    <li>These findings highlight that MLLMs need to improve their basic visual understanding to match human abilities.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u5f88\u591a\u4e0e\u4eba\u7c7b\u6d3b\u52a8\u76f8\u5173\u7684\u8bed\u4e49\u5b9e\u4f53\uff0c\u8bc6\u522b\u8fd9\u4e9b\u5b9e\u4f53\u5bf9\u4e8e\u5404\u79cd\u5e94\u7528\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u76ee\u524d\u7684\u5206\u5272\u6a21\u578b\u80fd\u5904\u7406\u7269\u7406\u5c5e\u6027\uff08\u5982\u5efa\u7b51\u3001\u6c34\u4f53\uff09\u7684\u5b9e\u4f53\uff0c\u4f46\u5728\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\uff08\u5982\u5b66\u6821\u3001\u516c\u56ed\uff09\u4e0a\u4ecd\u7136\u5b58\u5728\u6311\u6218\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSocioSeg\u7684\u57ce\u5e02\u793e\u4f1a\u8bed\u4e49\u5206\u5272\u6570\u636e\u96c6\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u6846\u67b6SocioReasoner\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u8bc6\u522b\u548c\u591a\u9636\u6bb5\u63a8\u7406\u6765\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u8d85\u8fc7\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban areas contain many important social entities that can be identified in satellite images.</li>\n    <li>Current models can identify physical features well but struggle with social categories like schools and parks.</li>\n    <li>We created a new dataset called SocioSeg that includes satellite images and detailed labels for social entities.</li>\n    <li>We developed a method called SocioReasoner that mimics how humans identify these social entities using advanced reasoning techniques.</li>\n    <li>Our experiments show that our approach outperforms existing models and can generalize well to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.16725", "authors": [{"_id": "6976d5405d41524304c13537", "name": "Meituan LongCat Team", "hidden": false}, {"_id": "6976d5405d41524304c13538", "name": "Anchun Gui", "hidden": false}, {"_id": "6976d5405d41524304c13539", "name": "Bei Li", "hidden": false}, {"_id": "6976d5405d41524304c1353a", "name": "Bingyang Tao", "hidden": false}, {"_id": "6976d5405d41524304c1353b", "name": "Bole Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1353c", "name": "Borun Chen", "hidden": false}, {"_id": "6976d5405d41524304c1353e", "name": "Chao Zhang", "hidden": false}, {"_id": "69772bc15d41524304c13739", "name": "Chao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1353f", "name": "Chen Gao", "hidden": false}, {"_id": "6976d5405d41524304c13540", "name": "Chen Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13541", "name": "Chengcheng Han", "hidden": false}, {"_id": "6976d5405d41524304c13542", "name": "Chenhui Yang", "hidden": false}, {"_id": "6976d5405d41524304c13543", "name": "Chuyu Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13544", "name": "Cong Chen", "hidden": false}, {"_id": "6976d5405d41524304c13545", "name": "Cunguang Wang", "hidden": false}, {"_id": "6976d5405d41524304c13546", "name": "Daoru Pan", "hidden": false}, {"_id": "6976d5405d41524304c13547", "name": "Defei Bu", "hidden": false}, {"_id": "6976d5405d41524304c13548", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13549", "name": "Di Xiu", "hidden": false}, {"_id": "6976d5405d41524304c1354a", "name": "Dishan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1354b", "name": "Dongyu Ru", "hidden": false}, {"_id": "6976d5405d41524304c1354c", "name": "Dunwei Tu", "hidden": false}, {"_id": "6976d5405d41524304c1354d", "name": "Fan Wu", "hidden": false}, {"_id": "6976d5405d41524304c1354e", "name": "Fengcheng Yuan", "hidden": false}, {"_id": "6976d5405d41524304c1354f", "name": "Fengcun Li", "hidden": false}, {"_id": "6976d5405d41524304c13550", "name": "Gang Xu", "hidden": false}, {"_id": "6976d5405d41524304c13551", "name": "Guanyu Wu", "hidden": false}, {"_id": "6976d5405d41524304c13552", "name": "Guoyuan Lin", "hidden": false}, {"_id": "6976d5405d41524304c13553", "name": "Haibin Wang", "hidden": false}, {"_id": "6976d5405d41524304c13554", "name": "Hansi Yang", "hidden": false}, {"_id": "6976d5405d41524304c13555", "name": "Hao Yang", "hidden": false}, {"_id": "6976d5405d41524304c13556", "name": "Haonan Yan", "hidden": false}, {"_id": "6976d5405d41524304c13557", "name": "Haoxiang Ma", "hidden": false}, {"_id": "6976d5405d41524304c13558", "name": "Haoxing Wen", "hidden": false}, {"_id": "6976d5405d41524304c13559", "name": "Hongyan Hao", "hidden": false}, {"_id": "6976d5405d41524304c1355a", "name": "Hongyin Tang", "hidden": false}, {"_id": "6976d5405d41524304c1355b", "name": "Hongyu Zang", "hidden": false}, {"_id": "6976d5405d41524304c1355c", "name": "Hongzhi Ni", "hidden": false}, {"_id": "6976d5405d41524304c1355d", "name": "Hui Su", "hidden": false}, {"_id": "6976d5405d41524304c1355e", "name": "Jiacheng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1355f", "name": "Jiahong Zhou", "hidden": false}, {"_id": "6976d5405d41524304c13560", "name": "Jiahuan Li", "hidden": false}, {"_id": "6976d5405d41524304c13561", "name": "Jiaming Wang", "hidden": false}, {"_id": "6976d5405d41524304c13562", "name": "Jian Yang", "hidden": false}, {"_id": "6976d5405d41524304c13563", "user": {"_id": "64008a0af4ff62c2616d8858", "avatarUrl": "/avatars/b52c98857916fba5377ace8089d658b2.svg", "isPro": false, "fullname": "zhangjf", "user": "zhangjf", "type": "user"}, "name": "Jianfei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:09.272Z", "hidden": false}, {"_id": "6976d5405d41524304c13564", "name": "Jianhao Xu", "hidden": false}, {"_id": "6976d5405d41524304c13565", "name": "Jianing Wang", "hidden": false}, {"_id": "6976d5405d41524304c13566", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13567", "name": "Jiaqi Sun", "hidden": false}, {"_id": "6976d5405d41524304c13568", "name": "Jiarong Shi", "hidden": false}, {"_id": "6976d5405d41524304c13569", "name": "Jiarui Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1356a", "name": "Jingang Wang", "hidden": false}, {"_id": "6976d5405d41524304c1356b", "user": {"_id": "6592472fccbc1e2cc7250903", "avatarUrl": "/avatars/6f04ae66944eb2ce65c5aca7927bab10.svg", "isPro": false, "fullname": "Jinluan Yang", "user": "Jinluan", "type": "user"}, "name": "Jinluan Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:47.175Z", "hidden": false}, {"_id": "6976d5405d41524304c1356c", "name": "Jinrui Ding", "hidden": false}, {"_id": "6976d5405d41524304c1356d", "name": "Jinwei Xiao", "hidden": false}, {"_id": "6976d5405d41524304c1356e", "name": "Jiyuan He", "hidden": false}, {"_id": "6976d5405d41524304c1356f", "name": "Juncan Xu", "hidden": false}, {"_id": "6976d5405d41524304c13570", "name": "Kefeng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13571", "name": "Keheng Wang", "hidden": false}, {"_id": "6976d5405d41524304c13572", "name": "Li Wei", "hidden": false}, {"_id": "6976d5405d41524304c13573", "name": "Lianhui Ma", "hidden": false}, {"_id": "6976d5405d41524304c13574", "name": "Lin Qiu", "hidden": false}, {"_id": "6976d5405d41524304c13575", "name": "Lingbing Kong", "hidden": false}, {"_id": "6976d5405d41524304c13576", "name": "Lingchuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c13577", "name": "Linsen Guo", "hidden": false}, {"_id": "6976d5405d41524304c13578", "name": "Mengshen Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13579", "name": "Mengxia Shen", "hidden": false}, {"_id": "6976d5405d41524304c1357a", "name": "Mingyang Zhu", "hidden": false}, {"_id": "6976d5405d41524304c1357b", "name": "Peiguang Li", "hidden": false}, {"_id": "6976d5405d41524304c1357c", "name": "Peng Pei", "hidden": false}, {"_id": "6976d5405d41524304c1357d", "name": "Pengcheng Jia", "hidden": false}, {"_id": "6976d5405d41524304c1357e", "name": "Pengtao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1357f", "name": "Peng Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13580", "name": "Qi Gu", "hidden": false}, {"_id": "6976d5405d41524304c13581", "name": "Qiong Huang", "hidden": false}, {"_id": "6976d5405d41524304c13582", "name": "Qiyuan Duan", "hidden": false}, {"_id": "6976d5405d41524304c13583", "name": "Quanchi Weng", "hidden": false}, {"_id": "6976d5405d41524304c13584", "name": "Rongxiang Weng", "hidden": false}, {"_id": "6976d5405d41524304c13585", "name": "Rongzhi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13586", "name": "Rumei Li", "hidden": false}, {"_id": "6976d5405d41524304c13587", "name": "Shanglin Lei", "hidden": false}, {"_id": "6976d5405d41524304c13588", "user": {"_id": "64db5f5dd68a6ddcc7bd89e9", "avatarUrl": "/avatars/69375ec915927b855813df8a6d486837.svg", "isPro": false, "fullname": "Shengnan An", "user": "ShengnanAn", "type": "user"}, "name": "Shengnan An", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:11.410Z", "hidden": false}, {"_id": "6976d5405d41524304c13589", "name": "Shijun Dai", "hidden": false}, {"_id": "6976d5405d41524304c1358a", "name": "Shuaikang Liu", "hidden": false}, {"_id": "6976d5405d41524304c1358b", "name": "Shuang Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1358c", "name": "Shuo Wang", "hidden": false}, {"_id": "6976d5405d41524304c1358d", "name": "Songyuan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1358e", "name": "Tao Liang", "hidden": false}, {"_id": "6976d5405d41524304c1358f", "name": "Tianhao Hu", "hidden": false}, {"_id": "6976d5405d41524304c13590", "name": "Tianze Chen", "hidden": false}, {"_id": "6976d5405d41524304c13591", "name": "Wei Liu", "hidden": false}, {"_id": "6976d5405d41524304c13592", "name": "Wei Shi", "hidden": false}, {"_id": "6976d5405d41524304c13593", "name": "Wei Wang", "hidden": false}, {"_id": "6976d5405d41524304c13594", "name": "Weifeng Tang", "hidden": false}, {"_id": "6976d5405d41524304c13595", "name": "Wenjie Shi", "hidden": false}, {"_id": "6976d5405d41524304c13596", "name": "Wenlong Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13597", "name": "Wentao Chen", "hidden": false}, {"_id": "6976d5405d41524304c13598", "name": "Wentao Shi", "hidden": false}, {"_id": "6976d5405d41524304c13599", "name": "Xi Su", "hidden": false}, {"_id": "6976d5405d41524304c1359a", "name": "Xiangcheng Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359b", "name": "Xiandi Ma", "hidden": false}, {"_id": "6976d5405d41524304c1359c", "user": {"_id": "63edb098679c2cc40abc6c2e", "avatarUrl": "/avatars/288c7229937c2c3f29fda6d17c7df2eb.svg", "isPro": false, "fullname": "Xiangyu", "user": "xixy", "type": "user"}, "name": "Xiangyu Xi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:13.312Z", "hidden": false}, {"_id": "6976d5405d41524304c1359d", "name": "Xiangyuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359e", "name": "Xiangzhou Huang", "hidden": false}, {"_id": "6976d5405d41524304c1359f", "name": "Xiao Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a0", "name": "Xiaodong Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a1", "name": "Xiaolong Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a2", "name": "Xiaowei Shi", "hidden": false}, {"_id": "6976d5405d41524304c135a3", "name": "Xiaoyu Li", "hidden": false}, {"_id": "6976d5405d41524304c135a4", "name": "Xin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a5", "name": "Xingchen Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a6", "name": "Xuan Huang", "hidden": false}, {"_id": "6976d5405d41524304c135a7", "name": "Xuezhi Cao", "hidden": false}, {"_id": "6976d5405d41524304c135a8", "name": "Xunliang Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a9", "name": "Yan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135aa", "user": {"_id": "63fc1c420aab06079200c15c", "avatarUrl": "/avatars/8e8e82a9a6552848581ca9f65011263c.svg", "isPro": false, "fullname": "yang bai", "user": "byang", "type": "user"}, "name": "Yang Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:07.036Z", "hidden": false}, {"_id": "6976d5405d41524304c135ab", "name": "Yang Liu", "hidden": false}, {"_id": "6976d5405d41524304c135ac", "name": "Yang Yang", "hidden": false}, {"_id": "6976d5405d41524304c135ad", "name": "Yang Zheng", "hidden": false}, {"_id": "6976d5405d41524304c135ae", "name": "Yaoming Wang", "hidden": false}, {"_id": "6976d5405d41524304c135af", "name": "Yaoming Zhu", "hidden": false}, {"_id": "6976d5405d41524304c135b0", "name": "Yaqi Huo", "hidden": false}, {"_id": "6976d5405d41524304c135b1", "name": "Yanyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b2", "name": "Yaorui Shi", "hidden": false}, {"_id": "6976d5405d41524304c135b3", "name": "Yerui Sun", "hidden": false}, {"_id": "6976d5405d41524304c135b4", "name": "Yi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b5", "name": "Yihao Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b6", "name": "Yi-Kai Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b7", "name": "Yifan Lu", "hidden": false}, {"_id": "6976d5405d41524304c135b8", "name": "Yifan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135b9", "name": "Yitao Zhai", "hidden": false}, {"_id": "6976d5405d41524304c135ba", "name": "Yongjing Yin", "hidden": false}, {"_id": "6976d5405d41524304c135bb", "name": "Yongwei Zhou", "hidden": false}, {"_id": "6976d5405d41524304c135bc", "name": "Youshao Xiao", "hidden": false}, {"_id": "6976d5405d41524304c135bd", "name": "Yuchuan Dai", "hidden": false}, {"_id": "6976d5405d41524304c135be", "name": "Yuchen Xie", "hidden": false}, {"_id": "6976d5405d41524304c135bf", "name": "Yuchen Yu", "hidden": false}, {"_id": "6976d5405d41524304c135c0", "name": "Yufei Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135c1", "name": "Yuhuai Wei", "hidden": false}, {"_id": "6976d5405d41524304c135c2", "name": "Yulei Qian", "hidden": false}, {"_id": "6976d5405d41524304c135c3", "name": "Yunfan Liang", "hidden": false}, {"_id": "6976d5405d41524304c135c4", "name": "Yunke Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135c5", "name": "Yuwei Jiang", "hidden": false}, {"_id": "6976d5405d41524304c135c6", "name": "Yuxin Bian", "hidden": false}, {"_id": "6976d5405d41524304c135c7", "name": "Yuxin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135c8", "name": "Yuxin Liu", "hidden": false}, {"_id": "6976d5405d41524304c135c9", "name": "Yue Xu", "hidden": false}, {"_id": "6976d5405d41524304c135ca", "name": "Yueqing Sun", "hidden": false}, {"_id": "6976d5405d41524304c135cb", "name": "Zeyang Yu", "hidden": false}, {"_id": "6976d5405d41524304c135cc", "name": "Zhao Yang", "hidden": false}, {"_id": "6976d5405d41524304c135cd", "name": "Zhengsheng Huang", "hidden": false}, {"_id": "6976d5405d41524304c135ce", "name": "Zhengyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135cf", "name": "Zhijian Liu", "hidden": false}, {"_id": "6976d5405d41524304c135d0", "name": "Zhikang Xia", "hidden": false}, {"_id": "6976d5405d41524304c135d1", "name": "Zhimin Lin", "hidden": false}, {"_id": "6976d5405d41524304c135d2", "name": "Zhiyuan Yao", "hidden": false}, {"_id": "6976d5405d41524304c135d3", "name": "Zhuofan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135d4", "name": "Zhuowen Han", "hidden": false}, {"_id": "6976d5405d41524304c135d5", "name": "Zijian Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135d6", "name": "Ziran Li", "hidden": false}, {"_id": "6976d5405d41524304c135d7", "name": "Ziwen Wang", "hidden": false}, {"_id": "6976d5405d41524304c135d8", "name": "Ziyuan Zhuang", "hidden": false}], "publishedAt": "2026-01-23T13:20:09.000Z", "submittedOnDailyAt": "2026-01-26T00:15:28.340Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "upvotes": 136, "discussionId": "6976d5405d41524304c135d9", "ai_summary": "A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for real-world robustness and complex reasoning.", "ai_keywords": ["Mixture-of-Experts", "agentic reasoning", "domain-parallel expert training", "fusion", "asynchronous reinforcement learning", "DORA", "long-tailed generation", "multi-turn interactions", "real-world noise patterns", "test-time scaling", "reasoning depth", "reasoning width", "parallel thinking"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86LongCat-Flash-Thinking-2601\uff0c\u8fd9\u662f\u4e00\u4e2a\u62e5\u67095600\u4ebf\u53c2\u6570\u7684\u5f00\u6e90\u6df7\u5408\u4e13\u5bb6\u63a8\u7406\u6a21\u578b\uff0c\u5177\u6709\u51fa\u8272\u7684\u4ee3\u7406\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5728\u591a\u4e2a\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5305\u62ec\u4ee3\u7406\u641c\u7d22\u548c\u5de5\u5177\u4f7f\u7528\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u901a\u7528\u5316\u80fd\u529b\u3002</li>\n    <li>\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u9886\u57df\u5e76\u884c\u4e13\u5bb6\u8bad\u7ec3\u548c\u540e\u7eed\u878d\u5408\u7684\u7edf\u4e00\u8bad\u7ec3\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u590d\u6742\u5de5\u5177\u4ea4\u4e92\u7684\u8868\u73b0\u3002</li>\n    <li>\u4e3a\u4f18\u5316\u591a\u73af\u5883\u8bad\u7ec3\uff0c\u6269\u5c55\u4e86DORA\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u3002</li>\n    <li>\u5f15\u5165\u91cd\u601d\u8003\u6a21\u5f0f\uff0c\u901a\u8fc7\u589e\u52a0\u63a8\u7406\u7684\u6df1\u5ea6\u548c\u5e7f\u5ea6\uff0c\u589e\u5f3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongCat-Flash-Thinking-2601 is a powerful open-source reasoning model with 560 billion parameters, excelling in complex reasoning tasks.</li>\n    <li>It outperforms other open-source models in various benchmarks, including tool use and agentic search.</li>\n    <li>The model's effectiveness comes from a unique training approach that integrates expert training and advanced data construction methods.</li>\n    <li>It is designed to handle real-world complexities and noise, enhancing its reliability in practical applications.</li>\n    <li>A special feature called Heavy Thinking mode allows the model to improve its reasoning capabilities during testing by increasing its processing depth and breadth.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:20:09.000Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16725.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u5e73\u8861\u7d27\u51d1\u6548\u7387\u548c\u591a\u6a21\u6001\u667a\u80fd\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u4e24\u79cd\u7b56\u7565\u5b9e\u73b0\uff1a\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>\u91c7\u7528\u4e86\u5e76\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u6d4b\u8bd5\u65f6\u7684\u8ba1\u7b97\u6548\u7387\u3002</li>\n    <li>\u5c3d\u7ba1\u6a21\u578b\u4f53\u79ef\u5c0f\uff08\u4ec510B\u53c2\u6570\uff09\uff0c\u4f46\u5176\u6027\u80fd\u53ef\u4ee5\u4e0e10\u500d\u523020\u500d\u66f4\u5927\u7684\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9b\u7ed9\u793e\u533a\u4e00\u4e2a\u5f3a\u5927\u3001\u9ad8\u6548\u4e14\u53ef\u91cd\u590d\u7684\u6a21\u578b\u57fa\u7ebf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a new, lightweight open-source model that balances efficiency with advanced multimodal intelligence.</li>\n    <li>The model uses a unique training approach that combines a language-focused Perception Encoder and a Qwen3-8B decoder with extensive pre-training on a large dataset.</li>\n    <li>It features a post-training process that includes over 1,000 iterations of reinforcement learning to enhance performance.</li>\n    <li>STEP3-VL-10B performs comparably or better than much larger models and top competitors, achieving high scores on various benchmarks.</li>\n    <li>The full model is available to the community, providing a powerful and efficient tool for further research and development.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\uff0c\u5229\u7528\u89c6\u89c9\u7ebf\u7d22\u8fdb\u884c\u5224\u65ad\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u65b9\u6cd5\u5ffd\u89c6\u4e86\u4eba\u7c7b\u5e38\u7528\u7684\u5730\u56fe\u7b56\u7565\u3002</li>\n    <li>\u672c\u7814\u7a76\u4e3a\u6a21\u578b\u589e\u52a0\u4e86\u201c\u601d\u8003\u5730\u56fe\u201d\u7684\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u6784\u5efa\u4e3a\u4e00\u4e2a\u4ee3\u7406-\u5730\u56fe\u5faa\u73af\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u6269\u5c55\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728500\u7c73\u51c6\u786e\u7387\u4e0a\uff0c\u4ece8.0%\u63d0\u5347\u81f322.1%\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>The goal of image geolocalization is to determine where a photo was taken on Earth using visual hints.</li>\n  <li>Current methods use advanced models but often ignore how humans use maps to find locations.</li>\n  <li>This study introduces a \"Thinking with Map\" approach that treats map usage as a loop process.</li>\n  <li>A two-step optimization process is developed, involving reinforcement learning for better decision-making and test-time scaling for exploring different paths.</li>\n  <li>The new method, tested on real-world images, shows significant improvement in accuracy compared to existing models.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.12538", "authors": [{"_id": "6971913fc1c7409747bf9564", "name": "Tianxin Wei", "hidden": false}, {"_id": "6971913fc1c7409747bf9565", "user": {"_id": "6742eb40924e80c3c80ebe13", "avatarUrl": "/avatars/e6ccb1a89a1ea0bfca70779966f4f429.svg", "isPro": false, "fullname": "Ting-Wei Li", "user": "tingwl0122", "type": "user"}, "name": "Ting-Wei Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T17:12:21.531Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9566", "name": "Zhining Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf9567", "name": "Xuying Ning", "hidden": false}, {"_id": "6971913fc1c7409747bf9568", "name": "Ze Yang", "hidden": false}, {"_id": "6971913fc1c7409747bf9569", "name": "Jiaru Zou", "hidden": false}, {"_id": "6971913fc1c7409747bf956a", "name": "Zhichen Zeng", "hidden": false}, {"_id": "6971913fc1c7409747bf956b", "name": "Ruizhong Qiu", "hidden": false}, {"_id": "6971913fc1c7409747bf956c", "name": "Xiao Lin", "hidden": false}, {"_id": "6971913fc1c7409747bf956d", "name": "Dongqi Fu", "hidden": false}, {"_id": "6971913fc1c7409747bf956e", "name": "Zihao Li", "hidden": false}, {"_id": "6971913fc1c7409747bf956f", "user": {"_id": "653962e75c8e4863e1a2068f", "avatarUrl": "/avatars/d4f5f5da141f37d53ca1986ff17b325e.svg", "isPro": false, "fullname": "Mengting Ai", "user": "famous-blue-raincoat", "type": "user"}, "name": "Mengting Ai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:10.378Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9570", "user": {"_id": "677830bd3f2e3ec475576303", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dhwqUDkk66m4oSGSbcd7j.png", "isPro": false, "fullname": "Duo Zhou", "user": "Claudius7", "type": "user"}, "name": "Duo Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:12.476Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9571", "name": "Wenxuan Bao", "hidden": false}, {"_id": "6971913fc1c7409747bf9572", "user": {"_id": "646323556c27a7e33b23f198", "avatarUrl": "/avatars/17fe142f689ab4be3c2374d1d90393db.svg", "isPro": false, "fullname": "Yunzhe Li", "user": "yunzhel2", "type": "user"}, "name": "Yunzhe Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:14.383Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9573", "name": "Gaotang Li", "hidden": false}, {"_id": "6971913fc1c7409747bf9574", "name": "Cheng Qian", "hidden": false}, {"_id": "6971913fc1c7409747bf9575", "name": "Yu Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf9576", "name": "Xiangru Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf9577", "name": "Yin Xiao", "hidden": false}, {"_id": "6971913fc1c7409747bf9578", "name": "Liri Fang", "hidden": false}, {"_id": "6971913fc1c7409747bf9579", "name": "Hui Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf957a", "name": "Xianfeng Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf957b", "name": "Yuji Zhang", "hidden": false}, {"_id": "6971913fc1c7409747bf957c", "name": "Chi Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf957d", "name": "Jiaxuan You", "hidden": false}, {"_id": "6971913fc1c7409747bf957e", "name": "Heng Ji", "hidden": false}, {"_id": "6971913fc1c7409747bf957f", "name": "Hanghang Tong", "hidden": false}, {"_id": "6971913fc1c7409747bf9580", "name": "Jingrui He", "hidden": false}], "publishedAt": "2026-01-18T18:58:23.000Z", "submittedOnDailyAt": "2026-01-22T00:27:25.162Z", "title": "Agentic Reasoning for Large Language Models", "submittedOnDailyBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "isPro": false, "fullname": "Jiaru Zou", "user": "jiaruz2", "type": "user"}, "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "upvotes": 125, "discussionId": "69719140c1c7409747bf9581", "githubRepo": "https://github.com/weitianxin/Awesome-Agentic-Reasoning", "githubRepoAddedBy": "user", "ai_summary": "Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.", "ai_keywords": ["large language models", "agentic reasoning", "autonomous agents", "planning", "tool use", "search", "feedback", "memory", "adaptation", "collaborative settings", "coordination", "knowledge sharing", "reinforcement learning", "supervised fine-tuning", "in-context reasoning", "post-training reasoning", "real-world applications", "benchmarks", "thought and action", "world modeling", "scalable multi-agent training", "governance"], "githubStars": 105, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "summary_zh": "<ul>\n    <li>\u63a8\u7406\u662f\u63a8\u65ad\u3001\u89e3\u51b3\u95ee\u9898\u548c\u51b3\u7b56\u7684\u57fa\u672c\u8ba4\u77e5\u8fc7\u7a0b\u3002</li>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5c01\u95ed\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u5f00\u653e\u548c\u52a8\u6001\u73af\u5883\u4e2d\u56f0\u96be\u91cd\u91cd\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u901a\u8fc7\u4e0d\u65ad\u4e92\u52a8\u8fdb\u884c\u89c4\u5212\u3001\u884c\u52a8\u548c\u5b66\u4e60\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u5206\u4e3a\u4e09\u4e2a\u5c42\u6b21\uff1a\u57fa\u7840\u4ee3\u7406\u63a8\u7406\u3001\u81ea\u6211\u53d1\u5c55\u4ee3\u7406\u63a8\u7406\u548c\u96c6\u4f53\u591a\u4ee3\u7406\u63a8\u7406\u3002</li>\n    <li>\u6587\u732e\u7efc\u8ff0\u8ba8\u8bba\u4e86\u4ee3\u7406\u63a8\u7406\u5728\u79d1\u5b66\u3001\u673a\u5668\u4eba\u3001\u533b\u7597\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6846\u67b6\u548c\u672a\u6765\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reasoning is key for making decisions and solving problems, but large language models (LLMs) have difficulty in changing environments.</li>\n    <li>Agentic reasoning treats LLMs as independent agents that can plan, act, and learn by interacting with their surroundings.</li>\n    <li>This survey categorizes agentic reasoning into three main areas: core capabilities, self-improvement through feedback, and teamwork among multiple agents.</li>\n    <li>It distinguishes between two types of reasoning: in-context reasoning for real-time interactions and post-training reasoning that improves models with learning techniques.</li>\n    <li>The survey also discusses real-world applications, challenges, and future goals for agentic reasoning, such as personalization and better training methods.</li>\n</ul>"}, "publishedAt": "2026-01-18T13:58:23.000Z", "title": "Agentic Reasoning for Large Language Models", "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12538.png", "numComments": 3, "submittedBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "fullname": "Jiaru Zou", "name": "jiaruz2", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.08763", "authors": [{"_id": "6969b0a232f0333869ff946a", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:38.232Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946b", "user": {"_id": "6891c906f3c31445cc040ab1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891c906f3c31445cc040ab1/NBqxXOY7al4CD0XBj8ke2.jpeg", "isPro": false, "fullname": "Yucheng Wang", "user": "DevilEnfant", "type": "user"}, "name": "Yucheng Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:48.080Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946c", "name": "Yufei He", "hidden": false}, {"_id": "6969b0a232f0333869ff946d", "user": {"_id": "682deb444988bd82847e2b03", "avatarUrl": "/avatars/15da087e84386ea72c6fa2db63571420.svg", "isPro": false, "fullname": "Jia-Ying Wu", "user": "EricaWu", "type": "user"}, "name": "Jiaying Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:59.692Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946e", "name": "Yilun Zhao", "hidden": false}, {"_id": "6969b0a232f0333869ff946f", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0a232f0333869ff9470", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:06.327Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9471", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:12.181Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9472", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:17.979Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9473", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:23.007Z", "hidden": false}], "publishedAt": "2026-01-13T17:48:43.000Z", "submittedOnDailyAt": "2026-01-16T01:00:36.686Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "upvotes": 111, "discussionId": "6969b0a232f0333869ff9474", "ai_summary": "Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.", "ai_keywords": ["reinforcement learning", "large language models", "exploration collapse", "pass@k", "pass@1", "rollout-level objective", "high-level solution strategies", "clustering", "policy advantages", "AUC@K"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65f6\u9762\u4e34\u201c\u63a2\u7d22\u5d29\u6e83\u201d\u95ee\u9898\uff0c\u5373\u7b56\u7565\u8fc7\u65e9\u96c6\u4e2d\u4e8e\u5c11\u6570\u4e3b\u5bfc\u63a8\u7406\u6a21\u5f0f\u3002</li>\n    <li>\u8fd9\u79cd\u95ee\u9898\u5bfc\u81f4\u6a21\u578b\u5728\u4e00\u4e9b\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u66f4\u9ad8\u6c34\u5e73\u7684\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u79f0\u4e3a\u201c\u5173\u6ce8\u72ec\u7279\u6027\u7684\u5f3a\u5316\u5b66\u4e60\u201d\uff0c\u65e8\u5728\u5956\u52b1\u5177\u6709\u7a00\u6709\u9ad8\u5c42\u7b56\u7565\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u76f8\u540c\u95ee\u9898\u7684\u4e0d\u540c\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u805a\u7c7b\uff0c\u5ffd\u7565\u8868\u9762\u5dee\u5f02\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u7a00\u6709\u7b56\u7565\u7684\u5956\u52b1\u3002</li>\n    <li>\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u533b\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e0d\u727a\u7272\u521d\u59cb\u8868\u73b0\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6837\u6027\u548c\u6574\u4f53\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) is important for improving large language models, especially for complex reasoning tasks.</li>\n    <li>Current RL methods often get stuck on a few common solutions, which reduces diversity and overall improvement.</li>\n    <li>The proposed method, called Uniqueness-Aware Reinforcement Learning, encourages more varied and rare solutions by rewarding them more.</li>\n    <li>This method uses a judge to group similar solutions and adjust rewards based on how unique the strategies are.</li>\n    <li>Tests show this approach improves overall performance across various subjects while maintaining strong results on common tasks.</li>\n</ul>"}, "publishedAt": "2026-01-13T12:48:43.000Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08763.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 109, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 104, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u662f\u4e00\u79cd\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5168\u7403\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u7684\u7b56\u7565\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u6a21\u5757\u4e3b\u8981\u4f5c\u4e3a\u88ab\u52a8\u5b58\u50a8\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u4e0d\u540c\u4e8b\u5b9e\u4e4b\u95f4\u7684\u9ad8\u9636\u5173\u8054\u3002</li>\n    <li>HGMem\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u8d85\u8d8a\u4e86\u7b80\u5355\u5b58\u50a8\uff0c\u80fd\u591f\u652f\u6301\u590d\u6742\u63a8\u7406\u548c\u5168\u7403\u7406\u89e3\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u56fe\u8fde\u63a5\u4e0d\u540c\u7684\u4e8b\u5b9e\u548c\u601d\u7ef4\uff0c\u5f62\u6210\u66f4\u9ad8\u9636\u7684\u4e92\u52a8\uff0c\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5728\u591a\u4e2a\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u540e\uff0cHGMem\u663e\u8457\u63d0\u5347\u4e86\u591a\u6b65\u9aa4RAG\u7684\u8868\u73b0\uff0c\u4f18\u4e8e\u591a\u4e2a\u5f3a\u57fa\u7ebf\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps large language models understand complex tasks better.</li>\n    <li>Current memory systems in RAG mainly store facts but do not connect them well, which limits reasoning ability.</li>\n    <li>HGMem is a new memory system that uses hypergraphs to create connections between facts for better reasoning.</li>\n    <li>This approach helps form a stronger knowledge structure that supports deeper thinking in problem-solving.</li>\n    <li>Tests show HGMem performs significantly better than existing systems on challenging tasks.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u589e\u5f3a\uff0c\u7528\u6237\u671f\u671b\u5b83\u4eec\u80fd\u63d0\u4f9b\u51c6\u786e\u7684\u56de\u590d\u548c\u7b26\u5408\u591a\u6837\u5316\u4eba\u7c7b\u504f\u597d\u7684\u884c\u4e3a\u3002</li>\n    <li>\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u88ab\u7528\u6765\u6307\u5bfc\u6a21\u578b\u671d\u7740\u8fd9\u4e9b\u671f\u671b\u7684\u884c\u4e3a\u53d1\u5c55\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5904\u7406\u4e0d\u540c\u5956\u52b1\u7684\u7ec4\u5408\u3002</li>\n    <li>\u76f4\u63a5\u5e94\u7528\u73b0\u6709\u7684\u591a\u5956\u52b1\u4f18\u5316\u65b9\u6cd5\uff08GRPO\uff09\u4f1a\u5bfc\u81f4\u5956\u52b1\u7ec4\u5408\u7684\u4f18\u52bf\u503c\u76f8\u540c\uff0c\u5f71\u54cd\u8bad\u7ec3\u6548\u679c\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u65b9\u6cd5\uff08GDPO\uff09\uff0c\u901a\u8fc7\u89e3\u8026\u5956\u52b1\u7684\u5f52\u4e00\u5316\uff0c\u6539\u5584\u591a\u5956\u52b1\u4f18\u5316\u7684\u6548\u679c\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>GDPO\u5728\u4e09\u4e2a\u4efb\u52a1\uff08\u5de5\u5177\u8c03\u7528\u3001\u6570\u5b66\u63a8\u7406\u548c\u7f16\u7a0b\u63a8\u7406\uff09\u4e2d\u8868\u73b0\u4f18\u4e8eGRPO\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Users want language models to give accurate answers and behave according to different human preferences.</li>\n    <li>Reinforcement learning (RL) uses multiple rewards to guide models towards these desired behaviors.</li>\n    <li>Applying the standard method, Group Relative Policy Optimization (GRPO), can lead to problems like reduced training effectiveness.</li>\n    <li>The new method, Group reward-Decoupled Normalization Policy Optimization (GDPO), improves performance by keeping individual rewards separate.</li>\n    <li>GDPO outperforms GRPO in various tasks, showing better accuracy and adherence to constraints.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Jan 27, 2026";
window.trendingPapers = {
    "today": [{"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u7528\u6237\u5e0c\u671b\u5b83\u4eec\u4e0d\u4ec5\u80fd\u63d0\u4f9b\u51c6\u786e\u7684\u56de\u7b54\uff0c\u8fd8\u80fd\u7b26\u5408\u591a\u6837\u7684\u4eba\u7c7b\u504f\u597d\u3002</li>\n    <li>\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5f00\u59cb\u91c7\u7528\u591a\u79cd\u5956\u52b1\u6765\u6307\u5bfc\u6a21\u578b\u884c\u4e3a\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u76f4\u63a5\u5e94\u7528\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u4f1a\u5bfc\u81f4\u5956\u52b1\u7ec4\u5408\u7684\u4f18\u52bf\u503c\u76f8\u540c\uff0c\u4ece\u800c\u964d\u4f4e\u8bad\u7ec3\u4fe1\u53f7\u7684\u5206\u8fa8\u7387\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff1a\u7fa4\u4f53\u5956\u52b1\u89e3\u8026\u5f52\u4e00\u5316\u7b56\u7565\u4f18\u5316\uff08GDPO\uff09\uff0c\u89e3\u51b3\u4e86GRPO\u7684\u95ee\u9898\uff0c\u4fdd\u6301\u4e86\u5956\u52b1\u95f4\u7684\u76f8\u5bf9\u5dee\u5f02\u3002</li>\n    <li>GDPO\u5728\u4e09\u4e2a\u4efb\u52a1\uff08\u5de5\u5177\u8c03\u7528\u3001\u6570\u5b66\u63a8\u7406\u548c\u7f16\u7801\u63a8\u7406\uff09\u4e2d\u8868\u73b0\u4f18\u4e8eGRPO\uff0c\u663e\u793a\u51fa\u5176\u5728\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Users expect language models to give accurate answers and behave according to different human preferences.</li>\n    <li>Reinforcement learning (RL) is using multiple rewards to guide models to meet these preferences.</li>\n    <li>Applying Group Relative Policy Optimization (GRPO) to these rewards can lead to problems, making training less effective.</li>\n    <li>The new method, Group reward-Decoupled Normalization Policy Optimization (GDPO), improves on GRPO by keeping the rewards separate for better training.</li>\n    <li>GDPO has shown better performance in tasks like tool calling, math reasoning, and coding reasoning compared to GRPO.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.04890", "authors": [{"_id": "69608e7c5b7998385e639583", "user": {"_id": "64670db15993aa7666cc6022", "avatarUrl": "/avatars/b68caad7e987c095b0cab4d9035aac25.svg", "isPro": false, "fullname": "Maksim Velikanov", "user": "yellowvm", "type": "user"}, "name": "Maksim Velikanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:44.975Z", "hidden": false}, {"_id": "69608e7c5b7998385e639584", "user": {"_id": "6697a9fb6d173ec7382e0392", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697a9fb6d173ec7382e0392/Q_myrIBbdWI3RmEvEHrtQ.jpeg", "isPro": false, "fullname": "Ilyas Chahed", "user": "IChahed", "type": "user"}, "name": "Ilyas Chahed", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T15:46:04.314Z", "hidden": false}, {"_id": "69608e7c5b7998385e639585", "user": {"_id": "6460c3811db65f878513bcaf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg", "isPro": false, "fullname": "Jingwei Zuo", "user": "JingweiZuo", "type": "user"}, "name": "Jingwei Zuo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:34:49.874Z", "hidden": false}, {"_id": "69608e7c5b7998385e639586", "name": "Dhia Eddine Rhaiem", "hidden": false}, {"_id": "69608e7c5b7998385e639587", "user": {"_id": "62441d1d9fdefb55a0b7d12c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png", "isPro": false, "fullname": "Younes B", "user": "ybelkada", "type": "user"}, "name": "Younes Belkada", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:34:47.914Z", "hidden": false}, {"_id": "69608e7c5b7998385e639588", "user": {"_id": "6471d727a2b0a376b8b6a4ed", "avatarUrl": "/avatars/aedda547f2ca40dfa898e76be787952f.svg", "isPro": false, "fullname": "Hakim Hacid", "user": "HakimHacid", "type": "user"}, "name": "Hakim Hacid", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:56.651Z", "hidden": false}], "publishedAt": "2026-01-08T12:41:49.000Z", "submittedOnDailyAt": "2026-01-09T02:55:50.938Z", "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers", "submittedOnDailyBy": {"_id": "6460c3811db65f878513bcaf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg", "isPro": false, "fullname": "Jingwei Zuo", "user": "JingweiZuo", "type": "user"}, "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.", "upvotes": 28, "discussionId": "69608e7c5b7998385e639589", "projectPage": "https://tiiuae.github.io/Falcon-H1/", "githubRepo": "https://github.com/tiiuae/falcon-h1", "githubRepoAddedBy": "user", "ai_summary": "Learnable multipliers are introduced to address weight decay-induced normalization artifacts in large language model training, outperforming traditional methods while reducing computational overhead.", "ai_keywords": ["weight decay", "stochastic gradient noise", "Brownian-like expansion", "WD-noise equilibrium", "learnable multipliers", "matrix layers", "weight norm", "muP multipliers", "Adam optimizer", "Muon optimizer"], "githubStars": 98, "organization": {"_id": "6448cad23adf50d86406b0a3", "name": "tiiuae", "fullname": "Technology Innovation Institute", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"}, "summary_zh": "<ul>\n    <li>\u5728\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u4e2d\uff0c\u5e94\u7528\u6743\u91cd\u8870\u51cf\uff08WD\uff09\u662f\u5e38\u89c4\u505a\u6cd5\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u968f\u673a\u68af\u5ea6\u566a\u58f0\u5bfc\u81f4\u6743\u91cd\u77e9\u9635\u7684\u589e\u957f\uff0c\u800c\u6743\u91cd\u8870\u51cf\u53ef\u4ee5\u62b5\u6d88\u8fd9\u79cd\u589e\u957f\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u4e58\u5b50\u6765\u4f18\u5316\u6743\u91cd\u7684\u5c3a\u5ea6\uff0c\u4ece\u800c\u6539\u5584\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5e73\u8861\u72b6\u6001\u3002</li>\n    <li>\u5c06\u53ef\u5b66\u4e60\u4e58\u5b50\u5e94\u7528\u4e8e\u6bcf\u4e00\u884c\u548c\u6bcf\u4e00\u5217\uff0c\u80fd\u591f\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u5e76\u7b80\u5316\u8c03\u6574\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4f18\u5316\u5668\u4e0a\u5747\u8868\u73b0\u826f\u597d\uff0c\u63d0\u5347\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Weight decay (WD) is commonly used in training large language models to control the growth of weight matrices.</li>\n    <li>This study introduces learnable multipliers to improve the scaling of weight matrices, addressing the limitations of the WD-noise equilibrium norm.</li>\n    <li>By using a scalar multiplier for the entire weight matrix and separate multipliers for each row and column, the model adapts better to data and enhances performance.</li>\n    <li>The proposed method is an advanced version of existing techniques, outperforming standard approaches while reducing the need for complex tuning.</li>\n    <li>Tests show that using learnable multipliers with different optimizers improves the model's performance significantly.</li>\n</ul>"}, "publishedAt": "2026-01-08T07:41:49.000Z", "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers", "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04890.png", "numComments": 1, "submittedBy": {"_id": "6460c3811db65f878513bcaf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg", "fullname": "Jingwei Zuo", "name": "JingweiZuo", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 37, "isUserFollowing": false}, "organization": {"_id": "6448cad23adf50d86406b0a3", "name": "tiiuae", "fullname": "Technology Innovation Institute", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05249", "authors": [{"_id": "6960a8ce5b7998385e639615", "user": {"_id": "676ce504027822ead2b5f193", "avatarUrl": "/avatars/91de797fc4a971a481b2dce82b579f66.svg", "isPro": false, "fullname": "YuanKangNeilLee", "user": "NeilLeeNTU", "type": "user"}, "name": "Yuan-Kang Lee", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:24:03.428Z", "hidden": false}, {"_id": "6960a8ce5b7998385e639616", "name": "Kuan-Lin Chen", "hidden": false}, {"_id": "6960a8ce5b7998385e639617", "name": "Chia-Che Chang", "hidden": false}, {"_id": "6960a8ce5b7998385e639618", "user": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "isPro": false, "fullname": "Yu-Lun Liu", "user": "yulunliu", "type": "user"}, "name": "Yu-Lun Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:50:20.802Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"], "publishedAt": "2026-01-08T18:59:55.000Z", "submittedOnDailyAt": "2026-01-09T04:35:57.571Z", "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes", "submittedOnDailyBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "isPro": false, "fullname": "Yu-Lun Liu", "user": "yulunliu", "type": "user"}, "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/", "upvotes": 24, "discussionId": "6960a8ce5b7998385e639619", "projectPage": "https://ntuneillee.github.io/research/rl-awb/", "githubRepo": "https://github.com/BrianChen1120/RL-AWB", "githubRepoAddedBy": "user", "ai_summary": "A novel nighttime color constancy framework combines statistical methods with deep reinforcement learning to improve white balance adjustment under low-light conditions.", "ai_keywords": ["deep reinforcement learning", "color constancy", "white balance", "statistical algorithms", "illumination estimation", "multi-sensor dataset"], "githubStars": 12, "summary_zh": "<ul>\n    <li>\u591c\u95f4\u8272\u5f69\u6052\u5b9a\u6027\u5728\u8ba1\u7b97\u6444\u5f71\u4e2d\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u4e3b\u8981\u56e0\u4e3a\u4f4e\u5149\u566a\u58f0\u548c\u590d\u6742\u7684\u7167\u660e\u6761\u4ef6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86RL-AWB\uff0c\u4e00\u4e2a\u7ed3\u5408\u7edf\u8ba1\u65b9\u6cd5\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u591c\u95f4\u767d\u5e73\u8861\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u9996\u5148\u4f7f\u7528\u4e3a\u591c\u95f4\u573a\u666f\u5b9a\u5236\u7684\u7edf\u8ba1\u7b97\u6cd5\uff0c\u7ed3\u5408\u663e\u8457\u7070\u8272\u50cf\u7d20\u68c0\u6d4b\u548c\u65b0\u578b\u7167\u660e\u4f30\u8ba1\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u7b2c\u4e00\u4e2a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u6a21\u62df\u4e13\u4e1a\u767d\u5e73\u8861\u8c03\u8282\u4e13\u5bb6\uff0c\u52a8\u6001\u4f18\u5316\u6bcf\u5f20\u56fe\u50cf\u7684\u53c2\u6570\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86\u7b2c\u4e00\u4e2a\u591a\u4f20\u611f\u5668\u591c\u95f4\u6570\u636e\u96c6\uff0c\u4ee5\u4fbf\u8fdb\u884c\u8de8\u4f20\u611f\u5668\u8bc4\u4f30\uff0c\u5e76\u4e14\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4f4e\u5149\u548c\u826f\u597d\u7167\u660e\u56fe\u50cf\u4e2d\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Nighttime color consistency in photos is difficult due to low light and complex lighting conditions.</li>\n    <li>We developed RL-AWB, a new system that combines statistics and deep learning for better nighttime white balance.</li>\n    <li>This system uses a statistical algorithm for nighttime scenes and improves it with deep reinforcement learning.</li>\n    <li>We created a new dataset for testing our method across different camera sensors.</li>\n    <li>Our results show that RL-AWB performs better in various lighting conditions compared to other methods.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:55.000Z", "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes", "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05249.png", "numComments": 1, "submittedBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "fullname": "Yu-Lun Liu", "name": "yulunliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05106", "authors": [{"_id": "696073d35b7998385e6394c4", "name": "Nuoya Xiong", "hidden": false}, {"_id": "696073d35b7998385e6394c5", "user": {"_id": "64887eb15cf73a16e767b56a", "avatarUrl": "/avatars/ada2b6a07346b1d61322ddd04d219318.svg", "isPro": false, "fullname": "Yuhang Zhou", "user": "zyhang1998", "type": "user"}, "name": "Yuhang Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T16:50:49.484Z", "hidden": false}, {"_id": "696073d35b7998385e6394c6", "name": "Hanqing Zeng", "hidden": false}, {"_id": "696073d35b7998385e6394c7", "name": "Zhaorun Chen", "hidden": false}, {"_id": "696073d35b7998385e6394c8", "name": "Furong Huang", "hidden": false}, {"_id": "696073d35b7998385e6394c9", "name": "Shuchao Bi", "hidden": false}, {"_id": "696073d35b7998385e6394ca", "name": "Lizhu Zhang", "hidden": false}, {"_id": "696073d35b7998385e6394cb", "name": "Zhuokai Zhao", "hidden": false}], "publishedAt": "2026-01-08T16:53:16.000Z", "submittedOnDailyAt": "2026-01-09T00:49:52.202Z", "title": "Token-Level LLM Collaboration via FusionRoute", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.", "upvotes": 23, "discussionId": "696073d45b7998385e6394cc", "ai_summary": "FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select optimal experts and add complementary logits, outperforming existing methods in diverse tasks while maintaining efficiency.", "ai_keywords": ["large language models", "token-level collaboration", "multi-LLM collaboration", "lightweight router", "expert selection", "logit addition", "complementary generator", "optimal decoding policy", "model merging", "direct fine-tuning"], "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8bad\u7ec3\u548c\u90e8\u7f72\u6210\u672c\u9ad8\u3002</li>\n    <li>\u5c0f\u578b\u4e13\u4e1a\u6a21\u578b\u6548\u7387\u66f4\u9ad8\uff0c\u4f46\u96be\u4ee5\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u8fdb\u884c\u63a8\u5e7f\u3002</li>\n    <li>\u63d0\u51faFusionRoute\u6846\u67b6\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u9009\u62e9\u6700\u5408\u9002\u7684\u4e13\u5bb6\uff0c\u5e76\u6539\u8fdb\u4e13\u5bb6\u7684\u8f93\u51fa\u3002</li>\n    <li>\u4e0e\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0cFusionRoute\u901a\u8fc7\u53ef\u8bad\u7ec3\u751f\u6210\u5668\u6269\u5927\u4e86\u6709\u6548\u7b56\u7565\u7c7b\uff0c\u63d0\u9ad8\u89e3\u7801\u6027\u80fd\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFusionRoute\u7684\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u534f\u4f5c\u65b9\u6cd5\u548c\u76f4\u63a5\u5fae\u8c03\uff0c\u4e14\u4e0e\u9886\u57df\u4e13\u5bb6\u7ade\u4e89\u529b\u5f3a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) work well in many areas but are costly to train and use in their full size.</li>\n    <li>Smaller, specialized models are efficient but can't easily adapt to new tasks outside their training.</li>\n    <li>FusionRoute is a new method that uses a lightweight router to choose the best expert model for each task step and to improve its output.</li>\n    <li>This approach combines expert selection with a flexible generator to enhance performance and achieve better results.</li>\n    <li>Tests show that FusionRoute performs better than other collaboration methods and is competitive with specialized models in various tasks.</li>\n</ul>"}, "publishedAt": "2026-01-08T11:53:16.000Z", "title": "Token-Level LLM Collaboration via FusionRoute", "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05106.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 205, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.05241", "authors": [{"_id": "6960775a5b7998385e6394ff", "user": {"_id": "64ed876a74d9b58eabc769a4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg", "isPro": true, "fullname": "Boyang Wang", "user": "HikariDawn", "type": "user"}, "name": "Boyang Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:11.481Z", "hidden": false}, {"_id": "6960775a5b7998385e639500", "name": "Haoran Zhang", "hidden": false}, {"_id": "6960775a5b7998385e639501", "name": "Shujie Zhang", "hidden": false}, {"_id": "6960775a5b7998385e639502", "user": {"_id": "64edb581067fbb625f893628", "avatarUrl": "/avatars/d59893d6f1f752bb73255bd78b325fe9.svg", "isPro": false, "fullname": "hao", "user": "wuzhi-hao", "type": "user"}, "name": "Jinkun Hao", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:09.450Z", "hidden": false}, {"_id": "6960775a5b7998385e639503", "name": "Mingda Jia", "hidden": false}, {"_id": "6960775a5b7998385e639504", "name": "Qi Lv", "hidden": false}, {"_id": "6960775a5b7998385e639505", "user": {"_id": "65de9c6cf68c3d3bac330509", "avatarUrl": "/avatars/150858545ea1a9e7c96d6f227093ac54.svg", "isPro": false, "fullname": "Yucheng Mao", "user": "matthewmao", "type": "user"}, "name": "Yucheng Mao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:52:47.293Z", "hidden": false}, {"_id": "6960775a5b7998385e639506", "user": {"_id": "63f2ec797ddf724fbcc75aee", "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg", "isPro": false, "fullname": "Zhaoyang Lyu", "user": "ZhaoyangLyu", "type": "user"}, "name": "Zhaoyang Lyu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:52:53.025Z", "hidden": false}, {"_id": "6960775a5b7998385e639507", "user": {"_id": "685d08b9fc7a0ff2f338dbd0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/685d08b9fc7a0ff2f338dbd0/bW3G1ZM-tAXIIbCbUdwC9.png", "isPro": false, "fullname": "Jia Zeng", "user": "Jia-Zeng", "type": "user"}, "name": "Jia Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T15:46:06.251Z", "hidden": false}, {"_id": "6960775a5b7998385e639508", "name": "Xudong Xu", "hidden": false}, {"_id": "6960775a5b7998385e639509", "user": {"_id": "65783ee6ee33d547aecc3ffc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg", "isPro": false, "fullname": "Jiangmiao Pang", "user": "Jiangmiao", "type": "user"}, "name": "Jiangmiao Pang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:52:00.637Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:22.000Z", "submittedOnDailyAt": "2026-01-09T02:54:13.651Z", "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation", "submittedOnDailyBy": {"_id": "64ed876a74d9b58eabc769a4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg", "isPro": true, "fullname": "Boyang Wang", "user": "HikariDawn", "type": "user"}, "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.", "upvotes": 19, "discussionId": "6960775a5b7998385e63950a", "projectPage": "https://robovip.github.io/RoboVIP/", "githubRepo": "https://github.com/RoboVIP/RoboVIP_VDM", "githubRepoAddedBy": "user", "ai_summary": "Visual identity prompting enhances manipulation data augmentation for robot policies by providing explicit visual guidance to diffusion models, improving policy performance in both simulation and real-world settings.", "ai_keywords": ["image diffusion models", "visual identity prompting", "manipulation data", "vision-language-action models", "visuomotor policy models", "visual identity pool"], "githubStars": 7, "summary_zh": "<ul>\n    <li>\u64cd\u7eb5\u6570\u636e\u7684\u591a\u6837\u6027\u3001\u6570\u91cf\u548c\u8d28\u91cf\u5bf9\u8bad\u7ec3\u6709\u6548\u7684\u673a\u5668\u4eba\u7b56\u7565\u975e\u5e38\u5173\u952e\u3002</li>\n    <li>\u7531\u4e8e\u786c\u4ef6\u548c\u7269\u7406\u73af\u5883\u7684\u9650\u5236\uff0c\u6536\u96c6\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u7684\u64cd\u7eb5\u6570\u636e\u5f88\u56f0\u96be\u3002</li>\n    <li>\u6700\u8fd1\u7684\u7814\u7a76\u5229\u7528\u6587\u672c\u63d0\u793a\u7684\u56fe\u50cf\u6269\u6563\u6a21\u578b\u6765\u589e\u5f3a\u64cd\u7eb5\u6570\u636e\uff0c\u4f46\u5e38\u5e38\u5ffd\u89c6\u4e86\u9700\u8981\u591a\u89c6\u89d2\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u89c2\u5bdf\u3002</li>\n    <li>\u6587\u672c\u63d0\u793a\u65e0\u6cd5\u53ef\u9760\u5730\u6307\u5b9a\u573a\u666f\u8bbe\u7f6e\uff0c\u56e0\u6b64\u6211\u4eec\u5f15\u5165\u4e86\u89c6\u89c9\u8eab\u4efd\u63d0\u793a\uff0c\u4ee5\u63d0\u4f9b\u793a\u4f8b\u56fe\u50cf\u6765\u6307\u5bfc\u751f\u6210\u6240\u9700\u7684\u573a\u666f\u3002</li>\n    <li>\u901a\u8fc7\u589e\u5f3a\u7684\u6570\u636e\u8bad\u7ec3\u673a\u5668\u4eba\u6a21\u578b\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u73af\u5883\u4e2d\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Collecting real-world manipulation data for training robots is challenging due to hardware limitations.</li>\n    <li>Recent methods improve manipulation data by changing backgrounds and objects in images, but they often miss important details like multiple viewpoints and time consistency.</li>\n    <li>Text prompts used in these methods do not always clearly define the scene setup needed for effective training.</li>\n    <li>To address this, a new method called visual identity prompting uses example images to help create better scene setups.</li>\n    <li>Using this improved data leads to better performance for robot training in both simulation and real-world tasks.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:22.000Z", "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation", "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05241.png", "numComments": 2, "submittedBy": {"_id": "64ed876a74d9b58eabc769a4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg", "fullname": "Boyang Wang", "name": "HikariDawn", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05167", "authors": [{"_id": "69606c325b7998385e639481", "user": {"_id": "62ea79dd01ed9b0e8f61ccd3", "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg", "isPro": false, "fullname": "Chengsong Huang", "user": "ChengsongHuang", "type": "user"}, "name": "Chengsong Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:21.851Z", "hidden": false}, {"_id": "69606c325b7998385e639482", "name": "Tong Zheng", "hidden": false}, {"_id": "69606c325b7998385e639483", "user": {"_id": "65e02d89574e5aa0e9ce3efa", "avatarUrl": "/avatars/2ab152a10b21d81fb1defc726b8e951a.svg", "isPro": false, "fullname": "Langlin Huang", "user": "shrango", "type": "user"}, "name": "Langlin Huang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:53:24.796Z", "hidden": false}, {"_id": "69606c325b7998385e639484", "name": "Jinyuan Li", "hidden": false}, {"_id": "69606c325b7998385e639485", "name": "Haolin Liu", "hidden": false}, {"_id": "69606c325b7998385e639486", "name": "Jiaxin Huang", "hidden": false}], "publishedAt": "2026-01-08T17:56:16.000Z", "submittedOnDailyAt": "2026-01-09T00:17:45.320Z", "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding", "submittedOnDailyBy": {"_id": "62ea79dd01ed9b0e8f61ccd3", "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg", "isPro": false, "fullname": "Chengsong Huang", "user": "ChengsongHuang", "type": "user"}, "summary": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.", "upvotes": 19, "discussionId": "69606c325b7998385e639487", "githubRepo": "https://github.com/Chengsong-Huang/RelayLLM", "githubRepoAddedBy": "user", "ai_summary": "RelayLLM enables efficient collaborative reasoning between small and large language models through token-level dynamic invocation, achieving high accuracy with minimal computational overhead.", "ai_keywords": ["Large Language Models", "Small Language Models", "collaborative decoding", "token-level collaboration", "Group Relative Policy Optimization", "policy optimization", "dynamic invocation", "computational efficiency", "reasoning capacity"], "githubStars": 6, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u590d\u6742\u63a8\u7406\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5ef6\u8fdf\u5927\uff0c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u5219\u7f3a\u4e4f\u8db3\u591f\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u73b0\u6709\u7684\u534f\u4f5c\u65b9\u6cd5\u901a\u5e38\u5c06\u6574\u4e2a\u67e5\u8be2\u4ea4\u7ed9LLM\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86RelayLLM\u6846\u67b6\uff0c\u901a\u8fc7\u4ee4\u724c\u7ea7\u7684\u534f\u540c\u89e3\u7801\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002</li>\n    <li>RelayLLM\u8ba9SLM\u4f5c\u4e3a\u4e3b\u52a8\u63a7\u5236\u8005\uff0c\u4ec5\u5728\u5173\u952e\u4ee4\u724c\u65f6\u8c03\u7528LLM\uff0c\u4ece\u800c\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRelayLLM\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u5e73\u574749.52%\u7684\u51c6\u786e\u7387\uff0c\u4e14\u4ec5\u8c03\u7528LLM\u751f\u62101.07%\u7684\u4ee4\u724c\uff0c\u6210\u672c\u51cf\u5c1198.2%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are powerful but expensive and slow for complex reasoning tasks.</li>\n    <li>Small Language Models (SLMs) are more resource-efficient but often struggle with reasoning.</li>\n    <li>The proposed RelayLLM framework allows SLMs to control when to use LLMs for important parts of a task, reducing waste.</li>\n    <li>RelayLLM uses a special training method to help the model know when to seek help from the LLM.</li>\n    <li>Tests show RelayLLM achieves good accuracy while only using the LLM for a tiny fraction of the work, saving costs significantly.</li>\n</ul>"}, "publishedAt": "2026-01-08T12:56:16.000Z", "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding", "summary": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05167.png", "numComments": 1, "submittedBy": {"_id": "62ea79dd01ed9b0e8f61ccd3", "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg", "fullname": "Chengsong Huang", "name": "ChengsongHuang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.04767", "authors": [{"_id": "69609fe35b7998385e6395ed", "user": {"_id": "64feba7efa64465422ce3003", "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg", "isPro": false, "fullname": "zongzefang", "user": "zzfoutofspace", "type": "user"}, "name": "Zefang Zong", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:54:07.260Z", "hidden": false}, {"_id": "69609fe35b7998385e6395ee", "user": {"_id": "6462271493f702673bf99c0b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462271493f702673bf99c0b/PyWyI2uoJGr0kpugGGr0t.jpeg", "isPro": false, "fullname": "Dingwei Chen", "user": "CuSO4-Chen", "type": "user"}, "name": "Dingwei Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:54:13.098Z", "hidden": false}, {"_id": "69609fe35b7998385e6395ef", "name": "Yang Li", "hidden": false}, {"_id": "69609fe35b7998385e6395f0", "name": "Qi Yi", "hidden": false}, {"_id": "69609fe35b7998385e6395f1", "name": "Bo Zhou", "hidden": false}, {"_id": "69609fe35b7998385e6395f2", "user": {"_id": "65d5f457d032b44853ae79e4", "avatarUrl": "/avatars/362180aff317ecee27513741c18fd98c.svg", "isPro": false, "fullname": "chengming li", "user": "daming8000", "type": "user"}, "name": "Chengming Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:54:28.358Z", "hidden": false}, {"_id": "69609fe35b7998385e6395f3", "name": "Bo Qian", "hidden": false}, {"_id": "69609fe35b7998385e6395f4", "user": {"_id": "60799b15921db717010c7c8e", "avatarUrl": "/avatars/5e33095aa5343538f09831c1ed2230d2.svg", "isPro": false, "fullname": "Peng Chen", "user": "pengchen", "type": "user"}, "name": "Peng Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:54:22.694Z", "hidden": false}, {"_id": "69609fe35b7998385e6395f5", "name": "Jie Jiang", "hidden": false}], "publishedAt": "2026-01-08T09:35:49.000Z", "submittedOnDailyAt": "2026-01-09T04:20:12.513Z", "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search", "submittedOnDailyBy": {"_id": "64feba7efa64465422ce3003", "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg", "isPro": false, "fullname": "zongzefang", "user": "zzfoutofspace", "type": "user"}, "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.", "upvotes": 18, "discussionId": "69609fe35b7998385e6395f6", "githubRepo": "https://github.com/zzfoutofspace/ATPO", "githubRepoAddedBy": "user", "ai_summary": "AT\u00b2PO is a unified framework for multi-turn agentic reinforcement learning that improves exploration diversity, credit assignment, and policy optimization through tree search and turn-level learning objectives.", "ai_keywords": ["Agentic Reinforcement Learning", "tree search", "Entropy-Guided Tree Expansion", "Turn-wise Credit Assignment", "Agentic Turn-based Policy Optimization", "multi-turn tasks", "policy optimization", "reward propagation", "turn-level learning objective"], "githubStars": 2, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>LLM\u4ee3\u7406\u53ef\u4ee5\u901a\u8fc7\u5185\u90e8\u63a8\u7406\u548c\u5916\u90e8\u5de5\u5177\u4ea4\u4e92\u6765\u5904\u7406\u591a\u56de\u5408\u4efb\u52a1\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86AT^2PO\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u591a\u56de\u5408\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\u3002</li>\n    <li>AT^2PO\u5f15\u5165\u4e86\u4e00\u79cd\u56de\u5408\u7ea7\u6811\u7ed3\u6784\uff0c\u652f\u6301\u6218\u7565\u63a2\u7d22\u548c\u7ec6\u81f4\u7684\u5956\u52b1\u4f20\u64ad\u3002</li>\n    <li>\u5b9e\u9a8c\u663e\u793a\uff0cAT^2PO\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u73b0\u6709\u6700\u4f73\u65b9\u6848\u5e73\u5747\u63d0\u9ad8\u4e861.84\u4e2a\u767e\u5206\u70b9\u3002</li>\n    <li>\u4ee3\u7801\u5df2\u516c\u5f00\uff0c\u65b9\u4fbf\u5176\u4ed6\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u548c\u53c2\u8003\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LLM agents are useful for handling complex tasks by combining reasoning and tool use.</li>\n    <li>This paper introduces AT^2PO, a new framework for improving multi-turn agentic reinforcement learning (RL).</li>\n    <li>AT^2PO addresses challenges like limited exploration and difficulty in assigning rewards effectively.</li>\n    <li>It uses a tree structure for better exploration strategies and fine-tuned reward distribution.</li>\n    <li>Tests show AT^2PO improves performance by up to 1.84 percentage points compared to existing methods.</li>\n</ul>"}, "publishedAt": "2026-01-08T04:35:49.000Z", "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search", "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04767.png", "numComments": 1, "submittedBy": {"_id": "64feba7efa64465422ce3003", "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg", "fullname": "zongzefang", "name": "zzfoutofspace", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.21815", "authors": [{"_id": "6960e5825b7998385e6396da", "name": "Mengqi He", "hidden": false}, {"_id": "6960e5825b7998385e6396db", "name": "Xinyu Tian", "hidden": false}, {"_id": "6960e5825b7998385e6396dc", "name": "Xin Shen", "hidden": false}, {"_id": "6960e5825b7998385e6396dd", "user": {"_id": "64c71a5647418a0a59e5c7cb", "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg", "isPro": false, "fullname": "Jinhong Ni", "user": "mcleanie", "type": "user"}, "name": "Jinhong Ni", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:57:14.555Z", "hidden": false}, {"_id": "6960e5825b7998385e6396de", "name": "Shu Zou", "hidden": false}, {"_id": "6960e5825b7998385e6396df", "user": {"_id": "6364187d969bdae89e12b209", "avatarUrl": "/avatars/290793f85e8dc2ae0cd4fe1becb4b1d0.svg", "isPro": false, "fullname": "zhaoyuan yang", "user": "zhaoyuan", "type": "user"}, "name": "Zhaoyuan Yang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:57:08.195Z", "hidden": false}, {"_id": "6960e5825b7998385e6396e0", "name": "Jing Zhang", "hidden": false}], "publishedAt": "2025-12-26T01:01:25.000Z", "submittedOnDailyAt": "2026-01-09T09:01:04.074Z", "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models", "submittedOnDailyBy": {"_id": "68a3f14e6dd0e4c74c014853", "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg", "isPro": true, "fullname": "Hubert", "user": "ANUHW", "type": "user"}, "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.", "upvotes": 16, "discussionId": "6960e5835b7998385e6396e1", "ai_summary": "Selective adversarial attacks targeting high-entropy tokens in vision-language models achieve significant semantic degradation with reduced budgets and demonstrate transferable vulnerabilities across different architectures.", "ai_keywords": ["vision-language models", "entropy", "adversarial attacks", "autoregressive generation", "high-entropy tokens", "semantic degradation", "attack success rates", "transferability"], "organization": {"_id": "64ed4ba2453a4b4bef2664c5", "name": "anu-cvml", "fullname": "Australian National University"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u6613\u53d7\u653b\u51fb\u3002</li>\n    <li>\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u4e0e\u53ef\u9760\u6027\u76f8\u5173\uff0c\u9ad8\u71b5\u6807\u8bb0\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002</li>\n    <li>\u653b\u51fb\u53ef\u4ee5\u96c6\u4e2d\u5728\u5c11\u6570(\u7ea620%)\u9ad8\u71b5\u6807\u8bb0\u4e0a\uff0c\u80fd\u6709\u6548\u964d\u4f4e\u6a21\u578b\u8f93\u51fa\u7684\u8d28\u91cf\u3002</li>\n    <li>\u8fd9\u79cd\u9009\u62e9\u6027\u653b\u51fb\u80fd\u5c0635-49%\u7684\u6b63\u5e38\u8f93\u51fa\u8f6c\u53d8\u4e3a\u6709\u5bb3\u8f93\u51fa\uff0c\u663e\u793a\u51fa\u66f4\u5927\u7684\u5b89\u5168\u98ce\u9669\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u653b\u51fb\u65b9\u6cd5\uff08EGA\uff09\uff0c\u5728\u653b\u51fb\u6210\u529f\u7387\u548c\u6709\u5bb3\u8f6c\u6362\u7387\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5b89\u5168\u5f31\u70b9\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-language models (VLMs) are effective but can be easily attacked.</li>\n    <li>Entropy, which measures uncertainty in a model, is linked to how reliable VLMs are.</li>\n    <li>Previous attacks assumed all parts of the model contribute equally to instability, but only about 20% of high-entropy tokens are crucial for output stability.</li>\n    <li>Focusing attacks on these critical tokens can cause significant harm with less effort.</li>\n    <li>The proposed method, Entropy-bank Guided Adversarial attacks (EGA), shows high success rates in causing harm, highlighting weaknesses in VLM safety.</li>\n</ul>"}, "publishedAt": "2025-12-25T20:01:25.000Z", "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models", "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21815.png", "numComments": 1, "submittedBy": {"_id": "68a3f14e6dd0e4c74c014853", "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg", "fullname": "Hubert", "name": "ANUHW", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "64ed4ba2453a4b4bef2664c5", "name": "anu-cvml", "fullname": "Australian National University"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.05175", "authors": [{"_id": "69606c015b7998385e639468", "user": {"_id": "65803d64defc9c0d30db4f88", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65803d64defc9c0d30db4f88/gVxJaB8vSUFj4r0vnTVbP.jpeg", "isPro": true, "fullname": "Shuming Liu", "user": "sming256", "type": "user"}, "name": "Shuming Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:23.773Z", "hidden": false}, {"_id": "69606c015b7998385e639469", "user": {"_id": "64403daae44f30a72323e4ca", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png", "isPro": false, "fullname": "mingchen zhuge", "user": "tjpxiaoming", "type": "user"}, "name": "Mingchen Zhuge", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:54:42.678Z", "hidden": false}, {"_id": "69606c015b7998385e63946a", "user": {"_id": "64d49ef30f76abaf363b88d6", "avatarUrl": "/avatars/93b5cc51305ac88198cea1dad8104db2.svg", "isPro": false, "fullname": "Changsheng Zhao", "user": "mikezhaocs", "type": "user"}, "name": "Changsheng Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:55:05.174Z", "hidden": false}, {"_id": "69606c015b7998385e63946b", "name": "Jun Chen", "hidden": false}, {"_id": "69606c015b7998385e63946c", "user": {"_id": "6364acf56fa33d2f59397c59", "avatarUrl": "/avatars/f642a7dcc098f50fef865f30f93a25c3.svg", "isPro": false, "fullname": "Lemeng Wu", "user": "klight", "type": "user"}, "name": "Lemeng Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:55:12.390Z", "hidden": false}, {"_id": "69606c015b7998385e63946d", "name": "Zechun Liu", "hidden": false}, {"_id": "69606c015b7998385e63946e", "user": {"_id": "66f6e80e7db9927533cbd3e1", "avatarUrl": "/avatars/f169a3cb4f48781e38cc111a35741d2d.svg", "isPro": false, "fullname": "Chenchen Zhu", "user": "zcckernel", "type": "user"}, "name": "Chenchen Zhu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:55:32.413Z", "hidden": false}, {"_id": "69606c015b7998385e63946f", "name": "Zhipeng Cai", "hidden": false}, {"_id": "69606c015b7998385e639470", "name": "Chong Zhou", "hidden": false}, {"_id": "69606c015b7998385e639471", "name": "Haozhe Liu", "hidden": false}, {"_id": "69606c015b7998385e639472", "name": "Ernie Chang", "hidden": false}, {"_id": "69606c015b7998385e639473", "user": {"_id": "645417e98617183806210cfc", "avatarUrl": "/avatars/7a3ba5b0c38878db51da7091fed15f99.svg", "isPro": false, "fullname": "Saksham Suri", "user": "sakshams", "type": "user"}, "name": "Saksham Suri", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:56:08.056Z", "hidden": false}, {"_id": "69606c015b7998385e639474", "name": "Hongyu Xu", "hidden": false}, {"_id": "69606c015b7998385e639475", "name": "Qi Qian", "hidden": false}, {"_id": "69606c015b7998385e639476", "name": "Wei Wen", "hidden": false}, {"_id": "69606c015b7998385e639477", "user": {"_id": "6570032714fa8cfccd39c6ad", "avatarUrl": "/avatars/f4b6140e21db5d18241dcc9b2e94ae33.svg", "isPro": false, "fullname": "Balakrishnan Varadarajan", "user": "balakv", "type": "user"}, "name": "Balakrishnan Varadarajan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:56:15.446Z", "hidden": false}, {"_id": "69606c015b7998385e639478", "name": "Zhuang Liu", "hidden": false}, {"_id": "69606c015b7998385e639479", "name": "Hu Xu", "hidden": false}, {"_id": "69606c015b7998385e63947a", "user": {"_id": "63166336894404e2506b8811", "avatarUrl": "/avatars/ec51a1dd86511127cf83afd4bf7c0f52.svg", "isPro": false, "fullname": "Florian Bordes", "user": "Fbordes", "type": "user"}, "name": "Florian Bordes", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:56:21.715Z", "hidden": false}, {"_id": "69606c015b7998385e63947b", "name": "Raghuraman Krishnamoorthi", "hidden": false}, {"_id": "69606c015b7998385e63947c", "user": {"_id": "6808bf97ffadd78ec71cb721", "avatarUrl": "/avatars/9adca3142c06b8f69889fcbe85fa374d.svg", "isPro": false, "fullname": "Bernard Ghanem", "user": "bernardghanem", "type": "user"}, "name": "Bernard Ghanem", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:56:31.147Z", "hidden": false}, {"_id": "69606c015b7998385e63947d", "name": "Vikas Chandra", "hidden": false}, {"_id": "69606c015b7998385e63947e", "user": {"_id": "65304b62e7535baecd85d080", "avatarUrl": "/avatars/6e546c7d1414bd92c5a7c8d8c404de92.svg", "isPro": false, "fullname": "Yunyang Xiong", "user": "yunyangx", "type": "user"}, "name": "Yunyang Xiong", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:56:41.516Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/ukFTFnGpjfjCtH8xLwHyV.png"], "publishedAt": "2026-01-08T18:00:59.000Z", "submittedOnDailyAt": "2026-01-09T00:16:39.681Z", "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.", "upvotes": 15, "discussionId": "69606c015b7998385e63947f", "projectPage": "https://ivul-kaust.github.io/projects/videoauto-r1/", "githubRepo": "https://github.com/IVUL-KAUST/VideoAuto-R1/", "githubRepoAddedBy": "user", "ai_summary": "VideoAuto-R1 framework employs a reason-when-necessary strategy for video understanding, using a Thinking Once, Answering Twice training paradigm with verifiable rewards and confidence-based reasoning activation during inference.", "ai_keywords": ["Chain-of-thought reasoning", "multimodal large language models", "video understanding", "RL-trained video models", "VideoAuto-R1", "Thinking Once Answering Twice", "verifiable rewards", "confidence score", "perception-oriented tasks", "reasoning-intensive tasks"], "githubStars": 11, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "summary_zh": "<ul>\n    <li>\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63a8\u7406\u5728\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u5f88\u6709\u7528\uff0c\u4f46\u5176\u5fc5\u8981\u6027\u548c\u4f18\u8d8a\u6027\u5c1a\u672a\u5145\u5206\u63a2\u8ba8\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u5bf9\u4e8e\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u89c6\u9891\u6a21\u578b\uff0c\u76f4\u63a5\u56de\u7b54\u95ee\u9898\u7684\u6548\u679c\u4e0eCoT\u76f8\u5f53\uff0c\u751a\u81f3\u66f4\u597d\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u7406\u89e3\u6846\u67b6VideoAuto-R1\uff0c\u91c7\u7528\u201c\u5fc5\u8981\u65f6\u63a8\u7406\u201d\u7684\u7b56\u7565\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5728\u8bad\u7ec3\u4e2d\u5148\u751f\u6210\u521d\u59cb\u7b54\u6848\uff0c\u518d\u8fdb\u884c\u63a8\u7406\uff0c\u6700\u540e\u8f93\u51fa\u7ecf\u8fc7\u5ba1\u6838\u7684\u7b54\u6848\uff0c\u5e76\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u8fdb\u884c\u76d1\u7763\u3002</li>\n    <li>VideoAuto-R1\u5728\u89c6\u9891\u95ee\u7b54\u548c\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u5e73\u5747\u54cd\u5e94\u957f\u5ea6\u51cf\u5c11\u7ea63.3\u500d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Chain-of-thought reasoning is useful for understanding videos, but its benefits compared to direct answers are not fully understood.</li>\n    <li>In some cases, direct answers can perform as well as or better than chain-of-thought reasoning, even though CoT takes more computational resources.</li>\n    <li>The authors introduce VideoAuto-R1, a new video understanding method that decides when to reason based on the initial answer's confidence.</li>\n    <li>VideoAuto-R1 shows improved accuracy and efficiency in video question-answering tasks, reducing the length of responses significantly.</li>\n    <li>Reasoning is more often needed for complex tasks, while simpler tasks may not require it, indicating that reasoning is beneficial but not always essential.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:00:59.000Z", "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice", "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/ukFTFnGpjfjCtH8xLwHyV.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05175.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 205, "isUserFollowing": false}, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.05138", "authors": [{"_id": "69606cd45b7998385e639489", "user": {"_id": "6528fe394513680346a500ed", "avatarUrl": "/avatars/3021a3be7d900e3928cdc375e2a2bf09.svg", "isPro": false, "fullname": "Sixiao Zheng (SII)", "user": "sxzheng", "type": "user"}, "name": "Sixiao Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:19.760Z", "hidden": false}, {"_id": "69606cd45b7998385e63948a", "name": "Minghao Yin", "hidden": false}, {"_id": "69606cd45b7998385e63948b", "name": "Wenbo Hu", "hidden": false}, {"_id": "69606cd45b7998385e63948c", "name": "Xiaoyu Li", "hidden": false}, {"_id": "69606cd45b7998385e63948d", "name": "Ying Shan", "hidden": false}, {"_id": "69606cd45b7998385e63948e", "user": {"_id": "6409fcc8f3dabf93824c84c6", "avatarUrl": "/avatars/dd8fd579630e50ba3058c3829604478e.svg", "isPro": false, "fullname": "YANWEI", "user": "yanweifuture", "type": "user"}, "name": "Yanwei Fu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:57:43.763Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/vfMPtMKg_t5C1-6WqzYur.mp4"], "publishedAt": "2026-01-08T17:28:52.000Z", "submittedOnDailyAt": "2026-01-09T00:21:14.757Z", "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.", "upvotes": 11, "discussionId": "69606cd45b7998385e63948f", "ai_summary": "VerseCrafter is a 4D-aware video world model that enables unified control over camera and object dynamics through 4D geometric control representation and video diffusion models.", "ai_keywords": ["video world models", "4D geometric control", "point cloud", "3D Gaussian trajectories", "video diffusion model", "view-consistent videos", "automatic data engine", "in-the-wild videos"], "summary_zh": "<ul>\n    <li>VerseCrafter\u662f\u4e00\u79cd4D\u89c6\u9891\u4e16\u754c\u6a21\u578b\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u63a7\u5236\u6444\u50cf\u673a\u548c\u591a\u7269\u4f53\u7684\u8fd0\u52a8\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4f7f\u7528\u4e00\u79cd\u65b0\u76844D\u51e0\u4f55\u63a7\u5236\u8868\u793a\uff0c\u80fd\u591f\u7f16\u7801\u4e16\u754c\u72b6\u6001\uff0c\u5305\u62ec\u9759\u6001\u80cc\u666f\u70b9\u4e91\u548c\u6bcf\u4e2a\u7269\u4f53\u76843D\u8f68\u8ff9\u3002</li>\n    <li>\u8fd9\u79cd\u8868\u793a\u65b9\u5f0f\u7075\u6d3b\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u4f20\u7edf\u7684\u8fb9\u754c\u6846\u6216\u53c2\u6570\u6a21\u578b\uff0c\u53ef\u4ee5\u6355\u6349\u7269\u4f53\u7684\u8fd0\u52a8\u8def\u5f84\u548c\u65f6\u95f4\u4e0a\u7684\u5360\u7528\u6982\u7387\u3002</li>\n    <li>VerseCrafter\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u89c6\u89d2\u4e00\u81f4\u7684\u89c6\u9891\uff0c\u7b26\u5408\u6307\u5b9a\u7684\u52a8\u6001\u8981\u6c42\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u7f3a\u4e4f\u5927\u89c4\u6a214D\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u52a8\u6570\u636e\u5f15\u64ce\uff0c\u4ece\u91ce\u5916\u89c6\u9891\u4e2d\u63d0\u53d6\u6240\u9700\u76844D\u63a7\u5236\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>VerseCrafter is a new model designed to simulate real-world environments in videos, giving users better control over camera and object movements.</li>\n    <li>It uses a unique 4D representation that combines a static background with 3D paths for objects, allowing for flexible video generation.</li>\n    <li>This model creates high-quality videos that stay consistent with the specified movements of the camera and objects.</li>\n    <li>One challenge is the lack of large training datasets with 4D information, which VerseCrafter addresses by automatically extracting data from existing videos.</li>\n    <li>This allows the model to be trained on a big and varied collection of videos, improving its performance.</li>\n</ul>"}, "publishedAt": "2026-01-08T12:28:52.000Z", "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control", "summary": "Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/vfMPtMKg_t5C1-6WqzYur.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05138.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 205, "isUserFollowing": false}, "isAuthorParticipating": true}],
    "week": [{"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u7528\u6237\u5e0c\u671b\u5b83\u4eec\u4e0d\u4ec5\u80fd\u63d0\u4f9b\u51c6\u786e\u7684\u56de\u7b54\uff0c\u8fd8\u80fd\u7b26\u5408\u591a\u6837\u7684\u4eba\u7c7b\u504f\u597d\u3002</li>\n    <li>\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5f00\u59cb\u91c7\u7528\u591a\u79cd\u5956\u52b1\u6765\u6307\u5bfc\u6a21\u578b\u884c\u4e3a\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u76f4\u63a5\u5e94\u7528\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u4f1a\u5bfc\u81f4\u5956\u52b1\u7ec4\u5408\u7684\u4f18\u52bf\u503c\u76f8\u540c\uff0c\u4ece\u800c\u964d\u4f4e\u8bad\u7ec3\u4fe1\u53f7\u7684\u5206\u8fa8\u7387\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff1a\u7fa4\u4f53\u5956\u52b1\u89e3\u8026\u5f52\u4e00\u5316\u7b56\u7565\u4f18\u5316\uff08GDPO\uff09\uff0c\u89e3\u51b3\u4e86GRPO\u7684\u95ee\u9898\uff0c\u4fdd\u6301\u4e86\u5956\u52b1\u95f4\u7684\u76f8\u5bf9\u5dee\u5f02\u3002</li>\n    <li>GDPO\u5728\u4e09\u4e2a\u4efb\u52a1\uff08\u5de5\u5177\u8c03\u7528\u3001\u6570\u5b66\u63a8\u7406\u548c\u7f16\u7801\u63a8\u7406\uff09\u4e2d\u8868\u73b0\u4f18\u4e8eGRPO\uff0c\u663e\u793a\u51fa\u5176\u5728\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Users expect language models to give accurate answers and behave according to different human preferences.</li>\n    <li>Reinforcement learning (RL) is using multiple rewards to guide models to meet these preferences.</li>\n    <li>Applying Group Relative Policy Optimization (GRPO) to these rewards can lead to problems, making training less effective.</li>\n    <li>The new method, Group reward-Decoupled Normalization Policy Optimization (GDPO), improves on GRPO by keeping the rewards separate for better training.</li>\n    <li>GDPO has shown better performance in tasks like tool calling, math reasoning, and coding reasoning compared to GRPO.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03252", "authors": [{"_id": "695dc956c03d6d81e4399ea4", "name": "Hao Yu", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea5", "user": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "isPro": false, "fullname": "Haotong Lin", "user": "haotongl", "type": "user"}, "name": "Haotong Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:15:04.783Z", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea6", "name": "Jiawei Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea7", "name": "Jiaxin Li", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea8", "name": "Yida Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea9", "user": {"_id": "6791a6c19ce382eae861ed61", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6791a6c19ce382eae861ed61/zerctN-RdeP4hSrWidtyN.jpeg", "isPro": false, "fullname": "Xueyang Zhang", "user": "zhangxueyang001", "type": "user"}, "name": "Xueyang Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:15:39.946Z", "hidden": false}, {"_id": "695dc956c03d6d81e4399eaa", "name": "Yue Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399eab", "name": "Xiaowei Zhou", "hidden": false}, {"_id": "695dc956c03d6d81e4399eac", "name": "Ruizhen Hu", "hidden": false}, {"_id": "695dc956c03d6d81e4399ead", "user": {"_id": "62986ca2b58e71e2ac9b8f01", "avatarUrl": "/avatars/83944db5f3dbb6f47c47c46fb2cb2849.svg", "isPro": false, "fullname": "Sida Peng", "user": "pengsida", "type": "user"}, "name": "Sida Peng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:16:12.074Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6489a01b8de3f9d810b0154f/XlXUo1VjGVhePk-xHRmkj.mp4"], "publishedAt": "2026-01-06T18:57:06.000Z", "submittedOnDailyAt": "2026-01-07T00:26:37.060Z", "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields", "submittedOnDailyBy": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "isPro": false, "fullname": "Haotong Lin", "user": "haotongl", "type": "user"}, "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.", "upvotes": 71, "discussionId": "695dc956c03d6d81e4399eae", "ai_summary": "InfiniDepth represents depth as neural implicit fields using a local implicit decoder, enabling continuous 2D coordinate querying for arbitrary-resolution depth estimation and superior performance in fine-detail regions.", "ai_keywords": ["neural implicit fields", "local implicit decoder", "continuous 2D coordinates", "arbitrary-resolution depth estimation", "synthetic benchmark", "4K synthetic benchmark", "novel view synthesis", "viewpoint shifts"], "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u53ea\u80fd\u5728\u79bb\u6563\u7684\u56fe\u50cf\u7f51\u683c\u4e0a\u9884\u6d4b\u6df1\u5ea6\uff0c\u9650\u5236\u4e86\u8f93\u51fa\u5206\u8fa8\u7387\u7684\u7075\u6d3b\u6027\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86InfiniDepth\uff0c\u901a\u8fc7\u795e\u7ecf\u9690\u5f0f\u573a\u8868\u793a\u6df1\u5ea6\uff0c\u80fd\u591f\u5728\u8fde\u7eed\u76842D\u5750\u6807\u4e0a\u67e5\u8be2\u6df1\u5ea6\u3002</li>\n    <li>InfiniDepth\u652f\u6301\u4efb\u610f\u5206\u8fa8\u7387\u548c\u7ec6\u81f4\u7684\u6df1\u5ea6\u4f30\u8ba1\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002</li>\n    <li>\u6211\u4eec\u5236\u4f5c\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u76844K\u5408\u6210\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e30\u5bcc\u7684\u51e0\u4f55\u548c\u5916\u89c2\u7ec6\u8282\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cInfiniDepth\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u662f\u5728\u7ec6\u8282\u4e30\u5bcc\u7684\u533a\u57df\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current depth estimation methods can only predict depth on fixed grids, limiting their ability to scale and capture fine details.</li>\n    <li>InfiniDepth uses neural implicit fields to allow depth estimation at any resolution and detail level by querying continuous 2D coordinates.</li>\n    <li>The researchers created a high-quality 4K synthetic dataset from five different games to test their method on various scenes.</li>\n    <li>InfiniDepth shows top performance on both synthetic and real-world tests, especially in areas with fine details.</li>\n    <li>It also improves the quality of new views in images, with fewer gaps and artifacts when viewpoints change significantly.</li>\n</ul>"}, "publishedAt": "2026-01-06T13:57:06.000Z", "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields", "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6489a01b8de3f9d810b0154f/XlXUo1VjGVhePk-xHRmkj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03252.png", "numComments": 8, "submittedBy": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "fullname": "Haotong Lin", "name": "haotongl", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.02151", "authors": [{"_id": "695f2d8a5fa3847525c41f8d", "user": {"_id": "6768c97367e4b4606a3c9cec", "avatarUrl": "/avatars/5ddafe7a05828366f66c79072556f370.svg", "isPro": false, "fullname": "diaomuxi", "user": "diaomuxi", "type": "user"}, "name": "Muxi Diao", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:31:56.507Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f8e", "user": {"_id": "666a6cf89a3e3ce05a519bcc", "avatarUrl": "/avatars/9e72481deec3bd5c5202e42c32894a32.svg", "isPro": false, "fullname": "\u6768\u4e50\u4e50", "user": "ssl-asuka", "type": "user"}, "name": "Lele Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:31:51.246Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f8f", "user": {"_id": "64c0f972d76592ba899c2c9c", "avatarUrl": "/avatars/d6940beb135f99241c6fb2cf0e8ccdbe.svg", "isPro": false, "fullname": "GongWuxuan", "user": "Wuxuan-Gong", "type": "user"}, "name": "Wuxuan Gong", "status": "admin_assigned", "statusLastChangedAt": "2026-01-08T08:44:19.470Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f90", "name": "Yutong Zhang", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f91", "user": {"_id": "64fbd4e69a62bb2791b3a665", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fbd4e69a62bb2791b3a665/ZEMtU8O0z98ryeRCG3l_K.jpeg", "isPro": false, "fullname": "Zhonghao Yan", "user": "zzzyzh", "type": "user"}, "name": "Zhonghao Yan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:31:53.904Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f92", "name": "Yufei Han", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f93", "user": {"_id": "67f4a56928cbc4f2f75c008d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qoX8HT0JsjqW2OQoENSvg.png", "isPro": false, "fullname": "Kongming Liang", "user": "KongmingLiang", "type": "user"}, "name": "Kongming Liang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-08T08:44:38.812Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f94", "name": "Weiran Xu", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f95", "name": "Zhanyu Ma", "hidden": false}], "publishedAt": "2026-01-05T14:28:17.000Z", "submittedOnDailyAt": "2026-01-08T01:42:04.476Z", "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting", "submittedOnDailyBy": {"_id": "666a6cf89a3e3ce05a519bcc", "avatarUrl": "/avatars/9e72481deec3bd5c5202e42c32894a32.svg", "isPro": false, "fullname": "\u6768\u4e50\u4e50", "user": "ssl-asuka", "type": "user"}, "summary": "Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as \"Confident Conflicts\" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.", "upvotes": 64, "discussionId": "695f2d8b5fa3847525c41f96", "projectPage": "https://ymxyll.github.io/EAFT/", "githubRepo": "https://github.com/PRIS-CV/EAFT", "githubRepoAddedBy": "user", "ai_summary": "Entropy-Adaptive Fine-Tuning addresses catastrophic forgetting in supervised fine-tuning by using token-level entropy to distinguish uncertainty from knowledge conflict, enabling better preservation of general capabilities.", "ai_keywords": ["supervised fine-tuning", "catastrophic forgetting", "on-policy reinforcement learning", "distributional gap", "Confident Conflicts", "token-level entropy", "epistemic uncertainty", "knowledge conflict", "gradient updates", "downstream performance"], "githubStars": 18, "organization": {"_id": "64283c0c68faf6ddab552684", "name": "BUPT-PRIS", "fullname": "BUPT AI PRIS"}, "summary_zh": "<ul>\n    <li>\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5728\u9886\u57df\u9002\u5e94\u4e2d\u5e38\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u3002</li>\n    <li>\u800c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u80fd\u591f\u6709\u6548\u4fdd\u6301\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0cSFT\u4e0eRL\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u7684\u5206\u5e03\u5dee\u8ddd\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u71b5\u81ea\u9002\u5e94\u5fae\u8c03\uff08EAFT\uff09\uff0c\u901a\u8fc7\u4f7f\u7528\u4ee4\u724c\u7ea7\u71b5\u6765\u533a\u5206\u4e0d\u786e\u5b9a\u6027\u548c\u77e5\u8bc6\u51b2\u7a81\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cEAFT\u5728\u5404\u4e2a\u9886\u57df\u4e2d\u7684\u6027\u80fd\u4e0e\u4f20\u7edfSFT\u76f8\u5f53\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u901a\u7528\u80fd\u529b\u7684\u4e0b\u964d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Supervised Fine-Tuning (SFT) often leads to \"catastrophic forgetting\" when adapting models to new tasks.</li>\n    <li>On-policy Reinforcement Learning (RL) helps maintain a model's general abilities without forgetting.</li>\n    <li>The problem arises because SFT makes the model fit external labels, which can conflict with its own understanding.</li>\n    <li>To solve this issue, the authors propose Entropy-Adaptive Fine-Tuning (EAFT), which uses token-level entropy to manage learning.</li>\n    <li>EAFT allows models to learn from uncertain data while reducing harmful updates from conflicting information, showing improved performance in various domains.</li>\n</ul>"}, "publishedAt": "2026-01-05T09:28:17.000Z", "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting", "summary": "Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as \"Confident Conflicts\" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02151.png", "numComments": 6, "submittedBy": {"_id": "666a6cf89a3e3ce05a519bcc", "avatarUrl": "/avatars/9e72481deec3bd5c5202e42c32894a32.svg", "fullname": "\u6768\u4e50\u4e50", "name": "ssl-asuka", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "64283c0c68faf6ddab552684", "name": "BUPT-PRIS", "fullname": "BUPT AI PRIS"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03509", "authors": [{"_id": "695fbc7d5b7998385e639349", "name": "Haochen Shi", "hidden": false}, {"_id": "695fbc7d5b7998385e63934a", "name": "Xingdi Yuan", "hidden": false}, {"_id": "695fbc7d5b7998385e63934b", "name": "Bang Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/654a97282d2fcd6bf2851173/sAKIzhLgfcEhgVZMBkHRW.png"], "publishedAt": "2026-01-07T01:43:25.000Z", "submittedOnDailyAt": "2026-01-08T11:50:05.687Z", "title": "Evolving Programmatic Skill Networks", "submittedOnDailyBy": {"_id": "654a97282d2fcd6bf2851173", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png", "isPro": false, "fullname": "Bang Liu", "user": "Bang-UdeM-Mila", "type": "user"}, "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.", "upvotes": 54, "discussionId": "695fbc7e5b7998385e63934c", "ai_summary": "Programmatic Skill Network enables continual skill acquisition through executable symbolic programs that evolve via reflection, progressive optimization, and structural refactoring mechanisms.", "ai_keywords": ["Programmatic Skill Network", "executable symbolic programs", "skill composition", "structured fault localization", "progressive optimization", "maturity-aware update gating", "canonical structural refactoring", "rollback validation", "neural network training", "skill reuse", "rapid adaptation", "generalization"], "organization": {"_id": "636e93488ba65db4a0987ab4", "name": "Universite-de-Montreal", "fullname": "Universit\u00e9 de Montr\u00e9al"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u7814\u7a76\u4e86\u5728\u5f00\u653e\u5f0f\u73af\u5883\u4e2d\u4e0d\u65ad\u5b66\u4e60\u6280\u80fd\u7684\u8fc7\u7a0b\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u7a0b\u5e8f\u6280\u80fd\u7f51\u7edc\uff08PSN\uff09\uff0c\u5b83\u5c06\u6280\u80fd\u89c6\u4e3a\u53ef\u6267\u884c\u7684\u7b26\u53f7\u7a0b\u5e8f\uff0c\u5e76\u968f\u7740\u7ecf\u9a8c\u4e0d\u65ad\u53d1\u5c55\u3002</li>\n    <li>PSN\u5305\u542b\u4e09\u79cd\u6838\u5fc3\u673a\u5236\uff1a\u7ed3\u6784\u5316\u6545\u969c\u5b9a\u4f4d\u3001\u6e10\u8fdb\u4f18\u5316\u548c\u7ed3\u6784\u91cd\u6784\uff0c\u4ee5\u63d0\u9ad8\u6280\u80fd\u7684\u53ef\u9760\u6027\u548c\u7075\u6d3b\u6027\u3002</li>\n    <li>PSN\u7684\u5b66\u4e60\u52a8\u6001\u4e0e\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6709\u76f8\u4f3c\u7684\u7ed3\u6784\u7279\u5f81\u3002</li>\n    <li>\u5728MineDojo\u548cCrafter\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cPSN\u53ef\u4ee5\u5feb\u901f\u9002\u5e94\u548c\u5f3a\u5927\u7684\u6280\u80fd\u91cd\u7528\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study focuses on how agents can continuously learn and improve skills in changing environments.</li>\n    <li>A new framework called Programmatic Skill Network (PSN) is introduced, where skills are like programs that can be combined and improved over time.</li>\n    <li>PSN uses large language models and includes three main features: fault finding in skill combinations, smart updates to improve skills while keeping reliable ones stable, and a way to reorganize skills without losing efficiency.</li>\n    <li>Experiments show that PSN helps agents reuse skills effectively, adapt quickly, and perform well across a variety of tasks.</li>\n    <li>The code for the framework will be made available for others to use.</li>\n</ul>"}, "publishedAt": "2026-01-06T20:43:25.000Z", "title": "Evolving Programmatic Skill Networks", "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/654a97282d2fcd6bf2851173/sAKIzhLgfcEhgVZMBkHRW.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03509.png", "numComments": 1, "submittedBy": {"_id": "654a97282d2fcd6bf2851173", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png", "fullname": "Bang Liu", "name": "Bang-UdeM-Mila", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10, "isUserFollowing": false}, "organization": {"_id": "636e93488ba65db4a0987ab4", "name": "Universite-de-Montreal", "fullname": "Universit\u00e9 de Montr\u00e9al"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.01554", "authors": [{"_id": "695dcda5c03d6d81e4399eb8", "name": "MOSI. AI", "hidden": false}, {"_id": "695dcda5c03d6d81e4399eb9", "name": "Donghua Yu", "hidden": false}, {"_id": "695dcda5c03d6d81e4399eba", "name": "Zhengyuan Lin", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebb", "user": {"_id": "660c345da15ab85523ad00d1", "avatarUrl": "/avatars/b0bfdee89a6c62ff12140b9e85de499a.svg", "isPro": false, "fullname": "Chen Yang", "user": "kiiic", "type": "user"}, "name": "Chen Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:58.894Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebc", "name": "Yiyang Zhang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebd", "name": "Hanfu Chen", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebe", "name": "Jingqi Chen", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ebf", "name": "Ke Chen", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec0", "name": "Liwei Fan", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec1", "name": "Yi Jiang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec2", "name": "Jie Zhu", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec3", "name": "Muchen Li", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec4", "name": "Wenxuan Wang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec5", "name": "Yang Wang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec6", "user": {"_id": "6443f7bf1bc692d87b25e234", "avatarUrl": "/avatars/fa9e62d96d0691a9a48e3db499a61557.svg", "isPro": false, "fullname": "Xu Zhe", "user": "Phospheneser", "type": "user"}, "name": "Zhe Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-07T09:25:55.950Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec7", "name": "Yitian Gong", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec8", "name": "Yuqian Zhang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ec9", "name": "Wenbo Zhang", "hidden": false}, {"_id": "695dcda5c03d6d81e4399eca", "user": {"_id": "629ef8544313a7c1dd671130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png", "isPro": false, "fullname": "Zhaoye Fei", "user": "ngc7293", "type": "user"}, "name": "Zhaoye Fei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:17:10.124Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ecb", "user": {"_id": "695757e4fd9dc6e9bac27935", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/_uZEu4oOlKJVYqrG763Z-.jpeg", "isPro": false, "fullname": "aa", "user": "qinyuancheng", "type": "user"}, "name": "Qinyuan Cheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:17:03.749Z", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ecc", "name": "Shimin Li", "hidden": false}, {"_id": "695dcda5c03d6d81e4399ecd", "user": {"_id": "61457b8deff2c9fdb4de4988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg", "isPro": false, "fullname": "Xipeng Qiu", "user": "xpqiu", "type": "user"}, "name": "Xipeng Qiu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:16:50.004Z", "hidden": false}], "publishedAt": "2026-01-04T15:01:10.000Z", "submittedOnDailyAt": "2026-01-07T00:52:16.123Z", "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization", "submittedOnDailyBy": {"_id": "629ef8544313a7c1dd671130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png", "isPro": false, "fullname": "Zhaoye Fei", "user": "ngc7293", "type": "user"}, "summary": "Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.", "upvotes": 45, "discussionId": "695dcda6c03d6d81e4399ece", "projectPage": "https://mosi.cn/models/moss-transcribe-diarize", "ai_summary": "A unified multimodal large language model for end-to-end speaker-attributed, time-stamped transcription with extended context window and strong generalization across benchmarks.", "ai_keywords": ["multimodal large language model", "end-to-end paradigm", "speaker diarization", "time-stamped transcription", "context window", "robust generalization"], "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>SATS\uff08\u8bf4\u8bdd\u8005\u5f52\u5c5e\u3001\u65f6\u95f4\u6233\u8f6c\u5f55\uff09\u65e8\u5728\u51c6\u786e\u8f6c\u5f55\u53d1\u8a00\u5185\u5bb9\u5e76\u786e\u5b9a\u6bcf\u4f4d\u8bf4\u8bdd\u8005\u7684\u53d1\u8a00\u65f6\u95f4\uff0c\u9002\u7528\u4e8e\u4f1a\u8bae\u8bb0\u5f55\u3002</li>\n    <li>\u73b0\u6709\u7684SATS\u7cfb\u7edf\u901a\u5e38\u4e0d\u91c7\u7528\u7aef\u5230\u7aef\u7684\u65b9\u6cd5\uff0c\u5e76\u53d7\u5230\u4e0a\u4e0b\u6587\u7a97\u53e3\u3001\u957f\u65f6\u95f4\u8bf4\u8bdd\u8005\u8bb0\u5fc6\u548c\u65e0\u6cd5\u8f93\u51fa\u65f6\u95f4\u6233\u7684\u9650\u5236\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86MOSS Transcribe Diarize\uff0c\u8fd9\u662f\u4e00\u6b3e\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u7aef\u5230\u7aef\u5730\u8fdb\u884c\u8bf4\u8bdd\u8005\u5f52\u5c5e\u548c\u65f6\u95f4\u6233\u8f6c\u5f55\u3002</li>\n    <li>MOSS Transcribe Diarize\u5728\u5927\u91cf\u771f\u5b9e\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5177\u6709128k\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u652f\u6301\u6700\u957f90\u5206\u949f\u7684\u8f93\u5165\u3002</li>\n    <li>\u7ecf\u8fc7\u5168\u9762\u8bc4\u4f30\uff0cMOSS Transcribe Diarize\u5728\u591a\u4e2a\u516c\u5171\u548c\u5185\u90e8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5546\u4e1a\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MOSS Transcribe Diarize is a new model for transcribing meetings that identifies who is speaking and when.</li>\n    <li>It solves issues found in existing systems, such as limited context and weak memory for long conversations.</li>\n    <li>The model can handle long inputs of up to 90 minutes and has a large context window of 128,000 tokens.</li>\n    <li>MOSS is trained on a wide range of real-life data, making it effective and reliable.</li>\n    <li>In tests, it performs better than leading commercial transcription systems.</li>\n</ul>"}, "publishedAt": "2026-01-04T10:01:10.000Z", "title": "MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization", "summary": "Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01554.png", "numComments": 2, "submittedBy": {"_id": "629ef8544313a7c1dd671130", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629ef8544313a7c1dd671130/i5xfHIgELcuO1Ew19ebTw.png", "fullname": "Zhaoye Fei", "name": "ngc7293", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.02204", "authors": [{"_id": "695c7d0d6aa73bc11f091433", "name": "Huichao Zhang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091434", "user": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "isPro": false, "fullname": "Liao Qu", "user": "leo1117", "type": "user"}, "name": "Liao Qu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:57:57.686Z", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091435", "name": "Yiheng Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091436", "name": "Hang Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091437", "name": "Yangyang Song", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091438", "name": "Yongsheng Dong", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091439", "name": "Shikun Sun", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143a", "name": "Xian Li", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143b", "name": "Xu Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143c", "user": {"_id": "6344dcb1cd37e44d9ed46508", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344dcb1cd37e44d9ed46508/J92UKSxKR3iziD2WJfih4.jpeg", "isPro": false, "fullname": "Yi Jiang", "user": "JiangYi", "type": "user"}, "name": "Yi Jiang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:57:55.158Z", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143d", "name": "Hu Ye", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143e", "name": "Bo Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143f", "name": "Yiming Gao", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091440", "name": "Peng Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091441", "name": "Akide Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091442", "name": "Zhipeng Yang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091443", "name": "Qili Deng", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091444", "name": "Linjie Xing", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091445", "name": "Jiyang Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091446", "name": "Zhao Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091447", "name": "Yang Zhou", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091448", "name": "Mingcong Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091449", "name": "Yi Zhang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144a", "name": "Qian He", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144b", "name": "Xiwei Hu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144c", "name": "Zhongqi Qi", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144d", "name": "Jie Shao", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144e", "name": "Zhiye Fu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144f", "name": "Shuai Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091450", "name": "Fangmin Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091451", "name": "Xuezhi Chai", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091452", "name": "Zhihua Wu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091453", "name": "Yitong Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091454", "name": "Zehuan Yuan", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091455", "name": "Daniel K. Du", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091456", "name": "Xinglong Wu", "hidden": false}], "publishedAt": "2026-01-05T15:27:04.000Z", "submittedOnDailyAt": "2026-01-06T00:52:35.953Z", "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation", "submittedOnDailyBy": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "isPro": false, "fullname": "Liao Qu", "user": "leo1117", "type": "user"}, "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.", "upvotes": 45, "discussionId": "695c7d0d6aa73bc11f091457", "githubRepo": "https://github.com/ByteVisionLab/NextFlow", "githubRepoAddedBy": "user", "ai_summary": "NextFlow is a unified decoder-only autoregressive transformer that processes interleaved text-image tokens, enabling fast multimodal generation through novel next-token and next-scale prediction strategies.", "ai_keywords": ["decoder-only autoregressive transformer", "interleaved text-image discrete tokens", "unified vision representation", "multimodal understanding", "multimodal generation", "next-token prediction", "next-scale prediction", "raster-scan methods", "visual generation", "prefix-tuning strategy", "reinforcement learning", "diffusion baselines"], "githubStars": 60, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>NextFlow \u662f\u4e00\u79cd\u7edf\u4e00\u7684\u89e3\u7801\u5668\u81ea\u56de\u5f52\u53d8\u6362\u5668\uff0c\u8bad\u7ec3\u4e86 6 \u4e07\u4ebf\u4e2a\u6587\u672c\u548c\u56fe\u50cf\u7684\u79bb\u6563\u7b26\u53f7\u3002</li>\n    <li>\u5b83\u80fd\u591f\u8fdb\u884c\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\uff0c\u5305\u62ec\u56fe\u50cf\u7f16\u8f91\u3001\u5185\u5bb9\u751f\u6210\u548c\u89c6\u9891\u751f\u6210\u3002</li>\n    <li>\u6587\u672c\u4f7f\u7528\u4e0b\u4e00\u4e2a\u7b26\u53f7\u9884\u6d4b\uff0c\u800c\u56fe\u50cf\u751f\u6210\u91c7\u7528\u4e0b\u4e00\u4e2a\u5c3a\u5ea6\u9884\u6d4b\uff0c\u901f\u5ea6\u66f4\u5feb\uff0c\u751f\u6210 1024x1024 \u56fe\u50cf\u53ea\u9700 5 \u79d2\u3002</li>\n    <li>NextFlow \u901a\u8fc7\u7a33\u5065\u7684\u8bad\u7ec3\u65b9\u6cd5\u89e3\u51b3\u591a\u5c3a\u5ea6\u751f\u6210\u7684\u4e0d\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cNextFlow \u5728\u7edf\u4e00\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u89c6\u89c9\u8d28\u91cf\u63a5\u8fd1\u4e13\u4e1a\u6269\u6563\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NextFlow is a powerful model that combines text and images, trained on 6 trillion data points.</li>\n    <li>It can understand and create both text and images, enabling features like image editing and video generation.</li>\n    <li>The model predicts the next part of a text normally but uses a different method for generating images, allowing it to create high-quality images quickly.</li>\n    <li>NextFlow generates 1024x1024 images in just 5 seconds, which is much faster than similar models.</li>\n    <li>It shows excellent performance in tests, competing well with specialized models for visual content.</li>\n</ul>"}, "publishedAt": "2026-01-05T10:27:04.000Z", "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation", "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02204.png", "numComments": 1, "submittedBy": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "fullname": "Liao Qu", "name": "leo1117", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.01739", "authors": [{"_id": "695c72346aa73bc11f0913bf", "user": {"_id": "6044fd39e6aa3e130cb92867", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6044fd39e6aa3e130cb92867/L5hb8vpHY6SKMEL-Xacma.jpeg", "isPro": false, "fullname": "Eunbi Choi", "user": "unbiarirang", "type": "user"}, "name": "Eunbi Choi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:17.472Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c0", "user": {"_id": "64d31ca9465b6039259838df", "avatarUrl": "/avatars/b3bde5067ed3fcd908d3d91c00680bfb.svg", "isPro": false, "fullname": "kibong choi", "user": "bongchoi", "type": "user"}, "name": "Kibong Choi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:11.322Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c1", "name": "Seokhee Hong", "hidden": false}, {"_id": "695c72346aa73bc11f0913c2", "user": {"_id": "63c50e590c24c8b53958f75e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673858632881-noauth.png", "isPro": false, "fullname": "Junwon Hwang", "user": "nuxlear", "type": "user"}, "name": "Junwon Hwang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:09.333Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c3", "name": "Hyojin Jeon", "hidden": false}, {"_id": "695c72346aa73bc11f0913c4", "user": {"_id": "66a9e066a203add977948988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a9e066a203add977948988/mwVS-vt-8p-DFC5T9H9H3.jpeg", "isPro": false, "fullname": "hyunjik.jo", "user": "switiz87", "type": "user"}, "name": "Hyunjik Jo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:13.551Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c5", "name": "Joonkee Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c6", "name": "Seonghwan Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c7", "name": "Soyeon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c8", "name": "Sunkyoung Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c9", "name": "Yireun Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913ca", "name": "Yongil Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913cb", "name": "Haeju Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913cc", "name": "Jinsik Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913cd", "name": "Kyungmin Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ce", "name": "Sangha Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913cf", "name": "Heuiyeen Yeen", "hidden": false}, {"_id": "695c72346aa73bc11f0913d0", "name": "Hwan Chang", "hidden": false}, {"_id": "695c72346aa73bc11f0913d1", "name": "Stanley Jungkyu Choi", "hidden": false}, {"_id": "695c72346aa73bc11f0913d2", "name": "Yejin Choi", "hidden": false}, {"_id": "695c72346aa73bc11f0913d3", "name": "Jiwon Ham", "hidden": false}, {"_id": "695c72346aa73bc11f0913d4", "name": "Kijeong Jeon", "hidden": false}, {"_id": "695c72346aa73bc11f0913d5", "name": "Geunyeong Jeong", "hidden": false}, {"_id": "695c72346aa73bc11f0913d6", "name": "Gerrard Jeongwon Jo", "hidden": false}, {"_id": "695c72346aa73bc11f0913d7", "name": "Yonghwan Jo", "hidden": false}, {"_id": "695c72346aa73bc11f0913d8", "name": "Jiyeon Jung", "hidden": false}, {"_id": "695c72346aa73bc11f0913d9", "name": "Naeun Kang", "hidden": false}, {"_id": "695c72346aa73bc11f0913da", "name": "Dohoon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913db", "name": "Euisoon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913dc", "name": "Hayeon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913dd", "name": "Hyosang Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913de", "name": "Hyunseo Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913df", "name": "Jieun Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e0", "name": "Minu Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e1", "name": "Myoungshin Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e2", "name": "Unsol Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e3", "name": "Youchul Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e4", "name": "YoungJin Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e5", "name": "Chaeeun Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e6", "name": "Chaeyoon Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e7", "user": {"_id": "6399ab9e92e12136b99ef60e", "avatarUrl": "/avatars/b76895c53f0f046586555c20292c78a1.svg", "isPro": false, "fullname": "Changhun Lee", "user": "xvyaward", "type": "user"}, "name": "Changhun Lee", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:15.420Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913e8", "name": "Dahm Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e9", "name": "Edward Hwayoung Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ea", "name": "Honglak Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913eb", "name": "Jinsang Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ec", "name": "Jiyoung Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ed", "name": "Sangeun Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ee", "name": "Seungwon Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913ef", "name": "Solji Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913f0", "name": "Woohyung Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913f1", "name": "Chanwoo Moon", "hidden": false}, {"_id": "695c72346aa73bc11f0913f2", "name": "Jaewoo Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f3", "name": "Jinho Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f4", "name": "Yongmin Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f5", "name": "Hyerin Seo", "hidden": false}, {"_id": "695c72346aa73bc11f0913f6", "name": "Wooseok Seo", "hidden": false}, {"_id": "695c72346aa73bc11f0913f7", "name": "Yongwoo Song", "hidden": false}, {"_id": "695c72346aa73bc11f0913f8", "name": "Sejong Yang", "hidden": false}, {"_id": "695c72346aa73bc11f0913f9", "name": "Sihoon Yang", "hidden": false}, {"_id": "695c72346aa73bc11f0913fa", "name": "Chang En Yea", "hidden": false}, {"_id": "695c72346aa73bc11f0913fb", "name": "Sihyuk Yi", "hidden": false}, {"_id": "695c72346aa73bc11f0913fc", "name": "Chansik Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913fd", "name": "Dongkeun Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913fe", "name": "Sangyeon Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913ff", "name": "Hyeongu Yun", "hidden": false}], "publishedAt": "2026-01-05T02:30:59.000Z", "submittedOnDailyAt": "2026-01-06T01:03:14.011Z", "title": "K-EXAONE Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.", "upvotes": 44, "discussionId": "695c72356aa73bc11f091400", "githubRepo": "https://github.com/LG-AI-EXAONE/K-EXAONE", "githubRepoAddedBy": "user", "ai_summary": "K-EXAONE is a multilingual language model with a Mixture-of-Experts architecture that achieves competitive performance on various benchmarks while supporting multiple languages and long-context windows.", "ai_keywords": ["Mixture-of-Experts", "256K-token context window", "multilingual language model", "parameter-efficient fine-tuning"], "githubStars": 39, "organization": {"_id": "66a89bc1d96a5adbccbe85d4", "name": "LGAI-EXAONE", "fullname": "LG AI Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a899a72f11aaf66001a8dc/UfdrP3GMo9pNT62BaMnhw.png"}, "summary_zh": "<ul>\n    <li>K-EXAONE\u662fLG AI\u7814\u7a76\u5f00\u53d1\u7684\u5927\u89c4\u6a21\u591a\u8bed\u79cd\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528Mixture-of-Experts\u67b6\u6784\uff0c\u62e5\u67092360\u4ebf\u4e2a\u53c2\u6570\uff0c\u63a8\u7406\u65f6\u6fc0\u6d3b230\u4ebf\u4e2a\u53c2\u6570\u3002</li>\n    <li>K-EXAONE\u652f\u6301256K\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u6db5\u76d6\u516d\u79cd\u8bed\u8a00\uff1a\u97e9\u8bed\u3001\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u5fb7\u8bed\u3001\u65e5\u8bed\u548c\u8d8a\u5357\u8bed\u3002</li>\n    <li>\u5728\u591a\u9879\u8bc4\u4f30\u4e2d\uff0cK-EXAONE\u7684\u8868\u73b0\u4e0e\u540c\u7c7b\u5f00\u6e90\u6a21\u578b\u76f8\u5f53\u3002</li>\n    <li>K-EXAONE\u65e8\u5728\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u5de5\u4e1a\u548c\u7814\u7a76\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>K-EXAONE is a large multilingual language model created by LG AI Research.</li>\n    <li>It has a total of 236 billion parameters but uses 23 billion parameters at a time for processing.</li>\n    <li>The model can handle a context of up to 256,000 tokens and supports six languages: Korean, English, Spanish, German, Japanese, and Vietnamese.</li>\n    <li>K-EXAONE is evaluated on various tests and shows performance similar to other large models with open weights.</li>\n    <li>The model aims to enhance AI for better living and serves as a strong foundation for various industrial and research uses.</li>\n</ul>"}, "publishedAt": "2026-01-04T21:30:59.000Z", "title": "K-EXAONE Technical Report", "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01739.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 200, "isUserFollowing": false}, "organization": {"_id": "66a89bc1d96a5adbccbe85d4", "name": "LGAI-EXAONE", "fullname": "LG AI Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a899a72f11aaf66001a8dc/UfdrP3GMo9pNT62BaMnhw.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.03233", "authors": [{"_id": "695dc6d9c03d6d81e4399e85", "user": {"_id": "6303cc5e0547362a22a51af0", "avatarUrl": "/avatars/8f3348f121565bf6c5e1af0e559a43a3.svg", "isPro": false, "fullname": "Yoav HaCohen", "user": "yoavhacohen", "type": "user"}, "name": "Yoav HaCohen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:29.722Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e86", "user": {"_id": "6489c487b9e9258ba065418f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6489c487b9e9258ba065418f/6rzmV3bQ3YxswG6NP2hDW.png", "isPro": false, "fullname": "Benny Brazowski", "user": "benibraz", "type": "user"}, "name": "Benny Brazowski", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:35.981Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e87", "user": {"_id": "62dd30a8d43078cd49ac8ad8", "avatarUrl": "/avatars/ad599719290637f7817b7508a91c2e2c.svg", "isPro": false, "fullname": "Nisan Chiprut", "user": "nisan", "type": "user"}, "name": "Nisan Chiprut", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:41.634Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e88", "user": {"_id": "64a7adc087cbd4dc7301fdd6", "avatarUrl": "/avatars/b4ec4c3a0409af8ec4a5de05db453034.svg", "isPro": false, "fullname": "Yaki Bitterman", "user": "jacobitterman", "type": "user"}, "name": "Yaki Bitterman", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:46.749Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e89", "user": {"_id": "65897258509bcae23fa162c9", "avatarUrl": "/avatars/29d277a0c425c936e25e82e79caa10a4.svg", "isPro": false, "fullname": "Andrew Kvochko", "user": "kvochko", "type": "user"}, "name": "Andrew Kvochko", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:51.722Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8a", "name": "Avishai Berkowitz", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8b", "name": "Daniel Shalem", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8c", "user": {"_id": "681af83e2f4aaa88639e703d", "avatarUrl": "/avatars/d69b664daad0afb529440c14fdb9bc3a.svg", "isPro": false, "fullname": "Daphna Lifschitz", "user": "Daphnal", "type": "user"}, "name": "Daphna Lifschitz", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:04.180Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8d", "user": {"_id": "636b97a57631fe5e86fe1fa2", "avatarUrl": "/avatars/c568ae26fd4fc2655cd12f15d539db58.svg", "isPro": false, "fullname": "Dudu Moshe", "user": "dudumoshe", "type": "user"}, "name": "Dudu Moshe", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:13.512Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8e", "name": "Eitan Porat", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8f", "user": {"_id": "677a422979d3c32a5dd87a0a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WUa6E68GpnT2mEMJ41nDd.png", "isPro": false, "fullname": "Eitan Richardson", "user": "eitanrich", "type": "user"}, "name": "Eitan Richardson", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:22.677Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e90", "user": {"_id": "673f6911d83832a6ce15e7bf", "avatarUrl": "/avatars/0da6cded3b0e785241a6ba5fdb5d8ceb.svg", "isPro": false, "fullname": "Guy Shiran", "user": "guysrn", "type": "user"}, "name": "Guy Shiran", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:28.250Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e91", "user": {"_id": "65744a2fe09de6aa74026d80", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65744a2fe09de6aa74026d80/kCxIKdeBJwAPKmvlm7fDP.jpeg", "isPro": false, "fullname": "Itay Chachy", "user": "ItayChachy", "type": "user"}, "name": "Itay Chachy", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:36.781Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e92", "name": "Jonathan Chetboun", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e93", "user": {"_id": "6678365ac411b340b32d6148", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6678365ac411b340b32d6148/7OhHzbu65pa95eYrAbbLW.jpeg", "isPro": false, "fullname": "Michael Finkelson", "user": "MichaelFinkelson", "type": "user"}, "name": "Michael Finkelson", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:53.574Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e94", "user": {"_id": "6318aa43cb4ca740c4c55651", "avatarUrl": "/avatars/24082c776d284393a5a38a99e5c0bab8.svg", "isPro": false, "fullname": "michael kupchick", "user": "michaellightricks", "type": "user"}, "name": "Michael Kupchick", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:59.945Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e95", "user": {"_id": "673f29b568595672b8d3e90e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673f29b568595672b8d3e90e/4sYADg3mpqMKmJ4fQwaTl.png", "isPro": false, "fullname": "Nir Zabari", "user": "NirZabariLTX", "type": "user"}, "name": "Nir Zabari", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:07.297Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e96", "user": {"_id": "64ae89c043dda9449a1eb1ba", "avatarUrl": "/avatars/12cf3de929d38ddd92cc3f3337dc2ed2.svg", "isPro": false, "fullname": "Nitzan Guetta", "user": "nitzanguetta", "type": "user"}, "name": "Nitzan Guetta", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:14.712Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e97", "name": "Noa Kotler", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e98", "user": {"_id": "631f58935ba8c026340b377c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631f58935ba8c026340b377c/4yoHLdNE99VBb7ji_Mzzj.jpeg", "isPro": false, "fullname": "Ofir Bibi", "user": "ofirbibi", "type": "user"}, "name": "Ofir Bibi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:27.196Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e99", "user": {"_id": "674348b46215a2c0878e219b", "avatarUrl": "/avatars/8a213e431a1583d1a93377410907c059.svg", "isPro": false, "fullname": "Ori Gordon", "user": "origordon", "type": "user"}, "name": "Ori Gordon", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:34.878Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9a", "name": "Poriya Panet", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9b", "name": "Roi Benita", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9c", "name": "Shahar Armon", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9d", "name": "Victor Kulikov", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9e", "name": "Yaron Inger", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9f", "name": "Yonatan Shiftan", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399ea0", "name": "Zeev Melumian", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399ea1", "name": "Zeev Farbman", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cYoXYuK3pjt85pl5-fvUv.mp4"], "publishedAt": "2026-01-06T18:24:41.000Z", "submittedOnDailyAt": "2026-01-07T00:07:29.528Z", "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.", "upvotes": 40, "discussionId": "695dc6d9c03d6d81e4399ea2", "projectPage": "https://app.ltx.studio/ltx-2-playground/i2v", "githubRepo": "https://github.com/Lightricks/LTX-2", "githubRepoAddedBy": "user", "ai_summary": "LTX-2 is an open-source audiovisual diffusion model that generates synchronized video and audio content using a dual-stream transformer architecture with cross-modal attention and classifier-free guidance.", "ai_keywords": ["text-to-video diffusion models", "audiovisual content", "dual-stream transformer", "cross-attention layers", "temporal positional embeddings", "AdaLN", "classifier-free guidance", "modality-aware classifier-free guidance", "multilingual text encoder", "diffusion models"], "githubStars": 922, "summary_zh": "<ul>\n    <li>LTX-2\u662f\u4e00\u4e2a\u5f00\u6e90\u6a21\u578b\uff0c\u53ef\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u540c\u6b65\u7684\u97f3\u89c6\u9891\u5185\u5bb9\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4f7f\u7528\u4e86\u4e00\u4e2a\u5177\u670914\u4ebf\u53c2\u6570\u7684\u89c6\u9891\u6d41\u548c\u4e00\u4e2a\u5177\u67095\u4ebf\u53c2\u6570\u7684\u97f3\u9891\u6d41\u7684\u53cc\u6d41\u53d8\u538b\u5668\u67b6\u6784\u3002</li>\n    <li>LTX-2\u80fd\u591f\u751f\u6210\u7b26\u5408\u573a\u666f\u89d2\u8272\u3001\u73af\u5883\u3001\u98ce\u683c\u548c\u60c5\u611f\u7684\u97f3\u9891\u8f68\u9053\uff0c\u5e76\u5305\u542b\u81ea\u7136\u7684\u80cc\u666f\u97f3\u6548\u3002</li>\n    <li>\u5728\u8bc4\u4f30\u4e2d\uff0cLTX-2\u5728\u97f3\u89c6\u9891\u8d28\u91cf\u548c\u7b26\u5408\u63d0\u793a\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u5f00\u6e90\u7cfb\u7edf\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u65f6\u95f4\u66f4\u4f4e\u3002</li>\n    <li>\u6240\u6709\u6a21\u578b\u6743\u91cd\u548c\u4ee3\u7801\u5df2\u516c\u5f00\u53d1\u5e03\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LTX-2 is a new model that creates high-quality videos with synchronized sound, addressing the lack of audio in previous text-to-video models.</li>\n    <li>It uses a unique architecture with two streams for video and audio, allowing it to generate content efficiently and effectively.</li>\n    <li>The model features advanced techniques for understanding prompts in multiple languages and improving the alignment of audio and video elements.</li>\n    <li>LTX-2 can generate not only speech but also detailed sound effects that match each scene, enhancing the overall experience.</li>\n    <li>The model outperforms other open-source systems in audiovisual quality and is more efficient than proprietary models, with all its resources publicly available.</li>\n</ul>"}, "publishedAt": "2026-01-06T13:24:41.000Z", "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model", "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cYoXYuK3pjt85pl5-fvUv.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03233.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 202, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.01425", "authors": [{"_id": "695c765d6aa73bc11f091402", "name": "Xu Guo", "hidden": false}, {"_id": "695c765d6aa73bc11f091403", "name": "Fulong Ye", "hidden": false}, {"_id": "695c765d6aa73bc11f091404", "name": "Xinghui Li", "hidden": false}, {"_id": "695c765d6aa73bc11f091405", "name": "Pengqi Tu", "hidden": false}, {"_id": "695c765d6aa73bc11f091406", "name": "Pengze Zhang", "hidden": false}, {"_id": "695c765d6aa73bc11f091407", "user": {"_id": "674566cb79d6f3a9da7be0de", "avatarUrl": "/avatars/b6a5384820e150405039aa2b9badac29.svg", "isPro": false, "fullname": "Qichao Sun", "user": "Simons212", "type": "user"}, "name": "Qichao Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:07.336Z", "hidden": false}, {"_id": "695c765d6aa73bc11f091408", "name": "Songtao Zhao", "hidden": false}, {"_id": "695c765d6aa73bc11f091409", "name": "Xiangwang Hou", "hidden": false}, {"_id": "695c765d6aa73bc11f09140a", "name": "Qian He", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/67d50738fed7787297d737d6/0nECkDL67gltfx3htZ82l.mp4"], "publishedAt": "2026-01-04T08:07:11.000Z", "submittedOnDailyAt": "2026-01-06T00:30:06.666Z", "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer", "submittedOnDailyBy": {"_id": "67d50738fed7787297d737d6", "avatarUrl": "/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg", "isPro": false, "fullname": "xuguo", "user": "XuGuo699", "type": "user"}, "summary": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.", "upvotes": 33, "discussionId": "695c765d6aa73bc11f09140b", "projectPage": "https://guoxu1233.github.io/DreamID-V/", "githubRepo": "https://github.com/bytedance/DreamID-V", "githubRepoAddedBy": "user", "ai_summary": "A novel video face swapping framework combines image face swapping techniques with diffusion transformers and curriculum learning to achieve superior identity preservation and visual realism.", "ai_keywords": ["Video Face Swapping", "Image Face Swapping", "diffusion transformer", "Modality-Aware Conditioning", "Synthetic-to-Real Curriculum", "Identity-Coherence Reinforcement Learning", "IDBench-V", "Identity-Anchored Video Synthesizer", "bidirectional ID quadruplets", "multi-model conditions"], "githubStars": 86, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u6362\u8138\u6280\u672f\u9700\u8981\u5c06\u4e00\u4e2a\u4eba\u7684\u8eab\u4efd\u65e0\u7f1d\u5730\u6ce8\u5165\u5230\u76ee\u6807\u89c6\u9891\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u59ff\u52bf\u3001\u8868\u60c5\u3001\u5149\u7167\u548c\u80cc\u666f\u7b49\u4fe1\u606f\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u8eab\u4efd\u76f8\u4f3c\u6027\u548c\u5c5e\u6027\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165SyncID-Pipe\u6570\u636e\u7ba1\u9053\uff0c\u7ed3\u5408\u56fe\u50cf\u6362\u8138\u6a21\u578b\uff0c\u6784\u5efa\u4e86\u53cc\u5411\u7684\u8eab\u4efd\u56db\u5143\u7ec4\u4ee5\u8fdb\u884c\u660e\u786e\u76d1\u7763\u3002</li>\n    <li>\u57fa\u4e8e\u914d\u5bf9\u6570\u636e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86DreamID-V\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u53d8\u6362\u5668\u548c\u6a21\u6001\u611f\u77e5\u6761\u4ef6\u6a21\u5757\u6765\u589e\u5f3a\u591a\u6a21\u6001\u6761\u4ef6\u7684\u6ce8\u5165\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5f15\u5165\u4e86IDBench-V\u57fa\u51c6\uff0c\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660eDreamID-V\u5728\u591a\u79cd\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u80fd\u9002\u5e94\u5404\u79cd\u6362\u8138\u76f8\u5173\u4efb\u52a1\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study focuses on improving Video Face Swapping (VFS) to maintain original video qualities like pose and lighting while swapping identities.</li>\n    <li>The researchers developed a new method called SyncID-Pipe to enhance the training process by combining video and image face swapping techniques.</li>\n    <li>They introduced a novel framework named DreamID-V, which uses a special module to manage different types of data effectively.</li>\n    <li>To improve the results, they implemented strategies for better visual quality and consistent identity in challenging conditions.</li>\n    <li>The team created a new benchmark called IDBench-V to test their method, which showed that DreamID-V outperforms existing techniques and is adaptable for various face-swapping tasks.</li>\n</ul>"}, "publishedAt": "2026-01-04T03:07:11.000Z", "title": "DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer", "summary": "Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/67d50738fed7787297d737d6/0nECkDL67gltfx3htZ82l.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01425.png", "numComments": 2, "submittedBy": {"_id": "67d50738fed7787297d737d6", "avatarUrl": "/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg", "fullname": "xuguo", "name": "XuGuo699", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.03872", "authors": [{"_id": "695f1a475fa3847525c41d06", "user": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "name": "Jinyang Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:32:36.055Z", "hidden": false}, {"_id": "695f1a475fa3847525c41d07", "name": "Guocheng Zhai", "hidden": false}, {"_id": "695f1a475fa3847525c41d08", "name": "Ruihan Jin", "hidden": false}, {"_id": "695f1a475fa3847525c41d09", "name": "Jiahao Yuan", "hidden": false}, {"_id": "695f1a475fa3847525c41d0a", "name": "Yuhao Shen", "hidden": false}, {"_id": "695f1a475fa3847525c41d0b", "name": "Shuai Zhang", "hidden": false}, {"_id": "695f1a475fa3847525c41d0c", "name": "Zhengqi Wen", "hidden": false}, {"_id": "695f1a475fa3847525c41d0d", "name": "Jianhua Tao", "hidden": false}], "publishedAt": "2026-01-07T12:38:33.000Z", "submittedOnDailyAt": "2026-01-08T04:50:03.287Z", "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning", "submittedOnDailyBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "summary": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.", "upvotes": 30, "discussionId": "695f1a475fa3847525c41d0e", "ai_summary": "ATLAS is a dual-path framework that dynamically selects optimal model-tool combinations for cross-domain reasoning through cluster-based routing and reinforcement learning-based multi-step routing, achieving superior performance on complex reasoning tasks.", "ai_keywords": ["large language models", "external tools", "model-tool combination", "high-dimensional optimization", "dual-path framework", "training-free cluster-based routing", "RL-based multi-step routing", "cross-domain complex reasoning", "domain-specific alignment", "out-of-distribution generalization"], "summary_zh": "<ul>\n  <li>\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u5916\u90e8\u5de5\u5177\u7ed3\u5408\uff0c\u589e\u5f3a\u4e86\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u80fd\u529b\u3002</li>\n  <li>\u9009\u62e9\u6700\u4f73\u6a21\u578b\u548c\u5de5\u5177\u7ec4\u5408\u662f\u4e00\u4e2a\u590d\u6742\u7684\u4f18\u5316\u6311\u6218\u3002</li>\n  <li>\u63d0\u51fa\u4e86ATLAS\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u65b9\u6cd5\u52a8\u6001\u4f7f\u7528\u5de5\u5177\u3002</li>\n  <li>ATLAS\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u578b\u548c\u8def\u7531\u65b9\u6cd5\u3002</li>\n  <li>\u5728\u89c6\u89c9\u63a8\u7406\u65b9\u9762\uff0cATLAS\u901a\u8fc7\u534f\u8c03\u591a\u79cd\u5de5\u5177\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) can work better with external tools, but choosing the best combinations can be complicated.</li>\n    <li>Many current methods use just one model or fixed tool logic, missing out on better performance options.</li>\n    <li>ATLAS is a new framework that helps dynamically match tools with LLMs for better reasoning across different domains.</li>\n    <li>ATLAS uses two main methods: one that groups based on existing knowledge and another that learns from experience to improve performance.</li>\n    <li>Tests show that ATLAS outperforms other models and methods, especially in tasks that require visual reasoning.</li>\n</ul>"}, "publishedAt": "2026-01-07T07:38:33.000Z", "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning", "summary": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03872.png", "numComments": 1, "submittedBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "fullname": "Jinyang Wu", "name": "Jinyang23", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\uff0c\u5f53\u524d\u7684\u6570\u636e\u51c6\u5907\u65b9\u6cd5\u9762\u4e34\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DataFlow\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684LLM\u9a71\u52a8\u7684\u6570\u636e\u51c6\u5907\u6846\u67b6\u3002</li>\n    <li>DataFlow\u63d0\u4f9b\u4e86\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u8f6c\u6362\uff0c\u652f\u6301\u8c03\u8bd5\u548c\u4f18\u5316\u7684\u6570\u636e\u6d41\u6784\u5efa\u3002</li>\n    <li>\u901a\u8fc7DataFlow-Agent\uff0c\u7528\u6237\u53ef\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u89c4\u683c\u81ea\u52a8\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u6570\u636e\u6d41\u3002</li>\n    <li>\u5728\u591a\u4e2a\u4f7f\u7528\u6848\u4f8b\u4e2d\uff0cDataFlow\u663e\u8457\u63d0\u9ad8\u4e86LLM\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u6570\u636e\u96c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is a growing need for high-quality data in Large Language Models (LLMs), but current data preparation methods are often inconsistent and hard to reproduce.</li>\n    <li>DataFlow is a new framework designed to improve data preparation for LLMs by allowing for modular and reusable data transformations.</li>\n    <li>It features nearly 200 reusable operators and supports multiple types of data processing, including text, code, and mathematical reasoning.</li>\n    <li>DataFlow-Agent helps users create data pipelines from natural language descriptions, making the process easier and more efficient.</li>\n    <li>Testing shows that DataFlow significantly enhances LLM performance compared to existing datasets, producing better results across various benchmarks.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni \u662f\u4e00\u4e2a\u901a\u7528\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u4e2d\u76f4\u63a5\u5408\u6210\u9ad8\u54c1\u8d28\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u6253\u7834\u4e86\u4ee5\u5f80\u529f\u80fd\u5206\u79bb\u7684\u65b9\u6cd5\u3002</li>\n    <li>Kling-Omni \u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5305\u62ec\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\uff0c\u80fd\u591f\u751f\u6210\u7535\u5f71\u8d28\u91cf\u7684\u89c6\u9891\u5185\u5bb9\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5efa\u7acb\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u6a21\u6001\u89c6\u9891\u521b\u4f5c\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u8fdb\u884c\u4f18\u5316\u3002</li>\n    <li>Kling-Omni \u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new framework for creating high-quality videos from various visual language inputs.</li>\n    <li>It combines video generation, editing, and reasoning into one complete system, unlike other methods that separate these tasks.</li>\n    <li>The framework can handle different types of user inputs like text, images, and videos to create unified video content.</li>\n    <li>Kling-Omni uses a strong data system and advanced training methods to improve video creation and editing.</li>\n    <li>It shows excellent performance in generating content, editing based on reasoning, and following multimodal instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 96, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 79, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u662f\u4e00\u79cd\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5168\u7403\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u7b56\u7565\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u8bbe\u8ba1\u4e3b\u8981\u4f5c\u4e3a\u88ab\u52a8\u5b58\u50a8\uff0c\u5bb9\u6613\u5bfc\u81f4\u63a8\u7406\u788e\u7247\u5316\u548c\u7f3a\u4e4f\u6574\u4f53\u7406\u89e3\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HGMem\uff0c\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u80fd\u591f\u52a8\u6001\u5730\u7ec4\u7ec7\u548c\u5173\u8054\u4fe1\u606f\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u56fe\u8868\u793a\u8bb0\u5fc6\uff0c\u4fc3\u8fdb\u66f4\u9ad8\u9636\u7684\u4e92\u52a8\u548c\u77e5\u8bc6\u7ed3\u6784\u7684\u5f62\u6210\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cHGMem\u5728\u591a\u4e2a\u5168\u7403\u7406\u89e3\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u5f3a\u57fa\u7ebf\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps improve large language models for tasks needing deep understanding and reasoning.</li>\n    <li>Current memory systems mainly store facts passively, missing important connections between them that could enhance reasoning.</li>\n    <li>HGMem is a new memory system that uses a hypergraph structure to create dynamic connections between facts, allowing for better reasoning.</li>\n    <li>This new approach helps build a more integrated knowledge structure that supports deeper reasoning in complex tasks.</li>\n    <li>Tests show that HGMem significantly improves performance on various challenging tasks compared to existing systems.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u7528\u6237\u5e0c\u671b\u5b83\u4eec\u4e0d\u4ec5\u80fd\u63d0\u4f9b\u51c6\u786e\u7684\u56de\u7b54\uff0c\u8fd8\u80fd\u7b26\u5408\u591a\u6837\u7684\u4eba\u7c7b\u504f\u597d\u3002</li>\n    <li>\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5f00\u59cb\u91c7\u7528\u591a\u79cd\u5956\u52b1\u6765\u6307\u5bfc\u6a21\u578b\u884c\u4e3a\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u76f4\u63a5\u5e94\u7528\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u4f1a\u5bfc\u81f4\u5956\u52b1\u7ec4\u5408\u7684\u4f18\u52bf\u503c\u76f8\u540c\uff0c\u4ece\u800c\u964d\u4f4e\u8bad\u7ec3\u4fe1\u53f7\u7684\u5206\u8fa8\u7387\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff1a\u7fa4\u4f53\u5956\u52b1\u89e3\u8026\u5f52\u4e00\u5316\u7b56\u7565\u4f18\u5316\uff08GDPO\uff09\uff0c\u89e3\u51b3\u4e86GRPO\u7684\u95ee\u9898\uff0c\u4fdd\u6301\u4e86\u5956\u52b1\u95f4\u7684\u76f8\u5bf9\u5dee\u5f02\u3002</li>\n    <li>GDPO\u5728\u4e09\u4e2a\u4efb\u52a1\uff08\u5de5\u5177\u8c03\u7528\u3001\u6570\u5b66\u63a8\u7406\u548c\u7f16\u7801\u63a8\u7406\uff09\u4e2d\u8868\u73b0\u4f18\u4e8eGRPO\uff0c\u663e\u793a\u51fa\u5176\u5728\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Users expect language models to give accurate answers and behave according to different human preferences.</li>\n    <li>Reinforcement learning (RL) is using multiple rewards to guide models to meet these preferences.</li>\n    <li>Applying Group Relative Policy Optimization (GRPO) to these rewards can lead to problems, making training less effective.</li>\n    <li>The new method, Group reward-Decoupled Normalization Policy Optimization (GDPO), improves on GRPO by keeping the rewards separate for better training.</li>\n    <li>GDPO has shown better performance in tasks like tool calling, math reasoning, and coding reasoning compared to GRPO.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.00393", "authors": [{"_id": "695b2297832867f253525d68", "user": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "name": "Yuxue Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:23.295Z", "hidden": false}, {"_id": "695b2297832867f253525d69", "user": {"_id": "649ecf9827145c4463240177", "avatarUrl": "/avatars/27696cf31790a3d58d8be2e0c983800e.svg", "isPro": false, "fullname": "Lue Fan", "user": "Abyssaledge", "type": "user"}, "name": "Lue Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T13:49:26.330Z", "hidden": false}, {"_id": "695b2297832867f253525d6a", "user": {"_id": "644cc2c36dfd5f8240d76a52", "avatarUrl": "/avatars/dcd9279af1c6d8535e48dc6e3e6511cd.svg", "isPro": false, "fullname": "Ziqi Shi", "user": "renshengjihe", "type": "user"}, "name": "Ziqi Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:21.077Z", "hidden": false}, {"_id": "695b2297832867f253525d6b", "name": "Junran Peng", "hidden": false}, {"_id": "695b2297832867f253525d6c", "name": "Feng Wang", "hidden": false}, {"_id": "695b2297832867f253525d6d", "name": "Zhaoxiang Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "publishedAt": "2026-01-01T17:07:30.000Z", "submittedOnDailyAt": "2026-01-05T02:49:46.994Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "submittedOnDailyBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "upvotes": 83, "discussionId": "695b2297832867f253525d6e", "projectPage": "https://neoverse-4d.github.io/", "githubRepo": "https://github.com/IamCreateAI/NeoVerse", "githubRepoAddedBy": "user", "ai_summary": "NeoVerse is a scalable 4D world model that enables pose-free reconstruction and novel-trajectory video generation from monocular videos with state-of-the-art performance.", "ai_keywords": ["4D world model", "4D reconstruction", "novel-trajectory video generation", "monocular videos", "pose-free", "feed-forward", "degradation pattern simulation"], "githubStars": 107, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86NeoVerse\uff0c\u4e00\u4e2a\u591a\u529f\u80fd\u76844D\u4e16\u754c\u6a21\u578b\u3002</li>\n    <li>\u89e3\u51b3\u4e86\u73b0\u67094D\u5efa\u6a21\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u5171\u540c\u9650\u5236\u3002</li>\n    <li>NeoVerse\u652f\u6301\u65e0\u59ff\u6001\u7684\u524d\u99884D\u91cd\u5efa\u548c\u5728\u7ebf\u5355\u76ee\u964d\u7ea7\u6a21\u5f0f\u6a21\u62df\u3002</li>\n    <li>\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u5728\u591a\u4e2a\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n    <li>\u5728\u6807\u51c6\u91cd\u5efa\u548c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NeoVerse is a new 4D world model for various tasks like 4D reconstruction and video generation.</li>\n    <li>It addresses scalability issues found in current 4D modeling methods that require expensive data or complex training.</li>\n    <li>NeoVerse can work with regular monocular videos without needing special setups.</li>\n    <li>It includes advanced features like pose-free reconstruction and online simulation of degradation patterns.</li>\n    <li>NeoVerse performs very well in standard tests for reconstruction and generation.</li>\n</ul>"}, "publishedAt": "2026-01-01T12:07:30.000Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00393.png", "numComments": 1, "submittedBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "fullname": "Yuxue Yang", "name": "Yuppie1204", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24615", "authors": [{"_id": "69564d96832867f2535257af", "user": {"_id": "622b00a776c20fee5d14501b", "avatarUrl": "/avatars/e00496dda1e309548e7b5b437839bb65.svg", "isPro": false, "fullname": "Eason shi", "user": "Easonshi", "type": "user"}, "name": "Yuchen Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:50.111Z", "hidden": false}, {"_id": "69564d96832867f2535257b0", "user": {"_id": "66e258bdc70c02b46dfed6e3", "avatarUrl": "/avatars/ccc2d604616c018f45a268a610472cac.svg", "isPro": false, "fullname": "Yuzheng Cai", "user": "Ucreate", "type": "user"}, "name": "Yuzheng Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:50.884Z", "hidden": false}, {"_id": "69564d96832867f2535257b1", "name": "Siqi Cai", "hidden": false}, {"_id": "69564d96832867f2535257b2", "name": "Zihan Xu", "hidden": false}, {"_id": "69564d96832867f2535257b3", "user": {"_id": "64154bfa385a75d7790f80e8", "avatarUrl": "/avatars/9e22f54b5eb7c4ebedad99a9a92c4b6a.svg", "isPro": false, "fullname": "Lichao Chen", "user": "nth233", "type": "user"}, "name": "Lichao Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:46.825Z", "hidden": false}, {"_id": "69564d96832867f2535257b4", "user": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "isPro": false, "fullname": "Yulei Qin", "user": "yolay", "type": "user"}, "name": "Yulei Qin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:48.064Z", "hidden": false}, {"_id": "69564d96832867f2535257b5", "name": "Zhijian Zhou", "hidden": false}, {"_id": "69564d96832867f2535257b6", "name": "Xiang Fei", "hidden": false}, {"_id": "69564d96832867f2535257b7", "user": {"_id": "6604e43869c47cd78fdebd08", "avatarUrl": "/avatars/4c11f5e1aeae3c5eb213f6ec6d5bfe72.svg", "isPro": false, "fullname": "Qiu", "user": "ChaofanDFG", "type": "user"}, "name": "Chaofan Qiu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:48.910Z", "hidden": false}, {"_id": "69564d96832867f2535257b8", "user": {"_id": "637af0a7bdf7309aa6da1c36", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637af0a7bdf7309aa6da1c36/NHZ-09otVCfbpXVxm8f-e.png", "isPro": false, "fullname": "Xiaoyu Tan", "user": "WIlliam1900", "type": "user"}, "name": "Xiaoyu Tan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:52.763Z", "hidden": false}, {"_id": "69564d96832867f2535257b9", "name": "Gang Li", "hidden": false}, {"_id": "69564d96832867f2535257ba", "name": "Zongyi Li", "hidden": false}, {"_id": "69564d96832867f2535257bb", "name": "Haojia Lin", "hidden": false}, {"_id": "69564d96832867f2535257bc", "name": "Guocan Cai", "hidden": false}, {"_id": "69564d96832867f2535257bd", "name": "Yong Mao", "hidden": false}, {"_id": "69564d96832867f2535257be", "name": "Yunsheng Wu", "hidden": false}, {"_id": "69564d96832867f2535257bf", "name": "Ke Li", "hidden": false}, {"_id": "69564d96832867f2535257c0", "user": {"_id": "647401e50da364bd0d002f2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png", "isPro": false, "fullname": "XING SUN", "user": "tedsun", "type": "user"}, "name": "Xing Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:39.390Z", "hidden": false}], "publishedAt": "2025-12-31T04:17:36.000Z", "submittedOnDailyAt": "2026-01-05T00:21:56.456Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "submittedOnDailyBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "isPro": false, "fullname": "Ke Li", "user": "tristanli", "type": "user"}, "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "upvotes": 82, "discussionId": "69564d96832867f2535257c1", "projectPage": "https://tencentcloudadp.github.io/youtu-agent/", "githubRepo": "https://github.com/TencentCloudADP/youtu-agent", "githubRepoAddedBy": "user", "githubStars": 4095, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u6846\u67b6\u9762\u4e34\u9ad8\u914d\u7f6e\u6210\u672c\u548c\u9759\u6001\u80fd\u529b\u7684\u6311\u6218\u3002</li>\n    <li>Youtu-Agent \u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u65e8\u5728\u81ea\u52a8\u751f\u6210\u548c\u6301\u7eed\u8fdb\u5316 LLM \u4ee3\u7406\u3002</li>\n    <li>\u5b83\u91c7\u7528\u7ed3\u6784\u5316\u914d\u7f6e\u7cfb\u7edf\uff0c\u652f\u6301\u7075\u6d3b\u91cd\u7528\u548c\u81ea\u52a8\u5408\u6210\uff0c\u5206\u4e3a\u5de5\u4f5c\u6d41\u6a21\u5f0f\u548c\u5143\u4ee3\u7406\u6a21\u5f0f\u3002</li>\n    <li>Youtu-Agent \u5305\u542b\u4ee3\u7406\u5b9e\u8df5\u6a21\u5757\u548c\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u6a21\u5757\uff0c\u4ee5\u63d0\u9ad8\u4ee3\u7406\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cYoutu-Agent \u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u81ea\u52a8\u751f\u6210\u5de5\u5177\u7684\u6210\u529f\u7387\u8d85\u8fc7 81%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current LLM agent frameworks are costly to configure and cannot easily adapt to changes.</li>\n    <li>Youtu-Agent is a new, modular framework that allows for automatic creation and ongoing improvement of LLM agents.</li>\n    <li>It includes two modes: Workflow for simple tasks and Meta-Agent for complex tasks, which can create necessary tools and settings automatically.</li>\n    <li>Youtu-Agent has a unique system that helps agents learn from experience and improve their performance without needing updates.</li>\n    <li>Tests show that Youtu-Agent performs very well on various benchmarks and improves efficiency and capabilities significantly.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:17:36.000Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24615.png", "numComments": 1, "submittedBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "fullname": "Ke Li", "name": "tristanli", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u89c6\u89c9\u4e0a\u5f88\u771f\u5b9e\uff0c\u4f46\u5176\u4f5c\u4e3a\u4e16\u754c\u6a21\u62df\u5668\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u5176\u662f\u5426\u6355\u6349\u5230\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7ea6\u675f\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u6307\u6807\uff08\u5982FVD\uff09\u4fa7\u91cd\u4e8e\u611f\u77e5\u8d28\u91cf\uff0c\u5ffd\u89c6\u4e86\u63a8\u7406\u5931\u8d25\uff0c\u5982\u56e0\u679c\u5173\u7cfb\u548c\u7269\u7406\u89c4\u5f8b\u7684\u8fdd\u53cd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MMGR\u6846\u67b6\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\uff08\u7269\u7406\u3001\u903b\u8f91\u30013D\u7a7a\u95f4\u30012D\u7a7a\u95f4\u548c\u65f6\u95f4\uff09\u8fdb\u884c\u8bc4\u4f30\u3002</li>\n    <li>MMGR\u8bc4\u4f30\u4e09\u4e2a\u9886\u57df\u7684\u751f\u6210\u63a8\u7406\uff0c\u5305\u62ec\u62bd\u8c61\u63a8\u7406\u3001\u5177\u8eab\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\uff0c\u4f7f\u7528\u7ec6\u81f4\u7684\u6307\u6807\u8981\u6c42\u89c6\u9891\u548c\u56fe\u50cf\u751f\u6210\u7684\u6574\u4f53\u6b63\u786e\u6027\u3002</li>\n    <li>\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6a21\u578b\u5728\u7269\u7406\u5e38\u8bc6\u4efb\u52a1\u4e0a\u8868\u73b0\u4e2d\u7b49\uff0c\u4f46\u5728\u62bd\u8c61\u63a8\u7406\u4e0a\u8868\u73b0\u8f83\u5dee\uff08\u5728ARC-AGI\u4e0a\u7684\u51c6\u786e\u7387\u4f4e\u4e8e10%\uff09\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video models can create realistic videos, but we need to check if they understand the rules of the world, like physics and logic.</li>\n    <li>Current evaluation methods focus too much on how videos look and not enough on whether the content makes sense or follows real-world rules.</li>\n    <li>We developed a new evaluation framework called MMGR, which tests five reasoning skills: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>\n    <li>MMGR assesses video models and image models in different areas, revealing that many models perform well in some tasks but poorly in others, especially in abstract reasoning.</li>\n    <li>This analysis shows that many existing models depend too much on visual appearance rather than on understanding the actual rules of the world.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u79d1\u5b66\u901a\u7528\u667a\u80fd\uff08SGI\uff09\u5c1a\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\uff0c\u6d89\u53ca\u81ea\u4e3b\u6784\u601d\u3001\u7814\u7a76\u548c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u63d0\u51fa\u57fa\u4e8e\u5b9e\u9645\u63a2\u7a76\u6a21\u578b\uff08PIM\uff09\u7684SGI\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u4e0e\u79d1\u5b66\u5bb6\u76f8\u5173\u7684\u4efb\u52a1\u8fdb\u884c\u64cd\u4f5c\u3002</li>\n    <li>SGI-Bench\u5305\u542b1000\u591a\u4e2a\u8de8\u5b66\u79d1\u6837\u672c\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u6700\u65b0\u7684\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u663e\u793a\u5728\u6df1\u5ea6\u7814\u7a76\u4e2d\u51c6\u786e\u5339\u914d\u7387\u4f4e\uff0c\u60f3\u6cd5\u7f3a\u4e4f\u53ef\u884c\u6027\u548c\u7ec6\u8282\u3002</li>\n    <li>\u5f15\u5165\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\uff0c\u4f18\u5316\u63a8\u7406\u4e2d\u7684\u65b0\u9896\u6027\u5956\u52b1\uff0c\u63d0\u5347\u5047\u8bbe\u7684\u65b0\u9896\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>There is currently no clear framework for Scientific General Intelligence (SGI), which is the ability of AI to independently think and work on scientific problems.</li>\n  <li>The authors propose a new definition of SGI based on a model that includes steps like thinking, generating ideas, and conducting experiments.</li>\n  <li>They created SGI-Bench, a benchmark with over 1,000 examples from various scientific fields to test AI's capabilities, based on significant scientific questions.</li>\n  <li>Results showed that AI struggles with deep research accuracy, idea feasibility, and executing experimental protocols effectively.</li>\n  <li>The authors introduced a new method called Test-Time Reinforcement Learning (TTRL) to improve AI's ability to generate novel hypotheses during testing.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20619", "authors": [{"_id": "694b614d746a34b55dd53d1a", "name": "Jianhong Bai", "hidden": false}, {"_id": "694b614d746a34b55dd53d1b", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "694b614d746a34b55dd53d1c", "name": "Xintao Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1d", "name": "Fu Xiao", "hidden": false}, {"_id": "694b614d746a34b55dd53d1e", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1f", "name": "Qinghe Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d20", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "694b614d746a34b55dd53d21", "name": "Menghan Xia", "hidden": false}, {"_id": "694b614d746a34b55dd53d22", "name": "Zuozhu Liu", "hidden": false}, {"_id": "694b614d746a34b55dd53d23", "name": "Haoji Hu", "hidden": false}, {"_id": "694b614d746a34b55dd53d24", "name": "Pengfei Wan", "hidden": false}, {"_id": "694b614d746a34b55dd53d25", "name": "Kun Gai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "publishedAt": "2025-12-23T18:59:56.000Z", "submittedOnDailyAt": "2025-12-24T01:20:51.117Z", "title": "SemanticGen: Video Generation in Semantic Space", "submittedOnDailyBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "isPro": false, "fullname": "Jianhong Bai", "user": "jianhongbai", "type": "user"}, "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "upvotes": 77, "discussionId": "694b614d746a34b55dd53d26", "projectPage": "https://jianhongbai.github.io/SemanticGen/", "ai_summary": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.", "ai_keywords": ["VAE space", "VAE decoder", "semantic space", "diffusion model", "semantic video features", "bi-directional attention"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u4f20\u7edf\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u901a\u8fc7VAE\u7a7a\u95f4\u5b66\u4e60\u89c6\u9891\u7279\u5f81\uff0c\u4f46\u751f\u6210\u957f\u89c6\u9891\u65f6\u901f\u5ea6\u6162\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002</li>\n    <li>SemanticGen\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u89c6\u9891\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u8fd9\u4e9b\u7279\u5f81\u751f\u6210\u6700\u7ec8\u89c6\u9891\u3002</li>\n    <li>\u5728\u8bed\u4e49\u7a7a\u95f4\u751f\u6210\u89c6\u9891\u6bd4\u5728VAE\u6f5c\u5728\u7a7a\u95f4\u751f\u6210\u89c6\u9891\u6536\u655b\u66f4\u5feb\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSemanticGen\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u4e14\u4f18\u4e8e\u73b0\u6709\u7684\u5148\u8fdb\u65b9\u6cd5\u548c\u57fa\u51c6\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>SemanticGen is a new video generation method that improves on traditional models.</li>\n    <li>It starts by creating a general layout of the video in a compact semantic space, rather than directly working with low-level details.</li>\n    <li>The process has two stages: first, it generates semantic features, then it creates detailed video content based on those features.</li>\n    <li>This approach allows for faster video generation and better performance, especially for longer videos.</li>\n    <li>Experiments show that SemanticGen produces high-quality videos and surpasses existing methods.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:56.000Z", "title": "SemanticGen: Video Generation in Semantic Space", "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png", "numComments": 2, "submittedBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "fullname": "Jianhong Bai", "name": "jianhongbai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.15431", "authors": [{"_id": "69437417542d62d58a7bf6c4", "name": "Haolong Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6c5", "name": "Jia Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c6", "name": "Xin Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c7", "name": "Yeqing Shen", "hidden": false}, {"_id": "69437417542d62d58a7bf6c8", "user": {"_id": "653614073f4248157d60ccdc", "avatarUrl": "/avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg", "isPro": false, "fullname": "mengziyang", "user": "zylate", "type": "user"}, "name": "Ziyang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:53.033Z", "hidden": false}, {"_id": "69437417542d62d58a7bf6c9", "name": "Zhimin Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6ca", "name": "Kaijun Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6cb", "name": "Jin Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf6cc", "name": "Lieyu Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6cd", "name": "Mi Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6ce", "name": "Shiliang Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6cf", "name": "Zhirui Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d0", "name": "Brian Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d1", "name": "Kang An", "hidden": false}, {"_id": "69437417542d62d58a7bf6d2", "name": "Chenyang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d3", "name": "Lei Lei", "hidden": false}, {"_id": "69437417542d62d58a7bf6d4", "name": "Mengmeng Duan", "hidden": false}, {"_id": "69437417542d62d58a7bf6d5", "name": "Danxun Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d6", "name": "Guodong Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d7", "name": "Hang Cheng", "hidden": false}, {"_id": "69437417542d62d58a7bf6d8", "name": "Hao Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d9", "name": "Jie Dong", "hidden": false}, {"_id": "69437417542d62d58a7bf6da", "name": "Junhao Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6db", "name": "Mei Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6dc", "name": "Renjie Yu", "hidden": false}, {"_id": "69437417542d62d58a7bf6dd", "name": "Shunshan Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6de", "name": "Xu Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf6df", "name": "Yiting Dai", "hidden": false}, {"_id": "69437417542d62d58a7bf6e0", "name": "Yineng Deng", "hidden": false}, {"_id": "69437417542d62d58a7bf6e1", "name": "Yingdan Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6e2", "name": "Zelin Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6e3", "name": "Wen Sun", "hidden": false}, {"_id": "69437417542d62d58a7bf6e4", "name": "Chengxu Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e5", "name": "Chunqin Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6e6", "name": "Dong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6e7", "name": "Fengqiong Xiao", "hidden": false}, {"_id": "69437417542d62d58a7bf6e8", "name": "Guanghao Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e9", "name": "Guopeng Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ea", "name": "Guozhen Peng", "hidden": false}, {"_id": "69437417542d62d58a7bf6eb", "name": "Hongbing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ec", "name": "Hang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ed", "name": "Hongming Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6ee", "name": "Jingjing Xie", "hidden": false}, {"_id": "69437417542d62d58a7bf6ef", "name": "Jianyong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6f0", "name": "Jingyang Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf6f1", "name": "Jiaju Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f2", "name": "Jiayu Yuan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f3", "name": "Jianpeng Yin", "hidden": false}, {"_id": "69437417542d62d58a7bf6f4", "name": "Kai Cao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f5", "name": "Liang Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f6", "name": "Liguo Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f7", "name": "Liying Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6f8", "name": "Mengqiang Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f9", "name": "Min Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fa", "name": "Manjiao Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fb", "name": "Mao Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf6fc", "name": "Mingxin Wan", "hidden": false}, {"_id": "69437417542d62d58a7bf6fd", "name": "Na Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6fe", "name": "Nan Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6ff", "name": "Ning Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf700", "name": "Peiyao Ma", "hidden": false}, {"_id": "69437417542d62d58a7bf701", "name": "Qingzhou Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf702", "name": "Qiao Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf703", "name": "Qinlin Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf704", "name": "Qiong Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf705", "name": "Qiongyao Li", "hidden": false}, {"_id": "69437417542d62d58a7bf706", "name": "Shangwu Zhong", "hidden": false}, {"_id": "69437417542d62d58a7bf707", "name": "Shuli Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf708", "name": "Shaofan Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf709", "name": "Shisi Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf70a", "name": "Shuang Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf70b", "name": "Xingbin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70c", "name": "Xiaojia Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70d", "name": "Xiaojie Hou", "hidden": false}, {"_id": "69437417542d62d58a7bf70e", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70f", "name": "Xuanti Feng", "hidden": false}, {"_id": "69437417542d62d58a7bf710", "name": "Xuedan Cai", "hidden": false}, {"_id": "69437417542d62d58a7bf711", "name": "Xuan Wen", "hidden": false}, {"_id": "69437417542d62d58a7bf712", "name": "Xianwei Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf713", "name": "Xin Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf714", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf715", "name": "Xin Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf716", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf717", "name": "Yukang Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf718", "name": "Yunfang Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf719", "name": "Yuqing Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf71a", "name": "Yixun Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf71b", "name": "Zejia Weng", "hidden": false}, {"_id": "69437417542d62d58a7bf71c", "name": "Zhonghao Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf71d", "name": "Zhiguo Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf71e", "name": "Zhuoyu Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf71f", "name": "Zheng Ge", "hidden": false}, {"_id": "69437417542d62d58a7bf720", "name": "Jing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf721", "name": "Yibo Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf722", "name": "Binxing Jiao", "hidden": false}, {"_id": "69437417542d62d58a7bf723", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf724", "name": "Daxin Jiang", "hidden": false}], "publishedAt": "2025-12-17T13:26:30.000Z", "submittedOnDailyAt": "2025-12-18T00:55:26.804Z", "title": "Step-GUI Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "upvotes": 77, "discussionId": "69437418542d62d58a7bf725", "projectPage": "https://opengelab.github.io/", "githubRepo": "https://github.com/stepfun-ai/gelab-zero", "githubRepoAddedBy": "user", "ai_summary": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.", "ai_keywords": ["multimodal large language models", "GUI automation", "self-evolving training pipeline", "Calibrated Step Reward System", "trajectory-level calibration", "Step-GUI", "GUI performance", "GUI-MCP", "Model Context Protocol", "AndroidWorld", "OSWorld", "ScreenShot-Pro", "AndroidDaily", "real-world mobile usage patterns", "hierarchical architecture", "low-level atomic operations", "high-level task delegation", "local specialist models", "high-privacy execution"], "githubStars": 1417, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\uff0c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u5c55\u4e3a\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u81ea\u52a8\u5316\u5e26\u6765\u4e86\u65b0\u7684\u673a\u9047\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6211\u8fdb\u5316\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u901a\u8fc7\u6821\u51c6\u6b65\u9aa4\u5956\u52b1\u7cfb\u7edf\uff0c\u63d0\u9ad8\u8bad\u7ec3\u6570\u636e\u7684\u8d28\u91cf\u548c\u6ce8\u91ca\u7684\u53ef\u9760\u6027\uff0c\u6210\u672c\u964d\u4f4e10-100\u500d\uff0c\u51c6\u786e\u7387\u8d85\u8fc790%\u3002</li>\n    <li>\u57fa\u4e8e\u6b64\u6d41\u7a0b\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86Step-GUI\u6a21\u578b\u7cfb\u5217\uff0c\u8fbe\u5230\u884c\u4e1a\u9886\u5148\u7684GUI\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u901a\u7528\u80fd\u529b\u3002</li>\n    <li>\u4e3a\u4e86\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u548c\u8bbe\u5907\u95f4\u7684\u6807\u51c6\u5316\u63a5\u53e3\uff0c\u6211\u4eec\u63d0\u51fa\u4e86GUI-MCP\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8eGUI\u81ea\u52a8\u5316\u7684\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u521b\u5efa\u4e86AndroidDaily\u57fa\u51c6\uff0c\u57fa\u4e8e\u771f\u5b9e\u7684\u79fb\u52a8\u4f7f\u7528\u6a21\u5f0f\uff0c\u8bc4\u4f30\u4ee3\u7406\u5728\u65e5\u5e38\u4f7f\u7528\u4e2d\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New methods in training models for GUI automation can create high-quality data efficiently and reliably.</li>\n    <li>We developed a system called Step-GUI that achieves top performance in GUI tasks while being cost-effective.</li>\n    <li>To ensure user privacy, we introduced GUI-MCP, a protocol that keeps sensitive data on the device while managing tasks.</li>\n    <li>We created a benchmark called AndroidDaily to test how well these agents perform in real-life scenarios with many everyday actions.</li>\n    <li>Our research enhances the ability of GUI agents to work effectively in daily digital interactions.</li>\n</ul>"}, "publishedAt": "2025-12-17T08:26:30.000Z", "title": "Step-GUI Technical Report", "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15431.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Jan 10, 2026";
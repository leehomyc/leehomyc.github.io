window.trendingPapers = {
    "today": [{"paper": {"id": "2601.20833", "authors": [{"_id": "697b9192a67238fac88cbee8", "name": "Tengyue Xu", "hidden": false}, {"_id": "697b9192a67238fac88cbee9", "name": "Zhuoyang Qian", "hidden": false}, {"_id": "697b9192a67238fac88cbeea", "name": "Gaoge Liu", "hidden": false}, {"_id": "697b9192a67238fac88cbeeb", "name": "Li Ling", "hidden": false}, {"_id": "697b9192a67238fac88cbeec", "name": "Zhentao Zhang", "hidden": false}, {"_id": "697b9192a67238fac88cbeed", "name": "Biao Wu", "hidden": false}, {"_id": "697b9192a67238fac88cbeee", "name": "Shuo Zhang", "hidden": false}, {"_id": "697b9192a67238fac88cbeef", "name": "Ke Lu", "hidden": false}, {"_id": "697b9192a67238fac88cbef0", "name": "Wei Shi", "hidden": false}, {"_id": "697b9192a67238fac88cbef1", "name": "Ziqi Wang", "hidden": false}, {"_id": "697b9192a67238fac88cbef2", "name": "Zheng Feng", "hidden": false}, {"_id": "697b9192a67238fac88cbef3", "name": "Yan Luo", "hidden": false}, {"_id": "697b9192a67238fac88cbef4", "name": "Shu Xu", "hidden": false}, {"_id": "697b9192a67238fac88cbef5", "name": "Yongjin Chen", "hidden": false}, {"_id": "697b9192a67238fac88cbef6", "name": "Zhibo Feng", "hidden": false}, {"_id": "697b9192a67238fac88cbef7", "name": "Zhuo Chen", "hidden": false}, {"_id": "697b9192a67238fac88cbef8", "name": "Bruce Yuan", "hidden": false}, {"_id": "697b9192a67238fac88cbef9", "name": "Harry Wang", "hidden": false}, {"_id": "697b9192a67238fac88cbefa", "name": "Kris Chen", "hidden": false}], "publishedAt": "2026-01-28T18:31:54.000Z", "submittedOnDailyAt": "2026-01-30T03:32:00.106Z", "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives", "submittedOnDailyBy": {"_id": "62baa0d6dd02fbf607ce97be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62baa0d6dd02fbf607ce97be/V0I6pANlLEf2YDd9ZLZgi.jpeg", "isPro": false, "fullname": "Wendy", "user": "Wendy-Fly", "type": "user"}, "summary": "Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.", "upvotes": 113, "discussionId": "697b9192a67238fac88cbefb", "githubRepo": "https://github.com/AgentAlphaAGI/Idea2Paper", "githubRepoAddedBy": "user", "ai_summary": "Offline knowledge construction through structured methodological graphs enables more reliable and scalable autonomous scientific discovery by reducing reliance on real-time literature processing.", "ai_keywords": ["large language model", "autonomous scientific discovery", "runtime-centric execution", "context window limitations", "hallucination", "pre-computation-driven framework", "peer-reviewed papers", "research patterns", "methodological knowledge graph", "end-to-end research workflows"], "githubStars": 54, "organization": {"_id": "69542731e1200d74c1c053d1", "name": "AgentAlphaAGI", "fullname": "AgentAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b78eb76ab5d14ca7faac87/TbMZ3y00APtRzHEfTSR7I.jpeg"}, "summary_zh": "<ul>\n    <li>\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u7684\u7814\u7a76\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u80fd\u591f\u81ea\u52a8\u5316\u6574\u4e2a\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u5728\u7ebf\u63a8\u7406\uff0c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u4e0d\u53ef\u9760\u7684\u63a8\u7406\u7ed3\u679c\u3002</li>\n    <li>\u63d0\u51fa\u4e86Idea2Story\u6846\u67b6\uff0c\u8f6c\u5411\u79bb\u7ebf\u77e5\u8bc6\u6784\u5efa\uff0c\u63d0\u9ad8\u6587\u732e\u7406\u89e3\u80fd\u529b\u3002</li>\n    <li>Idea2Story\u901a\u8fc7\u6536\u96c6\u540c\u884c\u8bc4\u5ba1\u8bba\u6587\u53ca\u53cd\u9988\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u7684\u77e5\u8bc6\u56fe\u8c31\u3002</li>\n    <li>\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7814\u7a76\u6a21\u5f0f\uff0c\u63d0\u4f9b\u53ef\u9760\u7684\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u57fa\u7840\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent advancements in autonomous scientific discovery using large language models (LLMs) have improved research workflows.</li>\n    <li>Current systems rely on online processing, which is costly and can lead to errors due to limitations in understanding context.</li>\n    <li>The proposed Idea2Story framework focuses on building knowledge offline, organizing research into a structured graph.</li>\n    <li>This approach allows for better retrieval and reuse of established research methods, reducing the need for extensive online reasoning.</li>\n    <li>Preliminary studies show that Idea2Story can create coherent and innovative research patterns effectively.</li>\n</ul>"}, "publishedAt": "2026-01-28T13:31:54.000Z", "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives", "summary": "Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20833.png", "numComments": 1, "submittedBy": {"_id": "62baa0d6dd02fbf607ce97be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62baa0d6dd02fbf607ce97be/V0I6pANlLEf2YDd9ZLZgi.jpeg", "fullname": "Wendy", "name": "Wendy-Fly", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "69542731e1200d74c1c053d1", "name": "AgentAlphaAGI", "fullname": "AgentAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b78eb76ab5d14ca7faac87/TbMZ3y00APtRzHEfTSR7I.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20354", "authors": [{"_id": "697c1857a67238fac88cc06e", "user": {"_id": "656c9cfef7be0986b49934ea", "avatarUrl": "/avatars/2030e77c28fb4c518b692cd9a20de665.svg", "isPro": false, "fullname": "MuMing", "user": "ZengbinWang", "type": "user"}, "name": "Zengbin Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T09:36:35.067Z", "hidden": false}, {"_id": "697c1857a67238fac88cc06f", "name": "Xuecai Hu", "hidden": false}, {"_id": "697c1857a67238fac88cc070", "name": "Yong Wang", "hidden": false}, {"_id": "697c1857a67238fac88cc071", "name": "Feng Xiong", "hidden": false}, {"_id": "697c1857a67238fac88cc072", "name": "Man Zhang", "hidden": false}, {"_id": "697c1857a67238fac88cc073", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-28T08:15:00.000Z", "submittedOnDailyAt": "2026-01-30T05:01:51.614Z", "title": "Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.", "upvotes": 96, "discussionId": "697c1857a67238fac88cc074", "githubRepo": "https://github.com/AMAP-ML/SpatialGenEval", "githubRepoAddedBy": "user", "ai_summary": "A new benchmark and dataset are introduced to evaluate and improve spatial reasoning capabilities in text-to-image models through information-dense prompts and fine-tuning.", "ai_keywords": ["text-to-image models", "spatial intelligence", "benchmark", "long prompts", "information-dense prompts", "spatial reasoning", "Stable Diffusion-XL", "Uniworld-V1", "OmniGen2", "fine-tuning"], "githubStars": 93, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u65f6\u5e38\u5e38\u5931\u8d25\u3002</li>\n    <li>\u672c\u7814\u7a76\u5f15\u5165\u4e86SpatialGenEval\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30T2I\u6a21\u578b\u7684\u7a7a\u95f4\u667a\u80fd\uff0c\u5305\u62ec1,230\u4e2a\u957f\u7684\u3001\u4fe1\u606f\u5bc6\u96c6\u7684\u63d0\u793a\uff0c\u8986\u76d625\u4e2a\u73b0\u5b9e\u573a\u666f\u3002</li>\n    <li>\u8fd9\u4e9b\u63d0\u793a\u5305\u542b10\u4e2a\u7a7a\u95f4\u5b50\u9886\u57df\u548c\u76f8\u5e94\u7684\u591a\u9879\u9009\u62e9\u95ee\u7b54\uff0c\u6d89\u53ca\u7269\u4f53\u4f4d\u7f6e\u3001\u5e03\u5c40\u3001\u906e\u6321\u548c\u56e0\u679c\u5173\u7cfb\u7b49\u5185\u5bb9\u3002</li>\n    <li>\u5bf921\u4e2a\u5148\u8fdb\u6a21\u578b\u7684\u8bc4\u4f30\u53d1\u73b0\uff0c\u9ad8\u9636\u7a7a\u95f4\u63a8\u7406\u4ecd\u7136\u662f\u4e3b\u8981\u74f6\u9888\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u6784\u5efa\u4e86SpatialT2I\u6570\u636e\u96c6\uff0c\u5305\u542b15,400\u5bf9\u6587\u672c-\u56fe\u50cf\uff0c\u786e\u4fdd\u56fe\u50cf\u4e00\u81f4\u6027\u5e76\u4fdd\u6301\u4fe1\u606f\u5bc6\u5ea6\uff0c\u7ecf\u8fc7\u5fae\u8c03\u540e\u5728\u73b0\u6709\u57fa\u7840\u6a21\u578b\u4e0a\u53d6\u5f97\u4e86\u6027\u80fd\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Text-to-image models are good at creating images but struggle with understanding complex spatial relationships.</li>\n    <li>The new benchmark called SpatialGenEval has 1,230 detailed prompts to test how well these models handle spatial intelligence.</li>\n    <li>SpatialGenEval evaluates 21 advanced models, showing that they still have problems with higher-order spatial reasoning.</li>\n    <li>A new dataset, SpatialT2I, was created with 15,400 text-image pairs to improve model performance while keeping the information rich.</li>\n    <li>Fine-tuning models on this dataset led to better results and more realistic spatial interactions in generated images.</li>\n</ul>"}, "publishedAt": "2026-01-28T03:15:00.000Z", "title": "Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models", "summary": "Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20354.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.21204", "authors": [{"_id": "697c3801a67238fac88cc1b1", "name": "Hong Liu", "hidden": false}, {"_id": "697c3801a67238fac88cc1b2", "name": "Jiaqi Zhang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b3", "name": "Chao Wang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b4", "name": "Xing Hu", "hidden": false}, {"_id": "697c3801a67238fac88cc1b5", "name": "Linkun Lyu", "hidden": false}, {"_id": "697c3801a67238fac88cc1b6", "name": "Jiaqi Sun", "hidden": false}, {"_id": "697c3801a67238fac88cc1b7", "name": "Xurui Yang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b8", "name": "Bo Wang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b9", "name": "Fengcun Li", "hidden": false}, {"_id": "697c3801a67238fac88cc1ba", "name": "Yulei Qian", "hidden": false}, {"_id": "697c3801a67238fac88cc1bb", "name": "Lingtong Si", "hidden": false}, {"_id": "697c3801a67238fac88cc1bc", "name": "Yerui Sun", "hidden": false}, {"_id": "697c3801a67238fac88cc1bd", "name": "Rumei Li", "hidden": false}, {"_id": "697c3801a67238fac88cc1be", "name": "Peng Pei", "hidden": false}, {"_id": "697c3801a67238fac88cc1bf", "name": "Yuchen Xie", "hidden": false}, {"_id": "697c3801a67238fac88cc1c0", "name": "Xunliang Cai", "hidden": false}], "publishedAt": "2026-01-29T03:11:19.000Z", "submittedOnDailyAt": "2026-01-30T02:18:11.112Z", "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.", "upvotes": 76, "discussionId": "697c3801a67238fac88cc1c1", "ai_summary": "Embedding scaling offers superior sparsity scaling compared to expert scaling in large language models, enabling efficient inference through system optimizations and speculative decoding.", "ai_keywords": ["Mixture-of-Experts", "sparsity scaling", "embedding scaling", "Pareto frontier", "parameter budgeting", "model width", "model depth", "system optimizations", "speculative decoding", "LongCat-Flash-Lite"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5e38\u7528\u4e8e\u7a00\u758f\u6027\u6269\u5c55\uff0c\u4f46\u9762\u4e34\u6536\u76ca\u9012\u51cf\u548c\u7cfb\u7edf\u74f6\u9888\u7684\u95ee\u9898\u3002</li>\n    <li>\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5d4c\u5165\u6269\u5c55\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u7a00\u758f\u6027\u6269\u5c55\u65b9\u5f0f\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u548c\u5b9e\u9a8c\u627e\u5230\u4e86\u5d4c\u5165\u6269\u5c55\u7684\u4f18\u52bf\u533a\u57df\u3002</li>\n    <li>\u6211\u4eec\u7cfb\u7edf\u5730\u5206\u6790\u4e86\u5f71\u54cd\u5d4c\u5165\u6269\u5c55\u6548\u679c\u7684\u5173\u952e\u67b6\u6784\u56e0\u7d20\uff0c\u5305\u62ec\u53c2\u6570\u9884\u7b97\u3001\u6a21\u578b\u5bbd\u5ea6\u548c\u6df1\u5ea6\u7684\u76f8\u4e92\u4f5c\u7528\u3002</li>\n    <li>\u901a\u8fc7\u96c6\u6210\u7cfb\u7edf\u4f18\u5316\u548c\u63a8\u6d4b\u89e3\u7801\uff0c\u6211\u4eec\u5c06\u7a00\u758f\u6027\u8f6c\u5316\u4e3a\u5b9e\u9645\u7684\u63a8\u7406\u52a0\u901f\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86LongCat-Flash-Lite\u6a21\u578b\uff0c\u62e5\u6709685\u4ebf\u53c2\u6570\uff0c\u5c3d\u7ba1\u53ea\u670930\u4ebf\u53c2\u6570\u88ab\u6fc0\u6d3b\uff0c\u4f46\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u4f18\u4e8e\u540c\u7c7b\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Mixture-of-Experts (MoE) models are popular for making large language models more efficient, but they have some limitations.</li>\n    <li>This study looks at using embedding scaling as a new way to improve efficiency instead of just expert scaling.</li>\n    <li>Through experiments, the researchers found that embedding scaling can perform better in certain situations compared to expert scaling.</li>\n    <li>They also identified key architectural factors that affect how well embedding scaling works, like model size and design.</li>\n    <li>The new model, LongCat-Flash-Lite, has 68.5 billion parameters and performs better than traditional MoE models, especially in tasks related to agents and coding.</li>\n</ul>"}, "publishedAt": "2026-01-28T22:11:19.000Z", "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "summary": "While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21204.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 221, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.22153", "authors": [{"_id": "697c2899a67238fac88cc115", "user": {"_id": "63f47b5321eb234ab739e91a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg", "isPro": false, "fullname": "Haozhe Xie", "user": "hzxie", "type": "user"}, "name": "Haozhe Xie", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T13:31:48.996Z", "hidden": false}, {"_id": "697c2899a67238fac88cc116", "user": {"_id": "672392c4a4c4381cefc06416", "avatarUrl": "/avatars/8ee84a7e3e91e5d13074bc3c407ff75d.svg", "isPro": false, "fullname": "Wen Beichen", "user": "wenbc21", "type": "user"}, "name": "Beichen Wen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T13:31:52.487Z", "hidden": false}, {"_id": "697c2899a67238fac88cc117", "user": {"_id": "6899ff3f4c5ca50a326bb456", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Nuqof2ofdaQUD5b07cDnG.png", "isPro": false, "fullname": "Zheng Jiarui", "user": "zghtyarecrenj", "type": "user"}, "name": "Jiarui Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T13:31:46.030Z", "hidden": false}, {"_id": "697c2899a67238fac88cc118", "name": "Zhaoxi Chen", "hidden": false}, {"_id": "697c2899a67238fac88cc119", "name": "Fangzhou Hong", "hidden": false}, {"_id": "697c2899a67238fac88cc11a", "name": "Haiwen Diao", "hidden": false}, {"_id": "697c2899a67238fac88cc11b", "name": "Ziwei Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/p9cPxETttQUS23woFb14M.mp4"], "publishedAt": "2026-01-29T18:59:51.000Z", "submittedOnDailyAt": "2026-01-30T01:46:39.673Z", "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation", "submittedOnDailyBy": {"_id": "63f47b5321eb234ab739e91a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg", "isPro": false, "fullname": "Haozhe Xie", "user": "hzxie", "type": "user"}, "summary": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.", "upvotes": 45, "discussionId": "697c2899a67238fac88cc11c", "projectPage": "https://haozhexie.com/project/dynamic-vla", "githubRepo": "https://github.com/hzxie/DynamicVLA", "githubRepoAddedBy": "user", "ai_summary": "DynamicVLA addresses dynamic object manipulation challenges through a compact vision-language-action model with temporal reasoning and closed-loop adaptation, supported by a new benchmark for dynamic manipulation tasks.", "ai_keywords": ["Vision-Language-Action models", "temporal reasoning", "closed-loop adaptation", "convolutional vision encoder", "multimodal inference", "Continuous Inference", "Latent-aware Action Streaming", "Dynamic Object Manipulation benchmark", "synthetic episodes", "real-world episodes"], "githubStars": 48, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "summary_zh": "<ul>\n    <li>\u52a8\u6001\u7269\u4f53\u64cd\u63a7\u4ecd\u7136\u662f\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u6a21\u578b\u7684\u4e00\u4e2a\u6311\u6218\uff0c\u5c24\u5176\u5728\u5feb\u901f\u611f\u77e5\u548c\u63a7\u5236\u65b9\u9762\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DynamicVLA\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\u6765\u6539\u5584\u52a8\u6001\u7269\u4f53\u64cd\u63a7\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6846\u67b6\u4f7f\u7528\u9ad8\u6548\u7684\u5377\u79ef\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u652f\u6301\u5feb\u901f\u7684\u591a\u6a21\u6001\u63a8\u7406\u3002</li>\n    <li>\u5f15\u5165\u4e86\u52a8\u6001\u7269\u4f53\u64cd\u63a7\u57fa\u51c6\uff08DOM\uff09\uff0c\u6536\u96c6\u4e8620\u4e07\u4e2a\u5408\u6210\u6837\u672c\u548c2000\u4e2a\u771f\u5b9e\u573a\u666f\u6570\u636e\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cDynamicVLA\u5728\u54cd\u5e94\u901f\u5ea6\u3001\u611f\u77e5\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DynamicVLA is a new system designed to help machines manipulate moving objects better than current models.</li>\n    <li>It uses a small but efficient vision encoder to quickly process visual information.</li>\n    <li>The system allows for continuous thinking and acting, which helps it to respond faster to moving objects.</li>\n    <li>DynamicVLA also includes a new dataset called the Dynamic Object Manipulation (DOM) benchmark, which contains a large number of synthetic and real-world scenarios for training and testing.</li>\n    <li>Tests show that DynamicVLA is much faster and more accurate at manipulating dynamic objects compared to previous methods.</li>\n</ul>"}, "publishedAt": "2026-01-29T13:59:51.000Z", "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation", "summary": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/p9cPxETttQUS23woFb14M.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22153.png", "numComments": 2, "submittedBy": {"_id": "63f47b5321eb234ab739e91a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg", "fullname": "Haozhe Xie", "name": "hzxie", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 19, "isUserFollowing": false}, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.21639", "authors": [{"_id": "697c5270a67238fac88cc226", "user": {"_id": "693f91d7ed7d40c019934508", "avatarUrl": "/avatars/0d73f098627c9ebd2ae7d90e693a34f6.svg", "isPro": false, "fullname": "Yufeng Zhong", "user": "Albert-Zhong", "type": "user"}, "name": "Yufeng Zhong", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T09:35:23.719Z", "hidden": false}, {"_id": "697c5270a67238fac88cc227", "name": "Lei Chen", "hidden": false}, {"_id": "697c5270a67238fac88cc228", "name": "Xuanle Zhao", "hidden": false}, {"_id": "697c5270a67238fac88cc229", "name": "Wenkang Han", "hidden": false}, {"_id": "697c5270a67238fac88cc22a", "name": "Liming Zheng", "hidden": false}, {"_id": "697c5270a67238fac88cc22b", "name": "Jing Huang", "hidden": false}, {"_id": "697c5270a67238fac88cc22c", "name": "Deyang Jiang", "hidden": false}, {"_id": "697c5270a67238fac88cc22d", "name": "Yilin Cao", "hidden": false}, {"_id": "697c5270a67238fac88cc22e", "name": "Lin Ma", "hidden": false}, {"_id": "697c5270a67238fac88cc22f", "name": "Zhixiong Zeng", "hidden": false}], "publishedAt": "2026-01-29T12:43:02.000Z", "submittedOnDailyAt": "2026-01-30T04:33:06.261Z", "title": "OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models", "submittedOnDailyBy": {"_id": "6572cbc42bb242937c0a1101", "avatarUrl": "/avatars/f2af45e6b242aa47578fe3f60e97ca86.svg", "isPro": false, "fullname": "Xuanle Zhao", "user": "xxxllz", "type": "user"}, "summary": "The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (Text-centric OCR), neglecting the identification of visual elements from visually information-dense image sources (Vision-centric OCR), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose OCRVerse, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.", "upvotes": 41, "discussionId": "697c5270a67238fac88cc230", "githubRepo": "https://github.com/DocTron-hub/OCRVerse", "githubRepoAddedBy": "user", "ai_summary": "OCRVerse is a novel end-to-end OCR method that unifies text-centric and vision-centric approaches through comprehensive data engineering and a two-stage SFT-RL training framework with domain-specific reward strategies.", "ai_keywords": ["OCR", "vision-centric OCR", "text-centric OCR", "end-to-end OCR", "data engineering", "SFT-RL training", "cross-domain training", "reward strategies", "domain-specific customization", "cross-domain fusion"], "githubStars": 13, "summary_zh": "<ul>\n    <li>OCR\u6280\u672f\u7528\u4e8e\u4ece\u89c6\u89c9\u56fe\u50cf\u4e2d\u63d0\u53d6\u4fe1\u606f\uff0c\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\u3002</li>\n    <li>\u73b0\u6709OCR\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u8bc6\u522b\uff0c\u5ffd\u7565\u4e86\u4ece\u4fe1\u606f\u5bc6\u96c6\u56fe\u50cf\uff08\u5982\u56fe\u8868\u548c\u7f51\u9875\uff09\u4e2d\u8bc6\u522b\u89c6\u89c9\u5143\u7d20\u3002</li>\n    <li>OCRVerse\u662f\u9996\u4e2a\u7edf\u4e00\u6587\u672c\u4e2d\u5fc3\u548c\u89c6\u89c9\u4e2d\u5fc3OCR\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u79cd\u6587\u6863\u7c7b\u578b\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u91c7\u7528\u4e24\u9636\u6bb5SFT-RL\u591a\u9886\u57df\u8bad\u7ec3\uff0c\u7ed3\u5408\u4e0d\u540c\u9886\u57df\u7684\u6570\u636e\u548c\u5956\u52b1\u7b56\u7565\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cOCRVerse\u5728\u6587\u672c\u548c\u89c6\u89c9\u6570\u636e\u7c7b\u578b\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u6548\u679c\u4e0e\u5927\u578b\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u76f8\u5f53\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>OCR technology is becoming more important for handling large amounts of visual data, especially with the rise of large vision language models.</li>\n    <li>Most current OCR methods focus only on text from images, ignoring the visual elements in complex images like charts and web pages.</li>\n    <li>OCRVerse is a new method that combines both text-centric and vision-centric OCR in one system.</li>\n    <li>This method uses a unique training approach that helps it learn from different types of data effectively.</li>\n    <li>Tests show that OCRVerse performs well with both text and visual data types, achieving results similar to other advanced models.</li>\n</ul>"}, "publishedAt": "2026-01-29T07:43:02.000Z", "title": "OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models", "summary": "The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (Text-centric OCR), neglecting the identification of visual elements from visually information-dense image sources (Vision-centric OCR), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose OCRVerse, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21639.png", "numComments": 2, "submittedBy": {"_id": "6572cbc42bb242937c0a1101", "avatarUrl": "/avatars/f2af45e6b242aa47578fe3f60e97ca86.svg", "fullname": "Xuanle Zhao", "name": "xxxllz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.21821", "authors": [{"_id": "697c2a0ca67238fac88cc11e", "name": "Honglin Lin", "hidden": false}, {"_id": "697c2a0ca67238fac88cc11f", "user": {"_id": "6625ef13605f46d05c1d0031", "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg", "isPro": false, "fullname": "Zheng Liu", "user": "starriver030515", "type": "user"}, "name": "Zheng Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T09:36:23.860Z", "hidden": false}, {"_id": "697c2a0ca67238fac88cc120", "name": "Yun Zhu", "hidden": false}, {"_id": "697c2a0ca67238fac88cc121", "user": {"_id": "67b30bb2c2e25cfcdeda4a2f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67b30bb2c2e25cfcdeda4a2f/K5ePD5uWNkwlpkI_43Oe1.jpeg", "isPro": false, "fullname": "Qin, Chonghan", "user": "J017athan", "type": "user"}, "name": "Chonghan Qin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T09:36:20.559Z", "hidden": false}, {"_id": "697c2a0ca67238fac88cc122", "name": "Juekai Lin", "hidden": false}, {"_id": "697c2a0ca67238fac88cc123", "name": "Xiaoran Shang", "hidden": false}, {"_id": "697c2a0ca67238fac88cc124", "name": "Conghui He", "hidden": false}, {"_id": "697c2a0ca67238fac88cc125", "name": "Wentao Zhang", "hidden": false}, {"_id": "697c2a0ca67238fac88cc126", "name": "Lijun Wu", "hidden": false}], "publishedAt": "2026-01-29T15:07:28.000Z", "submittedOnDailyAt": "2026-01-30T01:36:56.361Z", "title": "MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods", "submittedOnDailyBy": {"_id": "640d99628512ec51d7ef71c7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d99628512ec51d7ef71c7/fcBkqnxfxuuuZTqfN_BGy.jpeg", "isPro": false, "fullname": "Honglin Lin", "user": "LHL3341", "type": "user"}, "summary": "Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a \"less is more\" phenomenon via our difficulty-aware filtering strategy: a subset of just 7\\% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.", "upvotes": 25, "discussionId": "697c2a0da67238fac88cc127", "projectPage": "https://mmfinereason.github.io/", "ai_summary": "A large-scale multimodal reasoning dataset called MMFineReason is introduced to improve vision language models' performance through high-quality reasoning annotations and demonstrates superior parameter efficiency in fine-tuned models.", "ai_keywords": ["Vision Language Models", "Chain-of-Thought", "multimodal reasoning", "Qwen3-VL", "CoT rationale generation", "reasoning quality", "difficulty awareness", "parameter efficiency", "general capabilities"], "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n  <li>\u6700\u8fd1\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5f00\u6e90\u6a21\u578b\u4ecd\u843d\u540e\u4e8e\u4e13\u6709\u7cfb\u7edf\u3002</li>\n  <li>\u73b0\u6709\u7684\u6570\u636e\u96c6\u5728STEM\u56fe\u8868\u548c\u89c6\u89c9\u8c1c\u9898\u7b49\u56f0\u96be\u9886\u57df\u7684\u8986\u76d6\u9762\u6709\u9650\uff0c\u7f3a\u4e4f\u4e00\u81f4\u7684\u957f\u683c\u5f0f\u63a8\u7406\u6ce8\u91ca\u3002</li>\n  <li>\u6211\u4eec\u63a8\u51fa\u4e86MMFineReason\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b180\u4e07\u6837\u672c\u548c51\u4ebf\u89e3\u51b3\u65b9\u6848\u6807\u8bb0\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u63a8\u7406\u6570\u636e\u96c6\u3002</li>\n  <li>\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u7684\u7cfb\u7edf\u6d41\u7a0b\u5efa\u7acb\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u63a8\u7406\u4f9d\u636e\u751f\u6210\u548c\u8d28\u91cf\u9009\u62e9\u3002</li>\n  <li>\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u8d85\u8fc7\u4e86\u73b0\u6709\u540c\u7c7b\u6a21\u578b\uff0c\u4e14\u53d1\u73b0\u5c11\u91cf\u6837\u672c\uff087%\uff09\u4e5f\u80fd\u53d6\u5f97\u4e0e\u5168\u6570\u636e\u96c6\u76f8\u5f53\u7684\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent improvements in Vision Language Models (VLMs) have enhanced visual reasoning, but open-source models still fall short compared to proprietary ones due to a lack of quality reasoning data.</li>\n    <li>Current datasets do not adequately cover tough areas like STEM diagrams and visual puzzles and lack long-form reasoning annotations.</li>\n    <li>To address this, a new dataset called MMFineReason has been created, containing 1.8 million samples and extensive reasoning annotations sourced from a large model.</li>\n    <li>MMFineReason is built through a detailed process involving data collection, rationale generation, and careful selection based on quality and difficulty.</li>\n    <li>The fine-tuned models using this dataset have set new records in performance and show that a smaller, carefully chosen subset of data can achieve results similar to using the full dataset.</li>\n</ul>"}, "publishedAt": "2026-01-29T10:07:28.000Z", "title": "MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods", "summary": "Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a \"less is more\" phenomenon via our difficulty-aware filtering strategy: a subset of just 7\\% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21821.png", "numComments": 2, "submittedBy": {"_id": "640d99628512ec51d7ef71c7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d99628512ec51d7ef71c7/fcBkqnxfxuuuZTqfN_BGy.jpeg", "fullname": "Honglin Lin", "name": "LHL3341", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.21420", "authors": [{"_id": "697c36f4a67238fac88cc1a4", "user": {"_id": "65a62085576772f531e13856", "avatarUrl": "/avatars/72c67a60422e333ea4e323f7480ae0b7.svg", "isPro": false, "fullname": "Huang Zihao", "user": "FetchFortune", "type": "user"}, "name": "Zihao Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T09:35:39.380Z", "hidden": false}, {"_id": "697c36f4a67238fac88cc1a5", "name": "Jundong Zhou", "hidden": false}, {"_id": "697c36f4a67238fac88cc1a6", "name": "Xingwei Qu", "hidden": false}, {"_id": "697c36f4a67238fac88cc1a7", "name": "Qiyang Min", "hidden": false}, {"_id": "697c36f4a67238fac88cc1a8", "user": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "name": "Ge Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T13:31:44.281Z", "hidden": false}], "publishedAt": "2026-01-29T08:58:22.000Z", "submittedOnDailyAt": "2026-01-30T02:24:05.556Z", "title": "ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation", "submittedOnDailyBy": {"_id": "65a62085576772f531e13856", "avatarUrl": "/avatars/72c67a60422e333ea4e323f7480ae0b7.svg", "isPro": false, "fullname": "Huang Zihao", "user": "FetchFortune", "type": "user"}, "summary": "Large language models allocate uniform computation across all tokens, ignoring that some sequences are trivially predictable while others require deep reasoning. We introduce ConceptMoE, which dynamically merges semantically similar tokens into concept representations, performing implicit token-level compute allocation. A learnable chunk module identifies optimal boundaries by measuring inter-token similarity, compressing sequences by a target ratio R before they enter the compute-intensive concept model. Crucially, the MoE architecture enables controlled evaluation: we reallocate saved computation to match baseline activated FLOPs (excluding attention map computation) and total parameters, isolating genuine architectural benefits. Under these conditions, ConceptMoE consistently outperforms standard MoE across language and vision-language tasks, achieving +0.9 points on language pretraining, +2.3 points on long context understanding, and +0.6 points on multimodal benchmarks. When converting pretrained MoE during continual training with layer looping, gains reach +5.5 points, demonstrating practical applicability. Beyond performance, ConceptMoE reduces attention computation by up to R^2times and KV cache by Rtimes. At R=2, empirical measurements show prefill speedups reaching 175\\% and decoding speedups up to 117\\% on long sequences. The minimal architectural modifications enable straightforward integration into existing MoE, demonstrating that adaptive concept-level processing fundamentally improves both effectiveness and efficiency of large language models.", "upvotes": 22, "discussionId": "697c36f4a67238fac88cc1a9", "githubRepo": "https://github.com/ZihaoHuang-notabot/ConceptMoE", "githubRepoAddedBy": "user", "ai_summary": "ConceptMoE dynamically allocates computation by merging similar tokens into concept representations, improving both performance and efficiency in large language models through adaptive processing and reduced attention computation.", "ai_keywords": ["ConceptMoE", "token-level compute allocation", "semantically similar tokens", "concept representations", "learnable chunk module", "inter-token similarity", "MoE architecture", "attention computation", "KV cache", "layer looping", "prefill speedups", "decoding speedups"], "githubStars": 6, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6240\u6709\u6807\u8bb0\u4e0a\u5206\u914d\u76f8\u540c\u7684\u8ba1\u7b97\uff0c\u4f46\u4e00\u4e9b\u5e8f\u5217\u7b80\u5355\u53ef\u9884\u6d4b\uff0c\u800c\u5176\u4ed6\u5e8f\u5217\u9700\u8981\u6df1\u5165\u63a8\u7406\u3002</li>\n    <li>ConceptMoE\u901a\u8fc7\u52a8\u6001\u5408\u5e76\u8bed\u4e49\u76f8\u4f3c\u7684\u6807\u8bb0\uff0c\u8fdb\u884c\u9690\u5f0f\u7684\u6807\u8bb0\u7ea7\u8ba1\u7b97\u5206\u914d\u3002</li>\n    <li>\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u6a21\u5757\u8bc6\u522b\u6700\u4f73\u8fb9\u754c\uff0c\u538b\u7f29\u5e8f\u5217\uff0c\u4ece\u800c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002</li>\n    <li>\u5728\u8bed\u8a00\u548c\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\uff0cConceptMoE\u7684\u8868\u73b0\u4f18\u4e8e\u6807\u51c6MoE\uff0c\u5c24\u5176\u5728\u8bed\u8a00\u9884\u8bad\u7ec3\u548c\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u3002</li>\n    <li>ConceptMoE\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\uff0c\u5e76\u5728\u957f\u5e8f\u5217\u4e0a\u63d0\u9ad8\u9884\u586b\u5145\u548c\u89e3\u7801\u901f\u5ea6\uff0c\u6613\u4e8e\u4e0e\u73b0\u6709MoE\u96c6\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>ConceptMoE is a new model that improves how large language models handle different types of information by grouping similar tokens together.</li>\n    <li>It uses a learnable module to find the best way to compress sequences before processing, making computation more efficient.</li>\n    <li>ConceptMoE outperforms traditional models in various tasks, with improvements in language understanding and multimodal tasks.</li>\n    <li>It also significantly reduces computation needed for attention and cache, leading to faster processing speeds for long sequences.</li>\n    <li>This model can be easily integrated into existing systems, showing that smarter processing can enhance both performance and efficiency.</li>\n</ul>"}, "publishedAt": "2026-01-29T03:58:22.000Z", "title": "ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation", "summary": "Large language models allocate uniform computation across all tokens, ignoring that some sequences are trivially predictable while others require deep reasoning. We introduce ConceptMoE, which dynamically merges semantically similar tokens into concept representations, performing implicit token-level compute allocation. A learnable chunk module identifies optimal boundaries by measuring inter-token similarity, compressing sequences by a target ratio R before they enter the compute-intensive concept model. Crucially, the MoE architecture enables controlled evaluation: we reallocate saved computation to match baseline activated FLOPs (excluding attention map computation) and total parameters, isolating genuine architectural benefits. Under these conditions, ConceptMoE consistently outperforms standard MoE across language and vision-language tasks, achieving +0.9 points on language pretraining, +2.3 points on long context understanding, and +0.6 points on multimodal benchmarks. When converting pretrained MoE during continual training with layer looping, gains reach +5.5 points, demonstrating practical applicability. Beyond performance, ConceptMoE reduces attention computation by up to R^2times and KV cache by Rtimes. At R=2, empirical measurements show prefill speedups reaching 175\\% and decoding speedups up to 117\\% on long sequences. The minimal architectural modifications enable straightforward integration into existing MoE, demonstrating that adaptive concept-level processing fundamentally improves both effectiveness and efficiency of large language models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21420.png", "numComments": 2, "submittedBy": {"_id": "65a62085576772f531e13856", "avatarUrl": "/avatars/72c67a60422e333ea4e323f7480ae0b7.svg", "fullname": "Huang Zihao", "name": "FetchFortune", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.22046", "authors": [{"_id": "697c65a1a67238fac88cc24b", "name": "Changjian Jiang", "hidden": false}, {"_id": "697c65a1a67238fac88cc24c", "name": "Kerui Ren", "hidden": false}, {"_id": "697c65a1a67238fac88cc24d", "name": "Xudong Li", "hidden": false}, {"_id": "697c65a1a67238fac88cc24e", "name": "Kaiwen Song", "hidden": false}, {"_id": "697c65a1a67238fac88cc24f", "name": "Linning Xu", "hidden": false}, {"_id": "697c65a1a67238fac88cc250", "name": "Tao Lu", "hidden": false}, {"_id": "697c65a1a67238fac88cc251", "name": "Junting Dong", "hidden": false}, {"_id": "697c65a1a67238fac88cc252", "name": "Yu Zhang", "hidden": false}, {"_id": "697c65a1a67238fac88cc253", "name": "Bo Dai", "hidden": false}, {"_id": "697c65a1a67238fac88cc254", "name": "Mulin Yu", "hidden": false}], "publishedAt": "2026-01-29T17:47:26.000Z", "submittedOnDailyAt": "2026-01-30T05:34:08.168Z", "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction", "submittedOnDailyBy": {"_id": "656e9e26bbec423d88b603e8", "avatarUrl": "/avatars/10d8cb945a60e0401bfa4f74137cb203.svg", "isPro": false, "fullname": "MulinYu", "user": "UML", "type": "user"}, "summary": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: https://city-super.github.io/PLANING/ .", "upvotes": 19, "discussionId": "697c65a1a67238fac88cc255", "projectPage": "https://city-super.github.io/PLANING/", "ai_summary": "PLANING presents an efficient streaming reconstruction framework that combines explicit geometric primitives with neural Gaussians to achieve high-quality rendering and accurate geometry simultaneously through decoupled optimization.", "ai_keywords": ["monocular image sequences", "hybrid representation", "explicit geometric primitives", "neural Gaussians", "decoupled manner", "online initialization", "optimization strategy", "streaming reconstruction", "dense mesh Chamfer-L2", "PSNR", "ScanNetV2", "2D Gaussian Splatting"], "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>PLANING\u662f\u4e00\u4e2a\u65b0\u7684\u6d41\u5f0f\u91cd\u5efa\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u663e\u5f0f\u51e0\u4f55\u539f\u4ef6\u548c\u795e\u7ecf\u9ad8\u65af\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6846\u67b6\u652f\u6301\u5728\u7ebf\u521d\u59cb\u5316\u548c\u4f18\u5316\uff0c\u80fd\u591f\u5206\u79bb\u51e0\u4f55\u548c\u5916\u89c2\u7684\u66f4\u65b0\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u7684\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\uff0cPLANING\u6bd4PGSR\u63d0\u9ad8\u4e8618.52%\uff0c\u5e76\u4e14\u5728\u901f\u5ea6\u4e0a\u662f2D\u9ad8\u65af\u70b9\u4e91\u76845\u500d\u4ee5\u4e0a\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u573a\u666f\u5efa\u6a21\u548c\u4e3a\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u521b\u5efa\u6a21\u62df\u73af\u5883\u3002</li>\n    <li>\u9879\u76ee\u8be6\u7ec6\u4fe1\u606f\u53ef\u4ee5\u5728\u5176\u5b98\u65b9\u7f51\u7ad9\u4e0a\u627e\u5230: https://city-super.github.io/PLANING/\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>PLANING is a new framework for reconstructing 3D scenes from video images efficiently.</li>\n    <li>It combines geometric shapes with neural networks to separately manage geometry (shape) and appearance (texture).</li>\n    <li>This approach allows for quick updates and better quality, improving previous methods in both speed and accuracy.</li>\n    <li>PLANING reconstructs scenes much faster than older methods while maintaining high quality.</li>\n    <li>It is suitable for various applications, including large-scale modeling and AI environments.</li>\n</ul>"}, "publishedAt": "2026-01-29T12:47:26.000Z", "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction", "summary": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: https://city-super.github.io/PLANING/ .", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22046.png", "numComments": 2, "submittedBy": {"_id": "656e9e26bbec423d88b603e8", "avatarUrl": "/avatars/10d8cb945a60e0401bfa4f74137cb203.svg", "fullname": "MulinYu", "name": "UML", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": false}, {"paper": {"id": "2601.21337", "authors": [{"_id": "697c1ebea67238fac88cc0ae", "name": "Xian Shi", "hidden": false}, {"_id": "697c1ebea67238fac88cc0af", "name": "Xiong Wang", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b0", "name": "Zhifang Guo", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b1", "name": "Yongqi Wang", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b2", "name": "Pei Zhang", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b3", "name": "Xinyu Zhang", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b4", "name": "Zishan Guo", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b5", "name": "Hongkun Hao", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b6", "name": "Yu Xi", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b7", "name": "Baosong Yang", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b8", "name": "Jin Xu", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b9", "name": "Jingren Zhou", "hidden": false}, {"_id": "697c1ebea67238fac88cc0ba", "name": "Junyang Lin", "hidden": false}], "publishedAt": "2026-01-29T06:58:13.000Z", "submittedOnDailyAt": "2026-01-30T02:17:20.204Z", "title": "Qwen3-ASR Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "In this report, we introduce Qwen3-ASR family, which includes two powerful all-in-one speech recognition models and a novel non-autoregressive speech forced alignment model. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B are ASR models that support language identification and ASR for 52 languages and dialects. Both of them leverage large-scale speech training data and the strong audio understanding ability of their foundation model Qwen3-Omni. We conduct comprehensive internal evaluation besides the open-sourced benchmarks as ASR models might differ little on open-sourced benchmark scores but exhibit significant quality differences in real-world scenarios. The experiments reveal that the 1.7B version achieves SOTA performance among open-sourced ASR models and is competitive with the strongest proprietary APIs while the 0.6B version offers the best accuracy-efficiency trade-off. Qwen3-ASR-0.6B can achieve an average TTFT as low as 92ms and transcribe 2000 seconds speech in 1 second at a concurrency of 128. Qwen3-ForcedAligner-0.6B is an LLM based NAR timestamp predictor that is able to align text-speech pairs in 11 languages. Timestamp accuracy experiments show that the proposed model outperforms the three strongest force alignment models and takes more advantages in efficiency and versatility. To further accelerate the community research of ASR and audio understanding, we release these models under the Apache 2.0 license.", "upvotes": 18, "discussionId": "697c1ebea67238fac88cc0bb", "ai_summary": "The Qwen3-ASR family introduces speech recognition models with language identification capabilities and a non-autoregressive forced alignment model, achieving state-of-the-art performance and efficient processing.", "ai_keywords": ["speech recognition models", "language identification", "non-autoregressive models", "forced alignment", "timestamp prediction", "audio understanding", "large-scale speech training data", "foundation model", "TTFT", "concurrency"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86Qwen3-ASR\u7cfb\u5217\uff0c\u5305\u62ec\u4e24\u4e2a\u5f3a\u5927\u7684\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u548c\u4e00\u4e2a\u65b0\u7684\u975e\u81ea\u56de\u5f52\u8bed\u97f3\u5f3a\u5236\u5bf9\u9f50\u6a21\u578b\u3002</li>\n    <li>Qwen3-ASR-1.7B\u548cQwen3-ASR-0.6B\u652f\u630152\u79cd\u8bed\u8a00\u548c\u65b9\u8a00\u7684\u8bed\u8a00\u8bc6\u522b\u548c\u8bed\u97f3\u8bc6\u522b\u3002</li>\n    <li>1.7B\u7248\u672c\u5728\u5f00\u6e90\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c0.6B\u7248\u672c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6700\u4f73\u5e73\u8861\u3002</li>\n    <li>Qwen3-ASR-0.6B\u53ef\u5728128\u5e76\u53d1\u60c5\u51b5\u4e0b\uff0c\u5e73\u5747\u5904\u7406\u65f6\u95f4\u77ed\u81f392\u6beb\u79d2\uff0c\u5c062000\u79d2\u7684\u8bed\u97f3\u8f6c\u5f55\u4e3a1\u79d2\u3002</li>\n    <li>\u53d1\u5e03\u4e86\u8fd9\u4e9b\u6a21\u578b\uff0c\u4ee5\u4fc3\u8fdb\u8bed\u97f3\u8bc6\u522b\u548c\u97f3\u9891\u7406\u89e3\u7684\u793e\u533a\u7814\u7a76\uff0c\u91c7\u7528Apache 2.0\u8bb8\u53ef\u8bc1\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Introducing the Qwen3-ASR family with two speech recognition models: Qwen3-ASR-1.7B and Qwen3-ASR-0.6B, plus a new forced alignment model.</li>\n    <li>These models can identify languages and transcribe speech in 52 languages and dialects.</li>\n    <li>The 1.7B model offers top performance and competes with leading proprietary services, while the 0.6B model provides a great balance between accuracy and efficiency.</li>\n    <li>The 0.6B version has a fast average transcription speed and can handle multiple speech inputs at once.</li>\n    <li>The new forced alignment model aligns text and speech for 11 languages and is more efficient than existing models, and all models are available for public use.</li>\n</ul>"}, "publishedAt": "2026-01-29T01:58:13.000Z", "title": "Qwen3-ASR Technical Report", "summary": "In this report, we introduce Qwen3-ASR family, which includes two powerful all-in-one speech recognition models and a novel non-autoregressive speech forced alignment model. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B are ASR models that support language identification and ASR for 52 languages and dialects. Both of them leverage large-scale speech training data and the strong audio understanding ability of their foundation model Qwen3-Omni. We conduct comprehensive internal evaluation besides the open-sourced benchmarks as ASR models might differ little on open-sourced benchmark scores but exhibit significant quality differences in real-world scenarios. The experiments reveal that the 1.7B version achieves SOTA performance among open-sourced ASR models and is competitive with the strongest proprietary APIs while the 0.6B version offers the best accuracy-efficiency trade-off. Qwen3-ASR-0.6B can achieve an average TTFT as low as 92ms and transcribe 2000 seconds speech in 1 second at a concurrency of 128. Qwen3-ForcedAligner-0.6B is an LLM based NAR timestamp predictor that is able to align text-speech pairs in 11 languages. Timestamp accuracy experiments show that the proposed model outperforms the three strongest force alignment models and takes more advantages in efficiency and versatility. To further accelerate the community research of ASR and audio understanding, we release these models under the Apache 2.0 license.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21337.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 221, "isUserFollowing": false}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20730", "authors": [{"_id": "697b161fdf3e800774f13d05", "name": "Shicheng Fang", "hidden": false}, {"_id": "697b161fdf3e800774f13d06", "name": "Yuxin Wang", "hidden": false}, {"_id": "697b161fdf3e800774f13d07", "name": "XiaoRan Liu", "hidden": false}, {"_id": "697b161fdf3e800774f13d08", "name": "Jiahao Lu", "hidden": false}, {"_id": "697b161fdf3e800774f13d09", "name": "Chuanyuan Tan", "hidden": false}, {"_id": "697b161fdf3e800774f13d0a", "name": "Xinchi Chen", "hidden": false}, {"_id": "697b161fdf3e800774f13d0b", "name": "Yining Zheng. Xuanjing Huang", "hidden": false}, {"_id": "697b161fdf3e800774f13d0c", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-01-28T16:05:44.000Z", "submittedOnDailyAt": "2026-01-30T06:14:19.038Z", "title": "AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts", "submittedOnDailyBy": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "isPro": false, "fullname": "Xiaoran Liu (SII)", "user": "SII-xrliu", "type": "user"}, "summary": "The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce AgentLongBench, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues.", "upvotes": 17, "discussionId": "697b1620df3e800774f13d0d", "ai_summary": "AgentLongBench evaluates large language models as autonomous agents through dynamic environment interactions, revealing challenges in handling high-information-density tool responses compared to memory fragmentation in long conversations.", "ai_keywords": ["Large Language Models", "autonomous agents", "dynamic contexts", "AgentLongBench", "Lateral Thinking Puzzles", "environment rollouts", "knowledge-intensive scenarios", "knowledge-free scenarios", "information density", "tool responses", "memory fragmentation", "long-turn dialogues"], "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6b63\u5728\u53d1\u5c55\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u9700\u8981\u7ba1\u7406\u590d\u6742\u7684\u52a8\u6001\u73af\u5883\u3002</li>\n    <li>\u76ee\u524d\u7684\u8bc4\u4f30\u6807\u51c6\u4e3b\u8981\u662f\u9759\u6001\u7684\uff0c\u65e0\u6cd5\u6a21\u62df\u4ee3\u7406\u4e0e\u73af\u5883\u4e4b\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86AgentLongBench\uff0c\u901a\u8fc7\u6a21\u62df\u73af\u5883\u6765\u8bc4\u4f30\u4ee3\u7406\uff0c\u4f7f\u7528\u6a2a\u5411\u601d\u7ef4\u8c1c\u9898\u8fdb\u884c\u6d4b\u8bd5\u3002</li>\n    <li>\u5b9e\u9a8c\u663e\u793a\uff0c\u5c3d\u7ba1\u4ee3\u7406\u5728\u9759\u6001\u68c0\u7d22\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u52a8\u6001\u4fe1\u606f\u6574\u5408\u65b9\u9762\u5b58\u5728\u660e\u663e\u5f31\u70b9\u3002</li>\n    <li>\u8fd9\u79cd\u5f31\u70b9\u4e0e\u89e3\u51b3\u67e5\u8be2\u6240\u9700\u7684\u6700\u5c0f\u4ee4\u724c\u6570\u91cf\u6709\u5173\uff0c\u5bfc\u81f4\u9ad8\u4fe1\u606f\u5bc6\u5ea6\u7684\u5de5\u5177\u54cd\u5e94\u66f4\u52a0\u5177\u6311\u6218\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are evolving into autonomous agents that need to handle complex and changing information.</li>\n    <li>Current evaluation methods are not effective because they focus on simple retrieval tasks instead of real interactions with environments.</li>\n    <li>AgentLongBench is a new framework that tests these agents using simulated puzzles that require deep thinking and interaction.</li>\n    <li>Experiments show that while agents can retrieve information well, they have difficulties with combining dynamic information needed for tasks.</li>\n    <li>The challenges faced by agents are related to the number of tokens required to answer questions, making it harder for them to manage dense information.</li>\n</ul>"}, "publishedAt": "2026-01-28T11:05:44.000Z", "title": "AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts", "summary": "The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce AgentLongBench, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20730.png", "numComments": 2, "submittedBy": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "fullname": "Xiaoran Liu (SII)", "name": "SII-xrliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2601.20833", "authors": [{"_id": "697b9192a67238fac88cbee8", "name": "Tengyue Xu", "hidden": false}, {"_id": "697b9192a67238fac88cbee9", "name": "Zhuoyang Qian", "hidden": false}, {"_id": "697b9192a67238fac88cbeea", "name": "Gaoge Liu", "hidden": false}, {"_id": "697b9192a67238fac88cbeeb", "name": "Li Ling", "hidden": false}, {"_id": "697b9192a67238fac88cbeec", "name": "Zhentao Zhang", "hidden": false}, {"_id": "697b9192a67238fac88cbeed", "name": "Biao Wu", "hidden": false}, {"_id": "697b9192a67238fac88cbeee", "name": "Shuo Zhang", "hidden": false}, {"_id": "697b9192a67238fac88cbeef", "name": "Ke Lu", "hidden": false}, {"_id": "697b9192a67238fac88cbef0", "name": "Wei Shi", "hidden": false}, {"_id": "697b9192a67238fac88cbef1", "name": "Ziqi Wang", "hidden": false}, {"_id": "697b9192a67238fac88cbef2", "name": "Zheng Feng", "hidden": false}, {"_id": "697b9192a67238fac88cbef3", "name": "Yan Luo", "hidden": false}, {"_id": "697b9192a67238fac88cbef4", "name": "Shu Xu", "hidden": false}, {"_id": "697b9192a67238fac88cbef5", "name": "Yongjin Chen", "hidden": false}, {"_id": "697b9192a67238fac88cbef6", "name": "Zhibo Feng", "hidden": false}, {"_id": "697b9192a67238fac88cbef7", "name": "Zhuo Chen", "hidden": false}, {"_id": "697b9192a67238fac88cbef8", "name": "Bruce Yuan", "hidden": false}, {"_id": "697b9192a67238fac88cbef9", "name": "Harry Wang", "hidden": false}, {"_id": "697b9192a67238fac88cbefa", "name": "Kris Chen", "hidden": false}], "publishedAt": "2026-01-28T18:31:54.000Z", "submittedOnDailyAt": "2026-01-30T03:32:00.106Z", "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives", "submittedOnDailyBy": {"_id": "62baa0d6dd02fbf607ce97be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62baa0d6dd02fbf607ce97be/V0I6pANlLEf2YDd9ZLZgi.jpeg", "isPro": false, "fullname": "Wendy", "user": "Wendy-Fly", "type": "user"}, "summary": "Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.", "upvotes": 113, "discussionId": "697b9192a67238fac88cbefb", "githubRepo": "https://github.com/AgentAlphaAGI/Idea2Paper", "githubRepoAddedBy": "user", "ai_summary": "Offline knowledge construction through structured methodological graphs enables more reliable and scalable autonomous scientific discovery by reducing reliance on real-time literature processing.", "ai_keywords": ["large language model", "autonomous scientific discovery", "runtime-centric execution", "context window limitations", "hallucination", "pre-computation-driven framework", "peer-reviewed papers", "research patterns", "methodological knowledge graph", "end-to-end research workflows"], "githubStars": 54, "organization": {"_id": "69542731e1200d74c1c053d1", "name": "AgentAlphaAGI", "fullname": "AgentAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b78eb76ab5d14ca7faac87/TbMZ3y00APtRzHEfTSR7I.jpeg"}, "summary_zh": "<ul>\n    <li>\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u7684\u7814\u7a76\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u80fd\u591f\u81ea\u52a8\u5316\u6574\u4e2a\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u5728\u7ebf\u63a8\u7406\uff0c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u4e0d\u53ef\u9760\u7684\u63a8\u7406\u7ed3\u679c\u3002</li>\n    <li>\u63d0\u51fa\u4e86Idea2Story\u6846\u67b6\uff0c\u8f6c\u5411\u79bb\u7ebf\u77e5\u8bc6\u6784\u5efa\uff0c\u63d0\u9ad8\u6587\u732e\u7406\u89e3\u80fd\u529b\u3002</li>\n    <li>Idea2Story\u901a\u8fc7\u6536\u96c6\u540c\u884c\u8bc4\u5ba1\u8bba\u6587\u53ca\u53cd\u9988\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u7684\u77e5\u8bc6\u56fe\u8c31\u3002</li>\n    <li>\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7814\u7a76\u6a21\u5f0f\uff0c\u63d0\u4f9b\u53ef\u9760\u7684\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u57fa\u7840\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent advancements in autonomous scientific discovery using large language models (LLMs) have improved research workflows.</li>\n    <li>Current systems rely on online processing, which is costly and can lead to errors due to limitations in understanding context.</li>\n    <li>The proposed Idea2Story framework focuses on building knowledge offline, organizing research into a structured graph.</li>\n    <li>This approach allows for better retrieval and reuse of established research methods, reducing the need for extensive online reasoning.</li>\n    <li>Preliminary studies show that Idea2Story can create coherent and innovative research patterns effectively.</li>\n</ul>"}, "publishedAt": "2026-01-28T13:31:54.000Z", "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives", "summary": "Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20833.png", "numComments": 1, "submittedBy": {"_id": "62baa0d6dd02fbf607ce97be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62baa0d6dd02fbf607ce97be/V0I6pANlLEf2YDd9ZLZgi.jpeg", "fullname": "Wendy", "name": "Wendy-Fly", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "69542731e1200d74c1c053d1", "name": "AgentAlphaAGI", "fullname": "AgentAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b78eb76ab5d14ca7faac87/TbMZ3y00APtRzHEfTSR7I.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.18418", "authors": [{"_id": "69785315026bdf0473116f6a", "user": {"_id": "62dce08bb2c60f29c3d0a5da", "avatarUrl": "/avatars/87ce03e61c4c6eb686c9491ef4fda225.svg", "isPro": false, "fullname": "Ji Zeng", "user": "stargazerzj", "type": "user"}, "name": "Ji Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T08:31:51.245Z", "hidden": false}, {"_id": "69785315026bdf0473116f6b", "name": "Dayuan Fu", "hidden": false}, {"_id": "69785315026bdf0473116f6c", "name": "Tiantian Mi", "hidden": false}, {"_id": "69785315026bdf0473116f6d", "name": "Yumin Zhuang", "hidden": false}, {"_id": "69785315026bdf0473116f6e", "user": {"_id": "6865e6b362fc5689c5e67733", "avatarUrl": "/avatars/186f3d248791d961b0a810d5225167cc.svg", "isPro": false, "fullname": "Yaxing Huang", "user": "Rookie-Noob-Newbie", "type": "user"}, "name": "Yaxing Huang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:30.220Z", "hidden": false}, {"_id": "69785315026bdf0473116f6f", "user": {"_id": "67638cc0d63e4b348e8a5fa3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67638cc0d63e4b348e8a5fa3/BZNlw1uTGUcumCrXKkerx.png", "isPro": false, "fullname": "Xuefeng Li", "user": "drxuefeng", "type": "user"}, "name": "Xuefeng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:37.248Z", "hidden": false}, {"_id": "69785315026bdf0473116f70", "name": "Lyumanshan Ye", "hidden": false}, {"_id": "69785315026bdf0473116f71", "name": "Muhang Xie", "hidden": false}, {"_id": "69785315026bdf0473116f72", "name": "Qishuo Hua", "hidden": false}, {"_id": "69785315026bdf0473116f73", "name": "Zhen Huang", "hidden": false}, {"_id": "69785315026bdf0473116f74", "user": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "isPro": false, "fullname": "Mohan Jiang (SII)", "user": "mhjiang0408", "type": "user"}, "name": "Mohan Jiang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:23.438Z", "hidden": false}, {"_id": "69785315026bdf0473116f75", "name": "Hanning Wang", "hidden": false}, {"_id": "69785315026bdf0473116f76", "user": {"_id": "66fa544c54f87b607fbffd2e", "avatarUrl": "/avatars/94195dcda0eb68e8fd20d80718744697.svg", "isPro": false, "fullname": "Jifan Lin", "user": "evanlin2570", "type": "user"}, "name": "Jifan Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:57.029Z", "hidden": false}, {"_id": "69785315026bdf0473116f77", "name": "Yang Xiao", "hidden": false}, {"_id": "69785315026bdf0473116f78", "name": "Jie Sun", "hidden": false}, {"_id": "69785315026bdf0473116f79", "user": {"_id": "684faf712acd915b5afc055f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/684faf712acd915b5afc055f/K7icmL08HxniWDgdph73i.jpeg", "isPro": false, "fullname": "Yunze Wu", "user": "wyzmike", "type": "user"}, "name": "Yunze Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:01:06.063Z", "hidden": false}, {"_id": "69785315026bdf0473116f7a", "name": "Pengfei Liu", "hidden": false}], "publishedAt": "2026-01-26T12:20:18.000Z", "submittedOnDailyAt": "2026-01-27T03:34:37.777Z", "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "submittedOnDailyBy": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "isPro": false, "fullname": "Mohan Jiang (SII)", "user": "mhjiang0408", "type": "user"}, "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...", "upvotes": 104, "discussionId": "69785315026bdf0473116f7b", "githubRepo": "https://github.com/GAIR-NLP/daVinci-Dev", "githubRepoAddedBy": "user", "ai_summary": "Agentic mid-training enables large language models to develop autonomous software engineering capabilities through specialized data synthesis techniques that bridge the gap between static training data and dynamic development environments.", "ai_keywords": ["Large Language Model", "agentic software engineering", "mid-training", "distribution mismatch", "agent-native data", "contextually-native trajectories", "environmentally-native trajectories", "SWE-Bench Verified", "Kimi-Dev", "resolution rates"], "githubStars": 22, "organization": {"_id": "630bc2d186b8b9904c33ce1b", "name": "GAIR", "fullname": "SII - GAIR", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6b63\u5728\u4ece\u5355\u6b21\u4ee3\u7801\u751f\u6210\u8f6c\u5411\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\uff0c\u6a21\u578b\u80fd\u591f\u81ea\u52a8\u5bfc\u822a\u3001\u7f16\u8f91\u548c\u6d4b\u8bd5\u590d\u6742\u7684\u4ee3\u7801\u5e93\u3002</li>\n    <li>\u5c3d\u7ba1\u540e\u8bad\u7ec3\u65b9\u6cd5\u5df2\u6210\u4e3a\u4ee3\u7801\u4ee3\u7406\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u4f46\u201c\u4e2d\u671f\u8bad\u7ec3\u201d\uff08MT\uff09\u4ecd\u7136\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u5c3d\u7ba1\u5b83\u63d0\u4f9b\u4e86\u66f4\u53ef\u6269\u5c55\u7684\u57fa\u7840\u4ee3\u7406\u884c\u4e3a\u57f9\u517b\u8def\u5f84\u3002</li>\n    <li>\u4e2d\u671f\u8bad\u7ec3\u7684\u4e00\u5927\u6311\u6218\u662f\u9759\u6001\u8bad\u7ec3\u6570\u636e\u4e0e\u771f\u5b9e\u5f00\u53d1\u73af\u5883\u4e4b\u95f4\u7684\u5206\u5e03\u4e0d\u5339\u914d\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u4e2d\u671f\u8bad\u7ec3\u7814\u7a76\uff0c\u5efa\u7acb\u4e86\u6709\u6548\u4ee3\u7406\u5f00\u53d1\u7684\u6570\u636e\u5408\u6210\u539f\u5219\u548c\u8bad\u7ec3\u65b9\u6cd5\u3002</li>\n    <li>\u6211\u4eec\u7684\u6700\u4f73\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u663e\u793a\u51fa\u4e86\u66f4\u9ad8\u7684\u89e3\u51b3\u7387\uff0c\u4e14\u4f7f\u7528\u7684\u4e2d\u671f\u8bad\u7ec3\u6570\u636e\u91cf\u4e0d\u5230\u4e00\u534a\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Large Language Models (LLMs) are evolving from basic code generation to more advanced software engineering tasks where they can work independently.</li>\n  <li>Agentic mid-training, which uses real-world data to improve LLMs, is not widely explored due to high resource demands but could be more scalable than traditional methods.</li>\n  <li>A key challenge is the mismatch between static training data and the dynamic nature of real software development environments.</li>\n  <li>The study introduces new data synthesis principles and training methods for effective agent development using two types of data: contextually-native and environmentally-native trajectories.</li>\n  <li>The improved models show better performance in coding tasks, achieving notable resolution rates while using fewer training tokens compared to previous methods.</li>\n</ul>"}, "publishedAt": "2026-01-26T07:20:18.000Z", "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18418.png", "numComments": 2, "submittedBy": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "fullname": "Mohan Jiang (SII)", "name": "mhjiang0408", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "630bc2d186b8b9904c33ce1b", "name": "GAIR", "fullname": "SII - GAIR", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.20354", "authors": [{"_id": "697c1857a67238fac88cc06e", "user": {"_id": "656c9cfef7be0986b49934ea", "avatarUrl": "/avatars/2030e77c28fb4c518b692cd9a20de665.svg", "isPro": false, "fullname": "MuMing", "user": "ZengbinWang", "type": "user"}, "name": "Zengbin Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T09:36:35.067Z", "hidden": false}, {"_id": "697c1857a67238fac88cc06f", "name": "Xuecai Hu", "hidden": false}, {"_id": "697c1857a67238fac88cc070", "name": "Yong Wang", "hidden": false}, {"_id": "697c1857a67238fac88cc071", "name": "Feng Xiong", "hidden": false}, {"_id": "697c1857a67238fac88cc072", "name": "Man Zhang", "hidden": false}, {"_id": "697c1857a67238fac88cc073", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-28T08:15:00.000Z", "submittedOnDailyAt": "2026-01-30T05:01:51.614Z", "title": "Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.", "upvotes": 96, "discussionId": "697c1857a67238fac88cc074", "githubRepo": "https://github.com/AMAP-ML/SpatialGenEval", "githubRepoAddedBy": "user", "ai_summary": "A new benchmark and dataset are introduced to evaluate and improve spatial reasoning capabilities in text-to-image models through information-dense prompts and fine-tuning.", "ai_keywords": ["text-to-image models", "spatial intelligence", "benchmark", "long prompts", "information-dense prompts", "spatial reasoning", "Stable Diffusion-XL", "Uniworld-V1", "OmniGen2", "fine-tuning"], "githubStars": 93, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u65f6\u5e38\u5e38\u5931\u8d25\u3002</li>\n    <li>\u672c\u7814\u7a76\u5f15\u5165\u4e86SpatialGenEval\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30T2I\u6a21\u578b\u7684\u7a7a\u95f4\u667a\u80fd\uff0c\u5305\u62ec1,230\u4e2a\u957f\u7684\u3001\u4fe1\u606f\u5bc6\u96c6\u7684\u63d0\u793a\uff0c\u8986\u76d625\u4e2a\u73b0\u5b9e\u573a\u666f\u3002</li>\n    <li>\u8fd9\u4e9b\u63d0\u793a\u5305\u542b10\u4e2a\u7a7a\u95f4\u5b50\u9886\u57df\u548c\u76f8\u5e94\u7684\u591a\u9879\u9009\u62e9\u95ee\u7b54\uff0c\u6d89\u53ca\u7269\u4f53\u4f4d\u7f6e\u3001\u5e03\u5c40\u3001\u906e\u6321\u548c\u56e0\u679c\u5173\u7cfb\u7b49\u5185\u5bb9\u3002</li>\n    <li>\u5bf921\u4e2a\u5148\u8fdb\u6a21\u578b\u7684\u8bc4\u4f30\u53d1\u73b0\uff0c\u9ad8\u9636\u7a7a\u95f4\u63a8\u7406\u4ecd\u7136\u662f\u4e3b\u8981\u74f6\u9888\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u6784\u5efa\u4e86SpatialT2I\u6570\u636e\u96c6\uff0c\u5305\u542b15,400\u5bf9\u6587\u672c-\u56fe\u50cf\uff0c\u786e\u4fdd\u56fe\u50cf\u4e00\u81f4\u6027\u5e76\u4fdd\u6301\u4fe1\u606f\u5bc6\u5ea6\uff0c\u7ecf\u8fc7\u5fae\u8c03\u540e\u5728\u73b0\u6709\u57fa\u7840\u6a21\u578b\u4e0a\u53d6\u5f97\u4e86\u6027\u80fd\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Text-to-image models are good at creating images but struggle with understanding complex spatial relationships.</li>\n    <li>The new benchmark called SpatialGenEval has 1,230 detailed prompts to test how well these models handle spatial intelligence.</li>\n    <li>SpatialGenEval evaluates 21 advanced models, showing that they still have problems with higher-order spatial reasoning.</li>\n    <li>A new dataset, SpatialT2I, was created with 15,400 text-image pairs to improve model performance while keeping the information rich.</li>\n    <li>Fine-tuning models on this dataset led to better results and more realistic spatial interactions in generated images.</li>\n</ul>"}, "publishedAt": "2026-01-28T03:15:00.000Z", "title": "Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models", "summary": "Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20354.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20614", "authors": [{"_id": "697ac91bdf3e800774f13c12", "name": "Yanqi Dai", "hidden": false}, {"_id": "697ac91bdf3e800774f13c13", "name": "Yuxiang Ji", "hidden": false}, {"_id": "697ac91bdf3e800774f13c14", "name": "Xiao Zhang", "hidden": false}, {"_id": "697ac91bdf3e800774f13c15", "name": "Yong Wang", "hidden": false}, {"_id": "697ac91bdf3e800774f13c16", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "697ac91bdf3e800774f13c17", "name": "Zhiwu Lu", "hidden": false}], "publishedAt": "2026-01-28T13:49:23.000Z", "submittedOnDailyAt": "2026-01-29T00:15:35.371Z", "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation", "submittedOnDailyBy": {"_id": "66cde57cb1fe4c78fe3ab770", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66cde57cb1fe4c78fe3ab770/0R1aA-f_XLjCfy1HwqZ-p.jpeg", "isPro": false, "fullname": "Yanqi Dai", "user": "YanqiDai", "type": "user"}, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.", "upvotes": 93, "discussionId": "697ac91bdf3e800774f13c18", "githubRepo": "https://github.com/AMAP-ML/MathForge", "githubRepoAddedBy": "user", "ai_summary": "MathForge enhances mathematical reasoning in large models through a dual framework combining difficulty-aware policy optimization and multi-aspect question reformulation to address limitations in existing reinforcement learning methods.", "ai_keywords": ["Reinforcement Learning with Verifiable Rewards", "Group Relative Policy Optimization", "Difficulty-Aware Group Policy Optimization", "Multi-Aspect Question Reformulation", "mathematical reasoning", "policy updates", "group advantage estimation", "question-level weighting", "data augmentation"], "githubStars": 84, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u6539\u5584\u4e86\u5927\u578b\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u5728\u89e3\u51b3\u66f4\u96be\u95ee\u9898\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7b97\u6cd5\u548c\u6570\u636e\u4e24\u65b9\u9762\u90fd\u6709\u5f71\u54cd\u3002</li>\n    <li>\u63d0\u51fa\u4e86MathForge\u6846\u67b6\uff0c\u5305\u62ec\u96be\u5ea6\u611f\u77e5\u7684\u7fa4\u4f53\u7b56\u7565\u4f18\u5316\uff08DGPO\uff09\u548c\u591a\u65b9\u9762\u95ee\u9898\u91cd\u6784\uff08MQR\uff09\u7b56\u7565\u3002</li>\n    <li>DGPO\u901a\u8fc7\u96be\u5ea6\u5e73\u8861\u7684\u4f18\u52bf\u4f30\u8ba1\u548c\u95ee\u9898\u7ea7\u522b\u52a0\u6743\u6765\u4f18\u5148\u89e3\u51b3\u66f4\u96be\u7684\u95ee\u9898\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMathForge\u5728\u591a\u79cd\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement Learning with Verifiable Rewards (RLVR) helps improve mathematical reasoning in AI models.</li>\n    <li>Current methods often overlook challenging questions, which are crucial for developing better capabilities.</li>\n    <li>The proposed MathForge framework includes two parts: Difficulty-Aware Group Policy Optimization (DGPO) and Multi-Aspect Question Reformulation (MQR).</li>\n    <li>DGPO addresses issues in existing algorithms by balancing question difficulty and prioritizing harder ones.</li>\n    <li>MQR reformulates questions to increase their difficulty while keeping the correct answer, leading to better performance in math tasks.</li>\n</ul>"}, "publishedAt": "2026-01-28T08:49:23.000Z", "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20614.png", "numComments": 12, "submittedBy": {"_id": "66cde57cb1fe4c78fe3ab770", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66cde57cb1fe4c78fe3ab770/0R1aA-f_XLjCfy1HwqZ-p.jpeg", "fullname": "Yanqi Dai", "name": "YanqiDai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.21204", "authors": [{"_id": "697c3801a67238fac88cc1b1", "name": "Hong Liu", "hidden": false}, {"_id": "697c3801a67238fac88cc1b2", "name": "Jiaqi Zhang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b3", "name": "Chao Wang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b4", "name": "Xing Hu", "hidden": false}, {"_id": "697c3801a67238fac88cc1b5", "name": "Linkun Lyu", "hidden": false}, {"_id": "697c3801a67238fac88cc1b6", "name": "Jiaqi Sun", "hidden": false}, {"_id": "697c3801a67238fac88cc1b7", "name": "Xurui Yang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b8", "name": "Bo Wang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b9", "name": "Fengcun Li", "hidden": false}, {"_id": "697c3801a67238fac88cc1ba", "name": "Yulei Qian", "hidden": false}, {"_id": "697c3801a67238fac88cc1bb", "name": "Lingtong Si", "hidden": false}, {"_id": "697c3801a67238fac88cc1bc", "name": "Yerui Sun", "hidden": false}, {"_id": "697c3801a67238fac88cc1bd", "name": "Rumei Li", "hidden": false}, {"_id": "697c3801a67238fac88cc1be", "name": "Peng Pei", "hidden": false}, {"_id": "697c3801a67238fac88cc1bf", "name": "Yuchen Xie", "hidden": false}, {"_id": "697c3801a67238fac88cc1c0", "name": "Xunliang Cai", "hidden": false}], "publishedAt": "2026-01-29T03:11:19.000Z", "submittedOnDailyAt": "2026-01-30T02:18:11.112Z", "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.", "upvotes": 76, "discussionId": "697c3801a67238fac88cc1c1", "ai_summary": "Embedding scaling offers superior sparsity scaling compared to expert scaling in large language models, enabling efficient inference through system optimizations and speculative decoding.", "ai_keywords": ["Mixture-of-Experts", "sparsity scaling", "embedding scaling", "Pareto frontier", "parameter budgeting", "model width", "model depth", "system optimizations", "speculative decoding", "LongCat-Flash-Lite"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5e38\u7528\u4e8e\u7a00\u758f\u6027\u6269\u5c55\uff0c\u4f46\u9762\u4e34\u6536\u76ca\u9012\u51cf\u548c\u7cfb\u7edf\u74f6\u9888\u7684\u95ee\u9898\u3002</li>\n    <li>\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5d4c\u5165\u6269\u5c55\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u7a00\u758f\u6027\u6269\u5c55\u65b9\u5f0f\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u548c\u5b9e\u9a8c\u627e\u5230\u4e86\u5d4c\u5165\u6269\u5c55\u7684\u4f18\u52bf\u533a\u57df\u3002</li>\n    <li>\u6211\u4eec\u7cfb\u7edf\u5730\u5206\u6790\u4e86\u5f71\u54cd\u5d4c\u5165\u6269\u5c55\u6548\u679c\u7684\u5173\u952e\u67b6\u6784\u56e0\u7d20\uff0c\u5305\u62ec\u53c2\u6570\u9884\u7b97\u3001\u6a21\u578b\u5bbd\u5ea6\u548c\u6df1\u5ea6\u7684\u76f8\u4e92\u4f5c\u7528\u3002</li>\n    <li>\u901a\u8fc7\u96c6\u6210\u7cfb\u7edf\u4f18\u5316\u548c\u63a8\u6d4b\u89e3\u7801\uff0c\u6211\u4eec\u5c06\u7a00\u758f\u6027\u8f6c\u5316\u4e3a\u5b9e\u9645\u7684\u63a8\u7406\u52a0\u901f\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86LongCat-Flash-Lite\u6a21\u578b\uff0c\u62e5\u6709685\u4ebf\u53c2\u6570\uff0c\u5c3d\u7ba1\u53ea\u670930\u4ebf\u53c2\u6570\u88ab\u6fc0\u6d3b\uff0c\u4f46\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u4f18\u4e8e\u540c\u7c7b\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Mixture-of-Experts (MoE) models are popular for making large language models more efficient, but they have some limitations.</li>\n    <li>This study looks at using embedding scaling as a new way to improve efficiency instead of just expert scaling.</li>\n    <li>Through experiments, the researchers found that embedding scaling can perform better in certain situations compared to expert scaling.</li>\n    <li>They also identified key architectural factors that affect how well embedding scaling works, like model size and design.</li>\n    <li>The new model, LongCat-Flash-Lite, has 68.5 billion parameters and performs better than traditional MoE models, especially in tasks related to agents and coding.</li>\n</ul>"}, "publishedAt": "2026-01-28T22:11:19.000Z", "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "summary": "While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21204.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 221, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20540", "authors": [{"_id": "697ac48cdf3e800774f13bc1", "name": "Robbyant Team", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc2", "name": "Zelin Gao", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc3", "user": {"_id": "64981bea09cea550852652af", "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg", "isPro": false, "fullname": "Qiuyu Wang", "user": "qiuyuu", "type": "user"}, "name": "Qiuyu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:16:29.190Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc4", "name": "Yanhong Zeng", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc5", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc6", "user": {"_id": "64acd2ec39fcfebff8c79c00", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64acd2ec39fcfebff8c79c00/Avq66l5hO-aggNtk4Y1ss.png", "isPro": false, "fullname": "Ka Leong Cheng", "user": "felixcheng97", "type": "user"}, "name": "Ka Leong Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T13:56:36.041Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc7", "name": "Yixuan Li", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc8", "user": {"_id": "665f059a8947302aa2c63afe", "avatarUrl": "/avatars/50f560285946532321a0bd526494148d.svg", "isPro": false, "fullname": "hanlin wang", "user": "hlwang06", "type": "user"}, "name": "Hanlin Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:18:10.952Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc9", "name": "Yinghao Xu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bca", "name": "Shuailei Ma", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcb", "name": "Yihang Chen", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcc", "name": "Jie Liu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcd", "name": "Yansong Cheng", "hidden": false}, {"_id": "697ac48cdf3e800774f13bce", "name": "Yao Yao", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcf", "name": "Jiayi Zhu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd0", "user": {"_id": "656084f44e8918182d4f07c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/akAvCUCi7eR31PWOXrVPw.jpeg", "isPro": false, "fullname": "Yihao Meng", "user": "Yhmeng1106", "type": "user"}, "name": "Yihao Meng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T13:56:37.840Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd1", "name": "Kecheng Zheng", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd2", "user": {"_id": "63f0baf66309c84d5f4a2226", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f0baf66309c84d5f4a2226/ihOgtwseRkfP1t-60IgyT.jpeg", "isPro": true, "fullname": "Qingyan", "user": "QingyanBai", "type": "user"}, "name": "Qingyan Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:16:24.535Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd3", "user": {"_id": "6478a982256b62e219917d67", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/PUJ-N2cQxgEmDGfyjajyA.jpeg", "isPro": false, "fullname": "JingyeChen22", "user": "JingyeChen22", "type": "user"}, "name": "Jingye Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:17:37.828Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd4", "name": "Zehong Shen", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd5", "user": {"_id": "662128ec9ca2cd4e6db2fb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662128ec9ca2cd4e6db2fb44/uUg1V-pVfxT3mLuFgJuAN.jpeg", "isPro": false, "fullname": "Bruce Yu", "user": "bruceyyu", "type": "user"}, "name": "Yue Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:16:27.115Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd6", "name": "Xing Zhu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd7", "name": "Yujun Shen", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd8", "name": "Hao Ouyang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64981bea09cea550852652af/HObcL400nFnYaw2kOcjor.mp4"], "publishedAt": "2026-01-28T12:37:01.000Z", "submittedOnDailyAt": "2026-01-29T00:09:39.166Z", "title": "Advancing Open-source World Models", "submittedOnDailyBy": {"_id": "64981bea09cea550852652af", "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg", "isPro": false, "fullname": "Qiuyu Wang", "user": "qiuyuu", "type": "user"}, "summary": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.", "upvotes": 66, "discussionId": "697ac48cdf3e800774f13bd9", "projectPage": "https://technology.robbyant.com/lingbot-world", "githubRepo": "https://github.com/Robbyant/lingbot-world/", "githubRepoAddedBy": "user", "ai_summary": "LingBot-World is an open-source world simulator with high-fidelity dynamics, long-term memory capabilities, and real-time interactivity for diverse environments.", "ai_keywords": ["world simulator", "video generation", "world model", "long-term memory", "real-time interactivity"], "githubStars": 756, "organization": {"_id": "69709f892cd08371c1011a2e", "name": "robbyant", "fullname": "Robbyant", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"}, "summary_zh": "<ul>\n    <li>LingBot-World\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u4e16\u754c\u6a21\u62df\u5668\uff0c\u652f\u6301\u89c6\u9891\u751f\u6210\u3002</li>\n    <li>\u5b83\u5728\u591a\u79cd\u73af\u5883\u4e2d\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u548c\u5f3a\u5927\u7684\u52a8\u6001\u6548\u679c\uff0c\u5305\u62ec\u73b0\u5b9e\u4e3b\u4e49\u3001\u79d1\u5b66\u80cc\u666f\u548c\u5361\u901a\u98ce\u683c\u7b49\u3002</li>\n    <li>LingBot-World\u5177\u5907\u5206\u949f\u7ea7\u7684\u89c6\u91ce\uff0c\u5e76\u80fd\u4fdd\u6301\u957f\u671f\u8bb0\u5fc6\u7684\u4e00\u81f4\u6027\u3002</li>\n    <li>\u5b83\u652f\u6301\u5b9e\u65f6\u4e92\u52a8\uff0c\u751f\u6210\u901f\u5ea6\u4f4e\u4e8e1\u79d2\uff0c\u6bcf\u79d2\u53ef\u751f\u621016\u5e27\u3002</li>\n    <li>\u4ee3\u7801\u548c\u6a21\u578b\u516c\u5f00\uff0c\u65e8\u5728\u7f29\u5c0f\u5f00\u6e90\u548c\u95ed\u6e90\u6280\u672f\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4fc3\u8fdb\u5185\u5bb9\u521b\u4f5c\u3001\u6e38\u620f\u548c\u673a\u5668\u4eba\u5b66\u4e60\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LingBot-World is an open-source world simulator designed for video generation.</li>\n    <li>It works well in various environments, from realistic to cartoon styles, with strong performance and detail.</li>\n    <li>The simulator has a \"long-term memory\" feature that keeps track of context over time.</li>\n    <li>It supports real-time interaction with a quick response time, generating 16 frames per second.</li>\n    <li>The project aims to make technology more accessible and has potential uses in content creation, gaming, and robot learning.</li>\n</ul>"}, "publishedAt": "2026-01-28T07:37:01.000Z", "title": "Advancing Open-source World Models", "summary": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64981bea09cea550852652af/HObcL400nFnYaw2kOcjor.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20540.png", "numComments": 1, "submittedBy": {"_id": "64981bea09cea550852652af", "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg", "fullname": "Qiuyu Wang", "name": "qiuyuu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "69709f892cd08371c1011a2e", "name": "robbyant", "fullname": "Robbyant", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.18491", "authors": [{"_id": "697831d9026bdf0473116e5c", "name": "Dongrui Liu", "hidden": false}, {"_id": "697831d9026bdf0473116e5d", "user": {"_id": "66e2624a436a1798365e4581", "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg", "isPro": false, "fullname": "Qihan Ren", "user": "jasonrqh", "type": "user"}, "name": "Qihan Ren", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:31:15.765Z", "hidden": false}, {"_id": "697831d9026bdf0473116e5e", "name": "Chen Qian", "hidden": false}, {"_id": "697831d9026bdf0473116e5f", "name": "Shuai Shao", "hidden": false}, {"_id": "697831d9026bdf0473116e60", "name": "Yuejin Xie", "hidden": false}, {"_id": "697831d9026bdf0473116e61", "name": "Yu Li", "hidden": false}, {"_id": "697831d9026bdf0473116e62", "name": "Zhonghao Yang", "hidden": false}, {"_id": "697831d9026bdf0473116e63", "name": "Haoyu Luo", "hidden": false}, {"_id": "697831d9026bdf0473116e64", "name": "Peng Wang", "hidden": false}, {"_id": "697831d9026bdf0473116e65", "name": "Qingyu Liu", "hidden": false}, {"_id": "697831d9026bdf0473116e66", "name": "Binxin Hu", "hidden": false}, {"_id": "697831d9026bdf0473116e67", "name": "Ling Tang", "hidden": false}, {"_id": "697831d9026bdf0473116e68", "name": "Jilin Mei", "hidden": false}, {"_id": "697831d9026bdf0473116e69", "name": "Dadi Guo", "hidden": false}, {"_id": "697831d9026bdf0473116e6a", "name": "Leitao Yuan", "hidden": false}, {"_id": "697831d9026bdf0473116e6b", "name": "Junyao Yang", "hidden": false}, {"_id": "697831d9026bdf0473116e6c", "name": "Guanxu Chen", "hidden": false}, {"_id": "697831d9026bdf0473116e6d", "name": "Qihao Lin", "hidden": false}, {"_id": "697831d9026bdf0473116e6e", "name": "Yi Yu", "hidden": false}, {"_id": "697831d9026bdf0473116e6f", "name": "Bo Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e70", "name": "Jiaxuan Guo", "hidden": false}, {"_id": "697831d9026bdf0473116e71", "name": "Jie Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e72", "name": "Wenqi Shao", "hidden": false}, {"_id": "697831d9026bdf0473116e73", "name": "Huiqi Deng", "hidden": false}, {"_id": "697831d9026bdf0473116e74", "name": "Zhiheng Xi", "hidden": false}, {"_id": "697831d9026bdf0473116e75", "name": "Wenjie Wang", "hidden": false}, {"_id": "697831d9026bdf0473116e76", "name": "Wenxuan Wang", "hidden": false}, {"_id": "697831d9026bdf0473116e77", "name": "Wen Shen", "hidden": false}, {"_id": "697831d9026bdf0473116e78", "name": "Zhikai Chen", "hidden": false}, {"_id": "697831d9026bdf0473116e79", "name": "Haoyu Xie", "hidden": false}, {"_id": "697831d9026bdf0473116e7a", "name": "Jialing Tao", "hidden": false}, {"_id": "697831d9026bdf0473116e7b", "name": "Juntao Dai", "hidden": false}, {"_id": "697831d9026bdf0473116e7c", "name": "Jiaming Ji", "hidden": false}, {"_id": "697831d9026bdf0473116e7d", "name": "Zhongjie Ba", "hidden": false}, {"_id": "697831d9026bdf0473116e7e", "name": "Linfeng Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e7f", "name": "Yong Liu", "hidden": false}, {"_id": "697831d9026bdf0473116e80", "name": "Quanshi Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e81", "name": "Lei Zhu", "hidden": false}, {"_id": "697831d9026bdf0473116e82", "name": "Zhihua Wei", "hidden": false}, {"_id": "697831d9026bdf0473116e83", "name": "Hui Xue", "hidden": false}, {"_id": "697831d9026bdf0473116e84", "name": "Chaochao Lu", "hidden": false}, {"_id": "697831d9026bdf0473116e85", "name": "Jing Shao", "hidden": false}, {"_id": "697831d9026bdf0473116e86", "name": "Xia Hu", "hidden": false}], "publishedAt": "2026-01-26T13:45:41.000Z", "submittedOnDailyAt": "2026-01-28T01:26:49.833Z", "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security", "submittedOnDailyBy": {"_id": "66e2624a436a1798365e4581", "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg", "isPro": false, "fullname": "Qihan Ren", "user": "jasonrqh", "type": "user"}, "summary": "The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.", "upvotes": 60, "discussionId": "697831d9026bdf0473116e87", "githubRepo": "https://github.com/AI45Lab/AgentDoG", "githubRepoAddedBy": "user", "ai_summary": "AI agents face safety and security challenges from autonomous tool use and environmental interactions, requiring advanced guardrail frameworks for risk diagnosis and transparent monitoring.", "ai_keywords": ["agentic guardrail", "three-dimensional taxonomy", "agentic safety benchmark", "Diagnostic Guardrail framework", "agent safety and security", "agent trajectories", "root cause diagnosis", "fine-grained monitoring", "model variants", "state-of-the-art performance"], "githubStars": 178, "organization": {"_id": "68f716f832b31e42cbc2be7f", "name": "AI45Research", "fullname": "AI45Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68f6ffaa04d1019724af41fc/EVBafPHXvChszTM5tJcc9.png"}, "summary_zh": "<ul>\n    <li>\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u5174\u8d77\u5e26\u6765\u4e86\u5b89\u5168\u548c\u5b89\u5168\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u81ea\u4e3b\u5de5\u5177\u4f7f\u7528\u548c\u73af\u5883\u4e92\u52a8\u65b9\u9762\u3002</li>\n    <li>\u5f53\u524d\u7684\u5b89\u5168\u6a21\u578b\u7f3a\u4e4f\u5bf9\u4ee3\u7406\u98ce\u9669\u7684\u610f\u8bc6\u548c\u98ce\u9669\u8bca\u65ad\u7684\u900f\u660e\u5ea6\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u7ef4\u5206\u7c7b\u6cd5\uff0c\u4ece\u6765\u6e90\u3001\u5931\u8d25\u6a21\u5f0f\u548c\u540e\u679c\u4e09\u4e2a\u65b9\u9762\u5206\u7c7b\u4ee3\u7406\u98ce\u9669\u3002</li>\n    <li>\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u5b89\u5168\u57fa\u51c6\uff08ATBench\uff09\u548c\u4e00\u4e2a\u8bca\u65ad\u6846\u67b6\uff08AgentDoG\uff09\uff0c\u7528\u4e8e\u76d1\u6d4b\u4ee3\u7406\u7684\u5b89\u5168\u6027\u3002</li>\n    <li>AgentDoG\u80fd\u591f\u8bca\u65ad\u4e0d\u5b89\u5168\u884c\u4e3a\u7684\u6839\u672c\u539f\u56e0\uff0c\u5e76\u63d0\u4f9b\u900f\u660e\u5ea6\uff0c\u5e2e\u52a9\u6709\u6548\u5bf9\u9f50\u4ee3\u7406\uff0c\u5e76\u4e14\u6240\u6709\u6a21\u578b\u548c\u6570\u636e\u96c6\u90fd\u662f\u516c\u5f00\u7684\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The rise of AI agents creates new safety and security challenges due to their ability to act independently and interact with their environments.</li>\n    <li>Current safety models do not effectively recognize risks associated with AI agents and lack transparency in diagnosing these risks.</li>\n    <li>A new system called AgentDoG has been developed, which includes a detailed safety benchmark and a framework for monitoring and diagnosing AI agent behavior.</li>\n    <li>AgentDoG can identify the causes of unsafe actions and behaviors that appear safe but are unreasonable, providing better understanding and alignment for AI agents.</li>\n    <li>Different versions of AgentDoG are available, and tests show it performs better than existing methods in ensuring AI safety in various complex situations.</li>\n</ul>"}, "publishedAt": "2026-01-26T08:45:41.000Z", "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security", "summary": "The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18491.png", "numComments": 6, "submittedBy": {"_id": "66e2624a436a1798365e4581", "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg", "fullname": "Qihan Ren", "name": "jasonrqh", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "68f716f832b31e42cbc2be7f", "name": "AI45Research", "fullname": "AI45Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68f6ffaa04d1019724af41fc/EVBafPHXvChszTM5tJcc9.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.19325", "authors": [{"_id": "69798298df44b75fa47e47a9", "name": "Zichen Wen", "hidden": false}, {"_id": "69798298df44b75fa47e47aa", "user": {"_id": "688c72c011ef3399b561dee7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/688c72c011ef3399b561dee7/puhgnTOAfZYetsC46hqGm.jpeg", "isPro": false, "fullname": "BoxueYang", "user": "Boxue", "type": "user"}, "name": "Boxue Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:17:12.982Z", "hidden": false}, {"_id": "69798298df44b75fa47e47ab", "name": "Shuang Chen", "hidden": false}, {"_id": "69798298df44b75fa47e47ac", "name": "Yaojie Zhang", "hidden": false}, {"_id": "69798298df44b75fa47e47ad", "name": "Yuhang Han", "hidden": false}, {"_id": "69798298df44b75fa47e47ae", "name": "Junlong Ke", "hidden": false}, {"_id": "69798298df44b75fa47e47af", "name": "Cong Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47b0", "name": "Yicheng Fu", "hidden": false}, {"_id": "69798298df44b75fa47e47b1", "name": "Jiawang Zhao", "hidden": false}, {"_id": "69798298df44b75fa47e47b2", "name": "Jiangchao Yao", "hidden": false}, {"_id": "69798298df44b75fa47e47b3", "name": "Xi Fang", "hidden": false}, {"_id": "69798298df44b75fa47e47b4", "name": "Zhen Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47b5", "name": "Henxing Cai", "hidden": false}, {"_id": "69798298df44b75fa47e47b6", "name": "Lin Yao", "hidden": false}, {"_id": "69798298df44b75fa47e47b7", "name": "Zhifeng Gao", "hidden": false}, {"_id": "69798298df44b75fa47e47b8", "name": "Yanhui Hong", "hidden": false}, {"_id": "69798298df44b75fa47e47b9", "name": "Nang Yuan", "hidden": false}, {"_id": "69798298df44b75fa47e47ba", "name": "Yixuan Li", "hidden": false}, {"_id": "69798298df44b75fa47e47bb", "name": "Guojiang Zhao", "hidden": false}, {"_id": "69798298df44b75fa47e47bc", "name": "Haoyi Tao", "hidden": false}, {"_id": "69798298df44b75fa47e47bd", "name": "Nan Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47be", "name": "Han Lyu", "hidden": false}, {"_id": "69798298df44b75fa47e47bf", "name": "Guolin Ke", "hidden": false}, {"_id": "69798298df44b75fa47e47c0", "name": "Ning Liao", "hidden": false}, {"_id": "69798298df44b75fa47e47c1", "name": "Xiaoxing Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47c2", "name": "Kai Chen", "hidden": false}, {"_id": "69798298df44b75fa47e47c3", "name": "Zhiyu Li", "hidden": false}, {"_id": "69798298df44b75fa47e47c4", "name": "Feiyu Xiong", "hidden": false}, {"_id": "69798298df44b75fa47e47c5", "name": "Sihan Hu", "hidden": false}, {"_id": "69798298df44b75fa47e47c6", "name": "Kun Chen", "hidden": false}, {"_id": "69798298df44b75fa47e47c7", "name": "Yanfeng Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47c8", "name": "Weinan E", "hidden": false}, {"_id": "69798298df44b75fa47e47c9", "name": "Linfeng Zhang", "hidden": false}, {"_id": "69798298df44b75fa47e47ca", "name": "Linfeng Zhang", "hidden": false}], "publishedAt": "2026-01-27T08:12:18.000Z", "submittedOnDailyAt": "2026-01-29T01:20:58.570Z", "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery", "submittedOnDailyBy": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "isPro": false, "fullname": "Zichen Wen", "user": "zichenwen", "type": "user"}, "summary": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.", "upvotes": 53, "discussionId": "69798298df44b75fa47e47cb", "projectPage": "https://innovatorlm.github.io/Innovator-VL", "githubRepo": "https://github.com/InnovatorLM/Innovator-VL", "githubRepoAddedBy": "user", "ai_summary": "Innovator-VL demonstrates that principled training design and transparent methodology can achieve strong scientific intelligence with reduced data requirements while maintaining general vision performance.", "ai_keywords": ["multimodal large language model", "scientific multimodal large language model", "end-to-end reproducible training pipeline", "supervised fine-tuning", "reinforcement learning", "scientific reasoning", "data efficiency", "principled data selection", "generalization", "scientific alignment"], "githubStars": 70, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>Innovator-VL \u662f\u4e00\u4e2a\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u5347\u79d1\u5b66\u9886\u57df\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u900f\u660e\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u6db5\u76d6\u6570\u636e\u6536\u96c6\u3001\u6e05\u7406\u3001\u9884\u5904\u7406\u3001\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u8bc4\u4f30\uff0c\u4fbf\u4e8e\u793e\u533a\u7684\u7cfb\u7edf\u6269\u5c55\u3002</li>\n    <li>Innovator-VL \u5728\u79d1\u5b66\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u53ea\u9700\u4e0d\u5230\u4e94\u767e\u4e07\u4e2a\u6837\u672c\uff0c\u4e14\u65e0\u9700\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u663e\u793a\u51fa\u9ad8\u6548\u7684\u6570\u636e\u5229\u7528\u7387\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u901a\u7528\u89c6\u89c9\u548c\u591a\u6a21\u6001\u63a8\u7406\u7b49\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u826f\u597d\uff0c\u8868\u660e\u79d1\u5b66\u77e5\u8bc6\u53ef\u4ee5\u4e0e\u901a\u7528\u80fd\u529b\u7ed3\u5408\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u6ca1\u6709\u5927\u89c4\u6a21\u6570\u636e\uff0c\u4e5f\u53ef\u4ee5\u6784\u5efa\u9ad8\u6548\u3001\u53ef\u91cd\u73b0\u4e14\u6027\u80fd\u4f18\u826f\u7684\u79d1\u5b66\u591a\u6a21\u6001\u6a21\u578b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Innovator-VL is a new model that helps understand and reason in various scientific areas while performing well on general vision tasks.</li>\n    <li>The model uses a clear and reproducible training process, allowing others in the community to build on it easily.</li>\n    <li>It is data-efficient, performing well on scientific tasks with fewer than five million samples, showing that smart data choices are more important than just using a lot of data.</li>\n    <li>Innovator-VL can generalize well, performing competitively across both scientific and general vision tasks without losing effectiveness.</li>\n    <li>This work shows that high-performing scientific models can be created without needing large amounts of data, paving the way for future research.</li>\n</ul>"}, "publishedAt": "2026-01-27T03:12:18.000Z", "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery", "summary": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19325.png", "numComments": 1, "submittedBy": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "fullname": "Zichen Wen", "name": "zichenwen", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.17737", "authors": [{"_id": "6978310b026bdf0473116e44", "user": {"_id": "64545c77a7ce0a8fde809912", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VDaMEM77Xv09dP6B5v3sK.jpeg", "isPro": false, "fullname": "ChenYuMu", "user": "ChenYuMu", "type": "user"}, "name": "Chenyu Mu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:01:21.462Z", "hidden": false}, {"_id": "6978310b026bdf0473116e45", "user": {"_id": "6527a2df1eb78901534b0cc6", "avatarUrl": "/avatars/f811d8c108930b41e2612c609d35e2eb.svg", "isPro": false, "fullname": "Xin He", "user": "Kleinhe", "type": "user"}, "name": "Xin He", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:03:20.414Z", "hidden": false}, {"_id": "6978310b026bdf0473116e46", "user": {"_id": "64300415b009240418dac70c", "avatarUrl": "/avatars/5175cdbc7683b0b52d5c742e93d3be83.svg", "isPro": false, "fullname": "Qu Yang", "user": "quyang22", "type": "user"}, "name": "Qu Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:03:22.475Z", "hidden": false}, {"_id": "6978310b026bdf0473116e47", "name": "Wanshun Chen", "hidden": false}, {"_id": "6978310b026bdf0473116e48", "name": "Jiadi Yao", "hidden": false}, {"_id": "6978310b026bdf0473116e49", "name": "Huang Liu", "hidden": false}, {"_id": "6978310b026bdf0473116e4a", "name": "Zihao Yi", "hidden": false}, {"_id": "6978310b026bdf0473116e4b", "name": "Bo Zhao", "hidden": false}, {"_id": "6978310b026bdf0473116e4c", "name": "Xingyu Chen", "hidden": false}, {"_id": "6978310b026bdf0473116e4d", "name": "Ruotian Ma", "hidden": false}, {"_id": "6978310b026bdf0473116e4e", "name": "Fanghua Ye", "hidden": false}, {"_id": "6978310b026bdf0473116e4f", "name": "Erkun Yang", "hidden": false}, {"_id": "6978310b026bdf0473116e50", "name": "Cheng Deng", "hidden": false}, {"_id": "6978310b026bdf0473116e51", "name": "Zhaopeng Tu", "hidden": false}, {"_id": "6978310b026bdf0473116e52", "name": "Xiaolong Li", "hidden": false}, {"_id": "6978310b026bdf0473116e53", "name": "Linus", "hidden": false}], "publishedAt": "2026-01-25T08:10:28.000Z", "submittedOnDailyAt": "2026-01-27T01:05:46.612Z", "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation", "submittedOnDailyBy": {"_id": "67485743561b1e6f9579389f", "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg", "isPro": false, "fullname": "Zhaopeng Tu", "user": "zptu", "type": "user"}, "summary": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.", "upvotes": 46, "discussionId": "6978310b026bdf0473116e54", "projectPage": "https://xd-mu.github.io/ScriptIsAllYouNeed/", "githubRepo": "https://github.com/Tencent/digitalhuman/tree/main/ScriptAgent", "githubRepoAddedBy": "user", "ai_summary": "A novel end-to-end agentic framework translates dialogue into cinematic videos through specialized agents that generate and orchestrate video content while maintaining narrative coherence.", "ai_keywords": ["video generation", "dialogue-to-cinematic-video", "ScripterAgent", "DirectorAgent", "cross-scene continuous generation", "ScriptBench", "Visual-Script Alignment", "CriticAgent"], "githubStars": 228, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "summary_zh": "<ul>\n    <li>\u6700\u65b0\u7684\u89c6\u9891\u751f\u6210\u6280\u672f\u53ef\u4ee5\u6839\u636e\u7b80\u5355\u7684\u6587\u672c\u63d0\u793a\u751f\u6210\u4ee4\u4eba\u60ca\u53f9\u7684\u89c6\u89c9\u5185\u5bb9\u3002</li>\n    <li>\u8fd9\u4e9b\u6a21\u578b\u5728\u751f\u6210\u8fde\u8d2f\u7684\u957f\u7bc7\u53d9\u8ff0\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u65e0\u6cd5\u5c06\u9ad8\u5c42\u6b21\u7684\u6982\u5ff5\u8f6c\u5316\u4e3a\u7535\u5f71\u573a\u666f\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u8bdd\u5230\u7535\u5f71\u89c6\u9891\u751f\u6210\u7684\u6846\u67b6\uff0c\u6838\u5fc3\u662fScripterAgent\u6a21\u578b\uff0c\u7528\u4e8e\u5c06\u7c97\u7565\u5bf9\u8bdd\u7ffb\u8bd1\u4e3a\u53ef\u6267\u884c\u7684\u7ec6\u81f4\u5267\u672c\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86ScriptBench\uff0c\u4e00\u4e2a\u5927\u578b\u57fa\u51c6\uff0c\u63d0\u4f9b\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\uff0c\u5e2e\u52a9\u751f\u6210\u66f4\u597d\u7684\u5267\u672c\u3002</li>\n    <li>\u901a\u8fc7\u8bc4\u4f30\uff0c\u6211\u4eec\u53d1\u73b0\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u5267\u672c\u7684\u5fe0\u5b9e\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u89c6\u89c9\u6548\u679c\u548c\u5267\u672c\u9075\u5faa\u4e4b\u95f4\u7684\u6743\u8861\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New models can create impressive videos from text, but they struggle with long stories and dialogue.</li>\n    <li>To improve this, a new system called ScripterAgent translates basic dialogue into detailed scripts for videos.</li>\n    <li>ScriptBench is a new benchmark that helps train and evaluate this system with rich context and expert input.</li>\n    <li>Another part of the system, DirectorAgent, manages video creation to keep the story coherent over time.</li>\n    <li>Evaluation shows that this new approach improves how closely the video matches the script and maintains a good flow, but highlights a trade-off between visual appeal and sticking to the script.</li>\n</ul>"}, "publishedAt": "2026-01-25T03:10:28.000Z", "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation", "summary": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17737.png", "numComments": 3, "submittedBy": {"_id": "67485743561b1e6f9579389f", "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg", "fullname": "Zhaopeng Tu", "name": "zptu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.22153", "authors": [{"_id": "697c2899a67238fac88cc115", "user": {"_id": "63f47b5321eb234ab739e91a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg", "isPro": false, "fullname": "Haozhe Xie", "user": "hzxie", "type": "user"}, "name": "Haozhe Xie", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T13:31:48.996Z", "hidden": false}, {"_id": "697c2899a67238fac88cc116", "user": {"_id": "672392c4a4c4381cefc06416", "avatarUrl": "/avatars/8ee84a7e3e91e5d13074bc3c407ff75d.svg", "isPro": false, "fullname": "Wen Beichen", "user": "wenbc21", "type": "user"}, "name": "Beichen Wen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T13:31:52.487Z", "hidden": false}, {"_id": "697c2899a67238fac88cc117", "user": {"_id": "6899ff3f4c5ca50a326bb456", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Nuqof2ofdaQUD5b07cDnG.png", "isPro": false, "fullname": "Zheng Jiarui", "user": "zghtyarecrenj", "type": "user"}, "name": "Jiarui Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T13:31:46.030Z", "hidden": false}, {"_id": "697c2899a67238fac88cc118", "name": "Zhaoxi Chen", "hidden": false}, {"_id": "697c2899a67238fac88cc119", "name": "Fangzhou Hong", "hidden": false}, {"_id": "697c2899a67238fac88cc11a", "name": "Haiwen Diao", "hidden": false}, {"_id": "697c2899a67238fac88cc11b", "name": "Ziwei Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/p9cPxETttQUS23woFb14M.mp4"], "publishedAt": "2026-01-29T18:59:51.000Z", "submittedOnDailyAt": "2026-01-30T01:46:39.673Z", "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation", "submittedOnDailyBy": {"_id": "63f47b5321eb234ab739e91a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg", "isPro": false, "fullname": "Haozhe Xie", "user": "hzxie", "type": "user"}, "summary": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.", "upvotes": 45, "discussionId": "697c2899a67238fac88cc11c", "projectPage": "https://haozhexie.com/project/dynamic-vla", "githubRepo": "https://github.com/hzxie/DynamicVLA", "githubRepoAddedBy": "user", "ai_summary": "DynamicVLA addresses dynamic object manipulation challenges through a compact vision-language-action model with temporal reasoning and closed-loop adaptation, supported by a new benchmark for dynamic manipulation tasks.", "ai_keywords": ["Vision-Language-Action models", "temporal reasoning", "closed-loop adaptation", "convolutional vision encoder", "multimodal inference", "Continuous Inference", "Latent-aware Action Streaming", "Dynamic Object Manipulation benchmark", "synthetic episodes", "real-world episodes"], "githubStars": 48, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "summary_zh": "<ul>\n    <li>\u52a8\u6001\u7269\u4f53\u64cd\u63a7\u4ecd\u7136\u662f\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u6a21\u578b\u7684\u4e00\u4e2a\u6311\u6218\uff0c\u5c24\u5176\u5728\u5feb\u901f\u611f\u77e5\u548c\u63a7\u5236\u65b9\u9762\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DynamicVLA\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\u6765\u6539\u5584\u52a8\u6001\u7269\u4f53\u64cd\u63a7\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6846\u67b6\u4f7f\u7528\u9ad8\u6548\u7684\u5377\u79ef\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u652f\u6301\u5feb\u901f\u7684\u591a\u6a21\u6001\u63a8\u7406\u3002</li>\n    <li>\u5f15\u5165\u4e86\u52a8\u6001\u7269\u4f53\u64cd\u63a7\u57fa\u51c6\uff08DOM\uff09\uff0c\u6536\u96c6\u4e8620\u4e07\u4e2a\u5408\u6210\u6837\u672c\u548c2000\u4e2a\u771f\u5b9e\u573a\u666f\u6570\u636e\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cDynamicVLA\u5728\u54cd\u5e94\u901f\u5ea6\u3001\u611f\u77e5\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DynamicVLA is a new system designed to help machines manipulate moving objects better than current models.</li>\n    <li>It uses a small but efficient vision encoder to quickly process visual information.</li>\n    <li>The system allows for continuous thinking and acting, which helps it to respond faster to moving objects.</li>\n    <li>DynamicVLA also includes a new dataset called the Dynamic Object Manipulation (DOM) benchmark, which contains a large number of synthetic and real-world scenarios for training and testing.</li>\n    <li>Tests show that DynamicVLA is much faster and more accurate at manipulating dynamic objects compared to previous methods.</li>\n</ul>"}, "publishedAt": "2026-01-29T13:59:51.000Z", "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation", "summary": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/p9cPxETttQUS23woFb14M.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22153.png", "numComments": 2, "submittedBy": {"_id": "63f47b5321eb234ab739e91a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg", "fullname": "Haozhe Xie", "name": "hzxie", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 19, "isUserFollowing": false}, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u5728\u89c6\u9891\u95ee\u7b54\u4e2d\uff0c\u89c6\u9891\u901a\u5e38\u53ea\u63d0\u4f9b\u5c40\u90e8\u89c6\u89c9\u7ebf\u7d22\uff0c\u800c\u53ef\u4ee5\u9a8c\u8bc1\u7684\u7b54\u6848\u5206\u6563\u5728\u7f51\u7edc\u4e0a\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0c\u540d\u4e3aVideoDR\u3002</li>\n    <li>VideoDR\u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u4e0b\u7684\u5f00\u653e\u9886\u57df\u89c6\u9891\u95ee\u7b54\uff0c\u9700\u8981\u63d0\u53d6\u8de8\u5e27\u89c6\u89c9\u951a\u70b9\u3001\u8fdb\u884c\u4ea4\u4e92\u5f0f\u7f51\u7edc\u68c0\u7d22\u548c\u591a\u8df3\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u5c01\u95ed\u6e90\u548c\u5f00\u653e\u6e90\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u6a21\u578b\u5728\u5904\u7406\u80fd\u529b\u4e0a\u7684\u5dee\u5f02\u3002</li>\n    <li>VideoDR\u4e3a\u7814\u7a76\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u89c6\u9891\u4ee3\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u4e0b\u4e00\u4ee3\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u4e3b\u8981\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video question answering often relies on visual cues from videos and information from the internet.</li>\n    <li>The new benchmark called VideoDR helps test video-based question answering by using visual cues and web information.</li>\n    <li>VideoDR includes high-quality video samples from different topics and requires advanced reasoning and retrieval skills.</li>\n    <li>Tests show that different model approaches (Workflow vs. Agentic) have varying success based on how well they maintain video information during retrieval.</li>\n    <li>The study highlights important challenges for future video research agents, such as keeping track of goals and consistency over time.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u7406\u89e3\u4e0a\u4ecd\u7136\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\uff0c\u5bfc\u81f4\u5b83\u4eec\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0d\u5982\u4eba\u7c7b\uff0c\u751a\u81f3\u4e0d\u59823\u5c81\u7684\u513f\u7ae5\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86BabyVision\u57fa\u51c6\uff0c\u65e8\u5728\u72ec\u7acb\u4e8e\u8bed\u8a00\u77e5\u8bc6\u8bc4\u4f30MLLMs\u7684\u6838\u5fc3\u89c6\u89c9\u80fd\u529b\uff0c\u5171\u6709388\u4e2a\u9879\u76ee\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\u548c\u56db\u4e2a\u4e3b\u8981\u7c7b\u522b\u3002</li>\n    <li>\u5b9e\u9a8c\u8bc1\u660e\uff0c\u9886\u5148\u7684MLLMs\u5728BabyVision\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c6\uff0c\u5982Gemini3-Pro-Preview\u7684\u5f97\u5206\u4e3a49.7\uff0c\u8fdc\u4f4e\u4e8e6\u5c81\u513f\u7ae5\u548c94.1\u7684\u6210\u4eba\u5e73\u5747\u5f97\u5206\u3002</li>\n    <li>\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u8bc4\u4f30\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5f53\u524d\u7684MLLMs\u4ecd\u7f3a\u4e4f\u57fa\u672c\u7684\u89c6\u89c9\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86BabyVision-Gen\u548c\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\uff0c\u63a2\u7d22\u7528\u751f\u6210\u6a21\u578b\u89e3\u51b3\u89c6\u89c9\u63a8\u7406\u95ee\u9898\uff0c\u5e76\u5728https://github.com/UniPat-AI/BabyVision\u53d1\u5e03\u4e86\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans develop basic visual skills before they learn language, but current Multimodal LLMs (MLLMs) depend too much on language, making their visual understanding weak.</li>\n    <li>A benchmark called BabyVision was created to test MLLMs' core visual skills without using language, featuring 388 tasks in 22 subclasses across four categories.</li>\n    <li>Results show that top MLLMs perform poorly on visual tasks compared to humans; for example, Gemini3-Pro-Preview scored 49.7, much lower than the average adult score of 94.1.</li>\n    <li>This indicates that while MLLMs are good at knowledge-heavy tasks, they still struggle with basic visual skills that even young children can easily handle.</li>\n    <li>The BabyVision project aims to improve visual perception and reasoning in MLLMs, and additional tools for visual reasoning are also being developed.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u591a\u79cd\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\uff0c\u5206\u5272\u8fd9\u4e9b\u5b9e\u4f53\u5bf9\u5e94\u7528\u7a0b\u5e8f\u5f88\u91cd\u8981\u3002</li>\n    <li>\u76ee\u524d\u7684\u5206\u5272\u6a21\u578b\u80fd\u5904\u7406\u7269\u7406\u5c5e\u6027\u5b9e\u4f53\uff0c\u4f46\u5bf9\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\u4ecd\u6709\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u540d\u4e3aSocioSeg\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u548c\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86SocioReasoner\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u8bc6\u522b\u548c\u591a\u9636\u6bb5\u63a8\u7406\u6765\u8bc6\u522b\u548c\u6807\u6ce8\u5b9e\u4f53\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u73b0\u6709\u6a21\u578b\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5177\u5907\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban areas have many important features that need to be identified from satellite images.</li>\n    <li>Current models can detect physical features like buildings but struggle with social categories like schools and parks.</li>\n    <li>The study introduces a new dataset called SocioSeg, which includes satellite images, maps, and labels for social features.</li>\n    <li>A new framework called SocioReasoner helps identify these social features using advanced reasoning techniques.</li>\n    <li>Tests show that this approach outperforms existing models and works well even with new data.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.16725", "authors": [{"_id": "6976d5405d41524304c13537", "name": "Meituan LongCat Team", "hidden": false}, {"_id": "6976d5405d41524304c13538", "name": "Anchun Gui", "hidden": false}, {"_id": "6976d5405d41524304c13539", "name": "Bei Li", "hidden": false}, {"_id": "6976d5405d41524304c1353a", "name": "Bingyang Tao", "hidden": false}, {"_id": "6976d5405d41524304c1353b", "name": "Bole Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1353c", "name": "Borun Chen", "hidden": false}, {"_id": "6976d5405d41524304c1353e", "name": "Chao Zhang", "hidden": false}, {"_id": "69772bc15d41524304c13739", "name": "Chao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1353f", "name": "Chen Gao", "hidden": false}, {"_id": "6976d5405d41524304c13540", "name": "Chen Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13541", "name": "Chengcheng Han", "hidden": false}, {"_id": "6976d5405d41524304c13542", "name": "Chenhui Yang", "hidden": false}, {"_id": "6976d5405d41524304c13543", "name": "Chuyu Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13544", "name": "Cong Chen", "hidden": false}, {"_id": "6976d5405d41524304c13545", "name": "Cunguang Wang", "hidden": false}, {"_id": "6976d5405d41524304c13546", "name": "Daoru Pan", "hidden": false}, {"_id": "6976d5405d41524304c13547", "name": "Defei Bu", "hidden": false}, {"_id": "6976d5405d41524304c13548", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13549", "name": "Di Xiu", "hidden": false}, {"_id": "6976d5405d41524304c1354a", "name": "Dishan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1354b", "name": "Dongyu Ru", "hidden": false}, {"_id": "6976d5405d41524304c1354c", "name": "Dunwei Tu", "hidden": false}, {"_id": "6976d5405d41524304c1354d", "name": "Fan Wu", "hidden": false}, {"_id": "6976d5405d41524304c1354e", "name": "Fengcheng Yuan", "hidden": false}, {"_id": "6976d5405d41524304c1354f", "name": "Fengcun Li", "hidden": false}, {"_id": "6976d5405d41524304c13550", "name": "Gang Xu", "hidden": false}, {"_id": "6976d5405d41524304c13551", "name": "Guanyu Wu", "hidden": false}, {"_id": "6976d5405d41524304c13552", "name": "Guoyuan Lin", "hidden": false}, {"_id": "6976d5405d41524304c13553", "name": "Haibin Wang", "hidden": false}, {"_id": "6976d5405d41524304c13554", "name": "Hansi Yang", "hidden": false}, {"_id": "6976d5405d41524304c13555", "name": "Hao Yang", "hidden": false}, {"_id": "6976d5405d41524304c13556", "name": "Haonan Yan", "hidden": false}, {"_id": "6976d5405d41524304c13557", "name": "Haoxiang Ma", "hidden": false}, {"_id": "6976d5405d41524304c13558", "name": "Haoxing Wen", "hidden": false}, {"_id": "6976d5405d41524304c13559", "name": "Hongyan Hao", "hidden": false}, {"_id": "6976d5405d41524304c1355a", "name": "Hongyin Tang", "hidden": false}, {"_id": "6976d5405d41524304c1355b", "name": "Hongyu Zang", "hidden": false}, {"_id": "6976d5405d41524304c1355c", "name": "Hongzhi Ni", "hidden": false}, {"_id": "6976d5405d41524304c1355d", "name": "Hui Su", "hidden": false}, {"_id": "6976d5405d41524304c1355e", "name": "Jiacheng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1355f", "name": "Jiahong Zhou", "hidden": false}, {"_id": "6976d5405d41524304c13560", "name": "Jiahuan Li", "hidden": false}, {"_id": "6976d5405d41524304c13561", "name": "Jiaming Wang", "hidden": false}, {"_id": "6976d5405d41524304c13562", "name": "Jian Yang", "hidden": false}, {"_id": "6976d5405d41524304c13563", "user": {"_id": "64008a0af4ff62c2616d8858", "avatarUrl": "/avatars/b52c98857916fba5377ace8089d658b2.svg", "isPro": false, "fullname": "zhangjf", "user": "zhangjf", "type": "user"}, "name": "Jianfei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:09.272Z", "hidden": false}, {"_id": "6976d5405d41524304c13564", "name": "Jianhao Xu", "hidden": false}, {"_id": "6976d5405d41524304c13565", "name": "Jianing Wang", "hidden": false}, {"_id": "6976d5405d41524304c13566", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13567", "name": "Jiaqi Sun", "hidden": false}, {"_id": "6976d5405d41524304c13568", "name": "Jiarong Shi", "hidden": false}, {"_id": "6976d5405d41524304c13569", "name": "Jiarui Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1356a", "name": "Jingang Wang", "hidden": false}, {"_id": "6976d5405d41524304c1356b", "user": {"_id": "6592472fccbc1e2cc7250903", "avatarUrl": "/avatars/6f04ae66944eb2ce65c5aca7927bab10.svg", "isPro": false, "fullname": "Jinluan Yang", "user": "Jinluan", "type": "user"}, "name": "Jinluan Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:47.175Z", "hidden": false}, {"_id": "6976d5405d41524304c1356c", "name": "Jinrui Ding", "hidden": false}, {"_id": "6976d5405d41524304c1356d", "name": "Jinwei Xiao", "hidden": false}, {"_id": "6976d5405d41524304c1356e", "name": "Jiyuan He", "hidden": false}, {"_id": "6976d5405d41524304c1356f", "name": "Juncan Xu", "hidden": false}, {"_id": "6976d5405d41524304c13570", "name": "Kefeng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13571", "name": "Keheng Wang", "hidden": false}, {"_id": "6976d5405d41524304c13572", "name": "Li Wei", "hidden": false}, {"_id": "6976d5405d41524304c13573", "name": "Lianhui Ma", "hidden": false}, {"_id": "6976d5405d41524304c13574", "name": "Lin Qiu", "hidden": false}, {"_id": "6976d5405d41524304c13575", "name": "Lingbing Kong", "hidden": false}, {"_id": "6976d5405d41524304c13576", "name": "Lingchuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c13577", "name": "Linsen Guo", "hidden": false}, {"_id": "6976d5405d41524304c13578", "name": "Mengshen Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13579", "name": "Mengxia Shen", "hidden": false}, {"_id": "6976d5405d41524304c1357a", "name": "Mingyang Zhu", "hidden": false}, {"_id": "6976d5405d41524304c1357b", "name": "Peiguang Li", "hidden": false}, {"_id": "6976d5405d41524304c1357c", "name": "Peng Pei", "hidden": false}, {"_id": "6976d5405d41524304c1357d", "name": "Pengcheng Jia", "hidden": false}, {"_id": "6976d5405d41524304c1357e", "name": "Pengtao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1357f", "name": "Peng Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13580", "name": "Qi Gu", "hidden": false}, {"_id": "6976d5405d41524304c13581", "name": "Qiong Huang", "hidden": false}, {"_id": "6976d5405d41524304c13582", "name": "Qiyuan Duan", "hidden": false}, {"_id": "6976d5405d41524304c13583", "name": "Quanchi Weng", "hidden": false}, {"_id": "6976d5405d41524304c13584", "name": "Rongxiang Weng", "hidden": false}, {"_id": "6976d5405d41524304c13585", "name": "Rongzhi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13586", "name": "Rumei Li", "hidden": false}, {"_id": "6976d5405d41524304c13587", "name": "Shanglin Lei", "hidden": false}, {"_id": "6976d5405d41524304c13588", "user": {"_id": "64db5f5dd68a6ddcc7bd89e9", "avatarUrl": "/avatars/69375ec915927b855813df8a6d486837.svg", "isPro": false, "fullname": "Shengnan An", "user": "ShengnanAn", "type": "user"}, "name": "Shengnan An", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:11.410Z", "hidden": false}, {"_id": "6976d5405d41524304c13589", "name": "Shijun Dai", "hidden": false}, {"_id": "6976d5405d41524304c1358a", "name": "Shuaikang Liu", "hidden": false}, {"_id": "6976d5405d41524304c1358b", "name": "Shuang Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1358c", "name": "Shuo Wang", "hidden": false}, {"_id": "6976d5405d41524304c1358d", "name": "Songyuan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1358e", "name": "Tao Liang", "hidden": false}, {"_id": "6976d5405d41524304c1358f", "name": "Tianhao Hu", "hidden": false}, {"_id": "6976d5405d41524304c13590", "name": "Tianze Chen", "hidden": false}, {"_id": "6976d5405d41524304c13591", "name": "Wei Liu", "hidden": false}, {"_id": "6976d5405d41524304c13592", "name": "Wei Shi", "hidden": false}, {"_id": "6976d5405d41524304c13593", "name": "Wei Wang", "hidden": false}, {"_id": "6976d5405d41524304c13594", "name": "Weifeng Tang", "hidden": false}, {"_id": "6976d5405d41524304c13595", "name": "Wenjie Shi", "hidden": false}, {"_id": "6976d5405d41524304c13596", "name": "Wenlong Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13597", "name": "Wentao Chen", "hidden": false}, {"_id": "6976d5405d41524304c13598", "name": "Wentao Shi", "hidden": false}, {"_id": "6976d5405d41524304c13599", "name": "Xi Su", "hidden": false}, {"_id": "6976d5405d41524304c1359a", "name": "Xiangcheng Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359b", "name": "Xiandi Ma", "hidden": false}, {"_id": "6976d5405d41524304c1359c", "user": {"_id": "63edb098679c2cc40abc6c2e", "avatarUrl": "/avatars/288c7229937c2c3f29fda6d17c7df2eb.svg", "isPro": false, "fullname": "Xiangyu", "user": "xixy", "type": "user"}, "name": "Xiangyu Xi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:13.312Z", "hidden": false}, {"_id": "6976d5405d41524304c1359d", "name": "Xiangyuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359e", "name": "Xiangzhou Huang", "hidden": false}, {"_id": "6976d5405d41524304c1359f", "name": "Xiao Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a0", "name": "Xiaodong Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a1", "name": "Xiaolong Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a2", "name": "Xiaowei Shi", "hidden": false}, {"_id": "6976d5405d41524304c135a3", "name": "Xiaoyu Li", "hidden": false}, {"_id": "6976d5405d41524304c135a4", "name": "Xin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a5", "name": "Xingchen Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a6", "name": "Xuan Huang", "hidden": false}, {"_id": "6976d5405d41524304c135a7", "name": "Xuezhi Cao", "hidden": false}, {"_id": "6976d5405d41524304c135a8", "name": "Xunliang Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a9", "name": "Yan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135aa", "user": {"_id": "63fc1c420aab06079200c15c", "avatarUrl": "/avatars/8e8e82a9a6552848581ca9f65011263c.svg", "isPro": false, "fullname": "yang bai", "user": "byang", "type": "user"}, "name": "Yang Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:07.036Z", "hidden": false}, {"_id": "6976d5405d41524304c135ab", "name": "Yang Liu", "hidden": false}, {"_id": "6976d5405d41524304c135ac", "name": "Yang Yang", "hidden": false}, {"_id": "6976d5405d41524304c135ad", "name": "Yang Zheng", "hidden": false}, {"_id": "6976d5405d41524304c135ae", "name": "Yaoming Wang", "hidden": false}, {"_id": "6976d5405d41524304c135af", "name": "Yaoming Zhu", "hidden": false}, {"_id": "6976d5405d41524304c135b0", "name": "Yaqi Huo", "hidden": false}, {"_id": "6976d5405d41524304c135b1", "name": "Yanyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b2", "name": "Yaorui Shi", "hidden": false}, {"_id": "6976d5405d41524304c135b3", "name": "Yerui Sun", "hidden": false}, {"_id": "6976d5405d41524304c135b4", "name": "Yi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b5", "name": "Yihao Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b6", "name": "Yi-Kai Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b7", "name": "Yifan Lu", "hidden": false}, {"_id": "6976d5405d41524304c135b8", "name": "Yifan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135b9", "name": "Yitao Zhai", "hidden": false}, {"_id": "6976d5405d41524304c135ba", "name": "Yongjing Yin", "hidden": false}, {"_id": "6976d5405d41524304c135bb", "name": "Yongwei Zhou", "hidden": false}, {"_id": "6976d5405d41524304c135bc", "name": "Youshao Xiao", "hidden": false}, {"_id": "6976d5405d41524304c135bd", "name": "Yuchuan Dai", "hidden": false}, {"_id": "6976d5405d41524304c135be", "name": "Yuchen Xie", "hidden": false}, {"_id": "6976d5405d41524304c135bf", "name": "Yuchen Yu", "hidden": false}, {"_id": "6976d5405d41524304c135c0", "name": "Yufei Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135c1", "name": "Yuhuai Wei", "hidden": false}, {"_id": "6976d5405d41524304c135c2", "name": "Yulei Qian", "hidden": false}, {"_id": "6976d5405d41524304c135c3", "name": "Yunfan Liang", "hidden": false}, {"_id": "6976d5405d41524304c135c4", "name": "Yunke Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135c5", "name": "Yuwei Jiang", "hidden": false}, {"_id": "6976d5405d41524304c135c6", "name": "Yuxin Bian", "hidden": false}, {"_id": "6976d5405d41524304c135c7", "name": "Yuxin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135c8", "name": "Yuxin Liu", "hidden": false}, {"_id": "6976d5405d41524304c135c9", "name": "Yue Xu", "hidden": false}, {"_id": "6976d5405d41524304c135ca", "name": "Yueqing Sun", "hidden": false}, {"_id": "6976d5405d41524304c135cb", "name": "Zeyang Yu", "hidden": false}, {"_id": "6976d5405d41524304c135cc", "name": "Zhao Yang", "hidden": false}, {"_id": "6976d5405d41524304c135cd", "name": "Zhengsheng Huang", "hidden": false}, {"_id": "6976d5405d41524304c135ce", "name": "Zhengyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135cf", "name": "Zhijian Liu", "hidden": false}, {"_id": "6976d5405d41524304c135d0", "name": "Zhikang Xia", "hidden": false}, {"_id": "6976d5405d41524304c135d1", "name": "Zhimin Lin", "hidden": false}, {"_id": "6976d5405d41524304c135d2", "name": "Zhiyuan Yao", "hidden": false}, {"_id": "6976d5405d41524304c135d3", "name": "Zhuofan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135d4", "name": "Zhuowen Han", "hidden": false}, {"_id": "6976d5405d41524304c135d5", "name": "Zijian Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135d6", "name": "Ziran Li", "hidden": false}, {"_id": "6976d5405d41524304c135d7", "name": "Ziwen Wang", "hidden": false}, {"_id": "6976d5405d41524304c135d8", "name": "Ziyuan Zhuang", "hidden": false}], "publishedAt": "2026-01-23T13:20:09.000Z", "submittedOnDailyAt": "2026-01-26T00:15:28.340Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "upvotes": 136, "discussionId": "6976d5405d41524304c135d9", "ai_summary": "A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for real-world robustness and complex reasoning.", "ai_keywords": ["Mixture-of-Experts", "agentic reasoning", "domain-parallel expert training", "fusion", "asynchronous reinforcement learning", "DORA", "long-tailed generation", "multi-turn interactions", "real-world noise patterns", "test-time scaling", "reasoning depth", "reasoning width", "parallel thinking"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86LongCat-Flash-Thinking-2601\uff0c\u8fd9\u662f\u4e00\u4e2a\u5177\u67095600\u4ebf\u53c2\u6570\u7684\u5f00\u6e90\u6df7\u5408\u4e13\u5bb6\u63a8\u7406\u6a21\u578b\uff0c\u5177\u5907\u5353\u8d8a\u7684\u4ee3\u7406\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u591a\u9879\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u4ee3\u7406\u641c\u7d22\u3001\u5de5\u5177\u4f7f\u7528\u548c\u5de5\u5177\u96c6\u6210\u63a8\u7406\u3002</li>\n    <li>\u6a21\u578b\u80fd\u591f\u5728\u590d\u6742\u5de5\u5177\u4ea4\u4e92\u548c\u5608\u6742\u7684\u73b0\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u901a\u8fc7\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u8be5\u6a21\u578b\u7ed3\u5408\u4e86\u9886\u57df\u5e76\u884c\u4e13\u5bb6\u8bad\u7ec3\u548c\u540e\u7eed\u878d\u5408\uff0c\u4f18\u5316\u4e86\u6570\u636e\u6784\u5efa\u3001\u73af\u5883\u3001\u7b97\u6cd5\u548c\u57fa\u7840\u8bbe\u65bd\u3002</li>\n    <li>\u5f15\u5165\u4e86\u201c\u91cd\u601d\u8003\u201d\u6a21\u5f0f\uff0c\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u5e76\u884c\u601d\u7ef4\u6709\u6548\u6269\u5c55\u63a8\u7406\u6df1\u5ea6\u548c\u5bbd\u5ea6\uff0c\u63d0\u9ad8\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongCat-Flash-Thinking-2601 is a large open-source reasoning model with 560 billion parameters, designed for advanced reasoning tasks.</li>\n    <li>It performs exceptionally well on various benchmarks related to agentic tasks, such as using tools and complex reasoning.</li>\n    <li>The model's effectiveness comes from a unique training approach that focuses on expert training and careful design of data and environments.</li>\n    <li>It can handle complex interactions with tools and is robust in noisy real-world situations, thanks to its targeted training methods.</li>\n    <li>A special feature called Heavy Thinking allows the model to improve its reasoning capabilities during tests by expanding its thinking processes.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:20:09.000Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16725.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u591a\u6a21\u6001\u667a\u80fd\u7684\u6548\u7387\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u4e86\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u4e86\u8bed\u8a00\u5bf9\u9f50\u7684\u611f\u77e5\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\uff0c\u4ee5\u589e\u5f3a\u89c6\u89c9\u548c\u8bed\u8a00\u7684\u534f\u540c\u4f5c\u7528\u3002</li>\n    <li>\u901a\u8fc7\u8d85\u8fc71000\u6b21\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\uff0c\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u80fd\u591f\u9ad8\u6548\u5730\u8fdb\u884c\u611f\u77e5\u63a8\u7406\u3002</li>\n    <li>\u5c3d\u7ba1\u6a21\u578b\u53c2\u6570\u53ea\u670910\u4ebf\uff0c\u4f46\u5176\u6027\u80fd\u4e0e\u751a\u81f3\u8d85\u8fc7\u4e8610\u523020\u500d\u66f4\u5927\u7684\u6a21\u578b\u3002</li>\n    <li>STEP3-VL-10B \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u6a21\u578b\u5957\u4ef6\u4f9b\u793e\u533a\u4f7f\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a lightweight open-source model that balances efficiency and advanced multimodal intelligence.</li>\n    <li>It uses a unique training approach with 1.2 trillion multimodal tokens and combines a perception encoder with a decoder for better vision-language understanding.</li>\n    <li>The model includes a post-training process with over 1,000 iterations of reinforcement learning to enhance its capabilities.</li>\n    <li>It features a method called Parallel Coordinated Reasoning (PaCoRe) to improve reasoning and resource allocation during tests.</li>\n    <li>Despite its smaller size, STEP3-VL-10B performs as well as or better than much larger models and achieves high scores on various benchmarks.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u6839\u636e\u89c6\u89c9\u7ebf\u7d22\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5ffd\u89c6\u4e86\u4eba\u7c7b\u5e38\u7528\u7684\u5730\u56fe\u7b56\u7565\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u201c\u601d\u7ef4\u5730\u56fe\u201d\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u4e00\u4e2a\u201c\u5730\u56fe\u4e2d\u7684\u667a\u80fd\u4f53\u201d\u5faa\u73af\u8fdb\u884c\u5efa\u6a21\u3002</li>\n    <li>\u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u6269\u5c55\uff0c\u63d0\u5347\u6a21\u578b\u7684\u91c7\u6837\u6548\u7387\u548c\u9884\u6d4b\u51c6\u786e\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u51c6\u786e\u7387\u663e\u8457\u63d0\u9ad8\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>The task of image geolocalization is to find out where a photo was taken on Earth using visual clues.</li>\n    <li>This study introduces a new approach that includes using maps, a strategy humans often use, to improve the model's performance.</li>\n    <li>The method involves two steps: first, using reinforcement learning to enhance the model's decision-making, and then testing multiple paths to find the best location before making a final prediction.</li>\n    <li>A new benchmark called MAPBench has been created for testing the model, using only real-world images to ensure accuracy.</li>\n    <li>Results show that this new method significantly outperforms existing models, especially in accuracy for finding locations within 500 meters.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.17058", "authors": [{"_id": "69782c96026bdf0473116e06", "user": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "name": "Wei Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:49.701Z", "hidden": false}, {"_id": "69782c96026bdf0473116e07", "name": "Jun Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e08", "name": "Haoyu Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e09", "name": "Zhenghao Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0a", "name": "Qikang He", "hidden": false}, {"_id": "69782c96026bdf0473116e0b", "name": "Shaokun Han", "hidden": false}, {"_id": "69782c96026bdf0473116e0c", "name": "Guoliang Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0d", "user": {"_id": "64ef522242da8d2a897d62da", "avatarUrl": "/avatars/03611010d247da66696ac8976d4d3ed3.svg", "isPro": false, "fullname": "xuanhe zhou", "user": "zhouxh19", "type": "user"}, "name": "Xuanhe Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:19.930Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0e", "user": {"_id": "674fa2f067c963c50a066594", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674fa2f067c963c50a066594/hKZ46Mwm_UEguzBt63ys_.jpeg", "isPro": false, "fullname": "yeye he", "user": "yeyehe", "type": "user"}, "name": "Yeye He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:27.638Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0f", "name": "Chunwei Liu", "hidden": false}, {"_id": "69782c96026bdf0473116e10", "user": {"_id": "66724ce47e7ff5d8bd069c7c", "avatarUrl": "/avatars/953f66585390dbdb202c1d7b7250d7bd.svg", "isPro": false, "fullname": "Zirui Tang", "user": "TerryTang", "type": "user"}, "name": "Zirui Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:49.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e11", "name": "Bin Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e12", "user": {"_id": "695612aabf3c8959a3a05f9c", "avatarUrl": "/avatars/c18885f6dea6f3ee019405cd8cf6f484.svg", "isPro": false, "fullname": "ShenTang990", "user": "shentang", "type": "user"}, "name": "Shen Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:56.579Z", "hidden": false}, {"_id": "69782c96026bdf0473116e13", "name": "Kai Zuo", "hidden": false}, {"_id": "69782c96026bdf0473116e14", "user": {"_id": "67efa8a2ed790a2e999dc216", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0S4lQCJX61uCF8EkSLMkk.png", "isPro": false, "fullname": "Yuyu Luo", "user": "luoyuyu", "type": "user"}, "name": "Yuyu Luo", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:02.233Z", "hidden": false}, {"_id": "69782c96026bdf0473116e15", "name": "Zhenzhe Zheng", "hidden": false}, {"_id": "69782c96026bdf0473116e16", "user": {"_id": "63f9fca8d4349b157a109eec", "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg", "isPro": false, "fullname": "Conghui He", "user": "conghui", "type": "user"}, "name": "Conghui He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:57:14.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e17", "name": "Jingren Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e18", "name": "Fan Wu", "hidden": false}], "publishedAt": "2026-01-22T12:02:45.000Z", "submittedOnDailyAt": "2026-01-27T00:42:38.464Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "submittedOnDailyBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "upvotes": 127, "discussionId": "69782c97026bdf0473116e19", "projectPage": "https://github.com/weAIDB/awesome-data-llm", "githubRepo": "https://github.com/weAIDB/awesome-data-llm", "githubRepoAddedBy": "user", "ai_summary": "LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.", "ai_keywords": ["data preparation", "large language models", "prompt-driven workflows", "agentic workflows", "data cleaning", "data integration", "data enrichment", "entity matching", "schema matching", "data annotation", "data profiling"], "githubStars": 644, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u6570\u636e\u51c6\u5907\u7684\u76ee\u7684\u662f\u53bb\u9664\u539f\u59cb\u6570\u636e\u96c6\u4e2d\u7684\u566a\u58f0\uff0c\u53d1\u73b0\u8de8\u6570\u636e\u96c6\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002</li>\n    <li>\u968f\u7740\u5bf9\u5e94\u7528\u51c6\u5907\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\u548c\u5f3a\u5927LLM\u6280\u672f\u7684\u53d1\u5c55\uff0cLLM\u589e\u5f3a\u7684\u65b9\u6cd5\u6b63\u8fc5\u901f\u6210\u4e3a\u6570\u636e\u51c6\u5907\u7684\u4e3b\u6d41\u3002</li>\n    <li>\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u6700\u8fd1\u7684\u6587\u732e\uff0c\u5c06\u6570\u636e\u51c6\u5907\u4efb\u52a1\u5206\u4e3a\u6570\u636e\u6e05\u7406\u3001\u6570\u636e\u96c6\u6210\u548c\u6570\u636e\u4e30\u5bcc\u4e09\u4e2a\u4e3b\u8981\u4efb\u52a1\u3002</li>\n    <li>\u5bf9\u4e8e\u6bcf\u4e2a\u4efb\u52a1\uff0c\u6587\u7ae0\u8c03\u67e5\u4e86\u4ee3\u8868\u6027\u7684\u6280\u672f\uff0c\u5e76\u6307\u51fa\u4e86\u5b83\u4eec\u7684\u4f18\u7f3a\u70b9\u3002</li>\n    <li>\u6700\u540e\uff0c\u8ba8\u8bba\u4e86\u5f00\u653e\u7684\u7814\u7a76\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\uff0c\u5305\u62ec\u53ef\u6269\u5c55\u7684LLM\u6570\u636e\u7cfb\u7edf\u548c\u53ef\u9760\u7684\u8bc4\u4f30\u534f\u8bae\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Data preparation helps clean raw datasets and find useful connections between them for various applications.</li>\n    <li>There is a growing need for data ready for analysis, supported by advanced LLM techniques and new flexible infrastructure.</li>\n    <li>This paper reviews recent research on using LLMs for data preparation, focusing on three main tasks: cleaning, integration, and enrichment.</li>\n    <li>Each task has its own techniques with advantages (like better understanding of data) and challenges (like high costs and inaccuracies).</li>\n    <li>The paper also identifies datasets and evaluation methods, discusses ongoing research challenges, and suggests future directions for LLM-based data systems.</li>\n</ul>"}, "publishedAt": "2026-01-22T07:02:45.000Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17058.png", "numComments": 2, "submittedBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "fullname": "Wei Zhou", "name": "weizhoudb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.12538", "authors": [{"_id": "6971913fc1c7409747bf9564", "name": "Tianxin Wei", "hidden": false}, {"_id": "6971913fc1c7409747bf9565", "user": {"_id": "6742eb40924e80c3c80ebe13", "avatarUrl": "/avatars/e6ccb1a89a1ea0bfca70779966f4f429.svg", "isPro": false, "fullname": "Ting-Wei Li", "user": "tingwl0122", "type": "user"}, "name": "Ting-Wei Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T17:12:21.531Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9566", "name": "Zhining Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf9567", "name": "Xuying Ning", "hidden": false}, {"_id": "6971913fc1c7409747bf9568", "name": "Ze Yang", "hidden": false}, {"_id": "6971913fc1c7409747bf9569", "name": "Jiaru Zou", "hidden": false}, {"_id": "6971913fc1c7409747bf956a", "name": "Zhichen Zeng", "hidden": false}, {"_id": "6971913fc1c7409747bf956b", "name": "Ruizhong Qiu", "hidden": false}, {"_id": "6971913fc1c7409747bf956c", "name": "Xiao Lin", "hidden": false}, {"_id": "6971913fc1c7409747bf956d", "name": "Dongqi Fu", "hidden": false}, {"_id": "6971913fc1c7409747bf956e", "name": "Zihao Li", "hidden": false}, {"_id": "6971913fc1c7409747bf956f", "user": {"_id": "653962e75c8e4863e1a2068f", "avatarUrl": "/avatars/d4f5f5da141f37d53ca1986ff17b325e.svg", "isPro": false, "fullname": "Mengting Ai", "user": "famous-blue-raincoat", "type": "user"}, "name": "Mengting Ai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:10.378Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9570", "user": {"_id": "677830bd3f2e3ec475576303", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dhwqUDkk66m4oSGSbcd7j.png", "isPro": false, "fullname": "Duo Zhou", "user": "Claudius7", "type": "user"}, "name": "Duo Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:12.476Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9571", "name": "Wenxuan Bao", "hidden": false}, {"_id": "6971913fc1c7409747bf9572", "user": {"_id": "646323556c27a7e33b23f198", "avatarUrl": "/avatars/17fe142f689ab4be3c2374d1d90393db.svg", "isPro": false, "fullname": "Yunzhe Li", "user": "yunzhel2", "type": "user"}, "name": "Yunzhe Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:14.383Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9573", "name": "Gaotang Li", "hidden": false}, {"_id": "6971913fc1c7409747bf9574", "name": "Cheng Qian", "hidden": false}, {"_id": "6971913fc1c7409747bf9575", "name": "Yu Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf9576", "name": "Xiangru Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf9577", "name": "Yin Xiao", "hidden": false}, {"_id": "6971913fc1c7409747bf9578", "name": "Liri Fang", "hidden": false}, {"_id": "6971913fc1c7409747bf9579", "name": "Hui Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf957a", "name": "Xianfeng Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf957b", "name": "Yuji Zhang", "hidden": false}, {"_id": "6971913fc1c7409747bf957c", "name": "Chi Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf957d", "name": "Jiaxuan You", "hidden": false}, {"_id": "6971913fc1c7409747bf957e", "name": "Heng Ji", "hidden": false}, {"_id": "6971913fc1c7409747bf957f", "name": "Hanghang Tong", "hidden": false}, {"_id": "6971913fc1c7409747bf9580", "name": "Jingrui He", "hidden": false}], "publishedAt": "2026-01-18T18:58:23.000Z", "submittedOnDailyAt": "2026-01-22T00:27:25.162Z", "title": "Agentic Reasoning for Large Language Models", "submittedOnDailyBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "isPro": false, "fullname": "Jiaru Zou", "user": "jiaruz2", "type": "user"}, "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "upvotes": 125, "discussionId": "69719140c1c7409747bf9581", "githubRepo": "https://github.com/weitianxin/Awesome-Agentic-Reasoning", "githubRepoAddedBy": "user", "ai_summary": "Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.", "ai_keywords": ["large language models", "agentic reasoning", "autonomous agents", "planning", "tool use", "search", "feedback", "memory", "adaptation", "collaborative settings", "coordination", "knowledge sharing", "reinforcement learning", "supervised fine-tuning", "in-context reasoning", "post-training reasoning", "real-world applications", "benchmarks", "thought and action", "world modeling", "scalable multi-agent training", "governance"], "githubStars": 105, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "summary_zh": "<ul>\n    <li>\u63a8\u7406\u662f\u63a8\u65ad\u3001\u89e3\u51b3\u95ee\u9898\u548c\u51b3\u7b56\u7684\u57fa\u672c\u8ba4\u77e5\u8fc7\u7a0b\u3002</li>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5c01\u95ed\u73af\u5883\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u5f88\u5f3a\uff0c\u4f46\u5728\u5f00\u653e\u548c\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u8f83\u5dee\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u901a\u8fc7\u6301\u7eed\u4e92\u52a8\u8fdb\u884c\u89c4\u5212\u3001\u884c\u52a8\u548c\u5b66\u4e60\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u6709\u4e09\u4e2a\u5c42\u6b21\uff1a\u57fa\u7840\u4ee3\u7406\u63a8\u7406\u3001\u81ea\u6211\u6f14\u5316\u4ee3\u7406\u63a8\u7406\u548c\u96c6\u4f53\u591a\u4ee3\u7406\u63a8\u7406\u3002</li>\n    <li>\u8be5\u8c03\u67e5\u603b\u7ed3\u4e86\u4ee3\u7406\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7684\u6311\u6218\u548c\u65b9\u5411\uff0c\u5982\u4e2a\u6027\u5316\u3001\u957f\u671f\u4e92\u52a8\u548c\u591a\u4ee3\u7406\u8bad\u7ec3\u7b49\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Reasoning helps with making decisions and solving problems, but large language models (LLMs) have difficulty in changing and open environments.</li>\n  <li>Agentic reasoning turns LLMs into independent agents that can plan, act, and learn through ongoing interactions.</li>\n  <li>This type of reasoning is organized into three levels: basic skills for single agents, improving skills through feedback and memory, and working together with other agents.</li>\n  <li>It distinguishes between in-context reasoning (interacting during tests) and post-training reasoning (improving skills after training).</li>\n  <li>The survey highlights practical uses of agentic reasoning, future challenges, and areas for improvement like personalization and teamwork training.</li>\n</ul>"}, "publishedAt": "2026-01-18T13:58:23.000Z", "title": "Agentic Reasoning for Large Language Models", "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12538.png", "numComments": 3, "submittedBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "fullname": "Jiaru Zou", "name": "jiaruz2", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20833", "authors": [{"_id": "697b9192a67238fac88cbee8", "name": "Tengyue Xu", "hidden": false}, {"_id": "697b9192a67238fac88cbee9", "name": "Zhuoyang Qian", "hidden": false}, {"_id": "697b9192a67238fac88cbeea", "name": "Gaoge Liu", "hidden": false}, {"_id": "697b9192a67238fac88cbeeb", "name": "Li Ling", "hidden": false}, {"_id": "697b9192a67238fac88cbeec", "name": "Zhentao Zhang", "hidden": false}, {"_id": "697b9192a67238fac88cbeed", "name": "Biao Wu", "hidden": false}, {"_id": "697b9192a67238fac88cbeee", "name": "Shuo Zhang", "hidden": false}, {"_id": "697b9192a67238fac88cbeef", "name": "Ke Lu", "hidden": false}, {"_id": "697b9192a67238fac88cbef0", "name": "Wei Shi", "hidden": false}, {"_id": "697b9192a67238fac88cbef1", "name": "Ziqi Wang", "hidden": false}, {"_id": "697b9192a67238fac88cbef2", "name": "Zheng Feng", "hidden": false}, {"_id": "697b9192a67238fac88cbef3", "name": "Yan Luo", "hidden": false}, {"_id": "697b9192a67238fac88cbef4", "name": "Shu Xu", "hidden": false}, {"_id": "697b9192a67238fac88cbef5", "name": "Yongjin Chen", "hidden": false}, {"_id": "697b9192a67238fac88cbef6", "name": "Zhibo Feng", "hidden": false}, {"_id": "697b9192a67238fac88cbef7", "name": "Zhuo Chen", "hidden": false}, {"_id": "697b9192a67238fac88cbef8", "name": "Bruce Yuan", "hidden": false}, {"_id": "697b9192a67238fac88cbef9", "name": "Harry Wang", "hidden": false}, {"_id": "697b9192a67238fac88cbefa", "name": "Kris Chen", "hidden": false}], "publishedAt": "2026-01-28T18:31:54.000Z", "submittedOnDailyAt": "2026-01-30T03:32:00.106Z", "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives", "submittedOnDailyBy": {"_id": "62baa0d6dd02fbf607ce97be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62baa0d6dd02fbf607ce97be/V0I6pANlLEf2YDd9ZLZgi.jpeg", "isPro": false, "fullname": "Wendy", "user": "Wendy-Fly", "type": "user"}, "summary": "Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.", "upvotes": 113, "discussionId": "697b9192a67238fac88cbefb", "githubRepo": "https://github.com/AgentAlphaAGI/Idea2Paper", "githubRepoAddedBy": "user", "ai_summary": "Offline knowledge construction through structured methodological graphs enables more reliable and scalable autonomous scientific discovery by reducing reliance on real-time literature processing.", "ai_keywords": ["large language model", "autonomous scientific discovery", "runtime-centric execution", "context window limitations", "hallucination", "pre-computation-driven framework", "peer-reviewed papers", "research patterns", "methodological knowledge graph", "end-to-end research workflows"], "githubStars": 54, "organization": {"_id": "69542731e1200d74c1c053d1", "name": "AgentAlphaAGI", "fullname": "AgentAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b78eb76ab5d14ca7faac87/TbMZ3y00APtRzHEfTSR7I.jpeg"}, "summary_zh": "<ul>\n    <li>\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u7684\u7814\u7a76\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u80fd\u591f\u81ea\u52a8\u5316\u6574\u4e2a\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u5728\u7ebf\u63a8\u7406\uff0c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u4e0d\u53ef\u9760\u7684\u63a8\u7406\u7ed3\u679c\u3002</li>\n    <li>\u63d0\u51fa\u4e86Idea2Story\u6846\u67b6\uff0c\u8f6c\u5411\u79bb\u7ebf\u77e5\u8bc6\u6784\u5efa\uff0c\u63d0\u9ad8\u6587\u732e\u7406\u89e3\u80fd\u529b\u3002</li>\n    <li>Idea2Story\u901a\u8fc7\u6536\u96c6\u540c\u884c\u8bc4\u5ba1\u8bba\u6587\u53ca\u53cd\u9988\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u7684\u77e5\u8bc6\u56fe\u8c31\u3002</li>\n    <li>\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7814\u7a76\u6a21\u5f0f\uff0c\u63d0\u4f9b\u53ef\u9760\u7684\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u57fa\u7840\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent advancements in autonomous scientific discovery using large language models (LLMs) have improved research workflows.</li>\n    <li>Current systems rely on online processing, which is costly and can lead to errors due to limitations in understanding context.</li>\n    <li>The proposed Idea2Story framework focuses on building knowledge offline, organizing research into a structured graph.</li>\n    <li>This approach allows for better retrieval and reuse of established research methods, reducing the need for extensive online reasoning.</li>\n    <li>Preliminary studies show that Idea2Story can create coherent and innovative research patterns effectively.</li>\n</ul>"}, "publishedAt": "2026-01-28T13:31:54.000Z", "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives", "summary": "Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20833.png", "numComments": 1, "submittedBy": {"_id": "62baa0d6dd02fbf607ce97be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62baa0d6dd02fbf607ce97be/V0I6pANlLEf2YDd9ZLZgi.jpeg", "fullname": "Wendy", "name": "Wendy-Fly", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "69542731e1200d74c1c053d1", "name": "AgentAlphaAGI", "fullname": "AgentAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b78eb76ab5d14ca7faac87/TbMZ3y00APtRzHEfTSR7I.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.08763", "authors": [{"_id": "6969b0a232f0333869ff946a", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:38.232Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946b", "user": {"_id": "6891c906f3c31445cc040ab1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891c906f3c31445cc040ab1/NBqxXOY7al4CD0XBj8ke2.jpeg", "isPro": false, "fullname": "Yucheng Wang", "user": "DevilEnfant", "type": "user"}, "name": "Yucheng Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:48.080Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946c", "name": "Yufei He", "hidden": false}, {"_id": "6969b0a232f0333869ff946d", "user": {"_id": "682deb444988bd82847e2b03", "avatarUrl": "/avatars/15da087e84386ea72c6fa2db63571420.svg", "isPro": false, "fullname": "Jia-Ying Wu", "user": "EricaWu", "type": "user"}, "name": "Jiaying Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:59.692Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946e", "name": "Yilun Zhao", "hidden": false}, {"_id": "6969b0a232f0333869ff946f", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0a232f0333869ff9470", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:06.327Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9471", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:12.181Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9472", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:17.979Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9473", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:23.007Z", "hidden": false}], "publishedAt": "2026-01-13T17:48:43.000Z", "submittedOnDailyAt": "2026-01-16T01:00:36.686Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "upvotes": 111, "discussionId": "6969b0a232f0333869ff9474", "ai_summary": "Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.", "ai_keywords": ["reinforcement learning", "large language models", "exploration collapse", "pass@k", "pass@1", "rollout-level objective", "high-level solution strategies", "clustering", "policy advantages", "AUC@K"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u540e\u8bad\u7ec3\u4e2d\u53d8\u5f97\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5e38\u5e38\u4f1a\u51fa\u73b0\u63a2\u7d22\u5d29\u6e83\u7684\u95ee\u9898\u3002</li>\n    <li>\u8fd9\u79cd\u5d29\u6e83\u4f7f\u5f97\u7b56\u7565\u8fc7\u65e9\u96c6\u4e2d\u4e8e\u5c11\u6570\u4e3b\u5bfc\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u591a\u6837\u6027\u548c\u6574\u4f53\u8868\u73b0\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff1a\u72ec\u7279\u6027\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\uff0c\u5956\u52b1\u90a3\u4e9b\u5c55\u793a\u7f55\u89c1\u9ad8\u5c42\u7b56\u7565\u7684\u6b63\u786e\u89e3\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u76f8\u540c\u95ee\u9898\u7684\u89e3\u51b3\u7b56\u7565\u8fdb\u884c\u805a\u7c7b\uff0c\u91cd\u65b0\u52a0\u6743\u7b56\u7565\u4f18\u52bf\uff0c\u4ece\u800c\u63d0\u5347\u65b0\u9896\u7b56\u7565\u7684\u5956\u52b1\u3002</li>\n    <li>\u5728\u5404\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u901a\u8fc7\u7387\uff0c\u589e\u52a0\u4e86\u591a\u6837\u5316\u7684\u89e3\u51b3\u7b56\u7565\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u7684\u63a2\u7d22\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Reinforcement learning (RL) is important for improving large language models (LLMs) in complex reasoning tasks.</li>\n  <li>Current RL methods often focus too much on a few common reasoning patterns, which reduces diversity in solutions.</li>\n  <li>The proposed method, Uniqueness-Aware Reinforcement Learning, rewards unique and rare solution strategies.</li>\n  <li>This approach uses an LLM judge to group similar solutions and adjusts rewards based on the uniqueness of strategies.</li>\n  <li>The method shows better performance across various benchmarks without losing effectiveness on standard measures.</li>\n</ul>"}, "publishedAt": "2026-01-13T12:48:43.000Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08763.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Jan 31, 2026";
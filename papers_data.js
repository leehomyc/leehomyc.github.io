window.trendingPapers = {
    "today": [{"paper": {"id": "2602.21193", "authors": [{"_id": "699e72b4dfbcf0b800aecb63", "name": "Renjie Pi", "hidden": false}, {"_id": "699e72b4dfbcf0b800aecb64", "name": "Grace Lam", "hidden": false}, {"_id": "699e72b4dfbcf0b800aecb65", "name": "Mohammad Shoeybi", "hidden": false}, {"_id": "699e72b4dfbcf0b800aecb66", "name": "Pooya Jannaty", "hidden": false}, {"_id": "699e72b4dfbcf0b800aecb67", "name": "Bryan Catanzaro", "hidden": false}, {"_id": "699e72b4dfbcf0b800aecb68", "name": "Wei Ping", "hidden": false}], "publishedAt": "2026-02-24T18:51:04.000Z", "submittedOnDailyAt": "2026-02-25T01:39:40.130Z", "title": "On Data Engineering for Scaling LLM Terminal Capabilities", "submittedOnDailyBy": {"_id": "63f45b8d520c14618930d175", "avatarUrl": "/avatars/42b3aaf50748a25e4a596fc57ab1306d.svg", "isPro": false, "fullname": "renjie", "user": "renjiepi", "type": "user"}, "summary": "Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.", "upvotes": 60, "discussionId": "699e72b5dfbcf0b800aecb69", "projectPage": "https://huggingface.co/collections/nvidia/nemotron-terminal", "ai_summary": "Researchers developed a synthetic task generation pipeline and analyzed data strategies to improve terminal agent performance, creating a large-scale dataset and models that outperform larger counterparts on benchmark tests.", "ai_keywords": ["large language models", "terminal agents", "data engineering practices", "synthetic task generation", "Terminal-Task-Gen", "Terminal-Corpus", "Nemotron-Terminal", "Terminal-Bench 2.0", "curriculum learning", "long context training", "scaling behavior"], "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-24T13:51:04.000Z", "title": "On Data Engineering for Scaling LLM Terminal Capabilities", "summary": "Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21193.png", "numComments": 2, "submittedBy": {"_id": "63f45b8d520c14618930d175", "avatarUrl": "/avatars/42b3aaf50748a25e4a596fc57ab1306d.svg", "fullname": "renjie", "name": "renjiepi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.12192", "authors": [{"_id": "698ebfdacace060ff123aece", "user": {"_id": "6891bbff78946201296b4592", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891bbff78946201296b4592/ECmWBrlfeonPg0HzmQ_sW.png", "isPro": false, "fullname": "Yuqing Li", "user": "MindscapeRAG", "type": "user"}, "name": "Yuqing Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-13T09:36:07.802Z", "hidden": false}, {"_id": "698ebfdacace060ff123aecf", "name": "Jiangnan Li", "hidden": false}, {"_id": "698ebfdacace060ff123aed0", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-25T17:32:43.530Z", "hidden": false}, {"_id": "698ebfdacace060ff123aed1", "name": "Guoxuan Ding", "hidden": false}, {"_id": "698ebfdacace060ff123aed2", "name": "Zheng Lin", "hidden": false}, {"_id": "698ebfdacace060ff123aed3", "name": "Weiping Wang", "hidden": false}, {"_id": "698ebfdacace060ff123aed4", "name": "Jie Zhou", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6891bbff78946201296b4592/LPd7mcXZoPkkNY6Q8EE0E.gif"], "publishedAt": "2026-02-12T17:23:38.000Z", "submittedOnDailyAt": "2026-02-25T07:37:44.371Z", "title": "Query-focused and Memory-aware Reranker for Long Context Processing", "submittedOnDailyBy": {"_id": "6891bbff78946201296b4592", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891bbff78946201296b4592/ECmWBrlfeonPg0HzmQ_sW.png", "isPro": false, "fullname": "Yuqing Li", "user": "MindscapeRAG", "type": "user"}, "summary": "Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a listwise solution that leverages holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores, enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision. Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-of-the-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes a new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage. We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance.", "upvotes": 35, "discussionId": "698ebfdbcace060ff123aed5", "projectPage": "https://qdcassie-li.github.io/QRRanker/", "ai_summary": "A lightweight reranking framework uses attention scores from selected heads to estimate passage-query relevance, achieving strong performance across multiple domains and benchmarks.", "ai_keywords": ["retrieval heads", "reranking framework", "attention scores", "passage-query relevance", "listwise solution", "candidate shortlist", "continuous relevance scores", "Likert-scale supervision", "pointwise rerankers", "listwise rerankers", "LoCoMo benchmark", "dialogue understanding", "memory usage", "contextual information", "middle layers"], "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-12T12:23:38.000Z", "title": "Query-focused and Memory-aware Reranker for Long Context Processing", "summary": "Built upon the existing analysis of retrieval heads in large language models, we propose an alternative reranking framework that trains models to estimate passage-query relevance using the attention scores of selected heads. This approach provides a listwise solution that leverages holistic information within the entire candidate shortlist during ranking. At the same time, it naturally produces continuous relevance scores, enabling training on arbitrary retrieval datasets without requiring Likert-scale supervision. Our framework is lightweight and effective, requiring only small-scale models (e.g., 4B parameters) to achieve strong performance. Extensive experiments demonstrate that our method outperforms existing state-of-the-art pointwise and listwise rerankers across multiple domains, including Wikipedia and long narrative datasets. It further establishes a new state-of-the-art on the LoCoMo benchmark that assesses the capabilities of dialogue understanding and memory usage. We further demonstrate that our framework supports flexible extensions. For example, augmenting candidate passages with contextual information further improves ranking accuracy, while training attention heads from middle layers enhances efficiency without sacrificing performance.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6891bbff78946201296b4592/LPd7mcXZoPkkNY6Q8EE0E.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12192.png", "numComments": 2, "submittedBy": {"_id": "6891bbff78946201296b4592", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891bbff78946201296b4592/ECmWBrlfeonPg0HzmQ_sW.png", "fullname": "Yuqing Li", "name": "MindscapeRAG", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.20739", "authors": [{"_id": "699e674ddfbcf0b800aecae9", "user": {"_id": "62c66504031996c36c86976a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png", "isPro": false, "fullname": "steve z", "user": "stzhao", "type": "user"}, "name": "Shitian Zhao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-25T17:29:58.018Z", "hidden": false}, {"_id": "699e674ddfbcf0b800aecaea", "name": "Shaoheng Lin", "hidden": false}, {"_id": "699e674ddfbcf0b800aecaeb", "name": "Ming Li", "hidden": false}, {"_id": "699e674ddfbcf0b800aecaec", "user": {"_id": "67ff7f687351095d4b606b84", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67ff7f687351095d4b606b84/KhNPmbBC3zghuP5h1MK-c.png", "isPro": false, "fullname": "Haoquan Zhang", "user": "haoquan03", "type": "user"}, "name": "Haoquan Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-25T17:29:51.660Z", "hidden": false}, {"_id": "699e674ddfbcf0b800aecaed", "name": "Wenshuo Peng", "hidden": false}, {"_id": "699e674ddfbcf0b800aecaee", "name": "Kaipeng Zhang", "hidden": false}, {"_id": "699e674ddfbcf0b800aecaef", "name": "Chen Wei", "hidden": false}], "publishedAt": "2026-02-24T10:08:33.000Z", "submittedOnDailyAt": "2026-02-25T00:41:13.309Z", "title": "PyVision-RL: Forging Open Agentic Vision Models via RL", "submittedOnDailyBy": {"_id": "62c66504031996c36c86976a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png", "isPro": false, "fullname": "steve z", "user": "stzhao", "type": "user"}, "summary": "Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.", "upvotes": 22, "discussionId": "699e674edfbcf0b800aecaf0", "projectPage": "https://agent-x.space/pyvision-rl/", "githubRepo": "https://github.com/agents-x-project/PyVision-RL", "githubRepoAddedBy": "user", "ai_summary": "PyVision-RL framework addresses interaction collapse in multimodal models through enhanced reinforcement learning techniques and efficient video processing strategies.", "ai_keywords": ["reinforcement learning", "agentic multimodal models", "interaction collapse", "oversampling-filtering-ranking", "accumulative tool reward", "unified training pipeline", "on-demand context construction", "task-relevant frames", "visual token usage"], "githubStars": 43, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-24T05:08:33.000Z", "title": "PyVision-RL: Forging Open Agentic Vision Models via RL", "summary": "Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20739.png", "numComments": 1, "submittedBy": {"_id": "62c66504031996c36c86976a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png", "fullname": "steve z", "name": "stzhao", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.21015", "authors": [{"_id": "699e69fbdfbcf0b800aecafb", "user": {"_id": "63369da91ba5d5ece24118a4", "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg", "isPro": false, "fullname": "wuyuhao", "user": "mozhu", "type": "user"}, "name": "Yuhao Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-25T17:29:26.632Z", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecafc", "name": "Maojia Song", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecafd", "name": "Yihuai Lan", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecafe", "name": "Lei Wang", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecaff", "user": {"_id": "637f228152229c63921119c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg", "isPro": false, "fullname": "Zhiqiang Hu", "user": "Zhiqiang007", "type": "user"}, "name": "Zhiqiang Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-25T17:29:48.417Z", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecb00", "name": "Yao Xiao", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecb01", "user": {"_id": "660d17d6c9be0dcd31a30b3d", "avatarUrl": "/avatars/3743fe9b695c488ebe33f0d8fd607a8a.svg", "isPro": false, "fullname": "Zhou Heng", "user": "henggg", "type": "user"}, "name": "Heng Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-02-25T17:29:45.251Z", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecb02", "name": "Weihua Zheng", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecb03", "name": "Dylan Raharja", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecb04", "name": "Soujanya Poria", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecb05", "name": "Roy Ka-Wei Lee", "hidden": false}], "publishedAt": "2026-02-24T15:33:02.000Z", "submittedOnDailyAt": "2026-02-25T00:55:31.629Z", "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning", "submittedOnDailyBy": {"_id": "637f228152229c63921119c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg", "isPro": false, "fullname": "Zhiqiang Hu", "user": "Zhiqiang007", "type": "user"}, "summary": "Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.", "upvotes": 20, "discussionId": "699e69fbdfbcf0b800aecb06", "projectPage": "https://social-ai-studio.github.io/CHAIN/", "githubRepo": "https://github.com/Social-AI-Studio/CHAIN", "githubRepoAddedBy": "user", "ai_summary": "Current vision-language models lack capability to understand physical structures and causal constraints needed for complex, interactive 3D tasks, as demonstrated by the CHAIN benchmark evaluating structured action planning under physical constraints.", "ai_keywords": ["Vision-Language Model", "diffusion-based models", "physical constraints", "causal constraints", "interactive 3D", "structured action sequences", "long-horizon planning"], "githubStars": 3, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-24T10:33:02.000Z", "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning", "summary": "Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21015.png", "numComments": 2, "submittedBy": {"_id": "637f228152229c63921119c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg", "fullname": "Zhiqiang Hu", "name": "Zhiqiang007", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.21204", "authors": [{"_id": "699e6e60dfbcf0b800aecb16", "name": "Junchen Liu", "hidden": false}, {"_id": "699e6e60dfbcf0b800aecb17", "name": "Sven Elflein", "hidden": false}, {"_id": "699e6e60dfbcf0b800aecb18", "name": "Or Litany", "hidden": false}, {"_id": "699e6e60dfbcf0b800aecb19", "name": "Zan Gojcic", "hidden": false}, {"_id": "699e6e60dfbcf0b800aecb1a", "name": "Ruilong Li", "hidden": false}], "publishedAt": "2026-02-24T18:59:30.000Z", "submittedOnDailyAt": "2026-02-25T03:15:10.854Z", "title": "Test-Time Training with KV Binding Is Secretly Linear Attention", "submittedOnDailyBy": {"_id": "663a9ae58cf658ffaf3d02e5", "avatarUrl": "/avatars/aa35668dc088e675e794b9ceb935c56f.svg", "isPro": false, "fullname": "Junchen Liu", "user": "JunchenLiu", "type": "user"}, "summary": "Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.", "upvotes": 19, "discussionId": "699e6e61dfbcf0b800aecb1b", "projectPage": "https://research.nvidia.com/labs/sil/projects/tttla/", "ai_summary": "Test-time training is reinterpreted as learned linear attention rather than memorization, offering architectural simplifications and improved efficiency.", "ai_keywords": ["test-time training", "KV binding", "online meta-learning", "learned linear attention", "sequence modeling layer", "linear attention operator", "architectural simplifications", "parallel formulations", "representational capacity"], "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-24T13:59:30.000Z", "title": "Test-Time Training with KV Binding Is Secretly Linear Attention", "summary": "Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21204.png", "numComments": 1, "submittedBy": {"_id": "663a9ae58cf658ffaf3d02e5", "avatarUrl": "/avatars/aa35668dc088e675e794b9ceb935c56f.svg", "fullname": "Junchen Liu", "name": "JunchenLiu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.21202", "authors": [{"_id": "699f21fbebfce7fbcca919cd", "name": "Hanxiang Qin", "hidden": false}, {"_id": "699f21fbebfce7fbcca919ce", "name": "Alexander Martin", "hidden": false}, {"_id": "699f21fbebfce7fbcca919cf", "name": "Rohan Jha", "hidden": false}, {"_id": "699f21fbebfce7fbcca919d0", "name": "Chunsheng Zuo", "hidden": false}, {"_id": "699f21fbebfce7fbcca919d1", "name": "Reno Kriz", "hidden": false}, {"_id": "699f21fbebfce7fbcca919d2", "name": "Benjamin Van Durme", "hidden": false}], "publishedAt": "2026-02-24T18:57:33.000Z", "submittedOnDailyAt": "2026-02-25T13:53:39.027Z", "title": "Multi-Vector Index Compression in Any Modality", "submittedOnDailyBy": {"_id": "644ffb154f731658826945c8", "avatarUrl": "/avatars/46fa3a15c2b974692bee5b6ed6468be0.svg", "isPro": false, "fullname": "Alex Martin", "user": "alexmartin1722", "type": "user"}, "summary": "We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.", "upvotes": 18, "discussionId": "699f21fcebfce7fbcca919d3", "githubRepo": "https://github.com/hanxiangqin/omni-col-press", "githubRepoAddedBy": "user", "ai_summary": "Attention-guided clustering method for compressing multi-vector document representations in late interaction retrieval tasks shows superior performance compared to other compression techniques across text, visual-document, and video modalities.", "ai_keywords": ["late interaction", "multi-vector retrieval", "index compression", "attention-guided clustering", "sequence resizing", "memory tokens", "hierarchical pooling"], "githubStars": 0, "organization": {"_id": "643568fef81a16e74361e659", "name": "hltcoe", "fullname": "JHU Human Language Technology Center of Excellence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643568cdf3b08e267d9808f1/u-FGMfs5blYzYGX4ErT7d.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-24T13:57:33.000Z", "title": "Multi-Vector Index Compression in Any Modality", "summary": "We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21202.png", "numComments": 1, "submittedBy": {"_id": "644ffb154f731658826945c8", "avatarUrl": "/avatars/46fa3a15c2b974692bee5b6ed6468be0.svg", "fullname": "Alex Martin", "name": "alexmartin1722", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "643568fef81a16e74361e659", "name": "hltcoe", "fullname": "JHU Human Language Technology Center of Excellence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643568cdf3b08e267d9808f1/u-FGMfs5blYzYGX4ErT7d.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.20951", "authors": [{"_id": "699e74f6dfbcf0b800aecb74", "user": {"_id": "6682429bb1f69d3cd0104389", "avatarUrl": "/avatars/05745a9fba109c7cf87e03f1630ff8b0.svg", "isPro": false, "fullname": "Jaehyun Park", "user": "Cabbalett", "type": "user"}, "name": "Jaehyun Park", "status": "claimed_verified", "statusLastChangedAt": "2026-02-25T17:29:19.146Z", "hidden": false}, {"_id": "699e74f6dfbcf0b800aecb75", "name": "Minyoung Ahn", "hidden": false}, {"_id": "699e74f6dfbcf0b800aecb76", "name": "Minkyu Kim", "hidden": false}, {"_id": "699e74f6dfbcf0b800aecb77", "user": {"_id": "6632e41743f364c13e3418cd", "avatarUrl": "/avatars/e0394f1012d38ea354ea54d53d6c5e2f.svg", "isPro": false, "fullname": "Jonghyun Lee", "user": "Jhyun17", "type": "user"}, "name": "Jonghyun Lee", "status": "claimed_verified", "statusLastChangedAt": "2026-02-25T17:29:16.907Z", "hidden": false}, {"_id": "699e74f6dfbcf0b800aecb78", "name": "Jae-Gil Lee", "hidden": false}, {"_id": "699e74f6dfbcf0b800aecb79", "name": "Dongmin Park", "hidden": false}], "publishedAt": "2026-02-24T14:34:13.000Z", "submittedOnDailyAt": "2026-02-25T15:09:39.634Z", "title": "See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis", "submittedOnDailyBy": {"_id": "6682429bb1f69d3cd0104389", "avatarUrl": "/avatars/05745a9fba109c7cf87e03f1630ff8b0.svg", "isPro": false, "fullname": "Jaehyun Park", "user": "Cabbalett", "type": "user"}, "summary": "Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromise realism. Although more thorough pre-training and bigger models might reduce artifacts, there is no assurance that they can be completely eliminated, which makes artifact mitigation a highly crucial area of study. Previous artifact-aware methodologies depend on human-labeled artifact datasets, which are costly and difficult to scale, underscoring the need for an automated approach to reliably acquire artifact-annotated datasets. In this paper, we propose ArtiAgent, which efficiently creates pairs of real and artifact-injected images. It comprises three agents: a perception agent that recognizes and grounds entities and subentities from real images, a synthesis agent that introduces artifacts via artifact injection tools through novel patch-wise embedding manipulation within a diffusion transformer, and a curation agent that filters the synthesized artifacts and generates both local and global explanations for each instance. Using ArtiAgent, we synthesize 100K images with rich artifact annotations and demonstrate both efficacy and versatility across diverse applications. Code is available at link.", "upvotes": 11, "discussionId": "699e74f6dfbcf0b800aecb7a", "githubRepo": "https://github.com/krafton-ai/ArtiAgent", "githubRepoAddedBy": "user", "ai_summary": "ArtiAgent automates the creation of real-artifact image pairs using three agents for perception, synthesis, and curation in diffusion transformers.", "ai_keywords": ["diffusion transformer", "artifact injection tools", "patch-wise embedding manipulation", "artifact-aware methodologies", "artifact-annotated datasets", "perception agent", "synthesis agent", "curation agent"], "githubStars": 1, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-24T09:34:13.000Z", "title": "See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis", "summary": "Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromise realism. Although more thorough pre-training and bigger models might reduce artifacts, there is no assurance that they can be completely eliminated, which makes artifact mitigation a highly crucial area of study. Previous artifact-aware methodologies depend on human-labeled artifact datasets, which are costly and difficult to scale, underscoring the need for an automated approach to reliably acquire artifact-annotated datasets. In this paper, we propose ArtiAgent, which efficiently creates pairs of real and artifact-injected images. It comprises three agents: a perception agent that recognizes and grounds entities and subentities from real images, a synthesis agent that introduces artifacts via artifact injection tools through novel patch-wise embedding manipulation within a diffusion transformer, and a curation agent that filters the synthesized artifacts and generates both local and global explanations for each instance. Using ArtiAgent, we synthesize 100K images with rich artifact annotations and demonstrate both efficacy and versatility across diverse applications. Code is available at link.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20951.png", "numComments": 1, "submittedBy": {"_id": "6682429bb1f69d3cd0104389", "avatarUrl": "/avatars/05745a9fba109c7cf87e03f1630ff8b0.svg", "fullname": "Jaehyun Park", "name": "Cabbalett", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.18940", "authors": [{"_id": "699e9a47dfbcf0b800aecbd8", "name": "Elad Ben Avraham", "hidden": false}, {"_id": "699e9a47dfbcf0b800aecbd9", "name": "Changhao Li", "hidden": false}, {"_id": "699e9a47dfbcf0b800aecbda", "name": "Ron Dorfman", "hidden": false}, {"_id": "699e9a47dfbcf0b800aecbdb", "name": "Roy Ganz", "hidden": false}, {"_id": "699e9a47dfbcf0b800aecbdc", "name": "Oren Nuriel", "hidden": false}, {"_id": "699e9a47dfbcf0b800aecbdd", "name": "Amir Dudai", "hidden": false}, {"_id": "699e9a47dfbcf0b800aecbde", "name": "Aviad Aberdam", "hidden": false}, {"_id": "699e9a47dfbcf0b800aecbdf", "name": "Noah Flynn", "hidden": false}, {"_id": "699e9a47dfbcf0b800aecbe0", "name": "Elman Mansimov", "hidden": false}, {"_id": "699e9a47dfbcf0b800aecbe1", "name": "Adi Kalyanpur", "hidden": false}, {"_id": "699e9a47dfbcf0b800aecbe2", "name": "Ron Litman", "hidden": false}], "publishedAt": "2026-02-21T19:14:31.000Z", "submittedOnDailyAt": "2026-02-25T04:20:50.993Z", "title": "DREAM: Deep Research Evaluation with Agentic Metrics", "submittedOnDailyBy": {"_id": "62f0cb39671fc964b5063aba", "avatarUrl": "/avatars/02404cd78c395331b42500dd6ced35eb.svg", "isPro": false, "fullname": "Roy Ganz", "user": "proy", "type": "user"}, "summary": "Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.", "upvotes": 10, "discussionId": "699e9a47dfbcf0b800aecbe3", "ai_summary": "Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.", "ai_keywords": ["Deep Research Agents", "analyst-grade reports", "Mirage of Synthesis", "tool-use capabilities", "temporal validity", "factual correctness", "DREAM", "agentic metrics", "evaluation protocol", "query-agnostic metrics", "adaptive metrics", "tool-calling agent", "temporally aware coverage", "grounded verification", "systematic reasoning probes"], "organization": {"_id": "6058ec29102f61b42f65ae35", "name": "AWS", "fullname": "Amazon Web Services", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/wtuzZxWzijQ3do3zYoOFH.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-21T14:14:31.000Z", "title": "DREAM: Deep Research Evaluation with Agentic Metrics", "summary": "Deep Research Agents generate analyst-grade reports, yet evaluating them remains challenging due to the absence of a single ground truth and the multidimensional nature of research quality. Recent benchmarks propose distinct methodologies, yet they suffer from the Mirage of Synthesis, where strong surface-level fluency and citation alignment can obscure underlying factual and reasoning defects. We characterize this gap by introducing a taxonomy across four verticals that exposes a critical capability mismatch: static evaluators inherently lack the tool-use capabilities required to assess temporal validity and factual correctness. To address this, we propose DREAM (Deep Research Evaluation with Agentic Metrics), a framework that instantiates the principle of capability parity by making evaluation itself agentic. DREAM structures assessment through an evaluation protocol combining query-agnostic metrics with adaptive metrics generated by a tool-calling agent, enabling temporally aware coverage, grounded verification, and systematic reasoning probes. Controlled evaluations demonstrate DREAM is significantly more sensitive to factual and temporal decay than existing benchmarks, offering a scalable, reference-free evaluation paradigm.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18940.png", "numComments": 1, "submittedBy": {"_id": "62f0cb39671fc964b5063aba", "avatarUrl": "/avatars/02404cd78c395331b42500dd6ced35eb.svg", "fullname": "Roy Ganz", "name": "proy", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "6058ec29102f61b42f65ae35", "name": "AWS", "fullname": "Amazon Web Services", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66f19ed428ae41c20c470792/wtuzZxWzijQ3do3zYoOFH.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.14337", "authors": [{"_id": "699cea804e37ec6dfa1bc476", "name": "Yukang Feng", "hidden": false}, {"_id": "699cea804e37ec6dfa1bc477", "name": "Jianwen Sun", "hidden": false}, {"_id": "699cea804e37ec6dfa1bc478", "name": "Zelai Yang", "hidden": false}, {"_id": "699cea804e37ec6dfa1bc479", "name": "Jiaxin Ai", "hidden": false}, {"_id": "699cea804e37ec6dfa1bc47a", "name": "Chuanhao Li", "hidden": false}, {"_id": "699cea804e37ec6dfa1bc47b", "name": "Zizhen Li", "hidden": false}, {"_id": "699cea804e37ec6dfa1bc47c", "name": "Fanrui Zhang", "hidden": false}, {"_id": "699cea804e37ec6dfa1bc47d", "name": "Kang He", "hidden": false}, {"_id": "699cea804e37ec6dfa1bc47e", "name": "Rui Ma", "hidden": false}, {"_id": "699cea804e37ec6dfa1bc47f", "name": "Jifan Lin", "hidden": false}, {"_id": "699cea804e37ec6dfa1bc480", "name": "Jie Sun", "hidden": false}, {"_id": "699cea804e37ec6dfa1bc481", "name": "Yang Xiao", "hidden": false}, {"_id": "699cea804e37ec6dfa1bc482", "name": "Sizhuo Zhou", "hidden": false}, {"_id": "699cea804e37ec6dfa1bc483", "name": "Wenxiao Wu", "hidden": false}, {"_id": "699cea804e37ec6dfa1bc484", "name": "Yiming Liu", "hidden": false}, {"_id": "699cea804e37ec6dfa1bc485", "name": "Pengfei Liu", "hidden": false}, {"_id": "699cea804e37ec6dfa1bc486", "name": "Yu Qiao", "hidden": false}, {"_id": "699cea804e37ec6dfa1bc487", "name": "Shenglin Zhang", "hidden": false}, {"_id": "699cea804e37ec6dfa1bc488", "name": "Kaipeng Zhang", "hidden": false}], "publishedAt": "2026-02-15T23:12:57.000Z", "submittedOnDailyAt": "2026-02-25T01:19:35.669Z", "title": "LongCLI-Bench: A Preliminary Benchmark and Study for Long-horizon Agentic Programming in Command-Line Interfaces", "submittedOnDailyBy": {"_id": "65f1713552c38a91e0a445e8", "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg", "isPro": false, "fullname": "kaipeng", "user": "kpzhang996", "type": "user"}, "summary": "Recent advances in AI-assisted programming have empowered agents to execute complex workflows via command-line interfaces, however, existing benchmarks are limited by short task horizons, data contamination from GitHub scraping, and a lack of fine-grained evaluation metrics, fail to rigorously evaluate the long-horizon planning and execution capabilities essential for realistic software engineering. To address these gaps, we introduce LongCLI-Bench, a comprehensive benchmark designed to evaluate agentic capabilities across long-horizon, realistic tasks. We curated 20 high-quality, long-horizon tasks from over 1,000 computer science assignments and real-world workflows, covering four engineering categories: from scratch, feature addition, bug fixing, and refactoring. We propose a dual-set testing protocol for LongCLI-Bench, which measures requirement fulfillment (fail-to-pass) and regression avoidance (pass-to-pass), and incorporates step-level scoring to pinpoint execution failures. Extensive experiments reveal that even state-of-the-art agents achieve pass rates below 20% in LongCLI-Bench. Step-level analysis further indicates that the majority of tasks stall at less than 30% completion, highlighting that critical failures often occur in the early stages. Although self-correction offers marginal gains, human-agent collaboration through plan injection and interactive guidance yields significantly higher improvements. These results highlight that future research must emphasize the development of synergistic human-agent workflows alongside advances in agents' planning and execution capabilities to overcome key challenges in long-horizon task performance.", "upvotes": 10, "discussionId": "699cea804e37ec6dfa1bc489", "projectPage": "https://github.com/finyorko/longcli-bench", "githubRepo": "https://github.com/finyorko/longcli-bench", "githubRepoAddedBy": "user", "ai_summary": "LongCLI-Bench evaluates AI agents' ability to complete complex, multi-step programming tasks through command-line interfaces with detailed failure analysis and human-agent collaboration insights.", "ai_keywords": ["AI-assisted programming", "command-line interfaces", "long-horizon planning", "agent evaluation", "requirement fulfillment", "regression avoidance", "step-level scoring", "self-correction", "human-agent collaboration", "plan injection", "interactive guidance"], "githubStars": 22, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-15T18:12:57.000Z", "title": "LongCLI-Bench: A Preliminary Benchmark and Study for Long-horizon Agentic Programming in Command-Line Interfaces", "summary": "Recent advances in AI-assisted programming have empowered agents to execute complex workflows via command-line interfaces, however, existing benchmarks are limited by short task horizons, data contamination from GitHub scraping, and a lack of fine-grained evaluation metrics, fail to rigorously evaluate the long-horizon planning and execution capabilities essential for realistic software engineering. To address these gaps, we introduce LongCLI-Bench, a comprehensive benchmark designed to evaluate agentic capabilities across long-horizon, realistic tasks. We curated 20 high-quality, long-horizon tasks from over 1,000 computer science assignments and real-world workflows, covering four engineering categories: from scratch, feature addition, bug fixing, and refactoring. We propose a dual-set testing protocol for LongCLI-Bench, which measures requirement fulfillment (fail-to-pass) and regression avoidance (pass-to-pass), and incorporates step-level scoring to pinpoint execution failures. Extensive experiments reveal that even state-of-the-art agents achieve pass rates below 20% in LongCLI-Bench. Step-level analysis further indicates that the majority of tasks stall at less than 30% completion, highlighting that critical failures often occur in the early stages. Although self-correction offers marginal gains, human-agent collaboration through plan injection and interactive guidance yields significantly higher improvements. These results highlight that future research must emphasize the development of synergistic human-agent workflows alongside advances in agents' planning and execution capabilities to overcome key challenges in long-horizon task performance.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14337.png", "numComments": 2, "submittedBy": {"_id": "65f1713552c38a91e0a445e8", "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg", "fullname": "kaipeng", "name": "kpzhang996", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.20309", "authors": [{"_id": "699ecdf3dfbcf0b800aecce9", "name": "Jingxuan Zhang", "hidden": false}, {"_id": "699ecdf3dfbcf0b800aeccea", "name": "Yunta Hsieh", "hidden": false}, {"_id": "699ecdf3dfbcf0b800aecceb", "name": "Zhongwei Wang", "hidden": false}, {"_id": "699ecdf3dfbcf0b800aeccec", "name": "Haokun Lin", "hidden": false}, {"_id": "699ecdf3dfbcf0b800aecced", "name": "Xin Wang", "hidden": false}, {"_id": "699ecdf3dfbcf0b800aeccee", "name": "Ziqi Wang", "hidden": false}, {"_id": "699ecdf3dfbcf0b800aeccef", "name": "Yingtie Lei", "hidden": false}, {"_id": "699ecdf3dfbcf0b800aeccf0", "name": "Mi Zhang", "hidden": false}], "publishedAt": "2026-02-23T19:55:54.000Z", "submittedOnDailyAt": "2026-02-25T07:56:34.044Z", "title": "QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models", "submittedOnDailyBy": {"_id": "6841378dcb9938c7aed795be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WNI8gQVHdWyt0OvmRINDp.png", "isPro": false, "fullname": "Haokun Lin", "user": "Felix1023", "type": "user"}, "summary": "Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.", "upvotes": 8, "discussionId": "699ecdf3dfbcf0b800aeccf1", "projectPage": "https://quantvla.github.io/", "ai_summary": "QuantVLA is a post-training quantization framework for vision-language-action models that enables efficient deployment through selective quantization, attention temperature matching, and output head balancing while maintaining performance and reducing memory and latency.", "ai_keywords": ["post-training quantization", "diffusion transformer", "attention projections", "attention temperature matching", "output head balancing", "integer kernels", "calibration buffer", "end-to-end inference latency"], "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-23T14:55:54.000Z", "title": "QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models", "summary": "Vision-language-action (VLA) models unify perception, language, and control for embodied agents but face significant challenges in practical deployment due to rapidly increasing compute and memory demands, especially as models scale to longer horizons and larger backbones. To address these bottlenecks, we introduce QuantVLA, a training-free post-training quantization (PTQ) framework that, to our knowledge, is the first PTQ approach for VLA systems and the first to successfully quantize a diffusion transformer (DiT) action head. QuantVLA incorporates three scale-calibrated components: (1) a selective quantization layout that integerizes all linear layers in both the language backbone and the DiT while keeping attention projections in floating point to preserve the original operator schedule; (2) attention temperature matching, a lightweight per-head scaling mechanism that stabilizes attention logits and is folded into the dequantization scales at inference; and (3) output head balancing, a per-layer residual interface calibration that mitigates post-projection energy drift. The framework requires no additional training, uses only a small unlabeled calibration buffer, and supports integer kernels for low-bit weights and activations while leaving the architecture unchanged. Across representative VLA models on LIBERO, QuantVLA exceeds the task success rates of full-precision baselines, achieves about 70% relative memory savings on the quantized components, and delivers a 1.22x speedup in end-to-end inference latency, providing a practical pathway toward scalable low-bit embodied intelligence under strict compute, memory, and power constraints.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20309.png", "numComments": 1, "submittedBy": {"_id": "6841378dcb9938c7aed795be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WNI8gQVHdWyt0OvmRINDp.png", "fullname": "Haokun Lin", "name": "Felix1023", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2602.20159", "authors": [{"_id": "699d1e7a4e37ec6dfa1bc5b7", "user": {"_id": "67f87529318a17cc80365190", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/p1YkznAT-op1Cg-vBoFw7.png", "isPro": false, "fullname": "Maijunxian Wang", "user": "Mark7121983123", "type": "user"}, "name": "Maijunxian Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:19.409Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5b8", "user": {"_id": "65b74305e602b6c2c9125480", "avatarUrl": "/avatars/d36909e0f245bfeb632a4afc9d3fceca.svg", "isPro": false, "fullname": "wang ruisi", "user": "wruisi", "type": "user"}, "name": "Ruisi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:01.779Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5b9", "name": "Juyi Lin", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ba", "name": "Ran Ji", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bb", "name": "Thadd\u00e4us Wiedemer", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bc", "name": "Qingying Gao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bd", "name": "Dezhi Luo", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5be", "name": "Yaoyao Qian", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bf", "name": "Lianyu Huang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c0", "name": "Zelong Hong", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c1", "name": "Jiahui Ge", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c2", "name": "Qianli Ma", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c3", "name": "Hang He", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c4", "user": {"_id": "659d2dff20cf0b934bbee513", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659d2dff20cf0b934bbee513/9e9R852Zr2R82h64eUUQl.jpeg", "isPro": false, "fullname": "Yifan Zhou", "user": "yingmanji", "type": "user"}, "name": "Yifan Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:04.030Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c5", "name": "Lingzi Guo", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c6", "name": "Lantao Mei", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c7", "name": "Jiachen Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c8", "user": {"_id": "6532c3018bde2fae19578587", "avatarUrl": "/avatars/7231538f3d682a1e7b80e15ea91b2a97.svg", "isPro": false, "fullname": "Hanwen Xing", "user": "Hudx111", "type": "user"}, "name": "Hanwen Xing", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:09.132Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c9", "name": "Tianqi Zhao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ca", "name": "Fengyuan Yu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cb", "name": "Weihang Xiao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cc", "name": "Yizheng Jiao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cd", "name": "Jianheng Hou", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ce", "name": "Danyang Zhang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cf", "user": {"_id": "65d854e5d8134b93774b4080", "avatarUrl": "/avatars/0ff1db8c13095f420a856212d64f88ca.svg", "isPro": false, "fullname": "Pengcheng Xu", "user": "explcre", "type": "user"}, "name": "Pengcheng Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:21.447Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d0", "name": "Boyang Zhong", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d1", "name": "Zehong Zhao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d2", "name": "Gaoyun Fang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d3", "name": "John Kitaoka", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d4", "name": "Yile Xu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d5", "name": "Hua Xu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d6", "name": "Kenton Blacutt", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d7", "name": "Tin Nguyen", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d8", "name": "Siyuan Song", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d9", "name": "Haoran Sun", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5da", "name": "Shaoyue Wen", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5db", "name": "Linyang He", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5dc", "name": "Runming Wang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5dd", "name": "Yanzhi Wang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5de", "name": "Mengyue Yang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5df", "name": "Ziqiao Ma", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e0", "name": "Rapha\u00ebl Milli\u00e8re", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e1", "name": "Freda Shi", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e2", "name": "Nuno Vasconcelos", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e3", "name": "Daniel Khashabi", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e4", "name": "Alan Yuille", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e5", "name": "Yilun Du", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e6", "name": "Ziming Liu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e7", "name": "Bo Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e8", "name": "Dahua Lin", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e9", "name": "Ziwei Liu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ea", "name": "Vikash Kumar", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5eb", "name": "Yijiang Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ec", "user": {"_id": "6626a471430a124253f197c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6626a471430a124253f197c8/uVEk5nnW-bS6-no0rQ7Wh.png", "isPro": false, "fullname": "yl-1993", "user": "yl-1993", "type": "user"}, "name": "Lei Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:17.400Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ed", "user": {"_id": "652d06833b5997ed71ce5c46", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xZTXEcnEogEmBm_ledJQr.jpeg", "isPro": false, "fullname": "Zhongang Cai", "user": "caizhongang", "type": "user"}, "name": "Zhongang Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:06.679Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ee", "user": {"_id": "6793f65033629a5fa8ae47b5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/h11aIy3Yuw0kLCil5yeOt.png", "isPro": false, "fullname": "Hokin Deng", "user": "Hokin", "type": "user"}, "name": "Hokin Deng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:49:58.948Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IjK1EcVGF8x-SG3ymPI6T.mp4"], "publishedAt": "2026-02-23T18:59:41.000Z", "submittedOnDailyAt": "2026-02-24T01:14:31.428Z", "title": "A Very Big Video Reasoning Suite", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .", "upvotes": 302, "discussionId": "699d1e7b4e37ec6dfa1bc5ef", "projectPage": "https://video-reason.com/", "ai_summary": "A large-scale video reasoning dataset and benchmark are introduced to study video intelligence capabilities beyond visual quality, enabling systematic analysis of spatiotemporal reasoning and generalization across diverse tasks.", "ai_keywords": ["video reasoning", "spatiotemporal consistency", "emergent generalization", "video reasoning benchmark", "video reasoning dataset"], "organization": {"_id": "6986a6f58d72821326efbfbb", "name": "Video-Reason", "fullname": "Video-Reason", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6793f65033629a5fa8ae47b5/7JFt2ReogqVi_udM_OHWG.jpeg"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-23T13:59:41.000Z", "title": "A Very Big Video Reasoning Suite", "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IjK1EcVGF8x-SG3ymPI6T.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20159.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 238, "isUserFollowing": false}, "organization": {"_id": "6986a6f58d72821326efbfbb", "name": "Video-Reason", "fullname": "Video-Reason", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6793f65033629a5fa8ae47b5/7JFt2ReogqVi_udM_OHWG.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.21193", "authors": [{"_id": "699e72b4dfbcf0b800aecb63", "name": "Renjie Pi", "hidden": false}, {"_id": "699e72b4dfbcf0b800aecb64", "name": "Grace Lam", "hidden": false}, {"_id": "699e72b4dfbcf0b800aecb65", "name": "Mohammad Shoeybi", "hidden": false}, {"_id": "699e72b4dfbcf0b800aecb66", "name": "Pooya Jannaty", "hidden": false}, {"_id": "699e72b4dfbcf0b800aecb67", "name": "Bryan Catanzaro", "hidden": false}, {"_id": "699e72b4dfbcf0b800aecb68", "name": "Wei Ping", "hidden": false}], "publishedAt": "2026-02-24T18:51:04.000Z", "submittedOnDailyAt": "2026-02-25T01:39:40.130Z", "title": "On Data Engineering for Scaling LLM Terminal Capabilities", "submittedOnDailyBy": {"_id": "63f45b8d520c14618930d175", "avatarUrl": "/avatars/42b3aaf50748a25e4a596fc57ab1306d.svg", "isPro": false, "fullname": "renjie", "user": "renjiepi", "type": "user"}, "summary": "Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.", "upvotes": 60, "discussionId": "699e72b5dfbcf0b800aecb69", "projectPage": "https://huggingface.co/collections/nvidia/nemotron-terminal", "ai_summary": "Researchers developed a synthetic task generation pipeline and analyzed data strategies to improve terminal agent performance, creating a large-scale dataset and models that outperform larger counterparts on benchmark tests.", "ai_keywords": ["large language models", "terminal agents", "data engineering practices", "synthetic task generation", "Terminal-Task-Gen", "Terminal-Corpus", "Nemotron-Terminal", "Terminal-Bench 2.0", "curriculum learning", "long context training", "scaling behavior"], "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-24T13:51:04.000Z", "title": "On Data Engineering for Scaling LLM Terminal Capabilities", "summary": "Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21193.png", "numComments": 2, "submittedBy": {"_id": "63f45b8d520c14618930d175", "avatarUrl": "/avatars/42b3aaf50748a25e4a596fc57ab1306d.svg", "fullname": "renjie", "name": "renjiepi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.18532", "authors": [{"_id": "699d29bf4e37ec6dfa1bc66d", "user": {"_id": "61dd2b7389dddd97daead12f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61dd2b7389dddd97daead12f/xUL7YK3Mtvzz4rEhorFKF.jpeg", "isPro": false, "fullname": "Xiao-Ming Wu", "user": "DravenALG", "type": "user"}, "name": "Xiao-Ming Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:14.717Z", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc66e", "name": "Bin Fan", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc66f", "name": "Kang Liao", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc670", "name": "Jian-Jian Jiang", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc671", "name": "Runze Yang", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc672", "name": "Yihang Luo", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc673", "name": "Zhonghua Wu", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc674", "name": "Wei-Shi Zheng", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc675", "name": "Chen Change Loy", "hidden": false}], "publishedAt": "2026-02-20T09:26:17.000Z", "submittedOnDailyAt": "2026-02-24T07:48:05.528Z", "title": "VLANeXt: Recipes for Building Strong VLA Models", "submittedOnDailyBy": {"_id": "61dd2b7389dddd97daead12f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61dd2b7389dddd97daead12f/xUL7YK3Mtvzz4rEhorFKF.jpeg", "isPro": false, "fullname": "Xiao-Ming Wu", "user": "DravenALG", "type": "user"}, "summary": "Following the rise of large foundation models, Vision-Language-Action models (VLAs) emerged, leveraging strong visual and language understanding for general-purpose policy learning. Yet, the current VLA landscape remains fragmented and exploratory. Although many groups have proposed their own VLA models, inconsistencies in training protocols and evaluation settings make it difficult to identify which design choices truly matter. To bring structure to this evolving space, we reexamine the VLA design space under a unified framework and evaluation setup. Starting from a simple VLA baseline similar to RT-2 and OpenVLA, we systematically dissect design choices along three dimensions: foundational components, perception essentials, and action modelling perspectives. From this study, we distill 12 key findings that together form a practical recipe for building strong VLA models. The outcome of this exploration is a simple yet effective model, VLANeXt. VLANeXt outperforms prior state-of-the-art methods on the LIBERO and LIBERO-plus benchmarks and demonstrates strong generalization in real-world experiments. We will release a unified, easy-to-use codebase that serves as a common platform for the community to reproduce our findings, explore the design space, and build new VLA variants on top of a shared foundation.", "upvotes": 39, "discussionId": "699d29c04e37ec6dfa1bc676", "projectPage": "https://dravenalg.github.io/VLANeXt/", "githubRepo": "https://github.com/DravenALG/VLANeXt", "githubRepoAddedBy": "user", "ai_summary": "Vision-Language-Action models are systematically analyzed and optimized through a unified framework, resulting in the VLANeXt model that achieves superior performance on benchmark tasks and demonstrates strong real-world generalization.", "ai_keywords": ["Vision-Language-Action models", "foundation models", "policy learning", "VLA design space", "RT-2", "OpenVLA", "perception essentials", "action modelling perspectives", "LIBERO", "LIBERO-plus", "VLANeXt"], "githubStars": 43, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-20T04:26:17.000Z", "title": "VLANeXt: Recipes for Building Strong VLA Models", "summary": "Following the rise of large foundation models, Vision-Language-Action models (VLAs) emerged, leveraging strong visual and language understanding for general-purpose policy learning. Yet, the current VLA landscape remains fragmented and exploratory. Although many groups have proposed their own VLA models, inconsistencies in training protocols and evaluation settings make it difficult to identify which design choices truly matter. To bring structure to this evolving space, we reexamine the VLA design space under a unified framework and evaluation setup. Starting from a simple VLA baseline similar to RT-2 and OpenVLA, we systematically dissect design choices along three dimensions: foundational components, perception essentials, and action modelling perspectives. From this study, we distill 12 key findings that together form a practical recipe for building strong VLA models. The outcome of this exploration is a simple yet effective model, VLANeXt. VLANeXt outperforms prior state-of-the-art methods on the LIBERO and LIBERO-plus benchmarks and demonstrates strong generalization in real-world experiments. We will release a unified, easy-to-use codebase that serves as a common platform for the community to reproduce our findings, explore the design space, and build new VLA variants on top of a shared foundation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18532.png", "numComments": 1, "submittedBy": {"_id": "61dd2b7389dddd97daead12f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61dd2b7389dddd97daead12f/xUL7YK3Mtvzz4rEhorFKF.jpeg", "fullname": "Xiao-Ming Wu", "name": "DravenALG", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.19672", "authors": [{"_id": "699d407c4e37ec6dfa1bc6a7", "user": {"_id": "651651f5d93a51ceda3021c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651651f5d93a51ceda3021c3/FE2uGpTKBRWMKTDBv1H-g.png", "isPro": false, "fullname": "Jiayu (Mila) Wang", "user": "MilaWang", "type": "user"}, "name": "Jiayu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:49:47.846Z", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6a8", "name": "Yifei Ming", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6a9", "name": "Zixuan Ke", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6aa", "name": "Shafiq Joty", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6ab", "name": "Aws Albarghouthi", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6ac", "name": "Frederic Sala", "hidden": false}], "publishedAt": "2026-02-23T10:17:25.000Z", "submittedOnDailyAt": "2026-02-24T03:44:00.879Z", "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer", "submittedOnDailyBy": {"_id": "651651f5d93a51ceda3021c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651651f5d93a51ceda3021c3/FE2uGpTKBRWMKTDBv1H-g.png", "isPro": false, "fullname": "Jiayu (Mila) Wang", "user": "MilaWang", "type": "user"}, "summary": "Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.", "upvotes": 30, "discussionId": "699d407c4e37ec6dfa1bc6ad", "ai_summary": "SkillOrchestra presents a skill-aware orchestration framework that improves compound AI system performance through fine-grained skill modeling and efficient agent selection, achieving superior results with significantly reduced learning costs compared to reinforcement learning-based methods.", "ai_keywords": ["compound AI systems", "orchestration", "routing policy", "reinforcement learning", "skill modeling", "agent-specific competence", "performance-cost trade-off", "multi-turn scenarios", "routing collapse", "end-to-end learning"], "organization": {"_id": "61d090ec03bc10eb8e1c2970", "name": "uw-madison", "fullname": "University of Wisconsin - Madison", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-23T05:17:25.000Z", "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer", "summary": "Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19672.png", "numComments": 1, "submittedBy": {"_id": "651651f5d93a51ceda3021c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651651f5d93a51ceda3021c3/FE2uGpTKBRWMKTDBv1H-g.png", "fullname": "Jiayu (Mila) Wang", "name": "MilaWang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "61d090ec03bc10eb8e1c2970", "name": "uw-madison", "fullname": "University of Wisconsin - Madison", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.20739", "authors": [{"_id": "699e674ddfbcf0b800aecae9", "user": {"_id": "62c66504031996c36c86976a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png", "isPro": false, "fullname": "steve z", "user": "stzhao", "type": "user"}, "name": "Shitian Zhao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-25T17:29:58.018Z", "hidden": false}, {"_id": "699e674ddfbcf0b800aecaea", "name": "Shaoheng Lin", "hidden": false}, {"_id": "699e674ddfbcf0b800aecaeb", "name": "Ming Li", "hidden": false}, {"_id": "699e674ddfbcf0b800aecaec", "user": {"_id": "67ff7f687351095d4b606b84", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67ff7f687351095d4b606b84/KhNPmbBC3zghuP5h1MK-c.png", "isPro": false, "fullname": "Haoquan Zhang", "user": "haoquan03", "type": "user"}, "name": "Haoquan Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-25T17:29:51.660Z", "hidden": false}, {"_id": "699e674ddfbcf0b800aecaed", "name": "Wenshuo Peng", "hidden": false}, {"_id": "699e674ddfbcf0b800aecaee", "name": "Kaipeng Zhang", "hidden": false}, {"_id": "699e674ddfbcf0b800aecaef", "name": "Chen Wei", "hidden": false}], "publishedAt": "2026-02-24T10:08:33.000Z", "submittedOnDailyAt": "2026-02-25T00:41:13.309Z", "title": "PyVision-RL: Forging Open Agentic Vision Models via RL", "submittedOnDailyBy": {"_id": "62c66504031996c36c86976a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png", "isPro": false, "fullname": "steve z", "user": "stzhao", "type": "user"}, "summary": "Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.", "upvotes": 22, "discussionId": "699e674edfbcf0b800aecaf0", "projectPage": "https://agent-x.space/pyvision-rl/", "githubRepo": "https://github.com/agents-x-project/PyVision-RL", "githubRepoAddedBy": "user", "ai_summary": "PyVision-RL framework addresses interaction collapse in multimodal models through enhanced reinforcement learning techniques and efficient video processing strategies.", "ai_keywords": ["reinforcement learning", "agentic multimodal models", "interaction collapse", "oversampling-filtering-ranking", "accumulative tool reward", "unified training pipeline", "on-demand context construction", "task-relevant frames", "visual token usage"], "githubStars": 43, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-24T05:08:33.000Z", "title": "PyVision-RL: Forging Open Agentic Vision Models via RL", "summary": "Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20739.png", "numComments": 1, "submittedBy": {"_id": "62c66504031996c36c86976a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png", "fullname": "steve z", "name": "stzhao", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.19313", "authors": [{"_id": "699d1f754e37ec6dfa1bc5ff", "name": "Shirui Chen", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc600", "name": "Cole Harrison", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc601", "name": "Ying-Chun Lee", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc602", "name": "Angela Jin Yang", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc603", "name": "Zhongzheng Ren", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc604", "name": "Lillian J. Ratliff", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc605", "name": "Jiafei Duan", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc606", "name": "Dieter Fox", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc607", "name": "Ranjay Krishna", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/ez9whqUgnhTTjrPZiPdzP.png"], "publishedAt": "2026-02-22T19:25:48.000Z", "submittedOnDailyAt": "2026-02-24T01:19:53.486Z", "title": "TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics", "submittedOnDailyBy": {"_id": "632b42626110e37dba3d5bcb", "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg", "isPro": false, "fullname": "Duan", "user": "Jiafei1224", "type": "user"}, "summary": "While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM's internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.", "upvotes": 21, "discussionId": "699d1f754e37ec6dfa1bc608", "projectPage": "https://topreward.github.io/webpage/", "githubRepo": "https://github.com/TOPReward/TOPReward", "githubRepoAddedBy": "user", "ai_summary": "TOPReward is a probabilistically grounded temporal value function that uses pretrained video Vision-Language Models to estimate robotic task progress through internal token logits, achieving superior performance in zero-shot evaluations across diverse real-world tasks.", "ai_keywords": ["Vision-Language-Action models", "Reinforcement Learning", "temporal value functions", "Vision-Language Models", "token logits", "Value-Order Correlation", "reward-aligned behavior cloning"], "githubStars": 1, "organization": {"_id": "5e70f3648ce3c604d78fe132", "name": "allenai", "fullname": "Ai2", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-22T14:25:48.000Z", "title": "TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics", "summary": "While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM's internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/ez9whqUgnhTTjrPZiPdzP.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19313.png", "numComments": 1, "submittedBy": {"_id": "632b42626110e37dba3d5bcb", "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg", "fullname": "Duan", "name": "Jiafei1224", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "5e70f3648ce3c604d78fe132", "name": "allenai", "fullname": "Ai2", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.17270", "authors": [{"_id": "6997ce527a658569d5a10165", "name": "Jonathan Heek", "hidden": false}, {"_id": "6997ce527a658569d5a10166", "user": {"_id": "671feaa3e269c38584cb065f", "avatarUrl": "/avatars/d7bc35ab98b208ef1d009aa9b629682a.svg", "isPro": false, "fullname": "Emiel Hoogeboom", "user": "Emiel2", "type": "user"}, "name": "Emiel Hoogeboom", "status": "admin_assigned", "statusLastChangedAt": "2026-02-20T09:03:14.323Z", "hidden": false}, {"_id": "6997ce527a658569d5a10167", "user": {"_id": "642ad5bc508b7246f9d27ab9", "avatarUrl": "/avatars/6cdf6f855536896575733e273388d7fb.svg", "isPro": false, "fullname": "Thomas Mensink", "user": "tmensink", "type": "user"}, "name": "Thomas Mensink", "status": "admin_assigned", "statusLastChangedAt": "2026-02-20T09:03:21.125Z", "hidden": false}, {"_id": "6997ce527a658569d5a10168", "name": "Tim Salimans", "hidden": false}], "publishedAt": "2026-02-19T11:18:12.000Z", "submittedOnDailyAt": "2026-02-20T00:30:40.692Z", "title": "Unified Latents (UL): How to train your latents", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.", "upvotes": 21, "discussionId": "6997ce527a658569d5a10169", "ai_summary": "Unified Latents framework learns joint latent representations using diffusion prior regularization and diffusion model decoding, achieving competitive FID scores with reduced training compute.", "ai_keywords": ["diffusion prior", "diffusion model", "latent representations", "training objective", "latent bitrate", "FID", "PSNR", "FLOPs", "FVD"], "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-19T06:18:12.000Z", "title": "Unified Latents (UL): How to train your latents", "summary": "We present Unified Latents (UL), a framework for learning latent representations that are jointly regularized by a diffusion prior and decoded by a diffusion model. By linking the encoder's output noise to the prior's minimum noise level, we obtain a simple training objective that provides a tight upper bound on the latent bitrate. On ImageNet-512, our approach achieves competitive FID of 1.4, with high reconstruction quality (PSNR) while requiring fewer training FLOPs than models trained on Stable Diffusion latents. On Kinetics-600, we set a new state-of-the-art FVD of 1.3.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17270.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 235, "isUserFollowing": false}, "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.21015", "authors": [{"_id": "699e69fbdfbcf0b800aecafb", "user": {"_id": "63369da91ba5d5ece24118a4", "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg", "isPro": false, "fullname": "wuyuhao", "user": "mozhu", "type": "user"}, "name": "Yuhao Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-25T17:29:26.632Z", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecafc", "name": "Maojia Song", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecafd", "name": "Yihuai Lan", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecafe", "name": "Lei Wang", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecaff", "user": {"_id": "637f228152229c63921119c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg", "isPro": false, "fullname": "Zhiqiang Hu", "user": "Zhiqiang007", "type": "user"}, "name": "Zhiqiang Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-25T17:29:48.417Z", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecb00", "name": "Yao Xiao", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecb01", "user": {"_id": "660d17d6c9be0dcd31a30b3d", "avatarUrl": "/avatars/3743fe9b695c488ebe33f0d8fd607a8a.svg", "isPro": false, "fullname": "Zhou Heng", "user": "henggg", "type": "user"}, "name": "Heng Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-02-25T17:29:45.251Z", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecb02", "name": "Weihua Zheng", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecb03", "name": "Dylan Raharja", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecb04", "name": "Soujanya Poria", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecb05", "name": "Roy Ka-Wei Lee", "hidden": false}], "publishedAt": "2026-02-24T15:33:02.000Z", "submittedOnDailyAt": "2026-02-25T00:55:31.629Z", "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning", "submittedOnDailyBy": {"_id": "637f228152229c63921119c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg", "isPro": false, "fullname": "Zhiqiang Hu", "user": "Zhiqiang007", "type": "user"}, "summary": "Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.", "upvotes": 20, "discussionId": "699e69fbdfbcf0b800aecb06", "projectPage": "https://social-ai-studio.github.io/CHAIN/", "githubRepo": "https://github.com/Social-AI-Studio/CHAIN", "githubRepoAddedBy": "user", "ai_summary": "Current vision-language models lack capability to understand physical structures and causal constraints needed for complex, interactive 3D tasks, as demonstrated by the CHAIN benchmark evaluating structured action planning under physical constraints.", "ai_keywords": ["Vision-Language Model", "diffusion-based models", "physical constraints", "causal constraints", "interactive 3D", "structured action sequences", "long-horizon planning"], "githubStars": 3, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-24T10:33:02.000Z", "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning", "summary": "Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21015.png", "numComments": 2, "submittedBy": {"_id": "637f228152229c63921119c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg", "fullname": "Zhiqiang Hu", "name": "Zhiqiang007", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.21204", "authors": [{"_id": "699e6e60dfbcf0b800aecb16", "name": "Junchen Liu", "hidden": false}, {"_id": "699e6e60dfbcf0b800aecb17", "name": "Sven Elflein", "hidden": false}, {"_id": "699e6e60dfbcf0b800aecb18", "name": "Or Litany", "hidden": false}, {"_id": "699e6e60dfbcf0b800aecb19", "name": "Zan Gojcic", "hidden": false}, {"_id": "699e6e60dfbcf0b800aecb1a", "name": "Ruilong Li", "hidden": false}], "publishedAt": "2026-02-24T18:59:30.000Z", "submittedOnDailyAt": "2026-02-25T03:15:10.854Z", "title": "Test-Time Training with KV Binding Is Secretly Linear Attention", "submittedOnDailyBy": {"_id": "663a9ae58cf658ffaf3d02e5", "avatarUrl": "/avatars/aa35668dc088e675e794b9ceb935c56f.svg", "isPro": false, "fullname": "Junchen Liu", "user": "JunchenLiu", "type": "user"}, "summary": "Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.", "upvotes": 19, "discussionId": "699e6e61dfbcf0b800aecb1b", "projectPage": "https://research.nvidia.com/labs/sil/projects/tttla/", "ai_summary": "Test-time training is reinterpreted as learned linear attention rather than memorization, offering architectural simplifications and improved efficiency.", "ai_keywords": ["test-time training", "KV binding", "online meta-learning", "learned linear attention", "sequence modeling layer", "linear attention operator", "architectural simplifications", "parallel formulations", "representational capacity"], "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-24T13:59:30.000Z", "title": "Test-Time Training with KV Binding Is Secretly Linear Attention", "summary": "Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21204.png", "numComments": 1, "submittedBy": {"_id": "663a9ae58cf658ffaf3d02e5", "avatarUrl": "/avatars/aa35668dc088e675e794b9ceb935c56f.svg", "fullname": "Junchen Liu", "name": "JunchenLiu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.21202", "authors": [{"_id": "699f21fbebfce7fbcca919cd", "name": "Hanxiang Qin", "hidden": false}, {"_id": "699f21fbebfce7fbcca919ce", "name": "Alexander Martin", "hidden": false}, {"_id": "699f21fbebfce7fbcca919cf", "name": "Rohan Jha", "hidden": false}, {"_id": "699f21fbebfce7fbcca919d0", "name": "Chunsheng Zuo", "hidden": false}, {"_id": "699f21fbebfce7fbcca919d1", "name": "Reno Kriz", "hidden": false}, {"_id": "699f21fbebfce7fbcca919d2", "name": "Benjamin Van Durme", "hidden": false}], "publishedAt": "2026-02-24T18:57:33.000Z", "submittedOnDailyAt": "2026-02-25T13:53:39.027Z", "title": "Multi-Vector Index Compression in Any Modality", "submittedOnDailyBy": {"_id": "644ffb154f731658826945c8", "avatarUrl": "/avatars/46fa3a15c2b974692bee5b6ed6468be0.svg", "isPro": false, "fullname": "Alex Martin", "user": "alexmartin1722", "type": "user"}, "summary": "We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.", "upvotes": 18, "discussionId": "699f21fcebfce7fbcca919d3", "githubRepo": "https://github.com/hanxiangqin/omni-col-press", "githubRepoAddedBy": "user", "ai_summary": "Attention-guided clustering method for compressing multi-vector document representations in late interaction retrieval tasks shows superior performance compared to other compression techniques across text, visual-document, and video modalities.", "ai_keywords": ["late interaction", "multi-vector retrieval", "index compression", "attention-guided clustering", "sequence resizing", "memory tokens", "hierarchical pooling"], "githubStars": 0, "organization": {"_id": "643568fef81a16e74361e659", "name": "hltcoe", "fullname": "JHU Human Language Technology Center of Excellence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643568cdf3b08e267d9808f1/u-FGMfs5blYzYGX4ErT7d.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-24T13:57:33.000Z", "title": "Multi-Vector Index Compression in Any Modality", "summary": "We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21202.png", "numComments": 1, "submittedBy": {"_id": "644ffb154f731658826945c8", "avatarUrl": "/avatars/46fa3a15c2b974692bee5b6ed6468be0.svg", "fullname": "Alex Martin", "name": "alexmartin1722", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "643568fef81a16e74361e659", "name": "hltcoe", "fullname": "JHU Human Language Technology Center of Excellence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/643568cdf3b08e267d9808f1/u-FGMfs5blYzYGX4ErT7d.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2602.20159", "authors": [{"_id": "699d1e7a4e37ec6dfa1bc5b7", "user": {"_id": "67f87529318a17cc80365190", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/p1YkznAT-op1Cg-vBoFw7.png", "isPro": false, "fullname": "Maijunxian Wang", "user": "Mark7121983123", "type": "user"}, "name": "Maijunxian Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:19.409Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5b8", "user": {"_id": "65b74305e602b6c2c9125480", "avatarUrl": "/avatars/d36909e0f245bfeb632a4afc9d3fceca.svg", "isPro": false, "fullname": "wang ruisi", "user": "wruisi", "type": "user"}, "name": "Ruisi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:01.779Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5b9", "name": "Juyi Lin", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ba", "name": "Ran Ji", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bb", "name": "Thadd\u00e4us Wiedemer", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bc", "name": "Qingying Gao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bd", "name": "Dezhi Luo", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5be", "name": "Yaoyao Qian", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bf", "name": "Lianyu Huang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c0", "name": "Zelong Hong", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c1", "name": "Jiahui Ge", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c2", "name": "Qianli Ma", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c3", "name": "Hang He", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c4", "user": {"_id": "659d2dff20cf0b934bbee513", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659d2dff20cf0b934bbee513/9e9R852Zr2R82h64eUUQl.jpeg", "isPro": false, "fullname": "Yifan Zhou", "user": "yingmanji", "type": "user"}, "name": "Yifan Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:04.030Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c5", "name": "Lingzi Guo", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c6", "name": "Lantao Mei", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c7", "name": "Jiachen Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c8", "user": {"_id": "6532c3018bde2fae19578587", "avatarUrl": "/avatars/7231538f3d682a1e7b80e15ea91b2a97.svg", "isPro": false, "fullname": "Hanwen Xing", "user": "Hudx111", "type": "user"}, "name": "Hanwen Xing", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:09.132Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c9", "name": "Tianqi Zhao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ca", "name": "Fengyuan Yu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cb", "name": "Weihang Xiao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cc", "name": "Yizheng Jiao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cd", "name": "Jianheng Hou", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ce", "name": "Danyang Zhang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cf", "user": {"_id": "65d854e5d8134b93774b4080", "avatarUrl": "/avatars/0ff1db8c13095f420a856212d64f88ca.svg", "isPro": false, "fullname": "Pengcheng Xu", "user": "explcre", "type": "user"}, "name": "Pengcheng Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:21.447Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d0", "name": "Boyang Zhong", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d1", "name": "Zehong Zhao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d2", "name": "Gaoyun Fang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d3", "name": "John Kitaoka", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d4", "name": "Yile Xu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d5", "name": "Hua Xu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d6", "name": "Kenton Blacutt", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d7", "name": "Tin Nguyen", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d8", "name": "Siyuan Song", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d9", "name": "Haoran Sun", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5da", "name": "Shaoyue Wen", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5db", "name": "Linyang He", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5dc", "name": "Runming Wang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5dd", "name": "Yanzhi Wang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5de", "name": "Mengyue Yang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5df", "name": "Ziqiao Ma", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e0", "name": "Rapha\u00ebl Milli\u00e8re", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e1", "name": "Freda Shi", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e2", "name": "Nuno Vasconcelos", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e3", "name": "Daniel Khashabi", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e4", "name": "Alan Yuille", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e5", "name": "Yilun Du", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e6", "name": "Ziming Liu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e7", "name": "Bo Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e8", "name": "Dahua Lin", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e9", "name": "Ziwei Liu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ea", "name": "Vikash Kumar", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5eb", "name": "Yijiang Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ec", "user": {"_id": "6626a471430a124253f197c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6626a471430a124253f197c8/uVEk5nnW-bS6-no0rQ7Wh.png", "isPro": false, "fullname": "yl-1993", "user": "yl-1993", "type": "user"}, "name": "Lei Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:17.400Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ed", "user": {"_id": "652d06833b5997ed71ce5c46", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xZTXEcnEogEmBm_ledJQr.jpeg", "isPro": false, "fullname": "Zhongang Cai", "user": "caizhongang", "type": "user"}, "name": "Zhongang Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:06.679Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ee", "user": {"_id": "6793f65033629a5fa8ae47b5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/h11aIy3Yuw0kLCil5yeOt.png", "isPro": false, "fullname": "Hokin Deng", "user": "Hokin", "type": "user"}, "name": "Hokin Deng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:49:58.948Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IjK1EcVGF8x-SG3ymPI6T.mp4"], "publishedAt": "2026-02-23T18:59:41.000Z", "submittedOnDailyAt": "2026-02-24T01:14:31.428Z", "title": "A Very Big Video Reasoning Suite", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .", "upvotes": 302, "discussionId": "699d1e7b4e37ec6dfa1bc5ef", "projectPage": "https://video-reason.com/", "ai_summary": "A large-scale video reasoning dataset and benchmark are introduced to study video intelligence capabilities beyond visual quality, enabling systematic analysis of spatiotemporal reasoning and generalization across diverse tasks.", "ai_keywords": ["video reasoning", "spatiotemporal consistency", "emergent generalization", "video reasoning benchmark", "video reasoning dataset"], "organization": {"_id": "6986a6f58d72821326efbfbb", "name": "Video-Reason", "fullname": "Video-Reason", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6793f65033629a5fa8ae47b5/7JFt2ReogqVi_udM_OHWG.jpeg"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-23T13:59:41.000Z", "title": "A Very Big Video Reasoning Suite", "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IjK1EcVGF8x-SG3ymPI6T.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20159.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 238, "isUserFollowing": false}, "organization": {"_id": "6986a6f58d72821326efbfbb", "name": "Video-Reason", "fullname": "Video-Reason", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6793f65033629a5fa8ae47b5/7JFt2ReogqVi_udM_OHWG.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.05400", "authors": [{"_id": "698b396b1b2dc6b37d61b4be", "user": {"_id": "66968099c952e09a4cb29f78", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66968099c952e09a4cb29f78/n90NI2R3E9_RqCyMjDCQF.webp", "isPro": false, "fullname": "Wang", "user": "Steven-Shaobo", "type": "user"}, "name": "Shaobo Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:57.815Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4bf", "user": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "isPro": false, "fullname": "Xuan Ouyang", "user": "YoungXuan", "type": "user"}, "name": "Xuan Ouyang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:55.631Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c0", "user": {"_id": "6518a144a28f86d3e9e67c34", "avatarUrl": "/avatars/f2aed39e971cffe6c9d0b9c2f7a0df70.svg", "isPro": false, "fullname": "Tianyi Xu", "user": "tianyi0216", "type": "user"}, "name": "Tianyi Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:53.605Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c1", "name": "Yuzheng Hu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c2", "name": "Jialin Liu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c3", "name": "Guo Chen", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c4", "name": "Tianyu Zhang", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c5", "name": "Junhao Zheng", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c6", "name": "Kexin Yang", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c7", "name": "Xingzhang Ren", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c8", "name": "Dayiheng Liu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c9", "name": "Linfeng Zhang", "hidden": false}], "publishedAt": "2026-02-05T07:34:23.000Z", "submittedOnDailyAt": "2026-02-11T02:09:03.945Z", "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "submittedOnDailyBy": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "isPro": false, "fullname": "Xuan Ouyang", "user": "YoungXuan", "type": "user"}, "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "upvotes": 279, "discussionId": "698b396b1b2dc6b37d61b4ca", "ai_summary": "OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.", "ai_keywords": ["data selection", "optimizer-induced update space", "effective updates", "stable in-distribution proxy", "Ghost technique", "CountSketch", "Boltzmann sampling", "pre-training", "GPT-2", "Qwen3-8B-Base", "FineWeb", "FineWeb-Edu", "SciencePedia"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-05T02:34:23.000Z", "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05400.png", "numComments": 2, "submittedBy": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "fullname": "Xuan Ouyang", "name": "YoungXuan", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 23, "isUserFollowing": false}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.10388", "authors": [{"_id": "698d3bd265c0d15a6d16200e", "user": {"_id": "6951c555b519522f565dfd0c", "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg", "isPro": false, "fullname": "ZhongzhiLi", "user": "Zhongzhi1228", "type": "user"}, "name": "Zhongzhi Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:57:05.580Z", "hidden": false}, {"_id": "698d3bd265c0d15a6d16200f", "name": "Xuansheng Wu", "hidden": false}, {"_id": "698d3bd265c0d15a6d162010", "name": "Yijiang Li", "hidden": false}, {"_id": "698d3bd265c0d15a6d162011", "name": "Lijie Hu", "hidden": false}, {"_id": "698d3bd265c0d15a6d162012", "name": "Ninghao Liu", "hidden": false}], "publishedAt": "2026-02-11T00:23:13.000Z", "submittedOnDailyAt": "2026-02-16T02:31:34.708Z", "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs", "submittedOnDailyBy": {"_id": "6951c555b519522f565dfd0c", "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg", "isPro": false, "fullname": "ZhongzhiLi", "user": "Zhongzhi1228", "type": "user"}, "summary": "The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.", "upvotes": 200, "discussionId": "698d3bd265c0d15a6d162013", "projectPage": "https://website-sigma-three-35.vercel.app/", "githubRepo": "https://github.com/Zhongzhi660/FAC-Synthesis", "githubRepoAddedBy": "user", "ai_summary": "Feature Activation Coverage measures data diversity in an interpretable feature space and enables diversity-driven data synthesis that improves downstream performance across multiple language model architectures.", "ai_keywords": ["Feature Activation Coverage", "sparse autoencoder", "data diversity", "downstream performance", "instruction following", "toxicity detection", "reward modeling", "behavior steering", "cross-model knowledge transfer", "data-centric optimization"], "githubStars": 52, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-10T19:23:13.000Z", "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs", "summary": "The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10388.png", "numComments": 2, "submittedBy": {"_id": "6951c555b519522f565dfd0c", "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg", "fullname": "ZhongzhiLi", "name": "Zhongzhi1228", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.04705", "authors": [{"_id": "698424a7e34659da7e1f4e6f", "name": "Haifeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e70", "name": "Hua Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e71", "name": "Tian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e72", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4e73", "name": "Jing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e74", "name": "Dianhai Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e75", "name": "Yanjun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4e76", "name": "Jingzhou He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e77", "name": "Zhongjun He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e78", "name": "Dou Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e79", "name": "Qiwen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7a", "name": "Shuohuan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7b", "user": {"_id": "62cd9632342b1d5dab8df4c3", "avatarUrl": "/avatars/9080d20bb57a05a1eeb6800eba886cf9.svg", "isPro": false, "fullname": "Junyuan Shang", "user": "sjy1203", "type": "user"}, "name": "Junyuan Shang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:28.482Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7c", "user": {"_id": "67f37f78b36e82d366dedeec", "avatarUrl": "/avatars/678bb5891d5c2e80edc0799d2308a5d3.svg", "isPro": false, "fullname": "Max Zhenyu Zhang", "user": "max-zhenyu-zhang", "type": "user"}, "name": "Zhenyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:03.972Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7d", "name": "Yuchen Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7e", "name": "Jinle Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7f", "name": "Jiabin Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e80", "name": "Liang Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e81", "name": "Ruibiao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e82", "name": "Weichong Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4e83", "name": "Siyu Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e84", "name": "Dai Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4e85", "name": "Shikun Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e86", "name": "Siqi Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e87", "name": "Bolei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e88", "name": "Yan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e89", "name": "Zhenyu Jiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8a", "name": "Ruiqing Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8b", "name": "Zeyu Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8c", "name": "Qingqing Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8d", "name": "Kaipeng Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8e", "name": "Jiajun Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8f", "name": "Enlei Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e90", "name": "Guoxia Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e91", "name": "Yanlin Sha", "hidden": false}, {"_id": "698424a7e34659da7e1f4e92", "name": "Yi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e93", "name": "Yehan Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e94", "name": "Weijian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e95", "name": "Jiaxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e96", "name": "Zengfeng Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e97", "name": "Yingqi Qu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e98", "name": "Zhongli Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4e99", "name": "Zhengkun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9a", "name": "Xiyang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9b", "name": "Zixiang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9c", "name": "Xinchao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9d", "name": "Zhengjie Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9e", "name": "Dong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9f", "name": "Bingjin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea0", "name": "Yue Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea1", "name": "Xing Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea2", "name": "Shiwei Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea3", "name": "Qiao Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea4", "name": "Xinzhe Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea5", "name": "Shuangshuang Qiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea6", "name": "Baoshan Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea7", "name": "Bihong Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea8", "name": "Bin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea9", "name": "Bingquan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaa", "name": "Binhan Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eab", "name": "Binxiong Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4eac", "name": "Bo Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4ead", "name": "Bo Ke", "hidden": false}, {"_id": "698424a7e34659da7e1f4eae", "name": "Bo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaf", "name": "Bowen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb0", "name": "Boyan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb1", "name": "Boyang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb2", "name": "Caiji Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb3", "name": "Can Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb4", "name": "Chang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb5", "name": "Chao Pang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb6", "name": "Chao Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb7", "name": "Chaoyi Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb8", "name": "Chen Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb9", "name": "Cheng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4eba", "name": "Chenlin Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebb", "name": "Chun Gan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebc", "name": "Chunguang Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebd", "name": "Chuyu Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebe", "name": "Cuiyun Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebf", "name": "Dan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec0", "name": "Danlei Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec1", "name": "Danxiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec2", "name": "Dong Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec3", "name": "Dongbo Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec4", "name": "Dongdong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec5", "name": "Dongdong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec6", "name": "Dongxue Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec7", "name": "Fan Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec8", "name": "Fan Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec9", "name": "Fan Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eca", "name": "Fan Mo", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecb", "name": "Feisheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecc", "name": "Fengwei Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecd", "name": "Gangqiang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ece", "name": "Gaofeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecf", "name": "Gaopeng Yong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed0", "name": "Gexiao Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed1", "user": {"_id": "698419de94015f1e5eedacec", "avatarUrl": "/avatars/e80baa6f9efcd5e5d7cc9b93ac852c7b.svg", "isPro": false, "fullname": "Guan Wang", "user": "guanwcn", "type": "user"}, "name": "Guan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:38.213Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed2", "name": "Guangchen Ni", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed3", "name": "Guangshuo Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed4", "name": "Guanzhong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed5", "user": {"_id": "609cd5ab335f23cd2fa0f211", "avatarUrl": "/avatars/8331a7025a6aa4eabc5b6502bf8a0a63.svg", "isPro": false, "fullname": "Guihua Liu", "user": "LLLL", "type": "user"}, "name": "Guihua Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:31.029Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed6", "name": "Guishun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed7", "name": "Haibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed8", "name": "Haijian Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed9", "name": "Haipeng Ming", "hidden": false}, {"_id": "698424a7e34659da7e1f4eda", "name": "Haisu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4edb", "name": "Haiyang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4edc", "name": "Haiye Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4edd", "name": "Han Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ede", "name": "Hangting Lou", "hidden": false}, {"_id": "698424a7e34659da7e1f4edf", "name": "Hanwen Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee0", "name": "Hanzhi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee1", "name": "Hao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee2", "name": "Hao Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee3", "name": "Hao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee4", "name": "Hao Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee5", "name": "Haochen Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee6", "name": "Haodong Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee7", "name": "Haoshuang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee8", "name": "Haozhe Geng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee9", "name": "Heju Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4eea", "name": "Hong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eeb", "name": "Hongchen Xue", "hidden": false}, {"_id": "698424a7e34659da7e1f4eec", "name": "Hongen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eed", "name": "Honggeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eee", "name": "Hongji Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eef", "name": "Hongwei Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef0", "name": "Hongyang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef1", "name": "Hongyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef2", "name": "Hua Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef3", "name": "Huan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef4", "name": "Huan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef5", "name": "Huang He", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef6", "name": "Hui Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef7", "name": "Hui Zhong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef8", "name": "Huibin Ruan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef9", "name": "Jiafeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efa", "name": "Jiage Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efb", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efc", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efd", "name": "Jiajie Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efe", "name": "Jialin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eff", "name": "Jian Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f00", "name": "Jian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f01", "name": "Jianfeng Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f02", "name": "Jianguang Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f03", "name": "Jianhua Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f04", "name": "Jianye Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f05", "name": "Jiaodi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f06", "name": "Jiarui Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f07", "name": "Jiawei Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f08", "name": "Jiaxin Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f09", "name": "Jiaxuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0a", "name": "Jie Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0b", "name": "Jie Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0c", "name": "Jiefan Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0d", "name": "Jihan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0e", "name": "Jihua Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0f", "name": "Jing Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f10", "name": "Jing Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f11", "name": "Jing Yan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f12", "name": "Jingdong Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f13", "name": "Jingdong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f14", "name": "Jingjing Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f15", "name": "Jingyong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f16", "name": "Jinheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f17", "name": "Jinjin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f18", "name": "Jinliang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f19", "name": "Jinlin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1a", "name": "Jinnan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1b", "name": "Jixiang Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1c", "name": "Jiyi Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1d", "name": "Jiyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1e", "name": "Jun Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1f", "name": "Jun Xia", "hidden": false}, {"_id": "698424a7e34659da7e1f4f20", "name": "Jun Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f21", "name": "Junda Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f22", "name": "Junhao Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f23", "name": "Junhong Xiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f24", "name": "Junliang Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f25", "name": "Kai Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f26", "name": "Kailun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f27", "name": "Kairan Su", "hidden": false}, {"_id": "698424a7e34659da7e1f4f28", "name": "Kang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f29", "name": "Kangkang Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2a", "name": "Ke Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2b", "name": "Ke Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2c", "name": "Kui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2d", "name": "Kun Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2e", "name": "Kunbin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2f", "name": "Lei Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f30", "name": "Lei Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f31", "name": "Lei Wen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f32", "name": "Linghui Meng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f33", "user": {"_id": "641e69355c348064a8251471", "avatarUrl": "/avatars/acad3877df27ff44ea3921bb43e34d53.svg", "isPro": false, "fullname": "Linhao Yu", "user": "HasuerYu", "type": "user"}, "name": "Linhao Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:47.812Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f34", "name": "Liping Ouyang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f35", "name": "Liwen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f36", "user": {"_id": "65cf859f88d13d8128bb8545", "avatarUrl": "/avatars/aa18b993bd90d9c8a95913050cd955a8.svg", "isPro": false, "fullname": "Longbin Ji", "user": "robingg1", "type": "user"}, "name": "Longbin Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:40.295Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f37", "name": "Longzhi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f38", "name": "Meng Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f39", "name": "Meng Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3a", "name": "Mengfei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3b", "name": "Mengqi Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3c", "name": "Mengyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3d", "name": "Ming Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3e", "name": "Mingcheng Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3f", "name": "Mingming Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f40", "name": "Mingxin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f41", "name": "Mingzhu Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f42", "name": "Naibin Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f43", "name": "Nemin Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f44", "name": "Nian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f45", "name": "Peng Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f46", "name": "Peng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f47", "name": "Pengyu Zou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f48", "name": "Qi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f49", "name": "Qi Xin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4a", "name": "Qian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4b", "name": "Qiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4c", "name": "Qianhui Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4d", "name": "Qianwei Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4e", "name": "Qianyue He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4f", "name": "Qifei Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f50", "name": "Qinrui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f51", "name": "Qiwen Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f52", "name": "Quan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f53", "name": "Quanxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f54", "name": "Qunyi Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4f55", "name": "Rongrui Zhan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f56", "name": "Rufeng Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f57", "name": "Rui Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f58", "name": "Ruian Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f59", "name": "Ruihao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5a", "name": "Ruijie Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5b", "name": "Ruixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5c", "name": "Ruixuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5d", "name": "Runsheng Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5e", "name": "Ruting Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5f", "name": "Senbo Kang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f60", "name": "Shan Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f61", "name": "Shaofei Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f62", "name": "Shaotian Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f63", "name": "Shenwei Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f64", "name": "Shifeng Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f65", "name": "Shihao Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f66", "name": "Shilong Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f67", "name": "Shiqin Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f68", "name": "Shiwei Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f69", "name": "Shixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6a", "name": "Shuai Yao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6b", "name": "Shuang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6c", "name": "Shuangqiao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6d", "name": "Shuhao Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6e", "name": "Shuwei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6f", "name": "Shuwen Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f70", "user": {"_id": "62769a608483d8e9ecd9b4f8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672799958233-62769a608483d8e9ecd9b4f8.jpeg", "isPro": false, "fullname": "Sijun He", "user": "sijunhe", "type": "user"}, "name": "Sijun He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:33.392Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f71", "user": {"_id": "64fada13d82fc6977d5e9c74", "avatarUrl": "/avatars/776bf1257154289e919716637770ef52.svg", "isPro": false, "fullname": "Siming Dai", "user": "DesmonDay", "type": "user"}, "name": "Siming Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:50.302Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f72", "name": "Siming Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f73", "name": "Siyi Long", "hidden": false}, {"_id": "698424a7e34659da7e1f4f74", "name": "Songhe Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f75", "name": "Suhui Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f76", "name": "Suyin Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f77", "name": "Teng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f78", "name": "Tianchan Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f79", "name": "Tianliang Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7a", "user": {"_id": "67bbe929593452cc18877606", "avatarUrl": "/avatars/f50fd1cb35d628c26cf21ad0c95c55b1.svg", "isPro": false, "fullname": "tmyangcs", "user": "youngtimmy", "type": "user"}, "name": "Tianmeng Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:36.143Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7b", "name": "Tianyi Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7c", "name": "Tiezhu Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7d", "name": "Ting Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7e", "name": "Ting Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7f", "name": "Tingdan Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f80", "name": "Wei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f81", "name": "Wei Luan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f82", "name": "Wei Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f83", "name": "Wei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f84", "name": "Wei Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f85", "name": "Weibao Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f86", "name": "Weibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f87", "name": "Weicheng Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f88", "name": "Weichong Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f89", "name": "Weiguo Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8a", "name": "Weilong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8b", "name": "Weiqi Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8c", "name": "Wen Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8d", "name": "Wenbin Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8e", "name": "Wenjing Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8f", "name": "Wenlong Miao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f90", "name": "Wenpei Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f91", "name": "Wenquan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f92", "name": "Xi Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f93", "name": "Xi Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f94", "name": "Xiang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f95", "name": "Xiangguo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f96", "name": "Xiangrui Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f97", "name": "Xiangsen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f98", "name": "Xiangzhe Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f99", "name": "Xianlong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9a", "name": "Xianying Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9b", "name": "Xiao Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9c", "name": "Xiaocong Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9d", "name": "Xiaofei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9e", "name": "Xiaofeng Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9f", "name": "Xiaofeng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa0", "name": "Xiaojian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa1", "name": "Xiaolan Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa2", "name": "Xiaopeng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa3", "name": "Xiaotian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa4", "name": "Xiaoxiong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa5", "name": "Xiaoxu Fei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa6", "name": "Xiaoxuan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa7", "user": {"_id": "664395621b88258a527cd7d1", "avatarUrl": "/avatars/8489ccebe4fd1262679ba63a5cb50bb8.svg", "isPro": false, "fullname": "Kira", "user": "Kira-wang", "type": "user"}, "name": "Xiaoyu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:25.774Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa8", "name": "Xiaoyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa9", "name": "Xin Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4faa", "name": "Xin Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fab", "name": "Xinhui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fac", "name": "Xinming Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fad", "name": "Xintong Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fae", "name": "Xinyi Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4faf", "name": "Xinyu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb0", "name": "Xiuxian Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb1", "name": "XuanShi Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb2", "name": "Xue Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb3", "name": "Xueying Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb4", "name": "Xuhong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb5", "name": "Xulong Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb6", "name": "Xuyi Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb7", "name": "Yabing Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb8", "name": "Yafeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb9", "name": "Yamei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fba", "name": "Yan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbb", "name": "Yanfu Cheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbc", "name": "Yang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbd", "name": "Yang Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbe", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbf", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc0", "name": "Yang Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc1", "name": "Yanlong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc2", "name": "Yannian Fu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc3", "name": "Yanpeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc4", "name": "Yanzheng Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc5", "name": "Yao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc6", "name": "Yaozong Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc7", "name": "Yaqian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc8", "name": "Yehua Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc9", "name": "Yekun Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fca", "name": "Yesong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcb", "name": "Yi Song", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcc", "name": "Yichen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcd", "name": "Yifei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fce", "name": "Yifeng Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcf", "name": "Yifeng Kou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd0", "name": "Yilong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd1", "name": "Yilong Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd2", "name": "Yiming Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd3", "name": "Ying Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd4", "name": "Ying Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd5", "name": "Yingsheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd6", "name": "Yingzhan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd7", "name": "Yinqi Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd8", "name": "Yiran Xing", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd9", "name": "Yishu Lei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fda", "name": "Yixiang Tu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdb", "name": "Yiyan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdc", "name": "Yong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdd", "name": "Yonghua Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fde", "name": "Yongqiang Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdf", "name": "Yongxing Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe0", "name": "Yongyue Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe1", "name": "Yu Ran", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe2", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe3", "name": "Yu-Wen Michael Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe4", "name": "Yuang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe5", "name": "Yuanle Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe6", "name": "Yuanyuan Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe7", "name": "Yubo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe8", "name": "Yuchen Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe9", "name": "Yucheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fea", "name": "Yude Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4feb", "name": "Yuedong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fec", "name": "Yuehu Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4fed", "name": "Yufeng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fee", "name": "Yuhui Cao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fef", "name": "Yuhui Yun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff0", "name": "Yukun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff1", "name": "Yukun Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff2", "name": "Yukun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff3", "name": "Yumeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff4", "name": "Yun Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff5", "name": "Yun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff6", "name": "Yunfei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff7", "name": "Yunshen Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff8", "name": "Yuping Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff9", "name": "Yuqin Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffa", "name": "Yuqing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffb", "name": "Yurui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffc", "name": "Yuwen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffd", "name": "Yuxiang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffe", "name": "Zefeng Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fff", "name": "Zelin Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5000", "name": "Zelun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5001", "name": "Zenan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f5002", "name": "Zezhao Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5003", "name": "Zhaowu Pan", "hidden": false}, {"_id": "698424a7e34659da7e1f5004", "name": "Zhaoyu Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f5005", "name": "Zhe Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5006", "name": "Zhe Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5007", "name": "Zhen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5008", "name": "Zhengfan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f5009", "name": "Zhengrui Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f500a", "name": "Zhengsheng Ning", "hidden": false}, {"_id": "698424a7e34659da7e1f500b", "name": "Zhenxing Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500c", "name": "Zhenyu Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500d", "name": "Zhenyu Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f500e", "name": "Zhenyun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500f", "name": "Zhi Li", "hidden": false}, {"_id": "698424a7e34659da7e1f5010", "name": "Zhichao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5011", "name": "Zhicheng Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5012", "name": "Zhida Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5013", "name": "Zhifan Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5014", "name": "Zhihao Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f5015", "name": "Zhijin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f5016", "name": "Zhiyang Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5017", "name": "Zhonghui Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f5018", "name": "Zhuangzhuang Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f5019", "name": "Zhujun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f501a", "name": "Zhuo Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f501b", "name": "Zichang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f501c", "name": "Zihan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f501d", "name": "Zihao Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f501e", "name": "Zihe Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f501f", "name": "Ziheng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5020", "name": "Ziping Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5021", "name": "Zixuan Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f5022", "name": "Ziyang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f5023", "name": "Ziyi Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f5024", "name": "Ziyuan Gao", "hidden": false}], "publishedAt": "2026-02-04T16:18:15.000Z", "submittedOnDailyAt": "2026-02-05T02:34:05.150Z", "title": "ERNIE 5.0 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "upvotes": 198, "discussionId": "698424a7e34659da7e1f5025", "ai_summary": "ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.", "ai_keywords": ["autoregressive foundation model", "unified multimodal understanding", "unified next-group-of-tokens prediction objective", "mixture-of-experts", "modality-agnostic expert routing", "elastic training paradigm", "reinforcement learning", "sparse MoE architecture"], "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-04T11:18:15.000Z", "title": "ERNIE 5.0 Technical Report", "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04705.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 228, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.00919", "authors": [{"_id": "698186fdce18b1862809633b", "name": "I. Apanasevich", "hidden": false}, {"_id": "698186fdce18b1862809633c", "user": {"_id": "6718963e41abf87204dddaf5", "avatarUrl": "/avatars/05d4fdb330ccb52c53cb8f99f7497ab2.svg", "isPro": false, "fullname": "Mikhail Artemyev", "user": "Mixanik-43", "type": "user"}, "name": "M. Artemyev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:34.142Z", "hidden": false}, {"_id": "698186fdce18b1862809633d", "name": "R. Babakyan", "hidden": false}, {"_id": "698186fdce18b1862809633e", "user": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "name": "P. Fedotova", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:41.710Z", "hidden": false}, {"_id": "698186fdce18b1862809633f", "name": "D. Grankin", "hidden": false}, {"_id": "698186fdce18b18628096340", "name": "E. Kupryashin", "hidden": false}, {"_id": "698186fdce18b18628096341", "user": {"_id": "662ace3c4f711ee4e1dcb790", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/R5dlha7Lpy5gCYFEAtr1L.jpeg", "isPro": false, "fullname": "Anastas Misailidi", "user": "kazzart", "type": "user"}, "name": "A. Misailidi", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:18.667Z", "hidden": false}, {"_id": "698186fdce18b18628096342", "user": {"_id": "66eb27551a537888d2121ddc", "avatarUrl": "/avatars/9c807b058c972c307a24d85efbfbd4ae.svg", "isPro": false, "fullname": "Daniil", "user": "Defgy", "type": "user"}, "name": "D. Nerus", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:33.667Z", "hidden": false}, {"_id": "698186fdce18b18628096343", "user": {"_id": "65e5e3df92de33440675b5d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e5e3df92de33440675b5d9/UOVd40f_Htd5oMAa_L0cM.jpeg", "isPro": false, "fullname": "Alexander Nutalapati", "user": "AlexanderNutalapati", "type": "user"}, "name": "A. Nutalapati", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:20.959Z", "hidden": false}, {"_id": "698186fdce18b18628096344", "user": {"_id": "66b51b3ad4eea6ad6adfd611", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b51b3ad4eea6ad6adfd611/SC_01wvlLjB0FFZdDVgAp.jpeg", "isPro": false, "fullname": "Gena Sidorov", "user": "haksorus", "type": "user"}, "name": "G. Sidorov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:41.584Z", "hidden": false}, {"_id": "698186fdce18b18628096345", "user": {"_id": "631ee99d2225f12fc0ef39f4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662970571579-631ee99d2225f12fc0ef39f4.jpeg", "isPro": false, "fullname": "Ivan Efremov", "user": "4ku", "type": "user"}, "name": "I. Efremov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:36.696Z", "hidden": false}, {"_id": "698186fdce18b18628096346", "name": "M. Gerasyov", "hidden": false}, {"_id": "698186fdce18b18628096347", "name": "D. Pikurov", "hidden": false}, {"_id": "698186fdce18b18628096348", "name": "Y. Senchenko", "hidden": false}, {"_id": "698186fdce18b18628096349", "user": {"_id": "68113993ebc57966794e23d6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Yc3GIqYZyO97lzZ9rX8OE.png", "isPro": false, "fullname": "Sergei Davidenko", "user": "Ant346", "type": "user"}, "name": "S. Davidenko", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:21.332Z", "hidden": false}, {"_id": "698186fdce18b1862809634a", "user": {"_id": "6981bbf47f758a03b9c46550", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/oTPe_MzIrDlCeRDvWWeLK.png", "isPro": false, "fullname": "Daniil Kulikov", "user": "KulikovDR", "type": "user"}, "name": "D. Kulikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:46.458Z", "hidden": false}, {"_id": "698186fdce18b1862809634b", "name": "M. Sultankin", "hidden": false}, {"_id": "698186fdce18b1862809634c", "user": {"_id": "63518aa5a30fc3ba88ce51dd", "avatarUrl": "/avatars/2e6a8f4a3e76fcc1afe7e777d6b45e76.svg", "isPro": false, "fullname": "Kazybek A", "user": "wanjia", "type": "user"}, "name": "K. Askarbek", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:24.567Z", "hidden": false}, {"_id": "698186fdce18b1862809634d", "name": "O. Shamanin", "hidden": false}, {"_id": "698186fdce18b1862809634e", "name": "D. Statovoy", "hidden": false}, {"_id": "698186fdce18b1862809634f", "user": {"_id": "655f32a519fd101f14bf1fb0", "avatarUrl": "/avatars/adf2c494759ebe5a0d95c15631ac6312.svg", "isPro": false, "fullname": "Eduard", "user": "rjomba3000", "type": "user"}, "name": "E. Zalyaev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:43.976Z", "hidden": false}, {"_id": "698186fdce18b18628096350", "user": {"_id": "67dd1714817478ae84b18981", "avatarUrl": "/avatars/1209da3d4c4de3f419ebea6845bb0ed6.svg", "isPro": false, "fullname": "Zorin Ilya", "user": "Zora244", "type": "user"}, "name": "I. Zorin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:31.637Z", "hidden": false}, {"_id": "698186fdce18b18628096351", "name": "A. Letkin", "hidden": false}, {"_id": "698186fdce18b18628096352", "name": "E. Rusakov", "hidden": false}, {"_id": "698186fdce18b18628096353", "name": "A. Silchenko", "hidden": false}, {"_id": "698186fdce18b18628096354", "user": {"_id": "6981a821165e30591e1200e7", "avatarUrl": "/avatars/af72142b8ba8772926b247c31fc8e4c8.svg", "isPro": false, "fullname": "Vlad Vorobyov", "user": "GloomARK", "type": "user"}, "name": "V. Vorobyov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:18.218Z", "hidden": false}, {"_id": "698186fdce18b18628096355", "user": {"_id": "6901ce2d911da714e754422b", "avatarUrl": "/avatars/5ed8ce189ca92a04f7165751076ff446.svg", "isPro": false, "fullname": "SERGEI", "user": "sobolnikov", "type": "user"}, "name": "S. Sobolnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:28.887Z", "hidden": false}, {"_id": "698186fdce18b18628096356", "user": {"_id": "640e2ef88512ec51d7f34cd5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640e2ef88512ec51d7f34cd5/Xl8UiprL-0SvOWHeoAFW1.jpeg", "isPro": false, "fullname": "Aleksey Postnikov", "user": "AlekseyPostnikov", "type": "user"}, "name": "A. Postnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:39.139Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "publishedAt": "2026-01-31T22:13:23.000Z", "submittedOnDailyAt": "2026-02-03T03:13:09.153Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "submittedOnDailyBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "upvotes": 173, "discussionId": "698186fece18b18628096357", "projectPage": "https://greenvla.github.io", "githubRepo": "https://github.com/greenvla/GreenVLA", "githubRepoAddedBy": "user", "ai_summary": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.", "ai_keywords": ["Vision-Language-Action", "multimodal grounding", "multi-embodiment pretraining", "embodiment-specific adaptation", "reinforcement-learning", "episode-progress prediction", "out-of-distribution detection", "joint-prediction-based guidance"], "githubStars": 24, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-01-31T17:13:23.000Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00919.png", "numComments": 1, "submittedBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "fullname": "Polina Fedotova", "name": "2pd", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.09877", "authors": [{"_id": "698c7abdeb12ea7453916869", "user": {"_id": "674006451d2302f6aa9b026d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png", "isPro": false, "fullname": "Chenxu Wang", "user": "xunyoyo", "type": "user"}, "name": "Chenxu Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-12T16:49:45.534Z", "hidden": false}, {"_id": "698c7abdeb12ea745391686a", "name": "Chaozhuo Li", "hidden": false}, {"_id": "698c7abdeb12ea745391686b", "name": "Songyang Liu", "hidden": false}, {"_id": "698c7abdeb12ea745391686c", "name": "Zejian Chen", "hidden": false}, {"_id": "698c7abdeb12ea745391686d", "name": "Jinyu Hou", "hidden": false}, {"_id": "698c7abdeb12ea745391686e", "name": "Ji Qi", "hidden": false}, {"_id": "698c7abdeb12ea745391686f", "name": "Rui Li", "hidden": false}, {"_id": "698c7abdeb12ea7453916870", "name": "Litian Zhang", "hidden": false}, {"_id": "698c7abdeb12ea7453916871", "name": "Qiwei Ye", "hidden": false}, {"_id": "698c7abdeb12ea7453916872", "name": "Zheng Liu", "hidden": false}, {"_id": "698c7abdeb12ea7453916873", "name": "Xu Chen", "hidden": false}, {"_id": "698c7abdeb12ea7453916874", "name": "Xi Zhang", "hidden": false}, {"_id": "698c7abdeb12ea7453916875", "name": "Philip S. Yu", "hidden": false}], "publishedAt": "2026-02-10T15:18:19.000Z", "submittedOnDailyAt": "2026-02-13T00:53:30.377Z", "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies", "submittedOnDailyBy": {"_id": "674006451d2302f6aa9b026d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png", "isPro": false, "fullname": "Chenxu Wang", "user": "xunyoyo", "type": "user"}, "summary": "The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.", "upvotes": 169, "discussionId": "698c7abdeb12ea7453916876", "ai_summary": "Multi-agent LLM systems face fundamental limitations in achieving continuous self-improvement while maintaining safety alignment due to inherent statistical blind spots in isolated evolution.", "ai_keywords": ["multi-agent systems", "large language models", "self-evolution", "safety alignment", "information-theoretic framework", "anthropic value distributions", "statistical blind spots", "self-evolving AI societies", "external oversight", "safety-preserving mechanisms"], "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-10T10:18:19.000Z", "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies", "summary": "The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09877.png", "numComments": 2, "submittedBy": {"_id": "674006451d2302f6aa9b026d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png", "fullname": "Chenxu Wang", "name": "xunyoyo", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.09856", "authors": [{"_id": "698bf5b66052d3bed9630aa7", "user": {"_id": "64107c7df52d7eb22e062956", "avatarUrl": "/avatars/7b1cee9a2b8454fedfbd4c3d1df9865c.svg", "isPro": false, "fullname": "Yuhao Zheng", "user": "yhzheng1031", "type": "user"}, "name": "Yuhao Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:28.241Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aa8", "name": "Li'an Zhong", "hidden": false}, {"_id": "698bf5b66052d3bed9630aa9", "user": {"_id": "6773bcaa675a971ddf1e81dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a8VUwZYXd7O_mq_zFvXMh.png", "isPro": false, "fullname": "CokeWang", "user": "CokeWang", "type": "user"}, "name": "Yi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:30.778Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aaa", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:25.982Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aab", "name": "Kaikui Liu", "hidden": false}, {"_id": "698bf5b66052d3bed9630aac", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "698bf5b66052d3bed9630aad", "name": "Linyuan Lv", "hidden": false}, {"_id": "698bf5b66052d3bed9630aae", "name": "Philip Torr", "hidden": false}, {"_id": "698bf5b66052d3bed9630aaf", "user": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "isPro": false, "fullname": "Qinghong (Kevin) Lin", "user": "KevinQHLin", "type": "user"}, "name": "Kevin Qinghong Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:23.397Z", "hidden": false}], "publishedAt": "2026-02-10T14:56:19.000Z", "submittedOnDailyAt": "2026-02-11T01:02:42.385Z", "title": "Code2World: A GUI World Model via Renderable Code Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "upvotes": 168, "discussionId": "698bf5b66052d3bed9630ab0", "projectPage": "https://amap-ml.github.io/Code2World/", "githubRepo": "https://github.com/AMAP-ML/Code2World", "githubRepoAddedBy": "user", "ai_summary": "Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.", "ai_keywords": ["vision-language coder", "GUI World model", "action-conditioned prediction", "AndroidCode", "HTML generation", "visual-feedback revision mechanism", "SFT", "Render-Aware Reinforcement Learning", "visual semantic fidelity", "action consistency", "next UI prediction", "AndroidWorld navigation"], "githubStars": 131, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-10T09:56:19.000Z", "title": "Code2World: A GUI World Model via Renderable Code Generation", "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09856.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 230, "isUserFollowing": false}, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.10693", "authors": [{"_id": "6992047b50fb2c0be47837f0", "user": {"_id": "6475ff9b4c9fb8a4bf1cde76", "avatarUrl": "/avatars/61cf82cd0e15c4618f5bd8b1f7d52f37.svg", "isPro": false, "fullname": "floyed shen", "user": "floyed", "type": "user"}, "name": "Guobin Shen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-17T15:52:51.206Z", "hidden": false}, {"_id": "6992047b50fb2c0be47837f1", "user": {"_id": "63fc5b724c57549ad5e54558", "avatarUrl": "/avatars/1374c1e8969533dd7543959666f16d1a.svg", "isPro": false, "fullname": "Chenxiao Zhao", "user": "ChenShawn", "type": "user"}, "name": "Chenxiao Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-02-17T17:17:12.583Z", "hidden": false}, {"_id": "6992047b50fb2c0be47837f2", "user": {"_id": "655c43d6b426ec8f4b5e7652", "avatarUrl": "/avatars/ddcf9d1ef0e2dc1f564a56ba9153f24f.svg", "isPro": false, "fullname": "Xiang Cheng", "user": "FFFc2", "type": "user"}, "name": "Xiang Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-17T15:52:57.697Z", "hidden": false}, {"_id": "6992047b50fb2c0be47837f3", "user": {"_id": "61f4c2e981c4d30f58140279", "avatarUrl": "/avatars/c4a69f6563c952354e33682e86045b14.svg", "isPro": false, "fullname": "HuangMeow", "user": "Luckyyy", "type": "user"}, "name": "Lei Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-23T09:46:50.954Z", "hidden": false}, {"_id": "6992047b50fb2c0be47837f4", "name": "Xing Yu", "hidden": false}], "publishedAt": "2026-02-11T09:48:08.000Z", "submittedOnDailyAt": "2026-02-23T03:29:14.259Z", "title": "VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training", "submittedOnDailyBy": {"_id": "6475ff9b4c9fb8a4bf1cde76", "avatarUrl": "/avatars/61cf82cd0e15c4618f5bd8b1f7d52f37.svg", "isPro": false, "fullname": "floyed shen", "user": "floyed", "type": "user"}, "summary": "Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO", "upvotes": 158, "discussionId": "6992047c50fb2c0be47837f5", "githubRepo": "https://github.com/FloyedShen/VESPO", "githubRepoAddedBy": "user", "ai_summary": "VESPO addresses training instability in LLM reinforcement learning by using variational formulation with variance reduction to correct policy divergence without length normalization.", "ai_keywords": ["reinforcement learning", "large language models", "policy staleness", "asynchronous training", "importance sampling", "variance reduction", "variational formulation", "proposal distributions", "sequence-level importance weights", "mathematical reasoning benchmarks", "Mixture-of-Experts models"], "githubStars": 14, "organization": {"_id": "68246a0a98117c02df67a547", "name": "rednote-hilab", "fullname": "rednote-hilab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6807a1d6504547b3554b9c73/WgnnQDsz7FqnyTtv8mmRO.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-11T04:48:08.000Z", "title": "VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training", "summary": "Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10693.png", "numComments": 2, "submittedBy": {"_id": "6475ff9b4c9fb8a4bf1cde76", "avatarUrl": "/avatars/61cf82cd0e15c4618f5bd8b1f7d52f37.svg", "fullname": "floyed shen", "name": "floyed", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "68246a0a98117c02df67a547", "name": "rednote-hilab", "fullname": "rednote-hilab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6807a1d6504547b3554b9c73/WgnnQDsz7FqnyTtv8mmRO.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.10604", "authors": [{"_id": "698d417065c0d15a6d162026", "name": "Ailin Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162027", "name": "Ang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162028", "name": "Aobo Kong", "hidden": false}, {"_id": "698d417065c0d15a6d162029", "name": "Bin Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16202a", "name": "Binxing Jiao", "hidden": false}, {"_id": "698d417065c0d15a6d16202b", "name": "Bo Dong", "hidden": false}, {"_id": "698d417065c0d15a6d16202c", "name": "Bojun Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16202d", "name": "Boyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16202e", "name": "Brian Li", "hidden": false}, {"_id": "698d417065c0d15a6d16202f", "name": "Buyun Ma", "hidden": false}, {"_id": "698d417065c0d15a6d162030", "name": "Chang Su", "hidden": false}, {"_id": "698d417065c0d15a6d162031", "name": "Changxin Miao", "hidden": false}, {"_id": "698d417065c0d15a6d162032", "name": "Changyi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162033", "name": "Chao Lou", "hidden": false}, {"_id": "698d417065c0d15a6d162034", "name": "Chen Hu", "hidden": false}, {"_id": "698d417065c0d15a6d162035", "name": "Chen Xu", "hidden": false}, {"_id": "698d417065c0d15a6d162036", "name": "Chenfeng Yu", "hidden": false}, {"_id": "698d417065c0d15a6d162037", "name": "Chengting Feng", "hidden": false}, {"_id": "698d417065c0d15a6d162038", "name": "Chengyuan Yao", "hidden": false}, {"_id": "698d417065c0d15a6d162039", "name": "Chunrui Han", "hidden": false}, {"_id": "698d417065c0d15a6d16203a", "name": "Dan Ma", "hidden": false}, {"_id": "698d417065c0d15a6d16203b", "name": "Dapeng Shi", "hidden": false}, {"_id": "698d417065c0d15a6d16203c", "name": "Daxin Jiang", "hidden": false}, {"_id": "698d417065c0d15a6d16203d", "name": "Dehua Ma", "hidden": false}, {"_id": "698d417065c0d15a6d16203e", "name": "Deshan Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16203f", "name": "Di Qi", "hidden": false}, {"_id": "698d417065c0d15a6d162040", "name": "Enle Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162041", "name": "Fajie Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d162042", "name": "Fanqi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162043", "name": "Guanzhe Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162044", "name": "Gulin Yan", "hidden": false}, {"_id": "698d417065c0d15a6d162045", "name": "Guoliang Cao", "hidden": false}, {"_id": "698d417065c0d15a6d162046", "name": "Guopeng Li", "hidden": false}, {"_id": "698d417065c0d15a6d162047", "name": "Han Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d162048", "name": "Hangyu Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162049", "user": {"_id": "64b7874b9f5987572ca28461", "avatarUrl": "/avatars/d24ee0a6329ff93936aa7829481e2046.svg", "isPro": false, "fullname": "hanshanzhang", "user": "brain-zhang", "type": "user"}, "name": "Hanshan Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:52.602Z", "hidden": false}, {"_id": "698d417065c0d15a6d16204a", "name": "Hao Nie", "hidden": false}, {"_id": "698d417065c0d15a6d16204b", "name": "Haonan Jia", "hidden": false}, {"_id": "698d417065c0d15a6d16204c", "name": "Haoran Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16204d", "name": "Hebin Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16204e", "name": "Hekun Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16204f", "name": "Heng Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162050", "name": "Heung-Yeung Shum", "hidden": false}, {"_id": "698d417065c0d15a6d162051", "name": "Hongbo Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162052", "name": "Hongbo Peng", "hidden": false}, {"_id": "698d417065c0d15a6d162053", "name": "Hongyu Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d162054", "name": "Hongyuan Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162055", "name": "Houyong Chen", "hidden": false}, {"_id": "698d417065c0d15a6d162056", "name": "Huangxi Zhu", "hidden": false}, {"_id": "698d417065c0d15a6d162057", "name": "Huimin Wu", "hidden": false}, {"_id": "698d417065c0d15a6d162058", "name": "Huiyong Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162059", "name": "Jia Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16205a", "name": "Jian Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16205b", "name": "Jianjian Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16205c", "name": "Jiaoren Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16205d", "name": "Jiaran Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d16205e", "name": "Jiashu Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16205f", "name": "Jiashuo Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162060", "name": "Jiayi Fu", "hidden": false}, {"_id": "698d417065c0d15a6d162061", "name": "Jiayu Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162062", "name": "Jie Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d162063", "name": "Jie Luo", "hidden": false}, {"_id": "698d417065c0d15a6d162064", "name": "Jie Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162065", "name": "Jie Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d162066", "name": "Jieyi Hou", "hidden": false}, {"_id": "698d417065c0d15a6d162067", "name": "Jing Bai", "hidden": false}, {"_id": "698d417065c0d15a6d162068", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:28:37.335Z", "hidden": false}, {"_id": "698d417065c0d15a6d162069", "name": "Jingjing Xie", "hidden": false}, {"_id": "698d417065c0d15a6d16206a", "name": "Jingwei Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16206b", "name": "Jingyang Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d16206c", "name": "Jishi Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16206d", "name": "Junfeng Liu", "hidden": false}, {"_id": "698d417065c0d15a6d16206e", "name": "Junzhe Lin", "hidden": false}, {"_id": "698d417065c0d15a6d16206f", "name": "Ka Man Lo", "hidden": false}, {"_id": "698d417065c0d15a6d162070", "name": "Kai Liang", "hidden": false}, {"_id": "698d417065c0d15a6d162071", "name": "Kaibo Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162072", "name": "Kaijun Tan", "hidden": false}, {"_id": "698d417065c0d15a6d162073", "user": {"_id": "66668c591964b6188ee310c2", "avatarUrl": "/avatars/8a8265073dbacbb2c7139b1c8da3e055.svg", "isPro": false, "fullname": "Kaiwen Yan", "user": "linrany", "type": "user"}, "name": "Kaiwen Yan", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:58.524Z", "hidden": false}, {"_id": "698d417065c0d15a6d162074", "name": "Kaixiang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162075", "name": "Kang An", "hidden": false}, {"_id": "698d417065c0d15a6d162076", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:56.339Z", "hidden": false}, {"_id": "698d417065c0d15a6d162077", "name": "Lei Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162078", "name": "Liang Lv", "hidden": false}, {"_id": "698d417065c0d15a6d162079", "name": "Liang Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d16207a", "name": "Liangyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16207b", "name": "Lieyu Shi", "hidden": false}, {"_id": "698d417065c0d15a6d16207c", "name": "Liguo Tan", "hidden": false}, {"_id": "698d417065c0d15a6d16207d", "name": "Lin Lin", "hidden": false}, {"_id": "698d417065c0d15a6d16207e", "name": "Lina Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16207f", "name": "Luck Ma", "hidden": false}, {"_id": "698d417065c0d15a6d162080", "name": "Mengqiang Ren", "hidden": false}, {"_id": "698d417065c0d15a6d162081", "name": "Michael Li", "hidden": false}, {"_id": "698d417065c0d15a6d162082", "name": "Ming Li", "hidden": false}, {"_id": "698d417065c0d15a6d162083", "name": "Mingliang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162084", "name": "Mingming Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d162085", "name": "Mingrui Chen", "hidden": false}, {"_id": "698d417065c0d15a6d162086", "name": "Mitt Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162087", "name": "Na Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162088", "name": "Peng Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162089", "name": "Qi Han", "hidden": false}, {"_id": "698d417065c0d15a6d16208a", "name": "Qian Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d16208b", "name": "Qinglin He", "hidden": false}, {"_id": "698d417065c0d15a6d16208c", "name": "Qinxin Du", "hidden": false}, {"_id": "698d417065c0d15a6d16208d", "name": "Qiuping Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16208e", "name": "Quan Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16208f", "name": "Rongqiu Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162090", "name": "Ruihang Miao", "hidden": false}, {"_id": "698d417065c0d15a6d162091", "name": "Ruixin Han", "hidden": false}, {"_id": "698d417065c0d15a6d162092", "name": "Ruosi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162093", "name": "Ruyan Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162094", "name": "Shan Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162095", "name": "Shaoliang Pang", "hidden": false}, {"_id": "698d417065c0d15a6d162096", "name": "Shaowen Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162097", "name": "Shengjie Fan", "hidden": false}, {"_id": "698d417065c0d15a6d162098", "name": "Shijie Shang", "hidden": false}, {"_id": "698d417065c0d15a6d162099", "name": "Shiliang Yang", "hidden": false}, {"_id": "698d417065c0d15a6d16209a", "name": "Shiwei Li", "hidden": false}, {"_id": "698d417065c0d15a6d16209b", "name": "Shuangshuang Tian", "hidden": false}, {"_id": "698d417065c0d15a6d16209c", "name": "Siqi Liu", "hidden": false}, {"_id": "698d417065c0d15a6d16209d", "name": "Siye Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16209e", "name": "Siyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16209f", "name": "Song Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620a0", "name": "Tiancheng Cao", "hidden": false}, {"_id": "698d417065c0d15a6d1620a1", "name": "Tianchi Yue", "hidden": false}, {"_id": "698d417065c0d15a6d1620a2", "name": "Tianhao Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d1620a3", "name": "Tianning Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620a4", "name": "Tingdan Luo", "hidden": false}, {"_id": "698d417065c0d15a6d1620a5", "name": "Wang You", "hidden": false}, {"_id": "698d417065c0d15a6d1620a6", "name": "Wei Ji", "hidden": false}, {"_id": "698d417065c0d15a6d1620a7", "name": "Wei Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620a8", "name": "Wei Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620a9", "name": "Weibo Wu", "hidden": false}, {"_id": "698d417065c0d15a6d1620aa", "user": {"_id": "6657620ea496f7fcb67c3871", "avatarUrl": "/avatars/54fef1c835e6f6b478652d438a140d45.svg", "isPro": false, "fullname": "xieweihao", "user": "chalengr", "type": "user"}, "name": "Weihao Xie", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:48.216Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620ab", "name": "Wen Sun", "hidden": false}, {"_id": "698d417065c0d15a6d1620ac", "name": "Wenjin Deng", "hidden": false}, {"_id": "698d417065c0d15a6d1620ad", "user": {"_id": "650c04795510464e85b47470", "avatarUrl": "/avatars/98c194e77826b928c49659849f466dad.svg", "isPro": false, "fullname": "wen", "user": "zhengwenzhen", "type": "user"}, "name": "Wenzhen Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:45.930Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620ae", "name": "Wuxun Xie", "hidden": false}, {"_id": "698d417065c0d15a6d1620af", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b0", "name": "Xiangwen Kong", "hidden": false}, {"_id": "698d417065c0d15a6d1620b1", "name": "Xiangyu Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620b2", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b3", "name": "Xiaobo Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b4", "name": "Xiaojia Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620b5", "name": "Xiaolan Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620b6", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "698d417065c0d15a6d1620b7", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "698d417065c0d15a6d1620b8", "name": "Xiaoyun Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b9", "name": "Xin Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620ba", "name": "Xin Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620bb", "name": "Xin Wu", "hidden": false}, {"_id": "698d417065c0d15a6d1620bc", "name": "Xing Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620bd", "name": "Xingping Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620be", "name": "Xinran Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620bf", "name": "Xu Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620c0", "user": {"_id": "64ec5b64bfb2aa06a46ff2d6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Lgl55OtDWa0tzRI2ShpUe.jpeg", "isPro": false, "fullname": "xuan he", "user": "tpa115k31", "type": "user"}, "name": "Xuan He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:36.240Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620c1", "name": "Xuanti Feng", "hidden": false}, {"_id": "698d417065c0d15a6d1620c2", "name": "Xuedan Cai", "hidden": false}, {"_id": "698d417065c0d15a6d1620c3", "name": "Xuqiang Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d1620c4", "name": "Yanbo Yu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c5", "name": "Yang Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620c6", "name": "Yang Xu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c7", "name": "Yanlin Lai", "hidden": false}, {"_id": "698d417065c0d15a6d1620c8", "name": "Yanming Xu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c9", "name": "Yaoyu Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620ca", "name": "Yeqing Shen", "hidden": false}, {"_id": "698d417065c0d15a6d1620cb", "name": "Yibo Zhu", "hidden": false}, {"_id": "698d417065c0d15a6d1620cc", "name": "Yichen Lv", "hidden": false}, {"_id": "698d417065c0d15a6d1620cd", "name": "Yicheng Cao", "hidden": false}, {"_id": "698d417065c0d15a6d1620ce", "name": "Yifeng Gong", "hidden": false}, {"_id": "698d417065c0d15a6d1620cf", "name": "Yijing Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d0", "name": "Yikun Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d1", "name": "Yin Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d2", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d3", "name": "Yinmin Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d4", "name": "Yitong Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d5", "name": "Yixuan Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d6", "name": "Yiyang Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620d7", "name": "Yongchi Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d8", "name": "Yongshen Long", "hidden": false}, {"_id": "698d417065c0d15a6d1620d9", "name": "Yongyao Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620da", "name": "Yousong Guan", "hidden": false}, {"_id": "698d417065c0d15a6d1620db", "name": "Yu Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d1620dc", "name": "Yuang Peng", "hidden": false}, {"_id": "698d417065c0d15a6d1620dd", "name": "Yuanhao Ding", "hidden": false}, {"_id": "698d417065c0d15a6d1620de", "name": "Yuantao Fan", "hidden": false}, {"_id": "698d417065c0d15a6d1620df", "name": "Yuanzhen Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620e0", "name": "Yuchu Luo", "hidden": false}, {"_id": "698d417065c0d15a6d1620e1", "name": "Yudi Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620e2", "name": "Yue Peng", "hidden": false}, {"_id": "698d417065c0d15a6d1620e3", "name": "Yueqiang Lin", "hidden": false}, {"_id": "698d417065c0d15a6d1620e4", "name": "Yufan Lu", "hidden": false}, {"_id": "698d417065c0d15a6d1620e5", "name": "Yuling Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620e6", "name": "Yunzhou Ju", "hidden": false}, {"_id": "698d417065c0d15a6d1620e7", "name": "Yurong Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620e8", "name": "Yusheng Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620e9", "name": "Yuxiang Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620ea", "name": "Yuyang Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620eb", "name": "Yuzhu Cai", "hidden": false}, {"_id": "698d417065c0d15a6d1620ec", "name": "Zejia Weng", "hidden": false}, {"_id": "698d417065c0d15a6d1620ed", "name": "Zetao Hong", "hidden": false}, {"_id": "698d417065c0d15a6d1620ee", "name": "Zexi Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620ef", "name": "Zhe Xie", "hidden": false}, {"_id": "698d417065c0d15a6d1620f0", "name": "Zheng Ge", "hidden": false}, {"_id": "698d417065c0d15a6d1620f1", "name": "Zheng Gong", "hidden": false}, {"_id": "698d417065c0d15a6d1620f2", "name": "Zheng Zeng", "hidden": false}, {"_id": "698d417065c0d15a6d1620f3", "user": {"_id": "63607ace9ddc44e710e13f0f", "avatarUrl": "/avatars/b5f331549562aea4a5c8b681fd9da1ff.svg", "isPro": false, "fullname": "zy", "user": "lu-vae", "type": "user"}, "name": "Zhenyi Lu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:50.532Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620f4", "name": "Zhewei Huang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f5", "name": "Zhichao Chang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f6", "name": "Zhiguo Huang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f7", "name": "Zhiheng Hu", "hidden": false}, {"_id": "698d417065c0d15a6d1620f8", "name": "Zidong Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f9", "name": "Zili Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620fa", "name": "Ziqi Ren", "hidden": false}, {"_id": "698d417065c0d15a6d1620fb", "name": "Zixin Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620fc", "name": "Zixuan Wang", "hidden": false}], "publishedAt": "2026-02-11T07:53:51.000Z", "submittedOnDailyAt": "2026-02-12T00:26:49.880Z", "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.", "upvotes": 150, "discussionId": "698d417165c0d15a6d1620fd", "githubRepo": "https://github.com/stepfun-ai/Step-3.5-Flash", "githubRepoAddedBy": "user", "ai_summary": "Step 3.5 Flash is a sparse Mixture-of-Experts model that achieves frontier-level agentic intelligence through efficient parameter utilization and optimized attention mechanisms, demonstrating strong performance across multiple benchmarks.", "ai_keywords": ["Mixture-of-Experts", "sparse MoE", "foundation model", "active parameters", "interleaved attention", "sliding-window attention", "full attention", "Multi-Token Prediction", "reinforcement learning", "verifiable signals", "preference feedback", "off-policy training", "self-improvement", "IMO-AnswerBench", "LiveCodeBench", "tau2-Bench", "BrowseComp", "Terminal-Bench"], "githubStars": 1245, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-11T02:53:51.000Z", "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters", "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10604.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 231, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.02276", "authors": [{"_id": "69817e2cce18b1862809615b", "name": "Kimi Team", "hidden": false}, {"_id": "69817e2cce18b1862809615c", "name": "Tongtong Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615d", "name": "Yifan Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615e", "name": "Yiping Bao", "hidden": false}, {"_id": "69817e2cce18b1862809615f", "name": "S. H. Cai", "hidden": false}, {"_id": "69817e2cce18b18628096160", "name": "Yuan Cao", "hidden": false}, {"_id": "69817e2cce18b18628096161", "name": "Y. Charles", "hidden": false}, {"_id": "69817e2cce18b18628096162", "name": "H. S. Che", "hidden": false}, {"_id": "69817e2cce18b18628096163", "name": "Cheng Chen", "hidden": false}, {"_id": "69817e2cce18b18628096164", "name": "Guanduo Chen", "hidden": false}, {"_id": "69817e2cce18b18628096165", "name": "Huarong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096166", "name": "Jia Chen", "hidden": false}, {"_id": "69817e2cce18b18628096167", "name": "Jiahao Chen", "hidden": false}, {"_id": "69817e2cce18b18628096168", "name": "Jianlong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096169", "name": "Jun Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616a", "name": "Kefan Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616b", "name": "Liang Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616c", "name": "Ruijue Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616d", "name": "Xinhao Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616e", "name": "Yanru Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616f", "name": "Yanxu Chen", "hidden": false}, {"_id": "69817e2cce18b18628096170", "name": "Yicun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096171", "name": "Yimin Chen", "hidden": false}, {"_id": "69817e2cce18b18628096172", "name": "Yingjiang Chen", "hidden": false}, {"_id": "69817e2cce18b18628096173", "name": "Yuankun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096174", "name": "Yujie Chen", "hidden": false}, {"_id": "69817e2cce18b18628096175", "name": "Yutian Chen", "hidden": false}, {"_id": "69817e2cce18b18628096176", "name": "Zhirong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096177", "name": "Ziwei Chen", "hidden": false}, {"_id": "69817e2cce18b18628096178", "name": "Dazhi Cheng", "hidden": false}, {"_id": "69817e2cce18b18628096179", "name": "Minghan Chu", "hidden": false}, {"_id": "69817e2cce18b1862809617a", "name": "Jialei Cui", "hidden": false}, {"_id": "69817e2cce18b1862809617b", "name": "Jiaqi Deng", "hidden": false}, {"_id": "69817e2cce18b1862809617c", "name": "Muxi Diao", "hidden": false}, {"_id": "69817e2cce18b1862809617d", "name": "Hao Ding", "hidden": false}, {"_id": "69817e2cce18b1862809617e", "name": "Mengfan Dong", "hidden": false}, {"_id": "69817e2cce18b1862809617f", "name": "Mengnan Dong", "hidden": false}, {"_id": "69817e2cce18b18628096180", "name": "Yuxin Dong", "hidden": false}, {"_id": "69817e2cce18b18628096181", "user": {"_id": "652965773a416e1f2173443b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg", "isPro": true, "fullname": "Yuhao Dong", "user": "THUdyh", "type": "user"}, "name": "Yuhao Dong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:04:11.993Z", "hidden": false}, {"_id": "69817e2cce18b18628096182", "name": "Angang Du", "hidden": false}, {"_id": "69817e2cce18b18628096183", "name": "Chenzhuang Du", "hidden": false}, {"_id": "69817e2cce18b18628096184", "name": "Dikang Du", "hidden": false}, {"_id": "69817e2cce18b18628096185", "name": "Lingxiao Du", "hidden": false}, {"_id": "69817e2cce18b18628096186", "user": {"_id": "6340f31fb78ed99eab04ce33", "avatarUrl": "/avatars/2e7fcbf0233bdc0bc9a3f4603fd8bf90.svg", "isPro": false, "fullname": "Du", "user": "Yulun", "type": "user"}, "name": "Yulun Du", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:47.298Z", "hidden": false}, {"_id": "69817e2cce18b18628096187", "name": "Yu Fan", "hidden": false}, {"_id": "69817e2cce18b18628096188", "name": "Shengjun Fang", "hidden": false}, {"_id": "69817e2cce18b18628096189", "name": "Qiulin Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618a", "name": "Yichen Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618b", "name": "Garimugai Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618c", "name": "Kelin Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618d", "name": "Hongcheng Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618e", "name": "Tong Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618f", "name": "Yuyao Ge", "hidden": false}, {"_id": "69817e2cce18b18628096190", "user": {"_id": "650a5d79a0f81fbc0a9875a7", "avatarUrl": "/avatars/a76b1c932964602f2fc4a801ccad3ab5.svg", "isPro": false, "fullname": "ShangyiGeng", "user": "Reset23", "type": "user"}, "name": "Shangyi Geng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:10.446Z", "hidden": false}, {"_id": "69817e2cce18b18628096191", "name": "Chengyang Gong", "hidden": false}, {"_id": "69817e2cce18b18628096192", "name": "Xiaochen Gong", "hidden": false}, {"_id": "69817e2cce18b18628096193", "name": "Zhuoma Gongque", "hidden": false}, {"_id": "69817e2cce18b18628096194", "name": "Qizheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096195", "name": "Xinran Gu", "hidden": false}, {"_id": "69817e2cce18b18628096196", "name": "Yicheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096197", "name": "Longyu Guan", "hidden": false}, {"_id": "69817e2cce18b18628096198", "name": "Yuanying Guo", "hidden": false}, {"_id": "69817e2cce18b18628096199", "name": "Xiaoru Hao", "hidden": false}, {"_id": "69817e2cce18b1862809619a", "name": "Weiran He", "hidden": false}, {"_id": "69817e2cce18b1862809619b", "name": "Wenyang He", "hidden": false}, {"_id": "69817e2cce18b1862809619c", "name": "Yunjia He", "hidden": false}, {"_id": "69817e2cce18b1862809619d", "name": "Chao Hong", "hidden": false}, {"_id": "69817e2cce18b1862809619e", "name": "Hao Hu", "hidden": false}, {"_id": "69817e2cce18b1862809619f", "name": "Jiaxi Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a0", "name": "Yangyang Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a1", "name": "Zhenxing Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a2", "name": "Ke Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a3", "name": "Ruiyuan Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a4", "name": "Weixiao Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a5", "name": "Zhiqi Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a6", "name": "Tao Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a7", "name": "Zhejun Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a8", "name": "Xinyi Jin", "hidden": false}, {"_id": "69817e2cce18b186280961a9", "name": "Yu Jing", "hidden": false}, {"_id": "69817e2cce18b186280961aa", "name": "Guokun Lai", "hidden": false}, {"_id": "69817e2cce18b186280961ab", "name": "Aidi Li", "hidden": false}, {"_id": "69817e2cce18b186280961ac", "name": "C. Li", "hidden": false}, {"_id": "69817e2cce18b186280961ad", "name": "Cheng Li", "hidden": false}, {"_id": "69817e2cce18b186280961ae", "name": "Fang Li", "hidden": false}, {"_id": "69817e2cce18b186280961af", "name": "Guanghe Li", "hidden": false}, {"_id": "69817e2cce18b186280961b0", "name": "Guanyu Li", "hidden": false}, {"_id": "69817e2cce18b186280961b1", "name": "Haitao Li", "hidden": false}, {"_id": "69817e2cce18b186280961b2", "name": "Haoyang Li", "hidden": false}, {"_id": "69817e2cce18b186280961b3", "name": "Jia Li", "hidden": false}, {"_id": "69817e2cce18b186280961b4", "name": "Jingwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961b5", "name": "Junxiong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b6", "name": "Lincan Li", "hidden": false}, {"_id": "69817e2cce18b186280961b7", "user": {"_id": "6576fe2b42ab083faea19841", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/c91ZKOR2E0gL8iIVkvEUa.jpeg", "isPro": false, "fullname": "Mo Li", "user": "Mor-Li", "type": "user"}, "name": "Mo Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:51.899Z", "hidden": false}, {"_id": "69817e2cce18b186280961b8", "name": "Weihong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b9", "name": "Wentao Li", "hidden": false}, {"_id": "69817e2cce18b186280961ba", "name": "Xinhang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bb", "name": "Xinhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961bc", "name": "Yang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bd", "name": "Yanhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961be", "name": "Yiwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961bf", "name": "Yuxiao Li", "hidden": false}, {"_id": "69817e2cce18b186280961c0", "name": "Zhaowei Li", "hidden": false}, {"_id": "69817e2cce18b186280961c1", "name": "Zheming Li", "hidden": false}, {"_id": "69817e2cce18b186280961c2", "name": "Weilong Liao", "hidden": false}, {"_id": "69817e2cce18b186280961c3", "name": "Jiawei Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c4", "name": "Xiaohan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c5", "name": "Zhishan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c6", "name": "Zichao Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c7", "name": "Cheng Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c8", "name": "Chenyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c9", "name": "Hongzhang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ca", "name": "Liang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cb", "name": "Shaowei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cc", "name": "Shudong Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cd", "name": "Shuran Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ce", "name": "Tianwei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cf", "name": "Tianyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d0", "name": "Weizhou Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d1", "name": "Xiangyan Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d2", "name": "Yangyang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d3", "name": "Yanming Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d4", "name": "Yibo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d5", "name": "Yuanxin Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d6", "name": "Yue Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d7", "name": "Zhengying Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d8", "name": "Zhongnuo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d9", "name": "Enzhe Lu", "hidden": false}, {"_id": "69817e2cce18b186280961da", "name": "Haoyu Lu", "hidden": false}, {"_id": "69817e2cce18b186280961db", "name": "Zhiyuan Lu", "hidden": false}, {"_id": "69817e2cce18b186280961dc", "user": {"_id": "642da1cd99f3110ac27caca5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg", "isPro": false, "fullname": "junyu", "user": "luojunyu", "type": "user"}, "name": "Junyu Luo", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:08.357Z", "hidden": false}, {"_id": "69817e2cce18b186280961dd", "name": "Tongxu Luo", "hidden": false}, {"_id": "69817e2cce18b186280961de", "name": "Yashuo Luo", "hidden": false}, {"_id": "69817e2cce18b186280961df", "name": "Long Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e0", "name": "Yingwei Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e1", "name": "Shaoguang Mao", "hidden": false}, {"_id": "69817e2cce18b186280961e2", "name": "Yuan Mei", "hidden": false}, {"_id": "69817e2cce18b186280961e3", "name": "Xin Men", "hidden": false}, {"_id": "69817e2cce18b186280961e4", "name": "Fanqing Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e5", "name": "Zhiyong Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e6", "name": "Yibo Miao", "hidden": false}, {"_id": "69817e2cce18b186280961e7", "name": "Minqing Ni", "hidden": false}, {"_id": "69817e2cce18b186280961e8", "name": "Kun Ouyang", "hidden": false}, {"_id": "69817e2cce18b186280961e9", "name": "Siyuan Pan", "hidden": false}, {"_id": "69817e2cce18b186280961ea", "name": "Bo Pang", "hidden": false}, {"_id": "69817e2cce18b186280961eb", "name": "Yuchao Qian", "hidden": false}, {"_id": "69817e2cce18b186280961ec", "name": "Ruoyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ed", "name": "Zeyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ee", "name": "Jiezhong Qiu", "hidden": false}, {"_id": "69817e2cce18b186280961ef", "name": "Bowen Qu", "hidden": false}, {"_id": "69817e2cce18b186280961f0", "name": "Zeyu Shang", "hidden": false}, {"_id": "69817e2cce18b186280961f1", "name": "Youbo Shao", "hidden": false}, {"_id": "69817e2cce18b186280961f2", "name": "Tianxiao Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f3", "name": "Zhennan Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f4", "name": "Juanfeng Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f5", "name": "Lidong Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f6", "name": "Shengyuan Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f7", "name": "Feifan Song", "hidden": false}, {"_id": "69817e2cce18b186280961f8", "name": "Pengwei Song", "hidden": false}, {"_id": "69817e2cce18b186280961f9", "name": "Tianhui Song", "hidden": false}, {"_id": "69817e2cce18b186280961fa", "name": "Xiaoxi Song", "hidden": false}, {"_id": "69817e2cce18b186280961fb", "name": "Hongjin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fc", "name": "Jianlin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fd", "name": "Zhaochen Su", "hidden": false}, {"_id": "69817e2cce18b186280961fe", "name": "Lin Sui", "hidden": false}, {"_id": "69817e2cce18b186280961ff", "name": "Jinsong Sun", "hidden": false}, {"_id": "69817e2cce18b18628096200", "name": "Junyao Sun", "hidden": false}, {"_id": "69817e2cce18b18628096201", "name": "Tongyu Sun", "hidden": false}, {"_id": "69817e2cce18b18628096202", "name": "Flood Sung", "hidden": false}, {"_id": "69817e2cce18b18628096203", "name": "Yunpeng Tai", "hidden": false}, {"_id": "69817e2cce18b18628096204", "name": "Chuning Tang", "hidden": false}, {"_id": "69817e2cce18b18628096205", "name": "Heyi Tang", "hidden": false}, {"_id": "69817e2cce18b18628096206", "name": "Xiaojuan Tang", "hidden": false}, {"_id": "69817e2cce18b18628096207", "name": "Zhengyang Tang", "hidden": false}, {"_id": "69817e2cce18b18628096208", "name": "Jiawen Tao", "hidden": false}, {"_id": "69817e2cce18b18628096209", "name": "Shiyuan Teng", "hidden": false}, {"_id": "69817e2cce18b1862809620a", "name": "Chaoran Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620b", "name": "Pengfei Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620c", "name": "Ao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620d", "name": "Bowen Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620e", "name": "Chensi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620f", "name": "Chuang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096210", "name": "Congcong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096211", "name": "Dingkun Wang", "hidden": false}, {"_id": "69817e2cce18b18628096212", "name": "Dinglu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096213", "name": "Dongliang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096214", "name": "Feng Wang", "hidden": false}, {"_id": "69817e2cce18b18628096215", "name": "Hailong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096216", "name": "Haiming Wang", "hidden": false}, {"_id": "69817e2cce18b18628096217", "name": "Hengzhi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096218", "name": "Huaqing Wang", "hidden": false}, {"_id": "69817e2cce18b18628096219", "name": "Hui Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621a", "name": "Jiahao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621b", "name": "Jinhong Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621c", "name": "Jiuzheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621d", "name": "Kaixin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621e", "name": "Linian Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621f", "name": "Qibin Wang", "hidden": false}, {"_id": "69817e2cce18b18628096220", "name": "Shengjie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096221", "name": "Shuyi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096222", "name": "Si Wang", "hidden": false}, {"_id": "69817e2cce18b18628096223", "name": "Wei Wang", "hidden": false}, {"_id": "69817e2cce18b18628096224", "name": "Xiaochen Wang", "hidden": false}, {"_id": "69817e2cce18b18628096225", "name": "Xinyuan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096226", "name": "Yao Wang", "hidden": false}, {"_id": "69817e2cce18b18628096227", "name": "Yejie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096228", "name": "Yipu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096229", "name": "Yiqin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622a", "name": "Yucheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622b", "name": "Yuzhi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622c", "name": "Zhaoji Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622d", "name": "Zhaowei Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622e", "name": "Zhengtao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622f", "name": "Zhexu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096230", "name": "Zihan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096231", "name": "Zizhe Wang", "hidden": false}, {"_id": "69817e2cce18b18628096232", "user": {"_id": "635ddec594e5b275ca7941e8", "avatarUrl": "/avatars/28ebfaee74d31e1de020a3ae735a4c1b.svg", "isPro": false, "fullname": "Chu Wei", "user": "courage17340", "type": "user"}, "name": "Chu Wei", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:17.862Z", "hidden": false}, {"_id": "69817e2cce18b18628096233", "name": "Ming Wei", "hidden": false}, {"_id": "69817e2cce18b18628096234", "name": "Chuan Wen", "hidden": false}, {"_id": "69817e2cce18b18628096235", "user": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "isPro": false, "fullname": "Zichen Wen", "user": "zichenwen", "type": "user"}, "name": "Zichen Wen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:09:43.363Z", "hidden": false}, {"_id": "69817e2cce18b18628096236", "name": "Chengjie Wu", "hidden": false}, {"_id": "69817e2cce18b18628096237", "user": {"_id": "63047ed2412a1b9d381b09c9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg", "isPro": true, "fullname": "Haoning Wu, Teo", "user": "teowu", "type": "user"}, "name": "Haoning Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:49.884Z", "hidden": false}, {"_id": "69817e2cce18b18628096238", "name": "Junyan Wu", "hidden": false}, {"_id": "69817e2cce18b18628096239", "name": "Rucong Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623a", "name": "Wenhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623b", "name": "Yuefeng Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623c", "name": "Yuhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623d", "name": "Yuxin Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623e", "name": "Zijian Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623f", "name": "Chenjun Xiao", "hidden": false}, {"_id": "69817e2cce18b18628096240", "name": "Jin Xie", "hidden": false}, {"_id": "69817e2cce18b18628096241", "name": "Xiaotong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096242", "name": "Yuchong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096243", "name": "Yifei Xin", "hidden": false}, {"_id": "69817e2cce18b18628096244", "name": "Bowei Xing", "hidden": false}, {"_id": "69817e2cce18b18628096245", "name": "Boyu Xu", "hidden": false}, {"_id": "69817e2cce18b18628096246", "name": "Jianfan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096247", "name": "Jing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096248", "name": "Jinjing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096249", "name": "L. H. Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624a", "name": "Lin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624b", "name": "Suting Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624c", "name": "Weixin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624d", "name": "Xinbo Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624e", "name": "Xinran Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624f", "name": "Yangchuan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096250", "name": "Yichang Xu", "hidden": false}, {"_id": "69817e2cce18b18628096251", "name": "Yuemeng Xu", "hidden": false}, {"_id": "69817e2cce18b18628096252", "name": "Zelai Xu", "hidden": false}, {"_id": "69817e2cce18b18628096253", "name": "Ziyao Xu", "hidden": false}, {"_id": "69817e2cce18b18628096254", "name": "Junjie Yan", "hidden": false}, {"_id": "69817e2cce18b18628096255", "name": "Yuzi Yan", "hidden": false}, {"_id": "69817e2cce18b18628096256", "name": "Guangyao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096257", "name": "Hao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096258", "name": "Junwei Yang", "hidden": false}, {"_id": "69817e2cce18b18628096259", "name": "Kai Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625a", "name": "Ningyuan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625b", "name": "Ruihan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625c", "name": "Xiaofei Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625d", "name": "Xinlong Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625e", "name": "Ying Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625f", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096260", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096261", "name": "Zhen Yang", "hidden": false}, {"_id": "69817e2cce18b18628096262", "name": "Zhilin Yang", "hidden": false}, {"_id": "69817e2cce18b18628096263", "name": "Zonghan Yang", "hidden": false}, {"_id": "69817e2cce18b18628096264", "user": {"_id": "642bcd9be8dfcc1fe4f4f853", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642bcd9be8dfcc1fe4f4f853/M9Yqkyt66dnWWCwmBZ8l0.jpeg", "isPro": false, "fullname": "Haotian Yao", "user": "skylark-95", "type": "user"}, "name": "Haotian Yao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:12.739Z", "hidden": false}, {"_id": "69817e2cce18b18628096265", "name": "Dan Ye", "hidden": false}, {"_id": "69817e2cce18b18628096266", "name": "Wenjie Ye", "hidden": false}, {"_id": "69817e2cce18b18628096267", "name": "Zhuorui Ye", "hidden": false}, {"_id": "69817e2cce18b18628096268", "name": "Bohong Yin", "hidden": false}, {"_id": "69817e2cce18b18628096269", "name": "Chengzhen Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626a", "name": "Longhui Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626b", "name": "Tao Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626c", "name": "Tianxiang Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626d", "name": "Enming Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626e", "name": "Mengjie Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626f", "name": "Xiaokun Yuan", "hidden": false}, {"_id": "69817e2cce18b18628096270", "name": "Yang Yue", "hidden": false}, {"_id": "69817e2cce18b18628096271", "name": "Weihao Zeng", "hidden": false}, {"_id": "69817e2cce18b18628096272", "name": "Dunyuan Zha", "hidden": false}, {"_id": "69817e2cce18b18628096273", "name": "Haobing Zhan", "hidden": false}, {"_id": "69817e2cce18b18628096274", "name": "Dehao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096275", "name": "Hao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096276", "name": "Jin Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096277", "name": "Puqi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096278", "name": "Qiao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096279", "name": "Rui Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627a", "name": "Xiaobin Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627b", "name": "Y. Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627c", "name": "Yadong Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627d", "name": "Yangkun Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627e", "name": "Yichi Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627f", "name": "Yizhi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096280", "name": "Yongting Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096281", "name": "Yu Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096282", "name": "Yushun Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096283", "name": "Yutao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096284", "name": "Yutong Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096285", "name": "Zheng Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096286", "name": "Chenguang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096287", "name": "Feifan Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096288", "name": "Jinxiang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096289", "name": "Shuai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628a", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628b", "name": "Yikai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628c", "name": "Zijia Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628d", "name": "Huabin Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628e", "name": "Ruihan Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628f", "name": "Shaojie Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096290", "name": "Tengyang Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096291", "name": "Junfeng Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096292", "user": {"_id": "62b6d20416ff90e6198301b6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656148456743-noauth.png", "isPro": false, "fullname": "Longguang Zhong", "user": "GGLS", "type": "user"}, "name": "Longguang Zhong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:14.989Z", "hidden": false}, {"_id": "69817e2cce18b18628096293", "name": "Weiming Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096294", "name": "M. Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096295", "name": "Runjie Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096296", "name": "Xinyu Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096297", "name": "Zaida Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096298", "name": "Jinguo Zhu", "hidden": false}, {"_id": "69817e2cce18b18628096299", "name": "Liya Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629a", "name": "Xinhao Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629b", "name": "Yuxuan Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629c", "name": "Zhen Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629d", "name": "Jingze Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629e", "name": "Weiyu Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629f", "name": "Ying Zou", "hidden": false}, {"_id": "69817e2cce18b186280962a0", "name": "Xinxing Zu", "hidden": false}], "publishedAt": "2026-02-02T16:17:38.000Z", "submittedOnDailyAt": "2026-02-03T02:18:48.721Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "upvotes": 149, "discussionId": "69817e2cce18b186280962a1", "projectPage": "https://huggingface.co/moonshotai/Kimi-K2.5", "ai_summary": "Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.", "ai_keywords": ["multimodal agentic model", "joint text-vision pre-training", "zero-vision SFT", "joint text-vision reinforcement learning", "Agent Swarm", "self-directed parallel agent orchestration framework", "heterogeneous sub-problems"], "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-02T11:17:38.000Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02276.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 227, "isUserFollowing": false}, "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Feb 26, 2026";
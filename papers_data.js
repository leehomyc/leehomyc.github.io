window.trendingPapers = {
    "today": [{"paper": {"id": "2602.18283", "authors": [{"_id": "699d32bf4e37ec6dfa1bc690", "name": "Lei Xin", "hidden": false}, {"_id": "699d32bf4e37ec6dfa1bc691", "name": "Yuhao Zheng", "hidden": false}, {"_id": "699d32bf4e37ec6dfa1bc692", "name": "Ke Cheng", "hidden": false}, {"_id": "699d32bf4e37ec6dfa1bc693", "user": {"_id": "652fc2605615e57807e3db19", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652fc2605615e57807e3db19/kbRcpR0YFQnU3IlziqCHf.png", "isPro": false, "fullname": "Changjiang Jiang", "user": "arnodjiang", "type": "user"}, "name": "Changjiang Jiang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-26T10:03:02.605Z", "hidden": false}, {"_id": "699d32bf4e37ec6dfa1bc694", "name": "Zifan Zhang", "hidden": false}, {"_id": "699d32bf4e37ec6dfa1bc695", "name": "Fanhu Zeng", "hidden": false}], "publishedAt": "2026-02-20T15:11:40.000Z", "submittedOnDailyAt": "2026-02-26T01:11:59.076Z", "title": "HyTRec: A Hybrid Temporal-Aware Attention Architecture for Long Behavior Sequential Recommendation", "submittedOnDailyBy": {"_id": "64107c7df52d7eb22e062956", "avatarUrl": "/avatars/7b1cee9a2b8454fedfbd4c3d1df9865c.svg", "isPro": false, "fullname": "Yuhao Zheng", "user": "yhzheng1031", "type": "user"}, "summary": "Modeling long sequences of user behaviors has emerged as a critical frontier in generative recommendation. However, existing solutions face a dilemma: linear attention mechanisms achieve efficiency at the cost of retrieval precision due to limited state capacity, while softmax attention suffers from prohibitive computational overhead. To address this challenge, we propose HyTRec, a model featuring a Hybrid Attention architecture that explicitly decouples long-term stable preferences from short-term intent spikes. By assigning massive historical sequences to a linear attention branch and reserving a specialized softmax attention branch for recent interactions, our approach restores precise retrieval capabilities within industrial-scale contexts involving ten thousand interactions. To mitigate the lag in capturing rapid interest drifts within the linear layers, we furthermore design Temporal-Aware Delta Network (TADN) to dynamically upweight fresh behavioral signals while effectively suppressing historical noise. Empirical results on industrial-scale datasets confirm the superiority that our model maintains linear inference speed and outperforms strong baselines, notably delivering over 8% improvement in Hit Rate for users with ultra-long sequences with great efficiency.", "upvotes": 46, "discussionId": "699d32bf4e37ec6dfa1bc696", "ai_summary": "HyTRec addresses the challenge of modeling long user behavior sequences by combining linear and softmax attention mechanisms with a temporal-aware delta network to balance efficiency and retrieval precision.", "ai_keywords": ["Hybrid Attention", "linear attention", "softmax attention", "long-term stable preferences", "short-term intent spikes", "temporal-aware delta network", "TADN", "Hit Rate"], "organization": {"_id": "6350bdf559bfa9a85d42fea4", "name": "WuhanUniversity", "fullname": "Wuhan Univeristy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6350bd20aaee2ec378dfe506/Bu1Fwz4dAwjwzWv-vZ2FN.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-20T10:11:40.000Z", "title": "HyTRec: A Hybrid Temporal-Aware Attention Architecture for Long Behavior Sequential Recommendation", "summary": "Modeling long sequences of user behaviors has emerged as a critical frontier in generative recommendation. However, existing solutions face a dilemma: linear attention mechanisms achieve efficiency at the cost of retrieval precision due to limited state capacity, while softmax attention suffers from prohibitive computational overhead. To address this challenge, we propose HyTRec, a model featuring a Hybrid Attention architecture that explicitly decouples long-term stable preferences from short-term intent spikes. By assigning massive historical sequences to a linear attention branch and reserving a specialized softmax attention branch for recent interactions, our approach restores precise retrieval capabilities within industrial-scale contexts involving ten thousand interactions. To mitigate the lag in capturing rapid interest drifts within the linear layers, we furthermore design Temporal-Aware Delta Network (TADN) to dynamically upweight fresh behavioral signals while effectively suppressing historical noise. Empirical results on industrial-scale datasets confirm the superiority that our model maintains linear inference speed and outperforms strong baselines, notably delivering over 8% improvement in Hit Rate for users with ultra-long sequences with great efficiency.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18283.png", "numComments": 1, "submittedBy": {"_id": "64107c7df52d7eb22e062956", "avatarUrl": "/avatars/7b1cee9a2b8454fedfbd4c3d1df9865c.svg", "fullname": "Yuhao Zheng", "name": "yhzheng1031", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "6350bdf559bfa9a85d42fea4", "name": "WuhanUniversity", "fullname": "Wuhan Univeristy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6350bd20aaee2ec378dfe506/Bu1Fwz4dAwjwzWv-vZ2FN.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.17602", "authors": [{"_id": "699fd9f8ebfce7fbcca91c00", "user": {"_id": "65f836339e3737dc3040f3be", "avatarUrl": "/avatars/70b479f58338b71192a05331dfa1bb15.svg", "isPro": false, "fullname": "Hojung Jung", "user": "cossmoss", "type": "user"}, "name": "Hojung Jung", "status": "claimed_verified", "statusLastChangedAt": "2026-02-26T10:01:39.873Z", "hidden": false}, {"_id": "699fd9f8ebfce7fbcca91c01", "name": "Rodrigo Hormazabal", "hidden": false}, {"_id": "699fd9f8ebfce7fbcca91c02", "name": "Jaehyeong Jo", "hidden": false}, {"_id": "699fd9f8ebfce7fbcca91c03", "name": "Youngrok Park", "hidden": false}, {"_id": "699fd9f8ebfce7fbcca91c04", "name": "Kyunggeun Roh", "hidden": false}, {"_id": "699fd9f8ebfce7fbcca91c05", "name": "Se-Young Yun", "hidden": false}, {"_id": "699fd9f8ebfce7fbcca91c06", "name": "Sehui Han", "hidden": false}, {"_id": "699fd9f8ebfce7fbcca91c07", "name": "Dae-Woong Jeong", "hidden": false}], "publishedAt": "2026-02-19T18:27:11.000Z", "submittedOnDailyAt": "2026-02-26T08:10:14.563Z", "title": "MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models", "submittedOnDailyBy": {"_id": "65e5bd4568234ef5d6decadc", "avatarUrl": "/avatars/c41095a946c0176b949c0b3566136c05.svg", "isPro": false, "fullname": "Jaehyeong Jo", "user": "harryjo97", "type": "user"}, "summary": "Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.", "upvotes": 41, "discussionId": "699fd9f9ebfce7fbcca91c08", "ai_summary": "MolHIT presents a hierarchical discrete diffusion model for molecular graph generation that achieves superior chemical validity and property-guided synthesis compared to existing 1D and graph-based approaches.", "ai_keywords": ["diffusion models", "molecular generation", "graph diffusion models", "chemical validity", "hierarchical discrete diffusion model", "chemical priors", "atom encoding", "MOSES dataset", "multi-property guided generation", "scaffold extension"], "organization": {"_id": "6475760c33192631bad2bb38", "name": "kaist-ai", "fullname": "KAIST AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-19T13:27:11.000Z", "title": "MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models", "summary": "Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.17602.png", "numComments": 1, "submittedBy": {"_id": "65e5bd4568234ef5d6decadc", "avatarUrl": "/avatars/c41095a946c0176b949c0b3566136c05.svg", "fullname": "Jaehyeong Jo", "name": "harryjo97", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "6475760c33192631bad2bb38", "name": "kaist-ai", "fullname": "KAIST AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.12160", "authors": [{"_id": "698e9f3bcace060ff123ae16", "user": {"_id": "67d50738fed7787297d737d6", "avatarUrl": "/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg", "isPro": false, "fullname": "xuguo", "user": "XuGuo699", "type": "user"}, "name": "Xu Guo", "status": "claimed_verified", "statusLastChangedAt": "2026-02-26T10:03:53.038Z", "hidden": false}, {"_id": "698e9f3bcace060ff123ae17", "name": "Fulong Ye", "hidden": false}, {"_id": "698e9f3bcace060ff123ae18", "name": "Qichao Sun", "hidden": false}, {"_id": "698e9f3bcace060ff123ae19", "name": "Liyang Chen", "hidden": false}, {"_id": "698e9f3bcace060ff123ae1a", "name": "Bingchuan Li", "hidden": false}, {"_id": "698e9f3bcace060ff123ae1b", "name": "Pengze Zhang", "hidden": false}, {"_id": "698e9f3bcace060ff123ae1c", "name": "Jiawei Liu", "hidden": false}, {"_id": "698e9f3bcace060ff123ae1d", "name": "Songtao Zhao", "hidden": false}, {"_id": "698e9f3bcace060ff123ae1e", "name": "Qian He", "hidden": false}, {"_id": "698e9f3bcace060ff123ae1f", "name": "Xiangwang Hou", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/67d50738fed7787297d737d6/oyLOvr4XR4tSXLrO7N76E.mp4"], "publishedAt": "2026-02-12T16:41:52.000Z", "submittedOnDailyAt": "2026-02-26T01:31:25.512Z", "title": "DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation", "submittedOnDailyBy": {"_id": "67d50738fed7787297d737d6", "avatarUrl": "/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg", "isPro": false, "fullname": "xuguo", "user": "XuGuo699", "type": "user"}, "summary": "Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.", "upvotes": 29, "discussionId": "698e9f3ccace060ff123ae20", "projectPage": "https://guoxu1233.github.io/DreamID-Omni/", "githubRepo": "https://github.com/Guoxu1233/DreamID-Omni", "githubRepoAddedBy": "user", "ai_summary": "DreamID-Omni is a unified framework for controllable human-centric audio-video generation that uses a symmetric conditional diffusion transformer with dual-level disentanglement and multi-task progressive training to achieve state-of-the-art performance.", "ai_keywords": ["conditional diffusion transformer", "symmetric conditional injection scheme", "dual-level disentanglement", "synchronized RoPE", "structured captions", "multi-task progressive training", "audio-video generation", "reference-based audio-video generation", "video editing", "audio-driven video animation", "identity-timbre binding", "speaker confusion", "diffusion models"], "githubStars": 54, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-12T11:41:52.000Z", "title": "DreamID-Omni: Unified Framework for Controllable Human-Centric Audio-Video Generation", "summary": "Recent advancements in foundation models have revolutionized joint audio-video generation. However, existing approaches typically treat human-centric tasks including reference-based audio-video generation (R2AV), video editing (RV2AV) and audio-driven video animation (RA2V) as isolated objectives. Furthermore, achieving precise, disentangled control over multiple character identities and voice timbres within a single framework remains an open challenge. In this paper, we propose DreamID-Omni, a unified framework for controllable human-centric audio-video generation. Specifically, we design a Symmetric Conditional Diffusion Transformer that integrates heterogeneous conditioning signals via a symmetric conditional injection scheme. To resolve the pervasive identity-timbre binding failures and speaker confusion in multi-person scenarios, we introduce a Dual-Level Disentanglement strategy: Synchronized RoPE at the signal level to ensure rigid attention-space binding, and Structured Captions at the semantic level to establish explicit attribute-subject mappings. Furthermore, we devise a Multi-Task Progressive Training scheme that leverages weakly-constrained generative priors to regularize strongly-constrained tasks, preventing overfitting and harmonizing disparate objectives. Extensive experiments demonstrate that DreamID-Omni achieves comprehensive state-of-the-art performance across video, audio, and audio-visual consistency, even outperforming leading proprietary commercial models. We will release our code to bridge the gap between academic research and commercial-grade applications.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/67d50738fed7787297d737d6/oyLOvr4XR4tSXLrO7N76E.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12160.png", "numComments": 1, "submittedBy": {"_id": "67d50738fed7787297d737d6", "avatarUrl": "/avatars/30d7126f7c3feda732c5783ea3db9c7f.svg", "fullname": "xuguo", "name": "XuGuo699", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.21818", "authors": [{"_id": "699fba56ebfce7fbcca91b1a", "name": "Guibin Chen", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b1b", "name": "Dixuan Lin", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b1c", "name": "Jiangping Yang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b1d", "name": "Youqiang Zhang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b1e", "name": "Zhengcong Fei", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b1f", "name": "Debang Li", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b20", "name": "Sheng Chen", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b21", "name": "Chaofeng Ao", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b22", "name": "Nuo Pang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b23", "name": "Yiming Wang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b24", "name": "Yikun Dou", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b25", "name": "Zheng Chen", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b26", "name": "Mingyuan Fan", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b27", "name": "Tuanhui Li", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b28", "name": "Mingshan Chang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b29", "name": "Hao Zhang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b2a", "name": "Xiaopeng Sun", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b2b", "name": "Jingtao Xu", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b2c", "name": "Yuqiang Xie", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b2d", "name": "Jiahua Wang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b2e", "name": "Zhiheng Xu", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b2f", "name": "Weiming Xiong", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b30", "name": "Yuzhe Jin", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b31", "name": "Baoxuan Gu", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b32", "name": "Binjie Mao", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b33", "name": "Yunjie Yu", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b34", "name": "Jujie He", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b35", "name": "Yuhao Feng", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b36", "name": "Shiwen Tu", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b37", "name": "Chaojie Wang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b38", "name": "Rui Yan", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b39", "name": "Wei Shen", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b3a", "name": "Jingchen Wu", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b3b", "name": "Peng Zhao", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b3c", "name": "Xuanyue Zhong", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b3d", "name": "Zhuangzhuang Liu", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b3e", "name": "Kaifei Wang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b3f", "name": "Fuxiang Zhang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b40", "name": "Weikai Xu", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b41", "name": "Wenyan Liu", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b42", "name": "Binglu Zhang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b43", "name": "Yu Shen", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b44", "name": "Tianhui Xiong", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b45", "name": "Bin Peng", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b46", "name": "Liang Zeng", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b47", "name": "Xuchen Song", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b48", "name": "Haoxiang Guo", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b49", "name": "Peiyu Wang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b4a", "name": "Yahui Zhou", "hidden": false}], "publishedAt": "2026-02-25T11:47:00.000Z", "submittedOnDailyAt": "2026-02-26T00:43:38.210Z", "title": "SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "SkyReels V4 is a unified multi modal video foundation model for joint video audio generation, inpainting, and editing. The model adopts a dual stream Multimodal Diffusion Transformer (MMDiT) architecture, where one branch synthesizes video and the other generates temporally aligned audio, while sharing a powerful text encoder based on the Multimodal Large Language Models (MMLM). SkyReels V4 accepts rich multi modal instructions, including text, images, video clips, masks, and audio references. By combining the MMLMs multi modal instruction following capability with in context learning in the video branch MMDiT, the model can inject fine grained visual guidance under complex conditioning, while the audio branch MMDiT simultaneously leverages audio references to guide sound generation. On the video side, we adopt a channel concatenation formulation that unifies a wide range of inpainting style tasks, such as image to video, video extension, and video editing under a single interface, and naturally extends to vision referenced inpainting and editing via multi modal prompts. SkyReels V4 supports up to 1080p resolution, 32 FPS, and 15 second duration, enabling high fidelity, multi shot, cinema level video generation with synchronized audio. To make such high resolution, long-duration generation computationally feasible, we introduce an efficiency strategy: Joint generation of low resolution full sequences and high-resolution keyframes, followed by dedicated super-resolution and frame interpolation models. To our knowledge, SkyReels V4 is the first video foundation model that simultaneously supports multi-modal input, joint video audio generation, and a unified treatment of generation, inpainting, and editing, while maintaining strong efficiency and quality at cinematic resolutions and durations.", "upvotes": 19, "discussionId": "699fba56ebfce7fbcca91b4b", "ai_summary": "SkyReels V4 is a unified multimodal video foundation model that generates, edits, and inpaints video and audio simultaneously using a dual-stream architecture with shared text encoding and efficient high-resolution processing.", "ai_keywords": ["Multimodal Diffusion Transformer", "MMDiT", "Multimodal Large Language Models", "MMLM", "video audio generation", "video inpainting", "video editing", "channel concatenation formulation", "joint generation", "super-resolution", "frame interpolation"], "organization": {"_id": "6522615d9334173c627b0efa", "name": "Skywork", "fullname": "Skywork", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64535b71bcbd25618f7655da/AvtJ4GuPAyhLxl2-leVt6.jpeg"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-25T06:47:00.000Z", "title": "SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model", "summary": "SkyReels V4 is a unified multi modal video foundation model for joint video audio generation, inpainting, and editing. The model adopts a dual stream Multimodal Diffusion Transformer (MMDiT) architecture, where one branch synthesizes video and the other generates temporally aligned audio, while sharing a powerful text encoder based on the Multimodal Large Language Models (MMLM). SkyReels V4 accepts rich multi modal instructions, including text, images, video clips, masks, and audio references. By combining the MMLMs multi modal instruction following capability with in context learning in the video branch MMDiT, the model can inject fine grained visual guidance under complex conditioning, while the audio branch MMDiT simultaneously leverages audio references to guide sound generation. On the video side, we adopt a channel concatenation formulation that unifies a wide range of inpainting style tasks, such as image to video, video extension, and video editing under a single interface, and naturally extends to vision referenced inpainting and editing via multi modal prompts. SkyReels V4 supports up to 1080p resolution, 32 FPS, and 15 second duration, enabling high fidelity, multi shot, cinema level video generation with synchronized audio. To make such high resolution, long-duration generation computationally feasible, we introduce an efficiency strategy: Joint generation of low resolution full sequences and high-resolution keyframes, followed by dedicated super-resolution and frame interpolation models. To our knowledge, SkyReels V4 is the first video foundation model that simultaneously supports multi-modal input, joint video audio generation, and a unified treatment of generation, inpainting, and editing, while maintaining strong efficiency and quality at cinematic resolutions and durations.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21818.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 240, "isUserFollowing": false}, "organization": {"_id": "6522615d9334173c627b0efa", "name": "Skywork", "fullname": "Skywork", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64535b71bcbd25618f7655da/AvtJ4GuPAyhLxl2-leVt6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.21534", "authors": [{"_id": "699fbaf1ebfce7fbcca91b4d", "name": "Xiaoxuan Wang", "hidden": false}, {"_id": "699fbaf1ebfce7fbcca91b4e", "name": "Han Zhang", "hidden": false}, {"_id": "699fbaf1ebfce7fbcca91b4f", "name": "Haixin Wang", "hidden": false}, {"_id": "699fbaf1ebfce7fbcca91b50", "name": "Yidan Shi", "hidden": false}, {"_id": "699fbaf1ebfce7fbcca91b51", "name": "Ruoyan Li", "hidden": false}, {"_id": "699fbaf1ebfce7fbcca91b52", "name": "Kaiqiao Han", "hidden": false}, {"_id": "699fbaf1ebfce7fbcca91b53", "name": "Chenyi Tong", "hidden": false}, {"_id": "699fbaf1ebfce7fbcca91b54", "name": "Haoran Deng", "hidden": false}, {"_id": "699fbaf1ebfce7fbcca91b55", "name": "Renliang Sun", "hidden": false}, {"_id": "699fbaf1ebfce7fbcca91b56", "name": "Alexander Taylor", "hidden": false}, {"_id": "699fbaf1ebfce7fbcca91b57", "name": "Yanqiao Zhu", "hidden": false}, {"_id": "699fbaf1ebfce7fbcca91b58", "name": "Jason Cong", "hidden": false}, {"_id": "699fbaf1ebfce7fbcca91b59", "name": "Yizhou Sun", "hidden": false}, {"_id": "699fbaf1ebfce7fbcca91b5a", "name": "Wei Wang", "hidden": false}], "publishedAt": "2026-02-25T03:43:34.000Z", "submittedOnDailyAt": "2026-02-26T00:47:18.994Z", "title": "ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning", "submittedOnDailyBy": {"_id": "64ba5946c0f19c9025665a3c", "avatarUrl": "/avatars/bb148094ce52f1f385d30968dc22e0e6.svg", "isPro": false, "fullname": "Xiaoxuan Wang", "user": "xw27", "type": "user"}, "summary": "Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.", "upvotes": 16, "discussionId": "699fbaf1ebfce7fbcca91b5b", "githubRepo": "https://github.com/WillDreamer/ARL-Arena", "githubRepoAddedBy": "user", "ai_summary": "ARLArena framework analyzes training stability in agentic reinforcement learning and proposes SAMPO method for stable policy optimization across diverse tasks.", "ai_keywords": ["agentic reinforcement learning", "policy gradient", "training stability", "policy optimization", "ARLArena", "SAMPO"], "githubStars": 17, "organization": {"_id": "67784c39dac147922d8d09f0", "name": "UCLA", "fullname": "University of California, Los Angeles", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-24T22:43:34.000Z", "title": "ARLArena: A Unified Framework for Stable Agentic Reinforcement Learning", "summary": "Agentic reinforcement learning (ARL) has rapidly gained attention as a promising paradigm for training agents to solve complex, multi-step interactive tasks. Despite encouraging early results, ARL remains highly unstable, often leading to training collapse. This instability limits scalability to larger environments and longer interaction horizons, and constrains systematic exploration of algorithmic design choices. In this paper, we first propose ARLArena, a stable training recipe and systematic analysis framework that examines training stability in a controlled and reproducible setting. ARLArena first constructs a clean and standardized testbed. Then, we decompose policy gradient into four core design dimensions and assess the performance and stability of each dimension. Through this fine-grained analysis, we distill a unified perspective on ARL and propose SAMPO, a stable agentic policy optimization method designed to mitigate the dominant sources of instability in ARL. Empirically, SAMPO achieves consistently stable training and strong performance across diverse agentic tasks. Overall, this study provides a unifying policy gradient perspective for ARL and offers practical guidance for building stable and reproducible LLM-based agent training pipelines.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21534.png", "numComments": 1, "submittedBy": {"_id": "64ba5946c0f19c9025665a3c", "avatarUrl": "/avatars/bb148094ce52f1f385d30968dc22e0e6.svg", "fullname": "Xiaoxuan Wang", "name": "xw27", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "67784c39dac147922d8d09f0", "name": "UCLA", "fullname": "University of California, Los Angeles", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67784bd637dfa531fbce95a2/Nf0seEMEn66sPL3QsJXj4.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.22190", "authors": [{"_id": "699fc4f3ebfce7fbcca91bc5", "name": "Rui Yang", "hidden": false}, {"_id": "699fc4f3ebfce7fbcca91bc6", "name": "Qianhui Wu", "hidden": false}, {"_id": "699fc4f3ebfce7fbcca91bc7", "name": "Zhaoyang Wang", "hidden": false}, {"_id": "699fc4f3ebfce7fbcca91bc8", "name": "Hanyang Chen", "hidden": false}, {"_id": "699fc4f3ebfce7fbcca91bc9", "name": "Ke Yang", "hidden": false}, {"_id": "699fc4f3ebfce7fbcca91bca", "name": "Hao Cheng", "hidden": false}, {"_id": "699fc4f3ebfce7fbcca91bcb", "name": "Huaxiu Yao", "hidden": false}, {"_id": "699fc4f3ebfce7fbcca91bcc", "name": "Baoling Peng", "hidden": false}, {"_id": "699fc4f3ebfce7fbcca91bcd", "name": "Huan Zhang", "hidden": false}, {"_id": "699fc4f3ebfce7fbcca91bce", "name": "Jianfeng Gao", "hidden": false}, {"_id": "699fc4f3ebfce7fbcca91bcf", "name": "Tong Zhang", "hidden": false}], "publishedAt": "2026-02-25T18:34:57.000Z", "submittedOnDailyAt": "2026-02-26T02:17:51.385Z", "title": "GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL", "submittedOnDailyBy": {"_id": "64d45451c34a346181b130dd", "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg", "isPro": true, "fullname": "Rui Yang", "user": "Ray2333", "type": "user"}, "summary": "Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.", "upvotes": 11, "discussionId": "699fc4f3ebfce7fbcca91bd0", "projectPage": "https://gui-libra.github.io", "githubRepo": "https://github.com/GUI-Libra/GUI-Libra", "githubRepoAddedBy": "user", "ai_summary": "GUI-Libra addresses limitations in open-source GUI agents through specialized training methods that improve reasoning-grounding alignment and reinforcement learning under partial verifiability, demonstrating enhanced task completion across web and mobile platforms.", "ai_keywords": ["GUI agents", "action-aligned reasoning data", "post-training pipelines", "SFT", "CoT reasoning", "RLVR", "partial verifiability", "KL regularization", "KL trust region", "success-adaptive scaling", "step-wise accuracy", "end-to-end task completion", "data curation", "reasoning-grounding alignment"], "githubStars": 8, "organization": {"_id": "68ef00955bff0d62c986e4f9", "name": "UIUC-ScaleML", "fullname": "UIUC ScaleML Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64cb1ad1667f4f80852f6050/ZUSgn9xQjfqDOHiI_-7Tw.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-25T13:34:57.000Z", "title": "GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL", "summary": "Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22190.png", "numComments": 1, "submittedBy": {"_id": "64d45451c34a346181b130dd", "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg", "fullname": "Rui Yang", "name": "Ray2333", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 15, "isUserFollowing": false}, "organization": {"_id": "68ef00955bff0d62c986e4f9", "name": "UIUC-ScaleML", "fullname": "UIUC ScaleML Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64cb1ad1667f4f80852f6050/ZUSgn9xQjfqDOHiI_-7Tw.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.22208", "authors": [{"_id": "699fb974ebfce7fbcca91b04", "name": "Georgy Savva", "hidden": false}, {"_id": "699fb974ebfce7fbcca91b05", "name": "Oscar Michel", "hidden": false}, {"_id": "699fb974ebfce7fbcca91b06", "name": "Daohan Lu", "hidden": false}, {"_id": "699fb974ebfce7fbcca91b07", "name": "Suppakit Waiwitlikhit", "hidden": false}, {"_id": "699fb974ebfce7fbcca91b08", "name": "Timothy Meehan", "hidden": false}, {"_id": "699fb974ebfce7fbcca91b09", "name": "Dhairya Mishra", "hidden": false}, {"_id": "699fb974ebfce7fbcca91b0a", "name": "Srivats Poddar", "hidden": false}, {"_id": "699fb974ebfce7fbcca91b0b", "name": "Jack Lu", "hidden": false}, {"_id": "699fb974ebfce7fbcca91b0c", "name": "Saining Xie", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/nMpp08pwIVyS2FTX_4zWe.mp4"], "publishedAt": "2026-02-25T18:59:01.000Z", "submittedOnDailyAt": "2026-02-26T00:40:52.562Z", "title": "Solaris: Building a Multiplayer Video World Model in Minecraft", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Existing action-conditioned video generation models (video world models) are limited to single-agent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, a multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop a multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using a staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, a memory-efficient Self Forcing variant that enables a longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for a new generation of multi-agent world models.", "upvotes": 10, "discussionId": "699fb974ebfce7fbcca91b0d", "projectPage": "https://solaris-wm.github.io/", "githubRepo": "https://github.com/solaris-wm/solaris", "githubRepoAddedBy": "user", "ai_summary": "Solaris is a multiplayer video world model that simulates consistent multi-view observations through a novel data collection system and staged training approach.", "ai_keywords": ["video world models", "multiplayer", "multi-agent interactions", "data collection", "staged pipeline", "bidirectional", "causal", "Self Forcing", "checkpointed Self Forcing"], "githubStars": 56, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-25T13:59:01.000Z", "title": "Solaris: Building a Multiplayer Video World Model in Minecraft", "summary": "Existing action-conditioned video generation models (video world models) are limited to single-agent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, a multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop a multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using a staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, a memory-efficient Self Forcing variant that enables a longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for a new generation of multi-agent world models.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/nMpp08pwIVyS2FTX_4zWe.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.22208.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 240, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.21461", "authors": [{"_id": "699feb51ebfce7fbcca91c2e", "user": {"_id": "63318b2349a9563915469f3b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63318b2349a9563915469f3b/zlbeB2997i8YkoyOTb9FL.jpeg", "isPro": false, "fullname": "Xiaoke Huang", "user": "xk-huang", "type": "user"}, "name": "Xiaoke Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-26T10:01:35.636Z", "hidden": false}, {"_id": "699feb51ebfce7fbcca91c2f", "name": "Bhavul Gauri", "hidden": false}, {"_id": "699feb51ebfce7fbcca91c30", "name": "Kam Woh Ng", "hidden": false}, {"_id": "699feb51ebfce7fbcca91c31", "name": "Tony Ng", "hidden": false}, {"_id": "699feb51ebfce7fbcca91c32", "name": "Mengmeng Xu", "hidden": false}, {"_id": "699feb51ebfce7fbcca91c33", "name": "Zhiheng Liu", "hidden": false}, {"_id": "699feb51ebfce7fbcca91c34", "name": "Weiming Ren", "hidden": false}, {"_id": "699feb51ebfce7fbcca91c35", "name": "Zhaochong An", "hidden": false}, {"_id": "699feb51ebfce7fbcca91c36", "name": "Zijian Zhou", "hidden": false}, {"_id": "699feb51ebfce7fbcca91c37", "name": "Haonan Qiu", "hidden": false}, {"_id": "699feb51ebfce7fbcca91c38", "name": "Yuyin Zhou", "hidden": false}, {"_id": "699feb51ebfce7fbcca91c39", "name": "Sen He", "hidden": false}, {"_id": "699feb51ebfce7fbcca91c3a", "name": "Ziheng Wang", "hidden": false}, {"_id": "699feb51ebfce7fbcca91c3b", "name": "Tao Xiang", "hidden": false}, {"_id": "699feb51ebfce7fbcca91c3c", "name": "Xiao Han", "hidden": false}], "publishedAt": "2026-02-25T00:27:23.000Z", "submittedOnDailyAt": "2026-02-26T06:12:10.007Z", "title": "VecGlypher: Unified Vector Glyph Generation with Language Models", "submittedOnDailyBy": {"_id": "63318b2349a9563915469f3b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63318b2349a9563915469f3b/zlbeB2997i8YkoyOTb9FL.jpeg", "isPro": false, "fullname": "Xiaoke Huang", "user": "xk-huang", "type": "user"}, "summary": "Vector glyphs are the atomic units of digital typography, yet most learning-based pipelines still depend on carefully curated exemplar sheets and raster-to-vector postprocessing, which limits accessibility and editability. We introduce VecGlypher, a single multimodal language model that generates high-fidelity vector glyphs directly from text descriptions or image exemplars. Given a style prompt, optional reference glyph images, and a target character, VecGlypher autoregressively emits SVG path tokens, avoiding raster intermediates and producing editable, watertight outlines in one pass. A typography-aware data and training recipe makes this possible: (i) a large-scale continuation stage on 39K noisy Envato fonts to master SVG syntax and long-horizon geometry, followed by (ii) post-training on 2.5K expert-annotated Google Fonts with descriptive tags and exemplars to align language and imagery with geometry; preprocessing normalizes coordinate frames, canonicalizes paths, de-duplicates families, and quantizes coordinates for stable long-sequence decoding. On cross-family OOD evaluation, VecGlypher substantially outperforms both general-purpose LLMs and specialized vector-font baselines for text-only generation, while image-referenced generation reaches a state-of-the-art performance, with marked gains over DeepVecFont-v2 and DualVector. Ablations show that model scale and the two-stage recipe are critical and that absolute-coordinate serialization yields the best geometry. VecGlypher lowers the barrier to font creation by letting users design with words or exemplars, and provides a scalable foundation for future multimodal design tools.", "upvotes": 9, "discussionId": "699feb52ebfce7fbcca91c3d", "projectPage": "https://xk-huang.github.io/VecGlypher/", "githubRepo": "https://github.com/xk-huang/VecGlypher", "githubRepoAddedBy": "user", "ai_summary": "VecGlypher is a multimodal language model that generates high-fidelity vector glyphs directly from text or image inputs, bypassing traditional raster-to-vector processes and enabling direct SVG path generation.", "ai_keywords": ["multimodal language model", "vector glyphs", "SVG path tokens", "autoregressive generation", "typography-aware data", "two-stage training", "coordinate normalization", "path canonicalization", "font creation"], "githubStars": 1, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-24T19:27:23.000Z", "title": "VecGlypher: Unified Vector Glyph Generation with Language Models", "summary": "Vector glyphs are the atomic units of digital typography, yet most learning-based pipelines still depend on carefully curated exemplar sheets and raster-to-vector postprocessing, which limits accessibility and editability. We introduce VecGlypher, a single multimodal language model that generates high-fidelity vector glyphs directly from text descriptions or image exemplars. Given a style prompt, optional reference glyph images, and a target character, VecGlypher autoregressively emits SVG path tokens, avoiding raster intermediates and producing editable, watertight outlines in one pass. A typography-aware data and training recipe makes this possible: (i) a large-scale continuation stage on 39K noisy Envato fonts to master SVG syntax and long-horizon geometry, followed by (ii) post-training on 2.5K expert-annotated Google Fonts with descriptive tags and exemplars to align language and imagery with geometry; preprocessing normalizes coordinate frames, canonicalizes paths, de-duplicates families, and quantizes coordinates for stable long-sequence decoding. On cross-family OOD evaluation, VecGlypher substantially outperforms both general-purpose LLMs and specialized vector-font baselines for text-only generation, while image-referenced generation reaches a state-of-the-art performance, with marked gains over DeepVecFont-v2 and DualVector. Ablations show that model scale and the two-stage recipe are critical and that absolute-coordinate serialization yields the best geometry. VecGlypher lowers the barrier to font creation by letting users design with words or exemplars, and provides a scalable foundation for future multimodal design tools.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21461.png", "numComments": 1, "submittedBy": {"_id": "63318b2349a9563915469f3b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63318b2349a9563915469f3b/zlbeB2997i8YkoyOTb9FL.jpeg", "fullname": "Xiaoke Huang", "name": "xk-huang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.19163", "authors": [{"_id": "699fb334ebfce7fbcca91ad0", "name": "Kai Liu", "hidden": false}, {"_id": "699fb334ebfce7fbcca91ad1", "name": "Yanhao Zheng", "hidden": false}, {"_id": "699fb334ebfce7fbcca91ad2", "name": "Kai Wang", "hidden": false}, {"_id": "699fb334ebfce7fbcca91ad3", "name": "Shengqiong Wu", "hidden": false}, {"_id": "699fb334ebfce7fbcca91ad4", "name": "Rongjunchen Zhang", "hidden": false}, {"_id": "699fb334ebfce7fbcca91ad5", "name": "Jiebo Luo", "hidden": false}, {"_id": "699fb334ebfce7fbcca91ad6", "name": "Dimitrios Hatzinakos", "hidden": false}, {"_id": "699fb334ebfce7fbcca91ad7", "name": "Ziwei Liu", "hidden": false}, {"_id": "699fb334ebfce7fbcca91ad8", "name": "Hao Fei", "hidden": false}, {"_id": "699fb334ebfce7fbcca91ad9", "name": "Tat-Seng Chua", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/BdSUgdHoEbTgTu6tHanG1.mp4", "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/bhNigK8NVUKgJ33WobH4i.jpeg", "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/kggZ01w0SeC0sb3YHCq6m.jpeg", "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/fUrz6yQZoxYotNr_YL5E-.jpeg"], "publishedAt": "2026-02-22T12:44:28.000Z", "submittedOnDailyAt": "2026-02-26T00:22:24.649Z", "title": "JavisDiT++: Unified Modeling and Optimization for Joint Audio-Video Generation", "submittedOnDailyBy": {"_id": "678bdcbe600666579235a1f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FoPxPc-T3xbzzkgQkrIbg.png", "isPro": false, "fullname": "KAI LIU", "user": "kkail8", "type": "user"}, "summary": "AIGC has rapidly expanded from text-to-image generation toward high-quality multimodal synthesis across video and audio. Within this context, joint audio-video generation (JAVG) has emerged as a fundamental task that produces synchronized and semantically aligned sound and vision from textual descriptions. However, compared with advanced commercial models such as Veo3, existing open-source methods still suffer from limitations in generation quality, temporal synchrony, and alignment with human preferences. To bridge the gap, this paper presents JavisDiT++, a concise yet powerful framework for unified modeling and optimization of JAVG. First, we introduce a modality-specific mixture-of-experts (MS-MoE) design that enables cross-modal interaction efficacy while enhancing single-modal generation quality. Then, we propose a temporal-aligned RoPE (TA-RoPE) strategy to achieve explicit, frame-level synchronization between audio and video tokens. Besides, we develop an audio-video direct preference optimization (AV-DPO) method to align model outputs with human preference across quality, consistency, and synchrony dimensions. Built upon Wan2.1-1.3B-T2V, our model achieves state-of-the-art performance merely with around 1M public training entries, significantly outperforming prior approaches in both qualitative and quantitative evaluations. Comprehensive ablation studies have been conducted to validate the effectiveness of our proposed modules. All the code, model, and dataset are released at https://JavisVerse.github.io/JavisDiT2-page.", "upvotes": 9, "discussionId": "699fb335ebfce7fbcca91ada", "projectPage": "https://javisverse.github.io/JavisDiT2-page/", "githubRepo": "https://github.com/JavisVerse/JavisDiT", "githubRepoAddedBy": "user", "ai_summary": "JavisDiT++ presents a unified framework for joint audio-video generation using modality-specific mixture-of-experts, temporal-aligned RoPE, and audio-video direct preference optimization to achieve high-quality, synchronized multimedia synthesis.", "ai_keywords": ["joint audio-video generation", "modality-specific mixture-of-experts", "MS-MoE", "temporal-aligned RoPE", "TA-RoPE", "audio-video direct preference optimization", "AV-DPO", "multimodal synthesis", "synchronized generation", "semantically aligned"], "githubStars": 322, "organization": {"_id": "67adfac46083604e4b664e43", "name": "JavisVerse", "fullname": "JavisVerse", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/kZQXob4mMAq0CoeZH5ZfG.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-22T07:44:28.000Z", "title": "JavisDiT++: Unified Modeling and Optimization for Joint Audio-Video Generation", "summary": "AIGC has rapidly expanded from text-to-image generation toward high-quality multimodal synthesis across video and audio. Within this context, joint audio-video generation (JAVG) has emerged as a fundamental task that produces synchronized and semantically aligned sound and vision from textual descriptions. However, compared with advanced commercial models such as Veo3, existing open-source methods still suffer from limitations in generation quality, temporal synchrony, and alignment with human preferences. To bridge the gap, this paper presents JavisDiT++, a concise yet powerful framework for unified modeling and optimization of JAVG. First, we introduce a modality-specific mixture-of-experts (MS-MoE) design that enables cross-modal interaction efficacy while enhancing single-modal generation quality. Then, we propose a temporal-aligned RoPE (TA-RoPE) strategy to achieve explicit, frame-level synchronization between audio and video tokens. Besides, we develop an audio-video direct preference optimization (AV-DPO) method to align model outputs with human preference across quality, consistency, and synchrony dimensions. Built upon Wan2.1-1.3B-T2V, our model achieves state-of-the-art performance merely with around 1M public training entries, significantly outperforming prior approaches in both qualitative and quantitative evaluations. Comprehensive ablation studies have been conducted to validate the effectiveness of our proposed modules. All the code, model, and dataset are released at https://JavisVerse.github.io/JavisDiT2-page.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/BdSUgdHoEbTgTu6tHanG1.mp4", "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/bhNigK8NVUKgJ33WobH4i.jpeg", "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/kggZ01w0SeC0sb3YHCq6m.jpeg", "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/fUrz6yQZoxYotNr_YL5E-.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19163.png", "numComments": 1, "submittedBy": {"_id": "678bdcbe600666579235a1f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FoPxPc-T3xbzzkgQkrIbg.png", "fullname": "KAI LIU", "name": "kkail8", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "67adfac46083604e4b664e43", "name": "JavisVerse", "fullname": "JavisVerse", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/678bdcbe600666579235a1f3/kZQXob4mMAq0CoeZH5ZfG.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.15030", "authors": [{"_id": "69966aaf1268a6b79e0d0253", "name": "Kaiyu Yue", "hidden": false}, {"_id": "69966aaf1268a6b79e0d0254", "name": "Menglin Jia", "hidden": false}, {"_id": "69966aaf1268a6b79e0d0255", "name": "Ji Hou", "hidden": false}, {"_id": "69966aaf1268a6b79e0d0256", "name": "Tom Goldstein", "hidden": false}], "publishedAt": "2026-02-16T18:59:57.000Z", "submittedOnDailyAt": "2026-02-26T04:22:36.951Z", "title": "Image Generation with a Sphere Encoder", "submittedOnDailyBy": {"_id": "640d0dbc8036cc2142273a83", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d0dbc8036cc2142273a83/cicTWJVqqvQv_DgDucWgY.jpeg", "isPro": true, "fullname": "Kaiyu Yue", "user": "kaiyuyue", "type": "user"}, "summary": "We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io .", "upvotes": 9, "discussionId": "69966ab01268a6b79e0d0257", "projectPage": "https://sphere-encoder.github.io", "ai_summary": "The Sphere Encoder is an efficient generative model that produces images in a single forward pass by mapping images to a spherical latent space and decoding from random points on that sphere, achieving diffusion-like quality with significantly reduced inference costs.", "ai_keywords": ["sphere encoder", "generative framework", "spherical latent space", "encoder-decoder architecture", "image reconstruction losses", "conditional generation", "inference cost"], "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-16T13:59:57.000Z", "title": "Image Generation with a Sphere Encoder", "summary": "We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io .", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15030.png", "numComments": 1, "submittedBy": {"_id": "640d0dbc8036cc2142273a83", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d0dbc8036cc2142273a83/cicTWJVqqvQv_DgDucWgY.jpeg", "fullname": "Kaiyu Yue", "name": "kaiyuyue", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2602.20159", "authors": [{"_id": "699d1e7a4e37ec6dfa1bc5b7", "user": {"_id": "67f87529318a17cc80365190", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/p1YkznAT-op1Cg-vBoFw7.png", "isPro": false, "fullname": "Maijunxian Wang", "user": "Mark7121983123", "type": "user"}, "name": "Maijunxian Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:19.409Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5b8", "user": {"_id": "65b74305e602b6c2c9125480", "avatarUrl": "/avatars/d36909e0f245bfeb632a4afc9d3fceca.svg", "isPro": false, "fullname": "wang ruisi", "user": "wruisi", "type": "user"}, "name": "Ruisi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:01.779Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5b9", "name": "Juyi Lin", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ba", "name": "Ran Ji", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bb", "name": "Thadd\u00e4us Wiedemer", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bc", "name": "Qingying Gao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bd", "name": "Dezhi Luo", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5be", "name": "Yaoyao Qian", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bf", "name": "Lianyu Huang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c0", "name": "Zelong Hong", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c1", "name": "Jiahui Ge", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c2", "name": "Qianli Ma", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c3", "name": "Hang He", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c4", "user": {"_id": "659d2dff20cf0b934bbee513", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659d2dff20cf0b934bbee513/9e9R852Zr2R82h64eUUQl.jpeg", "isPro": false, "fullname": "Yifan Zhou", "user": "yingmanji", "type": "user"}, "name": "Yifan Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:04.030Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c5", "name": "Lingzi Guo", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c6", "name": "Lantao Mei", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c7", "name": "Jiachen Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c8", "user": {"_id": "6532c3018bde2fae19578587", "avatarUrl": "/avatars/7231538f3d682a1e7b80e15ea91b2a97.svg", "isPro": false, "fullname": "Hanwen Xing", "user": "Hudx111", "type": "user"}, "name": "Hanwen Xing", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:09.132Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c9", "name": "Tianqi Zhao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ca", "name": "Fengyuan Yu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cb", "name": "Weihang Xiao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cc", "name": "Yizheng Jiao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cd", "name": "Jianheng Hou", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ce", "name": "Danyang Zhang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cf", "user": {"_id": "65d854e5d8134b93774b4080", "avatarUrl": "/avatars/0ff1db8c13095f420a856212d64f88ca.svg", "isPro": false, "fullname": "Pengcheng Xu", "user": "explcre", "type": "user"}, "name": "Pengcheng Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:21.447Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d0", "name": "Boyang Zhong", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d1", "name": "Zehong Zhao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d2", "name": "Gaoyun Fang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d3", "name": "John Kitaoka", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d4", "name": "Yile Xu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d5", "name": "Hua Xu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d6", "name": "Kenton Blacutt", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d7", "name": "Tin Nguyen", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d8", "name": "Siyuan Song", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d9", "name": "Haoran Sun", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5da", "name": "Shaoyue Wen", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5db", "name": "Linyang He", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5dc", "name": "Runming Wang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5dd", "name": "Yanzhi Wang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5de", "name": "Mengyue Yang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5df", "name": "Ziqiao Ma", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e0", "name": "Rapha\u00ebl Milli\u00e8re", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e1", "name": "Freda Shi", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e2", "name": "Nuno Vasconcelos", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e3", "name": "Daniel Khashabi", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e4", "name": "Alan Yuille", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e5", "name": "Yilun Du", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e6", "name": "Ziming Liu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e7", "name": "Bo Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e8", "name": "Dahua Lin", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e9", "name": "Ziwei Liu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ea", "name": "Vikash Kumar", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5eb", "name": "Yijiang Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ec", "user": {"_id": "6626a471430a124253f197c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6626a471430a124253f197c8/uVEk5nnW-bS6-no0rQ7Wh.png", "isPro": false, "fullname": "yl-1993", "user": "yl-1993", "type": "user"}, "name": "Lei Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:17.400Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ed", "user": {"_id": "652d06833b5997ed71ce5c46", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xZTXEcnEogEmBm_ledJQr.jpeg", "isPro": false, "fullname": "Zhongang Cai", "user": "caizhongang", "type": "user"}, "name": "Zhongang Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:06.679Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ee", "user": {"_id": "6793f65033629a5fa8ae47b5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/h11aIy3Yuw0kLCil5yeOt.png", "isPro": false, "fullname": "Hokin Deng", "user": "Hokin", "type": "user"}, "name": "Hokin Deng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:49:58.948Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IjK1EcVGF8x-SG3ymPI6T.mp4"], "publishedAt": "2026-02-23T18:59:41.000Z", "submittedOnDailyAt": "2026-02-24T01:14:31.428Z", "title": "A Very Big Video Reasoning Suite", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .", "upvotes": 302, "discussionId": "699d1e7b4e37ec6dfa1bc5ef", "projectPage": "https://video-reason.com/", "ai_summary": "A large-scale video reasoning dataset and benchmark are introduced to study video intelligence capabilities beyond visual quality, enabling systematic analysis of spatiotemporal reasoning and generalization across diverse tasks.", "ai_keywords": ["video reasoning", "spatiotemporal consistency", "emergent generalization", "video reasoning benchmark", "video reasoning dataset"], "organization": {"_id": "6986a6f58d72821326efbfbb", "name": "Video-Reason", "fullname": "Video-Reason", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6793f65033629a5fa8ae47b5/7JFt2ReogqVi_udM_OHWG.jpeg"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-23T13:59:41.000Z", "title": "A Very Big Video Reasoning Suite", "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IjK1EcVGF8x-SG3ymPI6T.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20159.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 238, "isUserFollowing": false}, "organization": {"_id": "6986a6f58d72821326efbfbb", "name": "Video-Reason", "fullname": "Video-Reason", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6793f65033629a5fa8ae47b5/7JFt2ReogqVi_udM_OHWG.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.21193", "authors": [{"_id": "699e72b4dfbcf0b800aecb63", "name": "Renjie Pi", "hidden": false}, {"_id": "699e72b4dfbcf0b800aecb64", "name": "Grace Lam", "hidden": false}, {"_id": "699e72b4dfbcf0b800aecb65", "name": "Mohammad Shoeybi", "hidden": false}, {"_id": "699e72b4dfbcf0b800aecb66", "name": "Pooya Jannaty", "hidden": false}, {"_id": "699e72b4dfbcf0b800aecb67", "name": "Bryan Catanzaro", "hidden": false}, {"_id": "699e72b4dfbcf0b800aecb68", "name": "Wei Ping", "hidden": false}], "publishedAt": "2026-02-24T18:51:04.000Z", "submittedOnDailyAt": "2026-02-25T01:39:40.130Z", "title": "On Data Engineering for Scaling LLM Terminal Capabilities", "submittedOnDailyBy": {"_id": "63f45b8d520c14618930d175", "avatarUrl": "/avatars/42b3aaf50748a25e4a596fc57ab1306d.svg", "isPro": false, "fullname": "renjie", "user": "renjiepi", "type": "user"}, "summary": "Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.", "upvotes": 60, "discussionId": "699e72b5dfbcf0b800aecb69", "projectPage": "https://huggingface.co/collections/nvidia/nemotron-terminal", "ai_summary": "Researchers developed a synthetic task generation pipeline and analyzed data strategies to improve terminal agent performance, creating a large-scale dataset and models that outperform larger counterparts on benchmark tests.", "ai_keywords": ["large language models", "terminal agents", "data engineering practices", "synthetic task generation", "Terminal-Task-Gen", "Terminal-Corpus", "Nemotron-Terminal", "Terminal-Bench 2.0", "curriculum learning", "long context training", "scaling behavior"], "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-24T13:51:04.000Z", "title": "On Data Engineering for Scaling LLM Terminal Capabilities", "summary": "Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21193.png", "numComments": 2, "submittedBy": {"_id": "63f45b8d520c14618930d175", "avatarUrl": "/avatars/42b3aaf50748a25e4a596fc57ab1306d.svg", "fullname": "renjie", "name": "renjiepi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.18283", "authors": [{"_id": "699d32bf4e37ec6dfa1bc690", "name": "Lei Xin", "hidden": false}, {"_id": "699d32bf4e37ec6dfa1bc691", "name": "Yuhao Zheng", "hidden": false}, {"_id": "699d32bf4e37ec6dfa1bc692", "name": "Ke Cheng", "hidden": false}, {"_id": "699d32bf4e37ec6dfa1bc693", "user": {"_id": "652fc2605615e57807e3db19", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652fc2605615e57807e3db19/kbRcpR0YFQnU3IlziqCHf.png", "isPro": false, "fullname": "Changjiang Jiang", "user": "arnodjiang", "type": "user"}, "name": "Changjiang Jiang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-26T10:03:02.605Z", "hidden": false}, {"_id": "699d32bf4e37ec6dfa1bc694", "name": "Zifan Zhang", "hidden": false}, {"_id": "699d32bf4e37ec6dfa1bc695", "name": "Fanhu Zeng", "hidden": false}], "publishedAt": "2026-02-20T15:11:40.000Z", "submittedOnDailyAt": "2026-02-26T01:11:59.076Z", "title": "HyTRec: A Hybrid Temporal-Aware Attention Architecture for Long Behavior Sequential Recommendation", "submittedOnDailyBy": {"_id": "64107c7df52d7eb22e062956", "avatarUrl": "/avatars/7b1cee9a2b8454fedfbd4c3d1df9865c.svg", "isPro": false, "fullname": "Yuhao Zheng", "user": "yhzheng1031", "type": "user"}, "summary": "Modeling long sequences of user behaviors has emerged as a critical frontier in generative recommendation. However, existing solutions face a dilemma: linear attention mechanisms achieve efficiency at the cost of retrieval precision due to limited state capacity, while softmax attention suffers from prohibitive computational overhead. To address this challenge, we propose HyTRec, a model featuring a Hybrid Attention architecture that explicitly decouples long-term stable preferences from short-term intent spikes. By assigning massive historical sequences to a linear attention branch and reserving a specialized softmax attention branch for recent interactions, our approach restores precise retrieval capabilities within industrial-scale contexts involving ten thousand interactions. To mitigate the lag in capturing rapid interest drifts within the linear layers, we furthermore design Temporal-Aware Delta Network (TADN) to dynamically upweight fresh behavioral signals while effectively suppressing historical noise. Empirical results on industrial-scale datasets confirm the superiority that our model maintains linear inference speed and outperforms strong baselines, notably delivering over 8% improvement in Hit Rate for users with ultra-long sequences with great efficiency.", "upvotes": 46, "discussionId": "699d32bf4e37ec6dfa1bc696", "ai_summary": "HyTRec addresses the challenge of modeling long user behavior sequences by combining linear and softmax attention mechanisms with a temporal-aware delta network to balance efficiency and retrieval precision.", "ai_keywords": ["Hybrid Attention", "linear attention", "softmax attention", "long-term stable preferences", "short-term intent spikes", "temporal-aware delta network", "TADN", "Hit Rate"], "organization": {"_id": "6350bdf559bfa9a85d42fea4", "name": "WuhanUniversity", "fullname": "Wuhan Univeristy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6350bd20aaee2ec378dfe506/Bu1Fwz4dAwjwzWv-vZ2FN.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-20T10:11:40.000Z", "title": "HyTRec: A Hybrid Temporal-Aware Attention Architecture for Long Behavior Sequential Recommendation", "summary": "Modeling long sequences of user behaviors has emerged as a critical frontier in generative recommendation. However, existing solutions face a dilemma: linear attention mechanisms achieve efficiency at the cost of retrieval precision due to limited state capacity, while softmax attention suffers from prohibitive computational overhead. To address this challenge, we propose HyTRec, a model featuring a Hybrid Attention architecture that explicitly decouples long-term stable preferences from short-term intent spikes. By assigning massive historical sequences to a linear attention branch and reserving a specialized softmax attention branch for recent interactions, our approach restores precise retrieval capabilities within industrial-scale contexts involving ten thousand interactions. To mitigate the lag in capturing rapid interest drifts within the linear layers, we furthermore design Temporal-Aware Delta Network (TADN) to dynamically upweight fresh behavioral signals while effectively suppressing historical noise. Empirical results on industrial-scale datasets confirm the superiority that our model maintains linear inference speed and outperforms strong baselines, notably delivering over 8% improvement in Hit Rate for users with ultra-long sequences with great efficiency.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18283.png", "numComments": 1, "submittedBy": {"_id": "64107c7df52d7eb22e062956", "avatarUrl": "/avatars/7b1cee9a2b8454fedfbd4c3d1df9865c.svg", "fullname": "Yuhao Zheng", "name": "yhzheng1031", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "6350bdf559bfa9a85d42fea4", "name": "WuhanUniversity", "fullname": "Wuhan Univeristy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6350bd20aaee2ec378dfe506/Bu1Fwz4dAwjwzWv-vZ2FN.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.18532", "authors": [{"_id": "699d29bf4e37ec6dfa1bc66d", "user": {"_id": "61dd2b7389dddd97daead12f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61dd2b7389dddd97daead12f/xUL7YK3Mtvzz4rEhorFKF.jpeg", "isPro": false, "fullname": "Xiao-Ming Wu", "user": "DravenALG", "type": "user"}, "name": "Xiao-Ming Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:14.717Z", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc66e", "name": "Bin Fan", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc66f", "name": "Kang Liao", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc670", "name": "Jian-Jian Jiang", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc671", "name": "Runze Yang", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc672", "name": "Yihang Luo", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc673", "name": "Zhonghua Wu", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc674", "name": "Wei-Shi Zheng", "hidden": false}, {"_id": "699d29bf4e37ec6dfa1bc675", "name": "Chen Change Loy", "hidden": false}], "publishedAt": "2026-02-20T09:26:17.000Z", "submittedOnDailyAt": "2026-02-24T07:48:05.528Z", "title": "VLANeXt: Recipes for Building Strong VLA Models", "submittedOnDailyBy": {"_id": "61dd2b7389dddd97daead12f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61dd2b7389dddd97daead12f/xUL7YK3Mtvzz4rEhorFKF.jpeg", "isPro": false, "fullname": "Xiao-Ming Wu", "user": "DravenALG", "type": "user"}, "summary": "Following the rise of large foundation models, Vision-Language-Action models (VLAs) emerged, leveraging strong visual and language understanding for general-purpose policy learning. Yet, the current VLA landscape remains fragmented and exploratory. Although many groups have proposed their own VLA models, inconsistencies in training protocols and evaluation settings make it difficult to identify which design choices truly matter. To bring structure to this evolving space, we reexamine the VLA design space under a unified framework and evaluation setup. Starting from a simple VLA baseline similar to RT-2 and OpenVLA, we systematically dissect design choices along three dimensions: foundational components, perception essentials, and action modelling perspectives. From this study, we distill 12 key findings that together form a practical recipe for building strong VLA models. The outcome of this exploration is a simple yet effective model, VLANeXt. VLANeXt outperforms prior state-of-the-art methods on the LIBERO and LIBERO-plus benchmarks and demonstrates strong generalization in real-world experiments. We will release a unified, easy-to-use codebase that serves as a common platform for the community to reproduce our findings, explore the design space, and build new VLA variants on top of a shared foundation.", "upvotes": 39, "discussionId": "699d29c04e37ec6dfa1bc676", "projectPage": "https://dravenalg.github.io/VLANeXt/", "githubRepo": "https://github.com/DravenALG/VLANeXt", "githubRepoAddedBy": "user", "ai_summary": "Vision-Language-Action models are systematically analyzed and optimized through a unified framework, resulting in the VLANeXt model that achieves superior performance on benchmark tasks and demonstrates strong real-world generalization.", "ai_keywords": ["Vision-Language-Action models", "foundation models", "policy learning", "VLA design space", "RT-2", "OpenVLA", "perception essentials", "action modelling perspectives", "LIBERO", "LIBERO-plus", "VLANeXt"], "githubStars": 43, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-20T04:26:17.000Z", "title": "VLANeXt: Recipes for Building Strong VLA Models", "summary": "Following the rise of large foundation models, Vision-Language-Action models (VLAs) emerged, leveraging strong visual and language understanding for general-purpose policy learning. Yet, the current VLA landscape remains fragmented and exploratory. Although many groups have proposed their own VLA models, inconsistencies in training protocols and evaluation settings make it difficult to identify which design choices truly matter. To bring structure to this evolving space, we reexamine the VLA design space under a unified framework and evaluation setup. Starting from a simple VLA baseline similar to RT-2 and OpenVLA, we systematically dissect design choices along three dimensions: foundational components, perception essentials, and action modelling perspectives. From this study, we distill 12 key findings that together form a practical recipe for building strong VLA models. The outcome of this exploration is a simple yet effective model, VLANeXt. VLANeXt outperforms prior state-of-the-art methods on the LIBERO and LIBERO-plus benchmarks and demonstrates strong generalization in real-world experiments. We will release a unified, easy-to-use codebase that serves as a common platform for the community to reproduce our findings, explore the design space, and build new VLA variants on top of a shared foundation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.18532.png", "numComments": 1, "submittedBy": {"_id": "61dd2b7389dddd97daead12f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61dd2b7389dddd97daead12f/xUL7YK3Mtvzz4rEhorFKF.jpeg", "fullname": "Xiao-Ming Wu", "name": "DravenALG", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.19672", "authors": [{"_id": "699d407c4e37ec6dfa1bc6a7", "user": {"_id": "651651f5d93a51ceda3021c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651651f5d93a51ceda3021c3/FE2uGpTKBRWMKTDBv1H-g.png", "isPro": false, "fullname": "Jiayu (Mila) Wang", "user": "MilaWang", "type": "user"}, "name": "Jiayu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:49:47.846Z", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6a8", "name": "Yifei Ming", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6a9", "name": "Zixuan Ke", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6aa", "name": "Shafiq Joty", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6ab", "name": "Aws Albarghouthi", "hidden": false}, {"_id": "699d407c4e37ec6dfa1bc6ac", "name": "Frederic Sala", "hidden": false}], "publishedAt": "2026-02-23T10:17:25.000Z", "submittedOnDailyAt": "2026-02-24T03:44:00.879Z", "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer", "submittedOnDailyBy": {"_id": "651651f5d93a51ceda3021c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651651f5d93a51ceda3021c3/FE2uGpTKBRWMKTDBv1H-g.png", "isPro": false, "fullname": "Jiayu (Mila) Wang", "user": "MilaWang", "type": "user"}, "summary": "Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.", "upvotes": 30, "discussionId": "699d407c4e37ec6dfa1bc6ad", "ai_summary": "SkillOrchestra presents a skill-aware orchestration framework that improves compound AI system performance through fine-grained skill modeling and efficient agent selection, achieving superior results with significantly reduced learning costs compared to reinforcement learning-based methods.", "ai_keywords": ["compound AI systems", "orchestration", "routing policy", "reinforcement learning", "skill modeling", "agent-specific competence", "performance-cost trade-off", "multi-turn scenarios", "routing collapse", "end-to-end learning"], "organization": {"_id": "61d090ec03bc10eb8e1c2970", "name": "uw-madison", "fullname": "University of Wisconsin - Madison", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-23T05:17:25.000Z", "title": "SkillOrchestra: Learning to Route Agents via Skill Transfer", "summary": "Compound AI systems promise capabilities beyond those of individual models, yet their success depends critically on effective orchestration. Existing routing approaches face two limitations: (1) input-level routers make coarse query-level decisions that ignore evolving task requirements; (2) RL-trained orchestrators are expensive to adapt and often suffer from routing collapse, repeatedly invoking one strong but costly option in multi-turn scenarios. We introduce SkillOrchestra, a framework for skill-aware orchestration. Instead of directly learning a routing policy end-to-end, SkillOrchestra learns fine-grained skills from execution experience and models agent-specific competence and cost under those skills. At deployment, the orchestrator infers the skill demands of the current interaction and selects agents that best satisfy them under an explicit performance-cost trade-off. Extensive experiments across ten benchmarks demonstrate that SkillOrchestra outperforms SoTA RL-based orchestrators by up to 22.5% with 700x and 300x learning cost reduction compared to Router-R1 and ToolOrchestra, respectively. These results show that explicit skill modeling enables scalable, interpretable, and sample-efficient orchestration, offering a principled alternative to data-intensive RL-based approaches. The code is available at: https://github.com/jiayuww/SkillOrchestra.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19672.png", "numComments": 1, "submittedBy": {"_id": "651651f5d93a51ceda3021c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651651f5d93a51ceda3021c3/FE2uGpTKBRWMKTDBv1H-g.png", "fullname": "Jiayu (Mila) Wang", "name": "MilaWang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "61d090ec03bc10eb8e1c2970", "name": "uw-madison", "fullname": "University of Wisconsin - Madison", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/IYmUaLUc_rDVNC6F7-k8M.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.20739", "authors": [{"_id": "699e674ddfbcf0b800aecae9", "user": {"_id": "62c66504031996c36c86976a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png", "isPro": false, "fullname": "steve z", "user": "stzhao", "type": "user"}, "name": "Shitian Zhao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-25T17:29:58.018Z", "hidden": false}, {"_id": "699e674ddfbcf0b800aecaea", "name": "Shaoheng Lin", "hidden": false}, {"_id": "699e674ddfbcf0b800aecaeb", "name": "Ming Li", "hidden": false}, {"_id": "699e674ddfbcf0b800aecaec", "user": {"_id": "67ff7f687351095d4b606b84", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67ff7f687351095d4b606b84/KhNPmbBC3zghuP5h1MK-c.png", "isPro": false, "fullname": "Haoquan Zhang", "user": "haoquan03", "type": "user"}, "name": "Haoquan Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-25T17:29:51.660Z", "hidden": false}, {"_id": "699e674ddfbcf0b800aecaed", "name": "Wenshuo Peng", "hidden": false}, {"_id": "699e674ddfbcf0b800aecaee", "name": "Kaipeng Zhang", "hidden": false}, {"_id": "699e674ddfbcf0b800aecaef", "name": "Chen Wei", "hidden": false}], "publishedAt": "2026-02-24T10:08:33.000Z", "submittedOnDailyAt": "2026-02-25T00:41:13.309Z", "title": "PyVision-RL: Forging Open Agentic Vision Models via RL", "submittedOnDailyBy": {"_id": "62c66504031996c36c86976a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png", "isPro": false, "fullname": "steve z", "user": "stzhao", "type": "user"}, "summary": "Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.", "upvotes": 22, "discussionId": "699e674edfbcf0b800aecaf0", "projectPage": "https://agent-x.space/pyvision-rl/", "githubRepo": "https://github.com/agents-x-project/PyVision-RL", "githubRepoAddedBy": "user", "ai_summary": "PyVision-RL framework addresses interaction collapse in multimodal models through enhanced reinforcement learning techniques and efficient video processing strategies.", "ai_keywords": ["reinforcement learning", "agentic multimodal models", "interaction collapse", "oversampling-filtering-ranking", "accumulative tool reward", "unified training pipeline", "on-demand context construction", "task-relevant frames", "visual token usage"], "githubStars": 43, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-24T05:08:33.000Z", "title": "PyVision-RL: Forging Open Agentic Vision Models via RL", "summary": "Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20739.png", "numComments": 1, "submittedBy": {"_id": "62c66504031996c36c86976a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png", "fullname": "steve z", "name": "stzhao", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.19313", "authors": [{"_id": "699d1f754e37ec6dfa1bc5ff", "name": "Shirui Chen", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc600", "name": "Cole Harrison", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc601", "name": "Ying-Chun Lee", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc602", "name": "Angela Jin Yang", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc603", "name": "Zhongzheng Ren", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc604", "name": "Lillian J. Ratliff", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc605", "name": "Jiafei Duan", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc606", "name": "Dieter Fox", "hidden": false}, {"_id": "699d1f754e37ec6dfa1bc607", "name": "Ranjay Krishna", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/ez9whqUgnhTTjrPZiPdzP.png"], "publishedAt": "2026-02-22T19:25:48.000Z", "submittedOnDailyAt": "2026-02-24T01:19:53.486Z", "title": "TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics", "submittedOnDailyBy": {"_id": "632b42626110e37dba3d5bcb", "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg", "isPro": false, "fullname": "Duan", "user": "Jiafei1224", "type": "user"}, "summary": "While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM's internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.", "upvotes": 21, "discussionId": "699d1f754e37ec6dfa1bc608", "projectPage": "https://topreward.github.io/webpage/", "githubRepo": "https://github.com/TOPReward/TOPReward", "githubRepoAddedBy": "user", "ai_summary": "TOPReward is a probabilistically grounded temporal value function that uses pretrained video Vision-Language Models to estimate robotic task progress through internal token logits, achieving superior performance in zero-shot evaluations across diverse real-world tasks.", "ai_keywords": ["Vision-Language-Action models", "Reinforcement Learning", "temporal value functions", "Vision-Language Models", "token logits", "Value-Order Correlation", "reward-aligned behavior cloning"], "githubStars": 1, "organization": {"_id": "5e70f3648ce3c604d78fe132", "name": "allenai", "fullname": "Ai2", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-22T14:25:48.000Z", "title": "TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics", "summary": "While Vision-Language-Action (VLA) models have seen rapid progress in pretraining, their advancement in Reinforcement Learning (RL) remains hampered by low sample efficiency and sparse rewards in real-world settings. Developing generalizable process reward models is essential for providing the fine-grained feedback necessary to bridge this gap, yet existing temporal value functions often fail to generalize beyond their training domains. We introduce TOPReward, a novel, probabilistically grounded temporal value function that leverages the latent world knowledge of pretrained video Vision-Language Models (VLMs) to estimate robotic task progress. Unlike prior methods that prompt VLMs to directly output progress values, which are prone to numerical misrepresentation, TOPReward extracts task progress directly from the VLM's internal token logits. In zero-shot evaluations across 130+ distinct real-world tasks and multiple robot platforms (e.g., Franka, YAM, SO-100/101), TOPReward achieves 0.947 mean Value-Order Correlation (VOC) on Qwen3-VL, dramatically outperforming the state-of-the-art GVL baseline which achieves near-zero correlation on the same open-source model. We further demonstrate that TOPReward serves as a versatile tool for downstream applications, including success detection and reward-aligned behavior cloning.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/632b42626110e37dba3d5bcb/ez9whqUgnhTTjrPZiPdzP.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.19313.png", "numComments": 1, "submittedBy": {"_id": "632b42626110e37dba3d5bcb", "avatarUrl": "/avatars/ca70a15def71ee84f4f149db5e954843.svg", "fullname": "Duan", "name": "Jiafei1224", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "5e70f3648ce3c604d78fe132", "name": "allenai", "fullname": "Ai2", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652db071b62cf1f8463221e2/CxxwFiaomTa1MCX_B7-pT.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.21015", "authors": [{"_id": "699e69fbdfbcf0b800aecafb", "user": {"_id": "63369da91ba5d5ece24118a4", "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg", "isPro": false, "fullname": "wuyuhao", "user": "mozhu", "type": "user"}, "name": "Yuhao Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-25T17:29:26.632Z", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecafc", "name": "Maojia Song", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecafd", "name": "Yihuai Lan", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecafe", "name": "Lei Wang", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecaff", "user": {"_id": "637f228152229c63921119c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg", "isPro": false, "fullname": "Zhiqiang Hu", "user": "Zhiqiang007", "type": "user"}, "name": "Zhiqiang Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-25T17:29:48.417Z", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecb00", "name": "Yao Xiao", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecb01", "user": {"_id": "660d17d6c9be0dcd31a30b3d", "avatarUrl": "/avatars/3743fe9b695c488ebe33f0d8fd607a8a.svg", "isPro": false, "fullname": "Zhou Heng", "user": "henggg", "type": "user"}, "name": "Heng Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-02-25T17:29:45.251Z", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecb02", "name": "Weihua Zheng", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecb03", "name": "Dylan Raharja", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecb04", "name": "Soujanya Poria", "hidden": false}, {"_id": "699e69fbdfbcf0b800aecb05", "name": "Roy Ka-Wei Lee", "hidden": false}], "publishedAt": "2026-02-24T15:33:02.000Z", "submittedOnDailyAt": "2026-02-25T00:55:31.629Z", "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning", "submittedOnDailyBy": {"_id": "637f228152229c63921119c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg", "isPro": false, "fullname": "Zhiqiang Hu", "user": "Zhiqiang007", "type": "user"}, "summary": "Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.", "upvotes": 20, "discussionId": "699e69fbdfbcf0b800aecb06", "projectPage": "https://social-ai-studio.github.io/CHAIN/", "githubRepo": "https://github.com/Social-AI-Studio/CHAIN", "githubRepoAddedBy": "user", "ai_summary": "Current vision-language models lack capability to understand physical structures and causal constraints needed for complex, interactive 3D tasks, as demonstrated by the CHAIN benchmark evaluating structured action planning under physical constraints.", "ai_keywords": ["Vision-Language Model", "diffusion-based models", "physical constraints", "causal constraints", "interactive 3D", "structured action sequences", "long-horizon planning"], "githubStars": 3, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-24T10:33:02.000Z", "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning", "summary": "Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21015.png", "numComments": 2, "submittedBy": {"_id": "637f228152229c63921119c3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg", "fullname": "Zhiqiang Hu", "name": "Zhiqiang007", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.21818", "authors": [{"_id": "699fba56ebfce7fbcca91b1a", "name": "Guibin Chen", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b1b", "name": "Dixuan Lin", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b1c", "name": "Jiangping Yang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b1d", "name": "Youqiang Zhang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b1e", "name": "Zhengcong Fei", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b1f", "name": "Debang Li", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b20", "name": "Sheng Chen", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b21", "name": "Chaofeng Ao", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b22", "name": "Nuo Pang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b23", "name": "Yiming Wang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b24", "name": "Yikun Dou", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b25", "name": "Zheng Chen", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b26", "name": "Mingyuan Fan", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b27", "name": "Tuanhui Li", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b28", "name": "Mingshan Chang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b29", "name": "Hao Zhang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b2a", "name": "Xiaopeng Sun", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b2b", "name": "Jingtao Xu", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b2c", "name": "Yuqiang Xie", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b2d", "name": "Jiahua Wang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b2e", "name": "Zhiheng Xu", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b2f", "name": "Weiming Xiong", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b30", "name": "Yuzhe Jin", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b31", "name": "Baoxuan Gu", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b32", "name": "Binjie Mao", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b33", "name": "Yunjie Yu", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b34", "name": "Jujie He", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b35", "name": "Yuhao Feng", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b36", "name": "Shiwen Tu", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b37", "name": "Chaojie Wang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b38", "name": "Rui Yan", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b39", "name": "Wei Shen", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b3a", "name": "Jingchen Wu", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b3b", "name": "Peng Zhao", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b3c", "name": "Xuanyue Zhong", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b3d", "name": "Zhuangzhuang Liu", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b3e", "name": "Kaifei Wang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b3f", "name": "Fuxiang Zhang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b40", "name": "Weikai Xu", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b41", "name": "Wenyan Liu", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b42", "name": "Binglu Zhang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b43", "name": "Yu Shen", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b44", "name": "Tianhui Xiong", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b45", "name": "Bin Peng", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b46", "name": "Liang Zeng", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b47", "name": "Xuchen Song", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b48", "name": "Haoxiang Guo", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b49", "name": "Peiyu Wang", "hidden": false}, {"_id": "699fba56ebfce7fbcca91b4a", "name": "Yahui Zhou", "hidden": false}], "publishedAt": "2026-02-25T11:47:00.000Z", "submittedOnDailyAt": "2026-02-26T00:43:38.210Z", "title": "SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "SkyReels V4 is a unified multi modal video foundation model for joint video audio generation, inpainting, and editing. The model adopts a dual stream Multimodal Diffusion Transformer (MMDiT) architecture, where one branch synthesizes video and the other generates temporally aligned audio, while sharing a powerful text encoder based on the Multimodal Large Language Models (MMLM). SkyReels V4 accepts rich multi modal instructions, including text, images, video clips, masks, and audio references. By combining the MMLMs multi modal instruction following capability with in context learning in the video branch MMDiT, the model can inject fine grained visual guidance under complex conditioning, while the audio branch MMDiT simultaneously leverages audio references to guide sound generation. On the video side, we adopt a channel concatenation formulation that unifies a wide range of inpainting style tasks, such as image to video, video extension, and video editing under a single interface, and naturally extends to vision referenced inpainting and editing via multi modal prompts. SkyReels V4 supports up to 1080p resolution, 32 FPS, and 15 second duration, enabling high fidelity, multi shot, cinema level video generation with synchronized audio. To make such high resolution, long-duration generation computationally feasible, we introduce an efficiency strategy: Joint generation of low resolution full sequences and high-resolution keyframes, followed by dedicated super-resolution and frame interpolation models. To our knowledge, SkyReels V4 is the first video foundation model that simultaneously supports multi-modal input, joint video audio generation, and a unified treatment of generation, inpainting, and editing, while maintaining strong efficiency and quality at cinematic resolutions and durations.", "upvotes": 19, "discussionId": "699fba56ebfce7fbcca91b4b", "ai_summary": "SkyReels V4 is a unified multimodal video foundation model that generates, edits, and inpaints video and audio simultaneously using a dual-stream architecture with shared text encoding and efficient high-resolution processing.", "ai_keywords": ["Multimodal Diffusion Transformer", "MMDiT", "Multimodal Large Language Models", "MMLM", "video audio generation", "video inpainting", "video editing", "channel concatenation formulation", "joint generation", "super-resolution", "frame interpolation"], "organization": {"_id": "6522615d9334173c627b0efa", "name": "Skywork", "fullname": "Skywork", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64535b71bcbd25618f7655da/AvtJ4GuPAyhLxl2-leVt6.jpeg"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-25T06:47:00.000Z", "title": "SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model", "summary": "SkyReels V4 is a unified multi modal video foundation model for joint video audio generation, inpainting, and editing. The model adopts a dual stream Multimodal Diffusion Transformer (MMDiT) architecture, where one branch synthesizes video and the other generates temporally aligned audio, while sharing a powerful text encoder based on the Multimodal Large Language Models (MMLM). SkyReels V4 accepts rich multi modal instructions, including text, images, video clips, masks, and audio references. By combining the MMLMs multi modal instruction following capability with in context learning in the video branch MMDiT, the model can inject fine grained visual guidance under complex conditioning, while the audio branch MMDiT simultaneously leverages audio references to guide sound generation. On the video side, we adopt a channel concatenation formulation that unifies a wide range of inpainting style tasks, such as image to video, video extension, and video editing under a single interface, and naturally extends to vision referenced inpainting and editing via multi modal prompts. SkyReels V4 supports up to 1080p resolution, 32 FPS, and 15 second duration, enabling high fidelity, multi shot, cinema level video generation with synchronized audio. To make such high resolution, long-duration generation computationally feasible, we introduce an efficiency strategy: Joint generation of low resolution full sequences and high-resolution keyframes, followed by dedicated super-resolution and frame interpolation models. To our knowledge, SkyReels V4 is the first video foundation model that simultaneously supports multi-modal input, joint video audio generation, and a unified treatment of generation, inpainting, and editing, while maintaining strong efficiency and quality at cinematic resolutions and durations.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21818.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 240, "isUserFollowing": false}, "organization": {"_id": "6522615d9334173c627b0efa", "name": "Skywork", "fullname": "Skywork", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64535b71bcbd25618f7655da/AvtJ4GuPAyhLxl2-leVt6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.21204", "authors": [{"_id": "699e6e60dfbcf0b800aecb16", "name": "Junchen Liu", "hidden": false}, {"_id": "699e6e60dfbcf0b800aecb17", "name": "Sven Elflein", "hidden": false}, {"_id": "699e6e60dfbcf0b800aecb18", "name": "Or Litany", "hidden": false}, {"_id": "699e6e60dfbcf0b800aecb19", "name": "Zan Gojcic", "hidden": false}, {"_id": "699e6e60dfbcf0b800aecb1a", "name": "Ruilong Li", "hidden": false}], "publishedAt": "2026-02-24T18:59:30.000Z", "submittedOnDailyAt": "2026-02-25T03:15:10.854Z", "title": "Test-Time Training with KV Binding Is Secretly Linear Attention", "submittedOnDailyBy": {"_id": "663a9ae58cf658ffaf3d02e5", "avatarUrl": "/avatars/aa35668dc088e675e794b9ceb935c56f.svg", "isPro": false, "fullname": "Junchen Liu", "user": "JunchenLiu", "type": "user"}, "summary": "Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.", "upvotes": 19, "discussionId": "699e6e61dfbcf0b800aecb1b", "projectPage": "https://research.nvidia.com/labs/sil/projects/tttla/", "ai_summary": "Test-time training is reinterpreted as learned linear attention rather than memorization, offering architectural simplifications and improved efficiency.", "ai_keywords": ["test-time training", "KV binding", "online meta-learning", "learned linear attention", "sequence modeling layer", "linear attention operator", "architectural simplifications", "parallel formulations", "representational capacity"], "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-24T13:59:30.000Z", "title": "Test-Time Training with KV Binding Is Secretly Linear Attention", "summary": "Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.21204.png", "numComments": 1, "submittedBy": {"_id": "663a9ae58cf658ffaf3d02e5", "avatarUrl": "/avatars/aa35668dc088e675e794b9ceb935c56f.svg", "fullname": "Junchen Liu", "name": "JunchenLiu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2602.20159", "authors": [{"_id": "699d1e7a4e37ec6dfa1bc5b7", "user": {"_id": "67f87529318a17cc80365190", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/p1YkznAT-op1Cg-vBoFw7.png", "isPro": false, "fullname": "Maijunxian Wang", "user": "Mark7121983123", "type": "user"}, "name": "Maijunxian Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:19.409Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5b8", "user": {"_id": "65b74305e602b6c2c9125480", "avatarUrl": "/avatars/d36909e0f245bfeb632a4afc9d3fceca.svg", "isPro": false, "fullname": "wang ruisi", "user": "wruisi", "type": "user"}, "name": "Ruisi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:01.779Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5b9", "name": "Juyi Lin", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ba", "name": "Ran Ji", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bb", "name": "Thadd\u00e4us Wiedemer", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bc", "name": "Qingying Gao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bd", "name": "Dezhi Luo", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5be", "name": "Yaoyao Qian", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5bf", "name": "Lianyu Huang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c0", "name": "Zelong Hong", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c1", "name": "Jiahui Ge", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c2", "name": "Qianli Ma", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c3", "name": "Hang He", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c4", "user": {"_id": "659d2dff20cf0b934bbee513", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659d2dff20cf0b934bbee513/9e9R852Zr2R82h64eUUQl.jpeg", "isPro": false, "fullname": "Yifan Zhou", "user": "yingmanji", "type": "user"}, "name": "Yifan Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:04.030Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c5", "name": "Lingzi Guo", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c6", "name": "Lantao Mei", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c7", "name": "Jiachen Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c8", "user": {"_id": "6532c3018bde2fae19578587", "avatarUrl": "/avatars/7231538f3d682a1e7b80e15ea91b2a97.svg", "isPro": false, "fullname": "Hanwen Xing", "user": "Hudx111", "type": "user"}, "name": "Hanwen Xing", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:09.132Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5c9", "name": "Tianqi Zhao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ca", "name": "Fengyuan Yu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cb", "name": "Weihang Xiao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cc", "name": "Yizheng Jiao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cd", "name": "Jianheng Hou", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ce", "name": "Danyang Zhang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5cf", "user": {"_id": "65d854e5d8134b93774b4080", "avatarUrl": "/avatars/0ff1db8c13095f420a856212d64f88ca.svg", "isPro": false, "fullname": "Pengcheng Xu", "user": "explcre", "type": "user"}, "name": "Pengcheng Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:21.447Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d0", "name": "Boyang Zhong", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d1", "name": "Zehong Zhao", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d2", "name": "Gaoyun Fang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d3", "name": "John Kitaoka", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d4", "name": "Yile Xu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d5", "name": "Hua Xu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d6", "name": "Kenton Blacutt", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d7", "name": "Tin Nguyen", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d8", "name": "Siyuan Song", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5d9", "name": "Haoran Sun", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5da", "name": "Shaoyue Wen", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5db", "name": "Linyang He", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5dc", "name": "Runming Wang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5dd", "name": "Yanzhi Wang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5de", "name": "Mengyue Yang", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5df", "name": "Ziqiao Ma", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e0", "name": "Rapha\u00ebl Milli\u00e8re", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e1", "name": "Freda Shi", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e2", "name": "Nuno Vasconcelos", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e3", "name": "Daniel Khashabi", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e4", "name": "Alan Yuille", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e5", "name": "Yilun Du", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e6", "name": "Ziming Liu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e7", "name": "Bo Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e8", "name": "Dahua Lin", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5e9", "name": "Ziwei Liu", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ea", "name": "Vikash Kumar", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5eb", "name": "Yijiang Li", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ec", "user": {"_id": "6626a471430a124253f197c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6626a471430a124253f197c8/uVEk5nnW-bS6-no0rQ7Wh.png", "isPro": false, "fullname": "yl-1993", "user": "yl-1993", "type": "user"}, "name": "Lei Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:17.400Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ed", "user": {"_id": "652d06833b5997ed71ce5c46", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xZTXEcnEogEmBm_ledJQr.jpeg", "isPro": false, "fullname": "Zhongang Cai", "user": "caizhongang", "type": "user"}, "name": "Zhongang Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:50:06.679Z", "hidden": false}, {"_id": "699d1e7a4e37ec6dfa1bc5ee", "user": {"_id": "6793f65033629a5fa8ae47b5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/h11aIy3Yuw0kLCil5yeOt.png", "isPro": false, "fullname": "Hokin Deng", "user": "Hokin", "type": "user"}, "name": "Hokin Deng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-24T09:49:58.948Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IjK1EcVGF8x-SG3ymPI6T.mp4"], "publishedAt": "2026-02-23T18:59:41.000Z", "submittedOnDailyAt": "2026-02-24T01:14:31.428Z", "title": "A Very Big Video Reasoning Suite", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .", "upvotes": 302, "discussionId": "699d1e7b4e37ec6dfa1bc5ef", "projectPage": "https://video-reason.com/", "ai_summary": "A large-scale video reasoning dataset and benchmark are introduced to study video intelligence capabilities beyond visual quality, enabling systematic analysis of spatiotemporal reasoning and generalization across diverse tasks.", "ai_keywords": ["video reasoning", "spatiotemporal consistency", "emergent generalization", "video reasoning benchmark", "video reasoning dataset"], "organization": {"_id": "6986a6f58d72821326efbfbb", "name": "Video-Reason", "fullname": "Video-Reason", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6793f65033629a5fa8ae47b5/7JFt2ReogqVi_udM_OHWG.jpeg"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-23T13:59:41.000Z", "title": "A Very Big Video Reasoning Suite", "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/IjK1EcVGF8x-SG3ymPI6T.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.20159.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 238, "isUserFollowing": false}, "organization": {"_id": "6986a6f58d72821326efbfbb", "name": "Video-Reason", "fullname": "Video-Reason", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6793f65033629a5fa8ae47b5/7JFt2ReogqVi_udM_OHWG.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.05400", "authors": [{"_id": "698b396b1b2dc6b37d61b4be", "user": {"_id": "66968099c952e09a4cb29f78", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66968099c952e09a4cb29f78/n90NI2R3E9_RqCyMjDCQF.webp", "isPro": false, "fullname": "Wang", "user": "Steven-Shaobo", "type": "user"}, "name": "Shaobo Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:57.815Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4bf", "user": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "isPro": false, "fullname": "Xuan Ouyang", "user": "YoungXuan", "type": "user"}, "name": "Xuan Ouyang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:55.631Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c0", "user": {"_id": "6518a144a28f86d3e9e67c34", "avatarUrl": "/avatars/f2aed39e971cffe6c9d0b9c2f7a0df70.svg", "isPro": false, "fullname": "Tianyi Xu", "user": "tianyi0216", "type": "user"}, "name": "Tianyi Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:53.605Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c1", "name": "Yuzheng Hu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c2", "name": "Jialin Liu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c3", "name": "Guo Chen", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c4", "name": "Tianyu Zhang", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c5", "name": "Junhao Zheng", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c6", "name": "Kexin Yang", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c7", "name": "Xingzhang Ren", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c8", "name": "Dayiheng Liu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c9", "name": "Linfeng Zhang", "hidden": false}], "publishedAt": "2026-02-05T07:34:23.000Z", "submittedOnDailyAt": "2026-02-11T02:09:03.945Z", "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "submittedOnDailyBy": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "isPro": false, "fullname": "Xuan Ouyang", "user": "YoungXuan", "type": "user"}, "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "upvotes": 279, "discussionId": "698b396b1b2dc6b37d61b4ca", "ai_summary": "OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.", "ai_keywords": ["data selection", "optimizer-induced update space", "effective updates", "stable in-distribution proxy", "Ghost technique", "CountSketch", "Boltzmann sampling", "pre-training", "GPT-2", "Qwen3-8B-Base", "FineWeb", "FineWeb-Edu", "SciencePedia"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-05T02:34:23.000Z", "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05400.png", "numComments": 2, "submittedBy": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "fullname": "Xuan Ouyang", "name": "YoungXuan", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 23, "isUserFollowing": false}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.10388", "authors": [{"_id": "698d3bd265c0d15a6d16200e", "user": {"_id": "6951c555b519522f565dfd0c", "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg", "isPro": false, "fullname": "ZhongzhiLi", "user": "Zhongzhi1228", "type": "user"}, "name": "Zhongzhi Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:57:05.580Z", "hidden": false}, {"_id": "698d3bd265c0d15a6d16200f", "name": "Xuansheng Wu", "hidden": false}, {"_id": "698d3bd265c0d15a6d162010", "name": "Yijiang Li", "hidden": false}, {"_id": "698d3bd265c0d15a6d162011", "name": "Lijie Hu", "hidden": false}, {"_id": "698d3bd265c0d15a6d162012", "name": "Ninghao Liu", "hidden": false}], "publishedAt": "2026-02-11T00:23:13.000Z", "submittedOnDailyAt": "2026-02-16T02:31:34.708Z", "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs", "submittedOnDailyBy": {"_id": "6951c555b519522f565dfd0c", "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg", "isPro": false, "fullname": "ZhongzhiLi", "user": "Zhongzhi1228", "type": "user"}, "summary": "The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.", "upvotes": 200, "discussionId": "698d3bd265c0d15a6d162013", "projectPage": "https://website-sigma-three-35.vercel.app/", "githubRepo": "https://github.com/Zhongzhi660/FAC-Synthesis", "githubRepoAddedBy": "user", "ai_summary": "Feature Activation Coverage measures data diversity in an interpretable feature space and enables diversity-driven data synthesis that improves downstream performance across multiple language model architectures.", "ai_keywords": ["Feature Activation Coverage", "sparse autoencoder", "data diversity", "downstream performance", "instruction following", "toxicity detection", "reward modeling", "behavior steering", "cross-model knowledge transfer", "data-centric optimization"], "githubStars": 52, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-10T19:23:13.000Z", "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs", "summary": "The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10388.png", "numComments": 2, "submittedBy": {"_id": "6951c555b519522f565dfd0c", "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg", "fullname": "ZhongzhiLi", "name": "Zhongzhi1228", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.04705", "authors": [{"_id": "698424a7e34659da7e1f4e6f", "name": "Haifeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e70", "name": "Hua Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e71", "name": "Tian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e72", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4e73", "name": "Jing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e74", "name": "Dianhai Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e75", "name": "Yanjun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4e76", "name": "Jingzhou He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e77", "name": "Zhongjun He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e78", "name": "Dou Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e79", "name": "Qiwen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7a", "name": "Shuohuan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7b", "user": {"_id": "62cd9632342b1d5dab8df4c3", "avatarUrl": "/avatars/9080d20bb57a05a1eeb6800eba886cf9.svg", "isPro": false, "fullname": "Junyuan Shang", "user": "sjy1203", "type": "user"}, "name": "Junyuan Shang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:28.482Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7c", "user": {"_id": "67f37f78b36e82d366dedeec", "avatarUrl": "/avatars/678bb5891d5c2e80edc0799d2308a5d3.svg", "isPro": false, "fullname": "Max Zhenyu Zhang", "user": "max-zhenyu-zhang", "type": "user"}, "name": "Zhenyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:03.972Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7d", "name": "Yuchen Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7e", "name": "Jinle Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7f", "name": "Jiabin Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e80", "name": "Liang Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e81", "name": "Ruibiao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e82", "name": "Weichong Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4e83", "name": "Siyu Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e84", "name": "Dai Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4e85", "name": "Shikun Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e86", "name": "Siqi Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e87", "name": "Bolei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e88", "name": "Yan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e89", "name": "Zhenyu Jiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8a", "name": "Ruiqing Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8b", "name": "Zeyu Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8c", "name": "Qingqing Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8d", "name": "Kaipeng Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8e", "name": "Jiajun Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8f", "name": "Enlei Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e90", "name": "Guoxia Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e91", "name": "Yanlin Sha", "hidden": false}, {"_id": "698424a7e34659da7e1f4e92", "name": "Yi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e93", "name": "Yehan Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e94", "name": "Weijian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e95", "name": "Jiaxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e96", "name": "Zengfeng Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e97", "name": "Yingqi Qu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e98", "name": "Zhongli Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4e99", "name": "Zhengkun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9a", "name": "Xiyang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9b", "name": "Zixiang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9c", "name": "Xinchao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9d", "name": "Zhengjie Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9e", "name": "Dong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9f", "name": "Bingjin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea0", "name": "Yue Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea1", "name": "Xing Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea2", "name": "Shiwei Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea3", "name": "Qiao Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea4", "name": "Xinzhe Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea5", "name": "Shuangshuang Qiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea6", "name": "Baoshan Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea7", "name": "Bihong Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea8", "name": "Bin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea9", "name": "Bingquan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaa", "name": "Binhan Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eab", "name": "Binxiong Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4eac", "name": "Bo Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4ead", "name": "Bo Ke", "hidden": false}, {"_id": "698424a7e34659da7e1f4eae", "name": "Bo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaf", "name": "Bowen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb0", "name": "Boyan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb1", "name": "Boyang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb2", "name": "Caiji Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb3", "name": "Can Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb4", "name": "Chang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb5", "name": "Chao Pang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb6", "name": "Chao Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb7", "name": "Chaoyi Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb8", "name": "Chen Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb9", "name": "Cheng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4eba", "name": "Chenlin Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebb", "name": "Chun Gan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebc", "name": "Chunguang Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebd", "name": "Chuyu Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebe", "name": "Cuiyun Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebf", "name": "Dan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec0", "name": "Danlei Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec1", "name": "Danxiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec2", "name": "Dong Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec3", "name": "Dongbo Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec4", "name": "Dongdong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec5", "name": "Dongdong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec6", "name": "Dongxue Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec7", "name": "Fan Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec8", "name": "Fan Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec9", "name": "Fan Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eca", "name": "Fan Mo", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecb", "name": "Feisheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecc", "name": "Fengwei Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecd", "name": "Gangqiang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ece", "name": "Gaofeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecf", "name": "Gaopeng Yong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed0", "name": "Gexiao Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed1", "user": {"_id": "698419de94015f1e5eedacec", "avatarUrl": "/avatars/e80baa6f9efcd5e5d7cc9b93ac852c7b.svg", "isPro": false, "fullname": "Guan Wang", "user": "guanwcn", "type": "user"}, "name": "Guan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:38.213Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed2", "name": "Guangchen Ni", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed3", "name": "Guangshuo Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed4", "name": "Guanzhong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed5", "user": {"_id": "609cd5ab335f23cd2fa0f211", "avatarUrl": "/avatars/8331a7025a6aa4eabc5b6502bf8a0a63.svg", "isPro": false, "fullname": "Guihua Liu", "user": "LLLL", "type": "user"}, "name": "Guihua Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:31.029Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed6", "name": "Guishun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed7", "name": "Haibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed8", "name": "Haijian Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed9", "name": "Haipeng Ming", "hidden": false}, {"_id": "698424a7e34659da7e1f4eda", "name": "Haisu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4edb", "name": "Haiyang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4edc", "name": "Haiye Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4edd", "name": "Han Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ede", "name": "Hangting Lou", "hidden": false}, {"_id": "698424a7e34659da7e1f4edf", "name": "Hanwen Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee0", "name": "Hanzhi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee1", "name": "Hao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee2", "name": "Hao Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee3", "name": "Hao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee4", "name": "Hao Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee5", "name": "Haochen Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee6", "name": "Haodong Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee7", "name": "Haoshuang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee8", "name": "Haozhe Geng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee9", "name": "Heju Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4eea", "name": "Hong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eeb", "name": "Hongchen Xue", "hidden": false}, {"_id": "698424a7e34659da7e1f4eec", "name": "Hongen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eed", "name": "Honggeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eee", "name": "Hongji Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eef", "name": "Hongwei Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef0", "name": "Hongyang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef1", "name": "Hongyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef2", "name": "Hua Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef3", "name": "Huan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef4", "name": "Huan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef5", "name": "Huang He", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef6", "name": "Hui Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef7", "name": "Hui Zhong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef8", "name": "Huibin Ruan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef9", "name": "Jiafeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efa", "name": "Jiage Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efb", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efc", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efd", "name": "Jiajie Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efe", "name": "Jialin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eff", "name": "Jian Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f00", "name": "Jian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f01", "name": "Jianfeng Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f02", "name": "Jianguang Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f03", "name": "Jianhua Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f04", "name": "Jianye Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f05", "name": "Jiaodi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f06", "name": "Jiarui Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f07", "name": "Jiawei Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f08", "name": "Jiaxin Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f09", "name": "Jiaxuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0a", "name": "Jie Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0b", "name": "Jie Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0c", "name": "Jiefan Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0d", "name": "Jihan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0e", "name": "Jihua Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0f", "name": "Jing Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f10", "name": "Jing Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f11", "name": "Jing Yan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f12", "name": "Jingdong Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f13", "name": "Jingdong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f14", "name": "Jingjing Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f15", "name": "Jingyong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f16", "name": "Jinheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f17", "name": "Jinjin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f18", "name": "Jinliang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f19", "name": "Jinlin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1a", "name": "Jinnan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1b", "name": "Jixiang Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1c", "name": "Jiyi Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1d", "name": "Jiyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1e", "name": "Jun Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1f", "name": "Jun Xia", "hidden": false}, {"_id": "698424a7e34659da7e1f4f20", "name": "Jun Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f21", "name": "Junda Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f22", "name": "Junhao Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f23", "name": "Junhong Xiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f24", "name": "Junliang Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f25", "name": "Kai Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f26", "name": "Kailun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f27", "name": "Kairan Su", "hidden": false}, {"_id": "698424a7e34659da7e1f4f28", "name": "Kang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f29", "name": "Kangkang Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2a", "name": "Ke Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2b", "name": "Ke Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2c", "name": "Kui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2d", "name": "Kun Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2e", "name": "Kunbin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2f", "name": "Lei Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f30", "name": "Lei Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f31", "name": "Lei Wen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f32", "name": "Linghui Meng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f33", "user": {"_id": "641e69355c348064a8251471", "avatarUrl": "/avatars/acad3877df27ff44ea3921bb43e34d53.svg", "isPro": false, "fullname": "Linhao Yu", "user": "HasuerYu", "type": "user"}, "name": "Linhao Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:47.812Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f34", "name": "Liping Ouyang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f35", "name": "Liwen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f36", "user": {"_id": "65cf859f88d13d8128bb8545", "avatarUrl": "/avatars/aa18b993bd90d9c8a95913050cd955a8.svg", "isPro": false, "fullname": "Longbin Ji", "user": "robingg1", "type": "user"}, "name": "Longbin Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:40.295Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f37", "name": "Longzhi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f38", "name": "Meng Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f39", "name": "Meng Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3a", "name": "Mengfei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3b", "name": "Mengqi Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3c", "name": "Mengyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3d", "name": "Ming Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3e", "name": "Mingcheng Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3f", "name": "Mingming Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f40", "name": "Mingxin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f41", "name": "Mingzhu Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f42", "name": "Naibin Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f43", "name": "Nemin Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f44", "name": "Nian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f45", "name": "Peng Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f46", "name": "Peng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f47", "name": "Pengyu Zou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f48", "name": "Qi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f49", "name": "Qi Xin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4a", "name": "Qian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4b", "name": "Qiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4c", "name": "Qianhui Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4d", "name": "Qianwei Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4e", "name": "Qianyue He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4f", "name": "Qifei Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f50", "name": "Qinrui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f51", "name": "Qiwen Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f52", "name": "Quan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f53", "name": "Quanxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f54", "name": "Qunyi Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4f55", "name": "Rongrui Zhan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f56", "name": "Rufeng Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f57", "name": "Rui Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f58", "name": "Ruian Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f59", "name": "Ruihao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5a", "name": "Ruijie Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5b", "name": "Ruixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5c", "name": "Ruixuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5d", "name": "Runsheng Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5e", "name": "Ruting Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5f", "name": "Senbo Kang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f60", "name": "Shan Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f61", "name": "Shaofei Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f62", "name": "Shaotian Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f63", "name": "Shenwei Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f64", "name": "Shifeng Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f65", "name": "Shihao Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f66", "name": "Shilong Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f67", "name": "Shiqin Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f68", "name": "Shiwei Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f69", "name": "Shixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6a", "name": "Shuai Yao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6b", "name": "Shuang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6c", "name": "Shuangqiao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6d", "name": "Shuhao Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6e", "name": "Shuwei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6f", "name": "Shuwen Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f70", "user": {"_id": "62769a608483d8e9ecd9b4f8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672799958233-62769a608483d8e9ecd9b4f8.jpeg", "isPro": false, "fullname": "Sijun He", "user": "sijunhe", "type": "user"}, "name": "Sijun He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:33.392Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f71", "user": {"_id": "64fada13d82fc6977d5e9c74", "avatarUrl": "/avatars/776bf1257154289e919716637770ef52.svg", "isPro": false, "fullname": "Siming Dai", "user": "DesmonDay", "type": "user"}, "name": "Siming Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:50.302Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f72", "name": "Siming Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f73", "name": "Siyi Long", "hidden": false}, {"_id": "698424a7e34659da7e1f4f74", "name": "Songhe Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f75", "name": "Suhui Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f76", "name": "Suyin Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f77", "name": "Teng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f78", "name": "Tianchan Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f79", "name": "Tianliang Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7a", "user": {"_id": "67bbe929593452cc18877606", "avatarUrl": "/avatars/f50fd1cb35d628c26cf21ad0c95c55b1.svg", "isPro": false, "fullname": "tmyangcs", "user": "youngtimmy", "type": "user"}, "name": "Tianmeng Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:36.143Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7b", "name": "Tianyi Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7c", "name": "Tiezhu Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7d", "name": "Ting Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7e", "name": "Ting Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7f", "name": "Tingdan Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f80", "name": "Wei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f81", "name": "Wei Luan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f82", "name": "Wei Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f83", "name": "Wei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f84", "name": "Wei Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f85", "name": "Weibao Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f86", "name": "Weibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f87", "name": "Weicheng Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f88", "name": "Weichong Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f89", "name": "Weiguo Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8a", "name": "Weilong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8b", "name": "Weiqi Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8c", "name": "Wen Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8d", "name": "Wenbin Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8e", "name": "Wenjing Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8f", "name": "Wenlong Miao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f90", "name": "Wenpei Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f91", "name": "Wenquan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f92", "name": "Xi Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f93", "name": "Xi Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f94", "name": "Xiang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f95", "name": "Xiangguo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f96", "name": "Xiangrui Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f97", "name": "Xiangsen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f98", "name": "Xiangzhe Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f99", "name": "Xianlong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9a", "name": "Xianying Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9b", "name": "Xiao Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9c", "name": "Xiaocong Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9d", "name": "Xiaofei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9e", "name": "Xiaofeng Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9f", "name": "Xiaofeng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa0", "name": "Xiaojian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa1", "name": "Xiaolan Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa2", "name": "Xiaopeng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa3", "name": "Xiaotian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa4", "name": "Xiaoxiong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa5", "name": "Xiaoxu Fei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa6", "name": "Xiaoxuan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa7", "user": {"_id": "664395621b88258a527cd7d1", "avatarUrl": "/avatars/8489ccebe4fd1262679ba63a5cb50bb8.svg", "isPro": false, "fullname": "Kira", "user": "Kira-wang", "type": "user"}, "name": "Xiaoyu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:25.774Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa8", "name": "Xiaoyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa9", "name": "Xin Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4faa", "name": "Xin Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fab", "name": "Xinhui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fac", "name": "Xinming Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fad", "name": "Xintong Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fae", "name": "Xinyi Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4faf", "name": "Xinyu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb0", "name": "Xiuxian Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb1", "name": "XuanShi Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb2", "name": "Xue Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb3", "name": "Xueying Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb4", "name": "Xuhong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb5", "name": "Xulong Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb6", "name": "Xuyi Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb7", "name": "Yabing Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb8", "name": "Yafeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb9", "name": "Yamei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fba", "name": "Yan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbb", "name": "Yanfu Cheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbc", "name": "Yang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbd", "name": "Yang Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbe", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbf", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc0", "name": "Yang Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc1", "name": "Yanlong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc2", "name": "Yannian Fu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc3", "name": "Yanpeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc4", "name": "Yanzheng Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc5", "name": "Yao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc6", "name": "Yaozong Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc7", "name": "Yaqian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc8", "name": "Yehua Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc9", "name": "Yekun Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fca", "name": "Yesong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcb", "name": "Yi Song", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcc", "name": "Yichen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcd", "name": "Yifei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fce", "name": "Yifeng Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcf", "name": "Yifeng Kou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd0", "name": "Yilong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd1", "name": "Yilong Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd2", "name": "Yiming Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd3", "name": "Ying Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd4", "name": "Ying Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd5", "name": "Yingsheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd6", "name": "Yingzhan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd7", "name": "Yinqi Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd8", "name": "Yiran Xing", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd9", "name": "Yishu Lei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fda", "name": "Yixiang Tu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdb", "name": "Yiyan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdc", "name": "Yong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdd", "name": "Yonghua Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fde", "name": "Yongqiang Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdf", "name": "Yongxing Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe0", "name": "Yongyue Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe1", "name": "Yu Ran", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe2", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe3", "name": "Yu-Wen Michael Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe4", "name": "Yuang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe5", "name": "Yuanle Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe6", "name": "Yuanyuan Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe7", "name": "Yubo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe8", "name": "Yuchen Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe9", "name": "Yucheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fea", "name": "Yude Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4feb", "name": "Yuedong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fec", "name": "Yuehu Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4fed", "name": "Yufeng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fee", "name": "Yuhui Cao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fef", "name": "Yuhui Yun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff0", "name": "Yukun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff1", "name": "Yukun Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff2", "name": "Yukun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff3", "name": "Yumeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff4", "name": "Yun Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff5", "name": "Yun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff6", "name": "Yunfei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff7", "name": "Yunshen Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff8", "name": "Yuping Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff9", "name": "Yuqin Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffa", "name": "Yuqing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffb", "name": "Yurui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffc", "name": "Yuwen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffd", "name": "Yuxiang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffe", "name": "Zefeng Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fff", "name": "Zelin Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5000", "name": "Zelun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5001", "name": "Zenan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f5002", "name": "Zezhao Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5003", "name": "Zhaowu Pan", "hidden": false}, {"_id": "698424a7e34659da7e1f5004", "name": "Zhaoyu Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f5005", "name": "Zhe Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5006", "name": "Zhe Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5007", "name": "Zhen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5008", "name": "Zhengfan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f5009", "name": "Zhengrui Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f500a", "name": "Zhengsheng Ning", "hidden": false}, {"_id": "698424a7e34659da7e1f500b", "name": "Zhenxing Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500c", "name": "Zhenyu Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500d", "name": "Zhenyu Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f500e", "name": "Zhenyun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500f", "name": "Zhi Li", "hidden": false}, {"_id": "698424a7e34659da7e1f5010", "name": "Zhichao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5011", "name": "Zhicheng Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5012", "name": "Zhida Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5013", "name": "Zhifan Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5014", "name": "Zhihao Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f5015", "name": "Zhijin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f5016", "name": "Zhiyang Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5017", "name": "Zhonghui Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f5018", "name": "Zhuangzhuang Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f5019", "name": "Zhujun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f501a", "name": "Zhuo Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f501b", "name": "Zichang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f501c", "name": "Zihan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f501d", "name": "Zihao Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f501e", "name": "Zihe Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f501f", "name": "Ziheng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5020", "name": "Ziping Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5021", "name": "Zixuan Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f5022", "name": "Ziyang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f5023", "name": "Ziyi Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f5024", "name": "Ziyuan Gao", "hidden": false}], "publishedAt": "2026-02-04T16:18:15.000Z", "submittedOnDailyAt": "2026-02-05T02:34:05.150Z", "title": "ERNIE 5.0 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "upvotes": 198, "discussionId": "698424a7e34659da7e1f5025", "ai_summary": "ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.", "ai_keywords": ["autoregressive foundation model", "unified multimodal understanding", "unified next-group-of-tokens prediction objective", "mixture-of-experts", "modality-agnostic expert routing", "elastic training paradigm", "reinforcement learning", "sparse MoE architecture"], "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-04T11:18:15.000Z", "title": "ERNIE 5.0 Technical Report", "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04705.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 228, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.00919", "authors": [{"_id": "698186fdce18b1862809633b", "name": "I. Apanasevich", "hidden": false}, {"_id": "698186fdce18b1862809633c", "user": {"_id": "6718963e41abf87204dddaf5", "avatarUrl": "/avatars/05d4fdb330ccb52c53cb8f99f7497ab2.svg", "isPro": false, "fullname": "Mikhail Artemyev", "user": "Mixanik-43", "type": "user"}, "name": "M. Artemyev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:34.142Z", "hidden": false}, {"_id": "698186fdce18b1862809633d", "name": "R. Babakyan", "hidden": false}, {"_id": "698186fdce18b1862809633e", "user": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "name": "P. Fedotova", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:41.710Z", "hidden": false}, {"_id": "698186fdce18b1862809633f", "name": "D. Grankin", "hidden": false}, {"_id": "698186fdce18b18628096340", "name": "E. Kupryashin", "hidden": false}, {"_id": "698186fdce18b18628096341", "user": {"_id": "662ace3c4f711ee4e1dcb790", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/R5dlha7Lpy5gCYFEAtr1L.jpeg", "isPro": false, "fullname": "Anastas Misailidi", "user": "kazzart", "type": "user"}, "name": "A. Misailidi", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:18.667Z", "hidden": false}, {"_id": "698186fdce18b18628096342", "user": {"_id": "66eb27551a537888d2121ddc", "avatarUrl": "/avatars/9c807b058c972c307a24d85efbfbd4ae.svg", "isPro": false, "fullname": "Daniil", "user": "Defgy", "type": "user"}, "name": "D. Nerus", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:33.667Z", "hidden": false}, {"_id": "698186fdce18b18628096343", "user": {"_id": "65e5e3df92de33440675b5d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e5e3df92de33440675b5d9/UOVd40f_Htd5oMAa_L0cM.jpeg", "isPro": false, "fullname": "Alexander Nutalapati", "user": "AlexanderNutalapati", "type": "user"}, "name": "A. Nutalapati", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:20.959Z", "hidden": false}, {"_id": "698186fdce18b18628096344", "user": {"_id": "66b51b3ad4eea6ad6adfd611", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b51b3ad4eea6ad6adfd611/SC_01wvlLjB0FFZdDVgAp.jpeg", "isPro": false, "fullname": "Gena Sidorov", "user": "haksorus", "type": "user"}, "name": "G. Sidorov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:41.584Z", "hidden": false}, {"_id": "698186fdce18b18628096345", "user": {"_id": "631ee99d2225f12fc0ef39f4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662970571579-631ee99d2225f12fc0ef39f4.jpeg", "isPro": false, "fullname": "Ivan Efremov", "user": "4ku", "type": "user"}, "name": "I. Efremov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:36.696Z", "hidden": false}, {"_id": "698186fdce18b18628096346", "name": "M. Gerasyov", "hidden": false}, {"_id": "698186fdce18b18628096347", "name": "D. Pikurov", "hidden": false}, {"_id": "698186fdce18b18628096348", "name": "Y. Senchenko", "hidden": false}, {"_id": "698186fdce18b18628096349", "user": {"_id": "68113993ebc57966794e23d6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Yc3GIqYZyO97lzZ9rX8OE.png", "isPro": false, "fullname": "Sergei Davidenko", "user": "Ant346", "type": "user"}, "name": "S. Davidenko", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:21.332Z", "hidden": false}, {"_id": "698186fdce18b1862809634a", "user": {"_id": "6981bbf47f758a03b9c46550", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/oTPe_MzIrDlCeRDvWWeLK.png", "isPro": false, "fullname": "Daniil Kulikov", "user": "KulikovDR", "type": "user"}, "name": "D. Kulikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:46.458Z", "hidden": false}, {"_id": "698186fdce18b1862809634b", "name": "M. Sultankin", "hidden": false}, {"_id": "698186fdce18b1862809634c", "user": {"_id": "63518aa5a30fc3ba88ce51dd", "avatarUrl": "/avatars/2e6a8f4a3e76fcc1afe7e777d6b45e76.svg", "isPro": false, "fullname": "Kazybek A", "user": "wanjia", "type": "user"}, "name": "K. Askarbek", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:24.567Z", "hidden": false}, {"_id": "698186fdce18b1862809634d", "name": "O. Shamanin", "hidden": false}, {"_id": "698186fdce18b1862809634e", "name": "D. Statovoy", "hidden": false}, {"_id": "698186fdce18b1862809634f", "user": {"_id": "655f32a519fd101f14bf1fb0", "avatarUrl": "/avatars/adf2c494759ebe5a0d95c15631ac6312.svg", "isPro": false, "fullname": "Eduard", "user": "rjomba3000", "type": "user"}, "name": "E. Zalyaev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:43.976Z", "hidden": false}, {"_id": "698186fdce18b18628096350", "user": {"_id": "67dd1714817478ae84b18981", "avatarUrl": "/avatars/1209da3d4c4de3f419ebea6845bb0ed6.svg", "isPro": false, "fullname": "Zorin Ilya", "user": "Zora244", "type": "user"}, "name": "I. Zorin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:31.637Z", "hidden": false}, {"_id": "698186fdce18b18628096351", "name": "A. Letkin", "hidden": false}, {"_id": "698186fdce18b18628096352", "name": "E. Rusakov", "hidden": false}, {"_id": "698186fdce18b18628096353", "name": "A. Silchenko", "hidden": false}, {"_id": "698186fdce18b18628096354", "user": {"_id": "6981a821165e30591e1200e7", "avatarUrl": "/avatars/af72142b8ba8772926b247c31fc8e4c8.svg", "isPro": false, "fullname": "Vlad Vorobyov", "user": "GloomARK", "type": "user"}, "name": "V. Vorobyov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:18.218Z", "hidden": false}, {"_id": "698186fdce18b18628096355", "user": {"_id": "6901ce2d911da714e754422b", "avatarUrl": "/avatars/5ed8ce189ca92a04f7165751076ff446.svg", "isPro": false, "fullname": "SERGEI", "user": "sobolnikov", "type": "user"}, "name": "S. Sobolnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:28.887Z", "hidden": false}, {"_id": "698186fdce18b18628096356", "user": {"_id": "640e2ef88512ec51d7f34cd5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640e2ef88512ec51d7f34cd5/Xl8UiprL-0SvOWHeoAFW1.jpeg", "isPro": false, "fullname": "Aleksey Postnikov", "user": "AlekseyPostnikov", "type": "user"}, "name": "A. Postnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:39.139Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "publishedAt": "2026-01-31T22:13:23.000Z", "submittedOnDailyAt": "2026-02-03T03:13:09.153Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "submittedOnDailyBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "upvotes": 173, "discussionId": "698186fece18b18628096357", "projectPage": "https://greenvla.github.io", "githubRepo": "https://github.com/greenvla/GreenVLA", "githubRepoAddedBy": "user", "ai_summary": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.", "ai_keywords": ["Vision-Language-Action", "multimodal grounding", "multi-embodiment pretraining", "embodiment-specific adaptation", "reinforcement-learning", "episode-progress prediction", "out-of-distribution detection", "joint-prediction-based guidance"], "githubStars": 24, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-01-31T17:13:23.000Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00919.png", "numComments": 1, "submittedBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "fullname": "Polina Fedotova", "name": "2pd", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.09877", "authors": [{"_id": "698c7abdeb12ea7453916869", "user": {"_id": "674006451d2302f6aa9b026d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png", "isPro": false, "fullname": "Chenxu Wang", "user": "xunyoyo", "type": "user"}, "name": "Chenxu Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-12T16:49:45.534Z", "hidden": false}, {"_id": "698c7abdeb12ea745391686a", "name": "Chaozhuo Li", "hidden": false}, {"_id": "698c7abdeb12ea745391686b", "name": "Songyang Liu", "hidden": false}, {"_id": "698c7abdeb12ea745391686c", "name": "Zejian Chen", "hidden": false}, {"_id": "698c7abdeb12ea745391686d", "name": "Jinyu Hou", "hidden": false}, {"_id": "698c7abdeb12ea745391686e", "name": "Ji Qi", "hidden": false}, {"_id": "698c7abdeb12ea745391686f", "name": "Rui Li", "hidden": false}, {"_id": "698c7abdeb12ea7453916870", "name": "Litian Zhang", "hidden": false}, {"_id": "698c7abdeb12ea7453916871", "name": "Qiwei Ye", "hidden": false}, {"_id": "698c7abdeb12ea7453916872", "name": "Zheng Liu", "hidden": false}, {"_id": "698c7abdeb12ea7453916873", "name": "Xu Chen", "hidden": false}, {"_id": "698c7abdeb12ea7453916874", "name": "Xi Zhang", "hidden": false}, {"_id": "698c7abdeb12ea7453916875", "name": "Philip S. Yu", "hidden": false}], "publishedAt": "2026-02-10T15:18:19.000Z", "submittedOnDailyAt": "2026-02-13T00:53:30.377Z", "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies", "submittedOnDailyBy": {"_id": "674006451d2302f6aa9b026d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png", "isPro": false, "fullname": "Chenxu Wang", "user": "xunyoyo", "type": "user"}, "summary": "The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.", "upvotes": 169, "discussionId": "698c7abdeb12ea7453916876", "ai_summary": "Multi-agent LLM systems face fundamental limitations in achieving continuous self-improvement while maintaining safety alignment due to inherent statistical blind spots in isolated evolution.", "ai_keywords": ["multi-agent systems", "large language models", "self-evolution", "safety alignment", "information-theoretic framework", "anthropic value distributions", "statistical blind spots", "self-evolving AI societies", "external oversight", "safety-preserving mechanisms"], "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-10T10:18:19.000Z", "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies", "summary": "The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09877.png", "numComments": 2, "submittedBy": {"_id": "674006451d2302f6aa9b026d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png", "fullname": "Chenxu Wang", "name": "xunyoyo", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.09856", "authors": [{"_id": "698bf5b66052d3bed9630aa7", "user": {"_id": "64107c7df52d7eb22e062956", "avatarUrl": "/avatars/7b1cee9a2b8454fedfbd4c3d1df9865c.svg", "isPro": false, "fullname": "Yuhao Zheng", "user": "yhzheng1031", "type": "user"}, "name": "Yuhao Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:28.241Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aa8", "name": "Li'an Zhong", "hidden": false}, {"_id": "698bf5b66052d3bed9630aa9", "user": {"_id": "6773bcaa675a971ddf1e81dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a8VUwZYXd7O_mq_zFvXMh.png", "isPro": false, "fullname": "CokeWang", "user": "CokeWang", "type": "user"}, "name": "Yi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:30.778Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aaa", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:25.982Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aab", "name": "Kaikui Liu", "hidden": false}, {"_id": "698bf5b66052d3bed9630aac", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "698bf5b66052d3bed9630aad", "name": "Linyuan Lv", "hidden": false}, {"_id": "698bf5b66052d3bed9630aae", "name": "Philip Torr", "hidden": false}, {"_id": "698bf5b66052d3bed9630aaf", "user": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "isPro": false, "fullname": "Qinghong (Kevin) Lin", "user": "KevinQHLin", "type": "user"}, "name": "Kevin Qinghong Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:23.397Z", "hidden": false}], "publishedAt": "2026-02-10T14:56:19.000Z", "submittedOnDailyAt": "2026-02-11T01:02:42.385Z", "title": "Code2World: A GUI World Model via Renderable Code Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "upvotes": 168, "discussionId": "698bf5b66052d3bed9630ab0", "projectPage": "https://amap-ml.github.io/Code2World/", "githubRepo": "https://github.com/AMAP-ML/Code2World", "githubRepoAddedBy": "user", "ai_summary": "Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.", "ai_keywords": ["vision-language coder", "GUI World model", "action-conditioned prediction", "AndroidCode", "HTML generation", "visual-feedback revision mechanism", "SFT", "Render-Aware Reinforcement Learning", "visual semantic fidelity", "action consistency", "next UI prediction", "AndroidWorld navigation"], "githubStars": 131, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-10T09:56:19.000Z", "title": "Code2World: A GUI World Model via Renderable Code Generation", "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09856.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 230, "isUserFollowing": false}, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.10693", "authors": [{"_id": "6992047b50fb2c0be47837f0", "user": {"_id": "6475ff9b4c9fb8a4bf1cde76", "avatarUrl": "/avatars/61cf82cd0e15c4618f5bd8b1f7d52f37.svg", "isPro": false, "fullname": "floyed shen", "user": "floyed", "type": "user"}, "name": "Guobin Shen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-17T15:52:51.206Z", "hidden": false}, {"_id": "6992047b50fb2c0be47837f1", "user": {"_id": "63fc5b724c57549ad5e54558", "avatarUrl": "/avatars/1374c1e8969533dd7543959666f16d1a.svg", "isPro": false, "fullname": "Chenxiao Zhao", "user": "ChenShawn", "type": "user"}, "name": "Chenxiao Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-02-17T17:17:12.583Z", "hidden": false}, {"_id": "6992047b50fb2c0be47837f2", "user": {"_id": "655c43d6b426ec8f4b5e7652", "avatarUrl": "/avatars/ddcf9d1ef0e2dc1f564a56ba9153f24f.svg", "isPro": false, "fullname": "Xiang Cheng", "user": "FFFc2", "type": "user"}, "name": "Xiang Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-17T15:52:57.697Z", "hidden": false}, {"_id": "6992047b50fb2c0be47837f3", "user": {"_id": "61f4c2e981c4d30f58140279", "avatarUrl": "/avatars/c4a69f6563c952354e33682e86045b14.svg", "isPro": false, "fullname": "HuangMeow", "user": "Luckyyy", "type": "user"}, "name": "Lei Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-23T09:46:50.954Z", "hidden": false}, {"_id": "6992047b50fb2c0be47837f4", "name": "Xing Yu", "hidden": false}], "publishedAt": "2026-02-11T09:48:08.000Z", "submittedOnDailyAt": "2026-02-23T03:29:14.259Z", "title": "VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training", "submittedOnDailyBy": {"_id": "6475ff9b4c9fb8a4bf1cde76", "avatarUrl": "/avatars/61cf82cd0e15c4618f5bd8b1f7d52f37.svg", "isPro": false, "fullname": "floyed shen", "user": "floyed", "type": "user"}, "summary": "Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO", "upvotes": 158, "discussionId": "6992047c50fb2c0be47837f5", "githubRepo": "https://github.com/FloyedShen/VESPO", "githubRepoAddedBy": "user", "ai_summary": "VESPO addresses training instability in LLM reinforcement learning by using variational formulation with variance reduction to correct policy divergence without length normalization.", "ai_keywords": ["reinforcement learning", "large language models", "policy staleness", "asynchronous training", "importance sampling", "variance reduction", "variational formulation", "proposal distributions", "sequence-level importance weights", "mathematical reasoning benchmarks", "Mixture-of-Experts models"], "githubStars": 14, "organization": {"_id": "68246a0a98117c02df67a547", "name": "rednote-hilab", "fullname": "rednote-hilab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6807a1d6504547b3554b9c73/WgnnQDsz7FqnyTtv8mmRO.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-11T04:48:08.000Z", "title": "VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training", "summary": "Training stability remains a central challenge in reinforcement learning (RL) for large language models (LLMs). Policy staleness, asynchronous training, and mismatches between training and inference engines all cause the behavior policy to diverge from the current policy, risking training collapse. Importance sampling provides a principled correction for this distribution shift but suffers from high variance; existing remedies such as token-level clipping and sequence-level normalization lack a unified theoretical foundation. We propose Variational sEquence-level Soft Policy Optimization (VESPO). By incorporating variance reduction into a variational formulation over proposal distributions, VESPO derives a closed-form reshaping kernel that operates directly on sequence-level importance weights without length normalization. Experiments on mathematical reasoning benchmarks show that VESPO maintains stable training under staleness ratios up to 64x and fully asynchronous execution, and delivers consistent gains across both dense and Mixture-of-Experts models. Code is available at https://github.com/FloyedShen/VESPO", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10693.png", "numComments": 2, "submittedBy": {"_id": "6475ff9b4c9fb8a4bf1cde76", "avatarUrl": "/avatars/61cf82cd0e15c4618f5bd8b1f7d52f37.svg", "fullname": "floyed shen", "name": "floyed", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "68246a0a98117c02df67a547", "name": "rednote-hilab", "fullname": "rednote-hilab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6807a1d6504547b3554b9c73/WgnnQDsz7FqnyTtv8mmRO.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.10604", "authors": [{"_id": "698d417065c0d15a6d162026", "name": "Ailin Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162027", "name": "Ang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162028", "name": "Aobo Kong", "hidden": false}, {"_id": "698d417065c0d15a6d162029", "name": "Bin Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16202a", "name": "Binxing Jiao", "hidden": false}, {"_id": "698d417065c0d15a6d16202b", "name": "Bo Dong", "hidden": false}, {"_id": "698d417065c0d15a6d16202c", "name": "Bojun Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16202d", "name": "Boyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16202e", "name": "Brian Li", "hidden": false}, {"_id": "698d417065c0d15a6d16202f", "name": "Buyun Ma", "hidden": false}, {"_id": "698d417065c0d15a6d162030", "name": "Chang Su", "hidden": false}, {"_id": "698d417065c0d15a6d162031", "name": "Changxin Miao", "hidden": false}, {"_id": "698d417065c0d15a6d162032", "name": "Changyi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162033", "name": "Chao Lou", "hidden": false}, {"_id": "698d417065c0d15a6d162034", "name": "Chen Hu", "hidden": false}, {"_id": "698d417065c0d15a6d162035", "name": "Chen Xu", "hidden": false}, {"_id": "698d417065c0d15a6d162036", "name": "Chenfeng Yu", "hidden": false}, {"_id": "698d417065c0d15a6d162037", "name": "Chengting Feng", "hidden": false}, {"_id": "698d417065c0d15a6d162038", "name": "Chengyuan Yao", "hidden": false}, {"_id": "698d417065c0d15a6d162039", "name": "Chunrui Han", "hidden": false}, {"_id": "698d417065c0d15a6d16203a", "name": "Dan Ma", "hidden": false}, {"_id": "698d417065c0d15a6d16203b", "name": "Dapeng Shi", "hidden": false}, {"_id": "698d417065c0d15a6d16203c", "name": "Daxin Jiang", "hidden": false}, {"_id": "698d417065c0d15a6d16203d", "name": "Dehua Ma", "hidden": false}, {"_id": "698d417065c0d15a6d16203e", "name": "Deshan Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16203f", "name": "Di Qi", "hidden": false}, {"_id": "698d417065c0d15a6d162040", "name": "Enle Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162041", "name": "Fajie Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d162042", "name": "Fanqi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162043", "name": "Guanzhe Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162044", "name": "Gulin Yan", "hidden": false}, {"_id": "698d417065c0d15a6d162045", "name": "Guoliang Cao", "hidden": false}, {"_id": "698d417065c0d15a6d162046", "name": "Guopeng Li", "hidden": false}, {"_id": "698d417065c0d15a6d162047", "name": "Han Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d162048", "name": "Hangyu Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162049", "user": {"_id": "64b7874b9f5987572ca28461", "avatarUrl": "/avatars/d24ee0a6329ff93936aa7829481e2046.svg", "isPro": false, "fullname": "hanshanzhang", "user": "brain-zhang", "type": "user"}, "name": "Hanshan Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:52.602Z", "hidden": false}, {"_id": "698d417065c0d15a6d16204a", "name": "Hao Nie", "hidden": false}, {"_id": "698d417065c0d15a6d16204b", "name": "Haonan Jia", "hidden": false}, {"_id": "698d417065c0d15a6d16204c", "name": "Haoran Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16204d", "name": "Hebin Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16204e", "name": "Hekun Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16204f", "name": "Heng Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162050", "name": "Heung-Yeung Shum", "hidden": false}, {"_id": "698d417065c0d15a6d162051", "name": "Hongbo Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162052", "name": "Hongbo Peng", "hidden": false}, {"_id": "698d417065c0d15a6d162053", "name": "Hongyu Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d162054", "name": "Hongyuan Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162055", "name": "Houyong Chen", "hidden": false}, {"_id": "698d417065c0d15a6d162056", "name": "Huangxi Zhu", "hidden": false}, {"_id": "698d417065c0d15a6d162057", "name": "Huimin Wu", "hidden": false}, {"_id": "698d417065c0d15a6d162058", "name": "Huiyong Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162059", "name": "Jia Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16205a", "name": "Jian Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16205b", "name": "Jianjian Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16205c", "name": "Jiaoren Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16205d", "name": "Jiaran Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d16205e", "name": "Jiashu Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16205f", "name": "Jiashuo Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162060", "name": "Jiayi Fu", "hidden": false}, {"_id": "698d417065c0d15a6d162061", "name": "Jiayu Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162062", "name": "Jie Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d162063", "name": "Jie Luo", "hidden": false}, {"_id": "698d417065c0d15a6d162064", "name": "Jie Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162065", "name": "Jie Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d162066", "name": "Jieyi Hou", "hidden": false}, {"_id": "698d417065c0d15a6d162067", "name": "Jing Bai", "hidden": false}, {"_id": "698d417065c0d15a6d162068", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:28:37.335Z", "hidden": false}, {"_id": "698d417065c0d15a6d162069", "name": "Jingjing Xie", "hidden": false}, {"_id": "698d417065c0d15a6d16206a", "name": "Jingwei Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16206b", "name": "Jingyang Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d16206c", "name": "Jishi Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16206d", "name": "Junfeng Liu", "hidden": false}, {"_id": "698d417065c0d15a6d16206e", "name": "Junzhe Lin", "hidden": false}, {"_id": "698d417065c0d15a6d16206f", "name": "Ka Man Lo", "hidden": false}, {"_id": "698d417065c0d15a6d162070", "name": "Kai Liang", "hidden": false}, {"_id": "698d417065c0d15a6d162071", "name": "Kaibo Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162072", "name": "Kaijun Tan", "hidden": false}, {"_id": "698d417065c0d15a6d162073", "user": {"_id": "66668c591964b6188ee310c2", "avatarUrl": "/avatars/8a8265073dbacbb2c7139b1c8da3e055.svg", "isPro": false, "fullname": "Kaiwen Yan", "user": "linrany", "type": "user"}, "name": "Kaiwen Yan", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:58.524Z", "hidden": false}, {"_id": "698d417065c0d15a6d162074", "name": "Kaixiang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162075", "name": "Kang An", "hidden": false}, {"_id": "698d417065c0d15a6d162076", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:56.339Z", "hidden": false}, {"_id": "698d417065c0d15a6d162077", "name": "Lei Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162078", "name": "Liang Lv", "hidden": false}, {"_id": "698d417065c0d15a6d162079", "name": "Liang Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d16207a", "name": "Liangyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16207b", "name": "Lieyu Shi", "hidden": false}, {"_id": "698d417065c0d15a6d16207c", "name": "Liguo Tan", "hidden": false}, {"_id": "698d417065c0d15a6d16207d", "name": "Lin Lin", "hidden": false}, {"_id": "698d417065c0d15a6d16207e", "name": "Lina Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16207f", "name": "Luck Ma", "hidden": false}, {"_id": "698d417065c0d15a6d162080", "name": "Mengqiang Ren", "hidden": false}, {"_id": "698d417065c0d15a6d162081", "name": "Michael Li", "hidden": false}, {"_id": "698d417065c0d15a6d162082", "name": "Ming Li", "hidden": false}, {"_id": "698d417065c0d15a6d162083", "name": "Mingliang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162084", "name": "Mingming Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d162085", "name": "Mingrui Chen", "hidden": false}, {"_id": "698d417065c0d15a6d162086", "name": "Mitt Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162087", "name": "Na Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162088", "name": "Peng Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162089", "name": "Qi Han", "hidden": false}, {"_id": "698d417065c0d15a6d16208a", "name": "Qian Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d16208b", "name": "Qinglin He", "hidden": false}, {"_id": "698d417065c0d15a6d16208c", "name": "Qinxin Du", "hidden": false}, {"_id": "698d417065c0d15a6d16208d", "name": "Qiuping Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16208e", "name": "Quan Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16208f", "name": "Rongqiu Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162090", "name": "Ruihang Miao", "hidden": false}, {"_id": "698d417065c0d15a6d162091", "name": "Ruixin Han", "hidden": false}, {"_id": "698d417065c0d15a6d162092", "name": "Ruosi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162093", "name": "Ruyan Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162094", "name": "Shan Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162095", "name": "Shaoliang Pang", "hidden": false}, {"_id": "698d417065c0d15a6d162096", "name": "Shaowen Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162097", "name": "Shengjie Fan", "hidden": false}, {"_id": "698d417065c0d15a6d162098", "name": "Shijie Shang", "hidden": false}, {"_id": "698d417065c0d15a6d162099", "name": "Shiliang Yang", "hidden": false}, {"_id": "698d417065c0d15a6d16209a", "name": "Shiwei Li", "hidden": false}, {"_id": "698d417065c0d15a6d16209b", "name": "Shuangshuang Tian", "hidden": false}, {"_id": "698d417065c0d15a6d16209c", "name": "Siqi Liu", "hidden": false}, {"_id": "698d417065c0d15a6d16209d", "name": "Siye Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16209e", "name": "Siyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16209f", "name": "Song Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620a0", "name": "Tiancheng Cao", "hidden": false}, {"_id": "698d417065c0d15a6d1620a1", "name": "Tianchi Yue", "hidden": false}, {"_id": "698d417065c0d15a6d1620a2", "name": "Tianhao Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d1620a3", "name": "Tianning Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620a4", "name": "Tingdan Luo", "hidden": false}, {"_id": "698d417065c0d15a6d1620a5", "name": "Wang You", "hidden": false}, {"_id": "698d417065c0d15a6d1620a6", "name": "Wei Ji", "hidden": false}, {"_id": "698d417065c0d15a6d1620a7", "name": "Wei Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620a8", "name": "Wei Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620a9", "name": "Weibo Wu", "hidden": false}, {"_id": "698d417065c0d15a6d1620aa", "user": {"_id": "6657620ea496f7fcb67c3871", "avatarUrl": "/avatars/54fef1c835e6f6b478652d438a140d45.svg", "isPro": false, "fullname": "xieweihao", "user": "chalengr", "type": "user"}, "name": "Weihao Xie", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:48.216Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620ab", "name": "Wen Sun", "hidden": false}, {"_id": "698d417065c0d15a6d1620ac", "name": "Wenjin Deng", "hidden": false}, {"_id": "698d417065c0d15a6d1620ad", "user": {"_id": "650c04795510464e85b47470", "avatarUrl": "/avatars/98c194e77826b928c49659849f466dad.svg", "isPro": false, "fullname": "wen", "user": "zhengwenzhen", "type": "user"}, "name": "Wenzhen Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:45.930Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620ae", "name": "Wuxun Xie", "hidden": false}, {"_id": "698d417065c0d15a6d1620af", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b0", "name": "Xiangwen Kong", "hidden": false}, {"_id": "698d417065c0d15a6d1620b1", "name": "Xiangyu Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620b2", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b3", "name": "Xiaobo Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b4", "name": "Xiaojia Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620b5", "name": "Xiaolan Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620b6", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "698d417065c0d15a6d1620b7", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "698d417065c0d15a6d1620b8", "name": "Xiaoyun Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b9", "name": "Xin Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620ba", "name": "Xin Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620bb", "name": "Xin Wu", "hidden": false}, {"_id": "698d417065c0d15a6d1620bc", "name": "Xing Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620bd", "name": "Xingping Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620be", "name": "Xinran Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620bf", "name": "Xu Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620c0", "user": {"_id": "64ec5b64bfb2aa06a46ff2d6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Lgl55OtDWa0tzRI2ShpUe.jpeg", "isPro": false, "fullname": "xuan he", "user": "tpa115k31", "type": "user"}, "name": "Xuan He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:36.240Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620c1", "name": "Xuanti Feng", "hidden": false}, {"_id": "698d417065c0d15a6d1620c2", "name": "Xuedan Cai", "hidden": false}, {"_id": "698d417065c0d15a6d1620c3", "name": "Xuqiang Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d1620c4", "name": "Yanbo Yu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c5", "name": "Yang Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620c6", "name": "Yang Xu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c7", "name": "Yanlin Lai", "hidden": false}, {"_id": "698d417065c0d15a6d1620c8", "name": "Yanming Xu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c9", "name": "Yaoyu Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620ca", "name": "Yeqing Shen", "hidden": false}, {"_id": "698d417065c0d15a6d1620cb", "name": "Yibo Zhu", "hidden": false}, {"_id": "698d417065c0d15a6d1620cc", "name": "Yichen Lv", "hidden": false}, {"_id": "698d417065c0d15a6d1620cd", "name": "Yicheng Cao", "hidden": false}, {"_id": "698d417065c0d15a6d1620ce", "name": "Yifeng Gong", "hidden": false}, {"_id": "698d417065c0d15a6d1620cf", "name": "Yijing Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d0", "name": "Yikun Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d1", "name": "Yin Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d2", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d3", "name": "Yinmin Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d4", "name": "Yitong Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d5", "name": "Yixuan Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d6", "name": "Yiyang Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620d7", "name": "Yongchi Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d8", "name": "Yongshen Long", "hidden": false}, {"_id": "698d417065c0d15a6d1620d9", "name": "Yongyao Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620da", "name": "Yousong Guan", "hidden": false}, {"_id": "698d417065c0d15a6d1620db", "name": "Yu Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d1620dc", "name": "Yuang Peng", "hidden": false}, {"_id": "698d417065c0d15a6d1620dd", "name": "Yuanhao Ding", "hidden": false}, {"_id": "698d417065c0d15a6d1620de", "name": "Yuantao Fan", "hidden": false}, {"_id": "698d417065c0d15a6d1620df", "name": "Yuanzhen Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620e0", "name": "Yuchu Luo", "hidden": false}, {"_id": "698d417065c0d15a6d1620e1", "name": "Yudi Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620e2", "name": "Yue Peng", "hidden": false}, {"_id": "698d417065c0d15a6d1620e3", "name": "Yueqiang Lin", "hidden": false}, {"_id": "698d417065c0d15a6d1620e4", "name": "Yufan Lu", "hidden": false}, {"_id": "698d417065c0d15a6d1620e5", "name": "Yuling Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620e6", "name": "Yunzhou Ju", "hidden": false}, {"_id": "698d417065c0d15a6d1620e7", "name": "Yurong Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620e8", "name": "Yusheng Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620e9", "name": "Yuxiang Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620ea", "name": "Yuyang Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620eb", "name": "Yuzhu Cai", "hidden": false}, {"_id": "698d417065c0d15a6d1620ec", "name": "Zejia Weng", "hidden": false}, {"_id": "698d417065c0d15a6d1620ed", "name": "Zetao Hong", "hidden": false}, {"_id": "698d417065c0d15a6d1620ee", "name": "Zexi Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620ef", "name": "Zhe Xie", "hidden": false}, {"_id": "698d417065c0d15a6d1620f0", "name": "Zheng Ge", "hidden": false}, {"_id": "698d417065c0d15a6d1620f1", "name": "Zheng Gong", "hidden": false}, {"_id": "698d417065c0d15a6d1620f2", "name": "Zheng Zeng", "hidden": false}, {"_id": "698d417065c0d15a6d1620f3", "user": {"_id": "63607ace9ddc44e710e13f0f", "avatarUrl": "/avatars/b5f331549562aea4a5c8b681fd9da1ff.svg", "isPro": false, "fullname": "zy", "user": "lu-vae", "type": "user"}, "name": "Zhenyi Lu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:50.532Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620f4", "name": "Zhewei Huang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f5", "name": "Zhichao Chang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f6", "name": "Zhiguo Huang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f7", "name": "Zhiheng Hu", "hidden": false}, {"_id": "698d417065c0d15a6d1620f8", "name": "Zidong Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f9", "name": "Zili Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620fa", "name": "Ziqi Ren", "hidden": false}, {"_id": "698d417065c0d15a6d1620fb", "name": "Zixin Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620fc", "name": "Zixuan Wang", "hidden": false}], "publishedAt": "2026-02-11T07:53:51.000Z", "submittedOnDailyAt": "2026-02-12T00:26:49.880Z", "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.", "upvotes": 150, "discussionId": "698d417165c0d15a6d1620fd", "githubRepo": "https://github.com/stepfun-ai/Step-3.5-Flash", "githubRepoAddedBy": "user", "ai_summary": "Step 3.5 Flash is a sparse Mixture-of-Experts model that achieves frontier-level agentic intelligence through efficient parameter utilization and optimized attention mechanisms, demonstrating strong performance across multiple benchmarks.", "ai_keywords": ["Mixture-of-Experts", "sparse MoE", "foundation model", "active parameters", "interleaved attention", "sliding-window attention", "full attention", "Multi-Token Prediction", "reinforcement learning", "verifiable signals", "preference feedback", "off-policy training", "self-improvement", "IMO-AnswerBench", "LiveCodeBench", "tau2-Bench", "BrowseComp", "Terminal-Bench"], "githubStars": 1245, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-11T02:53:51.000Z", "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters", "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10604.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 231, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.02276", "authors": [{"_id": "69817e2cce18b1862809615b", "name": "Kimi Team", "hidden": false}, {"_id": "69817e2cce18b1862809615c", "name": "Tongtong Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615d", "name": "Yifan Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615e", "name": "Yiping Bao", "hidden": false}, {"_id": "69817e2cce18b1862809615f", "name": "S. H. Cai", "hidden": false}, {"_id": "69817e2cce18b18628096160", "name": "Yuan Cao", "hidden": false}, {"_id": "69817e2cce18b18628096161", "name": "Y. Charles", "hidden": false}, {"_id": "69817e2cce18b18628096162", "name": "H. S. Che", "hidden": false}, {"_id": "69817e2cce18b18628096163", "name": "Cheng Chen", "hidden": false}, {"_id": "69817e2cce18b18628096164", "name": "Guanduo Chen", "hidden": false}, {"_id": "69817e2cce18b18628096165", "name": "Huarong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096166", "name": "Jia Chen", "hidden": false}, {"_id": "69817e2cce18b18628096167", "name": "Jiahao Chen", "hidden": false}, {"_id": "69817e2cce18b18628096168", "name": "Jianlong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096169", "name": "Jun Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616a", "name": "Kefan Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616b", "name": "Liang Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616c", "name": "Ruijue Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616d", "name": "Xinhao Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616e", "name": "Yanru Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616f", "name": "Yanxu Chen", "hidden": false}, {"_id": "69817e2cce18b18628096170", "name": "Yicun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096171", "name": "Yimin Chen", "hidden": false}, {"_id": "69817e2cce18b18628096172", "name": "Yingjiang Chen", "hidden": false}, {"_id": "69817e2cce18b18628096173", "name": "Yuankun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096174", "name": "Yujie Chen", "hidden": false}, {"_id": "69817e2cce18b18628096175", "name": "Yutian Chen", "hidden": false}, {"_id": "69817e2cce18b18628096176", "name": "Zhirong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096177", "name": "Ziwei Chen", "hidden": false}, {"_id": "69817e2cce18b18628096178", "name": "Dazhi Cheng", "hidden": false}, {"_id": "69817e2cce18b18628096179", "name": "Minghan Chu", "hidden": false}, {"_id": "69817e2cce18b1862809617a", "name": "Jialei Cui", "hidden": false}, {"_id": "69817e2cce18b1862809617b", "name": "Jiaqi Deng", "hidden": false}, {"_id": "69817e2cce18b1862809617c", "name": "Muxi Diao", "hidden": false}, {"_id": "69817e2cce18b1862809617d", "name": "Hao Ding", "hidden": false}, {"_id": "69817e2cce18b1862809617e", "name": "Mengfan Dong", "hidden": false}, {"_id": "69817e2cce18b1862809617f", "name": "Mengnan Dong", "hidden": false}, {"_id": "69817e2cce18b18628096180", "name": "Yuxin Dong", "hidden": false}, {"_id": "69817e2cce18b18628096181", "user": {"_id": "652965773a416e1f2173443b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg", "isPro": true, "fullname": "Yuhao Dong", "user": "THUdyh", "type": "user"}, "name": "Yuhao Dong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:04:11.993Z", "hidden": false}, {"_id": "69817e2cce18b18628096182", "name": "Angang Du", "hidden": false}, {"_id": "69817e2cce18b18628096183", "name": "Chenzhuang Du", "hidden": false}, {"_id": "69817e2cce18b18628096184", "name": "Dikang Du", "hidden": false}, {"_id": "69817e2cce18b18628096185", "name": "Lingxiao Du", "hidden": false}, {"_id": "69817e2cce18b18628096186", "user": {"_id": "6340f31fb78ed99eab04ce33", "avatarUrl": "/avatars/2e7fcbf0233bdc0bc9a3f4603fd8bf90.svg", "isPro": false, "fullname": "Du", "user": "Yulun", "type": "user"}, "name": "Yulun Du", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:47.298Z", "hidden": false}, {"_id": "69817e2cce18b18628096187", "name": "Yu Fan", "hidden": false}, {"_id": "69817e2cce18b18628096188", "name": "Shengjun Fang", "hidden": false}, {"_id": "69817e2cce18b18628096189", "name": "Qiulin Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618a", "name": "Yichen Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618b", "name": "Garimugai Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618c", "name": "Kelin Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618d", "name": "Hongcheng Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618e", "name": "Tong Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618f", "name": "Yuyao Ge", "hidden": false}, {"_id": "69817e2cce18b18628096190", "user": {"_id": "650a5d79a0f81fbc0a9875a7", "avatarUrl": "/avatars/a76b1c932964602f2fc4a801ccad3ab5.svg", "isPro": false, "fullname": "ShangyiGeng", "user": "Reset23", "type": "user"}, "name": "Shangyi Geng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:10.446Z", "hidden": false}, {"_id": "69817e2cce18b18628096191", "name": "Chengyang Gong", "hidden": false}, {"_id": "69817e2cce18b18628096192", "name": "Xiaochen Gong", "hidden": false}, {"_id": "69817e2cce18b18628096193", "name": "Zhuoma Gongque", "hidden": false}, {"_id": "69817e2cce18b18628096194", "name": "Qizheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096195", "name": "Xinran Gu", "hidden": false}, {"_id": "69817e2cce18b18628096196", "name": "Yicheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096197", "name": "Longyu Guan", "hidden": false}, {"_id": "69817e2cce18b18628096198", "name": "Yuanying Guo", "hidden": false}, {"_id": "69817e2cce18b18628096199", "name": "Xiaoru Hao", "hidden": false}, {"_id": "69817e2cce18b1862809619a", "name": "Weiran He", "hidden": false}, {"_id": "69817e2cce18b1862809619b", "name": "Wenyang He", "hidden": false}, {"_id": "69817e2cce18b1862809619c", "name": "Yunjia He", "hidden": false}, {"_id": "69817e2cce18b1862809619d", "name": "Chao Hong", "hidden": false}, {"_id": "69817e2cce18b1862809619e", "name": "Hao Hu", "hidden": false}, {"_id": "69817e2cce18b1862809619f", "name": "Jiaxi Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a0", "name": "Yangyang Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a1", "name": "Zhenxing Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a2", "name": "Ke Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a3", "name": "Ruiyuan Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a4", "name": "Weixiao Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a5", "name": "Zhiqi Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a6", "name": "Tao Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a7", "name": "Zhejun Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a8", "name": "Xinyi Jin", "hidden": false}, {"_id": "69817e2cce18b186280961a9", "name": "Yu Jing", "hidden": false}, {"_id": "69817e2cce18b186280961aa", "name": "Guokun Lai", "hidden": false}, {"_id": "69817e2cce18b186280961ab", "name": "Aidi Li", "hidden": false}, {"_id": "69817e2cce18b186280961ac", "name": "C. Li", "hidden": false}, {"_id": "69817e2cce18b186280961ad", "name": "Cheng Li", "hidden": false}, {"_id": "69817e2cce18b186280961ae", "name": "Fang Li", "hidden": false}, {"_id": "69817e2cce18b186280961af", "name": "Guanghe Li", "hidden": false}, {"_id": "69817e2cce18b186280961b0", "name": "Guanyu Li", "hidden": false}, {"_id": "69817e2cce18b186280961b1", "name": "Haitao Li", "hidden": false}, {"_id": "69817e2cce18b186280961b2", "name": "Haoyang Li", "hidden": false}, {"_id": "69817e2cce18b186280961b3", "name": "Jia Li", "hidden": false}, {"_id": "69817e2cce18b186280961b4", "name": "Jingwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961b5", "name": "Junxiong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b6", "name": "Lincan Li", "hidden": false}, {"_id": "69817e2cce18b186280961b7", "user": {"_id": "6576fe2b42ab083faea19841", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/c91ZKOR2E0gL8iIVkvEUa.jpeg", "isPro": false, "fullname": "Mo Li", "user": "Mor-Li", "type": "user"}, "name": "Mo Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:51.899Z", "hidden": false}, {"_id": "69817e2cce18b186280961b8", "name": "Weihong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b9", "name": "Wentao Li", "hidden": false}, {"_id": "69817e2cce18b186280961ba", "name": "Xinhang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bb", "name": "Xinhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961bc", "name": "Yang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bd", "name": "Yanhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961be", "name": "Yiwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961bf", "name": "Yuxiao Li", "hidden": false}, {"_id": "69817e2cce18b186280961c0", "name": "Zhaowei Li", "hidden": false}, {"_id": "69817e2cce18b186280961c1", "name": "Zheming Li", "hidden": false}, {"_id": "69817e2cce18b186280961c2", "name": "Weilong Liao", "hidden": false}, {"_id": "69817e2cce18b186280961c3", "name": "Jiawei Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c4", "name": "Xiaohan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c5", "name": "Zhishan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c6", "name": "Zichao Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c7", "name": "Cheng Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c8", "name": "Chenyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c9", "name": "Hongzhang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ca", "name": "Liang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cb", "name": "Shaowei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cc", "name": "Shudong Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cd", "name": "Shuran Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ce", "name": "Tianwei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cf", "name": "Tianyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d0", "name": "Weizhou Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d1", "name": "Xiangyan Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d2", "name": "Yangyang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d3", "name": "Yanming Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d4", "name": "Yibo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d5", "name": "Yuanxin Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d6", "name": "Yue Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d7", "name": "Zhengying Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d8", "name": "Zhongnuo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d9", "name": "Enzhe Lu", "hidden": false}, {"_id": "69817e2cce18b186280961da", "name": "Haoyu Lu", "hidden": false}, {"_id": "69817e2cce18b186280961db", "name": "Zhiyuan Lu", "hidden": false}, {"_id": "69817e2cce18b186280961dc", "user": {"_id": "642da1cd99f3110ac27caca5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg", "isPro": false, "fullname": "junyu", "user": "luojunyu", "type": "user"}, "name": "Junyu Luo", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:08.357Z", "hidden": false}, {"_id": "69817e2cce18b186280961dd", "name": "Tongxu Luo", "hidden": false}, {"_id": "69817e2cce18b186280961de", "name": "Yashuo Luo", "hidden": false}, {"_id": "69817e2cce18b186280961df", "name": "Long Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e0", "name": "Yingwei Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e1", "name": "Shaoguang Mao", "hidden": false}, {"_id": "69817e2cce18b186280961e2", "name": "Yuan Mei", "hidden": false}, {"_id": "69817e2cce18b186280961e3", "name": "Xin Men", "hidden": false}, {"_id": "69817e2cce18b186280961e4", "name": "Fanqing Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e5", "name": "Zhiyong Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e6", "name": "Yibo Miao", "hidden": false}, {"_id": "69817e2cce18b186280961e7", "name": "Minqing Ni", "hidden": false}, {"_id": "69817e2cce18b186280961e8", "name": "Kun Ouyang", "hidden": false}, {"_id": "69817e2cce18b186280961e9", "name": "Siyuan Pan", "hidden": false}, {"_id": "69817e2cce18b186280961ea", "name": "Bo Pang", "hidden": false}, {"_id": "69817e2cce18b186280961eb", "name": "Yuchao Qian", "hidden": false}, {"_id": "69817e2cce18b186280961ec", "name": "Ruoyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ed", "name": "Zeyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ee", "name": "Jiezhong Qiu", "hidden": false}, {"_id": "69817e2cce18b186280961ef", "name": "Bowen Qu", "hidden": false}, {"_id": "69817e2cce18b186280961f0", "name": "Zeyu Shang", "hidden": false}, {"_id": "69817e2cce18b186280961f1", "name": "Youbo Shao", "hidden": false}, {"_id": "69817e2cce18b186280961f2", "name": "Tianxiao Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f3", "name": "Zhennan Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f4", "name": "Juanfeng Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f5", "name": "Lidong Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f6", "name": "Shengyuan Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f7", "name": "Feifan Song", "hidden": false}, {"_id": "69817e2cce18b186280961f8", "name": "Pengwei Song", "hidden": false}, {"_id": "69817e2cce18b186280961f9", "name": "Tianhui Song", "hidden": false}, {"_id": "69817e2cce18b186280961fa", "name": "Xiaoxi Song", "hidden": false}, {"_id": "69817e2cce18b186280961fb", "name": "Hongjin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fc", "name": "Jianlin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fd", "name": "Zhaochen Su", "hidden": false}, {"_id": "69817e2cce18b186280961fe", "name": "Lin Sui", "hidden": false}, {"_id": "69817e2cce18b186280961ff", "name": "Jinsong Sun", "hidden": false}, {"_id": "69817e2cce18b18628096200", "name": "Junyao Sun", "hidden": false}, {"_id": "69817e2cce18b18628096201", "name": "Tongyu Sun", "hidden": false}, {"_id": "69817e2cce18b18628096202", "name": "Flood Sung", "hidden": false}, {"_id": "69817e2cce18b18628096203", "name": "Yunpeng Tai", "hidden": false}, {"_id": "69817e2cce18b18628096204", "name": "Chuning Tang", "hidden": false}, {"_id": "69817e2cce18b18628096205", "name": "Heyi Tang", "hidden": false}, {"_id": "69817e2cce18b18628096206", "name": "Xiaojuan Tang", "hidden": false}, {"_id": "69817e2cce18b18628096207", "name": "Zhengyang Tang", "hidden": false}, {"_id": "69817e2cce18b18628096208", "name": "Jiawen Tao", "hidden": false}, {"_id": "69817e2cce18b18628096209", "name": "Shiyuan Teng", "hidden": false}, {"_id": "69817e2cce18b1862809620a", "name": "Chaoran Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620b", "name": "Pengfei Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620c", "name": "Ao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620d", "name": "Bowen Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620e", "name": "Chensi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620f", "name": "Chuang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096210", "name": "Congcong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096211", "name": "Dingkun Wang", "hidden": false}, {"_id": "69817e2cce18b18628096212", "name": "Dinglu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096213", "name": "Dongliang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096214", "name": "Feng Wang", "hidden": false}, {"_id": "69817e2cce18b18628096215", "name": "Hailong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096216", "name": "Haiming Wang", "hidden": false}, {"_id": "69817e2cce18b18628096217", "name": "Hengzhi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096218", "name": "Huaqing Wang", "hidden": false}, {"_id": "69817e2cce18b18628096219", "name": "Hui Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621a", "name": "Jiahao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621b", "name": "Jinhong Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621c", "name": "Jiuzheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621d", "name": "Kaixin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621e", "name": "Linian Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621f", "name": "Qibin Wang", "hidden": false}, {"_id": "69817e2cce18b18628096220", "name": "Shengjie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096221", "name": "Shuyi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096222", "name": "Si Wang", "hidden": false}, {"_id": "69817e2cce18b18628096223", "name": "Wei Wang", "hidden": false}, {"_id": "69817e2cce18b18628096224", "name": "Xiaochen Wang", "hidden": false}, {"_id": "69817e2cce18b18628096225", "name": "Xinyuan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096226", "name": "Yao Wang", "hidden": false}, {"_id": "69817e2cce18b18628096227", "name": "Yejie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096228", "name": "Yipu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096229", "name": "Yiqin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622a", "name": "Yucheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622b", "name": "Yuzhi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622c", "name": "Zhaoji Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622d", "name": "Zhaowei Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622e", "name": "Zhengtao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622f", "name": "Zhexu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096230", "name": "Zihan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096231", "name": "Zizhe Wang", "hidden": false}, {"_id": "69817e2cce18b18628096232", "user": {"_id": "635ddec594e5b275ca7941e8", "avatarUrl": "/avatars/28ebfaee74d31e1de020a3ae735a4c1b.svg", "isPro": false, "fullname": "Chu Wei", "user": "courage17340", "type": "user"}, "name": "Chu Wei", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:17.862Z", "hidden": false}, {"_id": "69817e2cce18b18628096233", "name": "Ming Wei", "hidden": false}, {"_id": "69817e2cce18b18628096234", "name": "Chuan Wen", "hidden": false}, {"_id": "69817e2cce18b18628096235", "user": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "isPro": false, "fullname": "Zichen Wen", "user": "zichenwen", "type": "user"}, "name": "Zichen Wen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:09:43.363Z", "hidden": false}, {"_id": "69817e2cce18b18628096236", "name": "Chengjie Wu", "hidden": false}, {"_id": "69817e2cce18b18628096237", "user": {"_id": "63047ed2412a1b9d381b09c9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg", "isPro": true, "fullname": "Haoning Wu, Teo", "user": "teowu", "type": "user"}, "name": "Haoning Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:49.884Z", "hidden": false}, {"_id": "69817e2cce18b18628096238", "name": "Junyan Wu", "hidden": false}, {"_id": "69817e2cce18b18628096239", "name": "Rucong Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623a", "name": "Wenhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623b", "name": "Yuefeng Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623c", "name": "Yuhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623d", "name": "Yuxin Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623e", "name": "Zijian Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623f", "name": "Chenjun Xiao", "hidden": false}, {"_id": "69817e2cce18b18628096240", "name": "Jin Xie", "hidden": false}, {"_id": "69817e2cce18b18628096241", "name": "Xiaotong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096242", "name": "Yuchong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096243", "name": "Yifei Xin", "hidden": false}, {"_id": "69817e2cce18b18628096244", "name": "Bowei Xing", "hidden": false}, {"_id": "69817e2cce18b18628096245", "name": "Boyu Xu", "hidden": false}, {"_id": "69817e2cce18b18628096246", "name": "Jianfan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096247", "name": "Jing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096248", "name": "Jinjing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096249", "name": "L. H. Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624a", "name": "Lin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624b", "name": "Suting Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624c", "name": "Weixin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624d", "name": "Xinbo Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624e", "name": "Xinran Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624f", "name": "Yangchuan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096250", "name": "Yichang Xu", "hidden": false}, {"_id": "69817e2cce18b18628096251", "name": "Yuemeng Xu", "hidden": false}, {"_id": "69817e2cce18b18628096252", "name": "Zelai Xu", "hidden": false}, {"_id": "69817e2cce18b18628096253", "name": "Ziyao Xu", "hidden": false}, {"_id": "69817e2cce18b18628096254", "name": "Junjie Yan", "hidden": false}, {"_id": "69817e2cce18b18628096255", "name": "Yuzi Yan", "hidden": false}, {"_id": "69817e2cce18b18628096256", "name": "Guangyao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096257", "name": "Hao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096258", "name": "Junwei Yang", "hidden": false}, {"_id": "69817e2cce18b18628096259", "name": "Kai Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625a", "name": "Ningyuan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625b", "name": "Ruihan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625c", "name": "Xiaofei Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625d", "name": "Xinlong Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625e", "name": "Ying Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625f", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096260", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096261", "name": "Zhen Yang", "hidden": false}, {"_id": "69817e2cce18b18628096262", "name": "Zhilin Yang", "hidden": false}, {"_id": "69817e2cce18b18628096263", "name": "Zonghan Yang", "hidden": false}, {"_id": "69817e2cce18b18628096264", "user": {"_id": "642bcd9be8dfcc1fe4f4f853", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642bcd9be8dfcc1fe4f4f853/M9Yqkyt66dnWWCwmBZ8l0.jpeg", "isPro": false, "fullname": "Haotian Yao", "user": "skylark-95", "type": "user"}, "name": "Haotian Yao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:12.739Z", "hidden": false}, {"_id": "69817e2cce18b18628096265", "name": "Dan Ye", "hidden": false}, {"_id": "69817e2cce18b18628096266", "name": "Wenjie Ye", "hidden": false}, {"_id": "69817e2cce18b18628096267", "name": "Zhuorui Ye", "hidden": false}, {"_id": "69817e2cce18b18628096268", "name": "Bohong Yin", "hidden": false}, {"_id": "69817e2cce18b18628096269", "name": "Chengzhen Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626a", "name": "Longhui Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626b", "name": "Tao Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626c", "name": "Tianxiang Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626d", "name": "Enming Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626e", "name": "Mengjie Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626f", "name": "Xiaokun Yuan", "hidden": false}, {"_id": "69817e2cce18b18628096270", "name": "Yang Yue", "hidden": false}, {"_id": "69817e2cce18b18628096271", "name": "Weihao Zeng", "hidden": false}, {"_id": "69817e2cce18b18628096272", "name": "Dunyuan Zha", "hidden": false}, {"_id": "69817e2cce18b18628096273", "name": "Haobing Zhan", "hidden": false}, {"_id": "69817e2cce18b18628096274", "name": "Dehao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096275", "name": "Hao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096276", "name": "Jin Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096277", "name": "Puqi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096278", "name": "Qiao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096279", "name": "Rui Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627a", "name": "Xiaobin Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627b", "name": "Y. Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627c", "name": "Yadong Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627d", "name": "Yangkun Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627e", "name": "Yichi Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627f", "name": "Yizhi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096280", "name": "Yongting Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096281", "name": "Yu Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096282", "name": "Yushun Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096283", "name": "Yutao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096284", "name": "Yutong Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096285", "name": "Zheng Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096286", "name": "Chenguang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096287", "name": "Feifan Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096288", "name": "Jinxiang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096289", "name": "Shuai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628a", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628b", "name": "Yikai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628c", "name": "Zijia Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628d", "name": "Huabin Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628e", "name": "Ruihan Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628f", "name": "Shaojie Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096290", "name": "Tengyang Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096291", "name": "Junfeng Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096292", "user": {"_id": "62b6d20416ff90e6198301b6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656148456743-noauth.png", "isPro": false, "fullname": "Longguang Zhong", "user": "GGLS", "type": "user"}, "name": "Longguang Zhong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:14.989Z", "hidden": false}, {"_id": "69817e2cce18b18628096293", "name": "Weiming Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096294", "name": "M. Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096295", "name": "Runjie Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096296", "name": "Xinyu Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096297", "name": "Zaida Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096298", "name": "Jinguo Zhu", "hidden": false}, {"_id": "69817e2cce18b18628096299", "name": "Liya Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629a", "name": "Xinhao Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629b", "name": "Yuxuan Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629c", "name": "Zhen Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629d", "name": "Jingze Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629e", "name": "Weiyu Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629f", "name": "Ying Zou", "hidden": false}, {"_id": "69817e2cce18b186280962a0", "name": "Xinxing Zu", "hidden": false}], "publishedAt": "2026-02-02T16:17:38.000Z", "submittedOnDailyAt": "2026-02-03T02:18:48.721Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "upvotes": 149, "discussionId": "69817e2cce18b186280962a1", "projectPage": "https://huggingface.co/moonshotai/Kimi-K2.5", "ai_summary": "Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.", "ai_keywords": ["multimodal agentic model", "joint text-vision pre-training", "zero-vision SFT", "joint text-vision reinforcement learning", "Agent Swarm", "self-directed parallel agent orchestration framework", "heterogeneous sub-problems"], "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "summary_zh": "Translation unavailable", "summary_simple": "Summary unavailable"}, "publishedAt": "2026-02-02T11:17:38.000Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02276.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 227, "isUserFollowing": false}, "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Feb 27, 2026";
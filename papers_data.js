window.trendingPapers = {
    "today": [{"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "name": "Chunkang Zhang", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 70, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 22, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u662f\u4e00\u79cd\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5168\u5c40\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u7b56\u7565\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u6a21\u5757\u4e3b\u8981\u4f5c\u4e3a\u88ab\u52a8\u5b58\u50a8\uff0c\u65e0\u6cd5\u6709\u6548\u6574\u5408\u4fe1\u606f\uff0c\u5bfc\u81f4\u63a8\u7406\u788e\u7247\u5316\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HGMem\uff0c\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u80fd\u591f\u52a8\u6001\u8868\u793a\u590d\u6742\u63a8\u7406\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u8fb9\u8fde\u63a5\u4e0d\u540c\u7684\u8bb0\u5fc6\u5355\u5143\uff0c\u5f62\u6210\u66f4\u9ad8\u9636\u7684\u4e92\u52a8\uff0c\u589e\u5f3a\u4e86\u6574\u4f53\u7406\u89e3\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHGMem\u5728\u591a\u4e2a\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u591a\u6b65\u9aa4RAG\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps large language models (LLMs) with complex reasoning tasks.</li>\n    <li>Current memory designs mainly store facts without connecting them, limiting the ability to understand relationships between information.</li>\n    <li>HGMem is a new memory mechanism that uses a hypergraph structure to create dynamic connections between facts for better reasoning.</li>\n    <li>This approach allows for deeper understanding and improved reasoning in multi-step tasks.</li>\n    <li>Tests show that HGMem significantly outperforms traditional memory systems on various challenging datasets.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24880", "authors": [{"_id": "69561fbf832867f253525726", "name": "Zhenda Xie", "hidden": false}, {"_id": "69561fbf832867f253525727", "name": "Yixuan Wei", "hidden": false}, {"_id": "69561fbf832867f253525728", "name": "Huanqi Cao", "hidden": false}, {"_id": "69561fbf832867f253525729", "name": "Chenggang Zhao", "hidden": false}, {"_id": "69561fbf832867f25352572a", "name": "Chengqi Deng", "hidden": false}, {"_id": "69561fbf832867f25352572b", "name": "Jiashi Li", "hidden": false}, {"_id": "69561fbf832867f25352572c", "name": "Damai Dai", "hidden": false}, {"_id": "69561fbf832867f25352572d", "name": "Huazuo Gao", "hidden": false}, {"_id": "69561fbf832867f25352572e", "name": "Jiang Chang", "hidden": false}, {"_id": "69561fbf832867f25352572f", "name": "Liang Zhao", "hidden": false}, {"_id": "69561fbf832867f253525730", "name": "Shangyan Zhou", "hidden": false}, {"_id": "69561fbf832867f253525731", "name": "Zhean Xu", "hidden": false}, {"_id": "69561fbf832867f253525732", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "69561fbf832867f253525733", "name": "Wangding Zeng", "hidden": false}, {"_id": "69561fbf832867f253525734", "name": "Shengding Hu", "hidden": false}, {"_id": "69561fbf832867f253525735", "name": "Yuqing Wang", "hidden": false}, {"_id": "69561fbf832867f253525736", "name": "Jingyang Yuan", "hidden": false}, {"_id": "69561fbf832867f253525737", "name": "Lean Wang", "hidden": false}, {"_id": "69561fbf832867f253525738", "name": "Wenfeng Liang", "hidden": false}], "publishedAt": "2025-12-31T14:16:26.000Z", "submittedOnDailyAt": "2026-01-01T07:30:32.169Z", "title": "mHC: Manifold-Constrained Hyper-Connections", "submittedOnDailyBy": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "isPro": false, "fullname": "Adina Yakefu", "user": "AdinaY", "type": "user"}, "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.", "upvotes": 59, "discussionId": "69561fc0832867f253525739", "ai_summary": "Manifold-Constrained Hyper-Connections (mHC) stabilize and scale residual connection architectures by restoring identity mapping properties through manifold projection and infrastructure optimization.", "ai_keywords": ["Hyper-Connections (HC)", "Manifold-Constrained Hyper-Connections (mHC)", "residual connections", "residual stream width", "connectivity patterns", "identity mapping property", "training instability", "memory access overhead", "manifold projection", "infrastructure optimization", "scalability"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fde\u63a5\u65b9\u5f0f\uff0c\u79f0\u4e3a\u8d85\u8fde\u63a5\uff08HC\uff09\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u7684\u6b8b\u5dee\u8fde\u63a5\u3002</li>\n    <li>\u867d\u7136\u8fd9\u79cd\u65b0\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f46\u4e5f\u5bfc\u81f4\u4e86\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u548c\u6269\u5c55\u6027\u95ee\u9898\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u201c\u6d41\u5f62\u7ea6\u675f\u8d85\u8fde\u63a5\u201d\uff08mHC\uff09\u6846\u67b6\uff0c\u6062\u590d\u4e86\u6b8b\u5dee\u8fde\u63a5\u7684\u8eab\u4efd\u6620\u5c04\u7279\u6027\u3002</li>\n    <li>mHC\u6846\u67b6\u5728\u4fdd\u8bc1\u6548\u7387\u7684\u540c\u65f6\uff0c\u7ecf\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5728\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\u6548\u679c\u663e\u8457\u3002</li>\n    <li>mHC\u88ab\u89c6\u4e3a\u8d85\u8fde\u63a5\u7684\u7075\u6d3b\u6269\u5c55\uff0c\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u62d3\u6251\u67b6\u6784\u8bbe\u8ba1\u548c\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u65b9\u5411\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent studies have improved residual connections in neural networks through Hyper-Connections (HC), but this has led to training issues and increased memory use.</li>\n    <li>To solve these problems, the authors propose a new framework called Manifold-Constrained Hyper-Connections (mHC) that restores the original benefits of residual connections.</li>\n    <li>mHC maintains the identity mapping property and optimizes efficiency for better performance during training.</li>\n    <li>Tests show that mHC works well for large-scale training, providing better performance and scalability compared to HC.</li>\n    <li>The authors believe mHC will enhance our understanding of neural network design and inspire future developments in foundational models.</li>\n</ul>"}, "publishedAt": "2025-12-31T09:16:26.000Z", "title": "mHC: Manifold-Constrained Hyper-Connections", "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24880.png", "numComments": 1, "submittedBy": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "fullname": "Adina Yakefu", "name": "AdinaY", "type": "user", "isPro": false, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 1140}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24618", "authors": [{"_id": "6955d543832867f25352555d", "name": "Junru Lu", "hidden": false}, {"_id": "6955d543832867f25352555e", "name": "Jiarui Qin", "hidden": false}, {"_id": "6955d543832867f25352555f", "name": "Lingfeng Qiao", "hidden": false}, {"_id": "6955d543832867f253525560", "name": "Yinghui Li", "hidden": false}, {"_id": "6955d543832867f253525561", "name": "Xinyi Dai", "hidden": false}, {"_id": "6955d543832867f253525562", "name": "Bo Ke", "hidden": false}, {"_id": "6955d543832867f253525563", "name": "Jianfeng He", "hidden": false}, {"_id": "6955d543832867f253525564", "name": "Ruizhi Qiao", "hidden": false}, {"_id": "6955d543832867f253525565", "name": "Di Yin", "hidden": false}, {"_id": "6955d543832867f253525566", "name": "Xing Sun", "hidden": false}, {"_id": "6955d543832867f253525567", "name": "Yunsheng Wu", "hidden": false}, {"_id": "6955d543832867f253525568", "name": "Yinsong Liu", "hidden": false}, {"_id": "6955d543832867f253525569", "name": "Shuangyin Liu", "hidden": false}, {"_id": "6955d543832867f25352556a", "name": "Mingkong Tang", "hidden": false}, {"_id": "6955d543832867f25352556b", "name": "Haodong Lin", "hidden": false}, {"_id": "6955d543832867f25352556c", "name": "Jiayi Kuang", "hidden": false}, {"_id": "6955d543832867f25352556d", "name": "Fanxu Meng", "hidden": false}, {"_id": "6955d543832867f25352556e", "name": "Xiaojuan Tang", "hidden": false}, {"_id": "6955d543832867f25352556f", "name": "Yunjia Xi", "hidden": false}, {"_id": "6955d543832867f253525570", "name": "Junjie Huang", "hidden": false}, {"_id": "6955d543832867f253525571", "name": "Haotong Yang", "hidden": false}, {"_id": "6955d543832867f253525572", "name": "Zhenyi Shen", "hidden": false}, {"_id": "6955d543832867f253525573", "name": "Yangning Li", "hidden": false}, {"_id": "6955d543832867f253525574", "name": "Qianwen Zhang", "hidden": false}, {"_id": "6955d543832867f253525575", "name": "Yifei Yu", "hidden": false}, {"_id": "6955d543832867f253525576", "name": "Siyu An", "hidden": false}, {"_id": "6955d543832867f253525577", "name": "Junnan Dong", "hidden": false}, {"_id": "6955d543832867f253525578", "name": "Qiufeng Wang", "hidden": false}, {"_id": "6955d543832867f253525579", "name": "Jie Wang", "hidden": false}, {"_id": "6955d543832867f25352557a", "name": "Keyu Chen", "hidden": false}, {"_id": "6955d543832867f25352557b", "name": "Wei Wen", "hidden": false}, {"_id": "6955d543832867f25352557c", "name": "Taian Guo", "hidden": false}, {"_id": "6955d543832867f25352557d", "name": "Zhifeng Shen", "hidden": false}, {"_id": "6955d543832867f25352557e", "name": "Daohai Yu", "hidden": false}, {"_id": "6955d543832867f25352557f", "name": "Jiahao Li", "hidden": false}, {"_id": "6955d543832867f253525580", "name": "Ke Li", "hidden": false}, {"_id": "6955d543832867f253525581", "name": "Zongyi Li", "hidden": false}, {"_id": "6955d543832867f253525582", "name": "Xiaoyu Tan", "hidden": false}], "publishedAt": "2025-12-31T04:25:11.000Z", "submittedOnDailyAt": "2026-01-01T00:33:09.720Z", "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "upvotes": 43, "discussionId": "6955d543832867f253525583", "ai_summary": "Youtu-LLM is a lightweight language model optimized for computational efficiency and agentic intelligence through a compact architecture, STEM-focused training curriculum, and scalable mid-training strategies for planning and reasoning tasks.", "ai_keywords": ["Multi-Latent Attention (MLA) architecture", "STEM-oriented vocabulary", "128k context window", "Commonsense-STEM-Agent Curriculum", "multi-stage training strategy", "agentic mid-training", "data construction schemes", "planning and reflection behaviors", "long-context reasoning", "state tracking", "agentic capabilities"], "summary_zh": "<ul>\n    <li>Youtu-LLM \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e14\u5f3a\u5927\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u6709\u9ad8\u6548\u7684\u8ba1\u7b97\u80fd\u529b\u548c\u667a\u80fd\u3002</li>\n    <li>\u5b83\u4ece\u5934\u5f00\u59cb\u9884\u8bad\u7ec3\uff0c\u4e13\u6ce8\u4e8e\u57f9\u517b\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4e8e\u84b8\u998f\u6280\u672f\u3002</li>\n    <li>\u8be5\u6a21\u578b\u652f\u6301 128k \u7684\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u9002\u7528\u4e8e\u957f\u671f\u63a8\u7406\u548c\u72b6\u6001\u8ddf\u8e2a\u4efb\u52a1\u3002</li>\n    <li>\u901a\u8fc7\u4f7f\u7528\u5927\u7ea6 11T \u4ee4\u724c\u7684\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u6a21\u578b\u83b7\u5f97\u4e86\u6df1\u5c42\u7684\u8ba4\u77e5\u80fd\u529b\u3002</li>\n    <li>Youtu-LLM \u5728\u5c0f\u4e8e 2B \u7684\u8bed\u8a00\u6a21\u578b\u4e2d\u8bbe\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u6807\u51c6\uff0c\u5c24\u5176\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Youtu-LLM is a new lightweight language model designed for efficiency and intelligent reasoning.</li>\n    <li>It has a unique architecture that allows it to handle long contexts, supporting tasks that require deep understanding over extended information.</li>\n    <li>The model was trained on a large dataset of 11 trillion tokens, focusing on both general knowledge and complex STEM tasks to build strong cognitive skills.</li>\n    <li>Youtu-LLM uses advanced training techniques to create diverse data for improving its planning and reflection abilities.</li>\n    <li>It outperforms other models of similar size in both general and specialized tasks, proving that smaller models can still be very capable.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:25:11.000Z", "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24618.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 197}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24873", "authors": [{"_id": "6955e3f8832867f2535255cd", "name": "Weixun Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255ce", "name": "XiaoXiao Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255cf", "name": "Wanhe An", "hidden": false}, {"_id": "6955e3f8832867f2535255d0", "name": "Fangwen Dai", "hidden": false}, {"_id": "6955e3f8832867f2535255d1", "name": "Wei Gao", "hidden": false}, {"_id": "6955e3f8832867f2535255d2", "name": "Yancheng He", "hidden": false}, {"_id": "6955e3f8832867f2535255d3", "name": "Ju Huang", "hidden": false}, {"_id": "6955e3f8832867f2535255d4", "name": "Qiang Ji", "hidden": false}, {"_id": "6955e3f8832867f2535255d5", "name": "Hanqi Jin", "hidden": false}, {"_id": "6955e3f8832867f2535255d6", "name": "Xiaoyang Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d7", "name": "Yang Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d8", "name": "Zhongwen Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d9", "name": "Shirong Lin", "hidden": false}, {"_id": "6955e3f8832867f2535255da", "name": "Jiashun Liu", "hidden": false}, {"_id": "6955e3f8832867f2535255db", "name": "Zenan Liu", "hidden": false}, {"_id": "6955e3f8832867f2535255dc", "name": "Tao Luo", "hidden": false}, {"_id": "6955e3f8832867f2535255dd", "name": "Dilxat Muhtar", "hidden": false}, {"_id": "6955e3f8832867f2535255de", "name": "Yuanbin Qu", "hidden": false}, {"_id": "6955e3f8832867f2535255df", "name": "Jiaqiang Shi", "hidden": false}, {"_id": "6955e3f8832867f2535255e0", "name": "Qinghui Sun", "hidden": false}, {"_id": "6955e3f8832867f2535255e1", "name": "Yingshui Tan", "hidden": false}, {"_id": "6955e3f8832867f2535255e2", "name": "Hao Tang", "hidden": false}, {"_id": "6955e3f8832867f2535255e3", "name": "Runze Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e4", "name": "Yi Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e5", "name": "Zhaoguo Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e6", "name": "Yanan Wu", "hidden": false}, {"_id": "6955e3f8832867f2535255e7", "name": "Shaopan Xiong", "hidden": false}, {"_id": "6955e3f8832867f2535255e8", "name": "Binchen Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255e9", "name": "Xander Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255ea", "name": "Yuchi Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255eb", "name": "Qipeng Zhang", "hidden": false}, {"_id": "6955e3f8832867f2535255ec", "name": "Xixia Zhang", "hidden": false}, {"_id": "6955e3f8832867f2535255ed", "name": "Haizhou Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255ee", "name": "Jie Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255ef", "name": "Shuaibing Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255f0", "name": "Baihui Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f1", "name": "Jianhui Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f2", "name": "Suhang Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f3", "name": "Yanni Zhu", "hidden": false}, {"_id": "6955e3f8832867f2535255f4", "name": "Mengze Cai", "hidden": false}, {"_id": "6955e3f8832867f2535255f5", "name": "Kerui Cao", "hidden": false}, {"_id": "6955e3f8832867f2535255f6", "name": "Xitong Chen", "hidden": false}, {"_id": "6955e3f8832867f2535255f7", "name": "Yue Dai", "hidden": false}, {"_id": "6955e3f8832867f2535255f8", "name": "Lifan Du", "hidden": false}, {"_id": "6955e3f8832867f2535255f9", "name": "Tao Feng", "hidden": false}, {"_id": "6955e3f8832867f2535255fa", "name": "Tao He", "hidden": false}, {"_id": "6955e3f8832867f2535255fb", "name": "Jin Hu", "hidden": false}, {"_id": "6955e3f8832867f2535255fc", "name": "Yijie Hu", "hidden": false}, {"_id": "6955e3f8832867f2535255fd", "name": "Ziyu Jiang", "hidden": false}, {"_id": "6955e3f8832867f2535255fe", "name": "Cheng Li", "hidden": false}, {"_id": "6955e3f8832867f2535255ff", "name": "Xiang Li", "hidden": false}, {"_id": "6955e3f8832867f253525600", "name": "Jing Liang", "hidden": false}, {"_id": "6955e3f8832867f253525601", "name": "Chonghuan Liu", "hidden": false}, {"_id": "6955e3f8832867f253525602", "name": "ZhenDong Liu", "hidden": false}, {"_id": "6955e3f8832867f253525603", "name": "Haodong Mi", "hidden": false}, {"_id": "6955e3f8832867f253525604", "name": "Yanhu Mo", "hidden": false}, {"_id": "6955e3f8832867f253525605", "name": "Junjia Ni", "hidden": false}, {"_id": "6955e3f8832867f253525606", "name": "Shixin Pei", "hidden": false}, {"_id": "6955e3f8832867f253525607", "name": "Jingyu Shen", "hidden": false}, {"_id": "6955e3f8832867f253525608", "name": "XiaoShuai Song", "hidden": false}, {"_id": "6955e3f8832867f253525609", "name": "Cecilia Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560a", "name": "Chaofan Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560b", "name": "Kangyu Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560c", "name": "Pei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560d", "name": "Tao Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560e", "name": "Wei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560f", "name": "Ke Xiao", "hidden": false}, {"_id": "6955e3f8832867f253525610", "name": "Mingyu Xu", "hidden": false}, {"_id": "6955e3f8832867f253525611", "name": "Tiange Xu", "hidden": false}, {"_id": "6955e3f8832867f253525612", "name": "Nan Ya", "hidden": false}, {"_id": "6955e3f8832867f253525613", "name": "Siran Yang", "hidden": false}, {"_id": "6955e3f8832867f253525614", "name": "Jianan Ye", "hidden": false}, {"_id": "6955e3f8832867f253525615", "name": "Yaxing Zang", "hidden": false}, {"_id": "6955e3f8832867f253525616", "name": "Duo Zhang", "hidden": false}, {"_id": "6955e3f8832867f253525617", "name": "Junbo Zhang", "hidden": false}, {"_id": "6955e3f8832867f253525618", "name": "Boren Zheng", "hidden": false}, {"_id": "6955e3f8832867f253525619", "name": "Wanxi Deng", "hidden": false}, {"_id": "6955e3f8832867f25352561a", "name": "Ling Pan", "hidden": false}, {"_id": "6955e3f8832867f25352561b", "name": "Lin Qu", "hidden": false}, {"_id": "6955e3f8832867f25352561c", "name": "Wenbo Su", "hidden": false}, {"_id": "6955e3f8832867f25352561d", "name": "Jiamang Wang", "hidden": false}, {"_id": "6955e3f8832867f25352561e", "name": "Wei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352561f", "name": "Hu Wei", "hidden": false}, {"_id": "6955e3f8832867f253525620", "name": "Minggang Wu", "hidden": false}, {"_id": "6955e3f8832867f253525621", "name": "Cheng Yu", "hidden": false}, {"_id": "6955e3f8832867f253525622", "name": "Bing Zhao", "hidden": false}, {"_id": "6955e3f8832867f253525623", "name": "Zhicheng Zheng", "hidden": false}, {"_id": "6955e3f8832867f253525624", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-12-31T14:03:39.000Z", "submittedOnDailyAt": "2026-01-01T00:33:23.374Z", "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.", "upvotes": 33, "discussionId": "6955e3f8832867f253525625", "ai_summary": "The Agentic Learning Ecosystem (ALE) introduces a principled infrastructure for agent development, combining post-training optimization, sandbox environments, and policy alignment to enhance long-horizon training stability and performance in real-world tasks.", "ai_keywords": ["ROLL (post-training framework)", "ROCK (sandbox environment manager)", "iFlow CLI (agent framework)", "ROME (agentic model)", "data composition protocols", "Interaction-based Policy Alignment (IPA)", "semantic interaction chunks", "Terminal Bench Pro", "SWE-bench Verified"], "summary_zh": "<ul>\n    <li>Agentic crafting \u9700\u8981\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8fdb\u884c\u591a\u8f6e\u64cd\u4f5c\uff0c\u901a\u8fc7\u884c\u52a8\u3001\u89c2\u5bdf\u7ed3\u679c\u548c\u8fed\u4ee3\u6539\u8fdb\u3002</li>\n    <li>\u5f00\u6e90\u793e\u533a\u7f3a\u4e4f\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u751f\u6001\u7cfb\u7edf\u6765\u7b80\u5316\u4ee3\u7406\u5f00\u53d1\uff0c\u56e0\u6b64\u6211\u4eec\u5f15\u5165\u4e86\u4ee3\u7406\u5b66\u4e60\u751f\u6001\u7cfb\u7edf\uff08ALE\uff09\u3002</li>\n    <li>ALE \u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1aROLL\uff08\u6743\u91cd\u4f18\u5316\u6846\u67b6\uff09\u3001ROCK\uff08\u8f68\u8ff9\u751f\u6210\u6c99\u7bb1\u73af\u5883\u7ba1\u7406\u5668\uff09\u548c iFlow CLI\uff08\u9ad8\u6548\u7684\u4ee3\u7406\u6846\u67b6\uff09\u3002</li>\n    <li>\u6211\u4eec\u53d1\u5e03\u4e86 ROME\uff0c\u4e00\u4e2a\u57fa\u4e8e ALE \u7684\u5f00\u6e90\u4ee3\u7406\u6a21\u578b\uff0c\u7ecf\u8fc7\u4e00\u767e\u4e07\u6761\u8f68\u8ff9\u7684\u8bad\u7ec3\u3002</li>\n    <li>ROME \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bc1\u660e\u4e86 ALE \u57fa\u7840\u8bbe\u65bd\u7684\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Agentic crafting helps large language models (LLMs) learn by acting, observing, and improving over time.</li>\n    <li>The open-source community lacks a complete system for developing these intelligent agents, which is addressed by the new Agentic Learning Ecosystem (ALE).</li>\n    <li>ALE has three main parts: ROLL for optimizing model weights, ROCK for managing environments, and iFlow CLI for organizing agent context.</li>\n    <li>ROME, an open-source agent developed using ALE, is trained on over one million interactions to enhance its performance.</li>\n    <li>ROME uses a new method for policy optimization and has shown strong results in various benchmarks, validating the ALE approach.</li>\n</ul>"}, "publishedAt": "2025-12-31T09:03:39.000Z", "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem", "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24873.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 197}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24617", "authors": [{"_id": "69573165832867f253525871", "user": {"_id": "646b4c9fdf2609a541c0866e", "avatarUrl": "/avatars/7ec6c709017dcf32d4ac49d1e3820328.svg", "isPro": false, "fullname": "Qu", "user": "ScottQu", "type": "user"}, "name": "Xingwei Qu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:07.662Z", "hidden": false}, {"_id": "69573165832867f253525872", "name": "Shaowen Wang", "hidden": false}, {"_id": "69573165832867f253525873", "name": "Zihao Huang", "hidden": false}, {"_id": "69573165832867f253525874", "user": {"_id": "64e851825ddcace745ba15bd", "avatarUrl": "/avatars/7b6612c411222974d9ea181784eef915.svg", "isPro": false, "fullname": "Kai Hua", "user": "kkish", "type": "user"}, "name": "Kai Hua", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:53.467Z", "hidden": false}, {"_id": "69573165832867f253525875", "name": "Fan Yin", "hidden": false}, {"_id": "69573165832867f253525876", "name": "Rui-Jie Zhu", "hidden": false}, {"_id": "69573165832867f253525877", "name": "Jundong Zhou", "hidden": false}, {"_id": "69573165832867f253525878", "name": "Qiyang Min", "hidden": false}, {"_id": "69573165832867f253525879", "name": "Zihao Wang", "hidden": false}, {"_id": "69573165832867f25352587a", "name": "Yizhi Li", "hidden": false}, {"_id": "69573165832867f25352587b", "name": "Tianyu Zhang", "hidden": false}, {"_id": "69573165832867f25352587c", "name": "He Xing", "hidden": false}, {"_id": "69573165832867f25352587d", "name": "Zheng Zhang", "hidden": false}, {"_id": "69573165832867f25352587e", "name": "Yuxuan Song", "hidden": false}, {"_id": "69573165832867f25352587f", "name": "Tianyu Zheng", "hidden": false}, {"_id": "69573165832867f253525880", "name": "Zhiyuan Zeng", "hidden": false}, {"_id": "69573165832867f253525881", "name": "Chenghua Lin", "hidden": false}, {"_id": "69573165832867f253525882", "name": "Ge Zhang", "hidden": false}, {"_id": "69573165832867f253525883", "name": "Wenhao Huang", "hidden": false}], "publishedAt": "2025-12-31T04:19:33.000Z", "submittedOnDailyAt": "2026-01-02T00:17:55.450Z", "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space", "submittedOnDailyBy": {"_id": "646b4c9fdf2609a541c0866e", "avatarUrl": "/avatars/7ec6c709017dcf32d4ac49d1e3820328.svg", "isPro": false, "fullname": "Qu", "user": "ScottQu", "type": "user"}, "summary": "Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first compression-aware scaling law, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a decoupled \u03bcP parametrization that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting (R=4, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a +2.69\\% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.", "upvotes": 32, "discussionId": "69573165832867f253525884", "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u6240\u6709\u8bcd\u5143\u8fdb\u884c\u7edf\u4e00\u8ba1\u7b97\uff0c\u4f46\u8bed\u8a00\u7684\u4fe1\u606f\u5bc6\u5ea6\u662f\u4e0d\u5747\u5300\u7684\u3002</li>\n    <li>\u52a8\u6001\u5927\u578b\u6982\u5ff5\u6a21\u578b\uff08DLCM\uff09\u662f\u4e00\u79cd\u65b0\u7684\u8bed\u8a00\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u8bed\u4e49\u8fb9\u754c\u6765\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002</li>\n    <li>DLCM \u4e0d\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u7684\u8bed\u8a00\u5355\u4f4d\uff0c\u800c\u662f\u7aef\u5230\u7aef\u5730\u53d1\u73b0\u53ef\u53d8\u957f\u5ea6\u7684\u6982\u5ff5\u3002</li>\n    <li>\u5f15\u5165\u4e86\u538b\u7f29\u611f\u77e5\u7684\u6269\u5c55\u6cd5\u5219\uff0c\u4f7f\u8ba1\u7b97\u5206\u914d\u66f4\u5408\u7406\uff0c\u5e76\u652f\u6301\u4e0d\u540c\u5bbd\u5ea6\u548c\u538b\u7f29\u6a21\u5f0f\u7684\u8d85\u53c2\u6570\u8f6c\u79fb\u3002</li>\n    <li>\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0cDLCM \u5c06\u7ea6\u4e09\u5206\u4e4b\u4e00\u7684\u63a8\u7406\u8ba1\u7b97\u91cd\u65b0\u5206\u914d\u5230\u66f4\u9ad8\u5bb9\u91cf\u7684\u63a8\u7406\u6838\u5fc3\uff0c\u5e73\u5747\u63d0\u9ad8\u4e862.69%\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) use the same amount of computation for all parts of the text, even though some parts are more important than others.</li>\n    <li>The proposed Dynamic Large Concept Models (DLCM) framework learns important language boundaries and focuses computation on these key areas.</li>\n    <li>DLCM can identify flexible concepts without needing predefined language rules, improving efficiency in reasoning.</li>\n    <li>It introduces a new way to understand how different capacities (token-level and concept-level) relate to each other, allowing for better allocation of computing resources.</li>\n    <li>In tests, DLCM showed a +2.69% improvement in performance on 12 benchmarks while using the same amount of computing power.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:19:33.000Z", "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space", "summary": "Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first compression-aware scaling law, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a decoupled \u03bcP parametrization that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting (R=4, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a +2.69\\% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24617.png", "numComments": 4, "submittedBy": {"_id": "646b4c9fdf2609a541c0866e", "avatarUrl": "/avatars/7ec6c709017dcf32d4ac49d1e3820328.svg", "fullname": "Qu", "name": "ScottQu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24165", "authors": [{"_id": "69571b38832867f25352584d", "user": {"_id": "67247adb73d1eb17b6bfd27c", "avatarUrl": "/avatars/57bdbb7362f9854c87dd0a71ae071652.svg", "isPro": false, "fullname": "Zefeng He", "user": "yhx12", "type": "user"}, "name": "Zefeng He", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:20.236Z", "hidden": false}, {"_id": "69571b38832867f25352584e", "name": "Xiaoye Qu", "hidden": false}, {"_id": "69571b38832867f25352584f", "name": "Yafu Li", "hidden": false}, {"_id": "69571b38832867f253525850", "user": {"_id": "629454301ae2138079f7ff31", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629454301ae2138079f7ff31/rVtbF-j06gDiYzomTeVTc.jpeg", "isPro": false, "fullname": "Tong Zhu", "user": "Spico", "type": "user"}, "name": "Tong Zhu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:16.903Z", "hidden": false}, {"_id": "69571b38832867f253525851", "name": "Siyuan Huang", "hidden": false}, {"_id": "69571b38832867f253525852", "name": "Yu Cheng", "hidden": false}], "publishedAt": "2025-12-30T11:51:18.000Z", "submittedOnDailyAt": "2026-01-02T03:34:41.232Z", "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models", "submittedOnDailyBy": {"_id": "629454301ae2138079f7ff31", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629454301ae2138079f7ff31/rVtbF-j06gDiYzomTeVTc.jpeg", "isPro": false, "fullname": "Tong Zhu", "user": "Spico", "type": "user"}, "summary": "While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\\%) and Gemini-3-Flash (+111.6\\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.", "upvotes": 21, "discussionId": "69571b38832867f253525853", "projectPage": "https://diffthinker-project.github.io/", "githubRepo": "https://github.com/lcqysl/DiffThinker", "githubRepoAddedBy": "user", "githubStars": 24, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u4f46\u4ecd\u7136\u4ee5\u6587\u672c\u4e3a\u4e2d\u5fc3\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4e0d\u4f73\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u591a\u6a21\u6001\u63a8\u7406\u8303\u5f0f\uff0c\u5e76\u4ecb\u7ecd\u4e86\u57fa\u4e8e\u6269\u6563\u7684\u63a8\u7406\u6846\u67b6DiffThinker\u3002</li>\n    <li>DiffThinker\u5c06\u591a\u6a21\u6001\u63a8\u7406\u91cd\u65b0\u6784\u5efa\u4e3a\u751f\u6210\u56fe\u50cf\u5230\u56fe\u50cf\u7684\u4efb\u52a1\uff0c\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u903b\u8f91\u4e00\u81f4\u6027\u548c\u7a7a\u95f4\u7cbe\u786e\u5ea6\u3002</li>\n    <li>\u901a\u8fc7\u4e0e\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf\u6bd4\u8f83\uff0c\u53d1\u73b0DiffThinker\u5177\u6709\u6548\u7387\u3001\u53ef\u63a7\u6027\u3001\u539f\u751f\u5e76\u884c\u6027\u548c\u534f\u4f5c\u7b49\u56db\u4e2a\u6838\u5fc3\u7279\u6027\u3002</li>\n    <li>\u5728\u591a\u4e2a\u9886\u57df\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cDiffThinker\u663e\u8457\u8d85\u8d8a\u4e86\u5305\u62ecGPT-5\u548cGemini-3-Flash\u5728\u5185\u7684\u9886\u5148\u95ed\u6e90\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u89c6\u89c9\u63a8\u7406\u4e2d\u7684\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent Multimodal Large Language Models (MLLMs) are good at reasoning but mainly focus on text, which limits their performance in complex visual tasks.</li>\n    <li>This paper presents a new approach called DiffThinker, a framework that improves multimodal reasoning by treating it as a generative image-to-image task.</li>\n    <li>DiffThinker shows better logical consistency and spatial accuracy in tasks that are focused on visual information.</li>\n    <li>The research compares DiffThinker with MLLMs and identifies four important features: efficiency, controllability, native parallelism, and collaboration.</li>\n    <li>Experiments in various areas show that DiffThinker outperforms major models like GPT-5 and Gemini-3-Flash, suggesting it is a strong method for visual reasoning tasks.</li>\n</ul>"}, "publishedAt": "2025-12-30T06:51:18.000Z", "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models", "summary": "While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\\%) and Gemini-3-Flash (+111.6\\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24165.png", "numComments": 4, "submittedBy": {"_id": "629454301ae2138079f7ff31", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629454301ae2138079f7ff31/rVtbF-j06gDiYzomTeVTc.jpeg", "fullname": "Tong Zhu", "name": "Spico", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 19}, "isAuthorParticipating": true}, {"paper": {"id": "2512.22630", "authors": [{"_id": "6957329e832867f253525886", "name": "Ziqi Jin", "hidden": false}, {"_id": "6957329e832867f253525887", "name": "Bin Wang", "hidden": false}, {"_id": "6957329e832867f253525888", "name": "Xiang Lin", "hidden": false}, {"_id": "6957329e832867f253525889", "name": "Lidong Bing", "hidden": false}, {"_id": "6957329e832867f25352588a", "name": "Aixin Sun", "hidden": false}], "publishedAt": "2025-12-27T16:03:08.000Z", "submittedOnDailyAt": "2026-01-02T00:23:07.951Z", "title": "On the Role of Discreteness in Diffusion LLMs", "submittedOnDailyBy": {"_id": "625921d05f80a3c1aad0bae3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625921d05f80a3c1aad0bae3/ElN3-6V5nGId2fzI3Dqlr.jpeg", "isPro": true, "fullname": "Phi", "user": "Xalphinions", "type": "user"}, "summary": "Diffusion models offer appealing properties for language generation, such as parallel decoding and iterative refinement, but the discrete and highly structured nature of text challenges the direct application of diffusion principles. In this paper, we revisit diffusion language modeling from the view of diffusion process and language modeling, and outline five properties that separate diffusion mechanics from language-specific requirements. We first categorize existing approaches into continuous diffusion in embedding space and discrete diffusion over tokens. We then show that each satisfies only part of the five essential properties and therefore reflects a structural trade-off. Through analyses of recent large diffusion language models, we identify two central issues: (i) uniform corruption does not respect how information is distributed across positions, and (ii) token-wise marginal training cannot capture multi-token dependencies during parallel decoding. These observations motivate diffusion processes that align more closely with the structure of text, and encourage future work toward more coherent diffusion language models.", "upvotes": 10, "discussionId": "6957329e832867f25352588b", "organization": {"_id": "682c435aa186ba2f1fdde607", "name": "miromind-ai", "fullname": "MiroMind AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/682c41fb2f8a52030ec93ce0/Cna52_IapEXuNBsyI3lvR.png"}, "summary_zh": "<ul>\n    <li>\u6269\u6563\u6a21\u578b\u5728\u8bed\u8a00\u751f\u6210\u4e2d\u6709\u5438\u5f15\u529b\u7684\u7279\u6027\uff0c\u5982\u5e76\u884c\u89e3\u7801\u548c\u8fed\u4ee3\u4f18\u5316\u3002</li>\n    <li>\u6587\u672c\u7684\u79bb\u6563\u548c\u7ed3\u6784\u5316\u7279\u6027\u4f7f\u5f97\u76f4\u63a5\u5e94\u7528\u6269\u6563\u539f\u5219\u9762\u4e34\u6311\u6218\u3002</li>\n    <li>\u672c\u6587\u5206\u6790\u4e86\u6269\u6563\u8bed\u8a00\u5efa\u6a21\uff0c\u5e76\u63d0\u51fa\u4e86\u4e94\u4e2a\u533a\u5206\u6269\u6563\u673a\u5236\u548c\u8bed\u8a00\u7279\u6027\u7684\u5c5e\u6027\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u5206\u4e3a\u5d4c\u5165\u7a7a\u95f4\u7684\u8fde\u7eed\u6269\u6563\u548c\u57fa\u4e8e\u6807\u8bb0\u7684\u79bb\u6563\u6269\u6563\uff0c\u4f46\u5b83\u4eec\u4ec5\u6ee1\u8db3\u90e8\u5206\u5c5e\u6027\uff0c\u5b58\u5728\u7ed3\u6784\u4e0a\u7684\u6743\u8861\u3002</li>\n    <li>\u6211\u4eec\u53d1\u73b0\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u5747\u5300\u8150\u8680\u4e0d\u8003\u8651\u4fe1\u606f\u5728\u4f4d\u7f6e\u4e0a\u7684\u5206\u5e03\uff0c\u6807\u8bb0\u7ea7\u8bad\u7ec3\u65e0\u6cd5\u6355\u6349\u591a\u6807\u8bb0\u4f9d\u8d56\u5173\u7cfb\uff0c\u8fd9\u4fc3\u4f7f\u6211\u4eec\u63a2\u7d22\u66f4\u7b26\u5408\u6587\u672c\u7ed3\u6784\u7684\u6269\u6563\u8fc7\u7a0b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion models are useful for language generation but face challenges due to the complex structure of text.</li>\n    <li>The paper identifies five key properties that differentiate diffusion mechanics from language requirements.</li>\n    <li>Existing methods are categorized into two types: continuous diffusion in embedding space and discrete diffusion over tokens.</li>\n    <li>Current approaches have limitations, like not properly handling information distribution and multi-token relationships.</li>\n    <li>The authors suggest improvements to make diffusion models better suited for language structure and encourage future research.</li>\n</ul>"}, "publishedAt": "2025-12-27T11:03:08.000Z", "title": "On the Role of Discreteness in Diffusion LLMs", "summary": "Diffusion models offer appealing properties for language generation, such as parallel decoding and iterative refinement, but the discrete and highly structured nature of text challenges the direct application of diffusion principles. In this paper, we revisit diffusion language modeling from the view of diffusion process and language modeling, and outline five properties that separate diffusion mechanics from language-specific requirements. We first categorize existing approaches into continuous diffusion in embedding space and discrete diffusion over tokens. We then show that each satisfies only part of the five essential properties and therefore reflects a structural trade-off. Through analyses of recent large diffusion language models, we identify two central issues: (i) uniform corruption does not respect how information is distributed across positions, and (ii) token-wise marginal training cannot capture multi-token dependencies during parallel decoding. These observations motivate diffusion processes that align more closely with the structure of text, and encourage future work toward more coherent diffusion language models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22630.png", "numComments": 3, "submittedBy": {"_id": "625921d05f80a3c1aad0bae3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625921d05f80a3c1aad0bae3/ElN3-6V5nGId2fzI3Dqlr.jpeg", "fullname": "Phi", "name": "Xalphinions", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "682c435aa186ba2f1fdde607", "name": "miromind-ai", "fullname": "MiroMind AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/682c41fb2f8a52030ec93ce0/Cna52_IapEXuNBsyI3lvR.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24766", "authors": [{"_id": "6957d6a1832867f253525983", "name": "Karthik Dharmarajan", "hidden": false}, {"_id": "6957d6a1832867f253525984", "name": "Wenlong Huang", "hidden": false}, {"_id": "6957d6a1832867f253525985", "name": "Jiajun Wu", "hidden": false}, {"_id": "6957d6a1832867f253525986", "name": "Li Fei-Fei", "hidden": false}, {"_id": "6957d6a1832867f253525987", "name": "Ruohan Zhang", "hidden": false}], "publishedAt": "2025-12-31T10:25:24.000Z", "submittedOnDailyAt": "2026-01-02T12:01:38.691Z", "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.", "upvotes": 5, "discussionId": "6957d6a2832867f253525988", "projectPage": "https://dream2flow.github.io/", "summary_zh": "<ul>\n    <li>\u751f\u6210\u89c6\u9891\u5efa\u6a21\u53ef\u4ee5\u5e2e\u52a9\u673a\u5668\u4eba\u7406\u89e3\u548c\u6267\u884c\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u7269\u7406\u4e92\u52a8\u3002</li>\n    <li>Dream2Flow\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u5c06\u89c6\u9891\u751f\u6210\u4e0e\u673a\u5668\u4eba\u63a7\u5236\u8fde\u63a5\u8d77\u6765\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc73D\u7269\u4f53\u8fd0\u52a8\u7684\u91cd\u5efa\u6765\u5b9e\u73b0\u7269\u4f53\u8f68\u8ff9\u8ddf\u8e2a\u3002</li>\n    <li>Dream2Flow\u80fd\u591f\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u7269\u4f53\uff0c\u5305\u62ec\u521a\u6027\u3001\u5173\u8282\u3001\u53ef\u53d8\u5f62\u548c\u9897\u7c92\u7269\u4f53\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c3D\u7269\u4f53\u6d41\u52a8\u662f\u5c06\u89c6\u9891\u751f\u6210\u6a21\u578b\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u901a\u7528\u754c\u9762\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Generative video modeling helps robots understand how to interact with objects in different scenarios.</li>\n    <li>Dream2Flow is a new framework that connects video generation with robotic controls using 3D object movements.</li>\n    <li>The method tracks how objects move in 3D space and converts these movements into actions robots can perform.</li>\n    <li>Dream2Flow allows robots to manipulate various types of objects without needing specific training for each task.</li>\n    <li>Experiments show that this approach is effective for both simulations and real-world applications.</li>\n</ul>"}, "publishedAt": "2025-12-31T05:25:24.000Z", "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow", "summary": "Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24766.png", "numComments": 3, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 9044}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24724", "authors": [{"_id": "6957d82d832867f25352598d", "name": "Jibin Song", "hidden": false}, {"_id": "6957d82d832867f25352598e", "name": "Mingi Kwon", "hidden": false}, {"_id": "6957d82d832867f25352598f", "name": "Jaeseok Jeong", "hidden": false}, {"_id": "6957d82d832867f253525990", "name": "Youngjung Uh", "hidden": false}], "publishedAt": "2025-12-31T08:41:27.000Z", "submittedOnDailyAt": "2026-01-02T12:08:10.653Z", "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "In this work, we show that the impact of model capacity varies across timesteps: it is crucial for the early and late stages but largely negligible during the intermediate stage. Accordingly, we propose FlowBlending, a stage-aware multi-model sampling strategy that employs a large model and a small model at capacity-sensitive stages and intermediate stages, respectively. We further introduce simple criteria to choose stage boundaries and provide a velocity-divergence analysis as an effective proxy for identifying capacity-sensitive regions. Across LTX-Video (2B/13B) and WAN 2.1 (1.3B/14B), FlowBlending achieves up to 1.65x faster inference with 57.35% fewer FLOPs, while maintaining the visual fidelity, temporal coherence, and semantic alignment of the large models. FlowBlending is also compatible with existing sampling-acceleration techniques, enabling up to 2x additional speedup. Project page is available at: https://jibin86.github.io/flowblending_project_page.", "upvotes": 3, "discussionId": "6957d82d832867f253525991", "summary_zh": "<ul>\n    <li>\u6a21\u578b\u7684\u80fd\u529b\u5728\u4e0d\u540c\u65f6\u95f4\u70b9\u7684\u5f71\u54cd\u4e0d\u540c\uff1a\u65e9\u671f\u548c\u665a\u671f\u9636\u6bb5\u5f88\u91cd\u8981\uff0c\u4f46\u4e2d\u95f4\u9636\u6bb5\u5f71\u54cd\u5f88\u5c0f\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFlowBlending\u7684\u591a\u6a21\u578b\u91c7\u6837\u7b56\u7565\uff0c\u6839\u636e\u9636\u6bb5\u4f7f\u7528\u5927\u6a21\u578b\u548c\u5c0f\u6a21\u578b\u3002</li>\n    <li>\u63d0\u4f9b\u4e86\u7b80\u5355\u7684\u6807\u51c6\u6765\u9009\u62e9\u9636\u6bb5\u8fb9\u754c\uff0c\u5e76\u901a\u8fc7\u901f\u5ea6-\u53d1\u6563\u5206\u6790\u8bc6\u522b\u5173\u952e\u533a\u57df\u3002</li>\n    <li>\u5728LTX-Video\u548cWAN 2.1\u6570\u636e\u96c6\u4e0a\uff0cFlowBlending\u5b9e\u73b0\u4e86\u9ad8\u8fbe1.65\u500d\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\uff0c\u51cf\u5c11\u4e8657.35%\u7684\u8ba1\u7b97\u91cf\u3002</li>\n    <li>FlowBlending\u4e0e\u73b0\u6709\u52a0\u901f\u6280\u672f\u517c\u5bb9\uff0c\u80fd\u591f\u8fdb\u4e00\u6b65\u5b9e\u73b0\u6700\u9ad82\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Model capacity affects performance differently at various stages of processing: it's important in the early and late stages, but not as much in the middle stage.</li>\n    <li>FlowBlending is a new strategy that uses a large model for crucial stages and a smaller model for less critical stages.</li>\n    <li>Simple criteria are provided to define when to switch between different model sizes based on processing needs.</li>\n    <li>FlowBlending significantly speeds up inference, achieving up to 1.65 times faster performance with 57.35% fewer calculations, while still producing high-quality results.</li>\n    <li>This method works well with other speed-up techniques, potentially doubling the speed improvement.</li>\n</ul>"}, "publishedAt": "2025-12-31T03:41:27.000Z", "title": "FlowBlending: Stage-Aware Multi-Model Sampling for Fast and High-Fidelity Video Generation", "summary": "In this work, we show that the impact of model capacity varies across timesteps: it is crucial for the early and late stages but largely negligible during the intermediate stage. Accordingly, we propose FlowBlending, a stage-aware multi-model sampling strategy that employs a large model and a small model at capacity-sensitive stages and intermediate stages, respectively. We further introduce simple criteria to choose stage boundaries and provide a velocity-divergence analysis as an effective proxy for identifying capacity-sensitive regions. Across LTX-Video (2B/13B) and WAN 2.1 (1.3B/14B), FlowBlending achieves up to 1.65x faster inference with 57.35% fewer FLOPs, while maintaining the visual fidelity, temporal coherence, and semantic alignment of the large models. FlowBlending is also compatible with existing sampling-acceleration techniques, enabling up to 2x additional speedup. Project page is available at: https://jibin86.github.io/flowblending_project_page.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24724.png", "numComments": 3, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 9044}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24007", "authors": [{"_id": "6957e7af832867f2535259cb", "name": "Bulent Soykan", "hidden": false}, {"_id": "6957e7af832867f2535259cc", "name": "Sean Mondesire", "hidden": false}, {"_id": "6957e7af832867f2535259cd", "name": "Ghaith Rabadi", "hidden": false}], "publishedAt": "2025-12-30T06:03:37.000Z", "submittedOnDailyAt": "2026-01-02T13:14:57.263Z", "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems", "submittedOnDailyBy": {"_id": "626273fbcbebf7e1ac2820ab", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626273fbcbebf7e1ac2820ab/VPc5BVvdSel6ox7yzrcvb.jpeg", "isPro": false, "fullname": "Bulent Soykan", "user": "bulentsoykan", "type": "user"}, "summary": "Simulation optimization (SO) is frequently challenged by noisy evaluations, high computational costs, and complex, multimodal search landscapes. This paper introduces Tabu-Enhanced Simulation Optimization (TESO), a novel metaheuristic framework integrating adaptive search with memory-based strategies. TESO leverages a short-term Tabu List to prevent cycling and encourage diversification, and a long-term Elite Memory to guide intensification by perturbing high-performing solutions. An aspiration criterion allows overriding tabu restrictions for exceptional candidates. This combination facilitates a dynamic balance between exploration and exploitation in stochastic environments. We demonstrate TESO's effectiveness and reliability using an queue optimization problem, showing improved performance compared to benchmarks and validating the contribution of its memory components. Source code and data are available at: https://github.com/bulentsoykan/TESO.", "upvotes": 1, "discussionId": "6957e7af832867f2535259ce", "githubRepo": "https://github.com/bulentsoykan/TESO", "githubRepoAddedBy": "user", "githubStars": 4, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5143\u542f\u53d1\u5f0f\u6846\u67b6\uff0c\u79f0\u4e3aTabu-Enhanced Simulation Optimization (TESO)\u3002</li>\n    <li>TESO\u7ed3\u5408\u4e86\u81ea\u9002\u5e94\u641c\u7d22\u548c\u57fa\u4e8e\u8bb0\u5fc6\u7684\u7b56\u7565\uff0c\u4f7f\u7528\u77ed\u671f\u7981\u5fcc\u8868\u548c\u957f\u671f\u7cbe\u82f1\u8bb0\u5fc6\u3002</li>\n    <li>\u77ed\u671f\u7981\u5fcc\u8868\u5e2e\u52a9\u907f\u514d\u5faa\u73af\u5e76\u4fc3\u8fdb\u591a\u6837\u6027\uff0c\u957f\u671f\u7cbe\u82f1\u8bb0\u5fc6\u5219\u7528\u4e8e\u6307\u5bfc\u4f18\u5316\u8fc7\u7a0b\u3002</li>\n    <li>\u901a\u8fc7\u4e00\u4e2a\u6392\u961f\u4f18\u5316\u95ee\u9898\uff0c\u5c55\u793a\u4e86TESO\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\uff0c\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u3002</li>\n    <li>\u6e90\u4ee3\u7801\u548c\u6570\u636e\u53ef\u4ee5\u5728\u6307\u5b9a\u7f51\u5740\u83b7\u53d6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>This paper presents a new method called Tabu-Enhanced Simulation Optimization (TESO) to solve complex optimization problems.</li>\n    <li>TESO combines adaptive searching with memory techniques to improve the search process.</li>\n    <li>The method uses a short-term Tabu List to avoid revisiting the same solutions and a long-term memory of good solutions to enhance performance.</li>\n    <li>An aspiration criterion allows TESO to explore exceptional solutions, even if they would normally be restricted.</li>\n    <li>Results from testing TESO on a queue optimization problem show it performs better than existing methods, and the source code is available online.</li>\n</ul>"}, "publishedAt": "2025-12-30T01:03:37.000Z", "title": "TESO Tabu Enhanced Simulation Optimization for Noisy Black Box Problems", "summary": "Simulation optimization (SO) is frequently challenged by noisy evaluations, high computational costs, and complex, multimodal search landscapes. This paper introduces Tabu-Enhanced Simulation Optimization (TESO), a novel metaheuristic framework integrating adaptive search with memory-based strategies. TESO leverages a short-term Tabu List to prevent cycling and encourage diversification, and a long-term Elite Memory to guide intensification by perturbing high-performing solutions. An aspiration criterion allows overriding tabu restrictions for exceptional candidates. This combination facilitates a dynamic balance between exploration and exploitation in stochastic environments. We demonstrate TESO's effectiveness and reliability using an queue optimization problem, showing improved performance compared to benchmarks and validating the contribution of its memory components. Source code and data are available at: https://github.com/bulentsoykan/TESO.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24007.png", "numComments": 2, "submittedBy": {"_id": "626273fbcbebf7e1ac2820ab", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626273fbcbebf7e1ac2820ab/VPc5BVvdSel6ox7yzrcvb.jpeg", "fullname": "Bulent Soykan", "name": "bulentsoykan", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2512.23447", "authors": [{"_id": "69534a9589916ff627aa3f5c", "name": "Ang Lv", "hidden": false}, {"_id": "69534a9589916ff627aa3f5d", "name": "Jin Ma", "hidden": false}, {"_id": "69534a9589916ff627aa3f5e", "name": "Yiyuan Ma", "hidden": false}, {"_id": "69534a9589916ff627aa3f5f", "name": "Siyuan Qiao", "hidden": false}], "publishedAt": "2025-12-29T13:03:18.000Z", "submittedOnDailyAt": "2025-12-30T01:18:40.635Z", "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss", "submittedOnDailyBy": {"_id": "64b8ca3c5067873176d4b436", "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg", "isPro": false, "fullname": "AngLv", "user": "AngLv", "type": "user"}, "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.", "upvotes": 71, "discussionId": "69534a9589916ff627aa3f60", "ai_summary": "An expert-router coupling (ERC) loss aligns router decisions with expert capabilities in Mixture-of-Experts (MoE) models by enforcing constraints on internal activations, improving performance and computational efficiency.", "ai_keywords": ["Mixture-of-Experts (MoE)", "expert-router coupling (ERC) loss", "router embeddings", "proxy tokens", "internal activations", "MoE-LLMs", "expert specialization levels", "n\u00b2 activations"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>Mixture-of-Experts (MoE) \u6a21\u578b\u7684\u8def\u7531\u5668\u51b3\u7b56\u4e0e\u4e13\u5bb6\u80fd\u529b\u4e4b\u95f4\u7f3a\u4e4f\u660e\u786e\u7ea6\u675f\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u79f0\u4e3a\u4e13\u5bb6-\u8def\u7531\u5668\u8026\u5408\u635f\u5931\uff08ERC\u635f\u5931\uff09\uff0c\u5c06\u8def\u7531\u5668\u7684\u51b3\u7b56\u4e0e\u4e13\u5bb6\u80fd\u529b\u7d27\u5bc6\u7ed3\u5408\u3002</li>\n    <li>ERC\u635f\u5931\u5f3a\u5236\u8981\u6c42\u6bcf\u4e2a\u4e13\u5bb6\u5bf9\u5176\u4ee3\u7406\u6807\u8bb0\u7684\u6fc0\u6d3b\u9ad8\u4e8e\u5176\u4ed6\u4e13\u5bb6\u7684\u4ee3\u7406\u6807\u8bb0\u3002</li>\n    <li>ERC\u635f\u5931\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u4ec5\u9700\u5904\u7406 n^2 \u6fc0\u6d3b\uff0c\u5176\u4e2d n \u662f\u4e13\u5bb6\u6570\u91cf\uff0c\u4e0e\u6279\u91cf\u5927\u5c0f\u65e0\u5173\u3002</li>\n    <li>\u901a\u8fc7\u5bf9\u5927\u89c4\u6a21 MoE-LLM \u7684\u9884\u8bad\u7ec3\uff0c\u8bc1\u660e\u4e86 ERC \u635f\u5931\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u4e13\u5bb6\u4e13\u4e1a\u5316\u6c34\u5e73\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u63a7\u5236\u548c\u91cf\u5316\u8ddf\u8e2a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Mixture-of-Experts (MoE) models struggle because the router's choices don't always match the experts' strengths.</li>\n    <li>To improve this, we introduced a new loss called expert-router coupling (ERC) loss, which connects the router's decisions to the experts' abilities.</li>\n    <li>The ERC loss ensures that each expert responds better to its own assigned tokens than to those of other experts.</li>\n    <li>This method is efficient, only needing calculations based on the number of experts, not the number of tokens, making it faster than previous methods.</li>\n    <li>We tested the ERC loss on large MoE models and found it effectively tracks how well each expert specializes in handling its assigned tokens.</li>\n</ul>"}, "publishedAt": "2025-12-29T08:03:18.000Z", "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss", "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23447.png", "numComments": 1, "submittedBy": {"_id": "64b8ca3c5067873176d4b436", "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg", "fullname": "AngLv", "name": "AngLv", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "name": "Chunkang Zhang", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 70, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 22, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u662f\u4e00\u79cd\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5168\u5c40\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u7b56\u7565\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u6a21\u5757\u4e3b\u8981\u4f5c\u4e3a\u88ab\u52a8\u5b58\u50a8\uff0c\u65e0\u6cd5\u6709\u6548\u6574\u5408\u4fe1\u606f\uff0c\u5bfc\u81f4\u63a8\u7406\u788e\u7247\u5316\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HGMem\uff0c\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u80fd\u591f\u52a8\u6001\u8868\u793a\u590d\u6742\u63a8\u7406\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u8fb9\u8fde\u63a5\u4e0d\u540c\u7684\u8bb0\u5fc6\u5355\u5143\uff0c\u5f62\u6210\u66f4\u9ad8\u9636\u7684\u4e92\u52a8\uff0c\u589e\u5f3a\u4e86\u6574\u4f53\u7406\u89e3\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHGMem\u5728\u591a\u4e2a\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u591a\u6b65\u9aa4RAG\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps large language models (LLMs) with complex reasoning tasks.</li>\n    <li>Current memory designs mainly store facts without connecting them, limiting the ability to understand relationships between information.</li>\n    <li>HGMem is a new memory mechanism that uses a hypergraph structure to create dynamic connections between facts for better reasoning.</li>\n    <li>This approach allows for deeper understanding and improved reasoning in multi-step tasks.</li>\n    <li>Tests show that HGMem significantly outperforms traditional memory systems on various challenging datasets.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24880", "authors": [{"_id": "69561fbf832867f253525726", "name": "Zhenda Xie", "hidden": false}, {"_id": "69561fbf832867f253525727", "name": "Yixuan Wei", "hidden": false}, {"_id": "69561fbf832867f253525728", "name": "Huanqi Cao", "hidden": false}, {"_id": "69561fbf832867f253525729", "name": "Chenggang Zhao", "hidden": false}, {"_id": "69561fbf832867f25352572a", "name": "Chengqi Deng", "hidden": false}, {"_id": "69561fbf832867f25352572b", "name": "Jiashi Li", "hidden": false}, {"_id": "69561fbf832867f25352572c", "name": "Damai Dai", "hidden": false}, {"_id": "69561fbf832867f25352572d", "name": "Huazuo Gao", "hidden": false}, {"_id": "69561fbf832867f25352572e", "name": "Jiang Chang", "hidden": false}, {"_id": "69561fbf832867f25352572f", "name": "Liang Zhao", "hidden": false}, {"_id": "69561fbf832867f253525730", "name": "Shangyan Zhou", "hidden": false}, {"_id": "69561fbf832867f253525731", "name": "Zhean Xu", "hidden": false}, {"_id": "69561fbf832867f253525732", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "69561fbf832867f253525733", "name": "Wangding Zeng", "hidden": false}, {"_id": "69561fbf832867f253525734", "name": "Shengding Hu", "hidden": false}, {"_id": "69561fbf832867f253525735", "name": "Yuqing Wang", "hidden": false}, {"_id": "69561fbf832867f253525736", "name": "Jingyang Yuan", "hidden": false}, {"_id": "69561fbf832867f253525737", "name": "Lean Wang", "hidden": false}, {"_id": "69561fbf832867f253525738", "name": "Wenfeng Liang", "hidden": false}], "publishedAt": "2025-12-31T14:16:26.000Z", "submittedOnDailyAt": "2026-01-01T07:30:32.169Z", "title": "mHC: Manifold-Constrained Hyper-Connections", "submittedOnDailyBy": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "isPro": false, "fullname": "Adina Yakefu", "user": "AdinaY", "type": "user"}, "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.", "upvotes": 59, "discussionId": "69561fc0832867f253525739", "ai_summary": "Manifold-Constrained Hyper-Connections (mHC) stabilize and scale residual connection architectures by restoring identity mapping properties through manifold projection and infrastructure optimization.", "ai_keywords": ["Hyper-Connections (HC)", "Manifold-Constrained Hyper-Connections (mHC)", "residual connections", "residual stream width", "connectivity patterns", "identity mapping property", "training instability", "memory access overhead", "manifold projection", "infrastructure optimization", "scalability"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fde\u63a5\u65b9\u5f0f\uff0c\u79f0\u4e3a\u8d85\u8fde\u63a5\uff08HC\uff09\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u7684\u6b8b\u5dee\u8fde\u63a5\u3002</li>\n    <li>\u867d\u7136\u8fd9\u79cd\u65b0\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f46\u4e5f\u5bfc\u81f4\u4e86\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u548c\u6269\u5c55\u6027\u95ee\u9898\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u201c\u6d41\u5f62\u7ea6\u675f\u8d85\u8fde\u63a5\u201d\uff08mHC\uff09\u6846\u67b6\uff0c\u6062\u590d\u4e86\u6b8b\u5dee\u8fde\u63a5\u7684\u8eab\u4efd\u6620\u5c04\u7279\u6027\u3002</li>\n    <li>mHC\u6846\u67b6\u5728\u4fdd\u8bc1\u6548\u7387\u7684\u540c\u65f6\uff0c\u7ecf\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5728\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\u6548\u679c\u663e\u8457\u3002</li>\n    <li>mHC\u88ab\u89c6\u4e3a\u8d85\u8fde\u63a5\u7684\u7075\u6d3b\u6269\u5c55\uff0c\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u62d3\u6251\u67b6\u6784\u8bbe\u8ba1\u548c\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u65b9\u5411\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent studies have improved residual connections in neural networks through Hyper-Connections (HC), but this has led to training issues and increased memory use.</li>\n    <li>To solve these problems, the authors propose a new framework called Manifold-Constrained Hyper-Connections (mHC) that restores the original benefits of residual connections.</li>\n    <li>mHC maintains the identity mapping property and optimizes efficiency for better performance during training.</li>\n    <li>Tests show that mHC works well for large-scale training, providing better performance and scalability compared to HC.</li>\n    <li>The authors believe mHC will enhance our understanding of neural network design and inspire future developments in foundational models.</li>\n</ul>"}, "publishedAt": "2025-12-31T09:16:26.000Z", "title": "mHC: Manifold-Constrained Hyper-Connections", "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24880.png", "numComments": 1, "submittedBy": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "fullname": "Adina Yakefu", "name": "AdinaY", "type": "user", "isPro": false, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 1140}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23576", "authors": [{"_id": "69534f1e89916ff627aa3fe3", "name": "Ethan Chern", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe4", "name": "Zhulin Hu", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe5", "name": "Bohao Tang", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe6", "name": "Jiadi Su", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe7", "name": "Steffi Chern", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe8", "name": "Zhijie Deng", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe9", "name": "Pengfei Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64bb5f9d8e051085bace4d1e/skNa_3Ly0Bg7F6aL0mk92.mp4"], "publishedAt": "2025-12-29T16:17:36.000Z", "submittedOnDailyAt": "2025-12-30T02:36:23.479Z", "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation", "submittedOnDailyBy": {"_id": "64bb5f9d8e051085bace4d1e", "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg", "isPro": false, "fullname": "Ethan Chern", "user": "ethanchern", "type": "user"}, "summary": "Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.", "upvotes": 51, "discussionId": "69534f1e89916ff627aa3fea", "githubRepo": "https://github.com/GAIR-NLP/LiveTalk", "githubRepoAddedBy": "user", "ai_summary": "Real-time multimodal video generation via diffusion is enabled by an improved distillation approach with multimodal conditioning and optimized scheduling, reducing inference latency while maintaining quality for interactive systems.", "ai_keywords": ["diffusion models", "bidirectional attention", "distillation methods", "on-policy distillation", "Self Forcing", "audio language models", "Anchor-Heavy Identity Sinks", "multimodal conditioning", "autoregressive", "on-policy optimization"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u65e8\u5728\u5b9e\u73b0\u5b9e\u65f6\u89c6\u9891\u751f\u6210\uff0c\u4ee5\u652f\u6301\u591a\u6a21\u6001\u4ea4\u4e92\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u3002</li>\n    <li>\u4f20\u7edf\u7684\u6269\u6563\u6a21\u578b\u5728\u53bb\u566a\u5904\u7406\u89c6\u9891\u5e27\u65f6\u65e0\u6cd5\u5b9e\u73b0\u5b9e\u65f6\u4e92\u52a8\uff0c\u73b0\u6709\u7684\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\uff0c\u5bfc\u81f4\u4eba\u673a\u4ea4\u4e92\u4e0d\u81ea\u7136\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u5f3a\u8c03\u8f93\u5165\u6761\u4ef6\u7684\u8d28\u91cf\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u6761\u4ef6\u4e0b\u7684\u89c6\u89c9\u4f2a\u5f71\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u7684\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u751f\u6210\u7684\u89c6\u9891\u8d28\u91cf\u4e0e\u5168\u6b65\u9aa4\u57fa\u7ebf\u76f8\u5f53\uff0c\u4f46\u63a8\u7406\u6210\u672c\u548c\u5ef6\u8fdf\u51cf\u5c11\u4e8620\u500d\u3002</li>\n    <li>\u7ed3\u5408\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u548c\u957f\u89c6\u9891\u63a8\u65ad\u6280\u672f\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u5b9e\u65f6\u591a\u6a21\u6001\u4ea4\u4e92\u7cfb\u7edfLiveTalk\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8f6e\u4e92\u52a8\u7684\u89c6\u9891\u8fde\u8d2f\u6027\u548c\u5185\u5bb9\u8d28\u91cf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper focuses on improving real-time video generation for interactive AI systems using diffusion models.</li>\n    <li>Current methods struggle with real-time interaction due to challenges in processing all video frames at once.</li>\n    <li>The authors propose a new method to enhance video quality and reduce issues like flickering and black frames during multimodal conditioning.</li>\n    <li>Their improved model achieves high visual quality with significantly lower costs and faster response times compared to existing models.</li>\n    <li>The final system, LiveTalk, excels in generating coherent and high-quality videos in real-time, improving human-AI interactions.</li>\n</ul>"}, "publishedAt": "2025-12-29T11:17:36.000Z", "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation", "summary": "Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64bb5f9d8e051085bace4d1e/skNa_3Ly0Bg7F6aL0mk92.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23576.png", "numComments": 1, "submittedBy": {"_id": "64bb5f9d8e051085bace4d1e", "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg", "fullname": "Ethan Chern", "name": "ethanchern", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24618", "authors": [{"_id": "6955d543832867f25352555d", "name": "Junru Lu", "hidden": false}, {"_id": "6955d543832867f25352555e", "name": "Jiarui Qin", "hidden": false}, {"_id": "6955d543832867f25352555f", "name": "Lingfeng Qiao", "hidden": false}, {"_id": "6955d543832867f253525560", "name": "Yinghui Li", "hidden": false}, {"_id": "6955d543832867f253525561", "name": "Xinyi Dai", "hidden": false}, {"_id": "6955d543832867f253525562", "name": "Bo Ke", "hidden": false}, {"_id": "6955d543832867f253525563", "name": "Jianfeng He", "hidden": false}, {"_id": "6955d543832867f253525564", "name": "Ruizhi Qiao", "hidden": false}, {"_id": "6955d543832867f253525565", "name": "Di Yin", "hidden": false}, {"_id": "6955d543832867f253525566", "name": "Xing Sun", "hidden": false}, {"_id": "6955d543832867f253525567", "name": "Yunsheng Wu", "hidden": false}, {"_id": "6955d543832867f253525568", "name": "Yinsong Liu", "hidden": false}, {"_id": "6955d543832867f253525569", "name": "Shuangyin Liu", "hidden": false}, {"_id": "6955d543832867f25352556a", "name": "Mingkong Tang", "hidden": false}, {"_id": "6955d543832867f25352556b", "name": "Haodong Lin", "hidden": false}, {"_id": "6955d543832867f25352556c", "name": "Jiayi Kuang", "hidden": false}, {"_id": "6955d543832867f25352556d", "name": "Fanxu Meng", "hidden": false}, {"_id": "6955d543832867f25352556e", "name": "Xiaojuan Tang", "hidden": false}, {"_id": "6955d543832867f25352556f", "name": "Yunjia Xi", "hidden": false}, {"_id": "6955d543832867f253525570", "name": "Junjie Huang", "hidden": false}, {"_id": "6955d543832867f253525571", "name": "Haotong Yang", "hidden": false}, {"_id": "6955d543832867f253525572", "name": "Zhenyi Shen", "hidden": false}, {"_id": "6955d543832867f253525573", "name": "Yangning Li", "hidden": false}, {"_id": "6955d543832867f253525574", "name": "Qianwen Zhang", "hidden": false}, {"_id": "6955d543832867f253525575", "name": "Yifei Yu", "hidden": false}, {"_id": "6955d543832867f253525576", "name": "Siyu An", "hidden": false}, {"_id": "6955d543832867f253525577", "name": "Junnan Dong", "hidden": false}, {"_id": "6955d543832867f253525578", "name": "Qiufeng Wang", "hidden": false}, {"_id": "6955d543832867f253525579", "name": "Jie Wang", "hidden": false}, {"_id": "6955d543832867f25352557a", "name": "Keyu Chen", "hidden": false}, {"_id": "6955d543832867f25352557b", "name": "Wei Wen", "hidden": false}, {"_id": "6955d543832867f25352557c", "name": "Taian Guo", "hidden": false}, {"_id": "6955d543832867f25352557d", "name": "Zhifeng Shen", "hidden": false}, {"_id": "6955d543832867f25352557e", "name": "Daohai Yu", "hidden": false}, {"_id": "6955d543832867f25352557f", "name": "Jiahao Li", "hidden": false}, {"_id": "6955d543832867f253525580", "name": "Ke Li", "hidden": false}, {"_id": "6955d543832867f253525581", "name": "Zongyi Li", "hidden": false}, {"_id": "6955d543832867f253525582", "name": "Xiaoyu Tan", "hidden": false}], "publishedAt": "2025-12-31T04:25:11.000Z", "submittedOnDailyAt": "2026-01-01T00:33:09.720Z", "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "upvotes": 43, "discussionId": "6955d543832867f253525583", "ai_summary": "Youtu-LLM is a lightweight language model optimized for computational efficiency and agentic intelligence through a compact architecture, STEM-focused training curriculum, and scalable mid-training strategies for planning and reasoning tasks.", "ai_keywords": ["Multi-Latent Attention (MLA) architecture", "STEM-oriented vocabulary", "128k context window", "Commonsense-STEM-Agent Curriculum", "multi-stage training strategy", "agentic mid-training", "data construction schemes", "planning and reflection behaviors", "long-context reasoning", "state tracking", "agentic capabilities"], "summary_zh": "<ul>\n    <li>Youtu-LLM \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e14\u5f3a\u5927\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u6709\u9ad8\u6548\u7684\u8ba1\u7b97\u80fd\u529b\u548c\u667a\u80fd\u3002</li>\n    <li>\u5b83\u4ece\u5934\u5f00\u59cb\u9884\u8bad\u7ec3\uff0c\u4e13\u6ce8\u4e8e\u57f9\u517b\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4e8e\u84b8\u998f\u6280\u672f\u3002</li>\n    <li>\u8be5\u6a21\u578b\u652f\u6301 128k \u7684\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u9002\u7528\u4e8e\u957f\u671f\u63a8\u7406\u548c\u72b6\u6001\u8ddf\u8e2a\u4efb\u52a1\u3002</li>\n    <li>\u901a\u8fc7\u4f7f\u7528\u5927\u7ea6 11T \u4ee4\u724c\u7684\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u6a21\u578b\u83b7\u5f97\u4e86\u6df1\u5c42\u7684\u8ba4\u77e5\u80fd\u529b\u3002</li>\n    <li>Youtu-LLM \u5728\u5c0f\u4e8e 2B \u7684\u8bed\u8a00\u6a21\u578b\u4e2d\u8bbe\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u6807\u51c6\uff0c\u5c24\u5176\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Youtu-LLM is a new lightweight language model designed for efficiency and intelligent reasoning.</li>\n    <li>It has a unique architecture that allows it to handle long contexts, supporting tasks that require deep understanding over extended information.</li>\n    <li>The model was trained on a large dataset of 11 trillion tokens, focusing on both general knowledge and complex STEM tasks to build strong cognitive skills.</li>\n    <li>Youtu-LLM uses advanced training techniques to create diverse data for improving its planning and reflection abilities.</li>\n    <li>It outperforms other models of similar size in both general and specialized tasks, proving that smaller models can still be very capable.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:25:11.000Z", "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24618.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 197}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24873", "authors": [{"_id": "6955e3f8832867f2535255cd", "name": "Weixun Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255ce", "name": "XiaoXiao Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255cf", "name": "Wanhe An", "hidden": false}, {"_id": "6955e3f8832867f2535255d0", "name": "Fangwen Dai", "hidden": false}, {"_id": "6955e3f8832867f2535255d1", "name": "Wei Gao", "hidden": false}, {"_id": "6955e3f8832867f2535255d2", "name": "Yancheng He", "hidden": false}, {"_id": "6955e3f8832867f2535255d3", "name": "Ju Huang", "hidden": false}, {"_id": "6955e3f8832867f2535255d4", "name": "Qiang Ji", "hidden": false}, {"_id": "6955e3f8832867f2535255d5", "name": "Hanqi Jin", "hidden": false}, {"_id": "6955e3f8832867f2535255d6", "name": "Xiaoyang Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d7", "name": "Yang Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d8", "name": "Zhongwen Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d9", "name": "Shirong Lin", "hidden": false}, {"_id": "6955e3f8832867f2535255da", "name": "Jiashun Liu", "hidden": false}, {"_id": "6955e3f8832867f2535255db", "name": "Zenan Liu", "hidden": false}, {"_id": "6955e3f8832867f2535255dc", "name": "Tao Luo", "hidden": false}, {"_id": "6955e3f8832867f2535255dd", "name": "Dilxat Muhtar", "hidden": false}, {"_id": "6955e3f8832867f2535255de", "name": "Yuanbin Qu", "hidden": false}, {"_id": "6955e3f8832867f2535255df", "name": "Jiaqiang Shi", "hidden": false}, {"_id": "6955e3f8832867f2535255e0", "name": "Qinghui Sun", "hidden": false}, {"_id": "6955e3f8832867f2535255e1", "name": "Yingshui Tan", "hidden": false}, {"_id": "6955e3f8832867f2535255e2", "name": "Hao Tang", "hidden": false}, {"_id": "6955e3f8832867f2535255e3", "name": "Runze Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e4", "name": "Yi Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e5", "name": "Zhaoguo Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e6", "name": "Yanan Wu", "hidden": false}, {"_id": "6955e3f8832867f2535255e7", "name": "Shaopan Xiong", "hidden": false}, {"_id": "6955e3f8832867f2535255e8", "name": "Binchen Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255e9", "name": "Xander Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255ea", "name": "Yuchi Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255eb", "name": "Qipeng Zhang", "hidden": false}, {"_id": "6955e3f8832867f2535255ec", "name": "Xixia Zhang", "hidden": false}, {"_id": "6955e3f8832867f2535255ed", "name": "Haizhou Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255ee", "name": "Jie Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255ef", "name": "Shuaibing Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255f0", "name": "Baihui Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f1", "name": "Jianhui Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f2", "name": "Suhang Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f3", "name": "Yanni Zhu", "hidden": false}, {"_id": "6955e3f8832867f2535255f4", "name": "Mengze Cai", "hidden": false}, {"_id": "6955e3f8832867f2535255f5", "name": "Kerui Cao", "hidden": false}, {"_id": "6955e3f8832867f2535255f6", "name": "Xitong Chen", "hidden": false}, {"_id": "6955e3f8832867f2535255f7", "name": "Yue Dai", "hidden": false}, {"_id": "6955e3f8832867f2535255f8", "name": "Lifan Du", "hidden": false}, {"_id": "6955e3f8832867f2535255f9", "name": "Tao Feng", "hidden": false}, {"_id": "6955e3f8832867f2535255fa", "name": "Tao He", "hidden": false}, {"_id": "6955e3f8832867f2535255fb", "name": "Jin Hu", "hidden": false}, {"_id": "6955e3f8832867f2535255fc", "name": "Yijie Hu", "hidden": false}, {"_id": "6955e3f8832867f2535255fd", "name": "Ziyu Jiang", "hidden": false}, {"_id": "6955e3f8832867f2535255fe", "name": "Cheng Li", "hidden": false}, {"_id": "6955e3f8832867f2535255ff", "name": "Xiang Li", "hidden": false}, {"_id": "6955e3f8832867f253525600", "name": "Jing Liang", "hidden": false}, {"_id": "6955e3f8832867f253525601", "name": "Chonghuan Liu", "hidden": false}, {"_id": "6955e3f8832867f253525602", "name": "ZhenDong Liu", "hidden": false}, {"_id": "6955e3f8832867f253525603", "name": "Haodong Mi", "hidden": false}, {"_id": "6955e3f8832867f253525604", "name": "Yanhu Mo", "hidden": false}, {"_id": "6955e3f8832867f253525605", "name": "Junjia Ni", "hidden": false}, {"_id": "6955e3f8832867f253525606", "name": "Shixin Pei", "hidden": false}, {"_id": "6955e3f8832867f253525607", "name": "Jingyu Shen", "hidden": false}, {"_id": "6955e3f8832867f253525608", "name": "XiaoShuai Song", "hidden": false}, {"_id": "6955e3f8832867f253525609", "name": "Cecilia Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560a", "name": "Chaofan Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560b", "name": "Kangyu Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560c", "name": "Pei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560d", "name": "Tao Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560e", "name": "Wei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560f", "name": "Ke Xiao", "hidden": false}, {"_id": "6955e3f8832867f253525610", "name": "Mingyu Xu", "hidden": false}, {"_id": "6955e3f8832867f253525611", "name": "Tiange Xu", "hidden": false}, {"_id": "6955e3f8832867f253525612", "name": "Nan Ya", "hidden": false}, {"_id": "6955e3f8832867f253525613", "name": "Siran Yang", "hidden": false}, {"_id": "6955e3f8832867f253525614", "name": "Jianan Ye", "hidden": false}, {"_id": "6955e3f8832867f253525615", "name": "Yaxing Zang", "hidden": false}, {"_id": "6955e3f8832867f253525616", "name": "Duo Zhang", "hidden": false}, {"_id": "6955e3f8832867f253525617", "name": "Junbo Zhang", "hidden": false}, {"_id": "6955e3f8832867f253525618", "name": "Boren Zheng", "hidden": false}, {"_id": "6955e3f8832867f253525619", "name": "Wanxi Deng", "hidden": false}, {"_id": "6955e3f8832867f25352561a", "name": "Ling Pan", "hidden": false}, {"_id": "6955e3f8832867f25352561b", "name": "Lin Qu", "hidden": false}, {"_id": "6955e3f8832867f25352561c", "name": "Wenbo Su", "hidden": false}, {"_id": "6955e3f8832867f25352561d", "name": "Jiamang Wang", "hidden": false}, {"_id": "6955e3f8832867f25352561e", "name": "Wei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352561f", "name": "Hu Wei", "hidden": false}, {"_id": "6955e3f8832867f253525620", "name": "Minggang Wu", "hidden": false}, {"_id": "6955e3f8832867f253525621", "name": "Cheng Yu", "hidden": false}, {"_id": "6955e3f8832867f253525622", "name": "Bing Zhao", "hidden": false}, {"_id": "6955e3f8832867f253525623", "name": "Zhicheng Zheng", "hidden": false}, {"_id": "6955e3f8832867f253525624", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-12-31T14:03:39.000Z", "submittedOnDailyAt": "2026-01-01T00:33:23.374Z", "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.", "upvotes": 33, "discussionId": "6955e3f8832867f253525625", "ai_summary": "The Agentic Learning Ecosystem (ALE) introduces a principled infrastructure for agent development, combining post-training optimization, sandbox environments, and policy alignment to enhance long-horizon training stability and performance in real-world tasks.", "ai_keywords": ["ROLL (post-training framework)", "ROCK (sandbox environment manager)", "iFlow CLI (agent framework)", "ROME (agentic model)", "data composition protocols", "Interaction-based Policy Alignment (IPA)", "semantic interaction chunks", "Terminal Bench Pro", "SWE-bench Verified"], "summary_zh": "<ul>\n    <li>Agentic crafting \u9700\u8981\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8fdb\u884c\u591a\u8f6e\u64cd\u4f5c\uff0c\u901a\u8fc7\u884c\u52a8\u3001\u89c2\u5bdf\u7ed3\u679c\u548c\u8fed\u4ee3\u6539\u8fdb\u3002</li>\n    <li>\u5f00\u6e90\u793e\u533a\u7f3a\u4e4f\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u751f\u6001\u7cfb\u7edf\u6765\u7b80\u5316\u4ee3\u7406\u5f00\u53d1\uff0c\u56e0\u6b64\u6211\u4eec\u5f15\u5165\u4e86\u4ee3\u7406\u5b66\u4e60\u751f\u6001\u7cfb\u7edf\uff08ALE\uff09\u3002</li>\n    <li>ALE \u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1aROLL\uff08\u6743\u91cd\u4f18\u5316\u6846\u67b6\uff09\u3001ROCK\uff08\u8f68\u8ff9\u751f\u6210\u6c99\u7bb1\u73af\u5883\u7ba1\u7406\u5668\uff09\u548c iFlow CLI\uff08\u9ad8\u6548\u7684\u4ee3\u7406\u6846\u67b6\uff09\u3002</li>\n    <li>\u6211\u4eec\u53d1\u5e03\u4e86 ROME\uff0c\u4e00\u4e2a\u57fa\u4e8e ALE \u7684\u5f00\u6e90\u4ee3\u7406\u6a21\u578b\uff0c\u7ecf\u8fc7\u4e00\u767e\u4e07\u6761\u8f68\u8ff9\u7684\u8bad\u7ec3\u3002</li>\n    <li>ROME \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bc1\u660e\u4e86 ALE \u57fa\u7840\u8bbe\u65bd\u7684\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Agentic crafting helps large language models (LLMs) learn by acting, observing, and improving over time.</li>\n    <li>The open-source community lacks a complete system for developing these intelligent agents, which is addressed by the new Agentic Learning Ecosystem (ALE).</li>\n    <li>ALE has three main parts: ROLL for optimizing model weights, ROCK for managing environments, and iFlow CLI for organizing agent context.</li>\n    <li>ROME, an open-source agent developed using ALE, is trained on over one million interactions to enhance its performance.</li>\n    <li>ROME uses a new method for policy optimization and has shown strong results in various benchmarks, validating the ALE approach.</li>\n</ul>"}, "publishedAt": "2025-12-31T09:03:39.000Z", "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem", "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24873.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 197}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24617", "authors": [{"_id": "69573165832867f253525871", "user": {"_id": "646b4c9fdf2609a541c0866e", "avatarUrl": "/avatars/7ec6c709017dcf32d4ac49d1e3820328.svg", "isPro": false, "fullname": "Qu", "user": "ScottQu", "type": "user"}, "name": "Xingwei Qu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:07.662Z", "hidden": false}, {"_id": "69573165832867f253525872", "name": "Shaowen Wang", "hidden": false}, {"_id": "69573165832867f253525873", "name": "Zihao Huang", "hidden": false}, {"_id": "69573165832867f253525874", "user": {"_id": "64e851825ddcace745ba15bd", "avatarUrl": "/avatars/7b6612c411222974d9ea181784eef915.svg", "isPro": false, "fullname": "Kai Hua", "user": "kkish", "type": "user"}, "name": "Kai Hua", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:53.467Z", "hidden": false}, {"_id": "69573165832867f253525875", "name": "Fan Yin", "hidden": false}, {"_id": "69573165832867f253525876", "name": "Rui-Jie Zhu", "hidden": false}, {"_id": "69573165832867f253525877", "name": "Jundong Zhou", "hidden": false}, {"_id": "69573165832867f253525878", "name": "Qiyang Min", "hidden": false}, {"_id": "69573165832867f253525879", "name": "Zihao Wang", "hidden": false}, {"_id": "69573165832867f25352587a", "name": "Yizhi Li", "hidden": false}, {"_id": "69573165832867f25352587b", "name": "Tianyu Zhang", "hidden": false}, {"_id": "69573165832867f25352587c", "name": "He Xing", "hidden": false}, {"_id": "69573165832867f25352587d", "name": "Zheng Zhang", "hidden": false}, {"_id": "69573165832867f25352587e", "name": "Yuxuan Song", "hidden": false}, {"_id": "69573165832867f25352587f", "name": "Tianyu Zheng", "hidden": false}, {"_id": "69573165832867f253525880", "name": "Zhiyuan Zeng", "hidden": false}, {"_id": "69573165832867f253525881", "name": "Chenghua Lin", "hidden": false}, {"_id": "69573165832867f253525882", "name": "Ge Zhang", "hidden": false}, {"_id": "69573165832867f253525883", "name": "Wenhao Huang", "hidden": false}], "publishedAt": "2025-12-31T04:19:33.000Z", "submittedOnDailyAt": "2026-01-02T00:17:55.450Z", "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space", "submittedOnDailyBy": {"_id": "646b4c9fdf2609a541c0866e", "avatarUrl": "/avatars/7ec6c709017dcf32d4ac49d1e3820328.svg", "isPro": false, "fullname": "Qu", "user": "ScottQu", "type": "user"}, "summary": "Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first compression-aware scaling law, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a decoupled \u03bcP parametrization that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting (R=4, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a +2.69\\% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.", "upvotes": 32, "discussionId": "69573165832867f253525884", "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u6240\u6709\u8bcd\u5143\u8fdb\u884c\u7edf\u4e00\u8ba1\u7b97\uff0c\u4f46\u8bed\u8a00\u7684\u4fe1\u606f\u5bc6\u5ea6\u662f\u4e0d\u5747\u5300\u7684\u3002</li>\n    <li>\u52a8\u6001\u5927\u578b\u6982\u5ff5\u6a21\u578b\uff08DLCM\uff09\u662f\u4e00\u79cd\u65b0\u7684\u8bed\u8a00\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u8bed\u4e49\u8fb9\u754c\u6765\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002</li>\n    <li>DLCM \u4e0d\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u7684\u8bed\u8a00\u5355\u4f4d\uff0c\u800c\u662f\u7aef\u5230\u7aef\u5730\u53d1\u73b0\u53ef\u53d8\u957f\u5ea6\u7684\u6982\u5ff5\u3002</li>\n    <li>\u5f15\u5165\u4e86\u538b\u7f29\u611f\u77e5\u7684\u6269\u5c55\u6cd5\u5219\uff0c\u4f7f\u8ba1\u7b97\u5206\u914d\u66f4\u5408\u7406\uff0c\u5e76\u652f\u6301\u4e0d\u540c\u5bbd\u5ea6\u548c\u538b\u7f29\u6a21\u5f0f\u7684\u8d85\u53c2\u6570\u8f6c\u79fb\u3002</li>\n    <li>\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0cDLCM \u5c06\u7ea6\u4e09\u5206\u4e4b\u4e00\u7684\u63a8\u7406\u8ba1\u7b97\u91cd\u65b0\u5206\u914d\u5230\u66f4\u9ad8\u5bb9\u91cf\u7684\u63a8\u7406\u6838\u5fc3\uff0c\u5e73\u5747\u63d0\u9ad8\u4e862.69%\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) use the same amount of computation for all parts of the text, even though some parts are more important than others.</li>\n    <li>The proposed Dynamic Large Concept Models (DLCM) framework learns important language boundaries and focuses computation on these key areas.</li>\n    <li>DLCM can identify flexible concepts without needing predefined language rules, improving efficiency in reasoning.</li>\n    <li>It introduces a new way to understand how different capacities (token-level and concept-level) relate to each other, allowing for better allocation of computing resources.</li>\n    <li>In tests, DLCM showed a +2.69% improvement in performance on 12 benchmarks while using the same amount of computing power.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:19:33.000Z", "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space", "summary": "Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first compression-aware scaling law, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a decoupled \u03bcP parametrization that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting (R=4, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a +2.69\\% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24617.png", "numComments": 4, "submittedBy": {"_id": "646b4c9fdf2609a541c0866e", "avatarUrl": "/avatars/7ec6c709017dcf32d4ac49d1e3820328.svg", "fullname": "Qu", "name": "ScottQu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.23705", "authors": [{"_id": "6953546989916ff627aa4002", "name": "Shaocong Xu", "hidden": false}, {"_id": "6953546989916ff627aa4003", "name": "Songlin Wei", "hidden": false}, {"_id": "6953546989916ff627aa4004", "name": "Qizhe Wei", "hidden": false}, {"_id": "6953546989916ff627aa4005", "name": "Zheng Geng", "hidden": false}, {"_id": "6953546989916ff627aa4006", "name": "Hong Li", "hidden": false}, {"_id": "6953546989916ff627aa4007", "name": "Licheng Shen", "hidden": false}, {"_id": "6953546989916ff627aa4008", "name": "Qianpu Sun", "hidden": false}, {"_id": "6953546989916ff627aa4009", "name": "Shu Han", "hidden": false}, {"_id": "6953546989916ff627aa400a", "name": "Bin Ma", "hidden": false}, {"_id": "6953546989916ff627aa400b", "name": "Bohan Li", "hidden": false}, {"_id": "6953546989916ff627aa400c", "name": "Chongjie Ye", "hidden": false}, {"_id": "6953546989916ff627aa400d", "name": "Yuhang Zheng", "hidden": false}, {"_id": "6953546989916ff627aa400e", "name": "Nan Wang", "hidden": false}, {"_id": "6953546989916ff627aa400f", "name": "Saining Zhang", "hidden": false}, {"_id": "6953546989916ff627aa4010", "name": "Hao Zhao", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/652bd2493a416e1f21beb01a/6NM5vLS_B3DlYtmX1N4A_.gif"], "publishedAt": "2025-12-29T18:59:24.000Z", "submittedOnDailyAt": "2025-12-30T01:56:18.708Z", "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation", "submittedOnDailyBy": {"_id": "652bd2493a416e1f21beb01a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652bd2493a416e1f21beb01a/tKijq1pbjmBZuRm82dNEV.jpeg", "isPro": true, "fullname": "Shaocong.Xu", "user": "Daniellesry", "type": "user"}, "summary": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.", "upvotes": 32, "discussionId": "6953546a89916ff627aa4011", "projectPage": "https://daniellli.github.io/projects/DKT/", "githubRepo": "https://github.com/Daniellli/DKT", "githubRepoAddedBy": "user", "githubStars": 94, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u900f\u660e\u7269\u4f53\u5bf9\u89c6\u89c9\u7cfb\u7edf\u6765\u8bf4\u5f88\u96be\u5904\u7406\uff0c\u56e0\u4e3a\u6298\u5c04\u3001\u53cd\u5c04\u548c\u900f\u5c04\u4f1a\u5bfc\u81f4\u6df1\u5ea6\u4f30\u8ba1\u4e0d\u51c6\u786e\u3002</li>\n    <li>\u6211\u4eec\u89c2\u5bdf\u5230\u73b0\u4ee3\u89c6\u9891\u6269\u6563\u6a21\u578b\u5df2\u7ecf\u80fd\u591f\u5408\u6210\u903c\u771f\u7684\u900f\u660e\u73b0\u8c61\uff0c\u8bf4\u660e\u5b83\u4eec\u638c\u63e1\u4e86\u5149\u5b66\u89c4\u5219\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86TransPhy3D\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b\u900f\u660e\u548c\u53cd\u5c04\u573a\u666f\u7684\u5408\u6210\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5171\u670911000\u4e2a\u5e8f\u5217\u3002</li>\n    <li>\u65b0\u6a21\u578bDKT\u5728\u900f\u660e\u5ea6\u76f8\u5173\u7684\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u51c6\u786e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u660e\u663e\u63d0\u9ad8\u3002</li>\n    <li>DKT\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8e\u5b9e\u9645\u64cd\u4f5c\u4e2d\u7684\u6df1\u5ea6\u4f30\u8ba1\uff0c\u63d0\u5347\u4e86\u5728\u5404\u79cd\u8868\u9762\u4e0a\u7684\u6210\u529f\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Transparent objects are difficult for perception systems due to issues like light refraction and reflection.</li>\n    <li>Researchers created TransPhy3D, a large dataset of transparent and reflective scenes using Blender/Cycles.</li>\n    <li>They developed a model called DKT that uses video diffusion techniques to accurately estimate depth and normals in videos.</li>\n    <li>DKT performs better than previous methods on benchmarks involving transparency and improves object grasping success rates.</li>\n    <li>The study suggests that generative video models can be effectively used for reliable perception in complex environments.</li>\n</ul>"}, "publishedAt": "2025-12-29T13:59:24.000Z", "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation", "summary": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/652bd2493a416e1f21beb01a/6NM5vLS_B3DlYtmX1N4A_.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23705.png", "numComments": 1, "submittedBy": {"_id": "652bd2493a416e1f21beb01a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652bd2493a416e1f21beb01a/tKijq1pbjmBZuRm82dNEV.jpeg", "fullname": "Shaocong.Xu", "name": "Daniellesry", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23709", "authors": [{"_id": "69537f4189916ff627aa40c0", "name": "Hau-Shiang Shiu", "hidden": false}, {"_id": "69537f4189916ff627aa40c1", "name": "Chin-Yang Lin", "hidden": false}, {"_id": "69537f4189916ff627aa40c2", "name": "Zhixiang Wang", "hidden": false}, {"_id": "69537f4189916ff627aa40c3", "name": "Chi-Wei Hsiao", "hidden": false}, {"_id": "69537f4189916ff627aa40c4", "name": "Po-Fan Yu", "hidden": false}, {"_id": "69537f4189916ff627aa40c5", "name": "Yu-Chih Chen", "hidden": false}, {"_id": "69537f4189916ff627aa40c6", "name": "Yu-Lun Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/4Urho_F4h3YB3NjJxTgCc.mp4"], "publishedAt": "2025-12-29T18:59:57.000Z", "submittedOnDailyAt": "2025-12-30T05:04:09.292Z", "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion", "submittedOnDailyBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "isPro": false, "fullname": "Yu-Lun Liu", "user": "yulunliu", "type": "user"}, "summary": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/", "upvotes": 29, "discussionId": "69537f4289916ff627aa40c7", "projectPage": "https://jamichss.github.io/stream-diffvsr-project-page/", "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStream-DiffVSR\u7684\u9ad8\u6548\u89c6\u9891\u8d85\u5206\u8fa8\u7387\uff08VSR\uff09\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u4e8e\u8fc7\u53bb\u7684\u5e27\uff0c\u907f\u514d\u4f7f\u7528\u672a\u6765\u5e27\u548c\u590d\u6742\u7684\u591a\u6b65\u53bb\u566a\u3002</li>\n    <li>\u7ed3\u5408\u4e86\u5feb\u901f\u63a8\u7406\u7684\u56db\u6b65\u53bb\u566a\u5668\u548c\u8fd0\u52a8\u5bf9\u9f50\u63d0\u793a\u7684\u81ea\u56de\u5f52\u65f6\u95f4\u5f15\u5bfc\u6a21\u5757\u3002</li>\n    <li>\u5728RTX4090 GPU\u4e0a\uff0c\u5904\u7406720p\u5e27\u4ec5\u97000.328\u79d2\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u50cf\u8d28\u91cf\u548c\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002</li>\n    <li>Stream-DiffVSR\u6210\u4e3a\u9996\u4e2a\u9002\u5408\u4f4e\u5ef6\u8fdf\u5728\u7ebf\u90e8\u7f72\u7684\u6269\u6563\u57fa\u7840VSR\u65b9\u6cd5\uff0c\u521d\u59cb\u5ef6\u8fdf\u51cf\u5c11\u52300.328\u79d2\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Stream-DiffVSR is a new method for improving video resolution that works quickly and efficiently.</li>\n    <li>It only uses past video frames, making it faster and more suitable for real-time applications.</li>\n    <li>The method includes advanced features that help maintain high video quality while reducing processing time.</li>\n    <li>Stream-DiffVSR can process 720p video frames in just 0.328 seconds, which is much faster than previous methods.</li>\n    <li>This approach achieves the lowest latency for diffusion-based video super-resolution, making it ideal for live streaming and other time-sensitive uses.</li>\n</ul>"}, "publishedAt": "2025-12-29T13:59:57.000Z", "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion", "summary": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/4Urho_F4h3YB3NjJxTgCc.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23709.png", "numComments": 1, "submittedBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "fullname": "Yu-Lun Liu", "name": "yulunliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23165", "authors": [{"_id": "6954cfbc869a8627b452c9ab", "name": "Qingyu Yin", "hidden": false}, {"_id": "6954cfbc869a8627b452c9ac", "name": "Yulun Wu", "hidden": false}, {"_id": "6954cfbc869a8627b452c9ad", "user": {"_id": "64bfa52094c0e3be4a33863e", "avatarUrl": "/avatars/bc0737bf4481433b669433facf6ce6f4.svg", "isPro": false, "fullname": "Shenzhennan", "user": "5456es", "type": "user"}, "name": "Zhennan Shen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:40:25.945Z", "hidden": false}, {"_id": "6954cfbc869a8627b452c9ae", "name": "Sunbowen Li", "hidden": false}, {"_id": "6954cfbc869a8627b452c9af", "name": "Zhilin Wang", "hidden": false}, {"_id": "6954cfbc869a8627b452c9b0", "name": "Yanshu Li", "hidden": false}, {"_id": "6954cfbc869a8627b452c9b1", "name": "Chak Tou Leong", "hidden": false}, {"_id": "6954cfbc869a8627b452c9b2", "name": "Jiale Kang", "hidden": false}, {"_id": "6954cfbc869a8627b452c9b3", "name": "Jinjin Gu", "hidden": false}], "publishedAt": "2025-12-29T03:13:08.000Z", "submittedOnDailyAt": "2025-12-31T04:55:40.592Z", "title": "Evaluating Parameter Efficient Methods for RLVR", "submittedOnDailyBy": {"_id": "6453cb22908e259483c0a061", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6453cb22908e259483c0a061/hMgdwZUsUbgquGalzPGzV.jpeg", "isPro": false, "fullname": "Qingyu_Yin", "user": "MikaStars39", "type": "user"}, "summary": "We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (e.g., PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (e.g., VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.", "upvotes": 22, "discussionId": "6954cfbc869a8627b452c9b4", "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5728\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u4e0b\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u6784\u53d8\u4f53\uff08\u5982DoRA\u3001AdaLoRA\u548cMiSS\uff09\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u7684LoRA\u3002</li>\n    <li>\u53d1\u73b0SVD\u521d\u59cb\u5316\u7b56\u7565\uff08\u5982PiSSA\u3001MiLoRA\uff09\u5b58\u5728\u8c31\u5d29\u6e83\u73b0\u8c61\uff0c\u5bfc\u81f4\u5176\u5931\u8d25\u3002</li>\n    <li>\u6781\u7aef\u53c2\u6570\u51cf\u5c11\uff08\u5982VeRA\u3001Rank-1\uff09\u4e25\u91cd\u9650\u5236\u4e86\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u8fd9\u9879\u5de5\u4f5c\u4e3a\u53c2\u6570\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u8fdb\u4e00\u6b65\u63a2\u7d22\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>This study evaluates different methods for fine-tuning language models in reinforcement learning focused on reasoning with verifiable feedback.</li>\n    <li>Over 12 methods were tested, revealing that alternatives to the common LoRA method, such as DoRA and AdaLoRA, perform better.</li>\n    <li>A problem called spectral collapse was found in some strategies, leading to poor performance due to mismatches in updates and optimization.</li>\n    <li>Extreme reductions in parameters can limit the model's ability to reason effectively.</li>\n    <li>The findings encourage further exploration of efficient methods in reinforcement learning.</li>\n</ul>"}, "publishedAt": "2025-12-28T22:13:08.000Z", "title": "Evaluating Parameter Efficient Methods for RLVR", "summary": "We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (e.g., PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (e.g., VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23165.png", "numComments": 3, "submittedBy": {"_id": "6453cb22908e259483c0a061", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6453cb22908e259483c0a061/hMgdwZUsUbgquGalzPGzV.jpeg", "fullname": "Qingyu_Yin", "name": "MikaStars39", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\uff0c\u9700\u8981\u53ef\u6269\u5c55\u548c\u53ef\u9760\u7684\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u3002</li>\n    <li>\u5f53\u524d\u7684\u6570\u636e\u5904\u7406\u65b9\u6cd5\u591a\u4e3a\u4e34\u65f6\u811a\u672c\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u548c\u53ef\u91cd\u73b0\u6027\uff0c\u5f71\u54cd\u6570\u636e\u751f\u6210\u7684\u6548\u7387\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DataFlow\uff0c\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u51c6\u5907\u6846\u67b6\uff0c\u652f\u6301\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>DataFlow\u5305\u542b\u8fd1200\u4e2a\u53ef\u91cd\u7528\u7684\u64cd\u4f5c\u7b26\u548c\u516d\u4e2a\u901a\u7528\u6570\u636e\u7ba1\u9053\uff0c\u8986\u76d6\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u9886\u57df\u3002</li>\n    <li>\u901a\u8fc7DataFlow\u751f\u6210\u7684\u6570\u636e\u96c6\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6570\u636e\u96c6\uff0c\u63d0\u5347\u4e86\u4e0b\u6e38\u6a21\u578b\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>High-quality data is essential for Large Language Models (LLMs), but current data preparation methods are often messy and unreliable.</li>\n    <li>DataFlow is a new framework that makes data preparation easier and more effective by using modular components and a user-friendly API.</li>\n    <li>It includes nearly 200 reusable tools and can handle various types of data, including text and code.</li>\n    <li>DataFlow-Agent helps users create data pipelines from simple language descriptions, improving usability.</li>\n    <li>The framework has shown to enhance LLM performance significantly, outperforming traditional datasets and methods in several tests.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni\u662f\u4e00\u4e2a\u901a\u7528\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\u3002</li>\n    <li>Kling-Omni\u80fd\u591f\u5904\u7406\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\uff0c\u751f\u6210\u7535\u5f71\u7ea7\u522b\u7684\u89c6\u9891\u5185\u5bb9\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5efa\u7acb\u5728\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\u548c\u9ad8\u6548\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u57fa\u7840\u4e0a\uff0c\u4f18\u5316\u4e86\u63a8\u7406\u8fc7\u7a0b\u3002</li>\n    <li>Kling-Omni\u5c55\u793a\u4e86\u51fa\u8272\u7684\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from different types of visual and language inputs.</li>\n    <li>It combines various tasks like video generation, editing, and reasoning into one integrated system.</li>\n    <li>The framework accepts different user inputs such as text, images, and videos to create cohesive video content.</li>\n    <li>Kling-Omni uses a strong data system and advanced training methods to enhance its video creation capabilities.</li>\n    <li>The system shows excellent performance in generating videos, editing based on reasoning, and following complex instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.08765", "authors": [{"_id": "6938da63dfc35938ba129f3c", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:07.767Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3d", "name": "Yefei He", "hidden": false}, {"_id": "6938da63dfc35938ba129f3e", "user": {"_id": "62d812e143df7719860d05d1", "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg", "isPro": false, "fullname": "zhekai chen", "user": "Azily", "type": "user"}, "name": "Zhekai Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:00.513Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3f", "name": "Shiwei Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f40", "user": {"_id": "637ee45b2438d7485b8d8f6a", "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg", "isPro": false, "fullname": "Xiaogang Xu", "user": "xiaogang00", "type": "user"}, "name": "Xiaogang Xu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:51.241Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f41", "name": "Bin Xia", "hidden": false}, {"_id": "6938da63dfc35938ba129f42", "name": "Dingdong Wang", "hidden": false}, {"_id": "6938da63dfc35938ba129f43", "name": "Hongwei Yi", "hidden": false}, {"_id": "6938da63dfc35938ba129f44", "user": {"_id": "65d5ec74cd05bc1eaa125040", "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg", "isPro": false, "fullname": "Xihui Liu", "user": "XihuiLiu", "type": "user"}, "name": "Xihui Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:32.582Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f45", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:26.372Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f46", "name": "Yu Liu", "hidden": false}, {"_id": "6938da63dfc35938ba129f47", "name": "Yingya Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f48", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:10.566Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "publishedAt": "2025-12-09T16:13:55.000Z", "submittedOnDailyAt": "2025-12-10T00:20:18.797Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "upvotes": 94, "discussionId": "6938da64dfc35938ba129f49", "githubRepo": "https://github.com/ali-vilab/Wan-Move", "githubRepoAddedBy": "user", "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.", "ai_keywords": ["motion control", "video generative models", "dense point trajectories", "latent space", "spatiotemporal feature map", "motion guidance", "image-to-video model", "auxiliary motion encoders", "fine-tuning", "MoveBench", "motion annotations"], "githubStars": 197, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>Wan-Move \u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u73b0\u6709\u7684\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5\u63a7\u5236\u7cbe\u5ea6\u4f4e\uff0c\u6269\u5c55\u6027\u6709\u9650\uff0c\u800c Wan-Move \u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u901a\u8fc7\u4f7f\u7528\u5bc6\u96c6\u7684\u70b9\u8f68\u8ff9\u8868\u793a\u7269\u4f53\u8fd0\u52a8\uff0cWan-Move \u5b9e\u73b0\u4e86\u5bf9\u573a\u666f\u7684\u7ec6\u81f4\u63a7\u5236\u3002</li>\n    <li>Wan-Move \u751f\u6210\u7684 5 \u79d2\u3001480p \u89c6\u9891\u7684\u8fd0\u52a8\u63a7\u5236\u80fd\u529b\u4e0e\u5546\u4e1a\u8f6f\u4ef6 Kling 1.5 Pro \u76f8\u5ab2\u7f8e\u3002</li>\n    <li>\u8be5\u6846\u67b6\u7684\u4ee3\u7801\u3001\u6a21\u578b\u548c\u57fa\u51c6\u6570\u636e\u5747\u5df2\u516c\u5f00\uff0c\u652f\u6301\u5168\u9762\u8bc4\u4f30\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Wan-Move is a new framework that improves motion control in video generation models.</li>\n    <li>It allows for precise and high-quality control over how objects move in videos.</li>\n    <li>The framework uses dense point trajectories to guide motion in a detailed way.</li>\n    <li>Wan-Move integrates easily with existing video models without needing major changes.</li>\n    <li>It has been shown to produce high-quality videos that compete with commercial tools, and a new benchmark called MoveBench has been created for evaluation.</li>\n</ul>"}, "publishedAt": "2025-12-09T11:13:55.000Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u57fa\u7840\u6a21\u578b\u80fd\u591f\u751f\u6210\u89c6\u89c9\u4e0a\u771f\u5b9e\u548c\u65f6\u95f4\u4e0a\u8fde\u8d2f\u7684\u5185\u5bb9\uff0c\u4f46\u5b83\u4eec\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u662f\u5426\u6355\u6349\u5230\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7ea6\u675f\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u6307\u6807\u5982Frechet\u89c6\u9891\u8ddd\u79bb(FVD)\u4fa7\u91cd\u4e8e\u611f\u77e5\u8d28\u91cf\uff0c\u5ffd\u89c6\u4e86\u63a8\u7406\u5931\u8d25\uff0c\u5982\u56e0\u679c\u5173\u7cfb\u548c\u7269\u7406\u4e00\u81f4\u6027\u7684\u8fdd\u53cd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MMGR\uff08\u591a\u6a21\u6001\u751f\u6210\u63a8\u7406\u8bc4\u4f30\u4e0e\u57fa\u51c6\uff09\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8bc4\u4f30\uff1a\u7269\u7406\u3001\u903b\u8f91\u3001\u4e09\u7ef4\u7a7a\u95f4\u3001\u4e8c\u7ef4\u7a7a\u95f4\u548c\u65f6\u95f4\u3002</li>\n    <li>MMGR\u5728\u4e09\u4e2a\u9886\u57df\u8bc4\u4f30\u751f\u6210\u63a8\u7406\uff0c\u5305\u62ec\u62bd\u8c61\u63a8\u7406\u3001\u5177\u8eab\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\uff0c\u5e76\u5e94\u7528\u7ec6\u81f4\u7684\u6307\u6807\u6765\u8bc4\u4f30\u89c6\u9891\u548c\u56fe\u50cf\u751f\u6210\u7684\u6574\u4f53\u6b63\u786e\u6027\u3002</li>\n    <li>\u6211\u4eec\u7684\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u4e3b\u8981\u5c40\u9650\u6027\uff0c\u5982\u8fc7\u5ea6\u4f9d\u8d56\u611f\u77e5\u6570\u636e\u3001\u5168\u7403\u72b6\u6001\u4e00\u81f4\u6027\u5dee\u4ee5\u53ca\u5956\u52b1\u89c6\u89c9\u53ef\u4fe1\u5ea6\u800c\u975e\u56e0\u679c\u6b63\u786e\u6027\u7684\u76ee\u6807\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video models can create realistic content, but they need to understand physical and logical rules to be reliable.</li>\n    <li>Current evaluation methods focus on visual quality, missing issues like causality and consistency.</li>\n    <li>MMGR is a new framework that assesses generative reasoning in five areas: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>\n    <li>It tests models in three fields: Abstract Reasoning, Embodied Navigation, and Physical Commonsense.</li>\n    <li>Results show that while models do okay in Physical Commonsense, they struggle with Abstract Reasoning and long-term spatial planning.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u79d1\u5b66\u901a\u7528\u667a\u80fd\uff08SGI\uff09\u7684\u5b9a\u4e49\u4ecd\u7f3a\u4e4f\u660e\u786e\u6846\u67b6\uff0cSGI\u6307\u7684\u662f\u81ea\u4e3b\u6784\u601d\u3001\u8c03\u67e5\u548c\u63a8\u7406\u79d1\u5b66\u9886\u57df\u7684\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u7528\u63a2\u7a76\u6a21\u578b\uff08PIM\uff09\u7684SGI\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u79d1\u5b66\u5bb6\u76f8\u5173\u7684\u4efb\u52a1\u6765\u5b9e\u73b0\uff1a\u6df1\u5165\u7814\u7a76\u3001\u521b\u610f\u751f\u6210\u3001\u5e72\u5b9e\u9a8c\u548c\u6e7f\u5b9e\u9a8c\u3002</li>\n    <li>SGI-Bench\u5305\u542b\u8d85\u8fc71000\u4e2a\u8de8\u5b66\u79d1\u7684\u6837\u672c\uff0c\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u6700\u65b0\u7684\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u663e\u793a\u5728\u6df1\u5165\u7814\u7a76\u4e2d\u5339\u914d\u5ea6\u8f83\u4f4e\uff0810-20%\uff09\uff0c\u521b\u610f\u7f3a\u4e4f\u53ef\u884c\u6027\u548c\u7ec6\u8282\uff0c\u5e72\u5b9e\u9a8c\u7684\u6267\u884c\u7ed3\u679c\u51c6\u786e\u6027\u4f4e\uff0c\u6e7f\u5b9e\u9a8c\u7684\u5e8f\u5217\u4fdd\u771f\u5ea6\u4f4e\uff0c\u5e76\u4e14\u591a\u6a21\u6001\u6bd4\u8f83\u63a8\u7406\u5b58\u5728\u6311\u6218\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\uff0c\u4f18\u5316\u63a8\u7406\u65f6\u7684\u68c0\u7d22\u589e\u5f3a\u65b0\u9896\u6027\u5956\u52b1\uff0c\u63d0\u5347\u5047\u8bbe\u7684\u65b0\u9896\u6027\uff0c\u672a\u4f9d\u8d56\u53c2\u8003\u7b54\u6848\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is no clear method for creating Scientific General Intelligence (SGI), which means AI can't fully think and work like a scientist.</li>\n    <li>We define SGI using the Practical Inquiry Model and test it with tasks like research, idea generation, and experiments.</li>\n    <li>SGI-Bench has over 1,000 tasks based on important scientific questions to evaluate AI performance.</li>\n    <li>Results show significant weaknesses in AI's ability to conduct deep research, generate practical ideas, and execute experiments accurately.</li>\n    <li>We propose a new technique called Test-Time Reinforcement Learning to improve AI's ability to generate new and useful scientific ideas during evaluation.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20619", "authors": [{"_id": "694b614d746a34b55dd53d1a", "name": "Jianhong Bai", "hidden": false}, {"_id": "694b614d746a34b55dd53d1b", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "694b614d746a34b55dd53d1c", "name": "Xintao Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1d", "name": "Fu Xiao", "hidden": false}, {"_id": "694b614d746a34b55dd53d1e", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1f", "name": "Qinghe Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d20", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "694b614d746a34b55dd53d21", "name": "Menghan Xia", "hidden": false}, {"_id": "694b614d746a34b55dd53d22", "name": "Zuozhu Liu", "hidden": false}, {"_id": "694b614d746a34b55dd53d23", "name": "Haoji Hu", "hidden": false}, {"_id": "694b614d746a34b55dd53d24", "name": "Pengfei Wan", "hidden": false}, {"_id": "694b614d746a34b55dd53d25", "name": "Kun Gai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "publishedAt": "2025-12-23T18:59:56.000Z", "submittedOnDailyAt": "2025-12-24T01:20:51.117Z", "title": "SemanticGen: Video Generation in Semantic Space", "submittedOnDailyBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "isPro": false, "fullname": "Jianhong Bai", "user": "jianhongbai", "type": "user"}, "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "upvotes": 77, "discussionId": "694b614d746a34b55dd53d26", "projectPage": "https://jianhongbai.github.io/SemanticGen/", "ai_summary": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.", "ai_keywords": ["VAE space", "VAE decoder", "semantic space", "diffusion model", "semantic video features", "bi-directional attention"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u4f20\u7edf\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u91c7\u7528VAE\u7a7a\u95f4\u6765\u5b66\u4e60\u89c6\u9891\u7684\u6f5c\u5728\u5206\u5e03\uff0c\u4f46\u751f\u6210\u957f\u89c6\u9891\u65f6\u901f\u5ea6\u6162\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002</li>\n    <li>\u672c\u8bba\u6587\u63d0\u51fa\u4e86SemanticGen\uff0c\u901a\u8fc7\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n    <li>SemanticGen\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u8fc7\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u8bed\u4e49\u89c6\u9891\u7279\u5f81\uff0c\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u8fd9\u4e9b\u7279\u5f81\u751f\u6210\u6700\u7ec8\u89c6\u9891\u3002</li>\n    <li>\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6bd4\u5728VAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u66f4\u5feb\u6536\u655b\uff0c\u4e14\u5bf9\u957f\u89c6\u9891\u751f\u6210\u66f4\u6709\u6548\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSemanticGen\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u548c\u5f3a\u57fa\u7ebf\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>SemanticGen is a new video generation model that improves upon traditional methods.</li>\n  <li>It starts by generating videos in a compact, high-level semantic space for better planning.</li>\n  <li>The process has two stages: first, it creates basic video features, then adds details for the final video.</li>\n  <li>This approach is faster and more efficient, especially for long videos.</li>\n  <li>SemanticGen produces high-quality videos and outperforms other leading methods in tests.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:56.000Z", "title": "SemanticGen: Video Generation in Semantic Space", "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png", "numComments": 2, "submittedBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "fullname": "Jianhong Bai", "name": "jianhongbai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.15431", "authors": [{"_id": "69437417542d62d58a7bf6c4", "name": "Haolong Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6c5", "name": "Jia Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c6", "name": "Xin Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c7", "name": "Yeqing Shen", "hidden": false}, {"_id": "69437417542d62d58a7bf6c8", "user": {"_id": "653614073f4248157d60ccdc", "avatarUrl": "/avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg", "isPro": false, "fullname": "mengziyang", "user": "zylate", "type": "user"}, "name": "Ziyang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:53.033Z", "hidden": false}, {"_id": "69437417542d62d58a7bf6c9", "name": "Zhimin Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6ca", "name": "Kaijun Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6cb", "name": "Jin Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf6cc", "name": "Lieyu Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6cd", "name": "Mi Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6ce", "name": "Shiliang Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6cf", "name": "Zhirui Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d0", "name": "Brian Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d1", "name": "Kang An", "hidden": false}, {"_id": "69437417542d62d58a7bf6d2", "name": "Chenyang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d3", "name": "Lei Lei", "hidden": false}, {"_id": "69437417542d62d58a7bf6d4", "name": "Mengmeng Duan", "hidden": false}, {"_id": "69437417542d62d58a7bf6d5", "name": "Danxun Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d6", "name": "Guodong Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d7", "name": "Hang Cheng", "hidden": false}, {"_id": "69437417542d62d58a7bf6d8", "name": "Hao Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d9", "name": "Jie Dong", "hidden": false}, {"_id": "69437417542d62d58a7bf6da", "name": "Junhao Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6db", "name": "Mei Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6dc", "name": "Renjie Yu", "hidden": false}, {"_id": "69437417542d62d58a7bf6dd", "name": "Shunshan Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6de", "name": "Xu Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf6df", "name": "Yiting Dai", "hidden": false}, {"_id": "69437417542d62d58a7bf6e0", "name": "Yineng Deng", "hidden": false}, {"_id": "69437417542d62d58a7bf6e1", "name": "Yingdan Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6e2", "name": "Zelin Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6e3", "name": "Wen Sun", "hidden": false}, {"_id": "69437417542d62d58a7bf6e4", "name": "Chengxu Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e5", "name": "Chunqin Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6e6", "name": "Dong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6e7", "name": "Fengqiong Xiao", "hidden": false}, {"_id": "69437417542d62d58a7bf6e8", "name": "Guanghao Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e9", "name": "Guopeng Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ea", "name": "Guozhen Peng", "hidden": false}, {"_id": "69437417542d62d58a7bf6eb", "name": "Hongbing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ec", "name": "Hang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ed", "name": "Hongming Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6ee", "name": "Jingjing Xie", "hidden": false}, {"_id": "69437417542d62d58a7bf6ef", "name": "Jianyong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6f0", "name": "Jingyang Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf6f1", "name": "Jiaju Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f2", "name": "Jiayu Yuan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f3", "name": "Jianpeng Yin", "hidden": false}, {"_id": "69437417542d62d58a7bf6f4", "name": "Kai Cao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f5", "name": "Liang Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f6", "name": "Liguo Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f7", "name": "Liying Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6f8", "name": "Mengqiang Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f9", "name": "Min Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fa", "name": "Manjiao Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fb", "name": "Mao Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf6fc", "name": "Mingxin Wan", "hidden": false}, {"_id": "69437417542d62d58a7bf6fd", "name": "Na Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6fe", "name": "Nan Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6ff", "name": "Ning Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf700", "name": "Peiyao Ma", "hidden": false}, {"_id": "69437417542d62d58a7bf701", "name": "Qingzhou Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf702", "name": "Qiao Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf703", "name": "Qinlin Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf704", "name": "Qiong Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf705", "name": "Qiongyao Li", "hidden": false}, {"_id": "69437417542d62d58a7bf706", "name": "Shangwu Zhong", "hidden": false}, {"_id": "69437417542d62d58a7bf707", "name": "Shuli Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf708", "name": "Shaofan Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf709", "name": "Shisi Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf70a", "name": "Shuang Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf70b", "name": "Xingbin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70c", "name": "Xiaojia Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70d", "name": "Xiaojie Hou", "hidden": false}, {"_id": "69437417542d62d58a7bf70e", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70f", "name": "Xuanti Feng", "hidden": false}, {"_id": "69437417542d62d58a7bf710", "name": "Xuedan Cai", "hidden": false}, {"_id": "69437417542d62d58a7bf711", "name": "Xuan Wen", "hidden": false}, {"_id": "69437417542d62d58a7bf712", "name": "Xianwei Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf713", "name": "Xin Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf714", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf715", "name": "Xin Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf716", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf717", "name": "Yukang Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf718", "name": "Yunfang Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf719", "name": "Yuqing Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf71a", "name": "Yixun Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf71b", "name": "Zejia Weng", "hidden": false}, {"_id": "69437417542d62d58a7bf71c", "name": "Zhonghao Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf71d", "name": "Zhiguo Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf71e", "name": "Zhuoyu Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf71f", "name": "Zheng Ge", "hidden": false}, {"_id": "69437417542d62d58a7bf720", "name": "Jing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf721", "name": "Yibo Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf722", "name": "Binxing Jiao", "hidden": false}, {"_id": "69437417542d62d58a7bf723", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf724", "name": "Daxin Jiang", "hidden": false}], "publishedAt": "2025-12-17T13:26:30.000Z", "submittedOnDailyAt": "2025-12-18T00:55:26.804Z", "title": "Step-GUI Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "upvotes": 77, "discussionId": "69437418542d62d58a7bf725", "projectPage": "https://opengelab.github.io/", "githubRepo": "https://github.com/stepfun-ai/gelab-zero", "githubRepoAddedBy": "user", "ai_summary": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.", "ai_keywords": ["multimodal large language models", "GUI automation", "self-evolving training pipeline", "Calibrated Step Reward System", "trajectory-level calibration", "Step-GUI", "GUI performance", "GUI-MCP", "Model Context Protocol", "AndroidWorld", "OSWorld", "ScreenShot-Pro", "AndroidDaily", "real-world mobile usage patterns", "hierarchical architecture", "low-level atomic operations", "high-level task delegation", "local specialist models", "high-privacy execution"], "githubStars": 1417, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u6211\u8fdb\u5316\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u901a\u8fc7\u6821\u51c6\u6b65\u5956\u52b1\u7cfb\u7edf\u63d0\u9ad8\u8bad\u7ec3\u6570\u636e\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\uff0c\u6210\u672c\u964d\u4f4e10-100\u500d\uff0c\u6807\u6ce8\u51c6\u786e\u7387\u8d85\u8fc790%\u3002</li>\n    <li>\u5f15\u5165\u4e86Step-GUI\u6a21\u578b\u7cfb\u5217\uff0c\u8fbe\u5230\u884c\u4e1a\u9886\u5148\u7684\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u901a\u7528\u80fd\u529b\u3002</li>\n    <li>\u4e3a\u4e86\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86GUI-MCP\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8eGUI\u81ea\u52a8\u5316\u7684\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff0c\u7ed3\u5408\u4e86\u4f4e\u7ea7\u539f\u5b50\u64cd\u4f5c\u548c\u9ad8\u7ea7\u4efb\u52a1\u59d4\u6d3e\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86AndroidDaily\u57fa\u51c6\uff0c\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u79fb\u52a8\u4f7f\u7528\u6a21\u5f0f\uff0c\u8bc4\u4f30\u4ee3\u7406\u662f\u5426\u80fd\u5904\u7406\u65e5\u5e38\u4f7f\u7528\u573a\u666f\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u63a8\u8fdb\u4e86\u5b9e\u7528GUI\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u5c55\u793a\u4e86\u5728\u65e5\u5e38\u6570\u5b57\u4ea4\u4e92\u4e2d\u7684\u5f3a\u5927\u5e94\u7528\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New methods for automating Graphical User Interfaces (GUIs) have been developed, focusing on getting reliable training data efficiently.</li>\n    <li>We created a self-evolving training system that improves data quality and reduces costs, achieving over 90% accuracy in annotations.</li>\n    <li>Our model, called Step-GUI, performs exceptionally well in GUI tasks, achieving high performance scores on various benchmarks.</li>\n    <li>To ensure user privacy and support different devices, we introduced the GUI-MCP, a protocol that organizes tasks and keeps sensitive data secure on devices.</li>\n    <li>We also launched AndroidDaily, a benchmark that tests how well our agents can perform real-world mobile tasks, with impressive results in daily usage scenarios.</li>\n</ul>"}, "publishedAt": "2025-12-17T08:26:30.000Z", "title": "Step-GUI Technical Report", "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15431.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13586", "authors": [{"_id": "6940d86d65f1e24a117805cd", "user": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "isPro": false, "fullname": "Jia-Nan Li", "user": "JinaLeejnl", "type": "user"}, "name": "Jia-Nan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:38:49.797Z", "hidden": false}, {"_id": "6940d86d65f1e24a117805ce", "name": "Jian Guan", "hidden": false}, {"_id": "6940d86d65f1e24a117805cf", "name": "Wei Wu", "hidden": false}, {"_id": "6940d86d65f1e24a117805d0", "name": "Chongxuan Li", "hidden": false}], "publishedAt": "2025-12-15T17:41:19.000Z", "submittedOnDailyAt": "2025-12-16T01:39:12.101Z", "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "submittedOnDailyBy": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "isPro": false, "fullname": "Jia-Nan Li", "user": "JinaLeejnl", "type": "user"}, "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.", "upvotes": 74, "discussionId": "6940d86d65f1e24a117805d1", "githubRepo": "https://github.com/ML-GSAI/ReFusion", "githubRepoAddedBy": "user", "ai_summary": "ReFusion, a novel masked diffusion model, improves performance and efficiency by using slot-based parallel decoding, achieving superior results compared to autoregressive models and traditional masked diffusion models.", "ai_keywords": ["autoregressive models", "masked diffusion models", "Key-Value caching", "parallel decoding", "token level", "slot level", "diffusion-based planning", "autoregressive infilling", "slot-based design", "slot-level permutation space"], "githubStars": 18, "summary_zh": "<ul>\n    <li>\u81ea\u56de\u5f52\u6a21\u578b\uff08ARMs\uff09\u63a8\u7406\u901f\u5ea6\u6162\uff0cmasked diffusion models\uff08MDMs\uff09\u4f5c\u4e3a\u5e76\u884c\u66ff\u4ee3\u65b9\u6848\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u751f\u6210\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002</li>\n    <li>ReFusion\u662f\u4e00\u79cd\u65b0\u578b\u7684masked diffusion model\uff0c\u901a\u8fc7\u5c06\u5e76\u884c\u89e3\u7801\u63d0\u5347\u5230\u66f4\u9ad8\u7684\u69fd\u7ea7\u522b\u6765\u63d0\u9ad8\u6027\u80fd\u548c\u6548\u7387\u3002</li>\n    <li>\u5b83\u91c7\u7528\u8fed\u4ee3\u7684\u201c\u8ba1\u5212-\u586b\u5145\u201d\u89e3\u7801\u8fc7\u7a0b\uff0c\u9996\u5148\u8bc6\u522b\u51fa\u5f31\u4f9d\u8d56\u7684\u69fd\uff0c\u7136\u540e\u5e76\u884c\u89e3\u7801\u8fd9\u4e9b\u69fd\u3002</li>\n    <li>\u69fd\u7ea7\u8bbe\u8ba1\u4f7f\u5f97KV\u7f13\u5b58\u7684\u5b8c\u5168\u91cd\u7528\u6210\u4e3a\u53ef\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u5b66\u4e60\u590d\u6742\u5ea6\u3002</li>\n    <li>\u5728\u4e03\u4e2a\u4e0d\u540c\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReFusion\u7684\u6027\u80fd\u63d0\u534734%\uff0c\u901f\u5ea6\u5e73\u5747\u63d0\u9ad8\u8d85\u8fc718\u500d\uff0c\u540c\u65f6\u5728\u4e0e\u5f3a\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6bd4\u8f83\u4e2d\u4fdd\u63012.33\u500d\u7684\u901f\u5ea6\u4f18\u52bf\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Autoregressive models (ARMs) take a long time to make predictions one at a time.</li>\n  <li>Masked diffusion models (MDMs) are faster but have high computational costs and can produce unclear results.</li>\n  <li>ReFusion is a new model that improves speed and performance by working with groups of tokens called \"slots\" instead of individual tokens.</li>\n  <li>It uses a two-step process: first planning which slots to focus on, then filling them in all at once.</li>\n  <li>ReFusion performs significantly better than previous MDMs and is nearly as efficient as ARMs, offering a major speed boost.</li>\n</ul>"}, "publishedAt": "2025-12-15T12:41:19.000Z", "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13586.png", "numComments": 2, "submittedBy": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "fullname": "Jia-Nan Li", "name": "JinaLeejnl", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "isAuthorParticipating": true}, {"paper": {"id": "2512.23447", "authors": [{"_id": "69534a9589916ff627aa3f5c", "name": "Ang Lv", "hidden": false}, {"_id": "69534a9589916ff627aa3f5d", "name": "Jin Ma", "hidden": false}, {"_id": "69534a9589916ff627aa3f5e", "name": "Yiyuan Ma", "hidden": false}, {"_id": "69534a9589916ff627aa3f5f", "name": "Siyuan Qiao", "hidden": false}], "publishedAt": "2025-12-29T13:03:18.000Z", "submittedOnDailyAt": "2025-12-30T01:18:40.635Z", "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss", "submittedOnDailyBy": {"_id": "64b8ca3c5067873176d4b436", "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg", "isPro": false, "fullname": "AngLv", "user": "AngLv", "type": "user"}, "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.", "upvotes": 71, "discussionId": "69534a9589916ff627aa3f60", "ai_summary": "An expert-router coupling (ERC) loss aligns router decisions with expert capabilities in Mixture-of-Experts (MoE) models by enforcing constraints on internal activations, improving performance and computational efficiency.", "ai_keywords": ["Mixture-of-Experts (MoE)", "expert-router coupling (ERC) loss", "router embeddings", "proxy tokens", "internal activations", "MoE-LLMs", "expert specialization levels", "n\u00b2 activations"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>Mixture-of-Experts (MoE) \u6a21\u578b\u7684\u8def\u7531\u5668\u51b3\u7b56\u4e0e\u4e13\u5bb6\u80fd\u529b\u4e4b\u95f4\u7f3a\u4e4f\u660e\u786e\u7ea6\u675f\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u79f0\u4e3a\u4e13\u5bb6-\u8def\u7531\u5668\u8026\u5408\u635f\u5931\uff08ERC\u635f\u5931\uff09\uff0c\u5c06\u8def\u7531\u5668\u7684\u51b3\u7b56\u4e0e\u4e13\u5bb6\u80fd\u529b\u7d27\u5bc6\u7ed3\u5408\u3002</li>\n    <li>ERC\u635f\u5931\u5f3a\u5236\u8981\u6c42\u6bcf\u4e2a\u4e13\u5bb6\u5bf9\u5176\u4ee3\u7406\u6807\u8bb0\u7684\u6fc0\u6d3b\u9ad8\u4e8e\u5176\u4ed6\u4e13\u5bb6\u7684\u4ee3\u7406\u6807\u8bb0\u3002</li>\n    <li>ERC\u635f\u5931\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u4ec5\u9700\u5904\u7406 n^2 \u6fc0\u6d3b\uff0c\u5176\u4e2d n \u662f\u4e13\u5bb6\u6570\u91cf\uff0c\u4e0e\u6279\u91cf\u5927\u5c0f\u65e0\u5173\u3002</li>\n    <li>\u901a\u8fc7\u5bf9\u5927\u89c4\u6a21 MoE-LLM \u7684\u9884\u8bad\u7ec3\uff0c\u8bc1\u660e\u4e86 ERC \u635f\u5931\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u4e13\u5bb6\u4e13\u4e1a\u5316\u6c34\u5e73\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u63a7\u5236\u548c\u91cf\u5316\u8ddf\u8e2a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Mixture-of-Experts (MoE) models struggle because the router's choices don't always match the experts' strengths.</li>\n    <li>To improve this, we introduced a new loss called expert-router coupling (ERC) loss, which connects the router's decisions to the experts' abilities.</li>\n    <li>The ERC loss ensures that each expert responds better to its own assigned tokens than to those of other experts.</li>\n    <li>This method is efficient, only needing calculations based on the number of experts, not the number of tokens, making it faster than previous methods.</li>\n    <li>We tested the ERC loss on large MoE models and found it effectively tracks how well each expert specializes in handling its assigned tokens.</li>\n</ul>"}, "publishedAt": "2025-12-29T08:03:18.000Z", "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss", "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23447.png", "numComments": 1, "submittedBy": {"_id": "64b8ca3c5067873176d4b436", "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg", "fullname": "AngLv", "name": "AngLv", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "name": "Chunkang Zhang", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 70, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 22, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u662f\u4e00\u79cd\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5168\u5c40\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u7b56\u7565\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u6a21\u5757\u4e3b\u8981\u4f5c\u4e3a\u88ab\u52a8\u5b58\u50a8\uff0c\u65e0\u6cd5\u6709\u6548\u6574\u5408\u4fe1\u606f\uff0c\u5bfc\u81f4\u63a8\u7406\u788e\u7247\u5316\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HGMem\uff0c\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u80fd\u591f\u52a8\u6001\u8868\u793a\u590d\u6742\u63a8\u7406\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u8fb9\u8fde\u63a5\u4e0d\u540c\u7684\u8bb0\u5fc6\u5355\u5143\uff0c\u5f62\u6210\u66f4\u9ad8\u9636\u7684\u4e92\u52a8\uff0c\u589e\u5f3a\u4e86\u6574\u4f53\u7406\u89e3\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHGMem\u5728\u591a\u4e2a\u6311\u6218\u6027\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u591a\u6b65\u9aa4RAG\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps large language models (LLMs) with complex reasoning tasks.</li>\n    <li>Current memory designs mainly store facts without connecting them, limiting the ability to understand relationships between information.</li>\n    <li>HGMem is a new memory mechanism that uses a hypergraph structure to create dynamic connections between facts for better reasoning.</li>\n    <li>This approach allows for deeper understanding and improved reasoning in multi-step tasks.</li>\n    <li>Tests show that HGMem significantly outperforms traditional memory systems on various challenging datasets.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Jan 04, 2026";
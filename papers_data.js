window.trendingPapers = {
    "today": [{"paper": {"id": "2512.15431", "authors": [{"_id": "69437417542d62d58a7bf6c4", "name": "Haolong Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6c5", "name": "Jia Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c6", "name": "Xin Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c7", "name": "Yeqing Shen", "hidden": false}, {"_id": "69437417542d62d58a7bf6c8", "user": {"_id": "653614073f4248157d60ccdc", "avatarUrl": "/avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg", "isPro": false, "fullname": "mengziyang", "user": "zylate", "type": "user"}, "name": "Ziyang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:53.033Z", "hidden": false}, {"_id": "69437417542d62d58a7bf6c9", "name": "Zhimin Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6ca", "name": "Kaijun Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6cb", "name": "Jin Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf6cc", "name": "Lieyu Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6cd", "name": "Mi Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6ce", "name": "Shiliang Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6cf", "name": "Zhirui Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d0", "name": "Brian Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d1", "name": "Kang An", "hidden": false}, {"_id": "69437417542d62d58a7bf6d2", "name": "Chenyang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d3", "name": "Lei Lei", "hidden": false}, {"_id": "69437417542d62d58a7bf6d4", "name": "Mengmeng Duan", "hidden": false}, {"_id": "69437417542d62d58a7bf6d5", "name": "Danxun Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d6", "name": "Guodong Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d7", "name": "Hang Cheng", "hidden": false}, {"_id": "69437417542d62d58a7bf6d8", "name": "Hao Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d9", "name": "Jie Dong", "hidden": false}, {"_id": "69437417542d62d58a7bf6da", "name": "Junhao Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6db", "name": "Mei Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6dc", "name": "Renjie Yu", "hidden": false}, {"_id": "69437417542d62d58a7bf6dd", "name": "Shunshan Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6de", "name": "Xu Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf6df", "name": "Yiting Dai", "hidden": false}, {"_id": "69437417542d62d58a7bf6e0", "name": "Yineng Deng", "hidden": false}, {"_id": "69437417542d62d58a7bf6e1", "name": "Yingdan Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6e2", "name": "Zelin Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6e3", "name": "Wen Sun", "hidden": false}, {"_id": "69437417542d62d58a7bf6e4", "name": "Chengxu Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e5", "name": "Chunqin Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6e6", "name": "Dong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6e7", "name": "Fengqiong Xiao", "hidden": false}, {"_id": "69437417542d62d58a7bf6e8", "name": "Guanghao Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e9", "name": "Guopeng Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ea", "name": "Guozhen Peng", "hidden": false}, {"_id": "69437417542d62d58a7bf6eb", "name": "Hongbing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ec", "name": "Hang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ed", "name": "Hongming Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6ee", "name": "Jingjing Xie", "hidden": false}, {"_id": "69437417542d62d58a7bf6ef", "name": "Jianyong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6f0", "name": "Jingyang Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf6f1", "name": "Jiaju Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f2", "name": "Jiayu Yuan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f3", "name": "Jianpeng Yin", "hidden": false}, {"_id": "69437417542d62d58a7bf6f4", "name": "Kai Cao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f5", "name": "Liang Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f6", "name": "Liguo Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f7", "name": "Liying Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6f8", "name": "Mengqiang Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f9", "name": "Min Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fa", "name": "Manjiao Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fb", "name": "Mao Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf6fc", "name": "Mingxin Wan", "hidden": false}, {"_id": "69437417542d62d58a7bf6fd", "name": "Na Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6fe", "name": "Nan Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6ff", "name": "Ning Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf700", "name": "Peiyao Ma", "hidden": false}, {"_id": "69437417542d62d58a7bf701", "name": "Qingzhou Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf702", "name": "Qiao Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf703", "name": "Qinlin Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf704", "name": "Qiong Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf705", "name": "Qiongyao Li", "hidden": false}, {"_id": "69437417542d62d58a7bf706", "name": "Shangwu Zhong", "hidden": false}, {"_id": "69437417542d62d58a7bf707", "name": "Shuli Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf708", "name": "Shaofan Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf709", "name": "Shisi Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf70a", "name": "Shuang Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf70b", "name": "Xingbin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70c", "name": "Xiaojia Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70d", "name": "Xiaojie Hou", "hidden": false}, {"_id": "69437417542d62d58a7bf70e", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70f", "name": "Xuanti Feng", "hidden": false}, {"_id": "69437417542d62d58a7bf710", "name": "Xuedan Cai", "hidden": false}, {"_id": "69437417542d62d58a7bf711", "name": "Xuan Wen", "hidden": false}, {"_id": "69437417542d62d58a7bf712", "name": "Xianwei Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf713", "name": "Xin Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf714", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf715", "name": "Xin Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf716", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf717", "name": "Yukang Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf718", "name": "Yunfang Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf719", "name": "Yuqing Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf71a", "name": "Yixun Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf71b", "name": "Zejia Weng", "hidden": false}, {"_id": "69437417542d62d58a7bf71c", "name": "Zhonghao Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf71d", "name": "Zhiguo Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf71e", "name": "Zhuoyu Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf71f", "name": "Zheng Ge", "hidden": false}, {"_id": "69437417542d62d58a7bf720", "name": "Jing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf721", "name": "Yibo Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf722", "name": "Binxing Jiao", "hidden": false}, {"_id": "69437417542d62d58a7bf723", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf724", "name": "Daxin Jiang", "hidden": false}], "publishedAt": "2025-12-17T13:26:30.000Z", "submittedOnDailyAt": "2025-12-18T00:55:26.804Z", "title": "Step-GUI Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "upvotes": 77, "discussionId": "69437418542d62d58a7bf725", "projectPage": "https://opengelab.github.io/", "githubRepo": "https://github.com/stepfun-ai/gelab-zero", "githubRepoAddedBy": "user", "ai_summary": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.", "ai_keywords": ["multimodal large language models", "GUI automation", "self-evolving training pipeline", "Calibrated Step Reward System", "trajectory-level calibration", "Step-GUI", "GUI performance", "GUI-MCP", "Model Context Protocol", "AndroidWorld", "OSWorld", "ScreenShot-Pro", "AndroidDaily", "real-world mobile usage patterns", "hierarchical architecture", "low-level atomic operations", "high-level task delegation", "local specialist models", "high-privacy execution"], "githubStars": 1417, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4f7f\u7528\u6821\u51c6\u6b65\u9aa4\u5956\u52b1\u7cfb\u7edf\u63d0\u9ad8\u8bad\u7ec3\u6570\u636e\u7684\u8d28\u91cf\u548c\u6210\u672c\u6548\u76ca\u3002</li>\n    <li>\u5f00\u53d1\u4e86Step-GUI\u6a21\u578b\u7cfb\u5217\uff0c\u5c55\u793a\u4e86\u5728GUI\u6027\u80fd\u65b9\u9762\u7684\u9886\u5148\u8868\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u7528\u80fd\u529b\u3002</li>\n    <li>\u5f15\u5165\u4e86GUI-MCP\u534f\u8bae\uff0c\u786e\u4fdd\u4e0d\u540c\u8bbe\u5907\u4e4b\u95f4\u7684\u6807\u51c6\u5316\u63a5\u53e3\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002</li>\n    <li>\u63a8\u51fa\u4e86AndroidDaily\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8e\u771f\u5b9e\u7684\u79fb\u52a8\u4f7f\u7528\u6a21\u5f0f\u8bc4\u4f30\u4ee3\u7406\u7684\u8868\u73b0\u3002</li>\n    <li>\u6b64\u7814\u7a76\u63a8\u52a8\u4e86\u5b9e\u7528GUI\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u65e5\u5e38\u6570\u5b57\u4ea4\u4e92\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New methods in large language models improve automation for graphical user interfaces (GUIs), but getting good training data is still a challenge.</li>\n    <li>We developed a training system called the Calibrated Step Reward System that makes it cheaper and more reliable to train models, achieving over 90% accuracy.</li>\n    <li>We created Step-GUI models that perform well on GUIs, with impressive scores on benchmarks, while also being generally capable in other areas.</li>\n    <li>To ensure user privacy and standardize interactions across devices, we introduced GUI-MCP, a new protocol that manages tasks effectively while keeping data secure on devices.</li>\n    <li>We also launched AndroidDaily, a benchmark that reflects real mobile usage, helping to test how well agents can perform everyday tasks.</li>\n</ul>"}, "publishedAt": "2025-12-17T08:26:30.000Z", "title": "Step-GUI Technical Report", "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15431.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.15176", "authors": [{"_id": "694370d7542d62d58a7bf698", "name": "Zicong Cheng", "hidden": false}, {"_id": "694370d7542d62d58a7bf699", "name": "Guo-Wei Yang", "hidden": false}, {"_id": "694370d7542d62d58a7bf69a", "name": "Jia Li", "hidden": false}, {"_id": "694370d7542d62d58a7bf69b", "name": "Zhijie Deng", "hidden": false}, {"_id": "694370d7542d62d58a7bf69c", "user": {"_id": "6571b51fd5c6a6d3b0ba68ad", "avatarUrl": "/avatars/0ccd8fe8de857753b534356a90eb10f0.svg", "isPro": false, "fullname": "gmh", "user": "menghao22", "type": "user"}, "name": "Meng-Hao Guo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:54.790Z", "hidden": false}, {"_id": "694370d7542d62d58a7bf69d", "name": "Shi-Min Hu", "hidden": false}], "publishedAt": "2025-12-17T08:19:04.000Z", "submittedOnDailyAt": "2025-12-18T00:44:13.153Z", "title": "DEER: Draft with Diffusion, Verify with Autoregressive Models", "submittedOnDailyBy": {"_id": "6571b51fd5c6a6d3b0ba68ad", "avatarUrl": "/avatars/0ccd8fe8de857753b534356a90eb10f0.svg", "isPro": false, "fullname": "gmh", "user": "menghao22", "type": "user"}, "summary": "Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/", "upvotes": 37, "discussionId": "694370d8542d62d58a7bf69e", "projectPage": "https://czc726.github.io/DEER/", "ai_summary": "DEER framework uses diffusion large language models for efficient speculative decoding, overcoming the limitations of autoregressive drafters with better speed and draft quality.", "ai_keywords": ["autoregressive decoding", "speculative decoding", "diffusion large language model", "dLLM", "draft-verify scheme", "step-wise uncertainty accumulation", "parallel decoding", "two-stage training pipeline", "single-step decoding", "HumanEval", "Qwen3-30B-A3B"], "summary_zh": "<ul>\n    <li>\u6548\u7387\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u63a8\u7406\u7cfb\u7edf\u9762\u4e34\u7684\u91cd\u8981\u6311\u6218\uff0c\u53d7\u9650\u4e8e\u81ea\u56de\u5f52\uff08AR\uff09\u89e3\u7801\u7684\u5ef6\u8fdf\u3002</li>\n    <li>\u73b0\u6709\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u4f9d\u8d56\u4e8eAR\u8349\u62df\u6a21\u578b\uff0c\u5b58\u5728\u4fe1\u4efb\u7d2f\u79ef\u4e0d\u7a33\u5b9a\u548c\u9010\u6b65\u89e3\u7801\u7684\u5c40\u9650\u6027\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6269\u6563\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08dLLM\uff09\u8349\u62df\u5668\uff0c\u80fd\u591f\u901a\u8fc7\u5e76\u884c\u89e3\u7801\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86DEER\uff0c\u4e00\u4e2a\u9ad8\u6548\u7684\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u6269\u6563\u8349\u62df\u548cAR\u6a21\u578b\u9a8c\u8bc1\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cDEER\u5728\u8349\u62df\u63a5\u53d7\u957f\u5ea6\u548c\u89e3\u7801\u901f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Efficiency is a major challenge for systems using large language models (LLMs) due to slow autoregressive (AR) decoding.</li>\n    <li>Speculative decoding helps speed things up but has issues with trust and speed when using AR models for drafting.</li>\n    <li>This paper introduces DEER, a new framework that uses diffusion large language models (dLLMs) for drafting and AR models for verification.</li>\n    <li>DEER trains its dLLM drafters to align closely with the target AR model, allowing for faster and longer draft segments.</li>\n    <li>Experiments show DEER performs better than previous methods, achieving significant speedups and longer drafts.</li>\n</ul>"}, "publishedAt": "2025-12-17T03:19:04.000Z", "title": "DEER: Draft with Diffusion, Verify with Autoregressive Models", "summary": "Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15176.png", "numComments": 2, "submittedBy": {"_id": "6571b51fd5c6a6d3b0ba68ad", "avatarUrl": "/avatars/0ccd8fe8de857753b534356a90eb10f0.svg", "fullname": "gmh", "name": "menghao22", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": true}, {"paper": {"id": "2512.14681", "authors": [{"_id": "69437375542d62d58a7bf6aa", "name": "Lanxiang Hu", "hidden": false}, {"_id": "69437375542d62d58a7bf6ab", "name": "Siqi Kou", "hidden": false}, {"_id": "69437375542d62d58a7bf6ac", "name": "Yichao Fu", "hidden": false}, {"_id": "69437375542d62d58a7bf6ad", "name": "Samyam Rajbhandari", "hidden": false}, {"_id": "69437375542d62d58a7bf6ae", "name": "Tajana Rosing", "hidden": false}, {"_id": "69437375542d62d58a7bf6af", "name": "Yuxiong He", "hidden": false}, {"_id": "69437375542d62d58a7bf6b0", "name": "Zhijie Deng", "hidden": false}, {"_id": "69437375542d62d58a7bf6b1", "name": "Hao Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/X_SGgKHd6c9A0-I-9QNyn.gif", "https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/MidLBXBtdUmG0fY7Wyh27.gif", "https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/qVt5G_vvppSDGMXPPG8Sf.png", "https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/tJhlEhoQSGi5hoe8KvTId.gif"], "publishedAt": "2025-12-16T18:45:18.000Z", "submittedOnDailyAt": "2025-12-18T00:57:47.270Z", "title": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing", "submittedOnDailyBy": {"_id": "6301d6455e305a35cb0846a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg", "isPro": true, "fullname": "Lanxiang Hu", "user": "Snyhlxde", "type": "user"}, "summary": "Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.", "upvotes": 36, "discussionId": "69437375542d62d58a7bf6b2", "githubRepo": "https://github.com/hao-ai-lab/JacobiForcing", "githubRepoAddedBy": "user", "ai_summary": "Jacobi Forcing is a progressive distillation method that enables efficient parallel decoding of transformer-based models while maintaining performance, significantly reducing inference latency.", "ai_keywords": ["Multi-token generation", "diffusion Large Language Models (dLLMs)", "parallel decoding", "inference latency", "adaptive autoregressive (AR) models", "masked data distribution", "bidirectional attention", "causal inference", "pretrained causal inference property", "trajectory characteristics", "multi-block decoding", "rejection recycling"], "githubStars": 52, "summary_zh": "<ul>\n    <li>\u591a\u6807\u8bb0\u751f\u6210\u6280\u672f\u53ef\u4ee5\u52a0\u901f\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u5927\u578b\u6a21\u578b\u63a8\u7406\u3002</li>\n    <li>\u6700\u8fd1\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u901a\u8fc7\u6269\u6563\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u5e76\u884c\u89e3\u7801\u6765\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\u3002</li>\n    <li>\u4e3a\u4e86\u5b9e\u73b0\u63a5\u8fd1\u81ea\u56de\u5f52\uff08AR\uff09\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\uff0c\u8bb8\u591a\u6280\u672f\u5c06AR\u6a21\u578b\u9002\u914d\u4e3adLLMs\uff0c\u4f46\u901f\u5ea6\u63d0\u5347\u6709\u9650\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Jacobi Forcing\uff0c\u8fd9\u662f\u4e00\u79cd\u6e10\u8fdb\u84b8\u998f\u8303\u5f0f\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u9884\u8bad\u7ec3\u56e0\u679c\u63a8\u7406\u5c5e\u6027\u7684\u540c\u65f6\uff0c\u5c06AR\u6a21\u578b\u9ad8\u6548\u8f6c\u53d8\u4e3a\u5e76\u884c\u89e3\u7801\u5668\u3002</li>\n    <li>Jacobi Forcing\u6a21\u578b\u5728\u7f16\u7801\u548c\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e863.8\u500d\u7684\u5899\u949f\u901f\u5ea6\u63d0\u5347\uff0c\u5e76\u5f15\u5165\u4e86\u591a\u5757\u89e3\u7801\u548c\u62d2\u7edd\u56de\u6536\u7684\u65b9\u6cd5\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-token generation helps speed up the inference of large models using transformers.</li>\n    <li>Recent work focuses on using diffusion Large Language Models (dLLMs) for faster decoding, but they face limitations due to differences between training and real-world data.</li>\n    <li>Jacobi Forcing is a new method that trains models on their own generated data, helping them become efficient parallel decoders while keeping their original strengths.</li>\n    <li>Models using Jacobi Forcing can achieve a 3.8x speed increase in tasks like coding and math, with little performance loss.</li>\n    <li>A further technique called multi-block decoding can increase the number of accepted tokens and provide up to a 4.0x speedup in inference time.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:45:18.000Z", "title": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing", "summary": "Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/X_SGgKHd6c9A0-I-9QNyn.gif", "https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/MidLBXBtdUmG0fY7Wyh27.gif", "https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/qVt5G_vvppSDGMXPPG8Sf.png", "https://cdn-uploads.huggingface.co/production/uploads/6301d6455e305a35cb0846a7/tJhlEhoQSGi5hoe8KvTId.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14681.png", "numComments": 2, "submittedBy": {"_id": "6301d6455e305a35cb0846a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6301d6455e305a35cb0846a7/aT2AtzRMSY_T3y02MIUap.jpeg", "fullname": "Lanxiang Hu", "name": "Snyhlxde", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "isAuthorParticipating": false}, {"paper": {"id": "2512.14052", "authors": [{"_id": "6942650d5d5b2dc1052749c8", "name": "HyperAI Team", "hidden": false}, {"_id": "6942650d5d5b2dc1052749c9", "name": "Yuchen Liu", "hidden": false}, {"_id": "6942650d5d5b2dc1052749ca", "name": "Kaiyang Han", "hidden": false}, {"_id": "6942650d5d5b2dc1052749cb", "name": "Zhiqiang Xia", "hidden": false}, {"_id": "6942650d5d5b2dc1052749cc", "name": "Yuhang Dong", "hidden": false}, {"_id": "6942650d5d5b2dc1052749cd", "name": "Chen Song", "hidden": false}, {"_id": "6942650d5d5b2dc1052749ce", "name": "Kangyu Tang", "hidden": false}, {"_id": "6942650d5d5b2dc1052749cf", "name": "Jiaming Xu", "hidden": false}, {"_id": "6942650d5d5b2dc1052749d0", "name": "Xiushi Feng", "hidden": false}, {"_id": "6942650d5d5b2dc1052749d1", "name": "WenXuan Yu", "hidden": false}, {"_id": "6942650d5d5b2dc1052749d2", "name": "Li Peng", "hidden": false}, {"_id": "6942650d5d5b2dc1052749d3", "name": "Mingyang Wang", "hidden": false}, {"_id": "6942650d5d5b2dc1052749d4", "name": "Kai Wang", "hidden": false}, {"_id": "6942650d5d5b2dc1052749d5", "name": "Changpeng Yang", "hidden": false}, {"_id": "6942650d5d5b2dc1052749d6", "name": "Yang Li", "hidden": false}, {"_id": "6942650d5d5b2dc1052749d7", "name": "Haoyu Lu", "hidden": false}, {"_id": "6942650d5d5b2dc1052749d8", "name": "Hao Wang", "hidden": false}, {"_id": "6942650d5d5b2dc1052749d9", "name": "Bingna Xu", "hidden": false}, {"_id": "6942650d5d5b2dc1052749da", "name": "Guangyao Liu", "hidden": false}, {"_id": "6942650d5d5b2dc1052749db", "name": "Long Huang", "hidden": false}, {"_id": "6942650d5d5b2dc1052749dc", "name": "Kaibin Guo", "hidden": false}, {"_id": "6942650d5d5b2dc1052749dd", "name": "Jinyang Wu", "hidden": false}, {"_id": "6942650d5d5b2dc1052749de", "name": "Dan Wu", "hidden": false}, {"_id": "6942650d5d5b2dc1052749df", "name": "Hongzhen Wang", "hidden": false}, {"_id": "6942650d5d5b2dc1052749e0", "name": "Peng Zhou", "hidden": false}, {"_id": "6942650d5d5b2dc1052749e1", "name": "Shuai Nie", "hidden": false}, {"_id": "6942650d5d5b2dc1052749e2", "name": "Shande Wang", "hidden": false}, {"_id": "6942650d5d5b2dc1052749e3", "name": "Runyu Shi", "hidden": false}, {"_id": "6942650d5d5b2dc1052749e4", "name": "Ying Huang", "hidden": false}], "publishedAt": "2025-12-16T03:36:41.000Z", "submittedOnDailyAt": "2025-12-18T05:34:36.317Z", "title": "HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices", "submittedOnDailyBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "summary": "Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference.", "upvotes": 30, "discussionId": "6942650e5d5b2dc1052749e5", "ai_summary": "HyperVL, an efficient multimodal large language model for on-device inference, uses image tiling, Visual Resolution Compressor, and Dual Consistency Learning to reduce memory usage, latency, and power consumption while maintaining performance.", "ai_keywords": ["multimodal large language models", "Vision Transformer (ViT)", "image-tiling strategy", "Visual Resolution Compressor", "Dual Consistency Learning", "on-device inference", "latency", "power consumption", "benchmarks"], "summary_zh": "<ul>\n    <li>HyperVL\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u9002\u5408\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u4f7f\u7528\u3002</li>\n    <li>\u5b83\u91c7\u7528\u56fe\u50cf\u5206\u5757\u7b56\u7565\uff0c\u964d\u4f4e\u4e86\u5185\u5b58\u4f7f\u7528\u5cf0\u503c\u3002</li>\n    <li>\u5f15\u5165\u4e86\u89c6\u89c9\u5206\u8fa8\u7387\u538b\u7f29\u5668\uff08VRC\uff09\uff0c\u4f18\u5316\u7f16\u7801\u5206\u8fa8\u7387\uff0c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u3002</li>\n    <li>\u4f7f\u7528\u53cc\u91cd\u4e00\u81f4\u6027\u5b66\u4e60\uff08DCL\uff09\uff0c\u5728\u7edf\u4e00\u6846\u67b6\u5185\u5bf9\u591a\u5c3a\u5ea6ViT\u7f16\u7801\u5668\u8fdb\u884c\u5bf9\u9f50\u3002</li>\n    <li>\u7ecf\u8fc7\u6d4b\u8bd5\uff0cHyperVL\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u79fb\u52a8\u8bbe\u5907\u7684\u5ef6\u8fdf\u548c\u529f\u8017\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>HyperVL is a new model designed to work well on mobile devices, despite the challenges of high memory and computation needs in existing models.</li>\n    <li>It uses an image-tiling method to manage memory use and two innovative techniques: a Visual Resolution Compressor to optimize processing, and Dual Consistency Learning to improve efficiency across different visual scales.</li>\n    <li>HyperVL has been shown to perform better than similar-sized models on various tests.</li>\n    <li>The model significantly cuts down on delay and energy use when running on actual mobile devices, making it practical for real-world applications.</li>\n</ul>"}, "publishedAt": "2025-12-15T22:36:41.000Z", "title": "HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices", "summary": "Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14052.png", "numComments": 2, "submittedBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "fullname": "Jinyang Wu", "name": "Jinyang23", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "isAuthorParticipating": false}, {"paper": {"id": "2512.14944", "authors": [{"_id": "6943feea47055eb55aade03e", "name": "Ahmadreza Jeddi", "hidden": false}, {"_id": "6943feea47055eb55aade03f", "name": "Hakki Can Karaimer", "hidden": false}, {"_id": "6943feea47055eb55aade040", "name": "Hue Nguyen", "hidden": false}, {"_id": "6943feea47055eb55aade041", "name": "Zhongling Wang", "hidden": false}, {"_id": "6943feea47055eb55aade042", "name": "Ke Zhao", "hidden": false}, {"_id": "6943feea47055eb55aade043", "name": "Javad Rajabi", "hidden": false}, {"_id": "6943feea47055eb55aade044", "name": "Ran Zhang", "hidden": false}, {"_id": "6943feea47055eb55aade045", "name": "Raghav Goyal", "hidden": false}, {"_id": "6943feea47055eb55aade046", "name": "Babak Taati", "hidden": false}, {"_id": "6943feea47055eb55aade047", "name": "Radek Grzeszczuk", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/Lu0YITas0nGkl6AvQfh8U.png", "https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/8lUn87mKrq_eRbQytBIj3.png", "https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/U9XN-imAoO1qiYvlUX1VM.png", "https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/zrAdLC21nkwWubq6s-6JM.png"], "publishedAt": "2025-12-16T22:17:25.000Z", "submittedOnDailyAt": "2025-12-18T13:40:40.167Z", "title": "Puzzle Curriculum GRPO for Vision-Centric Reasoning", "submittedOnDailyBy": {"_id": "65e8df170cda621164769f6f", "avatarUrl": "/avatars/b5d7c7c49d1a777a1bd41d06d299868a.svg", "isPro": false, "fullname": "Armen Jeddi", "user": "armenjeddi", "type": "user"}, "summary": "Recent reinforcement learning (RL) approaches like outcome-supervised GRPO have advanced chain-of-thought reasoning in Vision Language Models (VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce a difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RL post-training for VLMs.", "upvotes": 29, "discussionId": "6943feea47055eb55aade048", "projectPage": "https://pcgrpo.github.io/", "ai_summary": "Puzzle Curriculum GRPO enhances visual reasoning in Vision Language Models through self-supervised environments and a difficulty-aware curriculum, improving consistency and accuracy without external annotations.", "ai_keywords": ["reinforcement learning", "outcome-supervised GRPO", "chain-of-thought reasoning", "Vision Language Models", "self-supervised", "PatchFit", "Rotation", "Jigsaw", "graded partial credit", "reward sparsity", "difficulty-aware curriculum", "Reasoning-Answer Consistency", "LLMs", "Qwen-7B", "Qwen-3B", "verifiable rewards", "post-training"], "organization": {"_id": "686df54910a52f2c2cf03c06", "name": "SamsungResearch", "fullname": "Samsung Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPuzzle Curriculum GRPO (PC-GRPO)\u7684\u65b0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u589e\u5f3a\u89c6\u89c9\u63a8\u7406\uff0c\u65e0\u9700\u624b\u52a8\u6807\u6ce8\u6216\u5916\u90e8\u9a8c\u8bc1\u3002</li>\n    <li>PC-GRPO\u4f7f\u7528\u4e09\u79cd\u81ea\u6211\u76d1\u7763\u7684\u62fc\u56fe\u73af\u5883\u66ff\u4ee3\u4f20\u7edf\u7684\u6807\u7b7e\uff0c\u4ee5\u89e3\u51b3\u5956\u52b1\u7a00\u758f\u548c\u903b\u8f91\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002</li>\n    <li>\u5f15\u5165\u4e86\u4e00\u4e2a\u52a8\u6001\u6743\u91cd\u7684\u56f0\u96be\u611f\u77e5\u8bfe\u7a0b\uff0c\u4ee5\u63d0\u5347\u6837\u672c\u7684\u6709\u6548\u6027\u5e76\u89e3\u51b3\u5e73\u5766\u5956\u52b1\u7684\u95ee\u9898\u3002</li>\n    <li>\u901a\u8fc7\u76d1\u63a7\u63a8\u7406\u4e0e\u7b54\u6848\u7684\u4e00\u81f4\u6027\uff0c\u5728\u8bad\u7ec3\u540e\u671f\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\uff0c\u63d0\u9ad8\u4e86\u6700\u7ec8\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPC-GRPO\u663e\u8457\u6539\u5584\u4e86\u63a8\u7406\u8d28\u91cf\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u4e3a\u53ef\u6269\u5c55\u548c\u53ef\u9a8c\u8bc1\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u5207\u5b9e\u53ef\u884c\u7684\u8def\u5f84\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New RL method called Puzzle Curriculum GRPO (PC-GRPO) helps improve reasoning in Vision Language Models without needing costly annotations.</li>\n    <li>PC-GRPO uses self-supervised puzzle tasks instead of labels to train models.</li>\n    <li>A dynamic curriculum adjusts the difficulty of tasks to improve learning and reduce inconsistencies in answers.</li>\n    <li>The method helps maintain reasoning consistency over time, leading to better overall accuracy in tasks.</li>\n    <li>PC-GRPO shows improved performance on various benchmarks, making it a useful approach for enhancing RL in VLMs.</li>\n</ul>"}, "publishedAt": "2025-12-16T17:17:25.000Z", "title": "Puzzle Curriculum GRPO for Vision-Centric Reasoning", "summary": "Recent reinforcement learning (RL) approaches like outcome-supervised GRPO have advanced chain-of-thought reasoning in Vision Language Models (VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce a difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RL post-training for VLMs.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/Lu0YITas0nGkl6AvQfh8U.png", "https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/8lUn87mKrq_eRbQytBIj3.png", "https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/U9XN-imAoO1qiYvlUX1VM.png", "https://cdn-uploads.huggingface.co/production/uploads/65e8df170cda621164769f6f/zrAdLC21nkwWubq6s-6JM.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14944.png", "numComments": 2, "submittedBy": {"_id": "65e8df170cda621164769f6f", "avatarUrl": "/avatars/b5d7c7c49d1a777a1bd41d06d299868a.svg", "fullname": "Armen Jeddi", "name": "armenjeddi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "686df54910a52f2c2cf03c06", "name": "SamsungResearch", "fullname": "Samsung Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60ffc3e62403168abcae811d/lBrkzrpjrJ8k-3CGLKRLr.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.14693", "authors": [{"_id": "6942c644fb33037a39577c15", "user": {"_id": "641ddac5be3bd3a5a06ed4a4", "avatarUrl": "/avatars/14969dff861d53b0a75305606495eca7.svg", "isPro": false, "fullname": "Zitian Gao", "user": "zgao3186", "type": "user"}, "name": "Zitian Gao", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T08:20:55.038Z", "hidden": false}, {"_id": "6942c644fb33037a39577c16", "name": "Lynx Chen", "hidden": false}, {"_id": "6942c644fb33037a39577c17", "name": "Yihao Xiao", "hidden": false}, {"_id": "6942c644fb33037a39577c18", "name": "He Xing", "hidden": false}, {"_id": "6942c644fb33037a39577c19", "name": "Ran Tao", "hidden": false}, {"_id": "6942c644fb33037a39577c1a", "name": "Haoming Luo", "hidden": false}, {"_id": "6942c644fb33037a39577c1b", "name": "Joey Zhou", "hidden": false}, {"_id": "6942c644fb33037a39577c1c", "name": "Bryan Dai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/641ddac5be3bd3a5a06ed4a4/0uPUEtb7mYGoZ0RDxJUnI.png"], "publishedAt": "2025-12-16T18:58:45.000Z", "submittedOnDailyAt": "2025-12-18T00:22:44.947Z", "title": "Universal Reasoning Model", "submittedOnDailyBy": {"_id": "641ddac5be3bd3a5a06ed4a4", "avatarUrl": "/avatars/14969dff861d53b0a75305606495eca7.svg", "isPro": false, "fullname": "Zitian Gao", "user": "zgao3186", "type": "user"}, "summary": "Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.", "upvotes": 19, "discussionId": "6942c644fb33037a39577c1d", "githubRepo": "https://github.com/zitian-gao/URM", "githubRepoAddedBy": "user", "ai_summary": "The Universal Reasoning Model enhances Universal Transformers with short convolution and truncated backpropagation to improve reasoning performance on ARC-AGI tasks.", "ai_keywords": ["Universal Transformers", "ARC-AGI", "recurrent inductive bias", "nonlinear components", "truncated backpropagation", "Universal Reasoning Model"], "githubStars": 23, "organization": {"_id": "68c8248dda0e62cc830e7e49", "name": "UbiquantAI", "fullname": "Ubiquant", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68c80cc3a1ca9a73c17d29f7/AmD26BH3h3lGxFN_SgWra.jpeg"}, "summary_zh": "<ul>\n    <li>\u901a\u7528\u53d8\u6362\u5668\uff08UT\uff09\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f8b\u5982ARC-AGI\u548c\u6570\u72ec\u3002</li>\n    <li>UT\u7684\u6027\u80fd\u63d0\u5347\u4e3b\u8981\u6765\u6e90\u4e8e\u5176\u9012\u5f52\u7684\u5f52\u7eb3\u504f\u5dee\u548c\u5f3a\u975e\u7ebf\u6027\u7ec4\u4ef6\uff0c\u800c\u975e\u590d\u6742\u7684\u67b6\u6784\u8bbe\u8ba1\u3002</li>\n    <li>\u57fa\u4e8e\u8fd9\u4e00\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u901a\u7528\u63a8\u7406\u6a21\u578b\uff08URM\uff09\uff0c\u901a\u8fc7\u77ed\u5377\u79ef\u548c\u622a\u65ad\u53cd\u5411\u4f20\u64ad\u6765\u589e\u5f3aUT\u3002</li>\n    <li>\u6211\u4eec\u7684\u6a21\u578b\u5728ARC-AGI 1\u4e0a\u8fbe\u523053.8%\u7684\u901a\u8fc7\u7387\uff0c\u5728ARC-AGI 2\u4e0a\u8fbe\u523016.0%\u7684\u901a\u8fc7\u7387\uff0c\u8868\u73b0\u4f18\u5f02\u3002</li>\n    <li>\u6211\u4eec\u7684\u4ee3\u7801\u53ef\u5728https://github.com/zitian-gao/URM\u83b7\u53d6\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Universal transformers (UTs) are effective for complex reasoning tasks like ARC-AGI and Sudoku.</li>\n  <li>The performance improvements in UTs come mainly from their unique structure, not just advanced designs.</li>\n  <li>We introduce the Universal Reasoning Model (URM), which adds short convolution and truncated backpropagation to UTs.</li>\n  <li>Our method significantly boosts reasoning performance, achieving top scores on ARC-AGI 1 and 2.</li>\n  <li>The code for our approach is available online at GitHub.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:45.000Z", "title": "Universal Reasoning Model", "summary": "Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/641ddac5be3bd3a5a06ed4a4/0uPUEtb7mYGoZ0RDxJUnI.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14693.png", "numComments": 3, "submittedBy": {"_id": "641ddac5be3bd3a5a06ed4a4", "avatarUrl": "/avatars/14969dff861d53b0a75305606495eca7.svg", "fullname": "Zitian Gao", "name": "zgao3186", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "68c8248dda0e62cc830e7e49", "name": "UbiquantAI", "fullname": "Ubiquant", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68c80cc3a1ca9a73c17d29f7/AmD26BH3h3lGxFN_SgWra.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.15635", "authors": [{"_id": "6943902a542d62d58a7bf7a0", "user": {"_id": "64aa2210e04e7f92245f54d2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa2210e04e7f92245f54d2/OE43T22bLWBVgmqcJyUtu.png", "isPro": false, "fullname": "Li", "user": "kotion", "type": "user"}, "name": "Yuanhang Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:34.872Z", "hidden": false}, {"_id": "6943902a542d62d58a7bf7a1", "name": "Yiren Song", "hidden": false}, {"_id": "6943902a542d62d58a7bf7a2", "name": "Junzhe Bai", "hidden": false}, {"_id": "6943902a542d62d58a7bf7a3", "name": "Xinran Liang", "hidden": false}, {"_id": "6943902a542d62d58a7bf7a4", "name": "Hu Yang", "hidden": false}, {"_id": "6943902a542d62d58a7bf7a5", "name": "Libiao Jin", "hidden": false}, {"_id": "6943902a542d62d58a7bf7a6", "user": {"_id": "6388a7e98a5dbe2f3dc61faa", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6388a7e98a5dbe2f3dc61faa/Zj8pR7BG_wOQjsYzosb5p.jpeg", "isPro": false, "fullname": "Qi Mao", "user": "HelenMao", "type": "user"}, "name": "Qi Mao", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:37.087Z", "hidden": false}], "publishedAt": "2025-12-17T17:47:18.000Z", "submittedOnDailyAt": "2025-12-18T03:01:26.657Z", "title": "IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning", "submittedOnDailyBy": {"_id": "6388a7e98a5dbe2f3dc61faa", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6388a7e98a5dbe2f3dc61faa/Zj8pR7BG_wOQjsYzosb5p.jpeg", "isPro": false, "fullname": "Qi Mao", "user": "HelenMao", "type": "user"}, "summary": "We propose IC-Effect, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning 15 high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.", "upvotes": 17, "discussionId": "6943902a542d62d58a7bf7a7", "projectPage": "https://cuc-mipg.github.io/IC-Effect/", "githubRepo": "https://github.com/CUC-MIPG/IC-Effect", "githubRepoAddedBy": "user", "ai_summary": "IC-Effect, an instruction-guided DiT-based framework, synthesizes complex video VFX effects while preserving spatial and temporal consistency using a two-stage training strategy and spatiotemporal sparse tokenization.", "ai_keywords": ["DiT", "few-shot video VFX editing", "flames", "particles", "cartoon characters", "spatial consistency", "temporal consistency", "DiT models", "contextual learning", "general editing adaptation", "Effect-LoRA", "spatiotemporal sparse tokenization", "VFX editing dataset"], "githubStars": 25, "organization": {"_id": "67dab498ed21a53369f5de73", "name": "CUC-MIPG", "fullname": "Multimedia Intelligent Processing Group in Communication University of China", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640d704c8036cc2142299c19/B85B31gd7-0kjK_Rpvv3g.jpeg"}, "summary_zh": "<ul>\n    <li>IC-Effect\u662f\u4e00\u4e2a\u57fa\u4e8e\u6307\u4ee4\u5f15\u5bfc\u7684\u6846\u67b6\uff0c\u65e8\u5728\u8fdb\u884c\u5c11\u91cf\u6837\u672c\u7684\u89c6\u9891\u7279\u6548\u7f16\u8f91\u3002</li>\n    <li>\u8be5\u6846\u67b6\u80fd\u591f\u5408\u6210\u590d\u6742\u7684\u7279\u6548\uff08\u5982\u706b\u7130\u3001\u7c92\u5b50\u548c\u5361\u901a\u89d2\u8272\uff09\uff0c\u5e76\u4fdd\u6301\u7a7a\u95f4\u548c\u65f6\u95f4\u7684\u4e00\u81f4\u6027\u3002</li>\n    <li>IC-Effect\u5229\u7528\u6e90\u89c6\u9891\u4f5c\u4e3a\u6e05\u6670\u7684\u4e0a\u4e0b\u6587\u6761\u4ef6\uff0c\u786e\u4fdd\u80cc\u666f\u4e0d\u53d8\u5e76\u81ea\u7136\u5730\u6ce8\u5165\u7279\u6548\u3002</li>\n    <li>\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5148\u8fdb\u884c\u4e00\u822c\u7f16\u8f91\u9002\u5e94\uff0c\u518d\u901a\u8fc7Effect-LoRA\u8fdb\u884c\u7279\u6548\u7279\u5b9a\u5b66\u4e60\u3002</li>\n    <li>\u6a21\u578b\u7ed3\u5408\u65f6\u7a7a\u7a00\u758f\u6807\u8bb0\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u53d1\u5e03\u4e86\u6db5\u76d615\u79cd\u9ad8\u8d28\u91cf\u89c6\u89c9\u98ce\u683c\u7684\u914d\u5bf9\u7279\u6548\u7f16\u8f91\u6570\u636e\u96c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>IC-Effect is a new framework for editing video effects, allowing users to add complex visuals like flames and cartoon characters while keeping the background unchanged.</li>\n    <li>It learns from limited data and ensures effects blend well with the video context, maintaining both spatial and temporal consistency.</li>\n    <li>The framework uses a two-step training approach to improve its performance in editing and effect modeling.</li>\n    <li>It incorporates a technique called spatiotemporal sparse tokenization to enhance efficiency and reduce computation while maintaining high quality.</li>\n    <li>A new dataset of paired VFX editing styles was created to support the training and testing of this framework.</li>\n</ul>"}, "publishedAt": "2025-12-17T12:47:18.000Z", "title": "IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning", "summary": "We propose IC-Effect, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning 15 high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15635.png", "numComments": 2, "submittedBy": {"_id": "6388a7e98a5dbe2f3dc61faa", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6388a7e98a5dbe2f3dc61faa/Zj8pR7BG_wOQjsYzosb5p.jpeg", "fullname": "Qi Mao", "name": "HelenMao", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "67dab498ed21a53369f5de73", "name": "CUC-MIPG", "fullname": "Multimedia Intelligent Processing Group in Communication University of China", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640d704c8036cc2142299c19/B85B31gd7-0kjK_Rpvv3g.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.15693", "authors": [{"_id": "69436a60542d62d58a7bf65b", "user": {"_id": "64adfeac4beffa272dfaef21", "avatarUrl": "/avatars/883f6ba38b993476115dfafcef9ce3c1.svg", "isPro": false, "fullname": "Yifei Li", "user": "JoeLeelyf", "type": "user"}, "name": "Yifei Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T08:00:01.339Z", "hidden": false}, {"_id": "69436a60542d62d58a7bf65c", "name": "Wenzhao Zheng", "hidden": false}, {"_id": "69436a60542d62d58a7bf65d", "name": "Yanran Zhang", "hidden": false}, {"_id": "69436a60542d62d58a7bf65e", "name": "Runze Sun", "hidden": false}, {"_id": "69436a60542d62d58a7bf65f", "name": "Yu Zheng", "hidden": false}, {"_id": "69436a60542d62d58a7bf660", "name": "Lei Chen", "hidden": false}, {"_id": "69436a60542d62d58a7bf661", "name": "Jie Zhou", "hidden": false}, {"_id": "69436a60542d62d58a7bf662", "name": "Jiwen Lu", "hidden": false}], "publishedAt": "2025-12-17T18:48:26.000Z", "submittedOnDailyAt": "2025-12-18T00:31:01.463Z", "title": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning", "submittedOnDailyBy": {"_id": "64adfeac4beffa272dfaef21", "avatarUrl": "/avatars/883f6ba38b993476115dfafcef9ce3c1.svg", "isPro": false, "fullname": "Yifei Li", "user": "JoeLeelyf", "type": "user"}, "summary": "The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.", "upvotes": 16, "discussionId": "69436a60542d62d58a7bf663", "projectPage": "https://joeleelyf.github.io/Skyra/", "githubRepo": "https://github.com/JoeLeelyf/Skyra", "githubRepoAddedBy": "user", "ai_summary": "Skyra, a specialized multimodal large language model, detects and explains visual artifacts in AI-generated videos using a novel dataset and two-stage training strategy, outperforming existing methods.", "ai_keywords": ["multimodal large language model", "AI-generated video detectors", "visual artifacts", "ViF-CoT-4K", "supervised fine-tuning", "spatio-temporal artifact perception", "explanation capability", "detection accuracy", "ViF-Bench", "explainable AI-generated video detection"], "githubStars": 19, "organization": {"_id": "693649ff6df58c411109e13e", "name": "Tsinghua-IVG", "fullname": "Tsinghua-IVG", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/661cfae9a853782abad2a495/dYWQZAl7ZOpDET4PRQyDD.png"}, "summary_zh": "<ul>\n    <li>AI\u89c6\u9891\u751f\u6210\u6280\u672f\u7684\u6ee5\u7528\u5f15\u53d1\u4e86\u793e\u4f1a\u5173\u6ce8\uff0c\u9700\u8981\u53ef\u9760\u7684AI\u89c6\u9891\u68c0\u6d4b\u5668\u3002</li>\n    <li>\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u4ec5\u9650\u4e8e\u4e8c\u5206\u7c7b\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u89e3\u91ca\u3002</li>\n    <li>\u672c\u6587\u4ecb\u7ecd\u4e86Skyra\uff0c\u8fd9\u662f\u4e00\u79cd\u80fd\u591f\u8bc6\u522bAI\u751f\u6210\u89c6\u9891\u4e2d\u7684\u53ef\u89c1\u89c6\u89c9\u4f2a\u5f71\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86ViF-CoT-4K\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u7684AI\u89c6\u9891\u4f2a\u5f71\u6570\u636e\u96c6\uff0c\u5305\u542b\u8be6\u7ec6\u7684\u4eba\u7c7b\u6ce8\u91ca\u3002</li>\n    <li>Skyra\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9\u53ef\u89e3\u91caAI\u89c6\u9891\u68c0\u6d4b\u7684\u91cd\u8981\u89c1\u89e3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>AI video generation technology is being misused, creating a need for better detection tools.</li>\n    <li>Current detection methods often only classify videos as real or fake and don\u2019t provide clear explanations.</li>\n    <li>This paper introduces Skyra, a new model that finds visual mistakes in AI-generated videos and explains them.</li>\n    <li>To improve Skyra, a new dataset with detailed human annotations was created for training.</li>\n    <li>Skyra has been tested against other methods and shows better performance, helping to improve explainable AI detection.</li>\n</ul>"}, "publishedAt": "2025-12-17T13:48:26.000Z", "title": "Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning", "summary": "The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15693.png", "numComments": 2, "submittedBy": {"_id": "64adfeac4beffa272dfaef21", "avatarUrl": "/avatars/883f6ba38b993476115dfafcef9ce3c1.svg", "fullname": "Yifei Li", "name": "JoeLeelyf", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "693649ff6df58c411109e13e", "name": "Tsinghua-IVG", "fullname": "Tsinghua-IVG", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/661cfae9a853782abad2a495/dYWQZAl7ZOpDET4PRQyDD.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.15603", "authors": [{"_id": "694373cc542d62d58a7bf6b4", "name": "Shengming Yin", "hidden": false}, {"_id": "694373cc542d62d58a7bf6b5", "name": "Zekai Zhang", "hidden": false}, {"_id": "694373cc542d62d58a7bf6b6", "name": "Zecheng Tang", "hidden": false}, {"_id": "694373cc542d62d58a7bf6b7", "name": "Kaiyuan Gao", "hidden": false}, {"_id": "694373cc542d62d58a7bf6b8", "name": "Xiao Xu", "hidden": false}, {"_id": "694373cc542d62d58a7bf6b9", "name": "Kun Yan", "hidden": false}, {"_id": "694373cc542d62d58a7bf6ba", "name": "Jiahao Li", "hidden": false}, {"_id": "694373cc542d62d58a7bf6bb", "name": "Yilei Chen", "hidden": false}, {"_id": "694373cc542d62d58a7bf6bc", "name": "Yuxiang Chen", "hidden": false}, {"_id": "694373cc542d62d58a7bf6bd", "name": "Heung-Yeung Shum", "hidden": false}, {"_id": "694373cc542d62d58a7bf6be", "name": "Lionel M. Ni", "hidden": false}, {"_id": "694373cc542d62d58a7bf6bf", "name": "Jingren Zhou", "hidden": false}, {"_id": "694373cc542d62d58a7bf6c0", "name": "Junyang Lin", "hidden": false}, {"_id": "694373cc542d62d58a7bf6c1", "name": "Chenfei Wu", "hidden": false}], "publishedAt": "2025-12-17T17:12:42.000Z", "submittedOnDailyAt": "2025-12-18T00:54:15.231Z", "title": "Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose Qwen-Image-Layered, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling inherent editability, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing. Our code and models are released on https://github.com/QwenLM/Qwen-Image-Layered{https://github.com/QwenLM/Qwen-Image-Layered}", "upvotes": 16, "discussionId": "694373cc542d62d58a7bf6c2", "ai_summary": "Qwen-Image-Layered decomposes images into semantically disentangled RGBA layers using a diffusion model, enabling independent editing of each layer and improving decomposition quality and consistency.", "ai_keywords": ["diffusion model", "RGBA layers", "inherent editability", "RGBA-VAE", "VLD-MMDiT", "Multi-stage Training", "PSD", "decomposition quality", "consistent image editing"], "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u89c6\u89c9\u751f\u6210\u6a21\u578b\u5728\u56fe\u50cf\u7f16\u8f91\u65f6\u5e38\u5e38\u7f3a\u4e4f\u4e00\u81f4\u6027\uff0c\u56e0\u4e3a\u6240\u6709\u89c6\u89c9\u5185\u5bb9\u90fd\u878d\u5408\u5728\u4e00\u4e2a\u753b\u5e03\u4e0a\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Qwen-Image-Layered\uff0c\u8fd9\u662f\u4e00\u79cd\u5c06\u5355\u4e2aRGB\u56fe\u50cf\u5206\u89e3\u4e3a\u591a\u4e2a\u72ec\u7acb\u7684RGBA\u5c42\u7684\u6269\u6563\u6a21\u578b\u3002</li>\n    <li>\u6bcf\u4e2aRGBA\u5c42\u53ef\u4ee5\u72ec\u7acb\u7f16\u8f91\uff0c\u800c\u4e0d\u5f71\u54cd\u5176\u4ed6\u5185\u5bb9\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u53ef\u7f16\u8f91\u6027\u3002</li>\n    <li>\u4e3a\u652f\u6301\u53ef\u53d8\u957f\u5ea6\u5206\u89e3\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff0c\u5305\u62ecRGVA-VAE\u548cVLD-MMDiT\u67b6\u6784\u3002</li>\n    <li>\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5206\u89e3\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e3a\u4e00\u81f4\u7684\u56fe\u50cf\u7f16\u8f91\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Traditional image editing can be inconsistent because all parts of an image are mixed together.</li>\n    <li>Qwen-Image-Layered is a new model that breaks down images into separate layers, making it easier to edit without affecting other parts.</li>\n    <li>The model uses three main components: an RGBA-VAE for organizing image data, a special architecture for handling different numbers of layers, and a training strategy to improve its performance.</li>\n    <li>To train the model effectively, a system was created to extract and label multilayer images from Photoshop files.</li>\n    <li>Tests show that this new method works much better than previous techniques for image decomposition and editing.</li>\n</ul>"}, "publishedAt": "2025-12-17T12:12:42.000Z", "title": "Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition", "summary": "Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose Qwen-Image-Layered, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling inherent editability, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing. Our code and models are released on https://github.com/QwenLM/Qwen-Image-Layered{https://github.com/QwenLM/Qwen-Image-Layered}", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15603.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.15182", "authors": [{"_id": "69437830542d62d58a7bf752", "user": {"_id": "62676a94dacab364889bb36c", "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg", "isPro": false, "fullname": "SARIM HASHMI", "user": "Sarim-Hash", "type": "user"}, "name": "Sarim Hashmi", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:45.380Z", "hidden": false}, {"_id": "69437830542d62d58a7bf753", "name": "Abdelrahman Elsayed", "hidden": false}, {"_id": "69437830542d62d58a7bf754", "name": "Mohammed Talha Alam", "hidden": false}, {"_id": "69437830542d62d58a7bf755", "name": "Samuele Poppi", "hidden": false}, {"_id": "69437830542d62d58a7bf756", "name": "Nils Lukas", "hidden": false}], "publishedAt": "2025-12-17T08:31:40.000Z", "submittedOnDailyAt": "2025-12-18T01:20:18.954Z", "title": "Robust and Calibrated Detection of Authentic Multimedia Content", "submittedOnDailyBy": {"_id": "62676a94dacab364889bb36c", "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg", "isPro": false, "fullname": "SARIM HASHMI", "user": "Sarim-Hash", "type": "user"}, "summary": "Generative models can synthesize highly realistic content, so-called deepfakes, that are already being misused at scale to undermine digital media authenticity. Current deepfake detection methods are unreliable for two reasons: (i) distinguishing inauthentic content post-hoc is often impossible (e.g., with memorized samples), leading to an unbounded false positive rate (FPR); and (ii) detection lacks robustness, as adversaries can adapt to known detectors with near-perfect accuracy using minimal computational resources. To address these limitations, we propose a resynthesis framework to determine if a sample is authentic or if its authenticity can be plausibly denied. We make two key contributions focusing on the high-precision, low-recall setting against efficient (i.e., compute-restricted) adversaries. First, we demonstrate that our calibrated resynthesis method is the most reliable approach for verifying authentic samples while maintaining controllable, low FPRs. Second, we show that our method achieves adversarial robustness against efficient adversaries, whereas prior methods are easily evaded under identical compute budgets. Our approach supports multiple modalities and leverages state-of-the-art inversion techniques.", "upvotes": 15, "discussionId": "69437830542d62d58a7bf757", "ai_summary": "A resynthesis framework enhances deepfake detection by verifying authenticity with low false positive rates and robustness against efficient adversaries, supporting multiple modalities.", "ai_keywords": ["deepfakes", "deepfake detection", "resynthesis framework", "false positive rate", "adversarial robustness", "inversion techniques"], "organization": {"_id": "61fb9e24dc607a42af5f193f", "name": "MBZUAI", "fullname": "Mohamed Bin Zayed University of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"}, "summary_zh": "<ul>\n    <li>\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u5408\u6210\u975e\u5e38\u771f\u5b9e\u7684\u5185\u5bb9\uff08\u6df1\u5ea6\u4f2a\u9020\uff09\uff0c\u8fd9\u5df2\u88ab\u5e7f\u6cdb\u6ee5\u7528\uff0c\u5f71\u54cd\u6570\u5b57\u5a92\u4f53\u7684\u771f\u5b9e\u6027\u3002</li>\n    <li>\u76ee\u524d\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u4e0d\u53ef\u9760\uff0c\u4e3b\u8981\u56e0\u4e3a\u65e0\u6cd5\u53ef\u9760\u533a\u5206\u4f2a\u9020\u5185\u5bb9\uff0c\u5e76\u4e14\u5bf9\u5df2\u77e5\u68c0\u6d4b\u5668\u7684\u5bf9\u6297\u6027\u5f88\u5f3a\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u91cd\u65b0\u5408\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u5224\u65ad\u6837\u672c\u662f\u5426\u771f\u5b9e\u6216\u662f\u5426\u53ef\u4ee5\u5408\u7406\u5730\u5426\u8ba4\u5176\u771f\u5b9e\u6027\u3002</li>\n    <li>\u6211\u4eec\u7684\u91cd\u65b0\u5408\u6210\u65b9\u6cd5\u5728\u9a8c\u8bc1\u771f\u5b9e\u6837\u672c\u65b9\u9762\u6700\u4e3a\u53ef\u9760\uff0c\u5e76\u80fd\u4fdd\u6301\u4f4e\u8bef\u62a5\u7387\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5bf9\u9ad8\u6548\u7684\u5bf9\u624b\u5177\u6709\u6297\u5bf9\u6297\u6027\uff0c\u800c\u4e4b\u524d\u7684\u65b9\u6cd5\u5728\u76f8\u540c\u7684\u8ba1\u7b97\u9884\u7b97\u4e0b\u5bb9\u6613\u88ab\u89c4\u907f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Generative models can create realistic fake content (deepfakes), which can harm the trustworthiness of digital media.</li>\n    <li>Current methods for detecting deepfakes are often unreliable due to high false positive rates and vulnerability to adversaries who can adapt to detection techniques.</li>\n    <li>This study introduces a new resynthesis framework to better determine if content is authentic or fake.</li>\n    <li>The proposed method is highly reliable for verifying real samples while keeping false positives low.</li>\n    <li>It also offers stronger protection against adversaries compared to existing detection methods.</li>\n</ul>"}, "publishedAt": "2025-12-17T03:31:40.000Z", "title": "Robust and Calibrated Detection of Authentic Multimedia Content", "summary": "Generative models can synthesize highly realistic content, so-called deepfakes, that are already being misused at scale to undermine digital media authenticity. Current deepfake detection methods are unreliable for two reasons: (i) distinguishing inauthentic content post-hoc is often impossible (e.g., with memorized samples), leading to an unbounded false positive rate (FPR); and (ii) detection lacks robustness, as adversaries can adapt to known detectors with near-perfect accuracy using minimal computational resources. To address these limitations, we propose a resynthesis framework to determine if a sample is authentic or if its authenticity can be plausibly denied. We make two key contributions focusing on the high-precision, low-recall setting against efficient (i.e., compute-restricted) adversaries. First, we demonstrate that our calibrated resynthesis method is the most reliable approach for verifying authentic samples while maintaining controllable, low FPRs. Second, we show that our method achieves adversarial robustness against efficient adversaries, whereas prior methods are easily evaded under identical compute budgets. Our approach supports multiple modalities and leverages state-of-the-art inversion techniques.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15182.png", "numComments": 2, "submittedBy": {"_id": "62676a94dacab364889bb36c", "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg", "fullname": "SARIM HASHMI", "name": "Sarim-Hash", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "61fb9e24dc607a42af5f193f", "name": "MBZUAI", "fullname": "Mohamed Bin Zayed University of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643879908583-603ab5664a944b99e81476e8.jpeg"}, "isAuthorParticipating": true}],
    "week": [{"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u57fa\u7840\u6a21\u578b\u751f\u6210\u89c6\u89c9\u771f\u5b9e\u548c\u65f6\u95f4\u4e00\u81f4\u7684\u5185\u5bb9\uff0c\u4f46\u5b83\u4eec\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u662f\u5426\u6355\u6349\u5230\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7ea6\u675f\u3002</li>\n    <li>\u73b0\u6709\u6307\u6807\u5982Frechet\u89c6\u9891\u8ddd\u79bb\uff08FVD\uff09\u5173\u6ce8\u611f\u77e5\u8d28\u91cf\uff0c\u5ffd\u89c6\u4e86\u56e0\u679c\u5173\u7cfb\u3001\u7269\u7406\u548c\u5168\u5c40\u4e00\u81f4\u6027\u7684\u63a8\u7406\u5931\u8d25\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86MMGR\uff08\u591a\u6a21\u6001\u751f\u6210\u63a8\u7406\u8bc4\u4f30\u4e0e\u57fa\u51c6\uff09\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8bc4\u4f30\uff1a\u7269\u7406\u3001\u903b\u8f91\u3001\u4e09\u7ef4\u7a7a\u95f4\u3001\u4e8c\u7ef4\u7a7a\u95f4\u548c\u65f6\u95f4\u3002</li>\n    <li>MMGR\u5728\u4e09\u4e2a\u9886\u57df\u8fdb\u884c\u8bc4\u4f30\uff1a\u62bd\u8c61\u63a8\u7406\u3001\u5177\u8eab\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\uff0c\u4f7f\u7528\u7ec6\u81f4\u7684\u6307\u6807\u8bc4\u4f30\u89c6\u9891\u548c\u56fe\u50cf\u751f\u6210\u7684\u6574\u4f53\u6b63\u786e\u6027\u3002</li>\n    <li>\u6211\u4eec\u7684\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5305\u62ec\u8fc7\u5ea6\u4f9d\u8d56\u611f\u77e5\u6570\u636e\u3001\u5168\u5c40\u72b6\u6001\u4e00\u81f4\u6027\u5dee\uff0c\u4ee5\u53ca\u5956\u52b1\u89c6\u89c9\u53ef\u4fe1\u6027\u800c\u975e\u56e0\u679c\u6b63\u786e\u6027\u7684\u76ee\u6807\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video foundation models can create realistic content, but they may not always follow real-world rules and logic.</li>\n    <li>Current evaluation methods focus on how things look rather than if they make sense or follow logical reasoning.</li>\n    <li>MMGR is a new evaluation framework that checks five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>\n    <li>It tests models on different tasks like Abstract Reasoning, real-world navigation, and everyday interactions.</li>\n    <li>Results show that while models do okay on some tasks, they struggle significantly with reasoning and planning, highlighting major weaknesses.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.15431", "authors": [{"_id": "69437417542d62d58a7bf6c4", "name": "Haolong Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6c5", "name": "Jia Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c6", "name": "Xin Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c7", "name": "Yeqing Shen", "hidden": false}, {"_id": "69437417542d62d58a7bf6c8", "user": {"_id": "653614073f4248157d60ccdc", "avatarUrl": "/avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg", "isPro": false, "fullname": "mengziyang", "user": "zylate", "type": "user"}, "name": "Ziyang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:53.033Z", "hidden": false}, {"_id": "69437417542d62d58a7bf6c9", "name": "Zhimin Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6ca", "name": "Kaijun Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6cb", "name": "Jin Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf6cc", "name": "Lieyu Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6cd", "name": "Mi Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6ce", "name": "Shiliang Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6cf", "name": "Zhirui Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d0", "name": "Brian Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d1", "name": "Kang An", "hidden": false}, {"_id": "69437417542d62d58a7bf6d2", "name": "Chenyang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d3", "name": "Lei Lei", "hidden": false}, {"_id": "69437417542d62d58a7bf6d4", "name": "Mengmeng Duan", "hidden": false}, {"_id": "69437417542d62d58a7bf6d5", "name": "Danxun Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d6", "name": "Guodong Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d7", "name": "Hang Cheng", "hidden": false}, {"_id": "69437417542d62d58a7bf6d8", "name": "Hao Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d9", "name": "Jie Dong", "hidden": false}, {"_id": "69437417542d62d58a7bf6da", "name": "Junhao Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6db", "name": "Mei Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6dc", "name": "Renjie Yu", "hidden": false}, {"_id": "69437417542d62d58a7bf6dd", "name": "Shunshan Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6de", "name": "Xu Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf6df", "name": "Yiting Dai", "hidden": false}, {"_id": "69437417542d62d58a7bf6e0", "name": "Yineng Deng", "hidden": false}, {"_id": "69437417542d62d58a7bf6e1", "name": "Yingdan Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6e2", "name": "Zelin Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6e3", "name": "Wen Sun", "hidden": false}, {"_id": "69437417542d62d58a7bf6e4", "name": "Chengxu Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e5", "name": "Chunqin Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6e6", "name": "Dong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6e7", "name": "Fengqiong Xiao", "hidden": false}, {"_id": "69437417542d62d58a7bf6e8", "name": "Guanghao Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e9", "name": "Guopeng Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ea", "name": "Guozhen Peng", "hidden": false}, {"_id": "69437417542d62d58a7bf6eb", "name": "Hongbing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ec", "name": "Hang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ed", "name": "Hongming Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6ee", "name": "Jingjing Xie", "hidden": false}, {"_id": "69437417542d62d58a7bf6ef", "name": "Jianyong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6f0", "name": "Jingyang Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf6f1", "name": "Jiaju Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f2", "name": "Jiayu Yuan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f3", "name": "Jianpeng Yin", "hidden": false}, {"_id": "69437417542d62d58a7bf6f4", "name": "Kai Cao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f5", "name": "Liang Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f6", "name": "Liguo Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f7", "name": "Liying Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6f8", "name": "Mengqiang Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f9", "name": "Min Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fa", "name": "Manjiao Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fb", "name": "Mao Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf6fc", "name": "Mingxin Wan", "hidden": false}, {"_id": "69437417542d62d58a7bf6fd", "name": "Na Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6fe", "name": "Nan Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6ff", "name": "Ning Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf700", "name": "Peiyao Ma", "hidden": false}, {"_id": "69437417542d62d58a7bf701", "name": "Qingzhou Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf702", "name": "Qiao Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf703", "name": "Qinlin Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf704", "name": "Qiong Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf705", "name": "Qiongyao Li", "hidden": false}, {"_id": "69437417542d62d58a7bf706", "name": "Shangwu Zhong", "hidden": false}, {"_id": "69437417542d62d58a7bf707", "name": "Shuli Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf708", "name": "Shaofan Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf709", "name": "Shisi Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf70a", "name": "Shuang Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf70b", "name": "Xingbin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70c", "name": "Xiaojia Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70d", "name": "Xiaojie Hou", "hidden": false}, {"_id": "69437417542d62d58a7bf70e", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70f", "name": "Xuanti Feng", "hidden": false}, {"_id": "69437417542d62d58a7bf710", "name": "Xuedan Cai", "hidden": false}, {"_id": "69437417542d62d58a7bf711", "name": "Xuan Wen", "hidden": false}, {"_id": "69437417542d62d58a7bf712", "name": "Xianwei Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf713", "name": "Xin Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf714", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf715", "name": "Xin Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf716", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf717", "name": "Yukang Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf718", "name": "Yunfang Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf719", "name": "Yuqing Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf71a", "name": "Yixun Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf71b", "name": "Zejia Weng", "hidden": false}, {"_id": "69437417542d62d58a7bf71c", "name": "Zhonghao Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf71d", "name": "Zhiguo Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf71e", "name": "Zhuoyu Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf71f", "name": "Zheng Ge", "hidden": false}, {"_id": "69437417542d62d58a7bf720", "name": "Jing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf721", "name": "Yibo Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf722", "name": "Binxing Jiao", "hidden": false}, {"_id": "69437417542d62d58a7bf723", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf724", "name": "Daxin Jiang", "hidden": false}], "publishedAt": "2025-12-17T13:26:30.000Z", "submittedOnDailyAt": "2025-12-18T00:55:26.804Z", "title": "Step-GUI Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "upvotes": 77, "discussionId": "69437418542d62d58a7bf725", "projectPage": "https://opengelab.github.io/", "githubRepo": "https://github.com/stepfun-ai/gelab-zero", "githubRepoAddedBy": "user", "ai_summary": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.", "ai_keywords": ["multimodal large language models", "GUI automation", "self-evolving training pipeline", "Calibrated Step Reward System", "trajectory-level calibration", "Step-GUI", "GUI performance", "GUI-MCP", "Model Context Protocol", "AndroidWorld", "OSWorld", "ScreenShot-Pro", "AndroidDaily", "real-world mobile usage patterns", "hierarchical architecture", "low-level atomic operations", "high-level task delegation", "local specialist models", "high-privacy execution"], "githubStars": 1417, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4f7f\u7528\u6821\u51c6\u6b65\u9aa4\u5956\u52b1\u7cfb\u7edf\u63d0\u9ad8\u8bad\u7ec3\u6570\u636e\u7684\u8d28\u91cf\u548c\u6210\u672c\u6548\u76ca\u3002</li>\n    <li>\u5f00\u53d1\u4e86Step-GUI\u6a21\u578b\u7cfb\u5217\uff0c\u5c55\u793a\u4e86\u5728GUI\u6027\u80fd\u65b9\u9762\u7684\u9886\u5148\u8868\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u7528\u80fd\u529b\u3002</li>\n    <li>\u5f15\u5165\u4e86GUI-MCP\u534f\u8bae\uff0c\u786e\u4fdd\u4e0d\u540c\u8bbe\u5907\u4e4b\u95f4\u7684\u6807\u51c6\u5316\u63a5\u53e3\uff0c\u540c\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002</li>\n    <li>\u63a8\u51fa\u4e86AndroidDaily\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8e\u771f\u5b9e\u7684\u79fb\u52a8\u4f7f\u7528\u6a21\u5f0f\u8bc4\u4f30\u4ee3\u7406\u7684\u8868\u73b0\u3002</li>\n    <li>\u6b64\u7814\u7a76\u63a8\u52a8\u4e86\u5b9e\u7528GUI\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u65e5\u5e38\u6570\u5b57\u4ea4\u4e92\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New methods in large language models improve automation for graphical user interfaces (GUIs), but getting good training data is still a challenge.</li>\n    <li>We developed a training system called the Calibrated Step Reward System that makes it cheaper and more reliable to train models, achieving over 90% accuracy.</li>\n    <li>We created Step-GUI models that perform well on GUIs, with impressive scores on benchmarks, while also being generally capable in other areas.</li>\n    <li>To ensure user privacy and standardize interactions across devices, we introduced GUI-MCP, a new protocol that manages tasks effectively while keeping data secure on devices.</li>\n    <li>We also launched AndroidDaily, a benchmark that reflects real mobile usage, helping to test how well agents can perform everyday tasks.</li>\n</ul>"}, "publishedAt": "2025-12-17T08:26:30.000Z", "title": "Step-GUI Technical Report", "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15431.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13586", "authors": [{"_id": "6940d86d65f1e24a117805cd", "user": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "isPro": false, "fullname": "Jia-Nan Li", "user": "JinaLeejnl", "type": "user"}, "name": "Jia-Nan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:38:49.797Z", "hidden": false}, {"_id": "6940d86d65f1e24a117805ce", "name": "Jian Guan", "hidden": false}, {"_id": "6940d86d65f1e24a117805cf", "name": "Wei Wu", "hidden": false}, {"_id": "6940d86d65f1e24a117805d0", "name": "Chongxuan Li", "hidden": false}], "publishedAt": "2025-12-15T17:41:19.000Z", "submittedOnDailyAt": "2025-12-16T01:39:12.101Z", "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "submittedOnDailyBy": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "isPro": false, "fullname": "Jia-Nan Li", "user": "JinaLeejnl", "type": "user"}, "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.", "upvotes": 74, "discussionId": "6940d86d65f1e24a117805d1", "githubRepo": "https://github.com/ML-GSAI/ReFusion", "githubRepoAddedBy": "user", "ai_summary": "ReFusion, a novel masked diffusion model, improves performance and efficiency by using slot-based parallel decoding, achieving superior results compared to autoregressive models and traditional masked diffusion models.", "ai_keywords": ["autoregressive models", "masked diffusion models", "Key-Value caching", "parallel decoding", "token level", "slot level", "diffusion-based planning", "autoregressive infilling", "slot-based design", "slot-level permutation space"], "githubStars": 18, "summary_zh": "<ul>\n    <li>\u81ea\u56de\u5f52\u6a21\u578b\uff08ARMs\uff09\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u53d7\u9650\u4e8e\u987a\u5e8f\u5904\u7406\u3002</li>\n    <li>\u63a9\u853d\u6269\u6563\u6a21\u578b\uff08MDMs\uff09\u5e76\u884c\u5904\u7406\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u751f\u6210\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002</li>\n    <li>ReFusion\u662f\u4e00\u79cd\u65b0\u578b\u63a9\u853d\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u5e76\u884c\u89e3\u7801\u63d0\u5347\u5230\u66f4\u9ad8\u7684\u69fd\u7ea7\u522b\u6765\u63d0\u9ad8\u6548\u7387\u3002</li>\n    <li>\u91c7\u7528\u201c\u8ba1\u5212\u4e0e\u586b\u5145\u201d\u7684\u89e3\u7801\u8fc7\u7a0b\uff0c\u4f7f\u5f97\u89e3\u7801\u6b65\u9aa4\u53ef\u4ee5\u5e76\u884c\u8fdb\u884c\uff0c\u5e76\u4e14\u5145\u5206\u5229\u7528\u952e\u503c\u7f13\u5b58\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cReFusion\u5728\u6027\u80fd\u548c\u901f\u5ea6\u4e0a\u660e\u663e\u4f18\u4e8e\u4ee5\u5f80\u7684MDMs\uff0c\u5e76\u4e14\u63a5\u8fd1\u81ea\u56de\u5f52\u6a21\u578b\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoregressive models typically have slow inference times, while masked diffusion models are faster but face significant issues.</li>\n    <li>Masked diffusion models struggle with high computational costs and problems with generating coherent outputs.</li>\n    <li>The new model, ReFusion, improves efficiency by grouping tokens into fixed-length slots for better parallel processing.</li>\n    <li>ReFusion uses a two-step process: it first plans which slots to fill, then fills them in parallel, allowing for better performance.</li>\n    <li>Tests show ReFusion performs 34% better than previous models and is over 18 times faster, while also improving speed compared to autoregressive models.</li>\n</ul>"}, "publishedAt": "2025-12-15T12:41:19.000Z", "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13586.png", "numComments": 2, "submittedBy": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "fullname": "Jia-Nan Li", "name": "JinaLeejnl", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "isAuthorParticipating": true}, {"paper": {"id": "2512.13687", "authors": [{"_id": "6940ee0665f1e24a1178066d", "user": {"_id": "67756c9c846a267749304255", "avatarUrl": "/avatars/01f09805b561887c55d1b9ad4e96b461.svg", "isPro": false, "fullname": "Jingfeng Yao", "user": "MapleF9", "type": "user"}, "name": "Jingfeng Yao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T10:34:03.365Z", "hidden": false}, {"_id": "6940ee0665f1e24a1178066e", "user": {"_id": "6264bf5a1ed8d81e47ae3a62", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650769741842-noauth.jpeg", "isPro": false, "fullname": "Yuda Song", "user": "IDKiro", "type": "user"}, "name": "Yuda Song", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:38.071Z", "hidden": false}, {"_id": "6940ee0665f1e24a1178066f", "user": {"_id": "64192280d459c9e7fbb03aa1", "avatarUrl": "/avatars/89935de92f3f9107d7b768b82fb27e70.svg", "isPro": false, "fullname": "Zhou", "user": "Yucong", "type": "user"}, "name": "Yucong Zhou", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T10:34:50.881Z", "hidden": false}, {"_id": "6940ee0665f1e24a11780670", "user": {"_id": "62600de6d47e3dbae32ce1ce", "avatarUrl": "/avatars/a536417cfec6e10ac415091bd1829426.svg", "isPro": false, "fullname": "Xinggang Wang", "user": "xinggangw", "type": "user"}, "name": "Xinggang Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T10:34:40.199Z", "hidden": false}], "publishedAt": "2025-12-15T18:59:54.000Z", "submittedOnDailyAt": "2025-12-16T07:11:50.997Z", "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation", "submittedOnDailyBy": {"_id": "67756c9c846a267749304255", "avatarUrl": "/avatars/01f09805b561887c55d1b9ad4e96b461.svg", "isPro": false, "fullname": "Jingfeng Yao", "user": "MapleF9", "type": "user"}, "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.", "upvotes": 66, "discussionId": "6940ee0665f1e24a11780671", "githubRepo": "https://github.com/MiniMax-AI/VTP", "githubRepoAddedBy": "user", "ai_summary": "A unified visual tokenizer pre-training framework (VTP) improves generative performance by optimizing image-text contrastive, self-supervised, and reconstruction losses, leading to better scaling properties and higher zero-shot accuracy and faster convergence.", "ai_keywords": ["latent space", "visual tokenizers", "VAEs", "reconstruction-based training", "low-level information", "pre-training scaling problem", "high-level semantics", "VTP", "image-text contrastive", "self-supervised", "reconstruction losses", "generative performance", "ImageNet", "zero-shot accuracy", "rFID", "FLOPS", "DiT", "advanced distillation methods"], "githubStars": 92, "organization": {"_id": "6778fc29920093dbc0c24917", "name": "MiniMaxAI", "fullname": "MiniMax", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/676e38ad04af5bec20bc9faf/dUd-LsZEX0H_d4qefO_g6.jpeg"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u6807\u8bb0\u5668\uff08\u5982VAE\uff09\u7684\u6f5c\u5728\u7a7a\u95f4\u8d28\u91cf\u5bf9\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>\u4f20\u7edf\u7684\u91cd\u5efa\u8bad\u7ec3\u65b9\u6cd5\u5bfc\u81f4\u6f5c\u5728\u7a7a\u95f4\u504f\u5411\u4f4e\u7ea7\u4fe1\u606f\uff0c\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51faVTP\u6846\u67b6\uff0c\u4f18\u5316\u56fe\u50cf-\u6587\u672c\u5bf9\u6bd4\u3001\u81ea\u76d1\u7763\u548c\u91cd\u5efa\u635f\u5931\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\u7406\u89e3\u662f\u751f\u6210\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u4e14\u751f\u6210\u6027\u80fd\u4e0e\u8ba1\u7b97\u8d44\u6e90\u3001\u53c2\u6570\u548c\u6570\u636e\u7684\u5173\u7cfb\u66f4\u597d\u3002</li>\n    <li>\u7ecf\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u540e\uff0c\u6211\u4eec\u7684\u6807\u8bb0\u5668\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u52a0\u901f\u548c\u6548\u679c\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The quality of the latent space in visual tokenizers is very important for generative models.</li>\n    <li>Current training methods focus too much on low-level details, which doesn\u2019t help in generating better images.</li>\n    <li>We introduce VTP, a new pre-training framework that combines several training methods to improve generation quality.</li>\n    <li>Our research shows that understanding high-level information is key for better image generation.</li>\n    <li>VTP outperforms traditional methods, needing less computing power to achieve better results.</li>\n</ul>"}, "publishedAt": "2025-12-15T13:59:54.000Z", "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation", "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13687.png", "numComments": 1, "submittedBy": {"_id": "67756c9c846a267749304255", "avatarUrl": "/avatars/01f09805b561887c55d1b9ad4e96b461.svg", "fullname": "Jingfeng Yao", "name": "MapleF9", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "6778fc29920093dbc0c24917", "name": "MiniMaxAI", "fullname": "MiniMax", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/676e38ad04af5bec20bc9faf/dUd-LsZEX0H_d4qefO_g6.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.13564", "authors": [{"_id": "6940d68565f1e24a1178056c", "user": {"_id": "6544b9b646dbdeca34ee5f52", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png", "isPro": false, "fullname": "Yuyang Hu", "user": "namespace-ERI", "type": "user"}, "name": "Yuyang Hu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:40.040Z", "hidden": false}, {"_id": "6940d68565f1e24a1178056d", "user": {"_id": "65435cad429b80b14922ab8d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/N8oWq4ZZn3dRxmXi18FrA.jpeg", "isPro": false, "fullname": "Shichun Liu", "user": "Liusc2020", "type": "user"}, "name": "Shichun Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:40.844Z", "hidden": false}, {"_id": "6940d68565f1e24a1178056e", "name": "Yanwei Yue", "hidden": false}, {"_id": "6940d68565f1e24a1178056f", "name": "Guibin Zhang", "hidden": false}, {"_id": "6940d68565f1e24a11780570", "name": "Boyang Liu", "hidden": false}, {"_id": "6940d68565f1e24a11780571", "name": "Fangyi Zhu", "hidden": false}, {"_id": "6940d68565f1e24a11780572", "name": "Jiahang Lin", "hidden": false}, {"_id": "6940d68565f1e24a11780573", "user": {"_id": "638ef0b0c67af472d31674a6", "avatarUrl": "/avatars/02df97d15a0f46b47f9162221733b121.svg", "isPro": false, "fullname": "Honglin Guo", "user": "KYLN24", "type": "user"}, "name": "Honglin Guo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:38.698Z", "hidden": false}, {"_id": "6940d68565f1e24a11780574", "name": "Shihan Dou", "hidden": false}, {"_id": "6940d68565f1e24a11780575", "name": "Zhiheng Xi", "hidden": false}, {"_id": "6940d68565f1e24a11780576", "name": "Senjie Jin", "hidden": false}, {"_id": "6940d68565f1e24a11780577", "user": {"_id": "62e52483a944e2a56cd2c6ca", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg", "isPro": false, "fullname": "Jiejun Tan", "user": "zstanjj", "type": "user"}, "name": "Jiejun Tan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:42.726Z", "hidden": false}, {"_id": "6940d68565f1e24a11780578", "name": "Yanbin Yin", "hidden": false}, {"_id": "6940d68565f1e24a11780579", "name": "Jiongnan Liu", "hidden": false}, {"_id": "6940d68565f1e24a1178057a", "name": "Zeyu Zhang", "hidden": false}, {"_id": "6940d68565f1e24a1178057b", "user": {"_id": "6309bfdab8d7b3889319b588", "avatarUrl": "/avatars/572acdad470f765ef2e058ead3741e24.svg", "isPro": false, "fullname": "SunZX", "user": "Jeryi", "type": "user"}, "name": "Zhongxiang Sun", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T22:32:57.018Z", "hidden": false}, {"_id": "6940d68565f1e24a1178057c", "name": "Yutao Zhu", "hidden": false}, {"_id": "6940d68565f1e24a1178057d", "name": "Hao Sun", "hidden": false}, {"_id": "6940d68565f1e24a1178057e", "name": "Boci Peng", "hidden": false}, {"_id": "6940d68565f1e24a1178057f", "name": "Zhenrong Cheng", "hidden": false}, {"_id": "6940d68565f1e24a11780580", "name": "Xuanbo Fan", "hidden": false}, {"_id": "6940d68565f1e24a11780581", "name": "Jiaxin Guo", "hidden": false}, {"_id": "6940d68565f1e24a11780582", "name": "Xinlei Yu", "hidden": false}, {"_id": "6940d68565f1e24a11780583", "name": "Zhenhong Zhou", "hidden": false}, {"_id": "6940d68565f1e24a11780584", "name": "Zewen Hu", "hidden": false}, {"_id": "6940d68565f1e24a11780585", "name": "Jiahao Huo", "hidden": false}, {"_id": "6940d68565f1e24a11780586", "name": "Junhao Wang", "hidden": false}, {"_id": "6940d68565f1e24a11780587", "name": "Yuwei Niu", "hidden": false}, {"_id": "6940d68565f1e24a11780588", "name": "Yu Wang", "hidden": false}, {"_id": "6940d68565f1e24a11780589", "name": "Zhenfei Yin", "hidden": false}, {"_id": "6940d68565f1e24a1178058a", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6940d68565f1e24a1178058b", "name": "Yue Liao", "hidden": false}, {"_id": "6940d68565f1e24a1178058c", "name": "Qiankun Li", "hidden": false}, {"_id": "6940d68565f1e24a1178058d", "name": "Kun Wang", "hidden": false}, {"_id": "6940d68565f1e24a1178058e", "name": "Wangchunshu Zhou", "hidden": false}, {"_id": "6940d68565f1e24a1178058f", "name": "Yixin Liu", "hidden": false}, {"_id": "6940d68565f1e24a11780590", "name": "Dawei Cheng", "hidden": false}, {"_id": "6940d68565f1e24a11780591", "name": "Qi Zhang", "hidden": false}, {"_id": "6940d68565f1e24a11780592", "name": "Tao Gui", "hidden": false}, {"_id": "6940d68565f1e24a11780593", "name": "Shirui Pan", "hidden": false}, {"_id": "6940d68565f1e24a11780594", "name": "Yan Zhang", "hidden": false}, {"_id": "6940d68565f1e24a11780595", "name": "Philip Torr", "hidden": false}, {"_id": "6940d68565f1e24a11780596", "name": "Zhicheng Dou", "hidden": false}, {"_id": "6940d68565f1e24a11780597", "name": "Ji-Rong Wen", "hidden": false}, {"_id": "6940d68565f1e24a11780598", "name": "Xuanjing Huang", "hidden": false}, {"_id": "6940d68565f1e24a11780599", "name": "Yu-Gang Jiang", "hidden": false}, {"_id": "6940d68565f1e24a1178059a", "name": "Shuicheng Yan", "hidden": false}], "publishedAt": "2025-12-15T17:22:34.000Z", "submittedOnDailyAt": "2025-12-16T01:18:34.363Z", "title": "Memory in the Age of AI Agents", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.", "upvotes": 62, "discussionId": "6940d68565f1e24a1178059b", "githubRepo": "https://github.com/Shichun-Liu/Agent-Memory-Paper-List", "githubRepoAddedBy": "user", "ai_summary": "This survey provides an updated overview of agent memory research, distinguishing its forms, functions, and dynamics, and highlights emerging research directions.", "ai_keywords": ["agent memory", "LLM memory", "retrieval augmented generation (RAG)", "context engineering", "token-level memory", "parametric memory", "latent memory", "factual memory", "experiential memory", "working memory", "memory benchmarks", "open-source frameworks", "memory automation", "reinforcement learning integration", "multimodal memory", "multi-agent memory", "trustworthiness issues"], "githubStars": 115, "summary_zh": "<ul>\n    <li>\u8bb0\u5fc6\u662f\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u667a\u80fd\u4f53\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u7814\u7a76\u6b63\u5728\u5feb\u901f\u53d1\u5c55\u3002</li>\n    <li>\u5f53\u524d\u5173\u4e8e\u667a\u80fd\u4f53\u8bb0\u5fc6\u7684\u7814\u7a76\u5b58\u5728\u5206\u6563\u548c\u6982\u5ff5\u4e0d\u6e05\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u5206\u7c7b\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u6db5\u76d6\u5176\u591a\u6837\u6027\u3002</li>\n    <li>\u672c\u6587\u6e05\u6670\u754c\u5b9a\u667a\u80fd\u4f53\u8bb0\u5fc6\u7684\u8303\u56f4\uff0c\u5e76\u4e0e\u76f8\u5173\u6982\u5ff5\u8fdb\u884c\u533a\u5206\u3002</li>\n    <li>\u4ece\u5f62\u5f0f\u3001\u529f\u80fd\u548c\u52a8\u6001\u4e09\u4e2a\u65b9\u9762\u5206\u6790\u667a\u80fd\u4f53\u8bb0\u5fc6\uff0c\u63d0\u51fa\u4e86\u66f4\u7ec6\u81f4\u7684\u5206\u7c7b\u65b9\u6cd5\u3002</li>\n    <li>\u603b\u7ed3\u4e86\u8bb0\u5fc6\u57fa\u51c6\u6d4b\u8bd5\u548c\u5f00\u6e90\u6846\u67b6\uff0c\u5e76\u5c55\u671b\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5982\u8bb0\u5fc6\u81ea\u52a8\u5316\u548c\u591a\u6a21\u6001\u8bb0\u5fc6\u7b49\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Memory is a key feature of foundation model-based agents and is a growing area of research.</li>\n    <li>Current studies on agent memory are varied and often lack clear definitions and classifications.</li>\n    <li>This work aims to clarify agent memory by distinguishing it from related concepts and providing a detailed overview.</li>\n    <li>It categorizes agent memory into three forms (token-level, parametric, latent) and three functions (factual, experiential, working).</li>\n    <li>The study also looks at how memory changes over time and suggests future research areas like memory automation and multi-agent memory.</li>\n</ul>"}, "publishedAt": "2025-12-15T12:22:34.000Z", "title": "Memory in the Age of AI Agents", "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13564.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 185}, "isAuthorParticipating": false}, {"paper": {"id": "2512.12967", "authors": [{"_id": "6940d25d65f1e24a11780417", "user": {"_id": "64777a346e6c7ac608c1e9bf", "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg", "isPro": false, "fullname": "Weizhou Shen", "user": "shenwzh3", "type": "user"}, "name": "Weizhou Shen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:42.079Z", "hidden": false}, {"_id": "6940d25d65f1e24a11780418", "user": {"_id": "64c9b0f28d2d187c24d1e6c1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1CPnAaB3gsupdpiNWaoDc.png", "isPro": false, "fullname": "ZiYi Yang", "user": "AALF", "type": "user"}, "name": "Ziyi Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:56.062Z", "hidden": false}, {"_id": "6940d25d65f1e24a11780419", "name": "Chenliang Li", "hidden": false}, {"_id": "6940d25d65f1e24a1178041a", "name": "Zhiyuan Lu", "hidden": false}, {"_id": "6940d25d65f1e24a1178041b", "name": "Miao Peng", "hidden": false}, {"_id": "6940d25d65f1e24a1178041c", "name": "Huashan Sun", "hidden": false}, {"_id": "6940d25d65f1e24a1178041d", "name": "Yingcheng Shi", "hidden": false}, {"_id": "6940d25d65f1e24a1178041e", "name": "Shengyi Liao", "hidden": false}, {"_id": "6940d25d65f1e24a1178041f", "name": "Shaopeng Lai", "hidden": false}, {"_id": "6940d25d65f1e24a11780420", "name": "Bo Zhang", "hidden": false}, {"_id": "6940d25d65f1e24a11780421", "name": "Dayiheng Liu", "hidden": false}, {"_id": "6940d25d65f1e24a11780422", "name": "Fei Huang", "hidden": false}, {"_id": "6940d25d65f1e24a11780423", "name": "Jingren Zhou", "hidden": false}, {"_id": "6940d25d65f1e24a11780424", "name": "Ming Yan", "hidden": false}], "publishedAt": "2025-12-15T04:11:11.000Z", "submittedOnDailyAt": "2025-12-16T01:29:41.007Z", "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.", "upvotes": 54, "discussionId": "6940d25d65f1e24a11780425", "githubRepo": "https://github.com/Tongyi-Zhiwen/Qwen-Doc", "githubRepoAddedBy": "user", "ai_summary": "QwenLong-L1.5 enhances long-context reasoning through data synthesis, stabilized reinforcement learning, and memory-augmented architecture, achieving superior performance on benchmarks and general domains.", "ai_keywords": ["Long-Context Data Synthesis Pipeline", "atomic facts", "verifiable reasoning questions", "Stabilized Reinforcement Learning", "task-balanced sampling", "task-specific advantage estimation", "Adaptive Entropy-Controlled Policy Optimization", "Memory-Augmented Architecture", "multi-stage fusion RL training", "single-pass reasoning", "iterative memory-based processing", "memory-agent framework"], "githubStars": 311, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86QwenLong-L1.5\u6a21\u578b\uff0c\u5177\u6709\u5353\u8d8a\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e00\u79cd\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u751f\u6210\u9700\u8981\u591a\u6b65\u63a8\u7406\u7684\u590d\u6742\u4efb\u52a1\u3002</li>\n    <li>\u901a\u8fc7\u7a33\u5b9a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002</li>\n    <li>\u8bbe\u8ba1\u4e86\u5185\u5b58\u589e\u5f3a\u67b6\u6784\uff0c\u4ee5\u5904\u7406\u8d85\u8fc74M\u6807\u8bb0\u7684\u8d85\u957f\u4efb\u52a1\u3002</li>\n    <li>QwenLong-L1.5\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8eGPT-5\u548cGemini-2.5-Pro\uff0c\u63d0\u53479.90\u5206\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>QwenLong-L1.5 is a new model designed for better reasoning over long pieces of text.</li>\n    <li>It uses a unique method to create difficult reasoning tasks by breaking down documents into smaller facts and relationships.</li>\n    <li>The model employs advanced reinforcement learning techniques to stabilize training and improve performance.</li>\n    <li>It has a special memory system that allows it to handle very long text sequences, even up to 4 million tokens.</li>\n    <li>QwenLong-L1.5 performs as well as or better than other leading models in long-context reasoning tasks and improves skills in areas like scientific reasoning and dialogue.</li>\n</ul>"}, "publishedAt": "2025-12-14T23:11:11.000Z", "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management", "summary": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12967.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 185}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.13604", "authors": [{"_id": "6940d44165f1e24a11780535", "name": "Jianxiong Gao", "hidden": false}, {"_id": "6940d44165f1e24a11780536", "name": "Zhaoxi Chen", "hidden": false}, {"_id": "6940d44165f1e24a11780537", "name": "Xian Liu", "hidden": false}, {"_id": "6940d44165f1e24a11780538", "user": {"_id": "64970d3d9c3b29dca8633f87", "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg", "isPro": false, "fullname": "JunhaoZhuang", "user": "JunhaoZhuang", "type": "user"}, "name": "Junhao Zhuang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:51.620Z", "hidden": false}, {"_id": "6940d44165f1e24a11780539", "name": "Chengming Xu", "hidden": false}, {"_id": "6940d44165f1e24a1178053a", "name": "Jianfeng Feng", "hidden": false}, {"_id": "6940d44165f1e24a1178053b", "name": "Yu Qiao", "hidden": false}, {"_id": "6940d44165f1e24a1178053c", "name": "Yanwei Fu", "hidden": false}, {"_id": "6940d44165f1e24a1178053d", "user": {"_id": "635f8ed47c05eb9f59963d3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg", "isPro": false, "fullname": "ChenyangSi", "user": "ChenyangSi", "type": "user"}, "name": "Chenyang Si", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:49.025Z", "hidden": false}, {"_id": "6940d44165f1e24a1178053e", "name": "Ziwei Liu", "hidden": false}], "publishedAt": "2025-12-15T17:59:58.000Z", "submittedOnDailyAt": "2025-12-16T01:12:24.486Z", "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model", "submittedOnDailyBy": {"_id": "643815c4961bb61e463c5896", "avatarUrl": "/avatars/3b44592472f16c56105bff8c314d9939.svg", "isPro": false, "fullname": "Jianxiong Gao", "user": "Jianxiong", "type": "user"}, "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.", "upvotes": 52, "discussionId": "6940d44165f1e24a1178053f", "projectPage": "https://vchitect.github.io/LongVie2-project/", "ai_summary": "LongVie 2, an end-to-end autoregressive framework, enhances controllability, visual quality, and temporal consistency in video world models through three progressive training stages.", "ai_keywords": ["autoregressive framework", "multi-modal guidance", "degradation-aware training", "history-context guidance", "video world models", "controllability", "visual quality", "temporal consistency", "LongVGenBench", "state-of-the-art performance", "temporal coherence", "visual fidelity"], "summary_zh": "<ul>\n    <li>\u6784\u5efa\u89c6\u9891\u4e16\u754c\u6a21\u578b\u662f\u5b9e\u73b0\u66f4\u9ad8\u7a7a\u95f4\u65f6\u95f4\u667a\u80fd\u7684\u91cd\u8981\u4e00\u6b65\u3002</li>\n    <li>\u4e00\u4e2a\u597d\u7684\u4e16\u754c\u6a21\u578b\u5e94\u5177\u5907\u53ef\u63a7\u6027\u3001\u957f\u671f\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e09\u5927\u7279\u6027\u3002</li>\n    <li>LongVie 2 \u662f\u4e00\u4e2a\u9010\u6b65\u63d0\u5347\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u5206\u4e09\u4e2a\u9636\u6bb5\u8fdb\u884c\u8bad\u7ec3\u3002</li>\n    <li>\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u6307\u5bfc\u548c\u5386\u53f2\u4e0a\u4e0b\u6587\u6307\u5bfc\u6765\u63d0\u9ad8\u53ef\u63a7\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002</li>\n    <li>LongVGenBench \u662f\u4e00\u4e2a\u5305\u542b100\u4e2a\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u7684\u57fa\u51c6\uff0c\u5c55\u793a\u4e86LongVie 2\u7684\u4f18\u8d8a\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongVie 2 is a new system for creating video world models that aims to improve how machines understand and generate videos.</li>\n    <li>The system focuses on three key features: controllability, long-term visual quality, and consistency over time.</li>\n    <li>It uses a three-stage training process to enhance control, maintain visual quality, and ensure consistency between video clips.</li>\n    <li>LongVGenBench is a new benchmark that includes 100 high-quality, one-minute videos for testing the system.</li>\n    <li>LongVie 2 shows top performance in generating long videos that are coherent and visually appealing, advancing the field of video generation.</li>\n</ul>"}, "publishedAt": "2025-12-15T12:59:58.000Z", "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model", "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13604.png", "numComments": 1, "submittedBy": {"_id": "643815c4961bb61e463c5896", "avatarUrl": "/avatars/3b44592472f16c56105bff8c314d9939.svg", "fullname": "Jianxiong Gao", "name": "Jianxiong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13281", "authors": [{"_id": "6940d82465f1e24a117805c2", "name": "Jiaqi Wang", "hidden": false}, {"_id": "6940d82465f1e24a117805c3", "name": "Weijia Wu", "hidden": false}, {"_id": "6940d82465f1e24a117805c4", "name": "Yi Zhan", "hidden": false}, {"_id": "6940d82465f1e24a117805c5", "name": "Rui Zhao", "hidden": false}, {"_id": "6940d82465f1e24a117805c6", "name": "Ming Hu", "hidden": false}, {"_id": "6940d82465f1e24a117805c7", "name": "James Cheng", "hidden": false}, {"_id": "6940d82465f1e24a117805c8", "name": "Wei Liu", "hidden": false}, {"_id": "6940d82465f1e24a117805c9", "name": "Philip Torr", "hidden": false}, {"_id": "6940d82465f1e24a117805ca", "user": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "isPro": false, "fullname": "Qinghong (Kevin) Lin", "user": "KevinQHLin", "type": "user"}, "name": "Kevin Qinghong Lin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T14:04:37.407Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/g2dRdpFY8mVSrBnrK3C0q.mp4"], "publishedAt": "2025-12-15T12:41:23.000Z", "submittedOnDailyAt": "2025-12-17T08:26:01.077Z", "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?", "submittedOnDailyBy": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "isPro": false, "fullname": "Qinghong (Kevin) Lin", "user": "KevinQHLin", "type": "user"}, "summary": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: (i) Immersive ASMR video-audio sources. Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. (ii) Peer-Review evaluation. An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\\% accuracy (random 50\\%), far below that of human experts (81.25\\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.", "upvotes": 50, "discussionId": "6940d82565f1e24a117805cb", "projectPage": "https://video-reality-test.github.io/", "githubRepo": "https://github.com/video-reality-test/video-reality-test", "githubRepoAddedBy": "user", "ai_summary": "The Video Reality Test benchmark evaluates the realism and detection of AI-generated ASMR videos with audio, revealing that even the best models can deceive VLMs and humans, highlighting limitations in perceptual fidelity and audio-visual consistency.", "ai_keywords": ["ASMR", "audio-visual coupling", "Veo3.1-Fast", "Gemini 2.5-Pro", "perceptual realism", "real-fake discrimination", "audio-visual consistency"], "githubStars": 13, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u751f\u6210\u6280\u672f\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4ea7\u751f\u7684\u5185\u5bb9\u4e0e\u771f\u5b9e\u89c6\u9891\u96be\u4ee5\u533a\u5206\u3002</li>\n    <li>\u5f53\u524d\u7684AI\u751f\u6210\u5185\u5bb9\u68c0\u6d4b\u6807\u51c6\u4e3b\u8981\u8bc4\u4f30\u65e0\u97f3\u9891\u89c6\u9891\uff0c\u4e14\u805a\u7126\u4e8e\u5e7f\u6cdb\u53d9\u4e8b\u9886\u57df\u7684\u5206\u7c7b\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86\u201c\u89c6\u9891\u73b0\u5b9e\u6d4b\u8bd5\u201d\uff0c\u4e00\u4e2a\u7528\u4e8e\u6d4b\u8bd5\u97f3\u9891\u89c6\u89c9\u7ed3\u5408\u4e0b\u611f\u77e5\u771f\u5b9e\u611f\u7684\u65b0\u57fa\u51c6\u3002</li>\n    <li>\u5b9e\u9a8c\u53d1\u73b0\uff0c\u6700\u4f18\u79c0\u7684\u751f\u6210\u6a21\u578bVeo3.1-Fast\u80fd\u6b3a\u9a97\u5927\u591a\u6570\u89c6\u9891\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u5176\u51c6\u786e\u7387\u4f4e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u3002</li>\n    <li>\u6dfb\u52a0\u97f3\u9891\u53ef\u4ee5\u63d0\u9ad8\u771f\u5b9e\u4e0e\u5047\u5192\u7684\u533a\u5206\u80fd\u529b\uff0c\u4f46\u4ecd\u6709\u8868\u9762\u7ebf\u7d22\uff08\u5982\u6c34\u5370\uff09\u4f1a\u8bef\u5bfc\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent video generation technology can create videos that look very real, making it hard to tell them apart from actual videos.</li>\n    <li>Most existing tests for detecting AI-generated videos focus only on video without sound and do not assess the combination of audio and video.</li>\n    <li>We developed the Video Reality Test, which evaluates videos with both sound and visuals, using ASMR videos to create a more immersive experience.</li>\n    <li>The test involves a competition where video generation models try to trick reviewers, including humans and AI, into thinking their videos are real.</li>\n    <li>Results show that some AI models can fool reviewers, but adding audio helps to distinguish between real and fake videos, although some misleading cues still exist.</li>\n</ul>"}, "publishedAt": "2025-12-15T07:41:23.000Z", "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?", "summary": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: (i) Immersive ASMR video-audio sources. Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. (ii) Peer-Review evaluation. An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\\% accuracy (random 50\\%), far below that of human experts (81.25\\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/g2dRdpFY8mVSrBnrK3C0q.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13281.png", "numComments": 2, "submittedBy": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "fullname": "Qinghong (Kevin) Lin", "name": "KevinQHLin", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 41}, "isAuthorParticipating": true}, {"paper": {"id": "2512.14614", "authors": [{"_id": "694219f25d5b2dc1052747ff", "user": {"_id": "64897b1f0ec897cfe579a399", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64897b1f0ec897cfe579a399/ICR_75b877BaSE94gjBuj.jpeg", "isPro": false, "fullname": "wenq", "user": "wenqsun", "type": "user"}, "name": "Wenqiang Sun", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:38.348Z", "hidden": false}, {"_id": "694219f25d5b2dc105274800", "name": "Haiyu Zhang", "hidden": false}, {"_id": "694219f25d5b2dc105274801", "name": "Haoyuan Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274802", "name": "Junta Wu", "hidden": false}, {"_id": "694219f25d5b2dc105274803", "name": "Zehan Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274804", "name": "Zhenwei Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274805", "name": "Yunhong Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274806", "name": "Jun Zhang", "hidden": false}, {"_id": "694219f25d5b2dc105274807", "name": "Tengfei Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274808", "name": "Chunchao Guo", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/z23kgc1LgY1qvs-Tfp4zj.mp4"], "publishedAt": "2025-12-16T17:22:46.000Z", "submittedOnDailyAt": "2025-12-17T00:24:30.301Z", "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.", "upvotes": 48, "discussionId": "694219f35d5b2dc105274809", "projectPage": "https://3d-models.hunyuan.tencent.com/world/", "githubRepo": "https://github.com/Tencent-Hunyuan/HY-WorldPlay", "githubRepoAddedBy": "user", "ai_summary": "WorldPlay is a streaming video diffusion model that achieves real-time, interactive world modeling with long-term geometric consistency by using a Dual Action Representation, Reconstituted Context Memory, and Context Forcing.", "ai_keywords": ["Dual Action Representation", "Reconstituted Context Memory", "temporal reframing", "Context Forcing", "memory-aware model", "long-horizon streaming video"], "githubStars": 302, "summary_zh": "<ul>\n    <li>WorldPlay \u662f\u4e00\u79cd\u6d41\u5a92\u4f53\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u80fd\u5b9e\u65f6\u3001\u4e92\u52a8\u5730\u5efa\u6a21\u4e16\u754c\uff0c\u5e76\u4fdd\u6301\u957f\u671f\u51e0\u4f55\u4e00\u81f4\u6027\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4f7f\u7528\u53cc\u91cd\u52a8\u4f5c\u8868\u793a\uff0c\u589e\u5f3a\u4e86\u5bf9\u7528\u6237\u952e\u76d8\u548c\u9f20\u6807\u8f93\u5165\u7684\u63a7\u5236\u80fd\u529b\u3002</li>\n    <li>\u901a\u8fc7\u91cd\u6784\u4e0a\u4e0b\u6587\u8bb0\u5fc6\uff0c\u52a8\u6001\u91cd\u5efa\u5386\u53f2\u5e27\u7684\u4e0a\u4e0b\u6587\uff0c\u4fdd\u6301\u51e0\u4f55\u91cd\u8981\u7684\u65e7\u5e27\u53ef\u8bbf\u95ee\uff0c\u4ece\u800c\u6539\u5584\u5185\u5b58\u8870\u51cf\u3002</li>\n    <li>\u63d0\u51fa\u7684\u4e0a\u4e0b\u6587\u5f3a\u5236\u65b9\u6cd5\u6709\u52a9\u4e8e\u5728\u6559\u5e08\u548c\u5b66\u751f\u6a21\u578b\u4e4b\u95f4\u5bf9\u9f50\u8bb0\u5fc6\u4e0a\u4e0b\u6587\uff0c\u4fdd\u6301\u5b9e\u65f6\u901f\u5ea6\u5e76\u9632\u6b62\u9519\u8bef\u6f02\u79fb\u3002</li>\n    <li>WorldPlay \u80fd\u4ee5 24 FPS \u7684\u901f\u5ea6\u751f\u6210720p\u89c6\u9891\uff0c\u5177\u6709\u66f4\u597d\u7684\u4e00\u81f4\u6027\uff0c\u4e14\u5728\u5404\u79cd\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>WorldPlay is a new model for streaming video that allows real-time, interactive world modeling while maintaining consistent geometry over time.</li>\n    <li>It introduces a Dual Action Representation for better control based on user inputs from the keyboard and mouse.</li>\n    <li>The model uses a Reconstituted Context Memory to keep important past frames accessible, helping maintain long-term consistency and reduce memory loss.</li>\n    <li>Context Forcing is a new method that helps align memory between different parts of the model, allowing it to use long-term information effectively while running quickly.</li>\n    <li>WorldPlay can generate high-quality 720p video at 24 frames per second and performs well across various scenes compared to other methods.</li>\n</ul>"}, "publishedAt": "2025-12-16T12:22:46.000Z", "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/z23kgc1LgY1qvs-Tfp4zj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14614.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13168", "authors": [{"_id": "6940eedd65f1e24a11780673", "user": {"_id": "637b08057ce76c3b834da15d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PFmI1HNcZlHF0RoLjP3c2.png", "isPro": false, "fullname": "Haoyu Dong", "user": "HaoyuDong", "type": "user"}, "name": "Haoyu Dong", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:37:23.964Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780674", "user": {"_id": "684ae4a3a6d604475ef72f22", "avatarUrl": "/avatars/70e61614ab115f07361c0baec351255a.svg", "isPro": false, "fullname": "Pengkun Zhang", "user": "logicluo", "type": "user"}, "name": "Pengkun Zhang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T15:09:52.673Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780675", "user": {"_id": "69369fc780cb6886b08dcf5b", "avatarUrl": "/avatars/1159e73666a55706e551afd4518adf0b.svg", "isPro": false, "fullname": "Yan Gao", "user": "gaoyansdyt", "type": "user"}, "name": "Yan Gao", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:47:22.721Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780676", "user": {"_id": "69377b51e5af592ad600e1c5", "avatarUrl": "/avatars/72612a4a69da2bd565ed4777287cff86.svg", "isPro": false, "fullname": "Xuanyu Dong", "user": "KcNcooo", "type": "user"}, "name": "Xuanyu Dong", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T14:13:14.674Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780677", "user": {"_id": "6777bb751ee6b20009098ab6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6777bb751ee6b20009098ab6/OumKsg54DNDH8qkslg95x.jpeg", "isPro": false, "fullname": "Yilin Cheng", "user": "Yilin-Cheng24", "type": "user"}, "name": "Yilin Cheng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T10:18:24.145Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780678", "user": {"_id": "694135abb4ad5b6ca6c4c9fe", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/694135abb4ad5b6ca6c4c9fe/ZFl3u8k69DZpyErVLYRBk.jpeg", "isPro": false, "fullname": "Mingzhe Lu", "user": "metaphor2ml", "type": "user"}, "name": "Mingzhe Lu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:36.238Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780679", "user": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "isPro": false, "fullname": "Adina Yakefu", "user": "AdinaY", "type": "user"}, "name": "Adina Yakefu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T09:47:16.061Z", "hidden": false}, {"_id": "6940eedd65f1e24a1178067a", "name": "Shuxin Zheng", "hidden": false}], "publishedAt": "2025-12-15T10:28:45.000Z", "submittedOnDailyAt": "2025-12-16T09:53:57.731Z", "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows", "submittedOnDailyBy": {"_id": "637b08057ce76c3b834da15d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PFmI1HNcZlHF0RoLjP3c2.png", "isPro": false, "fullname": "Haoyu Dong", "user": "HaoyuDong", "type": "user"}, "summary": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.", "upvotes": 46, "discussionId": "6940eedd65f1e24a1178067b", "projectPage": "https://huggingface.co/datasets/FinWorkBench/Finch", "ai_summary": "Finch, a benchmark for AI agents in enterprise finance and accounting, evaluates performance across complex, real-world workflows using authentic data from Enron and other institutions.", "ai_keywords": ["LLM-assisted discovery", "expert annotation", "composite workflows", "spreadsheets", "PDFs", "AI systems", "GPT 5.1", "Claude Sonnet 4.5", "Gemini 3 Pro", "Grok 4", "Qwen 3 Max"], "organization": {"_id": "690e8a61e75888a0f707c0ee", "name": "FinWorkBench", "fullname": "Finch: Finance and Accounting Workflow Benchmark", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/637b08057ce76c3b834da15d/0HrDjcjNQK-5-FCZfkLzp.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aFinch\u7684\u8d22\u52a1\u548c\u4f1a\u8ba1\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u4f01\u4e1a\u4e13\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u8868\u73b0\u3002</li>\n    <li>Finch\u6570\u636e\u6765\u6e90\u4e8eEnron\u548c\u5176\u4ed6\u91d1\u878d\u673a\u6784\u7684\u771f\u5b9e\u5de5\u4f5c\u7a7a\u95f4\uff0c\u5305\u62ec150\u540d\u5458\u5de5\u768415,000\u4e2a\u7535\u5b50\u8868\u683c\u548c500,000\u5c01\u7535\u5b50\u90ae\u4ef6\u3002</li>\n    <li>\u8be5\u57fa\u51c6\u6db5\u76d6\u591a\u79cd\u4efb\u52a1\uff0c\u5982\u6570\u636e\u8f93\u5165\u3001\u683c\u5f0f\u5316\u3001\u7f51\u7edc\u641c\u7d22\u548c\u62a5\u544a\uff0c\u53cd\u6620\u4e86\u771f\u5b9e\u4f01\u4e1a\u5de5\u4f5c\u7684\u590d\u6742\u6027\u3002</li>\n    <li>\u6211\u4eec\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f85\u52a9\u53d1\u73b0\u548c\u4e13\u5bb6\u6ce8\u91ca\uff0c\u6784\u5efa\u4e86172\u4e2a\u590d\u5408\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5171384\u4e2a\u4efb\u52a1\u3002</li>\n    <li>\u5bf9\u524d\u6cbfAI\u7cfb\u7edf\u7684\u8bc4\u4f30\u663e\u793a\uff0cGPT 5.1\u548cClaude Sonnet 4.5\u5728\u5b8c\u6210\u5de5\u4f5c\u6d41\u7a0b\u65b9\u9762\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u5206\u522b\u4ec5\u901a\u8fc738.4%\u548c25.0%\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Finch is a new benchmark for testing AI systems in real-world finance and accounting tasks, involving various activities like data entry and reporting.</li>\n  <li>The benchmark is based on real data from Enron and other financial institutions, including emails and spreadsheets that reflect the complexity of actual work environments.</li>\n  <li>Creating Finch involved combining AI tools with expert input to derive and annotate workflows, resulting in 172 workflows with 384 tasks from millions of data points.</li>\n  <li>Tests on advanced AI systems show that even the best, like GPT 5.1, struggle with these workflows, achieving only 38.4% success after extensive testing.</li>\n  <li>Case studies reveal significant challenges AI faces when dealing with real-world enterprise workflows.</li>\n</ul>"}, "publishedAt": "2025-12-15T05:28:45.000Z", "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows", "summary": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13168.png", "numComments": 1, "submittedBy": {"_id": "637b08057ce76c3b834da15d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PFmI1HNcZlHF0RoLjP3c2.png", "fullname": "Haoyu Dong", "name": "HaoyuDong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "organization": {"_id": "690e8a61e75888a0f707c0ee", "name": "FinWorkBench", "fullname": "Finch: Finance and Accounting Workflow Benchmark", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/637b08057ce76c3b834da15d/0HrDjcjNQK-5-FCZfkLzp.png"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2511.18538", "authors": [{"_id": "692e667137312eaa83fd8832", "user": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "name": "Jian Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:34.886Z", "hidden": false}, {"_id": "692e667137312eaa83fd8833", "name": "Xianglong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8834", "name": "Weifeng Lv", "hidden": false}, {"_id": "692e667137312eaa83fd8835", "name": "Ken Deng", "hidden": false}, {"_id": "692e667137312eaa83fd8836", "name": "Shawn Guo", "hidden": false}, {"_id": "692e667137312eaa83fd8837", "name": "Lin Jing", "hidden": false}, {"_id": "692e667137312eaa83fd8838", "name": "Yizhi Li", "hidden": false}, {"_id": "692e667137312eaa83fd8839", "name": "Shark Liu", "hidden": false}, {"_id": "692e667137312eaa83fd883a", "name": "Xianzhen Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883b", "name": "Yuyu Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883c", "name": "Changzai Pan", "hidden": false}, {"_id": "692e667137312eaa83fd883d", "name": "Ensheng Shi", "hidden": false}, {"_id": "692e667137312eaa83fd883e", "name": "Yingshui Tan", "hidden": false}, {"_id": "692e667137312eaa83fd883f", "name": "Renshuai Tao", "hidden": false}, {"_id": "692e667137312eaa83fd8840", "user": {"_id": "66a8e2538407031e388c501f", "avatarUrl": "/avatars/d16d51f7b1e111efd6d0985995b614be.svg", "isPro": false, "fullname": "wjj", "user": "wuyuverse", "type": "user"}, "name": "Jiajun Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:36.195Z", "hidden": false}, {"_id": "692e667137312eaa83fd8841", "name": "Xianjie Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8842", "name": "Zhenhe Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8843", "name": "Daoguang Zan", "hidden": false}, {"_id": "692e667137312eaa83fd8844", "name": "Chenchen Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8845", "user": {"_id": "672c9ba69380700b602c46c1", "avatarUrl": "/avatars/3d0fd966df540d34095d2c84ce449180.svg", "isPro": false, "fullname": "wei zhang", "user": "zwpride", "type": "user"}, "name": "Wei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:37.970Z", "hidden": false}, {"_id": "692e667137312eaa83fd8846", "name": "He Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd8847", "user": {"_id": "62b7fb545233925f253531c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg", "isPro": false, "fullname": "Terry Yue Zhuo", "user": "terryyz", "type": "user"}, "name": "Terry Yue Zhuo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T16:50:22.285Z", "hidden": false}, {"_id": "692e667137312eaa83fd8848", "name": "Kerui Cao", "hidden": false}, {"_id": "692e667137312eaa83fd8849", "name": "Xianfu Cheng", "hidden": false}, {"_id": "692e667137312eaa83fd884a", "name": "Jun Dong", "hidden": false}, {"_id": "692e667137312eaa83fd884b", "name": "Shengjie Fang", "hidden": false}, {"_id": "692e667137312eaa83fd884c", "name": "Zhiwei Fei", "hidden": false}, {"_id": "692e667137312eaa83fd884d", "name": "Xiangyuan Guan", "hidden": false}, {"_id": "692e667137312eaa83fd884e", "name": "Qipeng Guo", "hidden": false}, {"_id": "692e667137312eaa83fd884f", "name": "Zhiguang Han", "hidden": false}, {"_id": "692e667137312eaa83fd8850", "name": "Joseph James", "hidden": false}, {"_id": "692e667137312eaa83fd8851", "name": "Tianqi Luo", "hidden": false}, {"_id": "692e667137312eaa83fd8852", "user": {"_id": "67f1037cd5f976f3d4777390", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0cXH40AcE-M-H21cSNBqZ.png", "isPro": false, "fullname": "RenyuanLi", "user": "RenyuanLi", "type": "user"}, "name": "Renyuan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:17:45.344Z", "hidden": false}, {"_id": "692e667137312eaa83fd8853", "name": "Yuhang Li", "hidden": false}, {"_id": "692e667137312eaa83fd8854", "name": "Yiming Liang", "hidden": false}, {"_id": "692e667137312eaa83fd8855", "name": "Congnan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8856", "name": "Jiaheng Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8857", "name": "Qian Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8858", "name": "Ruitong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8859", "name": "Tyler Loakman", "hidden": false}, {"_id": "692e667137312eaa83fd885a", "name": "Xiangxin Meng", "hidden": false}, {"_id": "692e667137312eaa83fd885b", "name": "Chuang Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885c", "name": "Tianhao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885d", "name": "Jiajun Shi", "hidden": false}, {"_id": "692e667137312eaa83fd885e", "name": "Mingjie Tang", "hidden": false}, {"_id": "692e667137312eaa83fd885f", "name": "Boyang Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8860", "name": "Haowen Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8861", "name": "Yunli Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8862", "user": {"_id": "668619ce7374cac565759731", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668619ce7374cac565759731/tUtiyIQRGsMdq3HB2yYIL.jpeg", "isPro": false, "fullname": "Fanglin Xu", "user": "Tswatery", "type": "user"}, "name": "Fanglin Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T15:15:03.333Z", "hidden": false}, {"_id": "692e667137312eaa83fd8863", "name": "Zihan Xu", "hidden": false}, {"_id": "692e667137312eaa83fd8864", "name": "Fei Yuan", "hidden": false}, {"_id": "692e667137312eaa83fd8865", "user": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "name": "Ge Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:34.025Z", "hidden": false}, {"_id": "692e667137312eaa83fd8866", "user": {"_id": "65f40e83653c231cbaf7defe", "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg", "isPro": false, "fullname": "Jiayi Zhang", "user": "didiforhugface", "type": "user"}, "name": "Jiayi Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:32.149Z", "hidden": false}, {"_id": "692e667137312eaa83fd8867", "name": "Xinhao Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8868", "name": "Wangchunshu Zhou", "hidden": false}, {"_id": "692e667137312eaa83fd8869", "name": "Hualei Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886a", "name": "King Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886b", "name": "Brown Dai", "hidden": false}, {"_id": "692e667137312eaa83fd886c", "name": "Aishan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd886d", "name": "Zhoujun Li", "hidden": false}, {"_id": "692e667137312eaa83fd886e", "name": "Chenghua Lin", "hidden": false}, {"_id": "692e667137312eaa83fd886f", "name": "Tianyu Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8870", "name": "Chao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd8871", "name": "Kai Shen", "hidden": false}, {"_id": "692e667137312eaa83fd8872", "name": "Libo Qin", "hidden": false}, {"_id": "692e667137312eaa83fd8873", "name": "Shuangyong Song", "hidden": false}, {"_id": "692e667137312eaa83fd8874", "name": "Zizheng Zhan", "hidden": false}, {"_id": "692e667137312eaa83fd8875", "name": "Jiajun Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8876", "name": "Jie Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8877", "name": "Zhaoxiang Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8878", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-11-23T17:09:34.000Z", "submittedOnDailyAt": "2025-12-02T02:55:07.234Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "submittedOnDailyBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "upvotes": 240, "discussionId": "692e667237312eaa83fd8879", "ai_summary": "A comprehensive guide to code LLMs, covering their lifecycle from data curation to deployment, including techniques, trade-offs, and research-practice gaps.", "ai_keywords": ["Transformer-based architectures", "HumanEval", "prompting paradigms", "code pre-training", "supervised fine-tuning", "reinforcement learning", "autonomous coding agents", "GPT-4", "Claude", "LLaMA", "StarCoder", "Code LLaMA", "DeepSeek-Coder", "QwenCoder", "code correctness", "security", "contextual awareness", "software-related code tasks", "scaling law", "framework selection", "hyperparameter sensitivity", "model architectures", "dataset comparisons"], "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6539\u53d8\u4e86\u81ea\u52a8\u8f6f\u4ef6\u5f00\u53d1\uff0c\u53ef\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u76f4\u63a5\u7ffb\u8bd1\u6210\u529f\u80fd\u4ee3\u7801\u3002</li>\n    <li>\u901a\u8fc7\u5de5\u5177\u5982Github Copilot\u7b49\uff0cLLMs\u5728\u5546\u4e1a\u4e0a\u5f97\u5230\u4e86\u5e7f\u6cdb\u5e94\u7528\uff0c\u6027\u80fd\u4ece\u4e2a\u4f4d\u6570\u63d0\u5347\u5230\u8d85\u8fc795%\u3002</li>\n    <li>\u672c\u6587\u5168\u9762\u5206\u6790\u4e86\u4ee3\u7801LLMs\u7684\u751f\u547d\u5468\u671f\uff0c\u4ece\u6570\u636e\u51c6\u5907\u5230\u8bad\u7ec3\u540e\u9636\u6bb5\uff0c\u5305\u62ec\u9ad8\u6548\u63d0\u793a\u3001\u4ee3\u7801\u9884\u8bad\u7ec3\u7b49\u3002</li>\n    <li>\u6211\u4eec\u6bd4\u8f83\u4e86\u901a\u7528LLMs\u548c\u4e13\u95e8\u7684\u4ee3\u7801LLMs\uff0c\u63a2\u8ba8\u4e86\u6280\u672f\u3001\u8bbe\u8ba1\u51b3\u7b56\u548c\u6743\u8861\u3002</li>\n    <li>\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u8fdb\u884c\u4e86\u591a\u9879\u5b9e\u9a8c\u4ee5\u8bc4\u4f30\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u7684\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) have changed software development by turning natural language into code, with tools like Github Copilot gaining popularity.</li>\n    <li>There have been significant improvements in performance, achieving over 95% success rates on coding benchmarks.</li>\n    <li>This work offers a detailed guide on code LLMs, exploring everything from data preparation to training methods and autonomous coding.</li>\n    <li>It examines both general and code-specialized LLMs, discussing their techniques and design choices.</li>\n    <li>The study highlights the gap between academic research and real-world coding challenges, suggesting ways to align research with practical needs.</li>\n</ul>"}, "publishedAt": "2025-11-23T12:09:34.000Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18538.png", "numComments": 11, "submittedBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "fullname": "Yang Jian", "name": "CSJianYang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 21}, "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>DeepSeek-V3.2\u662f\u4e00\u79cd\u9ad8\u6548\u8ba1\u7b97\u4e0e\u4f18\u8d8a\u63a8\u7406\u8868\u73b0\u76f8\u7ed3\u5408\u7684\u6a21\u578b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236(DSA)\uff0c\u5728\u957f\u6587\u672c\u573a\u666f\u4e2d\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0cDeepSeek-V3.2\u7684\u8868\u73b0\u4e0eGPT-5\u76f8\u5f53\uff0c\u7279\u522b\u7248\u8d85\u8d8a\u4e86GPT-5\uff0c\u63a8\u7406\u80fd\u529b\u63a5\u8fd1Gemini-3.0-Pro\u3002</li>\n    <li>\u57282025\u5e74\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\u548c\u4fe1\u606f\u5b66\u5965\u6797\u5339\u514b\u4e2d\u83b7\u5f97\u91d1\u724c\u8868\u73b0\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u5408\u6210\u7ba1\u9053\u6765\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6307\u4ee4\u9075\u5faa\u7684\u7a33\u5065\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that balances efficient computing with strong reasoning and performance.</li>\n    <li>It features a new attention method called DeepSeek Sparse Attention (DSA) that reduces computing needs while maintaining effectiveness in long contexts.</li>\n    <li>The model uses an advanced reinforcement learning system and performs similarly to GPT-5, with a special version outperforming it and showing excellent reasoning skills.</li>\n    <li>DeepSeek-V3.2 has achieved top performance in prestigious competitions like the International Mathematical Olympiad and the International Olympiad in Informatics.</li>\n    <li>It includes a unique data generation pipeline that helps improve the model's ability to follow instructions and adapt in complex environments.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.22699", "authors": [{"_id": "692d06234397b1ec214f6788", "name": "Z-Image Team", "hidden": false}, {"_id": "692d06234397b1ec214f6789", "user": {"_id": "692d0e6bb14ceb758205d0dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/692d0e6bb14ceb758205d0dd/gGVq2KSJE11Sr3LkVn-n5.jpeg", "isPro": false, "fullname": "Huanqia Cai", "user": "Orion-Cai", "type": "user"}, "name": "Huanqia Cai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:26.669Z", "hidden": false}, {"_id": "692d06234397b1ec214f678a", "user": {"_id": "67777b7a8376dfe003afa951", "avatarUrl": "/avatars/2af9d3181306d4c53329d047eeadaf1e.svg", "isPro": false, "fullname": "Sihan Cao", "user": "Sihan-Cao", "type": "user"}, "name": "Sihan Cao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:33.191Z", "hidden": false}, {"_id": "692d06234397b1ec214f678b", "user": {"_id": "64a54586c0f13de8e7093314", "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg", "isPro": false, "fullname": "Ruoyi Du", "user": "RuoyiDu", "type": "user"}, "name": "Ruoyi Du", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:18:53.948Z", "hidden": false}, {"_id": "692d06234397b1ec214f678c", "name": "Peng Gao", "hidden": false}, {"_id": "692d06234397b1ec214f678d", "name": "Steven Hoi", "hidden": false}, {"_id": "692d06234397b1ec214f678e", "name": "Shijie Huang", "hidden": false}, {"_id": "692d06234397b1ec214f678f", "name": "Zhaohui Hou", "hidden": false}, {"_id": "692d06234397b1ec214f6790", "user": {"_id": "662a0f2d4bab737c1a279843", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662a0f2d4bab737c1a279843/fC2p3mjMHkVpDQdEqkuR4.png", "isPro": false, "fullname": "Dengyang Jiang", "user": "DyJiang", "type": "user"}, "name": "Dengyang Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:15.555Z", "hidden": false}, {"_id": "692d06234397b1ec214f6791", "user": {"_id": "6537e8eab01250d1d6efed3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gMx73gwdfEhcCFioStGCE.jpeg", "isPro": false, "fullname": "Xin", "user": "Srameo", "type": "user"}, "name": "Xin Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:15.288Z", "hidden": false}, {"_id": "692d06234397b1ec214f6792", "name": "Liangchen Li", "hidden": false}, {"_id": "692d06234397b1ec214f6793", "user": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "name": "Zhen Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:16.899Z", "hidden": false}, {"_id": "692d06234397b1ec214f6794", "user": {"_id": "6740a5730bb4a675446a80ad", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740a5730bb4a675446a80ad/dmruwMdQK3zluJm7YXUtN.jpeg", "isPro": false, "fullname": "Zhong-Yu Li", "user": "lzyhha", "type": "user"}, "name": "Zhong-Yu Li", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:08.972Z", "hidden": false}, {"_id": "692d06234397b1ec214f6795", "name": "David Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6796", "name": "Dongyang Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6797", "user": {"_id": "66332475351231c428653b6b", "avatarUrl": "/avatars/3997bcde54158f7ff9770c85a20875f1.svg", "isPro": false, "fullname": "Junhan Shi", "user": "jshmsjh", "type": "user"}, "name": "Junhan Shi", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:38.865Z", "hidden": false}, {"_id": "692d06234397b1ec214f6798", "user": {"_id": "64379d79fac5ea753f1c10f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png", "isPro": false, "fullname": "Jerry Wu", "user": "QJerry", "type": "user"}, "name": "Qilong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:18.709Z", "hidden": false}, {"_id": "692d06234397b1ec214f6799", "name": "Feng Yu", "hidden": false}, {"_id": "692d06234397b1ec214f679a", "name": "Chi Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679b", "name": "Shifeng Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679c", "user": {"_id": "641988978e0baaeed5a066c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641988978e0baaeed5a066c6/TdCjJ63gw5gdX1RqTvy9a.png", "isPro": false, "fullname": "Shilin", "user": "zsLin", "type": "user"}, "name": "Shilin Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T16:24:44.624Z", "hidden": false}], "publishedAt": "2025-11-27T18:52:07.000Z", "submittedOnDailyAt": "2025-12-01T00:38:17.269Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "submittedOnDailyBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "upvotes": 155, "discussionId": "692d06234397b1ec214f679d", "projectPage": "https://tongyi-mai.github.io/Z-Image-blog/", "githubRepo": "https://github.com/Tongyi-MAI/Z-Image", "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.", "ai_keywords": ["Scalable Single-Stream Diffusion Transformer", "S3-DiT", "diffusion transformer", "omni-pre-training", "instruction-following capabilities", "photorealistic image generation", "bilingual text rendering", "distillation scheme", "reward post-training", "H800 GPU", "VRAM"], "githubStars": 5595, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "summary_zh": "<ul>\n    <li>\u5f53\u524d\u9ad8\u6027\u80fd\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e3b\u8981\u7531\u4e13\u6709\u7cfb\u7edf\u4e3b\u5bfc\uff0c\u5982Nano Banana Pro\u548cSeedream 4.0\u3002</li>\n    <li>\u5f00\u6e90\u66ff\u4ee3\u65b9\u6848\u5982Qwen-Image\u548cHunyuan-Image-3.0\u53c2\u6570\u91cf\u5de8\u5927\uff0820B\u523080B\uff09\uff0c\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u96be\u4ee5\u4f7f\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Z-Image\uff0c\u4e00\u4e2a\u9ad8\u6548\u76846B\u53c2\u6570\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5355\u6d41\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u3002</li>\n    <li>Z-Image\u5b8c\u6210\u6574\u4e2a\u8bad\u7ec3\u6d41\u7a0b\u4ec5\u9700314K H800 GPU\u5c0f\u65f6\uff0c\u4e14\u652f\u6301\u6d88\u8d39\u7ea7\u786c\u4ef6\u3002</li>\n    <li>\u6211\u4eec\u7684\u6a21\u578b\u5728\u903c\u771f\u56fe\u50cf\u751f\u6210\u548c\u53cc\u8bed\u6587\u672c\u6e32\u67d3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u516c\u5f00\u53d1\u5e03\u4ee3\u7801\u548c\u5728\u7ebf\u6f14\u793a\u4ee5\u4fc3\u8fdb\u5f00\u53d1\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>High-performance image generation models are mostly proprietary, like Nano Banana Pro and Seedream 4.0, with some open-source options being too large for regular use.</li>\n    <li>Z-Image is a new 6B-parameter model that uses a unique architecture to offer a more efficient alternative to larger models.</li>\n    <li>The model was trained in just 314K GPU hours, costing about $630K, using optimized processes for better performance.</li>\n    <li>Z-Image-Turbo allows fast image generation on both high-end and consumer-grade hardware.</li>\n    <li>The model performs very well in generating realistic images and bilingual text, competing with top commercial models while being more accessible and cost-effective.</li>\n</ul>"}, "publishedAt": "2025-11-27T13:52:07.000Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png", "numComments": 3, "submittedBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "fullname": "Zhen Li", "name": "Paper99", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 29}, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20626", "authors": [{"_id": "6927ab26243b2216fb75cd1b", "name": "Wei He", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1c", "user": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "name": "Kai Han", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:11.052Z", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1d", "name": "Hang Zhou", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1e", "name": "Hanting Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1f", "name": "Zhicheng Liu", "hidden": false}, {"_id": "6927ab26243b2216fb75cd20", "name": "Xinghao Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd21", "name": "Yunhe Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "publishedAt": "2025-11-25T18:48:05.000Z", "submittedOnDailyAt": "2025-11-26T23:08:13.066Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "submittedOnDailyBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "upvotes": 154, "discussionId": "6927ab27243b2216fb75cd22", "projectPage": "https://github.com/huawei-noah/noah-research/tree/master/ROOT", "githubRepo": "https://github.com/huawei-noah/noah-research", "ai_summary": "ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.", "ai_keywords": ["large language models", "LLMs", "momentum orthogonalization", "dimensional fragility", "outlier-induced noise", "adaptive Newton iterations", "proximal optimization", "Muon", "Adam-based optimizers", "robust optimizer"], "githubStars": 909, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u52a0\u5267\u4e86\u7b97\u6cd5\u4e0d\u7cbe\u786e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668ROOT\uff0c\u65e8\u5728\u901a\u8fc7\u53cc\u91cd\u7a33\u5065\u673a\u5236\u6765\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>ROOT\u91c7\u7528\u81ea\u9002\u5e94\u725b\u987f\u8fed\u4ee3\u7684\u7ef4\u5ea6\u7a33\u5065\u6b63\u4ea4\u5316\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4e0d\u540c\u67b6\u6784\u914d\u7f6e\u4e0b\u7684\u7cbe\u786e\u5ea6\u3002</li>\n    <li>ROOT\u8fd8\u901a\u8fc7\u90bb\u8fd1\u4f18\u5316\u6846\u67b6\u6291\u5236\u5f02\u5e38\u503c\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u610f\u4e49\u7684\u68af\u5ea6\u65b9\u5411\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cROOT\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e2d\u6bd4Muon\u548c\u57fa\u4e8eAdam\u7684\u4f18\u5316\u5668\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) face challenges with training stability and precision as they grow in size.</li>\n    <li>Recent optimizers have improved efficiency but struggle with robustness against noise and dimensional changes.</li>\n    <li>ROOT is a new optimizer that strengthens training stability using two innovative methods: dimension-robust orthogonalization and noise suppression.</li>\n    <li>Experiments show ROOT outperforms other optimizers like Muon and Adam in tough conditions, achieving faster training and better results.</li>\n    <li>The ROOT code will be available online for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:48:05.000Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20626.png", "numComments": 2, "submittedBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "fullname": "hankai", "name": "hankaixyz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20785", "authors": [{"_id": "692d430f4397b1ec214f696e", "user": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "name": "Zuhao Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:11.311Z", "hidden": false}, {"_id": "692d430f4397b1ec214f696f", "user": {"_id": "6690f58e2f9f6f9c88e91031", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6690f58e2f9f6f9c88e91031/QQ_VoEh7NlE6BUvii08zk.png", "isPro": false, "fullname": "Sudong Wang", "user": "xiao45791", "type": "user"}, "name": "Sudong Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:14.173Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6970", "user": {"_id": "64bb77e786e7fb5b8a317a43", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bb77e786e7fb5b8a317a43/J0jOrlZJ9gazdYaeSH2Bo.png", "isPro": false, "fullname": "kcz", "user": "kcz358", "type": "user"}, "name": "Kaichen Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T10:06:41.343Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6971", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:09.461Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6972", "name": "Sicong Leng", "hidden": false}, {"_id": "692d430f4397b1ec214f6973", "name": "Yifan Zhang", "hidden": false}, {"_id": "692d430f4397b1ec214f6974", "name": "Chengwei Qin", "hidden": false}, {"_id": "692d430f4397b1ec214f6975", "name": "Shijian Lu", "hidden": false}, {"_id": "692d430f4397b1ec214f6976", "name": "Xingxuan Li", "hidden": false}, {"_id": "692d430f4397b1ec214f6977", "user": {"_id": "6454685a548f22be598414c4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg", "isPro": false, "fullname": "Lidong Bing", "user": "LidongBing", "type": "user"}, "name": "Lidong Bing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:49:36.056Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "publishedAt": "2025-11-25T19:22:48.000Z", "submittedOnDailyAt": "2025-12-02T00:35:56.511Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "submittedOnDailyBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "upvotes": 148, "discussionId": "692d430f4397b1ec214f6978", "projectPage": "https://evolvinglmms-lab.github.io/LongVT/", "githubRepo": "https://github.com/EvolvingLMMs-Lab/LongVT", "ai_summary": "LongVT, an end-to-end framework, enhances long video reasoning by interleaving global and local analysis using multimodal tools, outperforming existing methods on challenging benchmarks.", "ai_keywords": ["multimodal models", "video reasoning", "textual Chain-of-Thought", "hallucinations", "long-form videos", "temporal grounding", "video cropping", "fine-grained question-answering", "VideoSIAH", "tool-integrated cold-start supervised fine-tuning", "agentic reinforcement learning", "agentic reinforcement fine-tuning"], "githubStars": 121, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u89c6\u9891\u63a8\u7406\u65b9\u9762\u5c55\u73b0\u51fa\u5f88\u5927\u6f5c\u529b\uff0c\u4f46\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86LongVT\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u66ff\u7684\u591a\u6a21\u6001\u601d\u7ef4\u94fe\uff0c\u5e2e\u52a9\u7406\u89e3\u957f\u89c6\u9891\u3002</li>\n    <li>LongVT\u5229\u7528LMMs\u7684\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\uff0c\u53ef\u4ee5\u805a\u7126\u4e8e\u7279\u5b9a\u89c6\u9891\u7247\u6bb5\u8fdb\u884c\u8be6\u7ec6\u5206\u6790\u3002</li>\n    <li>\u4e3a\u4e86\u652f\u6301\u957f\u89c6\u9891\u63a8\u7406\u4efb\u52a1\uff0c\u6211\u4eec\u5c06\u53d1\u5e03\u540d\u4e3aVideoSIAH\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u5927\u91cf\u95ee\u7b54\u6837\u672c\u3002</li>\n    <li>LongVT\u5728\u56db\u4e2a\u6311\u6218\u6027\u7684\u957f\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u53ef\u516c\u5f00\u83b7\u53d6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large multimodal models (LMMs) are good at video reasoning but can make mistakes, especially with long videos.</li>\n    <li>LongVT is a new framework that helps these models think better about long videos by focusing on important clips.</li>\n    <li>It uses a method similar to how humans watch videos: first looking at the whole video, then zooming in on details.</li>\n    <li>A new dataset called VideoSIAH will be released to help train and test this framework, containing a large number of samples for different tasks.</li>\n    <li>LongVT has been shown to perform better than existing methods in various tests, and its resources are available online.</li>\n</ul>"}, "publishedAt": "2025-11-25T14:22:48.000Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20785.png", "numComments": 3, "submittedBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "fullname": "Zuhao Yang", "name": "mwxely", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u8ba1\u7b97\u901f\u5ea6\u548c\u957f\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\uff0c\u5f71\u54cd\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u5934\u50cf\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Live Avatar\uff0c\u4e00\u4e2a\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u3001\u65e0\u9650\u957f\u5ea6\u7684\u5934\u50cf\u751f\u6210\u6846\u67b6\uff0c\u57fa\u4e8e\u4e00\u4e2a140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\uff08TPP\uff09\u6280\u672f\uff0c\u5b9e\u73b0\u591a\u4e2aGPU\u95f4\u7684\u53bb\u566a\u6b65\u9aa4\u6d41\u6c34\u7ebf\uff0c\u63d0\u9ad8\u4e86\u5b9e\u65f6\u6d41\u5a92\u4f53\u7684\u7a33\u5b9a\u6027\u548c\u4f4e\u5ef6\u8fdf\u3002</li>\n    <li>\u4e3a\u4e86\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6eda\u52a8\u53c2\u8003\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5916\u89c2\u6765\u4fdd\u6301\u5e8f\u5217\u7684\u7cbe\u51c6\u5ea6\u3002</li>\n    <li>Live Avatar\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e8620\u5e27\u6bcf\u79d2\u7684\u751f\u6210\u901f\u5ea6\uff0c\u5f00\u521b\u4e86\u5de5\u4e1a\u957f\u89c6\u9891\u5408\u6210\u5e94\u7528\u7684\u65b0\u6a21\u5f0f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new system for creating high-quality, real-time avatars using a powerful diffusion model with 14 billion parameters.</li>\n    <li>The system uses a technique called Timestep-forcing Pipeline Parallelism (TPP) to speed up processing by using multiple GPUs, allowing for smooth and quick video generation.</li>\n    <li>To improve video quality and consistency, it incorporates the Rolling Sink Frame Mechanism (RSFM), which helps maintain visual fidelity by using a reference image.</li>\n    <li>Live Avatar can generate videos at 20 frames per second on 5 GPUs, making it one of the fastest and most advanced systems for creating avatars in real time.</li>\n    <li>This work sets a new standard for using advanced video generation models in practical applications, like long-form video synthesis.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2511.18423", "authors": [{"_id": "692518ff16eb3a9f1310391c", "name": "B. Y. Yan", "hidden": false}, {"_id": "692518ff16eb3a9f1310391d", "name": "Chaofan Li", "hidden": false}, {"_id": "692518ff16eb3a9f1310391e", "name": "Hongjin Qian", "hidden": false}, {"_id": "692518ff16eb3a9f1310391f", "user": {"_id": "6145b3fd35135ec7e8d4ca45", "avatarUrl": "/avatars/5dc25d18d6a8418c9b1a29ece9a48f5a.svg", "isPro": false, "fullname": "Shuqi Lu", "user": "shuqi", "type": "user"}, "name": "Shuqi Lu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:18:11.163Z", "hidden": false}, {"_id": "692518ff16eb3a9f13103920", "user": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "name": "Zheng Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:17:59.618Z", "hidden": false}], "publishedAt": "2025-11-23T12:29:33.000Z", "submittedOnDailyAt": "2025-11-25T00:25:04.757Z", "title": "General Agentic Memory Via Deep Research", "submittedOnDailyBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "upvotes": 140, "discussionId": "692518ff16eb3a9f13103921", "projectPage": "https://github.com/VectorSpaceLab/general-agentic-memory", "githubRepo": "https://github.com/VectorSpaceLab/general-agentic-memory", "ai_summary": "GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.", "ai_keywords": ["general agentic memory", "GAM", "just-in time compilation", "JIT compilation", "memorizer", "researcher", "universal page-store", "large language models", "LLMs", "reinforcement learning"], "githubStars": 246, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u5185\u5b58\u5bf9\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u9759\u6001\u5185\u5b58\u5bb9\u6613\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u79f0\u4e3a\u4e00\u822c\u4ee3\u7406\u5185\u5b58\uff08GAM\uff09\uff0c\u91c7\u7528\u201c\u53ca\u65f6\u7f16\u8bd1\u201d\uff08JIT\uff09\u539f\u5219\u3002</li>\n    <li>GAM\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u8bb0\u5fc6\u5668\u548c\u7814\u7a76\u8005\uff0c\u5206\u522b\u7528\u4e8e\u7ba1\u7406\u5386\u53f2\u4fe1\u606f\u548c\u5728\u7ebf\u68c0\u7d22\u6709\u7528\u4fe1\u606f\u3002</li>\n    <li>\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u6709\u6548\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6574\u4f53\u6027\u80fd\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cGAM\u5728\u591a\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684\u4efb\u52a1\u5b8c\u6210\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5185\u5b58\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Memory is very important for AI agents, but traditional static memory can lead to significant information loss.</li>\n  <li>We introduce a new system called general agentic memory (GAM) that creates memory when needed, rather than in advance.</li>\n  <li>GAM consists of two main parts: a Memorizer that keeps important past information, and a Researcher that finds useful information when needed.</li>\n  <li>This system helps AI agents use large language models more effectively and improves their overall performance.</li>\n  <li>Tests show that GAM performs much better in memory-related tasks compared to existing memory systems.</li>\n</ul>"}, "publishedAt": "2025-11-23T07:29:33.000Z", "title": "General Agentic Memory Via Deep Research", "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18423.png", "numComments": 2, "submittedBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "fullname": "zhengliu", "name": "lz1001", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp\u662f\u4e00\u4e2a\u5305\u542b210\u9879\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6a21\u62df\u4e86\u4f01\u4e1a\u6570\u636e\u667a\u80fd\u7684\u590d\u6742\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u9700\u8981\u5728\u5de5\u4e1a\u6a21\u5f0f\u4e0a\u8fdb\u884c\u4ed3\u5e93\u7ea7\u7684\u5de5\u7a0b\uff0c\u5305\u62ec\u4ece\u5934\u8bbe\u8ba1\u591a\u9636\u6bb5SQL\u7ba1\u9053\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\u4efb\u52a1\u6d89\u53ca\u5f00\u653e\u5f0f\u5546\u4e1a\u95ee\u9898\uff0c\u9700\u8981\u6218\u7565\u89c4\u5212\u548c\u8fed\u4ee3\u5206\u6790\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709\u7684\u5148\u8fdb\u4ee3\u7406\u5728DAComp\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u4e0a\u6210\u529f\u7387\u4f4e\u4e8e20%\u3002</li>\n    <li>DAComp\u5e2e\u52a9\u8bc6\u522b\u6570\u636e\u5de5\u7a0b\u548c\u5206\u6790\u7684\u80fd\u529b\u5dee\u5f02\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u5e73\u53f0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DAComp is a benchmark with 210 tasks that reflect real-world data workflows in companies.</li>\n    <li>Data engineering tasks involve creating and managing complex SQL pipelines and adapting to changing requirements.</li>\n    <li>Data analysis tasks focus on solving open-ended business problems through strategic planning and coding.</li>\n    <li>Results show that even advanced agents struggle with DAComp, especially in engineering tasks with success rates below 20%.</li>\n    <li>DAComp helps identify weaknesses in data processing and analysis, aiming to improve autonomous data agents for businesses.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.21631", "authors": [{"_id": "692ffb1a26742347f61daf38", "user": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "isPro": false, "fullname": "shuai bai", "user": "ShuaiBai623", "type": "user"}, "name": "Shuai Bai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:34:29.118Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf39", "name": "Yuxuan Cai", "hidden": false}, {"_id": "692ffb1a26742347f61daf3a", "name": "Ruizhe Chen", "hidden": false}, {"_id": "692ffb1a26742347f61daf3b", "name": "Keqin Chen", "hidden": false}, {"_id": "692ffb1a26742347f61daf3c", "user": {"_id": "63f30b870a16587ea970edfe", "avatarUrl": "/avatars/b58ab2d8a85a6d83462c297de2714ce4.svg", "isPro": false, "fullname": "Xiong-Hui Chen", "user": "xionghuichen", "type": "user"}, "name": "Xionghui Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:35:42.689Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf3d", "user": {"_id": "65b2529285b6c21448a10d65", "avatarUrl": "/avatars/1b09e2742aecce1bbdc57f0c4504cf38.svg", "isPro": false, "fullname": "Zesen Cheng", "user": "ClownRat", "type": "user"}, "name": "Zesen Cheng", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:35:51.365Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf3e", "name": "Lianghao Deng", "hidden": false}, {"_id": "692ffb1a26742347f61daf3f", "name": "Wei Ding", "hidden": false}, {"_id": "692ffb1a26742347f61daf40", "name": "Chang Gao", "hidden": false}, {"_id": "692ffb1a26742347f61daf41", "name": "Chunjiang Ge", "hidden": false}, {"_id": "692ffb1a26742347f61daf42", "name": "Wenbin Ge", "hidden": false}, {"_id": "692ffb1a26742347f61daf43", "name": "Zhifang Guo", "hidden": false}, {"_id": "692ffb1a26742347f61daf44", "user": {"_id": "656f1b21b075b63c90ba02ee", "avatarUrl": "/avatars/d6856815ef06261394178161e4d511b4.svg", "isPro": false, "fullname": "Huang Qidong", "user": "shikiw", "type": "user"}, "name": "Qidong Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:49.065Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf45", "name": "Jie Huang", "hidden": false}, {"_id": "692ffb1a26742347f61daf46", "name": "Fei Huang", "hidden": false}, {"_id": "692ffb1a26742347f61daf47", "name": "Binyuan Hui", "hidden": false}, {"_id": "692ffb1a26742347f61daf48", "name": "Shutong Jiang", "hidden": false}, {"_id": "692ffb1a26742347f61daf49", "name": "Zhaohai Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4a", "name": "Mingsheng Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4b", "name": "Mei Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4c", "user": {"_id": "6346be8f7fb9f11870c63998", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6346be8f7fb9f11870c63998/tFWawSkXL6bv1zgvzFWQd.png", "isPro": false, "fullname": "Kaixin Li", "user": "likaixin", "type": "user"}, "name": "Kaixin Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:53.648Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf4d", "user": {"_id": "67a31313cf9d856beb7f9afb", "avatarUrl": "/avatars/69395b134716f750545eab35a164e51f.svg", "isPro": false, "fullname": "Zicheng Lin", "user": "etonlin", "type": "user"}, "name": "Zicheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:36:50.803Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf4e", "name": "Junyang Lin", "hidden": false}, {"_id": "692ffb1a26742347f61daf4f", "name": "Xuejing Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf50", "name": "Jiawei Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf51", "name": "Chenglong Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf52", "name": "Yang Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf53", "name": "Dayiheng Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf54", "user": {"_id": "64e72776e9fc9d0475ef5188", "avatarUrl": "/avatars/d32b9d4e1da5486c3d5f9b04fa29d167.svg", "isPro": false, "fullname": "Shixuan Liu", "user": "liusx", "type": "user"}, "name": "Shixuan Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:58.470Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf55", "name": "Dunjie Lu", "hidden": false}, {"_id": "692ffb1a26742347f61daf56", "name": "Ruilin Luo", "hidden": false}, {"_id": "692ffb1a26742347f61daf57", "name": "Chenxu Lv", "hidden": false}, {"_id": "692ffb1a26742347f61daf58", "name": "Rui Men", "hidden": false}, {"_id": "692ffb1a26742347f61daf59", "name": "Lingchen Meng", "hidden": false}, {"_id": "692ffb1a26742347f61daf5a", "name": "Xuancheng Ren", "hidden": false}, {"_id": "692ffb1a26742347f61daf5b", "name": "Xingzhang Ren", "hidden": false}, {"_id": "692ffb1a26742347f61daf5c", "name": "Sibo Song", "hidden": false}, {"_id": "692ffb1a26742347f61daf5d", "name": "Yuchong Sun", "hidden": false}, {"_id": "692ffb1a26742347f61daf5e", "name": "Jun Tang", "hidden": false}, {"_id": "692ffb1a26742347f61daf5f", "name": "Jianhong Tu", "hidden": false}, {"_id": "692ffb1a26742347f61daf60", "name": "Jianqiang Wan", "hidden": false}, {"_id": "692ffb1a26742347f61daf61", "name": "Peng Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf62", "name": "Pengfei Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf63", "name": "Qiuyue Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf64", "name": "Yuxuan Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf65", "name": "Tianbao Xie", "hidden": false}, {"_id": "692ffb1a26742347f61daf66", "name": "Yiheng Xu", "hidden": false}, {"_id": "692ffb1a26742347f61daf67", "user": {"_id": "645b10e80c73ea27d13f7aca", "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg", "isPro": false, "fullname": "xuhaiyang", "user": "xhyandwyy", "type": "user"}, "name": "Haiyang Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:51.583Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf68", "name": "Jin Xu", "hidden": false}, {"_id": "692ffb1a26742347f61daf69", "name": "Zhibo Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6a", "name": "Mingkun Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6b", "name": "Jianxin Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6c", "name": "An Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6d", "name": "Bowen Yu", "hidden": false}, {"_id": "692ffb1a26742347f61daf6e", "name": "Fei Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6f", "name": "Hang Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf70", "name": "Xi Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf71", "name": "Bo Zheng", "hidden": false}, {"_id": "692ffb1a26742347f61daf72", "name": "Humen Zhong", "hidden": false}, {"_id": "692ffb1a26742347f61daf73", "name": "Jingren Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf74", "name": "Fan Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf75", "name": "Jing Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf76", "user": {"_id": "627d2723401f42c57b6b7c0c", "avatarUrl": "/avatars/6ff754e56aaee63d8572881a6a966171.svg", "isPro": false, "fullname": "Yuanzhi Zhu", "user": "Yuanzhi", "type": "user"}, "name": "Yuanzhi Zhu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:36:59.879Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf77", "name": "Ke Zhu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63451cf0a05b51f7ded25505/kVvzrCGrjwoK0CYh4DKif.jpeg"], "publishedAt": "2025-11-26T17:59:08.000Z", "submittedOnDailyAt": "2025-12-04T01:02:46.772Z", "title": "Qwen3-VL Technical Report", "submittedOnDailyBy": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "isPro": false, "fullname": "shuai bai", "user": "ShuaiBai623", "type": "user"}, "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.", "upvotes": 110, "discussionId": "692ffb1b26742347f61daf78", "ai_summary": "Qwen3-VL, a vision-language model, excels in text and multimodal understanding through advanced architectures and larger contexts, achieving superior performance across benchmarks.", "ai_keywords": ["vision-language model", "interleaved contexts", "multimodal benchmarks", "dense variants", "mixture-of-experts", "pure-text understanding", "long-context comprehension", "multimodal reasoning", "MMMU", "visual-math benchmarks", "interleaved-MRoPE", "DeepStack", "text-based time alignment", "T-RoPE", "explicit textual timestamp alignment", "vision-language alignment", "image-grounded reasoning", "agentic decision-making", "multimodal code intelligence"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>Qwen3-VL\u662fQwen\u7cfb\u5217\u4e2d\u6700\u5f3a\u5927\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u6027\u80fd\u5728\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002</li>\n    <li>\u6a21\u578b\u652f\u6301\u6700\u591a256K\u4e2a\u6807\u8bb0\u7684\u4ea4\u9519\u4e0a\u4e0b\u6587\uff0c\u53ef\u4ee5\u65e0\u7f1d\u6574\u5408\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u3002</li>\n    <li>\u6709\u4e0d\u540c\u7248\u672c\u53ef\u4f9b\u9009\u62e9\uff0c\u5305\u62ec\u5bc6\u96c6\u6a21\u578b\u548c\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u5ef6\u8fdf\u548c\u8d28\u91cf\u9700\u6c42\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u6587\u672c\u7406\u89e3\u3001\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u957f\u6587\u6863\u548c\u89c6\u9891\u5904\u7406\u4e0a\u3002</li>\n    <li>\u5f15\u5165\u4e86\u4e09\u4e2a\u5173\u952e\u5347\u7ea7\uff0c\u589e\u5f3a\u4e86\u7a7a\u95f4-\u65f6\u95f4\u5efa\u6a21\uff0c\u6539\u8fdb\u4e86\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u89c6\u9891\u65f6\u95f4\u5bf9\u9f50\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Qwen3-VL is a powerful vision-language model that excels in various multimodal tasks, handling text, images, and videos effectively.</li>\n    <li>It can process up to 256,000 tokens at once, improving understanding of long documents and videos.</li>\n    <li>The model comes in different sizes to balance speed and quality, catering to different needs.</li>\n    <li>Key improvements include better handling of spatial and temporal information in images and videos, enhanced alignment between vision and language, and improved timing accuracy for video content.</li>\n    <li>Qwen3-VL is designed to support advanced reasoning and decision-making in real-world applications involving multiple types of media.</li>\n</ul>"}, "publishedAt": "2025-11-26T12:59:08.000Z", "title": "Qwen3-VL Technical Report", "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63451cf0a05b51f7ded25505/kVvzrCGrjwoK0CYh4DKif.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21631.png", "numComments": 3, "submittedBy": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "fullname": "shuai bai", "name": "ShuaiBai623", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 42}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.21689", "authors": [{"_id": "692f11ffbfc6eea1b6fb6900", "name": "Hongjin Su", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6901", "name": "Shizhe Diao", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6902", "name": "Ximing Lu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6903", "name": "Mingjie Liu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6904", "name": "Jiacheng Xu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6905", "name": "Xin Dong", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6906", "name": "Yonggan Fu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6907", "name": "Peter Belcak", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6908", "name": "Hanrong Ye", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6909", "name": "Hongxu Yin", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690a", "name": "Yi Dong", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690b", "name": "Evelina Bakhturina", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690c", "name": "Tao Yu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690d", "name": "Yejin Choi", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690e", "name": "Jan Kautz", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690f", "name": "Pavlo Molchanov", "hidden": false}], "publishedAt": "2025-11-26T18:59:46.000Z", "submittedOnDailyAt": "2025-12-03T13:46:43.945Z", "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration", "submittedOnDailyBy": {"_id": "633bd54b00732349209a18fe", "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg", "isPro": false, "fullname": "Shizhe Diao", "user": "shizhediao", "type": "user"}, "summary": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.", "upvotes": 96, "discussionId": "692f11ffbfc6eea1b6fb6910", "projectPage": "https://research.nvidia.com/labs/lpr/ToolOrchestra/", "githubRepo": "https://github.com/NVlabs/ToolOrchestra/", "ai_summary": "A small orchestrator using ToolOrchestra method coordinates various intelligent tools with reinforcement learning, achieving higher accuracy and efficiency in solving complex tasks like Humanity's Last Exam compared to larger models.", "ai_keywords": ["large language models", "ToolOrchestra", "reinforcement learning", "outcome-aware rewards", "efficiency-aware rewards", "user-preference-aware rewards", "Orchestrator", "tool-use agents", "humanity's last exam", "tau2-bench", "FRAMES", "tool-augmented reasoning systems"], "githubStars": 297, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u95ee\u9898\u65f6\u4ecd\u7136\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u4eba\u7c7b\u6700\u540e\u8003\u8bd5\uff08HLE\uff09\u7b49\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86ToolOrchestra\uff0c\u4e00\u79cd\u8bad\u7ec3\u5c0f\u578b\u534f\u8c03\u8005\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u7ba1\u7406\u667a\u80fd\u5de5\u5177\u3002</li>\n    <li>\u901a\u8fc7\u4f7f\u7528ToolOrchestra\uff0c\u6211\u4eec\u5f00\u53d1\u4e86Orchestrator\uff0c\u4e00\u4e2a8B\u6a21\u578b\uff0c\u5176\u51c6\u786e\u7387\u66f4\u9ad8\u4e14\u6210\u672c\u66f4\u4f4e\u3002</li>\n    <li>\u5728HLE\u6d4b\u8bd5\u4e2d\uff0cOrchestrator\u53d6\u5f97\u4e8637.1%\u7684\u5206\u6570\uff0c\u8d85\u8d8a\u4e86GPT-5\uff0c\u5e76\u4e14\u6548\u7387\u63d0\u9ad8\u4e862.5\u500d\u3002</li>\n    <li>Orchestrator\u5728\u591a\u4e2a\u6307\u6807\u4e0b\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u6210\u672c\u7684\u6700\u4f73\u5e73\u8861\uff0c\u5e76\u80fd\u6709\u6548\u9002\u5e94\u672a\u89c1\u8fc7\u7684\u5de5\u5177\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models are strong but struggle with complex problems, like Humanity's Last Exam (HLE).</li>\n    <li>ToolOrchestra is a method that trains small models to manage and coordinate various intelligent tools.</li>\n    <li>The Orchestrator model, created using ToolOrchestra, is more accurate and cost-effective than previous models.</li>\n    <li>Orchestrator scored 37.1% on HLE, beating GPT-5's 35.1% score while being 2.5 times more efficient.</li>\n    <li>This approach shows that using a small orchestration model with different tools is better for performance and cost-effectiveness.</li>\n</ul>"}, "publishedAt": "2025-11-26T13:59:46.000Z", "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration", "summary": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21689.png", "numComments": 3, "submittedBy": {"_id": "633bd54b00732349209a18fe", "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg", "fullname": "Shizhe Diao", "name": "shizhediao", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Dec 19, 2025";
window.trendingPapers = {
    "today": [{"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u7528\u6237\u5e0c\u671b\u5b83\u4eec\u4e0d\u4ec5\u63d0\u4f9b\u51c6\u786e\u56de\u7b54\uff0c\u8fd8\u80fd\u7b26\u5408\u591a\u6837\u5316\u7684\u4eba\u7c7b\u504f\u597d\u3002</li>\n    <li>\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5f00\u59cb\u5f15\u5165\u591a\u79cd\u5956\u52b1\u6765\u6307\u5bfc\u6a21\u578b\u7684\u884c\u4e3a\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u76f4\u63a5\u5e94\u7528GRPO\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u4e0d\u540c\u5956\u52b1\u7ec4\u5408\u7684\u4f18\u52bf\u503c\u53d8\u5f97\u76f8\u540c\uff0c\u4ece\u800c\u5f71\u54cd\u8bad\u7ec3\u6548\u679c\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5GDPO\uff0c\u901a\u8fc7\u89e3\u8026\u4e2a\u522b\u5956\u52b1\u7684\u5f52\u4e00\u5316\uff0c\u4fdd\u6301\u5b83\u4eec\u7684\u76f8\u5bf9\u5dee\u5f02\uff0c\u6539\u5584\u591a\u5956\u52b1\u4f18\u5316\u7684\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\uff0cGDPO\u5728\u51c6\u786e\u6027\u548c\u7ea6\u675f\u9075\u5b88\u65b9\u9762\u90fd\u4f18\u4e8eGRPO\uff0c\u663e\u793a\u51fa\u5176\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Users want language models to give accurate answers and behave according to different human preferences.</li>\n    <li>Reinforcement learning (RL) uses multiple rewards to guide models toward desired behaviors.</li>\n    <li>Applying Group Relative Policy Optimization (GRPO) to these rewards can cause problems, leading to poor training outcomes.</li>\n    <li>The paper introduces a new method called Group reward-Decoupled Normalization Policy Optimization (GDPO) that improves training by keeping rewards separate and better preserving their differences.</li>\n    <li>Tests show that GDPO works better than GRPO in various tasks, improving both accuracy and adherence to constraints.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.04890", "authors": [{"_id": "69608e7c5b7998385e639583", "user": {"_id": "64670db15993aa7666cc6022", "avatarUrl": "/avatars/b68caad7e987c095b0cab4d9035aac25.svg", "isPro": false, "fullname": "Maksim Velikanov", "user": "yellowvm", "type": "user"}, "name": "Maksim Velikanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:44.975Z", "hidden": false}, {"_id": "69608e7c5b7998385e639584", "user": {"_id": "6697a9fb6d173ec7382e0392", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697a9fb6d173ec7382e0392/Q_myrIBbdWI3RmEvEHrtQ.jpeg", "isPro": false, "fullname": "Ilyas Chahed", "user": "IChahed", "type": "user"}, "name": "Ilyas Chahed", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T15:46:04.314Z", "hidden": false}, {"_id": "69608e7c5b7998385e639585", "user": {"_id": "6460c3811db65f878513bcaf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg", "isPro": false, "fullname": "Jingwei Zuo", "user": "JingweiZuo", "type": "user"}, "name": "Jingwei Zuo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:34:49.874Z", "hidden": false}, {"_id": "69608e7c5b7998385e639586", "name": "Dhia Eddine Rhaiem", "hidden": false}, {"_id": "69608e7c5b7998385e639587", "user": {"_id": "62441d1d9fdefb55a0b7d12c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png", "isPro": false, "fullname": "Younes B", "user": "ybelkada", "type": "user"}, "name": "Younes Belkada", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:34:47.914Z", "hidden": false}, {"_id": "69608e7c5b7998385e639588", "user": {"_id": "6471d727a2b0a376b8b6a4ed", "avatarUrl": "/avatars/aedda547f2ca40dfa898e76be787952f.svg", "isPro": false, "fullname": "Hakim Hacid", "user": "HakimHacid", "type": "user"}, "name": "Hakim Hacid", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:56.651Z", "hidden": false}], "publishedAt": "2026-01-08T12:41:49.000Z", "submittedOnDailyAt": "2026-01-09T02:55:50.938Z", "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers", "submittedOnDailyBy": {"_id": "6460c3811db65f878513bcaf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg", "isPro": false, "fullname": "Jingwei Zuo", "user": "JingweiZuo", "type": "user"}, "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.", "upvotes": 28, "discussionId": "69608e7c5b7998385e639589", "projectPage": "https://tiiuae.github.io/Falcon-H1/", "githubRepo": "https://github.com/tiiuae/falcon-h1", "githubRepoAddedBy": "user", "ai_summary": "Learnable multipliers are introduced to address weight decay-induced normalization artifacts in large language model training, outperforming traditional methods while reducing computational overhead.", "ai_keywords": ["weight decay", "stochastic gradient noise", "Brownian-like expansion", "WD-noise equilibrium", "learnable multipliers", "matrix layers", "weight norm", "muP multipliers", "Adam optimizer", "Muon optimizer"], "githubStars": 98, "organization": {"_id": "6448cad23adf50d86406b0a3", "name": "tiiuae", "fullname": "Technology Innovation Institute", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"}, "summary_zh": "<ul>\n    <li>\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u4e2d\uff0c\u5e94\u7528\u6743\u91cd\u8870\u51cf\uff08WD\uff09\u662f\u5e38\u89c1\u505a\u6cd5\u3002</li>\n    <li>\u4ee5\u5f80\u7814\u7a76\u8868\u660e\uff0c\u968f\u673a\u68af\u5ea6\u566a\u58f0\u5bfc\u81f4\u6743\u91cd\u77e9\u9635\u7684\u6269\u5c55\uff0cWD\u53ef\u4ee5\u62b5\u6d88\u8fd9\u79cd\u589e\u957f\u3002</li>\n    <li>\u672c\u7814\u7a76\u8ba4\u4e3a\u8fd9\u79cd\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u5e73\u8861\u6743\u91cd\u8303\u6570\u662f\u6709\u5bb3\u7684\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u4e58\u5b50\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u53d1\u73b0\uff0c\u5b66\u4e60\u5230\u7684\u4e58\u5b50\u53ef\u4ee5\u9002\u5e94\u6570\u636e\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u8fd9\u79cd\u65b9\u6cd5\u5728\u4f7f\u7528Adam\u548cMuon\u4f18\u5316\u5668\u65f6\u90fd\u80fd\u63d0\u9ad8\u4e0b\u6e38\u8bc4\u4f30\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Weight decay (WD) is commonly used in training large language models to control the growth of weight matrices.</li>\n    <li>The authors propose a new method using learnable multipliers to improve the scaling of weight matrices, addressing issues with the standard WD-noise balance.</li>\n    <li>By using a single learnable multiplier for the entire weight matrix, they found that performance improved compared to the fixed WD scale.</li>\n    <li>The approach was further enhanced by introducing per-row and per-column multipliers, allowing for more flexibility and better performance.</li>\n    <li>The new method outperformed traditional tuning methods, reduced computational costs, and showed significant improvements in model performance with various optimizers.</li>\n</ul>"}, "publishedAt": "2026-01-08T07:41:49.000Z", "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers", "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04890.png", "numComments": 1, "submittedBy": {"_id": "6460c3811db65f878513bcaf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg", "fullname": "Jingwei Zuo", "name": "JingweiZuo", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 37, "isUserFollowing": false}, "organization": {"_id": "6448cad23adf50d86406b0a3", "name": "tiiuae", "fullname": "Technology Innovation Institute", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05249", "authors": [{"_id": "6960a8ce5b7998385e639615", "user": {"_id": "676ce504027822ead2b5f193", "avatarUrl": "/avatars/91de797fc4a971a481b2dce82b579f66.svg", "isPro": false, "fullname": "YuanKangNeilLee", "user": "NeilLeeNTU", "type": "user"}, "name": "Yuan-Kang Lee", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:24:03.428Z", "hidden": false}, {"_id": "6960a8ce5b7998385e639616", "name": "Kuan-Lin Chen", "hidden": false}, {"_id": "6960a8ce5b7998385e639617", "name": "Chia-Che Chang", "hidden": false}, {"_id": "6960a8ce5b7998385e639618", "user": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "isPro": false, "fullname": "Yu-Lun Liu", "user": "yulunliu", "type": "user"}, "name": "Yu-Lun Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:50:20.802Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"], "publishedAt": "2026-01-08T18:59:55.000Z", "submittedOnDailyAt": "2026-01-09T04:35:57.571Z", "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes", "submittedOnDailyBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "isPro": false, "fullname": "Yu-Lun Liu", "user": "yulunliu", "type": "user"}, "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/", "upvotes": 24, "discussionId": "6960a8ce5b7998385e639619", "projectPage": "https://ntuneillee.github.io/research/rl-awb/", "githubRepo": "https://github.com/BrianChen1120/RL-AWB", "githubRepoAddedBy": "user", "ai_summary": "A novel nighttime color constancy framework combines statistical methods with deep reinforcement learning to improve white balance adjustment under low-light conditions.", "ai_keywords": ["deep reinforcement learning", "color constancy", "white balance", "statistical algorithms", "illumination estimation", "multi-sensor dataset"], "githubStars": 12, "summary_zh": "<ul>\n    <li>\u591c\u95f4\u8272\u5f69\u6052\u5e38\u6027\u5728\u8ba1\u7b97\u6444\u5f71\u4e2d\u662f\u4e00\u4e2a\u96be\u9898\uff0c\u4e3b\u8981\u53d7\u4f4e\u5149\u566a\u58f0\u548c\u590d\u6742\u5149\u7167\u6761\u4ef6\u7684\u5f71\u54cd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86RL-AWB\uff0c\u4e00\u4e2a\u7ed3\u5408\u7edf\u8ba1\u65b9\u6cd5\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u591c\u95f4\u767d\u5e73\u8861\u65b0\u6846\u67b6\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u9996\u5148\u4f7f\u7528\u9488\u5bf9\u591c\u95f4\u573a\u666f\u7684\u7edf\u8ba1\u7b97\u6cd5\uff0c\u7ed3\u5408\u663e\u8457\u7070\u8272\u50cf\u7d20\u68c0\u6d4b\u548c\u65b0\u578b\u5149\u7167\u4f30\u8ba1\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u9996\u4e2a\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u7edf\u8ba1\u7b97\u6cd5\u4e3a\u6838\u5fc3\uff0c\u52a8\u6001\u4f18\u5316\u6bcf\u5f20\u56fe\u50cf\u7684\u53c2\u6570\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86\u9996\u4e2a\u591a\u4f20\u611f\u5668\u591c\u95f4\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4f4e\u5149\u548c\u826f\u597d\u5149\u7167\u56fe\u50cf\u4e2d\u5177\u6709\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Nighttime color balance in photos is difficult due to low light and complicated lighting conditions.</li>\n    <li>We introduce RL-AWB, a new system that combines statistical methods with deep reinforcement learning for better white balance at night.</li>\n    <li>Our method starts with a statistical approach that detects important gray pixels and estimates lighting conditions in nighttime images.</li>\n    <li>We created the first deep reinforcement learning method for color balance, which adapts like human experts do by adjusting settings for each image.</li>\n    <li>We also developed a new dataset to test our method across different camera types, showing it works well in various lighting situations.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:55.000Z", "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes", "summary": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/-GwLalmxp_ctrLFDuUq73.webp"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05249.png", "numComments": 1, "submittedBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "fullname": "Yu-Lun Liu", "name": "yulunliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05106", "authors": [{"_id": "696073d35b7998385e6394c4", "name": "Nuoya Xiong", "hidden": false}, {"_id": "696073d35b7998385e6394c5", "user": {"_id": "64887eb15cf73a16e767b56a", "avatarUrl": "/avatars/ada2b6a07346b1d61322ddd04d219318.svg", "isPro": false, "fullname": "Yuhang Zhou", "user": "zyhang1998", "type": "user"}, "name": "Yuhang Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T16:50:49.484Z", "hidden": false}, {"_id": "696073d35b7998385e6394c6", "name": "Hanqing Zeng", "hidden": false}, {"_id": "696073d35b7998385e6394c7", "name": "Zhaorun Chen", "hidden": false}, {"_id": "696073d35b7998385e6394c8", "name": "Furong Huang", "hidden": false}, {"_id": "696073d35b7998385e6394c9", "name": "Shuchao Bi", "hidden": false}, {"_id": "696073d35b7998385e6394ca", "name": "Lizhu Zhang", "hidden": false}, {"_id": "696073d35b7998385e6394cb", "name": "Zhuokai Zhao", "hidden": false}], "publishedAt": "2026-01-08T16:53:16.000Z", "submittedOnDailyAt": "2026-01-09T00:49:52.202Z", "title": "Token-Level LLM Collaboration via FusionRoute", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.", "upvotes": 23, "discussionId": "696073d45b7998385e6394cc", "ai_summary": "FusionRoute is a token-level multi-LLM collaboration framework that uses a lightweight router to select optimal experts and add complementary logits, outperforming existing methods in diverse tasks while maintaining efficiency.", "ai_keywords": ["large language models", "token-level collaboration", "multi-LLM collaboration", "lightweight router", "expert selection", "logit addition", "complementary generator", "optimal decoding policy", "model merging", "direct fine-tuning"], "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u8bad\u7ec3\u548c\u90e8\u7f72\u6210\u672c\u9ad8\u3002</li>\n    <li>\u8f83\u5c0f\u7684\u4e13\u4e1a\u6a21\u578b\u6548\u7387\u9ad8\uff0c\u4f46\u65e0\u6cd5\u6709\u6548\u63a8\u5e7f\u5230\u5176\u4ed6\u9886\u57df\u3002</li>\n    <li>FusionRoute\u662f\u4e00\u79cd\u591a\u6a21\u578b\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u9009\u62e9\u6700\u5408\u9002\u7684\u4e13\u5bb6\u5e76\u6539\u8fdb\u8f93\u51fa\u3002</li>\n    <li>\u4e0e\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\uff0cFusionRoute\u7ed3\u5408\u4e86\u53ef\u8bad\u7ec3\u7684\u751f\u6210\u5668\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u7075\u6d3b\u6027\u3002</li>\n    <li>\u5728\u5404\u7c7b\u4efb\u52a1\u4e2d\uff0cFusionRoute\u7684\u8868\u73b0\u8d85\u8fc7\u4e86\u5176\u4ed6\u534f\u4f5c\u548c\u5fae\u8c03\u65b9\u6cd5\uff0c\u540c\u65f6\u4e0e\u9886\u57df\u4e13\u5bb6\u4fdd\u6301\u7ade\u4e89\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) are powerful but expensive to train and use effectively across many areas.</li>\n    <li>Smaller, specialized models are cheaper but struggle to perform well outside their training data.</li>\n    <li>The proposed solution, FusionRoute, uses a lightweight router to choose the best expert model for each step and improve its output with additional information.</li>\n    <li>FusionRoute overcomes limitations of existing methods by allowing for better decision-making and achieving optimal results under certain conditions.</li>\n    <li>Tests show that FusionRoute outperforms other collaboration methods and remains effective compared to specialized models in various tasks like math, coding, and following instructions.</li>\n</ul>"}, "publishedAt": "2026-01-08T11:53:16.000Z", "title": "Token-Level LLM Collaboration via FusionRoute", "summary": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05106.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 205, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.05241", "authors": [{"_id": "6960775a5b7998385e6394ff", "user": {"_id": "64ed876a74d9b58eabc769a4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg", "isPro": true, "fullname": "Boyang Wang", "user": "HikariDawn", "type": "user"}, "name": "Boyang Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:11.481Z", "hidden": false}, {"_id": "6960775a5b7998385e639500", "name": "Haoran Zhang", "hidden": false}, {"_id": "6960775a5b7998385e639501", "name": "Shujie Zhang", "hidden": false}, {"_id": "6960775a5b7998385e639502", "user": {"_id": "64edb581067fbb625f893628", "avatarUrl": "/avatars/d59893d6f1f752bb73255bd78b325fe9.svg", "isPro": false, "fullname": "hao", "user": "wuzhi-hao", "type": "user"}, "name": "Jinkun Hao", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:09.450Z", "hidden": false}, {"_id": "6960775a5b7998385e639503", "name": "Mingda Jia", "hidden": false}, {"_id": "6960775a5b7998385e639504", "name": "Qi Lv", "hidden": false}, {"_id": "6960775a5b7998385e639505", "user": {"_id": "65de9c6cf68c3d3bac330509", "avatarUrl": "/avatars/150858545ea1a9e7c96d6f227093ac54.svg", "isPro": false, "fullname": "Yucheng Mao", "user": "matthewmao", "type": "user"}, "name": "Yucheng Mao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:52:47.293Z", "hidden": false}, {"_id": "6960775a5b7998385e639506", "user": {"_id": "63f2ec797ddf724fbcc75aee", "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg", "isPro": false, "fullname": "Zhaoyang Lyu", "user": "ZhaoyangLyu", "type": "user"}, "name": "Zhaoyang Lyu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:52:53.025Z", "hidden": false}, {"_id": "6960775a5b7998385e639507", "user": {"_id": "685d08b9fc7a0ff2f338dbd0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/685d08b9fc7a0ff2f338dbd0/bW3G1ZM-tAXIIbCbUdwC9.png", "isPro": false, "fullname": "Jia Zeng", "user": "Jia-Zeng", "type": "user"}, "name": "Jia Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T15:46:06.251Z", "hidden": false}, {"_id": "6960775a5b7998385e639508", "name": "Xudong Xu", "hidden": false}, {"_id": "6960775a5b7998385e639509", "user": {"_id": "65783ee6ee33d547aecc3ffc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg", "isPro": false, "fullname": "Jiangmiao Pang", "user": "Jiangmiao", "type": "user"}, "name": "Jiangmiao Pang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:52:00.637Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:22.000Z", "submittedOnDailyAt": "2026-01-09T02:54:13.651Z", "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation", "submittedOnDailyBy": {"_id": "64ed876a74d9b58eabc769a4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg", "isPro": true, "fullname": "Boyang Wang", "user": "HikariDawn", "type": "user"}, "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.", "upvotes": 19, "discussionId": "6960775a5b7998385e63950a", "projectPage": "https://robovip.github.io/RoboVIP/", "githubRepo": "https://github.com/RoboVIP/RoboVIP_VDM", "githubRepoAddedBy": "user", "ai_summary": "Visual identity prompting enhances manipulation data augmentation for robot policies by providing explicit visual guidance to diffusion models, improving policy performance in both simulation and real-world settings.", "ai_keywords": ["image diffusion models", "visual identity prompting", "manipulation data", "vision-language-action models", "visuomotor policy models", "visual identity pool"], "githubStars": 7, "summary_zh": "<ul>\n    <li>\u6536\u96c6\u5927\u91cf\u771f\u5b9e\u4e16\u754c\u7684\u64cd\u4f5c\u6570\u636e\u5f88\u56f0\u96be\uff0c\u56e0\u4e3a\u786c\u4ef6\u548c\u73af\u5883\u8bbe\u7f6e\u6709\u9650\u5236\u3002</li>\n    <li>\u6700\u8fd1\u7684\u7814\u7a76\u4f7f\u7528\u6587\u672c\u63d0\u793a\u548c\u56fe\u50cf\u6269\u6563\u6a21\u578b\u6765\u589e\u5f3a\u64cd\u4f5c\u6570\u636e\uff0c\u4f46\u672a\u8003\u8651\u591a\u89c6\u89d2\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u3002</li>\n    <li>\u4ec5\u9760\u6587\u672c\u63d0\u793a\u65e0\u6cd5\u53ef\u9760\u5730\u6307\u5b9a\u573a\u666f\u8bbe\u7f6e\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86\u89c6\u89c9\u8eab\u4efd\u63d0\u793a\uff0c\u4f7f\u7528\u793a\u4f8b\u56fe\u50cf\u6765\u6307\u5bfc\u751f\u6210\u6240\u9700\u7684\u573a\u666f\u8bbe\u7f6e\u3002</li>\n    <li>\u901a\u8fc7\u6211\u4eec\u7684\u589e\u5f3a\u64cd\u4f5c\u6570\u636e\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u73af\u5883\u4e2d\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Collecting real-world manipulation data for robots is challenging due to hardware limitations and diverse environments.</li>\n    <li>Recent methods use image diffusion models to improve manipulation data by changing backgrounds and objects.</li>\n    <li>These methods often miss the need for multi-view and time-consistent data required for advanced robot policies.</li>\n    <li>The authors introduce \"visual identity prompting,\" using example images to better guide scene generation.</li>\n    <li>Their new data approach improves robot training results in both simulations and real-world scenarios.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:22.000Z", "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation", "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05241.png", "numComments": 2, "submittedBy": {"_id": "64ed876a74d9b58eabc769a4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ed876a74d9b58eabc769a4/K4bJVW0FlqRtAAxJBJifR.jpeg", "fullname": "Boyang Wang", "name": "HikariDawn", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05167", "authors": [{"_id": "69606c325b7998385e639481", "user": {"_id": "62ea79dd01ed9b0e8f61ccd3", "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg", "isPro": false, "fullname": "Chengsong Huang", "user": "ChengsongHuang", "type": "user"}, "name": "Chengsong Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:21.851Z", "hidden": false}, {"_id": "69606c325b7998385e639482", "name": "Tong Zheng", "hidden": false}, {"_id": "69606c325b7998385e639483", "user": {"_id": "65e02d89574e5aa0e9ce3efa", "avatarUrl": "/avatars/2ab152a10b21d81fb1defc726b8e951a.svg", "isPro": false, "fullname": "Langlin Huang", "user": "shrango", "type": "user"}, "name": "Langlin Huang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:53:24.796Z", "hidden": false}, {"_id": "69606c325b7998385e639484", "name": "Jinyuan Li", "hidden": false}, {"_id": "69606c325b7998385e639485", "name": "Haolin Liu", "hidden": false}, {"_id": "69606c325b7998385e639486", "name": "Jiaxin Huang", "hidden": false}], "publishedAt": "2026-01-08T17:56:16.000Z", "submittedOnDailyAt": "2026-01-09T00:17:45.320Z", "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding", "submittedOnDailyBy": {"_id": "62ea79dd01ed9b0e8f61ccd3", "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg", "isPro": false, "fullname": "Chengsong Huang", "user": "ChengsongHuang", "type": "user"}, "summary": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.", "upvotes": 19, "discussionId": "69606c325b7998385e639487", "githubRepo": "https://github.com/Chengsong-Huang/RelayLLM", "githubRepoAddedBy": "user", "ai_summary": "RelayLLM enables efficient collaborative reasoning between small and large language models through token-level dynamic invocation, achieving high accuracy with minimal computational overhead.", "ai_keywords": ["Large Language Models", "Small Language Models", "collaborative decoding", "token-level collaboration", "Group Relative Policy Optimization", "policy optimization", "dynamic invocation", "computational efficiency", "reasoning capacity"], "githubStars": 6, "summary_zh": "<ul>\n    <li>\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u63a8\u7406\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u5ef6\u8fdf\u5927\uff0c\u800c\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u901a\u5e38\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002</li>\n    <li>\u73b0\u6709\u7684\u534f\u4f5c\u65b9\u6cd5\u5982\u7ea7\u8054\u6216\u8def\u7531\u5b58\u5728\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u5b83\u4eec\u5c06\u6574\u4e2a\u67e5\u8be2\u4ea4\u7ed9LLMs\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86RelayLLM\uff0c\u4e00\u4e2a\u901a\u8fc7\u4ee4\u724c\u7ea7\u534f\u4f5c\u89e3\u7801\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u7684\u65b0\u6846\u67b6\u3002</li>\n    <li>RelayLLM\u4f7fSLM\u80fd\u591f\u4f5c\u4e3a\u4e3b\u52a8\u63a7\u5236\u8005\uff0c\u6839\u636e\u9700\u8981\u52a8\u6001\u8c03\u7528LLM\uff0c\u4ece\u800c\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cRelayLLM\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u51c6\u786e\u7387\u4e3a49.52%\uff0c\u5e76\u4e14\u53ea\u8c03\u7528LLM\u751f\u62101.07%\u7684\u603b\u4ee4\u724c\uff0c\u5b9e\u73b0\u4e8698.2%\u7684\u6210\u672c\u964d\u4f4e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are powerful but expensive and slow, while Small Language Models (SLMs) are efficient but struggle with complex reasoning.</li>\n    <li>Current methods for using both models waste resources by sending entire queries to LLMs, even when SLMs could handle most reasoning tasks.</li>\n    <li>RelayLLM is a new system that allows SLMs to decide when to ask LLMs for help, using a command for important tokens only.</li>\n    <li>The system uses a two-step training process to teach models when to work independently and when to seek help from LLMs.</li>\n    <li>RelayLLM shows impressive results with an average accuracy of 49.52%, using the LLM only for 1.07% of the tokens and reducing costs by 98.2% compared to traditional methods.</li>\n</ul>"}, "publishedAt": "2026-01-08T12:56:16.000Z", "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding", "summary": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05167.png", "numComments": 1, "submittedBy": {"_id": "62ea79dd01ed9b0e8f61ccd3", "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg", "fullname": "Chengsong Huang", "name": "ChengsongHuang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.04767", "authors": [{"_id": "69609fe35b7998385e6395ed", "user": {"_id": "64feba7efa64465422ce3003", "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg", "isPro": false, "fullname": "zongzefang", "user": "zzfoutofspace", "type": "user"}, "name": "Zefang Zong", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:54:07.260Z", "hidden": false}, {"_id": "69609fe35b7998385e6395ee", "user": {"_id": "6462271493f702673bf99c0b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462271493f702673bf99c0b/PyWyI2uoJGr0kpugGGr0t.jpeg", "isPro": false, "fullname": "Dingwei Chen", "user": "CuSO4-Chen", "type": "user"}, "name": "Dingwei Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:54:13.098Z", "hidden": false}, {"_id": "69609fe35b7998385e6395ef", "name": "Yang Li", "hidden": false}, {"_id": "69609fe35b7998385e6395f0", "name": "Qi Yi", "hidden": false}, {"_id": "69609fe35b7998385e6395f1", "name": "Bo Zhou", "hidden": false}, {"_id": "69609fe35b7998385e6395f2", "user": {"_id": "65d5f457d032b44853ae79e4", "avatarUrl": "/avatars/362180aff317ecee27513741c18fd98c.svg", "isPro": false, "fullname": "chengming li", "user": "daming8000", "type": "user"}, "name": "Chengming Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:54:28.358Z", "hidden": false}, {"_id": "69609fe35b7998385e6395f3", "name": "Bo Qian", "hidden": false}, {"_id": "69609fe35b7998385e6395f4", "user": {"_id": "60799b15921db717010c7c8e", "avatarUrl": "/avatars/5e33095aa5343538f09831c1ed2230d2.svg", "isPro": false, "fullname": "Peng Chen", "user": "pengchen", "type": "user"}, "name": "Peng Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:54:22.694Z", "hidden": false}, {"_id": "69609fe35b7998385e6395f5", "name": "Jie Jiang", "hidden": false}], "publishedAt": "2026-01-08T09:35:49.000Z", "submittedOnDailyAt": "2026-01-09T04:20:12.513Z", "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search", "submittedOnDailyBy": {"_id": "64feba7efa64465422ce3003", "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg", "isPro": false, "fullname": "zongzefang", "user": "zzfoutofspace", "type": "user"}, "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.", "upvotes": 18, "discussionId": "69609fe35b7998385e6395f6", "githubRepo": "https://github.com/zzfoutofspace/ATPO", "githubRepoAddedBy": "user", "ai_summary": "AT\u00b2PO is a unified framework for multi-turn agentic reinforcement learning that improves exploration diversity, credit assignment, and policy optimization through tree search and turn-level learning objectives.", "ai_keywords": ["Agentic Reinforcement Learning", "tree search", "Entropy-Guided Tree Expansion", "Turn-wise Credit Assignment", "Agentic Turn-based Policy Optimization", "multi-turn tasks", "policy optimization", "reward propagation", "turn-level learning objective"], "githubStars": 2, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>LLM\u4ee3\u7406\uff08\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\uff09\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u8f6e\u4efb\u52a1\uff0c\u901a\u8fc7\u5185\u90e8\u63a8\u7406\u548c\u5916\u90e8\u5de5\u5177\u4e92\u52a8\u76f8\u7ed3\u5408\u3002</li>\n    <li>\u63d0\u51fa\u4e86AT\u00b2PO\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u6709\u9650\u63a2\u7d22\u591a\u6837\u6027\u3001\u7a00\u758f\u4fe1\u7528\u5206\u914d\u548c\u4e0d\u4e00\u81f4\u7684\u7b56\u7565\u4f18\u5316\u7b49\u6838\u5fc3\u6311\u6218\u3002</li>\n    <li>AT\u00b2PO\u4f7f\u7528\u6811\u7ed3\u6784\u8fdb\u884c\u63a2\u7d22\u548c\u4fe1\u7528\u5206\u914d\uff0c\u4ee5\u63d0\u9ad8\u5956\u52b1\u4f20\u64ad\u7684\u7cbe\u7ec6\u5ea6\u3002</li>\n    <li>\u8be5\u6846\u67b6\u53ef\u4ee5\u4e0e\u4efb\u4f55\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6d41\u7a0b\u96c6\u6210\uff0c\u5e76\u4e14\u5728\u4e03\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u5148\u8fdb\u65b9\u6cd5\u3002</li>\n    <li>\u76f8\u5173\u4ee3\u7801\u53ef\u5728GitHub\u4e0a\u83b7\u53d6\uff0c\u94fe\u63a5\u4e3a https://github.com/zzfoutofspace/ATPO\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LLM agents are effective for multi-turn tasks, combining reasoning and tool use.</li>\n    <li>Agentic Reinforcement Learning is a new method to improve these agents after initial training.</li>\n    <li>The paper introduces AT^2PO, a framework that helps solve three main issues in multi-turn agentic RL: exploration diversity, credit assignment, and policy optimization.</li>\n    <li>AT^2PO uses a tree structure for better exploration and reward tracking, along with a specific learning objective that matches the decision-making process of agents.</li>\n    <li>Tests show that AT^2PO outperforms previous methods, improving results by up to 1.84 percentage points on average.</li>\n</ul>"}, "publishedAt": "2026-01-08T04:35:49.000Z", "title": "AT^2PO: Agentic Turn-based Policy Optimization via Tree Search", "summary": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT^2PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT^2PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04767.png", "numComments": 1, "submittedBy": {"_id": "64feba7efa64465422ce3003", "avatarUrl": "/avatars/abdc7a3748f6a4e15ebc6aa8d616c87d.svg", "fullname": "zongzefang", "name": "zzfoutofspace", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.21815", "authors": [{"_id": "6960e5825b7998385e6396da", "name": "Mengqi He", "hidden": false}, {"_id": "6960e5825b7998385e6396db", "name": "Xinyu Tian", "hidden": false}, {"_id": "6960e5825b7998385e6396dc", "name": "Xin Shen", "hidden": false}, {"_id": "6960e5825b7998385e6396dd", "user": {"_id": "64c71a5647418a0a59e5c7cb", "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg", "isPro": false, "fullname": "Jinhong Ni", "user": "mcleanie", "type": "user"}, "name": "Jinhong Ni", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:57:14.555Z", "hidden": false}, {"_id": "6960e5825b7998385e6396de", "name": "Shu Zou", "hidden": false}, {"_id": "6960e5825b7998385e6396df", "user": {"_id": "6364187d969bdae89e12b209", "avatarUrl": "/avatars/290793f85e8dc2ae0cd4fe1becb4b1d0.svg", "isPro": false, "fullname": "zhaoyuan yang", "user": "zhaoyuan", "type": "user"}, "name": "Zhaoyuan Yang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:57:08.195Z", "hidden": false}, {"_id": "6960e5825b7998385e6396e0", "name": "Jing Zhang", "hidden": false}], "publishedAt": "2025-12-26T01:01:25.000Z", "submittedOnDailyAt": "2026-01-09T09:01:04.074Z", "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models", "submittedOnDailyBy": {"_id": "68a3f14e6dd0e4c74c014853", "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg", "isPro": true, "fullname": "Hubert", "user": "ANUHW", "type": "user"}, "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.", "upvotes": 16, "discussionId": "6960e5835b7998385e6396e1", "ai_summary": "Selective adversarial attacks targeting high-entropy tokens in vision-language models achieve significant semantic degradation with reduced budgets and demonstrate transferable vulnerabilities across different architectures.", "ai_keywords": ["vision-language models", "entropy", "adversarial attacks", "autoregressive generation", "high-entropy tokens", "semantic degradation", "attack success rates", "transferability"], "organization": {"_id": "64ed4ba2453a4b4bef2664c5", "name": "anu-cvml", "fullname": "Australian National University"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u653b\u51fb\u7684\u5f71\u54cd\u3002</li>\n    <li>\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u4e0e\u5176\u53ef\u9760\u6027\u76f8\u5173\uff0c\u4e4b\u524d\u7684\u653b\u51fb\u65b9\u6cd5\u5047\u8bbe\u6240\u6709\u8bcd\u6c47\u5bf9\u751f\u6210\u7684\u4e0d\u7a33\u5b9a\u6027\u5f71\u54cd\u76f8\u540c\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u7ea620%\u7684\u9ad8\u71b5\u8bcd\u6c47\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u53ef\u4ee5\u5f71\u54cd\u8f93\u51fa\u7ed3\u679c\u3002</li>\n    <li>\u9488\u5bf9\u8fd9\u4e9b\u5173\u952e\u4f4d\u7f6e\u8fdb\u884c\u653b\u51fb\uff0c\u80fd\u591f\u5728\u4f7f\u7528\u66f4\u5c11\u8d44\u6e90\u7684\u60c5\u51b5\u4e0b\uff0c\u9020\u6210\u4e0e\u5168\u5c40\u65b9\u6cd5\u76f8\u4f3c\u7684\u8bed\u4e49\u635f\u5931\u3002</li>\n    <li>\u63d0\u51fa\u7684\u201c\u71b5\u94f6\u884c\u5f15\u5bfc\u5bf9\u6297\u653b\u51fb\u201d\uff08EGA\uff09\u80fd\u591f\u5b9e\u73b093-95%\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5f53\u524dVLM\u5b89\u5168\u673a\u5236\u7684\u65b0\u5f31\u70b9\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Vision-language models (VLMs) perform well but can be easily attacked.</li>\n  <li>Entropy indicates how uncertain a model is, and is linked to VLM reliability.</li>\n  <li>Instead of affecting all parts of the model, only about 20% of high-entropy tokens significantly impact the model's output.</li>\n  <li>Targeting these key tokens with selective attacks can convert a large portion (35-49%) of normal outputs into harmful ones.</li>\n  <li>The study introduces a new attack method called Entropy-bank Guided Adversarial attacks (EGA), which has a high success rate and highlights vulnerabilities in VLM safety. </li>\n</ul>"}, "publishedAt": "2025-12-25T20:01:25.000Z", "title": "Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models", "summary": "Vision-language models (VLMs) achieve remarkable performance but remain vulnerable to adversarial attacks. Entropy, a measure of model uncertainty, is strongly correlated with the reliability of VLM. Prior entropy-based attacks maximize uncertainty at all decoding steps, implicitly assuming that every token contributes equally to generation instability. We show instead that a small fraction (about 20%) of high-entropy tokens, i.e., critical decision points in autoregressive generation, disproportionately governs output trajectories. By concentrating adversarial perturbations on these positions, we achieve semantic degradation comparable to global methods while using substantially smaller budgets. More importantly, across multiple representative VLMs, such selective attacks convert 35-49% of benign outputs into harmful ones, exposing a more critical safety risk. Remarkably, these vulnerable high-entropy forks recur across architecturally diverse VLMs, enabling feasible transferability (17-26% harmful rates on unseen targets). Motivated by these findings, we propose Entropy-bank Guided Adversarial attacks (EGA), which achieves competitive attack success rates (93-95%) alongside high harmful conversion, thereby revealing new weaknesses in current VLM safety mechanisms.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21815.png", "numComments": 1, "submittedBy": {"_id": "68a3f14e6dd0e4c74c014853", "avatarUrl": "/avatars/90d122257250ac640e30b9c5efb90f67.svg", "fullname": "Hubert", "name": "ANUHW", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "64ed4ba2453a4b4bef2664c5", "name": "anu-cvml", "fullname": "Australian National University"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.05175", "authors": [{"_id": "69606c015b7998385e639468", "user": {"_id": "65803d64defc9c0d30db4f88", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65803d64defc9c0d30db4f88/gVxJaB8vSUFj4r0vnTVbP.jpeg", "isPro": true, "fullname": "Shuming Liu", "user": "sming256", "type": "user"}, "name": "Shuming Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:23.773Z", "hidden": false}, {"_id": "69606c015b7998385e639469", "user": {"_id": "64403daae44f30a72323e4ca", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64403daae44f30a72323e4ca/skJ9h0pdNfmE4VbQL8xDR.png", "isPro": false, "fullname": "mingchen zhuge", "user": "tjpxiaoming", "type": "user"}, "name": "Mingchen Zhuge", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:54:42.678Z", "hidden": false}, {"_id": "69606c015b7998385e63946a", "user": {"_id": "64d49ef30f76abaf363b88d6", "avatarUrl": "/avatars/93b5cc51305ac88198cea1dad8104db2.svg", "isPro": false, "fullname": "Changsheng Zhao", "user": "mikezhaocs", "type": "user"}, "name": "Changsheng Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:55:05.174Z", "hidden": false}, {"_id": "69606c015b7998385e63946b", "name": "Jun Chen", "hidden": false}, {"_id": "69606c015b7998385e63946c", "user": {"_id": "6364acf56fa33d2f59397c59", "avatarUrl": "/avatars/f642a7dcc098f50fef865f30f93a25c3.svg", "isPro": false, "fullname": "Lemeng Wu", "user": "klight", "type": "user"}, "name": "Lemeng Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:55:12.390Z", "hidden": false}, {"_id": "69606c015b7998385e63946d", "name": "Zechun Liu", "hidden": false}, {"_id": "69606c015b7998385e63946e", "user": {"_id": "66f6e80e7db9927533cbd3e1", "avatarUrl": "/avatars/f169a3cb4f48781e38cc111a35741d2d.svg", "isPro": false, "fullname": "Chenchen Zhu", "user": "zcckernel", "type": "user"}, "name": "Chenchen Zhu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:55:32.413Z", "hidden": false}, {"_id": "69606c015b7998385e63946f", "name": "Zhipeng Cai", "hidden": false}, {"_id": "69606c015b7998385e639470", "name": "Chong Zhou", "hidden": false}, {"_id": "69606c015b7998385e639471", "name": "Haozhe Liu", "hidden": false}, {"_id": "69606c015b7998385e639472", "name": "Ernie Chang", "hidden": false}, {"_id": "69606c015b7998385e639473", "user": {"_id": "645417e98617183806210cfc", "avatarUrl": "/avatars/7a3ba5b0c38878db51da7091fed15f99.svg", "isPro": false, "fullname": "Saksham Suri", "user": "sakshams", "type": "user"}, "name": "Saksham Suri", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:56:08.056Z", "hidden": false}, {"_id": "69606c015b7998385e639474", "name": "Hongyu Xu", "hidden": false}, {"_id": "69606c015b7998385e639475", "name": "Qi Qian", "hidden": false}, {"_id": "69606c015b7998385e639476", "name": "Wei Wen", "hidden": false}, {"_id": "69606c015b7998385e639477", "user": {"_id": "6570032714fa8cfccd39c6ad", "avatarUrl": "/avatars/f4b6140e21db5d18241dcc9b2e94ae33.svg", "isPro": false, "fullname": "Balakrishnan Varadarajan", "user": "balakv", "type": "user"}, "name": "Balakrishnan Varadarajan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:56:15.446Z", "hidden": false}, {"_id": "69606c015b7998385e639478", "name": "Zhuang Liu", "hidden": false}, {"_id": "69606c015b7998385e639479", "name": "Hu Xu", "hidden": false}, {"_id": "69606c015b7998385e63947a", "user": {"_id": "63166336894404e2506b8811", "avatarUrl": "/avatars/ec51a1dd86511127cf83afd4bf7c0f52.svg", "isPro": false, "fullname": "Florian Bordes", "user": "Fbordes", "type": "user"}, "name": "Florian Bordes", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:56:21.715Z", "hidden": false}, {"_id": "69606c015b7998385e63947b", "name": "Raghuraman Krishnamoorthi", "hidden": false}, {"_id": "69606c015b7998385e63947c", "user": {"_id": "6808bf97ffadd78ec71cb721", "avatarUrl": "/avatars/9adca3142c06b8f69889fcbe85fa374d.svg", "isPro": false, "fullname": "Bernard Ghanem", "user": "bernardghanem", "type": "user"}, "name": "Bernard Ghanem", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:56:31.147Z", "hidden": false}, {"_id": "69606c015b7998385e63947d", "name": "Vikas Chandra", "hidden": false}, {"_id": "69606c015b7998385e63947e", "user": {"_id": "65304b62e7535baecd85d080", "avatarUrl": "/avatars/6e546c7d1414bd92c5a7c8d8c404de92.svg", "isPro": false, "fullname": "Yunyang Xiong", "user": "yunyangx", "type": "user"}, "name": "Yunyang Xiong", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:56:41.516Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/ukFTFnGpjfjCtH8xLwHyV.png"], "publishedAt": "2026-01-08T18:00:59.000Z", "submittedOnDailyAt": "2026-01-09T00:16:39.681Z", "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.", "upvotes": 15, "discussionId": "69606c015b7998385e63947f", "projectPage": "https://ivul-kaust.github.io/projects/videoauto-r1/", "githubRepo": "https://github.com/IVUL-KAUST/VideoAuto-R1/", "githubRepoAddedBy": "user", "ai_summary": "VideoAuto-R1 framework employs a reason-when-necessary strategy for video understanding, using a Thinking Once, Answering Twice training paradigm with verifiable rewards and confidence-based reasoning activation during inference.", "ai_keywords": ["Chain-of-thought reasoning", "multimodal large language models", "video understanding", "RL-trained video models", "VideoAuto-R1", "Thinking Once Answering Twice", "verifiable rewards", "confidence score", "perception-oriented tasks", "reasoning-intensive tasks"], "githubStars": 11, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "summary_zh": "<ul>\n    <li>\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63a8\u7406\u5728\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5fc5\u8981\u6027\u548c\u4f18\u52bf\u8fd8\u672a\u5145\u5206\u7814\u7a76\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u5bf9\u4e8e\u7ecf\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u89c6\u9891\u6a21\u578b\uff0c\u76f4\u63a5\u56de\u7b54\u5e38\u5e38\u80fd\u5339\u914d\u6216\u8d85\u8d8aCoT\u7684\u8868\u73b0\uff0c\u5c3d\u7ba1CoT\u9700\u8981\u66f4\u9ad8\u7684\u8ba1\u7b97\u6210\u672c\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVideoAuto-R1\u7684\u89c6\u9891\u7406\u89e3\u6846\u67b6\uff0c\u91c7\u7528\u6709\u5fc5\u8981\u65f6\u63a8\u7406\u7684\u7b56\u7565\u3002</li>\n    <li>\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u6a21\u578b\u5148\u751f\u6210\u521d\u6b65\u7b54\u6848\uff0c\u518d\u8fdb\u884c\u63a8\u7406\uff0c\u6700\u540e\u8f93\u51fa\u5ba1\u67e5\u540e\u7684\u7b54\u6848\uff0c\u5e76\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u8fdb\u884c\u76d1\u7763\u3002</li>\n    <li>VideoAuto-R1\u5728\u89c6\u9891\u95ee\u7b54\u548c\u5b9a\u4f4d\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u5e73\u5747\u54cd\u5e94\u957f\u5ea6\u51cf\u5c11\u4e86\u7ea63.3\u500d\uff0c\u663e\u793a\u51fa\u6548\u7387\u663e\u8457\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Chain-of-thought (CoT) reasoning is used for understanding videos with large language models, but its benefits compared to direct answers are not fully understood.</li>\n    <li>Research shows that for video models trained with reinforcement learning, direct answers can be as good as or better than CoT answers, even though CoT is more complex.</li>\n    <li>The authors introduce a new framework called VideoAuto-R1, which uses a strategy of reasoning only when needed.</li>\n    <li>VideoAuto-R1 first gives an initial answer, then reasons about it, and finally provides a revised answer, which is guided by rewards during training.</li>\n    <li>This new approach improves accuracy and efficiency, reducing the length of responses significantly while showing that reasoning is helpful but not always necessary.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:00:59.000Z", "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice", "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/ukFTFnGpjfjCtH8xLwHyV.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05175.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 205, "isUserFollowing": false}, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.05138", "authors": [{"_id": "69606cd45b7998385e639489", "user": {"_id": "6528fe394513680346a500ed", "avatarUrl": "/avatars/3021a3be7d900e3928cdc375e2a2bf09.svg", "isPro": false, "fullname": "Sixiao Zheng (SII)", "user": "sxzheng", "type": "user"}, "name": "Sixiao Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:19.760Z", "hidden": false}, {"_id": "69606cd45b7998385e63948a", "name": "Minghao Yin", "hidden": false}, {"_id": "69606cd45b7998385e63948b", "name": "Wenbo Hu", "hidden": false}, {"_id": "69606cd45b7998385e63948c", "name": "Xiaoyu Li", "hidden": false}, {"_id": "69606cd45b7998385e63948d", "name": "Ying Shan", "hidden": false}, {"_id": "69606cd45b7998385e63948e", "user": {"_id": "6409fcc8f3dabf93824c84c6", "avatarUrl": "/avatars/dd8fd579630e50ba3058c3829604478e.svg", "isPro": false, "fullname": "YANWEI", "user": "yanweifuture", "type": "user"}, "name": "Yanwei Fu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:57:43.763Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/vfMPtMKg_t5C1-6WqzYur.mp4"], "publishedAt": "2026-01-08T17:28:52.000Z", "submittedOnDailyAt": "2026-01-09T00:21:14.757Z", "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.", "upvotes": 11, "discussionId": "69606cd45b7998385e63948f", "ai_summary": "VerseCrafter is a 4D-aware video world model that enables unified control over camera and object dynamics through 4D geometric control representation and video diffusion models.", "ai_keywords": ["video world models", "4D geometric control", "point cloud", "3D Gaussian trajectories", "video diffusion model", "view-consistent videos", "automatic data engine", "in-the-wild videos"], "summary_zh": "<ul>\n    <li>VerseCrafter\u662f\u4e00\u79cd4D\u89c6\u9891\u4e16\u754c\u6a21\u578b\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u63a7\u5236\u76f8\u673a\u548c\u7269\u4f53\u7684\u52a8\u6001\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4f7f\u7528\u4e00\u79cd\u65b0\u9896\u76844D\u51e0\u4f55\u63a7\u5236\u8868\u793a\uff0c\u80fd\u591f\u7f16\u7801\u4e16\u754c\u72b6\u6001\u3002</li>\n    <li>\u5b83\u901a\u8fc7\u9759\u6001\u80cc\u666f\u70b9\u4e91\u548c\u6bcf\u4e2a\u7269\u4f53\u76843D\u9ad8\u65af\u8f68\u8ff9\u6765\u6355\u6349\u7269\u4f53\u7684\u8fd0\u52a8\u548c\u5360\u636e\u60c5\u51b5\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\uff0c\u80fd\u591f\u51c6\u786e\u53cd\u6620\u6307\u5b9a\u7684\u52a8\u6001\u3002</li>\n    <li>\u4e3a\u4e86\u514b\u670d\u7f3a\u4e4f\u5927\u89c4\u6a214D\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u6570\u636e\u5f15\u64ce\uff0c\u4ece\u91ce\u5916\u89c6\u9891\u4e2d\u63d0\u53d6\u6240\u9700\u76844D\u63a7\u5236\u4fe1\u606f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>VerseCrafter is a new video world model designed to better control camera and object movements in videos.</li>\n    <li>It uses a unique 4D Geometric Control system to represent the environment, capturing both the paths and spaces occupied by objects over time.</li>\n    <li>This method allows for generating high-quality videos that maintain consistency and follow specified movements.</li>\n    <li>To overcome the lack of large datasets with 4D details, VerseCrafter includes an automatic system to extract these controls from existing videos.</li>\n</ul>"}, "publishedAt": "2026-01-08T12:28:52.000Z", "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control", "summary": "Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/vfMPtMKg_t5C1-6WqzYur.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05138.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 205, "isUserFollowing": false}, "isAuthorParticipating": true}],
    "week": [{"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u7528\u6237\u5e0c\u671b\u5b83\u4eec\u4e0d\u4ec5\u63d0\u4f9b\u51c6\u786e\u56de\u7b54\uff0c\u8fd8\u80fd\u7b26\u5408\u591a\u6837\u5316\u7684\u4eba\u7c7b\u504f\u597d\u3002</li>\n    <li>\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5f00\u59cb\u5f15\u5165\u591a\u79cd\u5956\u52b1\u6765\u6307\u5bfc\u6a21\u578b\u7684\u884c\u4e3a\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u76f4\u63a5\u5e94\u7528GRPO\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u4e0d\u540c\u5956\u52b1\u7ec4\u5408\u7684\u4f18\u52bf\u503c\u53d8\u5f97\u76f8\u540c\uff0c\u4ece\u800c\u5f71\u54cd\u8bad\u7ec3\u6548\u679c\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5GDPO\uff0c\u901a\u8fc7\u89e3\u8026\u4e2a\u522b\u5956\u52b1\u7684\u5f52\u4e00\u5316\uff0c\u4fdd\u6301\u5b83\u4eec\u7684\u76f8\u5bf9\u5dee\u5f02\uff0c\u6539\u5584\u591a\u5956\u52b1\u4f18\u5316\u7684\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\uff0cGDPO\u5728\u51c6\u786e\u6027\u548c\u7ea6\u675f\u9075\u5b88\u65b9\u9762\u90fd\u4f18\u4e8eGRPO\uff0c\u663e\u793a\u51fa\u5176\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Users want language models to give accurate answers and behave according to different human preferences.</li>\n    <li>Reinforcement learning (RL) uses multiple rewards to guide models toward desired behaviors.</li>\n    <li>Applying Group Relative Policy Optimization (GRPO) to these rewards can cause problems, leading to poor training outcomes.</li>\n    <li>The paper introduces a new method called Group reward-Decoupled Normalization Policy Optimization (GDPO) that improves training by keeping rewards separate and better preserving their differences.</li>\n    <li>Tests show that GDPO works better than GRPO in various tasks, improving both accuracy and adherence to constraints.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03252", "authors": [{"_id": "695dc956c03d6d81e4399ea4", "name": "Hao Yu", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea5", "user": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "isPro": false, "fullname": "Haotong Lin", "user": "haotongl", "type": "user"}, "name": "Haotong Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:15:04.783Z", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea6", "name": "Jiawei Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea7", "name": "Jiaxin Li", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea8", "name": "Yida Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea9", "user": {"_id": "6791a6c19ce382eae861ed61", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6791a6c19ce382eae861ed61/zerctN-RdeP4hSrWidtyN.jpeg", "isPro": false, "fullname": "Xueyang Zhang", "user": "zhangxueyang001", "type": "user"}, "name": "Xueyang Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:15:39.946Z", "hidden": false}, {"_id": "695dc956c03d6d81e4399eaa", "name": "Yue Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399eab", "name": "Xiaowei Zhou", "hidden": false}, {"_id": "695dc956c03d6d81e4399eac", "name": "Ruizhen Hu", "hidden": false}, {"_id": "695dc956c03d6d81e4399ead", "user": {"_id": "62986ca2b58e71e2ac9b8f01", "avatarUrl": "/avatars/83944db5f3dbb6f47c47c46fb2cb2849.svg", "isPro": false, "fullname": "Sida Peng", "user": "pengsida", "type": "user"}, "name": "Sida Peng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:16:12.074Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6489a01b8de3f9d810b0154f/XlXUo1VjGVhePk-xHRmkj.mp4"], "publishedAt": "2026-01-06T18:57:06.000Z", "submittedOnDailyAt": "2026-01-07T00:26:37.060Z", "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields", "submittedOnDailyBy": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "isPro": false, "fullname": "Haotong Lin", "user": "haotongl", "type": "user"}, "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.", "upvotes": 71, "discussionId": "695dc956c03d6d81e4399eae", "ai_summary": "InfiniDepth represents depth as neural implicit fields using a local implicit decoder, enabling continuous 2D coordinate querying for arbitrary-resolution depth estimation and superior performance in fine-detail regions.", "ai_keywords": ["neural implicit fields", "local implicit decoder", "continuous 2D coordinates", "arbitrary-resolution depth estimation", "synthetic benchmark", "4K synthetic benchmark", "novel view synthesis", "viewpoint shifts"], "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u53ea\u80fd\u5728\u79bb\u6563\u56fe\u50cf\u7f51\u683c\u4e0a\u9884\u6d4b\u6df1\u5ea6\uff0c\u9650\u5236\u4e86\u5176\u6269\u5c55\u6027\u548c\u51e0\u4f55\u7ec6\u8282\u7684\u6062\u590d\u3002</li>\n    <li>\u672c\u6587\u4ecb\u7ecd\u4e86InfiniDepth\uff0c\u901a\u8fc7\u795e\u7ecf\u9690\u5f0f\u573a\u8868\u793a\u6df1\u5ea6\uff0c\u80fd\u591f\u5728\u8fde\u7eed\u76842D\u5750\u6807\u4e0a\u67e5\u8be2\u6df1\u5ea6\u3002</li>\n    <li>InfiniDepth\u652f\u6301\u4efb\u610f\u5206\u8fa8\u7387\u548c\u7ec6\u81f4\u7684\u6df1\u5ea6\u4f30\u8ba1\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u76844K\u5408\u6210\u57fa\u51c6\uff0c\u5305\u542b\u6765\u81ea\u4e94\u6b3e\u4e0d\u540c\u6e38\u620f\u7684\u4e30\u5bcc\u573a\u666f\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cInfiniDepth\u5728\u6df1\u5ea6\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u79c0\uff0c\u7279\u522b\u662f\u5728\u7ec6\u8282\u533a\u57df\uff0c\u5e76\u4e14\u5728\u5927\u89c6\u89d2\u53d8\u5316\u4e0b\u7684\u65b0\u89c6\u56fe\u5408\u6210\u4efb\u52a1\u4e2d\u6548\u679c\u826f\u597d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current methods for depth estimation are limited to fixed image grids, making it hard to scale and capture detailed geometry.</li>\n    <li>InfiniDepth uses neural implicit fields to allow depth estimation at any resolution and detail level.</li>\n    <li>The method includes a local implicit decoder that lets us query depth at continuous 2D points.</li>\n    <li>A new 4K synthetic benchmark was created from five different games to test InfiniDepth's performance.</li>\n    <li>InfiniDepth shows top performance in depth estimation tasks and produces high-quality results for generating new views with fewer errors.</li>\n</ul>"}, "publishedAt": "2026-01-06T13:57:06.000Z", "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields", "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6489a01b8de3f9d810b0154f/XlXUo1VjGVhePk-xHRmkj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03252.png", "numComments": 8, "submittedBy": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "fullname": "Haotong Lin", "name": "haotongl", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.02151", "authors": [{"_id": "695f2d8a5fa3847525c41f8d", "user": {"_id": "6768c97367e4b4606a3c9cec", "avatarUrl": "/avatars/5ddafe7a05828366f66c79072556f370.svg", "isPro": false, "fullname": "diaomuxi", "user": "diaomuxi", "type": "user"}, "name": "Muxi Diao", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:31:56.507Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f8e", "user": {"_id": "666a6cf89a3e3ce05a519bcc", "avatarUrl": "/avatars/9e72481deec3bd5c5202e42c32894a32.svg", "isPro": false, "fullname": "\u6768\u4e50\u4e50", "user": "ssl-asuka", "type": "user"}, "name": "Lele Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:31:51.246Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f8f", "user": {"_id": "64c0f972d76592ba899c2c9c", "avatarUrl": "/avatars/d6940beb135f99241c6fb2cf0e8ccdbe.svg", "isPro": false, "fullname": "GongWuxuan", "user": "Wuxuan-Gong", "type": "user"}, "name": "Wuxuan Gong", "status": "admin_assigned", "statusLastChangedAt": "2026-01-08T08:44:19.470Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f90", "name": "Yutong Zhang", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f91", "user": {"_id": "64fbd4e69a62bb2791b3a665", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fbd4e69a62bb2791b3a665/ZEMtU8O0z98ryeRCG3l_K.jpeg", "isPro": false, "fullname": "Zhonghao Yan", "user": "zzzyzh", "type": "user"}, "name": "Zhonghao Yan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:31:53.904Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f92", "name": "Yufei Han", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f93", "user": {"_id": "67f4a56928cbc4f2f75c008d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qoX8HT0JsjqW2OQoENSvg.png", "isPro": false, "fullname": "Kongming Liang", "user": "KongmingLiang", "type": "user"}, "name": "Kongming Liang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-08T08:44:38.812Z", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f94", "name": "Weiran Xu", "hidden": false}, {"_id": "695f2d8a5fa3847525c41f95", "name": "Zhanyu Ma", "hidden": false}], "publishedAt": "2026-01-05T14:28:17.000Z", "submittedOnDailyAt": "2026-01-08T01:42:04.476Z", "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting", "submittedOnDailyBy": {"_id": "666a6cf89a3e3ce05a519bcc", "avatarUrl": "/avatars/9e72481deec3bd5c5202e42c32894a32.svg", "isPro": false, "fullname": "\u6768\u4e50\u4e50", "user": "ssl-asuka", "type": "user"}, "summary": "Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as \"Confident Conflicts\" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.", "upvotes": 64, "discussionId": "695f2d8b5fa3847525c41f96", "projectPage": "https://ymxyll.github.io/EAFT/", "githubRepo": "https://github.com/PRIS-CV/EAFT", "githubRepoAddedBy": "user", "ai_summary": "Entropy-Adaptive Fine-Tuning addresses catastrophic forgetting in supervised fine-tuning by using token-level entropy to distinguish uncertainty from knowledge conflict, enabling better preservation of general capabilities.", "ai_keywords": ["supervised fine-tuning", "catastrophic forgetting", "on-policy reinforcement learning", "distributional gap", "Confident Conflicts", "token-level entropy", "epistemic uncertainty", "knowledge conflict", "gradient updates", "downstream performance"], "githubStars": 18, "organization": {"_id": "64283c0c68faf6ddab552684", "name": "BUPT-PRIS", "fullname": "BUPT AI PRIS"}, "summary_zh": "<ul>\n    <li>\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5bb9\u6613\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u800c\u57fa\u4e8e\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002</li>\n    <li>RL\u4e0e\u6a21\u578b\u5185\u90e8\u4fe1\u5ff5\u76f8\u4e00\u81f4\uff0c\u800cSFT\u5219\u4f9d\u8d56\u5916\u90e8\u76d1\u7763\uff0c\u8fd9\u9020\u6210\u4e86\u57fa\u672c\u7684\u5206\u5e03\u5dee\u5f02\u3002</li>\n    <li>\u5728\u201c\u81ea\u4fe1\u51b2\u7a81\u201d\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u5bf9\u81ea\u8eab\u9884\u6d4b\u975e\u5e38\u81ea\u4fe1\uff0c\u4f46\u5374\u88ab\u8feb\u5b66\u4e60\u4e0d\u540c\u7684\u771f\u5b9e\u60c5\u51b5\uff0c\u5bfc\u81f4\u7834\u574f\u6027\u7684\u68af\u5ea6\u66f4\u65b0\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u71b5\u81ea\u9002\u5e94\u5fae\u8c03\uff08EAFT\uff09\uff0c\u5b83\u5229\u7528\u6807\u8bb0\u7ea7\u71b5\u6765\u533a\u5206\u4e0d\u786e\u5b9a\u6027\u548c\u77e5\u8bc6\u51b2\u7a81\uff0c\u4ece\u800c\u6539\u5584\u5b66\u4e60\u8fc7\u7a0b\u3002</li>\n    <li>\u5728\u591a\u4e2a\u9886\u57df\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEAFT\u80fd\u591f\u7ef4\u6301\u6807\u51c6SFT\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u51cf\u8f7b\u901a\u7528\u80fd\u529b\u7684\u4e0b\u964d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Supervised Fine-Tuning (SFT) often leads to catastrophic forgetting, losing the model's general skills.</li>\n    <li>On-policy Reinforcement Learning (RL) helps maintain the model\u2019s general abilities without losing knowledge.</li>\n    <li>The problem arises because SFT makes the model adapt to external instructions, which can conflict with its own understanding.</li>\n    <li>To solve this, a new method called Entropy-Adaptive Fine-Tuning (EAFT) is introduced, which uses uncertainty levels to manage learning from conflicting data.</li>\n    <li>EAFT has been tested on various large models and shows similar performance to SFT while better preserving the model\u2019s overall capabilities.</li>\n</ul>"}, "publishedAt": "2026-01-05T09:28:17.000Z", "title": "Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting", "summary": "Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as \"Confident Conflicts\" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02151.png", "numComments": 6, "submittedBy": {"_id": "666a6cf89a3e3ce05a519bcc", "avatarUrl": "/avatars/9e72481deec3bd5c5202e42c32894a32.svg", "fullname": "\u6768\u4e50\u4e50", "name": "ssl-asuka", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "64283c0c68faf6ddab552684", "name": "BUPT-PRIS", "fullname": "BUPT AI PRIS"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03509", "authors": [{"_id": "695fbc7d5b7998385e639349", "name": "Haochen Shi", "hidden": false}, {"_id": "695fbc7d5b7998385e63934a", "name": "Xingdi Yuan", "hidden": false}, {"_id": "695fbc7d5b7998385e63934b", "name": "Bang Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/654a97282d2fcd6bf2851173/sAKIzhLgfcEhgVZMBkHRW.png"], "publishedAt": "2026-01-07T01:43:25.000Z", "submittedOnDailyAt": "2026-01-08T11:50:05.687Z", "title": "Evolving Programmatic Skill Networks", "submittedOnDailyBy": {"_id": "654a97282d2fcd6bf2851173", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png", "isPro": false, "fullname": "Bang Liu", "user": "Bang-UdeM-Mila", "type": "user"}, "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.", "upvotes": 54, "discussionId": "695fbc7e5b7998385e63934c", "ai_summary": "Programmatic Skill Network enables continual skill acquisition through executable symbolic programs that evolve via reflection, progressive optimization, and structural refactoring mechanisms.", "ai_keywords": ["Programmatic Skill Network", "executable symbolic programs", "skill composition", "structured fault localization", "progressive optimization", "maturity-aware update gating", "canonical structural refactoring", "rollback validation", "neural network training", "skill reuse", "rapid adaptation", "generalization"], "organization": {"_id": "636e93488ba65db4a0987ab4", "name": "Universite-de-Montreal", "fullname": "Universit\u00e9 de Montr\u00e9al"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u7814\u7a76\u5728\u5f00\u653e\u5f0f\u73af\u5883\u4e2d\u4e0d\u65ad\u5b66\u4e60\u6280\u80fd\uff0c\u4ee3\u7406\u9700\u8981\u6784\u5efa\u3001\u5b8c\u5584\u548c\u91cd\u7528\u6280\u80fd\u5e93\u3002</li>\n    <li>\u5f15\u5165\u4e86\u7a0b\u5e8f\u5316\u6280\u80fd\u7f51\u7edc\uff08PSN\uff09\uff0c\u5b83\u5c06\u6280\u80fd\u89c6\u4e3a\u53ef\u6267\u884c\u7684\u7b26\u53f7\u7a0b\u5e8f\uff0c\u5f62\u6210\u4e00\u4e2a\u968f\u7740\u7ecf\u9a8c\u800c\u53d1\u5c55\u7684\u7f51\u7edc\u3002</li>\n    <li>PSN\u5305\u62ec\u4e09\u79cd\u6838\u5fc3\u673a\u5236\uff1a\u53cd\u601d\u3001\u9010\u6b65\u4f18\u5316\u548c\u6807\u51c6\u7ed3\u6784\u91cd\u6784\u3002</li>\n    <li>PSN\u7684\u5b66\u4e60\u52a8\u6001\u4e0e\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6709\u76f8\u4f3c\u4e4b\u5904\u3002</li>\n    <li>\u5728MineDojo\u548cCrafter\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cPSN\u80fd\u591f\u6709\u6548\u91cd\u7528\u6280\u80fd\u3001\u5feb\u901f\u9002\u5e94\u5e76\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u79c0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study focuses on how agents can continuously learn and improve skills in complex environments.</li>\n    <li>A new framework called the Programmatic Skill Network (PSN) is introduced, allowing skills to be represented as executable programs that can grow over time.</li>\n    <li>PSN includes three main features: fault detection in skill use, a method for optimizing skills while keeping reliable ones stable, and a way to simplify the network structure.</li>\n    <li>Experiments show that PSN allows for effective reuse of skills, quick adaptation to new tasks, and strong performance across various challenges.</li>\n    <li>The researchers plan to make the code available to the public for further exploration and use.</li>\n</ul>"}, "publishedAt": "2026-01-06T20:43:25.000Z", "title": "Evolving Programmatic Skill Networks", "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/654a97282d2fcd6bf2851173/sAKIzhLgfcEhgVZMBkHRW.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03509.png", "numComments": 1, "submittedBy": {"_id": "654a97282d2fcd6bf2851173", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png", "fullname": "Bang Liu", "name": "Bang-UdeM-Mila", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10, "isUserFollowing": false}, "organization": {"_id": "636e93488ba65db4a0987ab4", "name": "Universite-de-Montreal", "fullname": "Universit\u00e9 de Montr\u00e9al"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.02204", "authors": [{"_id": "695c7d0d6aa73bc11f091433", "name": "Huichao Zhang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091434", "user": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "isPro": false, "fullname": "Liao Qu", "user": "leo1117", "type": "user"}, "name": "Liao Qu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:57:57.686Z", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091435", "name": "Yiheng Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091436", "name": "Hang Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091437", "name": "Yangyang Song", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091438", "name": "Yongsheng Dong", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091439", "name": "Shikun Sun", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143a", "name": "Xian Li", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143b", "name": "Xu Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143c", "user": {"_id": "6344dcb1cd37e44d9ed46508", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344dcb1cd37e44d9ed46508/J92UKSxKR3iziD2WJfih4.jpeg", "isPro": false, "fullname": "Yi Jiang", "user": "JiangYi", "type": "user"}, "name": "Yi Jiang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:57:55.158Z", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143d", "name": "Hu Ye", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143e", "name": "Bo Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09143f", "name": "Yiming Gao", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091440", "name": "Peng Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091441", "name": "Akide Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091442", "name": "Zhipeng Yang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091443", "name": "Qili Deng", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091444", "name": "Linjie Xing", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091445", "name": "Jiyang Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091446", "name": "Zhao Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091447", "name": "Yang Zhou", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091448", "name": "Mingcong Liu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091449", "name": "Yi Zhang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144a", "name": "Qian He", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144b", "name": "Xiwei Hu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144c", "name": "Zhongqi Qi", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144d", "name": "Jie Shao", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144e", "name": "Zhiye Fu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f09144f", "name": "Shuai Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091450", "name": "Fangmin Chen", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091451", "name": "Xuezhi Chai", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091452", "name": "Zhihua Wu", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091453", "name": "Yitong Wang", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091454", "name": "Zehuan Yuan", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091455", "name": "Daniel K. Du", "hidden": false}, {"_id": "695c7d0d6aa73bc11f091456", "name": "Xinglong Wu", "hidden": false}], "publishedAt": "2026-01-05T15:27:04.000Z", "submittedOnDailyAt": "2026-01-06T00:52:35.953Z", "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation", "submittedOnDailyBy": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "isPro": false, "fullname": "Liao Qu", "user": "leo1117", "type": "user"}, "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.", "upvotes": 45, "discussionId": "695c7d0d6aa73bc11f091457", "githubRepo": "https://github.com/ByteVisionLab/NextFlow", "githubRepoAddedBy": "user", "ai_summary": "NextFlow is a unified decoder-only autoregressive transformer that processes interleaved text-image tokens, enabling fast multimodal generation through novel next-token and next-scale prediction strategies.", "ai_keywords": ["decoder-only autoregressive transformer", "interleaved text-image discrete tokens", "unified vision representation", "multimodal understanding", "multimodal generation", "next-token prediction", "next-scale prediction", "raster-scan methods", "visual generation", "prefix-tuning strategy", "reinforcement learning", "diffusion baselines"], "githubStars": 60, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>NextFlow\u662f\u4e00\u79cd\u7edf\u4e00\u7684\u89e3\u7801\u5668\u81ea\u56de\u5f52\u53d8\u6362\u5668\uff0c\u8bad\u7ec3\u4e866\u4e07\u4ebf\u4e2a\u6587\u672c-\u56fe\u50cf\u79bb\u6563\u7b26\u53f7\u3002</li>\n    <li>\u5b83\u80fd\u591f\u5b9e\u73b0\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\uff0c\u652f\u6301\u56fe\u50cf\u7f16\u8f91\u3001\u4ea4\u9519\u5185\u5bb9\u548c\u89c6\u9891\u751f\u6210\u3002</li>\n    <li>\u6587\u672c\u91c7\u7528\u4e0b\u4e00\u4e2a\u7b26\u53f7\u9884\u6d4b\uff0c\u800c\u56fe\u50cf\u751f\u6210\u5219\u91c7\u7528\u4e0b\u4e00\u4e2a\u5c3a\u5ea6\u9884\u6d4b\uff0c\u8fd9\u6837\u53ef\u4ee5\u66f4\u5feb\u751f\u62101024x1024\u7684\u56fe\u50cf\u3002</li>\n    <li>NextFlow\u7684\u56fe\u50cf\u751f\u6210\u901f\u5ea6\u6bd4\u5176\u4ed6\u81ea\u56de\u5f52\u6a21\u578b\u5feb\u5f97\u591a\uff0c\u4ec5\u97005\u79d2\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cNextFlow\u5728\u7edf\u4e00\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u4e0e\u4e13\u4e1a\u7684\u6269\u6563\u57fa\u7ebf\u76f8\u5ab2\u7f8e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NextFlow is a new AI model that combines text and images, trained on a huge amount of data (6 trillion tokens).</li>\n    <li>It can understand and create both text and images, allowing for tasks like image editing and video creation.</li>\n    <li>The model uses a unique approach to generate images quickly, producing high-quality 1024x1024 images in just 5 seconds.</li>\n    <li>NextFlow has improved training methods to ensure stable and high-quality results during image generation.</li>\n    <li>It performs exceptionally well compared to other models and matches the quality of specialized image generation models.</li>\n</ul>"}, "publishedAt": "2026-01-05T10:27:04.000Z", "title": "NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation", "summary": "We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.02204.png", "numComments": 1, "submittedBy": {"_id": "64b796079ebb7e6c7ddcdabf", "avatarUrl": "/avatars/51af43cab078705b8745b4f942f542e5.svg", "fullname": "Liao Qu", "name": "leo1117", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.01739", "authors": [{"_id": "695c72346aa73bc11f0913bf", "user": {"_id": "6044fd39e6aa3e130cb92867", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6044fd39e6aa3e130cb92867/L5hb8vpHY6SKMEL-Xacma.jpeg", "isPro": false, "fullname": "Eunbi Choi", "user": "unbiarirang", "type": "user"}, "name": "Eunbi Choi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:17.472Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c0", "user": {"_id": "64d31ca9465b6039259838df", "avatarUrl": "/avatars/b3bde5067ed3fcd908d3d91c00680bfb.svg", "isPro": false, "fullname": "kibong choi", "user": "bongchoi", "type": "user"}, "name": "Kibong Choi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:11.322Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c1", "name": "Seokhee Hong", "hidden": false}, {"_id": "695c72346aa73bc11f0913c2", "user": {"_id": "63c50e590c24c8b53958f75e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673858632881-noauth.png", "isPro": false, "fullname": "Junwon Hwang", "user": "nuxlear", "type": "user"}, "name": "Junwon Hwang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:09.333Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c3", "name": "Hyojin Jeon", "hidden": false}, {"_id": "695c72346aa73bc11f0913c4", "user": {"_id": "66a9e066a203add977948988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a9e066a203add977948988/mwVS-vt-8p-DFC5T9H9H3.jpeg", "isPro": false, "fullname": "hyunjik.jo", "user": "switiz87", "type": "user"}, "name": "Hyunjik Jo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:13.551Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913c5", "name": "Joonkee Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c6", "name": "Seonghwan Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c7", "name": "Soyeon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c8", "name": "Sunkyoung Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913c9", "name": "Yireun Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913ca", "name": "Yongil Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913cb", "name": "Haeju Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913cc", "name": "Jinsik Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913cd", "name": "Kyungmin Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ce", "name": "Sangha Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913cf", "name": "Heuiyeen Yeen", "hidden": false}, {"_id": "695c72346aa73bc11f0913d0", "name": "Hwan Chang", "hidden": false}, {"_id": "695c72346aa73bc11f0913d1", "name": "Stanley Jungkyu Choi", "hidden": false}, {"_id": "695c72346aa73bc11f0913d2", "name": "Yejin Choi", "hidden": false}, {"_id": "695c72346aa73bc11f0913d3", "name": "Jiwon Ham", "hidden": false}, {"_id": "695c72346aa73bc11f0913d4", "name": "Kijeong Jeon", "hidden": false}, {"_id": "695c72346aa73bc11f0913d5", "name": "Geunyeong Jeong", "hidden": false}, {"_id": "695c72346aa73bc11f0913d6", "name": "Gerrard Jeongwon Jo", "hidden": false}, {"_id": "695c72346aa73bc11f0913d7", "name": "Yonghwan Jo", "hidden": false}, {"_id": "695c72346aa73bc11f0913d8", "name": "Jiyeon Jung", "hidden": false}, {"_id": "695c72346aa73bc11f0913d9", "name": "Naeun Kang", "hidden": false}, {"_id": "695c72346aa73bc11f0913da", "name": "Dohoon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913db", "name": "Euisoon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913dc", "name": "Hayeon Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913dd", "name": "Hyosang Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913de", "name": "Hyunseo Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913df", "name": "Jieun Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e0", "name": "Minu Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e1", "name": "Myoungshin Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e2", "name": "Unsol Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e3", "name": "Youchul Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e4", "name": "YoungJin Kim", "hidden": false}, {"_id": "695c72346aa73bc11f0913e5", "name": "Chaeeun Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e6", "name": "Chaeyoon Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e7", "user": {"_id": "6399ab9e92e12136b99ef60e", "avatarUrl": "/avatars/b76895c53f0f046586555c20292c78a1.svg", "isPro": false, "fullname": "Changhun Lee", "user": "xvyaward", "type": "user"}, "name": "Changhun Lee", "status": "claimed_verified", "statusLastChangedAt": "2026-01-06T09:58:15.420Z", "hidden": false}, {"_id": "695c72346aa73bc11f0913e8", "name": "Dahm Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913e9", "name": "Edward Hwayoung Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ea", "name": "Honglak Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913eb", "name": "Jinsang Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ec", "name": "Jiyoung Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ed", "name": "Sangeun Lee", "hidden": false}, {"_id": "695c72346aa73bc11f0913ee", "name": "Seungwon Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913ef", "name": "Solji Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913f0", "name": "Woohyung Lim", "hidden": false}, {"_id": "695c72346aa73bc11f0913f1", "name": "Chanwoo Moon", "hidden": false}, {"_id": "695c72346aa73bc11f0913f2", "name": "Jaewoo Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f3", "name": "Jinho Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f4", "name": "Yongmin Park", "hidden": false}, {"_id": "695c72346aa73bc11f0913f5", "name": "Hyerin Seo", "hidden": false}, {"_id": "695c72346aa73bc11f0913f6", "name": "Wooseok Seo", "hidden": false}, {"_id": "695c72346aa73bc11f0913f7", "name": "Yongwoo Song", "hidden": false}, {"_id": "695c72346aa73bc11f0913f8", "name": "Sejong Yang", "hidden": false}, {"_id": "695c72346aa73bc11f0913f9", "name": "Sihoon Yang", "hidden": false}, {"_id": "695c72346aa73bc11f0913fa", "name": "Chang En Yea", "hidden": false}, {"_id": "695c72346aa73bc11f0913fb", "name": "Sihyuk Yi", "hidden": false}, {"_id": "695c72346aa73bc11f0913fc", "name": "Chansik Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913fd", "name": "Dongkeun Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913fe", "name": "Sangyeon Yoon", "hidden": false}, {"_id": "695c72346aa73bc11f0913ff", "name": "Hyeongu Yun", "hidden": false}], "publishedAt": "2026-01-05T02:30:59.000Z", "submittedOnDailyAt": "2026-01-06T01:03:14.011Z", "title": "K-EXAONE Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.", "upvotes": 44, "discussionId": "695c72356aa73bc11f091400", "githubRepo": "https://github.com/LG-AI-EXAONE/K-EXAONE", "githubRepoAddedBy": "user", "ai_summary": "K-EXAONE is a multilingual language model with a Mixture-of-Experts architecture that achieves competitive performance on various benchmarks while supporting multiple languages and long-context windows.", "ai_keywords": ["Mixture-of-Experts", "256K-token context window", "multilingual language model", "parameter-efficient fine-tuning"], "githubStars": 39, "organization": {"_id": "66a89bc1d96a5adbccbe85d4", "name": "LGAI-EXAONE", "fullname": "LG AI Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a899a72f11aaf66001a8dc/UfdrP3GMo9pNT62BaMnhw.png"}, "summary_zh": "<ul>\n    <li>K-EXAONE\u662fLG AI\u7814\u7a76\u5f00\u53d1\u7684\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u6a21\u578b\uff0c\u53c2\u6570\u603b\u91cf\u4e3a2360\u4ebf\u3002</li>\n    <li>\u91c7\u7528Mixture-of-Experts\u67b6\u6784\uff0c\u5728\u63a8\u7406\u65f6\u6fc0\u6d3b230\u4ebf\u53c2\u6570\uff0c\u652f\u6301256K-token\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002</li>\n    <li>\u652f\u6301\u516d\u79cd\u8bed\u8a00\uff1a\u97e9\u8bed\u3001\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u5fb7\u8bed\u3001\u65e5\u8bed\u548c\u8d8a\u5357\u8bed\u3002</li>\n    <li>\u5728\u63a8\u7406\u3001\u667a\u80fd\u4ee3\u7406\u3001\u901a\u7528\u80fd\u529b\u3001\u97e9\u8bed\u548c\u591a\u8bed\u8a00\u80fd\u529b\u7684\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0e\u540c\u7c7b\u5f00\u6e90\u6a21\u578b\u76f8\u5f53\u3002</li>\n    <li>K-EXAONE\u65e8\u5728\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u5de5\u4e1a\u548c\u7814\u7a76\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>K-EXAONE is a large multilingual language model created by LG AI Research.</li>\n    <li>It has 236 billion total parameters, using 23 billion during processing.</li>\n    <li>The model can handle a context of 256,000 tokens and supports six languages: Korean, English, Spanish, German, Japanese, and Vietnamese.</li>\n    <li>K-EXAONE has been tested on various benchmarks and performs similarly to other models of its size.</li>\n    <li>It aims to improve AI technologies for various industries and research purposes.</li>\n</ul>"}, "publishedAt": "2026-01-04T21:30:59.000Z", "title": "K-EXAONE Technical Report", "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.01739.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 200, "isUserFollowing": false}, "organization": {"_id": "66a89bc1d96a5adbccbe85d4", "name": "LGAI-EXAONE", "fullname": "LG AI Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66a899a72f11aaf66001a8dc/UfdrP3GMo9pNT62BaMnhw.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.03233", "authors": [{"_id": "695dc6d9c03d6d81e4399e85", "user": {"_id": "6303cc5e0547362a22a51af0", "avatarUrl": "/avatars/8f3348f121565bf6c5e1af0e559a43a3.svg", "isPro": false, "fullname": "Yoav HaCohen", "user": "yoavhacohen", "type": "user"}, "name": "Yoav HaCohen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:29.722Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e86", "user": {"_id": "6489c487b9e9258ba065418f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6489c487b9e9258ba065418f/6rzmV3bQ3YxswG6NP2hDW.png", "isPro": false, "fullname": "Benny Brazowski", "user": "benibraz", "type": "user"}, "name": "Benny Brazowski", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:35.981Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e87", "user": {"_id": "62dd30a8d43078cd49ac8ad8", "avatarUrl": "/avatars/ad599719290637f7817b7508a91c2e2c.svg", "isPro": false, "fullname": "Nisan Chiprut", "user": "nisan", "type": "user"}, "name": "Nisan Chiprut", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:41.634Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e88", "user": {"_id": "64a7adc087cbd4dc7301fdd6", "avatarUrl": "/avatars/b4ec4c3a0409af8ec4a5de05db453034.svg", "isPro": false, "fullname": "Yaki Bitterman", "user": "jacobitterman", "type": "user"}, "name": "Yaki Bitterman", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:46.749Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e89", "user": {"_id": "65897258509bcae23fa162c9", "avatarUrl": "/avatars/29d277a0c425c936e25e82e79caa10a4.svg", "isPro": false, "fullname": "Andrew Kvochko", "user": "kvochko", "type": "user"}, "name": "Andrew Kvochko", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:51.722Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8a", "name": "Avishai Berkowitz", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8b", "name": "Daniel Shalem", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8c", "user": {"_id": "681af83e2f4aaa88639e703d", "avatarUrl": "/avatars/d69b664daad0afb529440c14fdb9bc3a.svg", "isPro": false, "fullname": "Daphna Lifschitz", "user": "Daphnal", "type": "user"}, "name": "Daphna Lifschitz", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:04.180Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8d", "user": {"_id": "636b97a57631fe5e86fe1fa2", "avatarUrl": "/avatars/c568ae26fd4fc2655cd12f15d539db58.svg", "isPro": false, "fullname": "Dudu Moshe", "user": "dudumoshe", "type": "user"}, "name": "Dudu Moshe", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:13.512Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8e", "name": "Eitan Porat", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8f", "user": {"_id": "677a422979d3c32a5dd87a0a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WUa6E68GpnT2mEMJ41nDd.png", "isPro": false, "fullname": "Eitan Richardson", "user": "eitanrich", "type": "user"}, "name": "Eitan Richardson", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:22.677Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e90", "user": {"_id": "673f6911d83832a6ce15e7bf", "avatarUrl": "/avatars/0da6cded3b0e785241a6ba5fdb5d8ceb.svg", "isPro": false, "fullname": "Guy Shiran", "user": "guysrn", "type": "user"}, "name": "Guy Shiran", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:28.250Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e91", "user": {"_id": "65744a2fe09de6aa74026d80", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65744a2fe09de6aa74026d80/kCxIKdeBJwAPKmvlm7fDP.jpeg", "isPro": false, "fullname": "Itay Chachy", "user": "ItayChachy", "type": "user"}, "name": "Itay Chachy", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:36.781Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e92", "name": "Jonathan Chetboun", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e93", "user": {"_id": "6678365ac411b340b32d6148", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6678365ac411b340b32d6148/7OhHzbu65pa95eYrAbbLW.jpeg", "isPro": false, "fullname": "Michael Finkelson", "user": "MichaelFinkelson", "type": "user"}, "name": "Michael Finkelson", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:53.574Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e94", "user": {"_id": "6318aa43cb4ca740c4c55651", "avatarUrl": "/avatars/24082c776d284393a5a38a99e5c0bab8.svg", "isPro": false, "fullname": "michael kupchick", "user": "michaellightricks", "type": "user"}, "name": "Michael Kupchick", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:59.945Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e95", "user": {"_id": "673f29b568595672b8d3e90e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673f29b568595672b8d3e90e/4sYADg3mpqMKmJ4fQwaTl.png", "isPro": false, "fullname": "Nir Zabari", "user": "NirZabariLTX", "type": "user"}, "name": "Nir Zabari", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:07.297Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e96", "user": {"_id": "64ae89c043dda9449a1eb1ba", "avatarUrl": "/avatars/12cf3de929d38ddd92cc3f3337dc2ed2.svg", "isPro": false, "fullname": "Nitzan Guetta", "user": "nitzanguetta", "type": "user"}, "name": "Nitzan Guetta", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:14.712Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e97", "name": "Noa Kotler", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e98", "user": {"_id": "631f58935ba8c026340b377c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631f58935ba8c026340b377c/4yoHLdNE99VBb7ji_Mzzj.jpeg", "isPro": false, "fullname": "Ofir Bibi", "user": "ofirbibi", "type": "user"}, "name": "Ofir Bibi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:27.196Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e99", "user": {"_id": "674348b46215a2c0878e219b", "avatarUrl": "/avatars/8a213e431a1583d1a93377410907c059.svg", "isPro": false, "fullname": "Ori Gordon", "user": "origordon", "type": "user"}, "name": "Ori Gordon", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:34.878Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9a", "name": "Poriya Panet", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9b", "name": "Roi Benita", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9c", "name": "Shahar Armon", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9d", "name": "Victor Kulikov", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9e", "name": "Yaron Inger", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9f", "name": "Yonatan Shiftan", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399ea0", "name": "Zeev Melumian", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399ea1", "name": "Zeev Farbman", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cYoXYuK3pjt85pl5-fvUv.mp4"], "publishedAt": "2026-01-06T18:24:41.000Z", "submittedOnDailyAt": "2026-01-07T00:07:29.528Z", "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.", "upvotes": 40, "discussionId": "695dc6d9c03d6d81e4399ea2", "projectPage": "https://app.ltx.studio/ltx-2-playground/i2v", "githubRepo": "https://github.com/Lightricks/LTX-2", "githubRepoAddedBy": "user", "ai_summary": "LTX-2 is an open-source audiovisual diffusion model that generates synchronized video and audio content using a dual-stream transformer architecture with cross-modal attention and classifier-free guidance.", "ai_keywords": ["text-to-video diffusion models", "audiovisual content", "dual-stream transformer", "cross-attention layers", "temporal positional embeddings", "AdaLN", "classifier-free guidance", "modality-aware classifier-free guidance", "multilingual text encoder", "diffusion models"], "githubStars": 922, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86LTX-2\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u57fa\u7840\u6a21\u578b\uff0c\u53ef\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u540c\u6b65\u89c6\u542c\u5185\u5bb9\u3002</li>\n    <li>LTX-2\u4f7f\u7528\u4e86\u4e00\u4e2a\u4e0d\u5bf9\u79f0\u7684\u53cc\u6d41\u53d8\u6362\u5668\uff0c\u5305\u542b14B\u53c2\u6570\u7684\u89c6\u9891\u6d41\u548c5B\u53c2\u6570\u7684\u97f3\u9891\u6d41\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u53cc\u5411\u97f3\u89c6\u9891\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\uff0c\u4f7f\u5f97\u89c6\u542c\u751f\u6210\u66f4\u52a0\u9ad8\u6548\u3002</li>\n    <li>LTX-2\u4e0d\u4ec5\u80fd\u751f\u6210\u8bed\u97f3\uff0c\u8fd8\u80fd\u5236\u4f5c\u4e0e\u573a\u666f\u3001\u89d2\u8272\u548c\u60c5\u611f\u76f8\u7b26\u7684\u4e30\u5bcc\u97f3\u8f68\u3002</li>\n    <li>\u5728\u8bc4\u4f30\u4e2d\uff0cLTX-2\u5728\u89c6\u542c\u8d28\u91cf\u548c\u54cd\u5e94\u63d0\u793a\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u5f00\u6e90\u7cfb\u7edf\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u65f6\u95f4\u66f4\u4f4e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LTX-2 is a new model that can create high-quality videos with synchronized audio.</li>\n    <li>The model uses a special architecture with two parts: one for video and one for audio, allowing it to generate both together efficiently.</li>\n    <li>It includes a multilingual text encoder to understand prompts better and has a unique guidance system for better audiovisual alignment.</li>\n    <li>LTX-2 produces not only speech but also rich audio that matches the scenes, including background sounds.</li>\n    <li>The model is open-source and provides high-quality results at a lower cost and faster speed compared to other models.</li>\n</ul>"}, "publishedAt": "2026-01-06T13:24:41.000Z", "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model", "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cYoXYuK3pjt85pl5-fvUv.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03233.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 202, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.03872", "authors": [{"_id": "695f1a475fa3847525c41d06", "user": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "name": "Jinyang Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:32:36.055Z", "hidden": false}, {"_id": "695f1a475fa3847525c41d07", "name": "Guocheng Zhai", "hidden": false}, {"_id": "695f1a475fa3847525c41d08", "name": "Ruihan Jin", "hidden": false}, {"_id": "695f1a475fa3847525c41d09", "name": "Jiahao Yuan", "hidden": false}, {"_id": "695f1a475fa3847525c41d0a", "name": "Yuhao Shen", "hidden": false}, {"_id": "695f1a475fa3847525c41d0b", "name": "Shuai Zhang", "hidden": false}, {"_id": "695f1a475fa3847525c41d0c", "name": "Zhengqi Wen", "hidden": false}, {"_id": "695f1a475fa3847525c41d0d", "name": "Jianhua Tao", "hidden": false}], "publishedAt": "2026-01-07T12:38:33.000Z", "submittedOnDailyAt": "2026-01-08T04:50:03.287Z", "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning", "submittedOnDailyBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "summary": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.", "upvotes": 30, "discussionId": "695f1a475fa3847525c41d0e", "ai_summary": "ATLAS is a dual-path framework that dynamically selects optimal model-tool combinations for cross-domain reasoning through cluster-based routing and reinforcement learning-based multi-step routing, achieving superior performance on complex reasoning tasks.", "ai_keywords": ["large language models", "external tools", "model-tool combination", "high-dimensional optimization", "dual-path framework", "training-free cluster-based routing", "RL-based multi-step routing", "cross-domain complex reasoning", "domain-specific alignment", "out-of-distribution generalization"], "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u5916\u90e8\u5de5\u5177\u7684\u7ed3\u5408\u63d0\u5347\u4e86\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u80fd\u529b\u3002</li>\n    <li>\u968f\u7740\u6a21\u578b\u548c\u5de5\u5177\u7684\u591a\u6837\u5316\uff0c\u9009\u62e9\u6700\u4f73\u7684\u6a21\u578b-\u5de5\u5177\u7ec4\u5408\u53d8\u5f97\u590d\u6742\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u5355\u4e00\u6a21\u578b\u6216\u56fa\u5b9a\u5de5\u5177\u8c03\u7528\u903b\u8f91\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u4e0d\u540c\u6a21\u578b-\u5de5\u5177\u7ec4\u5408\u7684\u6027\u80fd\u5dee\u5f02\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86ATLAS\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u8def\u5f84\u65b9\u6cd5\u5b9e\u73b0\u52a8\u6001\u5de5\u5177\u4f7f\u7528\uff0c\u63d0\u5347\u8de8\u9886\u57df\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>ATLAS\u572815\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5c24\u5176\u5728\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Combining large language models (LLMs) with external tools enhances AI capabilities.</li>\n    <li>Selecting the best model-tool pair is challenging due to their variety and complexity.</li>\n    <li>The paper introduces ATLAS, a framework for better using tools with LLMs in complex tasks.</li>\n    <li>ATLAS uses two methods: one for aligning tools with specific domains and another for exploring new use cases.</li>\n    <li>Tests show ATLAS outperforms other models and methods, especially in visual reasoning tasks.</li>\n</ul>"}, "publishedAt": "2026-01-07T07:38:33.000Z", "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning", "summary": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03872.png", "numComments": 1, "submittedBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "fullname": "Jinyang Wu", "name": "Jinyang23", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03986", "authors": [{"_id": "695f290d5fa3847525c41d7d", "name": "Qi Qian", "hidden": false}, {"_id": "695f290d5fa3847525c41d7e", "user": {"_id": "62ea79dd01ed9b0e8f61ccd3", "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg", "isPro": false, "fullname": "Chengsong Huang", "user": "ChengsongHuang", "type": "user"}, "name": "Chengsong Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:31:58.524Z", "hidden": false}, {"_id": "695f290d5fa3847525c41d7f", "name": "Jingwen Xu", "hidden": false}, {"_id": "695f290d5fa3847525c41d80", "name": "Changze Lv", "hidden": false}, {"_id": "695f290d5fa3847525c41d81", "name": "Muling Wu", "hidden": false}, {"_id": "695f290d5fa3847525c41d82", "name": "Wenhao Liu", "hidden": false}, {"_id": "695f290d5fa3847525c41d83", "name": "Xiaohua Wang", "hidden": false}, {"_id": "695f290d5fa3847525c41d84", "name": "Zhenghua Wang", "hidden": false}, {"_id": "695f290d5fa3847525c41d85", "name": "Zisu Huang", "hidden": false}, {"_id": "695f290d5fa3847525c41d86", "name": "Muzhao Tian", "hidden": false}, {"_id": "695f290d5fa3847525c41d87", "name": "Jianhan Xu", "hidden": false}, {"_id": "695f290d5fa3847525c41d88", "name": "Kun Hu", "hidden": false}, {"_id": "695f290d5fa3847525c41d89", "name": "He-Da Wang", "hidden": false}, {"_id": "695f290d5fa3847525c41d8a", "name": "Yao Hu", "hidden": false}, {"_id": "695f290d5fa3847525c41d8b", "name": "Xuanjing Huang", "hidden": false}, {"_id": "695f290d5fa3847525c41d8c", "name": "Xiaoqing Zheng", "hidden": false}], "publishedAt": "2026-01-07T14:59:03.000Z", "submittedOnDailyAt": "2026-01-08T01:59:31.547Z", "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks", "submittedOnDailyBy": {"_id": "62ea79dd01ed9b0e8f61ccd3", "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg", "isPro": false, "fullname": "Chengsong Huang", "user": "ChengsongHuang", "type": "user"}, "summary": "The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.", "upvotes": 28, "discussionId": "695f290d5fa3847525c41d8d", "ai_summary": "Researchers developed Benchmark^2, a framework with three metrics to evaluate benchmark quality for large language models, revealing significant variations in existing benchmarks and enabling more efficient evaluation through selective benchmark construction.", "ai_keywords": ["Benchmark^2", "cross-benchmark ranking consistency", "discriminability score", "capability alignment deviation", "large language models", "benchmarks", "model rankings", "evaluation performance", "test sets"], "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBenchmark^2\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u7684\u8d28\u91cf\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6307\u6807\uff1a\u57fa\u51c6\u6392\u540d\u4e00\u81f4\u6027\u3001\u53ef\u533a\u5206\u6027\u8bc4\u5206\u548c\u80fd\u529b\u5bf9\u9f50\u504f\u5dee\u3002</li>\n    <li>\u901a\u8fc7\u572815\u4e2a\u57fa\u51c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e8611\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u5206\u6790\u663e\u793a\u73b0\u6709\u57fa\u51c6\u7684\u8d28\u91cf\u5dee\u5f02\u663e\u8457\uff0c\u9009\u62e9\u6027\u6784\u5efa\u57fa\u51c6\u53ef\u4ee5\u51cf\u5c11\u6d4b\u8bd5\u96c6\u7684\u89c4\u6a21\uff0c\u540c\u65f6\u4fdd\u6301\u8bc4\u4f30\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Many benchmarks for testing large language models (LLMs) have emerged, creating a need to evaluate their quality.</li>\n    <li>We introduce Benchmark^2, a framework with three key metrics to assess benchmark quality.</li>\n    <li>These metrics include consistency in rankings compared to other benchmarks, how well a benchmark distinguishes between models, and identifying cases where stronger models fail.</li>\n    <li>We tested 11 LLMs across 15 different benchmarks, focusing on areas like math and reasoning.</li>\n    <li>Our findings show big differences in benchmark quality, and using our metrics can help create effective benchmarks with smaller test sets.</li>\n</ul>"}, "publishedAt": "2026-01-07T09:59:03.000Z", "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks", "summary": "The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03986.png", "numComments": 2, "submittedBy": {"_id": "62ea79dd01ed9b0e8f61ccd3", "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg", "fullname": "Chengsong Huang", "name": "ChengsongHuang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.04890", "authors": [{"_id": "69608e7c5b7998385e639583", "user": {"_id": "64670db15993aa7666cc6022", "avatarUrl": "/avatars/b68caad7e987c095b0cab4d9035aac25.svg", "isPro": false, "fullname": "Maksim Velikanov", "user": "yellowvm", "type": "user"}, "name": "Maksim Velikanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:44.975Z", "hidden": false}, {"_id": "69608e7c5b7998385e639584", "user": {"_id": "6697a9fb6d173ec7382e0392", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697a9fb6d173ec7382e0392/Q_myrIBbdWI3RmEvEHrtQ.jpeg", "isPro": false, "fullname": "Ilyas Chahed", "user": "IChahed", "type": "user"}, "name": "Ilyas Chahed", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T15:46:04.314Z", "hidden": false}, {"_id": "69608e7c5b7998385e639585", "user": {"_id": "6460c3811db65f878513bcaf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg", "isPro": false, "fullname": "Jingwei Zuo", "user": "JingweiZuo", "type": "user"}, "name": "Jingwei Zuo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:34:49.874Z", "hidden": false}, {"_id": "69608e7c5b7998385e639586", "name": "Dhia Eddine Rhaiem", "hidden": false}, {"_id": "69608e7c5b7998385e639587", "user": {"_id": "62441d1d9fdefb55a0b7d12c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648631057413-noauth.png", "isPro": false, "fullname": "Younes B", "user": "ybelkada", "type": "user"}, "name": "Younes Belkada", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:34:47.914Z", "hidden": false}, {"_id": "69608e7c5b7998385e639588", "user": {"_id": "6471d727a2b0a376b8b6a4ed", "avatarUrl": "/avatars/aedda547f2ca40dfa898e76be787952f.svg", "isPro": false, "fullname": "Hakim Hacid", "user": "HakimHacid", "type": "user"}, "name": "Hakim Hacid", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:56.651Z", "hidden": false}], "publishedAt": "2026-01-08T12:41:49.000Z", "submittedOnDailyAt": "2026-01-09T02:55:50.938Z", "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers", "submittedOnDailyBy": {"_id": "6460c3811db65f878513bcaf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg", "isPro": false, "fullname": "Jingwei Zuo", "user": "JingweiZuo", "type": "user"}, "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.", "upvotes": 28, "discussionId": "69608e7c5b7998385e639589", "projectPage": "https://tiiuae.github.io/Falcon-H1/", "githubRepo": "https://github.com/tiiuae/falcon-h1", "githubRepoAddedBy": "user", "ai_summary": "Learnable multipliers are introduced to address weight decay-induced normalization artifacts in large language model training, outperforming traditional methods while reducing computational overhead.", "ai_keywords": ["weight decay", "stochastic gradient noise", "Brownian-like expansion", "WD-noise equilibrium", "learnable multipliers", "matrix layers", "weight norm", "muP multipliers", "Adam optimizer", "Muon optimizer"], "githubStars": 98, "organization": {"_id": "6448cad23adf50d86406b0a3", "name": "tiiuae", "fullname": "Technology Innovation Institute", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"}, "summary_zh": "<ul>\n    <li>\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u4e2d\uff0c\u5e94\u7528\u6743\u91cd\u8870\u51cf\uff08WD\uff09\u662f\u5e38\u89c1\u505a\u6cd5\u3002</li>\n    <li>\u4ee5\u5f80\u7814\u7a76\u8868\u660e\uff0c\u968f\u673a\u68af\u5ea6\u566a\u58f0\u5bfc\u81f4\u6743\u91cd\u77e9\u9635\u7684\u6269\u5c55\uff0cWD\u53ef\u4ee5\u62b5\u6d88\u8fd9\u79cd\u589e\u957f\u3002</li>\n    <li>\u672c\u7814\u7a76\u8ba4\u4e3a\u8fd9\u79cd\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u5e73\u8861\u6743\u91cd\u8303\u6570\u662f\u6709\u5bb3\u7684\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u4e58\u5b50\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u53d1\u73b0\uff0c\u5b66\u4e60\u5230\u7684\u4e58\u5b50\u53ef\u4ee5\u9002\u5e94\u6570\u636e\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u8fd9\u79cd\u65b9\u6cd5\u5728\u4f7f\u7528Adam\u548cMuon\u4f18\u5316\u5668\u65f6\u90fd\u80fd\u63d0\u9ad8\u4e0b\u6e38\u8bc4\u4f30\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Weight decay (WD) is commonly used in training large language models to control the growth of weight matrices.</li>\n    <li>The authors propose a new method using learnable multipliers to improve the scaling of weight matrices, addressing issues with the standard WD-noise balance.</li>\n    <li>By using a single learnable multiplier for the entire weight matrix, they found that performance improved compared to the fixed WD scale.</li>\n    <li>The approach was further enhanced by introducing per-row and per-column multipliers, allowing for more flexibility and better performance.</li>\n    <li>The new method outperformed traditional tuning methods, reduced computational costs, and showed significant improvements in model performance with various optimizers.</li>\n</ul>"}, "publishedAt": "2026-01-08T07:41:49.000Z", "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers", "summary": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04890.png", "numComments": 1, "submittedBy": {"_id": "6460c3811db65f878513bcaf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6460c3811db65f878513bcaf/CRdJ8lXixDku3k8Rm5Stn.jpeg", "fullname": "Jingwei Zuo", "name": "JingweiZuo", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 37, "isUserFollowing": false}, "organization": {"_id": "6448cad23adf50d86406b0a3", "name": "tiiuae", "fullname": "Technology Innovation Institute", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61a8d1aac664736898ffc84f/AT6cAB5ZNwCcqFMal71WD.jpeg"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\u63a8\u52a8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u7684\u5feb\u901f\u53d1\u5c55\u3002</li>\n    <li>\u76ee\u524d\u7684\u6570\u636e\u51c6\u5907\u591a\u4f9d\u8d56\u4e34\u65f6\u811a\u672c\uff0c\u7f3a\u4e4f\u89c4\u8303\u5316\uff0c\u5bfc\u81f4\u53ef\u91cd\u590d\u6027\u5dee\uff0c\u652f\u6301\u6709\u9650\u3002</li>\n    <li>DataFlow\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u53ef\u6269\u5c55\u7684\u6570\u636e\u51c6\u5907\u6846\u67b6\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>DataFlow\u5305\u542b\u8fd1200\u4e2a\u53ef\u91cd\u7528\u64cd\u4f5c\u7b26\u548c\u591a\u4e2a\u9886\u57df\u901a\u7528\u7684\u7ba1\u9053\uff0c\u652f\u6301\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u3002</li>\n    <li>\u4f7f\u7528DataFlow\u7684\u7cfb\u7edf\u5728\u591a\u4e2a\u7528\u4f8b\u4e2d\u63d0\u9ad8\u4e86LLM\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u4eba\u7c7b\u6570\u636e\u96c6\u548c\u7279\u5b9a\u5408\u6210\u57fa\u51c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is a growing need for better data preparation for Large Language Models (LLMs) due to high demand for quality data.</li>\n    <li>Current methods are often inconsistent and lack structure, making it hard to reproduce results and integrate models into data generation.</li>\n    <li>DataFlow is a new framework that offers a more organized way to prepare data with reusable components and a user-friendly API.</li>\n    <li>It includes nearly 200 tools and pipelines for various tasks, improving the performance of LLMs significantly in several tests compared to traditional methods.</li>\n    <li>DataFlow also features an agent that converts plain language instructions into functional data preparation processes, enhancing usability and effectiveness.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni\u662f\u4e00\u79cd\u901a\u7528\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u76f4\u63a5\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u6210\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5982\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u80cc\u666f\u3002</li>\n    <li>Kling-Omni\u901a\u8fc7\u521b\u5efa\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\uff0c\u63d0\u4f9b\u7535\u5f71\u7ea7\u522b\u7684\u667a\u80fd\u89c6\u9891\u5185\u5bb9\u521b\u4f5c\u3002</li>\n    <li>\u8be5\u6846\u67b6\u57fa\u4e8e\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u63d0\u5347\u6027\u80fd\u3002</li>\n    <li>Kling-Omni\u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6f5c\u529b\u6210\u4e3a\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from various types of visual and language inputs.</li>\n    <li>It combines video generation, editing, and reasoning tasks into one system instead of using separate processes.</li>\n    <li>The framework can handle different user inputs like text, images, and videos to produce cinematic-quality videos.</li>\n    <li>Kling-Omni is built on a strong data system and benefits from advanced training and optimization techniques.</li>\n    <li>It shows excellent performance in generating content, editing based on reasoning, and following complex instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 96, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 81, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u662f\u4e00\u79cd\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u7684\u65b9\u6cd5\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u6a21\u5757\u4e3b\u8981\u7528\u4e8e\u88ab\u52a8\u5b58\u50a8\u4fe1\u606f\uff0c\u5ffd\u89c6\u4e86\u4e8b\u5b9e\u4e4b\u95f4\u7684\u91cd\u8981\u5173\u8054\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HGMem\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u590d\u6742\u7684\u63a8\u7406\u548c\u5168\u7403\u7406\u89e3\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u56fe\u8868\u793a\u8bb0\u5fc6\uff0c\u4f7f\u5f97\u4fe1\u606f\u4e4b\u95f4\u7684\u9ad8\u9636\u4e92\u52a8\u5f97\u4ee5\u5f62\u6210\uff0c\u4ece\u800c\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHGMem\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7684RAG\u7cfb\u7edf\uff0c\u63d0\u5347\u4e86\u591a\u6b65\u9aa4\u63a8\u7406\u7684\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps improve large language models (LLMs) for complex tasks that require deep reasoning.</li>\n    <li>Current memory systems mainly store facts and do not effectively connect them, limiting reasoning abilities.</li>\n    <li>HGMem is a new memory system that uses a hypergraph to link facts dynamically, allowing for better reasoning and understanding.</li>\n    <li>This new memory approach helps create a more integrated knowledge structure that aids in complex problem-solving.</li>\n    <li>Tests show that HGMem significantly enhances the performance of multi-step RAG compared to existing methods.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u7528\u6237\u5e0c\u671b\u5b83\u4eec\u4e0d\u4ec5\u63d0\u4f9b\u51c6\u786e\u56de\u7b54\uff0c\u8fd8\u80fd\u7b26\u5408\u591a\u6837\u5316\u7684\u4eba\u7c7b\u504f\u597d\u3002</li>\n    <li>\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5f00\u59cb\u5f15\u5165\u591a\u79cd\u5956\u52b1\u6765\u6307\u5bfc\u6a21\u578b\u7684\u884c\u4e3a\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u76f4\u63a5\u5e94\u7528GRPO\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u4e0d\u540c\u5956\u52b1\u7ec4\u5408\u7684\u4f18\u52bf\u503c\u53d8\u5f97\u76f8\u540c\uff0c\u4ece\u800c\u5f71\u54cd\u8bad\u7ec3\u6548\u679c\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5GDPO\uff0c\u901a\u8fc7\u89e3\u8026\u4e2a\u522b\u5956\u52b1\u7684\u5f52\u4e00\u5316\uff0c\u4fdd\u6301\u5b83\u4eec\u7684\u76f8\u5bf9\u5dee\u5f02\uff0c\u6539\u5584\u591a\u5956\u52b1\u4f18\u5316\u7684\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\uff0cGDPO\u5728\u51c6\u786e\u6027\u548c\u7ea6\u675f\u9075\u5b88\u65b9\u9762\u90fd\u4f18\u4e8eGRPO\uff0c\u663e\u793a\u51fa\u5176\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Users want language models to give accurate answers and behave according to different human preferences.</li>\n    <li>Reinforcement learning (RL) uses multiple rewards to guide models toward desired behaviors.</li>\n    <li>Applying Group Relative Policy Optimization (GRPO) to these rewards can cause problems, leading to poor training outcomes.</li>\n    <li>The paper introduces a new method called Group reward-Decoupled Normalization Policy Optimization (GDPO) that improves training by keeping rewards separate and better preserving their differences.</li>\n    <li>Tests show that GDPO works better than GRPO in various tasks, improving both accuracy and adherence to constraints.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.00393", "authors": [{"_id": "695b2297832867f253525d68", "user": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "name": "Yuxue Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:23.295Z", "hidden": false}, {"_id": "695b2297832867f253525d69", "user": {"_id": "649ecf9827145c4463240177", "avatarUrl": "/avatars/27696cf31790a3d58d8be2e0c983800e.svg", "isPro": false, "fullname": "Lue Fan", "user": "Abyssaledge", "type": "user"}, "name": "Lue Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T13:49:26.330Z", "hidden": false}, {"_id": "695b2297832867f253525d6a", "user": {"_id": "644cc2c36dfd5f8240d76a52", "avatarUrl": "/avatars/dcd9279af1c6d8535e48dc6e3e6511cd.svg", "isPro": false, "fullname": "Ziqi Shi", "user": "renshengjihe", "type": "user"}, "name": "Ziqi Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:21.077Z", "hidden": false}, {"_id": "695b2297832867f253525d6b", "name": "Junran Peng", "hidden": false}, {"_id": "695b2297832867f253525d6c", "name": "Feng Wang", "hidden": false}, {"_id": "695b2297832867f253525d6d", "name": "Zhaoxiang Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "publishedAt": "2026-01-01T17:07:30.000Z", "submittedOnDailyAt": "2026-01-05T02:49:46.994Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "submittedOnDailyBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "upvotes": 83, "discussionId": "695b2297832867f253525d6e", "projectPage": "https://neoverse-4d.github.io/", "githubRepo": "https://github.com/IamCreateAI/NeoVerse", "githubRepoAddedBy": "user", "ai_summary": "NeoVerse is a scalable 4D world model that enables pose-free reconstruction and novel-trajectory video generation from monocular videos with state-of-the-art performance.", "ai_keywords": ["4D world model", "4D reconstruction", "novel-trajectory video generation", "monocular videos", "pose-free", "feed-forward", "degradation pattern simulation"], "githubStars": 107, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86NeoVerse\uff0c\u4e00\u4e2a\u591a\u529f\u80fd\u76844D\u4e16\u754c\u6a21\u578b\u3002</li>\n    <li>\u89e3\u51b3\u4e86\u5f53\u524d4D\u5efa\u6a21\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u9650\u5236\u3002</li>\n    <li>NeoVerse\u652f\u6301\u4ece\u5355\u76ee\u89c6\u9891\u8fdb\u884c4D\u91cd\u5efa\u548c\u89c6\u9891\u751f\u6210\u3002</li>\n    <li>\u5b9e\u73b0\u4e86\u5728\u6807\u51c6\u91cd\u5efa\u548c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6700\u4f73\u6027\u80fd\u3002</li>\n    <li>\u9879\u76ee\u9875\u9762\u53ef\u5728https://neoverse-4d.github.io\u8bbf\u95ee\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NeoVerse is a new 4D world model that can create 4D reconstructions and generate videos with new trajectories.</li>\n    <li>Current 4D modeling methods struggle with scalability due to expensive data and complex training processes.</li>\n    <li>NeoVerse is designed to work with regular monocular videos, making it more accessible and versatile.</li>\n    <li>It includes features like pose-free 4D reconstruction and simulation of degradation patterns.</li>\n    <li>NeoVerse performs at a high level in standard benchmarks for reconstruction and video generation.</li>\n</ul>"}, "publishedAt": "2026-01-01T12:07:30.000Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00393.png", "numComments": 1, "submittedBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "fullname": "Yuxue Yang", "name": "Yuppie1204", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24615", "authors": [{"_id": "69564d96832867f2535257af", "user": {"_id": "622b00a776c20fee5d14501b", "avatarUrl": "/avatars/e00496dda1e309548e7b5b437839bb65.svg", "isPro": false, "fullname": "Eason shi", "user": "Easonshi", "type": "user"}, "name": "Yuchen Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:50.111Z", "hidden": false}, {"_id": "69564d96832867f2535257b0", "user": {"_id": "66e258bdc70c02b46dfed6e3", "avatarUrl": "/avatars/ccc2d604616c018f45a268a610472cac.svg", "isPro": false, "fullname": "Yuzheng Cai", "user": "Ucreate", "type": "user"}, "name": "Yuzheng Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:50.884Z", "hidden": false}, {"_id": "69564d96832867f2535257b1", "name": "Siqi Cai", "hidden": false}, {"_id": "69564d96832867f2535257b2", "name": "Zihan Xu", "hidden": false}, {"_id": "69564d96832867f2535257b3", "user": {"_id": "64154bfa385a75d7790f80e8", "avatarUrl": "/avatars/9e22f54b5eb7c4ebedad99a9a92c4b6a.svg", "isPro": false, "fullname": "Lichao Chen", "user": "nth233", "type": "user"}, "name": "Lichao Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:46.825Z", "hidden": false}, {"_id": "69564d96832867f2535257b4", "user": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "isPro": false, "fullname": "Yulei Qin", "user": "yolay", "type": "user"}, "name": "Yulei Qin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:48.064Z", "hidden": false}, {"_id": "69564d96832867f2535257b5", "name": "Zhijian Zhou", "hidden": false}, {"_id": "69564d96832867f2535257b6", "name": "Xiang Fei", "hidden": false}, {"_id": "69564d96832867f2535257b7", "user": {"_id": "6604e43869c47cd78fdebd08", "avatarUrl": "/avatars/4c11f5e1aeae3c5eb213f6ec6d5bfe72.svg", "isPro": false, "fullname": "Qiu", "user": "ChaofanDFG", "type": "user"}, "name": "Chaofan Qiu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:48.910Z", "hidden": false}, {"_id": "69564d96832867f2535257b8", "user": {"_id": "637af0a7bdf7309aa6da1c36", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637af0a7bdf7309aa6da1c36/NHZ-09otVCfbpXVxm8f-e.png", "isPro": false, "fullname": "Xiaoyu Tan", "user": "WIlliam1900", "type": "user"}, "name": "Xiaoyu Tan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:52.763Z", "hidden": false}, {"_id": "69564d96832867f2535257b9", "name": "Gang Li", "hidden": false}, {"_id": "69564d96832867f2535257ba", "name": "Zongyi Li", "hidden": false}, {"_id": "69564d96832867f2535257bb", "name": "Haojia Lin", "hidden": false}, {"_id": "69564d96832867f2535257bc", "name": "Guocan Cai", "hidden": false}, {"_id": "69564d96832867f2535257bd", "name": "Yong Mao", "hidden": false}, {"_id": "69564d96832867f2535257be", "name": "Yunsheng Wu", "hidden": false}, {"_id": "69564d96832867f2535257bf", "name": "Ke Li", "hidden": false}, {"_id": "69564d96832867f2535257c0", "user": {"_id": "647401e50da364bd0d002f2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png", "isPro": false, "fullname": "XING SUN", "user": "tedsun", "type": "user"}, "name": "Xing Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:39.390Z", "hidden": false}], "publishedAt": "2025-12-31T04:17:36.000Z", "submittedOnDailyAt": "2026-01-05T00:21:56.456Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "submittedOnDailyBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "isPro": false, "fullname": "Ke Li", "user": "tristanli", "type": "user"}, "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "upvotes": 82, "discussionId": "69564d96832867f2535257c1", "projectPage": "https://tencentcloudadp.github.io/youtu-agent/", "githubRepo": "https://github.com/TencentCloudADP/youtu-agent", "githubRepoAddedBy": "user", "githubStars": 4095, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u6846\u67b6\u9762\u4e34\u9ad8\u914d\u7f6e\u6210\u672c\u548c\u9759\u6001\u80fd\u529b\u7684\u6311\u6218\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Youtu-Agent\uff0c\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u65e8\u5728\u81ea\u52a8\u751f\u6210\u548c\u4e0d\u65ad\u8fdb\u5316LLM\u4ee3\u7406\u3002</li>\n    <li>Youtu-Agent\u5177\u6709\u7ed3\u6784\u5316\u914d\u7f6e\u7cfb\u7edf\uff0c\u652f\u6301\u7075\u6d3b\u91cd\u7528\u548c\u81ea\u52a8\u5408\u6210\u3002</li>\n    <li>\u5b83\u5f15\u5165\u4e86\u4e24\u79cd\u751f\u6210\u6a21\u5f0f\uff1a\u5de5\u4f5c\u6d41\u6a21\u5f0f\u7528\u4e8e\u6807\u51c6\u4efb\u52a1\uff0c\u5143\u4ee3\u7406\u6a21\u5f0f\u7528\u4e8e\u590d\u6742\u9700\u6c42\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cYoutu-Agent\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u4e14\u901a\u8fc7\u5b9e\u8df5\u6a21\u5757\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current LLM agent frameworks have high setup costs and are not adaptable to changes.</li>\n    <li>Youtu-Agent is a new modular framework that simplifies the creation and ongoing improvement of LLM agents.</li>\n    <li>It has a flexible configuration system that allows for easy reuse of components and automatic generation of tools and prompts.</li>\n    <li>Youtu-Agent uses two modes: Workflow for standard tasks and Meta-Agent for more complex needs.</li>\n    <li>Experiments show Youtu-Agent performs well, with improvements in performance and efficiency over existing models.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:17:36.000Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24615.png", "numComments": 1, "submittedBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "fullname": "Ke Li", "name": "tristanli", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u771f\u5b9e\u7684\u89c6\u89c9\u5185\u5bb9\uff0c\u4f46\u5b83\u4eec\u662f\u5426\u53ef\u9760\u53d6\u51b3\u4e8e\u5b83\u4eec\u662f\u5426\u80fd\u6355\u6349\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7684\u7ea6\u675f\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u6807\u51c6\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u8d28\u91cf\uff0c\u800c\u5ffd\u89c6\u4e86\u63a8\u7406\u9519\u8bef\uff0c\u5982\u56e0\u679c\u5173\u7cfb\u548c\u7269\u7406\u6cd5\u5219\u7684\u8fdd\u53cd\u3002</li>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86MMGR\u8bc4\u4f30\u6846\u67b6\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\uff1a\u7269\u7406\u3001\u903b\u8f91\u3001\u4e09\u7ef4\u7a7a\u95f4\u3001\u4e8c\u7ef4\u7a7a\u95f4\u548c\u65f6\u95f4\u3002</li>\n    <li>MMGR\u5728\u4e09\u4e2a\u9886\u57df\u8fdb\u884c\u8bc4\u4f30\uff1a\u62bd\u8c61\u63a8\u7406\u3001\u5177\u8eab\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\uff0c\u4f7f\u7528\u7ec6\u81f4\u7684\u5ea6\u91cf\u6807\u51c6\u3002</li>\n    <li>\u6211\u4eec\u7684\u5206\u6790\u663e\u793a\u5f53\u524d\u6a21\u578b\u7684\u4e3b\u8981\u5c40\u9650\u6027\uff0c\u5305\u62ec\u5bf9\u611f\u77e5\u6570\u636e\u7684\u8fc7\u5ea6\u4f9d\u8d56\u548c\u5bf9\u89c6\u89c9\u53ef\u4fe1\u5ea6\u7684\u91cd\u89c6\uff0c\u800c\u7f3a\u4e4f\u56e0\u679c\u6b63\u786e\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video foundation models create realistic videos but may not reliably simulate the real world due to their lack of understanding of physical and logical rules.</li>\n    <li>Current evaluation methods focus on visual quality but miss reasoning problems like causality and consistency.</li>\n    <li>MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark) is a new framework that assesses reasoning abilities in five areas: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>\n    <li>MMGR tests both video and image generation models across various tasks, revealing that many models struggle with abstract reasoning and long-term spatial planning.</li>\n    <li>The analysis shows that current models often prioritize visual appeal over logical accuracy, indicating important areas for improvement in generative world models.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u5c3d\u7ba1\u79d1\u5b66\u4eba\u5de5\u667a\u80fd\u6709\u8fdb\u6b65\uff0c\u4f46\u8fd8\u7f3a\u4e4f\u4e00\u4e2a\u6e05\u6670\u7684\u79d1\u5b66\u901a\u7528\u667a\u80fd\uff08SGI\uff09\u6846\u67b6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b9e\u7528\u63a2\u7a76\u6a21\u578b\uff08PIM\uff09\u7684SGI\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u4e0e\u79d1\u5b66\u5bb6\u76f8\u5173\u7684\u4efb\u52a1\u6765\u5b9e\u73b0\u5b83\u3002</li>\n    <li>SGI-Bench\u5305\u542b1000\u591a\u4e2a\u8de8\u5b66\u79d1\u7684\u6837\u672c\uff0c\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u663e\u793a\u5728\u6df1\u5ea6\u7814\u7a76\u3001\u521b\u610f\u751f\u6210\u548c\u5b9e\u9a8c\u6267\u884c\u7b49\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\uff0c\u4ee5\u589e\u5f3a\u5047\u8bbe\u7684\u65b0\u9896\u6027\uff0c\u63a8\u52a8\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684AI\u7cfb\u7edf\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is no clear framework for Scientific General Intelligence (SGI), which is the ability of AI to think and work like a scientist.</li>\n    <li>The authors propose a definition of SGI based on a model that includes steps like thinking, creating, acting, and observing.</li>\n    <li>They created SGI-Bench, a tool with over 1,000 tasks to test AI's scientific abilities across different fields.</li>\n    <li>Results show that AI struggles with deep research accuracy, generating practical ideas, executing experiments correctly, and comparing different types of information.</li>\n    <li>They also introduced a new method, Test-Time Reinforcement Learning (TTRL), to improve AI's creativity in generating hypotheses during testing.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20619", "authors": [{"_id": "694b614d746a34b55dd53d1a", "name": "Jianhong Bai", "hidden": false}, {"_id": "694b614d746a34b55dd53d1b", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "694b614d746a34b55dd53d1c", "name": "Xintao Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1d", "name": "Fu Xiao", "hidden": false}, {"_id": "694b614d746a34b55dd53d1e", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1f", "name": "Qinghe Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d20", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "694b614d746a34b55dd53d21", "name": "Menghan Xia", "hidden": false}, {"_id": "694b614d746a34b55dd53d22", "name": "Zuozhu Liu", "hidden": false}, {"_id": "694b614d746a34b55dd53d23", "name": "Haoji Hu", "hidden": false}, {"_id": "694b614d746a34b55dd53d24", "name": "Pengfei Wan", "hidden": false}, {"_id": "694b614d746a34b55dd53d25", "name": "Kun Gai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "publishedAt": "2025-12-23T18:59:56.000Z", "submittedOnDailyAt": "2025-12-24T01:20:51.117Z", "title": "SemanticGen: Video Generation in Semantic Space", "submittedOnDailyBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "isPro": false, "fullname": "Jianhong Bai", "user": "jianhongbai", "type": "user"}, "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "upvotes": 77, "discussionId": "694b614d746a34b55dd53d26", "projectPage": "https://jianhongbai.github.io/SemanticGen/", "ai_summary": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.", "ai_keywords": ["VAE space", "VAE decoder", "semantic space", "diffusion model", "semantic video features", "bi-directional attention"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u65f6\u6548\u7387\u4f4e\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86SemanticGen\uff0c\u901a\u8fc7\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n    <li>SemanticGen\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u8fc7\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u89c6\u9891\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u7b2c\u4e8c\u9636\u6bb5\u751f\u6210\u6700\u7ec8\u7684\u89c6\u9891\u8f93\u51fa\u3002</li>\n    <li>\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6bd4\u5728VAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u66f4\u5feb\u6536\u655b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSemanticGen\u751f\u6210\u7684\u89c6\u9891\u8d28\u91cf\u9ad8\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>SemanticGen is a new method for generating videos that starts in a simpler, high-level semantic space to improve efficiency.</li>\n    <li>The approach consists of two stages: first, it creates a basic layout of the video, and then it adds detailed features.</li>\n    <li>Generating videos in the semantic space allows for faster processing and better performance than traditional methods.</li>\n    <li>This method is more efficient for creating longer videos and results in high-quality outputs.</li>\n    <li>Tests show that SemanticGen performs better than existing video generation techniques.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:56.000Z", "title": "SemanticGen: Video Generation in Semantic Space", "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png", "numComments": 2, "submittedBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "fullname": "Jianhong Bai", "name": "jianhongbai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.15431", "authors": [{"_id": "69437417542d62d58a7bf6c4", "name": "Haolong Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6c5", "name": "Jia Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c6", "name": "Xin Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c7", "name": "Yeqing Shen", "hidden": false}, {"_id": "69437417542d62d58a7bf6c8", "user": {"_id": "653614073f4248157d60ccdc", "avatarUrl": "/avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg", "isPro": false, "fullname": "mengziyang", "user": "zylate", "type": "user"}, "name": "Ziyang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:53.033Z", "hidden": false}, {"_id": "69437417542d62d58a7bf6c9", "name": "Zhimin Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6ca", "name": "Kaijun Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6cb", "name": "Jin Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf6cc", "name": "Lieyu Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6cd", "name": "Mi Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6ce", "name": "Shiliang Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6cf", "name": "Zhirui Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d0", "name": "Brian Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d1", "name": "Kang An", "hidden": false}, {"_id": "69437417542d62d58a7bf6d2", "name": "Chenyang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d3", "name": "Lei Lei", "hidden": false}, {"_id": "69437417542d62d58a7bf6d4", "name": "Mengmeng Duan", "hidden": false}, {"_id": "69437417542d62d58a7bf6d5", "name": "Danxun Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d6", "name": "Guodong Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d7", "name": "Hang Cheng", "hidden": false}, {"_id": "69437417542d62d58a7bf6d8", "name": "Hao Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d9", "name": "Jie Dong", "hidden": false}, {"_id": "69437417542d62d58a7bf6da", "name": "Junhao Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6db", "name": "Mei Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6dc", "name": "Renjie Yu", "hidden": false}, {"_id": "69437417542d62d58a7bf6dd", "name": "Shunshan Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6de", "name": "Xu Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf6df", "name": "Yiting Dai", "hidden": false}, {"_id": "69437417542d62d58a7bf6e0", "name": "Yineng Deng", "hidden": false}, {"_id": "69437417542d62d58a7bf6e1", "name": "Yingdan Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6e2", "name": "Zelin Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6e3", "name": "Wen Sun", "hidden": false}, {"_id": "69437417542d62d58a7bf6e4", "name": "Chengxu Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e5", "name": "Chunqin Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6e6", "name": "Dong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6e7", "name": "Fengqiong Xiao", "hidden": false}, {"_id": "69437417542d62d58a7bf6e8", "name": "Guanghao Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e9", "name": "Guopeng Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ea", "name": "Guozhen Peng", "hidden": false}, {"_id": "69437417542d62d58a7bf6eb", "name": "Hongbing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ec", "name": "Hang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ed", "name": "Hongming Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6ee", "name": "Jingjing Xie", "hidden": false}, {"_id": "69437417542d62d58a7bf6ef", "name": "Jianyong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6f0", "name": "Jingyang Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf6f1", "name": "Jiaju Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f2", "name": "Jiayu Yuan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f3", "name": "Jianpeng Yin", "hidden": false}, {"_id": "69437417542d62d58a7bf6f4", "name": "Kai Cao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f5", "name": "Liang Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f6", "name": "Liguo Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f7", "name": "Liying Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6f8", "name": "Mengqiang Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f9", "name": "Min Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fa", "name": "Manjiao Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fb", "name": "Mao Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf6fc", "name": "Mingxin Wan", "hidden": false}, {"_id": "69437417542d62d58a7bf6fd", "name": "Na Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6fe", "name": "Nan Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6ff", "name": "Ning Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf700", "name": "Peiyao Ma", "hidden": false}, {"_id": "69437417542d62d58a7bf701", "name": "Qingzhou Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf702", "name": "Qiao Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf703", "name": "Qinlin Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf704", "name": "Qiong Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf705", "name": "Qiongyao Li", "hidden": false}, {"_id": "69437417542d62d58a7bf706", "name": "Shangwu Zhong", "hidden": false}, {"_id": "69437417542d62d58a7bf707", "name": "Shuli Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf708", "name": "Shaofan Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf709", "name": "Shisi Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf70a", "name": "Shuang Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf70b", "name": "Xingbin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70c", "name": "Xiaojia Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70d", "name": "Xiaojie Hou", "hidden": false}, {"_id": "69437417542d62d58a7bf70e", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70f", "name": "Xuanti Feng", "hidden": false}, {"_id": "69437417542d62d58a7bf710", "name": "Xuedan Cai", "hidden": false}, {"_id": "69437417542d62d58a7bf711", "name": "Xuan Wen", "hidden": false}, {"_id": "69437417542d62d58a7bf712", "name": "Xianwei Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf713", "name": "Xin Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf714", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf715", "name": "Xin Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf716", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf717", "name": "Yukang Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf718", "name": "Yunfang Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf719", "name": "Yuqing Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf71a", "name": "Yixun Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf71b", "name": "Zejia Weng", "hidden": false}, {"_id": "69437417542d62d58a7bf71c", "name": "Zhonghao Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf71d", "name": "Zhiguo Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf71e", "name": "Zhuoyu Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf71f", "name": "Zheng Ge", "hidden": false}, {"_id": "69437417542d62d58a7bf720", "name": "Jing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf721", "name": "Yibo Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf722", "name": "Binxing Jiao", "hidden": false}, {"_id": "69437417542d62d58a7bf723", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf724", "name": "Daxin Jiang", "hidden": false}], "publishedAt": "2025-12-17T13:26:30.000Z", "submittedOnDailyAt": "2025-12-18T00:55:26.804Z", "title": "Step-GUI Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "upvotes": 77, "discussionId": "69437418542d62d58a7bf725", "projectPage": "https://opengelab.github.io/", "githubRepo": "https://github.com/stepfun-ai/gelab-zero", "githubRepoAddedBy": "user", "ai_summary": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.", "ai_keywords": ["multimodal large language models", "GUI automation", "self-evolving training pipeline", "Calibrated Step Reward System", "trajectory-level calibration", "Step-GUI", "GUI performance", "GUI-MCP", "Model Context Protocol", "AndroidWorld", "OSWorld", "ScreenShot-Pro", "AndroidDaily", "real-world mobile usage patterns", "hierarchical architecture", "low-level atomic operations", "high-level task delegation", "local specialist models", "high-privacy execution"], "githubStars": 1417, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u5f15\u5165\u4e86\u4e00\u79cd\u81ea\u6211\u6f14\u53d8\u7684\u8bad\u7ec3\u7ba1\u9053\uff0c\u5229\u7528\u6821\u51c6\u6b65\u9aa4\u5956\u52b1\u7cfb\u7edf\u63d0\u9ad8\u8bad\u7ec3\u6570\u636e\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3002</li>\n    <li>\u5f00\u53d1\u4e86Step-GUI\u6a21\u578b\uff0c\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u4e86\u6700\u65b0\u7684GUI\u6027\u80fd\u6807\u51c6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u901a\u7528\u80fd\u529b\u3002</li>\n    <li>\u63d0\u51fa\u4e86GUI-MCP\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8eGUI\u81ea\u52a8\u5316\u7684\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff0c\u6ce8\u91cd\u7528\u6237\u9690\u79c1\u548c\u6807\u51c6\u5316\u63a5\u53e3\u3002</li>\n    <li>\u63a8\u51fa\u4e86AndroidDaily\u57fa\u51c6\uff0c\u57fa\u4e8e\u771f\u5b9e\u624b\u673a\u4f7f\u7528\u6a21\u5f0f\uff0c\u8bc4\u4f30\u667a\u80fd\u4ee3\u7406\u7684\u5b9e\u9645\u4f7f\u7528\u80fd\u529b\u3002</li>\n    <li>\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u5b9e\u7528GUI\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u5e76\u663e\u793a\u51fa\u5728\u65e5\u5e38\u6570\u5b57\u4ea4\u4e92\u4e2d\u7684\u5f3a\u5927\u5e94\u7528\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New models and methods improve the automation of graphical user interfaces (GUIs) using multimodal large language models.</li>\n    <li>A self-evolving training system helps create high-quality training data efficiently, achieving over 90% accuracy at a lower cost.</li>\n    <li>Step-GUI models show excellent performance in GUI tasks while ensuring general capabilities, with significant success rates in various benchmarks.</li>\n    <li>GUI-MCP is introduced as a new protocol for GUI automation that prioritizes user privacy by keeping sensitive data on the device.</li>\n    <li>AndroidDaily benchmark evaluates agents based on real mobile usage patterns, showing promising results for practical deployment in daily tasks.</li>\n</ul>"}, "publishedAt": "2025-12-17T08:26:30.000Z", "title": "Step-GUI Technical Report", "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15431.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Jan 12, 2026";
window.trendingPapers = {
    "today": [{"paper": {"id": "2601.15876", "authors": [{"_id": "6972d8d5fb12c92b735b73a2", "name": "Taofeng Xue", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a3", "user": {"_id": "6801cbd5f06f2d08f3fd5455", "avatarUrl": "/avatars/c728b48f6938c0d7cb6b14011927ede8.svg", "isPro": false, "fullname": "chong.peng", "user": "KleinChong", "type": "user"}, "name": "Chong Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T20:15:18.698Z", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a4", "name": "Mianqiu Huang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a5", "name": "Linsen Guo", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a6", "user": {"_id": "6764279e684ed3b61b2316a4", "avatarUrl": "/avatars/63a6b6c3b9f442b42480679425951187.svg", "isPro": false, "fullname": "SII-TianchengHAN", "user": "GenSouKai", "type": "user"}, "name": "Tiancheng Han", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:30.106Z", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a7", "name": "Haozhe Wang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a8", "name": "Jianing Wang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a9", "name": "Xiaocheng Zhang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73aa", "name": "Xin Yang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ab", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ac", "name": "Jinrui Ding", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ad", "name": "Xiandi Ma", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ae", "name": "Yuchen Xie", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73af", "name": "Peng Pei", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73b0", "name": "Xunliang Cai", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73b1", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-01-22T11:36:43.000Z", "submittedOnDailyAt": "2026-01-23T07:54:00.525Z", "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience", "submittedOnDailyBy": {"_id": "6459c7c10aba070266e41bb1", "avatarUrl": "/avatars/2178cac69cf4123db5e85191160f3795.svg", "isPro": false, "fullname": "mqhuang", "user": "LutherXD", "type": "user"}, "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.", "upvotes": 62, "discussionId": "6972d8d5fb12c92b735b73b2", "ai_summary": "EvoCUA introduces an evolutionary approach to computer-use agents that combines autonomous task generation with policy optimization to achieve superior performance in complex, long-horizon tasks.", "ai_keywords": ["computer-use agents", "native computer-use agents", "data generation", "policy optimization", "evolutionary cycle", "verifiable synthesis engine", "executable validators", "sandbox rollouts", "iterative evolving learning", "capability boundaries", "error analysis", "self-correction", "OSWorld benchmark", "foundation models"], "summary_zh": "<ul>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u6a21\u578bEvoCUA\uff0c\u80fd\u5728\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u65b9\u9762\u53d6\u5f97\u91cd\u5927\u8fdb\u5c55\u3002</li>\n    <li>EvoCUA\u901a\u8fc7\u81ea\u6211\u751f\u6210\u6570\u636e\u548c\u4f18\u5316\u7b56\u7565\uff0c\u514b\u670d\u4e86\u9759\u6001\u6570\u636e\u7684\u9650\u5236\uff0c\u5f62\u6210\u4e00\u4e2a\u81ea\u6211\u7ef4\u6301\u7684\u8fdb\u5316\u5faa\u73af\u3002</li>\n    <li>\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u7684\u5408\u6210\u5f15\u64ce\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u7684\u4efb\u52a1\u548c\u53ef\u6267\u884c\u7684\u9a8c\u8bc1\u5668\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002</li>\n    <li>EvoCUA\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6210\u529f\u7387\u8fbe\u5230\u4e8656.7%\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u5f00\u6e90\u6a21\u578b\u548c\u4e00\u4e9b\u95ed\u6e90\u6a21\u578b\u3002</li>\n    <li>\u8fd9\u79cd\u4ee5\u7ecf\u9a8c\u5b66\u4e60\u9a71\u52a8\u7684\u8fdb\u5316\u8303\u5f0f\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u57fa\u7840\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Native computer-use agents (CUA) are advancing multimodal AI but face limitations due to static data.</li>\n    <li>EvoCUA is a new model that combines data generation and policy improvement in a self-sustaining cycle.</li>\n    <li>A synthesis engine creates diverse tasks and validators to help overcome data shortages.</li>\n    <li>EvoCUA uses a large-scale infrastructure to gather experience from many simulations, improving learning efficiency.</li>\n    <li>Tests show EvoCUA has a 56.7% success rate, outperforming previous models and demonstrating its scalability and effectiveness.</li>\n</ul>"}, "publishedAt": "2026-01-22T06:36:43.000Z", "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience", "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15876.png", "numComments": 1, "submittedBy": {"_id": "6459c7c10aba070266e41bb1", "avatarUrl": "/avatars/2178cac69cf4123db5e85191160f3795.svg", "fullname": "mqhuang", "name": "LutherXD", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.15165", "authors": [{"_id": "6971933ac1c7409747bf9597", "name": "Zanlin Ni", "hidden": false}, {"_id": "6971933ac1c7409747bf9598", "user": {"_id": "6486dde1f74857df3f1a5828", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg", "isPro": false, "fullname": "Shenzhi Wang", "user": "shenzhi-wang", "type": "user"}, "name": "Shenzhi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:54.254Z", "hidden": false}, {"_id": "6971933ac1c7409747bf9599", "name": "Yang Yue", "hidden": false}, {"_id": "6971933ac1c7409747bf959a", "name": "Tianyu Yu", "hidden": false}, {"_id": "6971933ac1c7409747bf959b", "name": "Weilin Zhao", "hidden": false}, {"_id": "6971933ac1c7409747bf959c", "name": "Yeguo Hua", "hidden": false}, {"_id": "6971933ac1c7409747bf959d", "name": "Tianyi Chen", "hidden": false}, {"_id": "6971933ac1c7409747bf959e", "name": "Jun Song", "hidden": false}, {"_id": "6971933ac1c7409747bf959f", "name": "Cheng Yu", "hidden": false}, {"_id": "6971933ac1c7409747bf95a0", "name": "Bo Zheng", "hidden": false}, {"_id": "6971933ac1c7409747bf95a1", "name": "Gao Huang", "hidden": false}], "publishedAt": "2026-01-21T16:41:58.000Z", "submittedOnDailyAt": "2026-01-23T00:11:51.141Z", "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models", "submittedOnDailyBy": {"_id": "63987ffb2ceb55aabe0852f3", "avatarUrl": "/avatars/343b796ff6b8906203904e8c620d7eb5.svg", "isPro": false, "fullname": "Zanlin Ni", "user": "nzl-thu", "type": "user"}, "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap", "upvotes": 55, "discussionId": "6971933ac1c7409747bf95a2", "projectPage": "https://nzl-thu.github.io/the-flexibility-trap", "githubRepo": "https://github.com/LeapLabTHU/JustGRPO", "githubRepoAddedBy": "user", "ai_summary": "Arbitrary order generation in diffusion large language models limits reasoning capability by causing premature solution space collapse, making standard policy optimization more effective.", "ai_keywords": ["diffusion large language models", "left-to-right constraint", "token generation", "reinforcement learning", "reasoning potential", "mathematical reasoning", "coding tasks", "combinatorial trajectories", "likelihoods", "Group Relative Policy Optimization", "GRPO", "parallel decoding"], "githubStars": 66, "organization": {"_id": "69719700e3846c07669d13ee", "name": "Tsinghua-LeapLab", "fullname": "Tsinghua-LeapLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63987ffb2ceb55aabe0852f3/hflTWNTGxeJx83xNkYrDB.png"}, "summary_zh": "<ul>\n    <li>\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u6253\u7834\u4e86\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u7684\u5de6\u5230\u53f3\u751f\u6210\u9650\u5236\uff0c\u53ef\u4ee5\u4ee5\u4efb\u610f\u987a\u5e8f\u751f\u6210\u6807\u8bb0\u3002</li>\n    <li>\u867d\u7136\u8fd9\u79cd\u7075\u6d3b\u6027\u770b\u4f3c\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b9e\u9645\u4e0a\u5374\u9650\u5236\u4e86 dLLMs \u7684\u63a8\u7406\u8fb9\u754c\u3002</li>\n    <li>dLLMs \u503e\u5411\u4e8e\u5229\u7528\u8fd9\u79cd\u987a\u5e8f\u7075\u6d3b\u6027\uff0c\u8df3\u8fc7\u5bf9\u63a2\u7d22\u81f3\u5173\u91cd\u8981\u7684\u9ad8\u4e0d\u786e\u5b9a\u6027\u6807\u8bb0\uff0c\u4ece\u800c\u5bfc\u81f4\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u8fc7\u65e9\u5d29\u6e83\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u7684 JustGRPO \u65b9\u6cd5\u653e\u5f03\u4e86\u4efb\u610f\u987a\u5e8f\u751f\u6210\uff0c\u91c7\u7528\u6807\u51c6\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u63a8\u7406\u6548\u679c\u3002</li>\n    <li>JustGRPO \u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u5728 GSM8K \u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86 89.1% \u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u7559\u4e86 dLLMs \u7684\u5e76\u884c\u89e3\u7801\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion Large Language Models (dLLMs) can generate text in any order, unlike traditional models that go left to right.</li>\n    <li>This flexibility was thought to improve reasoning for tasks like math and coding.</li>\n    <li>However, the study shows that this order flexibility can actually limit reasoning by avoiding important, uncertain tokens.</li>\n    <li>Current reinforcement learning methods for dLLMs may not work well because they complicate the use of this flexibility.</li>\n    <li>The authors propose a simpler method called JustGRPO, which achieves high accuracy while still using the benefits of dLLMs.</li>\n</ul>"}, "publishedAt": "2026-01-21T11:41:58.000Z", "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models", "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15165.png", "numComments": 1, "submittedBy": {"_id": "63987ffb2ceb55aabe0852f3", "avatarUrl": "/avatars/343b796ff6b8906203904e8c620d7eb5.svg", "fullname": "Zanlin Ni", "name": "nzl-thu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "69719700e3846c07669d13ee", "name": "Tsinghua-LeapLab", "fullname": "Tsinghua-LeapLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63987ffb2ceb55aabe0852f3/hflTWNTGxeJx83xNkYrDB.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.14724", "authors": [{"_id": "6972ee7afb12c92b735b74b4", "user": {"_id": "637169557a5e5d8efdc3e58e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg", "isPro": false, "fullname": "Haowei Zhang", "user": "freesky", "type": "user"}, "name": "Haowei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:23.639Z", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b5", "name": "Shudong Yang", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b6", "name": "Jinlan Fu", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b7", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b8", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-01-21T07:26:15.000Z", "submittedOnDailyAt": "2026-01-23T01:43:37.582Z", "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding", "submittedOnDailyBy": {"_id": "637169557a5e5d8efdc3e58e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg", "isPro": false, "fullname": "Haowei Zhang", "user": "freesky", "type": "user"}, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10times faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.", "upvotes": 52, "discussionId": "6972ee7bfb12c92b735b74b9", "projectPage": "https://hermes-streaming.github.io/", "githubRepo": "https://github.com/haowei-freesky/HERMES", "githubRepoAddedBy": "user", "ai_summary": "HERMES is a training-free architecture that enables real-time video stream understanding by utilizing a hierarchical memory framework based on KV cache reuse, achieving faster response times and maintained accuracy even with reduced video token input.", "ai_keywords": ["Multimodal Large Language Models", "video understanding", "streaming video inputs", "real-time responses", "KV cache", "hierarchical memory framework", "mechanistic attention", "video tokens", "TTFT"], "githubStars": 24, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\uff0c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u79bb\u7ebf\u89c6\u9891\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002</li>\n    <li>\u5c06\u8fd9\u4e9b\u80fd\u529b\u6269\u5c55\u5230\u6d41\u5a92\u4f53\u89c6\u9891\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u6a21\u578b\u5728\u7406\u89e3\u6027\u80fd\u548c\u5b9e\u65f6\u54cd\u5e94\u65b9\u9762\u5b58\u5728\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HERMES\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u8bad\u7ec3\u67b6\u6784\uff0c\u53ef\u4ee5\u5b9e\u65f6\u51c6\u786e\u5730\u7406\u89e3\u89c6\u9891\u6d41\u3002</li>\n    <li>HERMES\u5229\u7528\u7d27\u51d1\u7684KV\u7f13\u5b58\uff0c\u80fd\u591f\u5728\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u9ad8\u6548\u5904\u7406\u6d41\u5a92\u4f53\u89c6\u9891\u3002</li>\n    <li>HERMES\u5728\u7528\u6237\u67e5\u8be2\u5230\u8fbe\u65f6\u65e0\u9700\u989d\u5916\u8ba1\u7b97\uff0c\u4ece\u800c\u786e\u4fdd\u5b9e\u65f6\u54cd\u5e94\uff0c\u5e76\u5728\u6d41\u5a92\u4f53\u6570\u636e\u96c6\u4e0a\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>HERMES is a new system designed to understand video streams in real-time without needing extra training.</    <li>It uses a special memory system called KV cache to manage video information efficiently.</li>\n    <li>HERMES can respond quickly to user queries without needing extra calculations, making it suitable for live video.</li>\n    <li>It is 10 times faster than previous models in terms of time to first response (TTFT).</li>\n    <li>HERMES also reduces video data usage by up to 68% while maintaining or improving accuracy on various tests.</li>\n</ul>"}, "publishedAt": "2026-01-21T02:26:15.000Z", "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10times faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14724.png", "numComments": 2, "submittedBy": {"_id": "637169557a5e5d8efdc3e58e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg", "fullname": "Haowei Zhang", "name": "freesky", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.15197", "authors": [{"_id": "6971c608c1c7409747bf96a5", "user": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "isPro": false, "fullname": "Shijie Lian", "user": "LiamLian0727", "type": "user"}, "name": "Shijie Lian", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:44:25.547Z", "hidden": false}, {"_id": "6971c608c1c7409747bf96a6", "name": "Bin Yu", "hidden": false}, {"_id": "6971c608c1c7409747bf96a7", "name": "Xiaopeng Lin", "hidden": false}, {"_id": "6971c608c1c7409747bf96a8", "name": "Laurence T. Yang", "hidden": false}, {"_id": "6971c608c1c7409747bf96a9", "name": "Zhaolong Shen", "hidden": false}, {"_id": "6971c608c1c7409747bf96aa", "name": "Changti Wu", "hidden": false}, {"_id": "6971c608c1c7409747bf96ab", "name": "Yuzhuo Miao", "hidden": false}, {"_id": "6971c608c1c7409747bf96ac", "name": "Cong Huang", "hidden": false}, {"_id": "6971c608c1c7409747bf96ad", "name": "Kai Chen", "hidden": false}], "publishedAt": "2026-01-21T17:15:22.000Z", "submittedOnDailyAt": "2026-01-23T00:45:20.588Z", "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries", "submittedOnDailyBy": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "isPro": false, "fullname": "Shijie Lian", "user": "LiamLian0727", "type": "user"}, "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior \u03c0(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.", "upvotes": 50, "discussionId": "6971c609c1c7409747bf96ae", "projectPage": "https://github.com/ZGC-EmbodyAI/BayesianVLA", "githubRepo": "https://github.com/ZGC-EmbodyAI/BayesianVLA", "githubRepoAddedBy": "user", "ai_summary": "BayesianVLA addresses language-action grounding issues in robot manipulation by using Bayesian decomposition to prevent information collapse and improve out-of-distribution generalization.", "ai_keywords": ["Vision-Language-Action models", "Information Collapse", "Bayesian decomposition", "latent action queries", "conditional Pointwise Mutual Information", "vision-only policies", "out-of-distribution generalization", "SimplerEnv", "RoboCasa"], "githubStars": 11, "organization": {"_id": "68896d3a716ee5bfb1428441", "name": "ZGCA", "fullname": "Zhongguancun Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\uff08VLA\uff09\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u65b0\u6307\u4ee4\u6216\u590d\u6742\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u5e38\u5e38\u65e0\u6cd5\u6cdb\u5316\u3002</li>\n    <li>\u5f53\u524d\u8bad\u7ec3\u65b9\u6cd5\u5bfc\u81f4\u6570\u636e\u96c6\u504f\u89c1\uff0c\u4f7f\u5f97\u8bed\u8a00\u6307\u4ee4\u53ef\u4ee5\u4ec5\u901a\u8fc7\u89c6\u89c9\u89c2\u5bdf\u6765\u9884\u6d4b\uff0c\u4ece\u800c\u5bfc\u81f4\u4fe1\u606f\u5d29\u6e83\u73b0\u8c61\u3002</li>\n    <li>\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86BayesianVLA\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u5206\u89e3\u6765\u5f3a\u5236\u9075\u5faa\u6307\u4ee4\u3002</li>\n    <li>\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u6f5c\u5728\u52a8\u4f5c\u67e5\u8be2\uff0c\u6784\u5efa\u53cc\u5206\u652f\u7ed3\u6784\u4ee5\u4f30\u8ba1\u89c6\u89c9\u4f18\u5148\u548c\u8bed\u8a00\u6761\u4ef6\u7684\u540e\u9a8c\u3002</li>\n    <li>BayesianVLA\u5728\u4e0d\u9700\u8981\u65b0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u5404\u7c7b\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-Language-Action (VLA) models help robots understand and follow instructions but often fail with new tasks or complex scenarios.</li>\n    <li>Current training methods create biased datasets, leading to a problem called Information Collapse where models rely too much on visual data and ignore language.</li>\n    <li>To solve this, the authors propose a new method called BayesianVLA that uses a dual-branch system to better connect language instructions and actions.</li>\n    <li>BayesianVLA improves models by maximizing the connection between actions and language instructions, reducing reliance on visual data alone.</li>\n    <li>Tests on different environments show that BayesianVLA significantly enhances the models' ability to follow instructions, especially in challenging situations.</li>\n</ul>"}, "publishedAt": "2026-01-21T12:15:22.000Z", "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries", "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior \u03c0(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15197.png", "numComments": 2, "submittedBy": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "fullname": "Shijie Lian", "name": "LiamLian0727", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "68896d3a716ee5bfb1428441", "name": "ZGCA", "fullname": "Zhongguancun Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16206", "authors": [{"_id": "6972e04dfb12c92b735b73cf", "user": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "isPro": false, "fullname": "Daixuan Cheng", "user": "daixuancheng", "type": "user"}, "name": "Daixuan Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:28.070Z", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d0", "name": "Shaohan Huang", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d1", "name": "Yuxian Gu", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d2", "name": "Huatong Song", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d3", "name": "Guoxin Chen", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d4", "name": "Li Dong", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d5", "name": "Wayne Xin Zhao", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d6", "name": "Ji-Rong Wen", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d7", "name": "Furu Wei", "hidden": false}], "publishedAt": "2026-01-22T18:57:09.000Z", "submittedOnDailyAt": "2026-01-23T00:27:12.305Z", "title": "LLM-in-Sandbox Elicits General Agentic Intelligence", "submittedOnDailyBy": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "isPro": false, "fullname": "Daixuan Cheng", "user": "daixuancheng", "type": "user"}, "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.", "upvotes": 46, "discussionId": "6972e04dfb12c92b735b73d8", "projectPage": "https://llm-in-sandbox.github.io", "githubRepo": "https://github.com/llm-in-sandbox/llm-in-sandbox", "githubRepoAddedBy": "user", "ai_summary": "LLM-in-Sandbox enables large language models to perform general intelligence tasks across diverse domains by allowing them to explore a code sandbox environment, achieving robust generalization without additional training.", "ai_keywords": ["LLM-in-Sandbox", "code sandbox", "virtual computer", "reinforcement learning", "non-agentic data", "sandbox exploration", "general intelligence", "long-context understanding", "instruction following"], "githubStars": 38, "organization": {"_id": "68151d0f51add3813f3f7d1b", "name": "MicrosoftResearch", "fullname": "Microsoft Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86LLM-in-Sandbox\uff0c\u5141\u8bb8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4ee3\u7801\u6c99\u7bb1\u4e2d\u63a2\u7d22\uff0c\u4ee5\u5728\u975e\u4ee3\u7801\u9886\u57df\u5c55\u73b0\u901a\u7528\u667a\u80fd\u3002</li>\n    <li>\u5f3a\u5927\u7684LLM\u80fd\u591f\u81ea\u53d1\u5229\u7528\u4ee3\u7801\u6c99\u7bb1\u5b8c\u6210\u975e\u4ee3\u7801\u4efb\u52a1\uff0c\u4f8b\u5982\u83b7\u53d6\u65b0\u77e5\u8bc6\u3001\u5904\u7406\u957f\u6587\u672c\u548c\u6267\u884c\u811a\u672c\u3002</li>\n    <li>\u901a\u8fc7LLM-in-Sandbox\u5f3a\u5316\u5b66\u4e60\uff08LLM-in-Sandbox-RL\uff09\uff0c\u53ef\u4ee5\u589e\u5f3a\u8fd9\u4e9b\u667a\u80fd\u80fd\u529b\uff0c\u53ea\u4f7f\u7528\u975e\u667a\u80fd\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cLLM-in-Sandbox\u5728\u6570\u5b66\u3001\u7269\u7406\u3001\u5316\u5b66\u3001\u751f\u7269\u533b\u5b66\u7b49\u9886\u57df\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u901a\u7528\u5316\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u5206\u6790\u4e86LLM-in-Sandbox\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5c06\u5176\u5f00\u6e90\u4e3aPython\u5305\uff0c\u4ee5\u4fbf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4f7f\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LLM-in-Sandbox lets language models (LLMs) work in a virtual environment to improve their abilities in non-code areas.</li>\n    <li>Strong LLMs can use this sandbox without extra training to handle various tasks, like finding information and managing long texts.</li>\n    <li>These models can be made even better with a training method called LLM-in-Sandbox Reinforcement Learning, which focuses on exploring the sandbox.</li>\n    <li>Tests show that LLM-in-Sandbox is effective in many areas, including math, science, and understanding instructions.</li>\n    <li>The system's efficiency has been studied, and it is available as an open-source Python package for practical use.</li>\n</ul>"}, "publishedAt": "2026-01-22T13:57:09.000Z", "title": "LLM-in-Sandbox Elicits General Agentic Intelligence", "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16206.png", "numComments": 2, "submittedBy": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "fullname": "Daixuan Cheng", "name": "daixuancheng", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 16, "isUserFollowing": false}, "organization": {"_id": "68151d0f51add3813f3f7d1b", "name": "MicrosoftResearch", "fullname": "Microsoft Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16208", "authors": [{"_id": "6972ea8bfb12c92b735b74a8", "name": "Shengbang Tong", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74a9", "name": "Boyang Zheng", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74aa", "user": {"_id": "64249f76d476e4ad55665d59", "avatarUrl": "/avatars/a0fec7e423ffae944a874ca267b55c1f.svg", "isPro": false, "fullname": "Ziteng Wang", "user": "AustinWang0330", "type": "user"}, "name": "Ziteng Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:01.136Z", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ab", "name": "Bingda Tang", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ac", "name": "Nanye Ma", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ad", "user": {"_id": "626dc5105f7327906f0b2a4e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626dc5105f7327906f0b2a4e/QCSzuwYqsv8ozRnusVb-F.jpeg", "isPro": true, "fullname": "Ellis Brown", "user": "ellisbrown", "type": "user"}, "name": "Ellis Brown", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T20:15:10.637Z", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ae", "name": "Jihan Yang", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74af", "name": "Rob Fergus", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74b0", "name": "Yann LeCun", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74b1", "name": "Saining Xie", "hidden": false}], "publishedAt": "2026-01-22T18:58:16.000Z", "submittedOnDailyAt": "2026-01-23T00:57:17.761Z", "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "submittedOnDailyBy": {"_id": "6434226da4c9c55871a78052", "avatarUrl": "/avatars/3309832b3115bc6ad08ae1d10f43118b.svg", "isPro": false, "fullname": "BoYang Zheng", "user": "bytetriper", "type": "user"}, "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.", "upvotes": 40, "discussionId": "6972ea8cfb12c92b735b74b2", "projectPage": "https://rae-dit.github.io/scale-rae/", "githubRepo": "https://github.com/ZitengWangNYU/Scale-RAE", "githubRepoAddedBy": "user", "ai_summary": "Representation Autoencoders (RAEs) demonstrate superior performance over VAEs in large-scale text-to-image generation, showing improved stability, faster convergence, and better quality while enabling unified multimodal reasoning in shared representation spaces.", "ai_keywords": ["representation autoencoders", "diffusion modeling", "semantic latent spaces", "text-to-image generation", "frozen representation encoder", "SigLIP-2", "noise scheduling", "diffusion transformers", "pretraining", "finetuning", "catastrophic overfitting", "multimodal model", "shared representation space"], "githubStars": 58, "organization": {"_id": "662741612ada5b77e310d171", "name": "nyu-visionx", "fullname": "VISIONx @ NYU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"}, "summary_zh": "<ul>\n    <li>RAEs\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7ef4\u8bed\u4e49\u6f5c\u5728\u7a7a\u95f4\u4e2d\u3002</li>\n    <li>\u901a\u8fc7\u4f7f\u7528\u7f51\u7edc\u3001\u5408\u6210\u548c\u6587\u672c\u6e32\u67d3\u6570\u636e\uff0c\u5bf9RAE\u89e3\u7801\u5668\u8fdb\u884c\u6269\u5c55\uff0c\u53d1\u73b0\u7279\u5b9a\u9886\u57df\uff08\u5982\u6587\u672c\uff09\u9700\u8981\u7279\u5b9a\u7684\u6570\u636e\u7ec4\u5408\u3002</li>\n    <li>RAE\u8bbe\u8ba1\u7684\u7b80\u5316\u4f7f\u5f97\u67b6\u6784\u66f4\u5bb9\u6613\u4f18\u5316\uff0c\u5c3d\u7ba1\u566a\u58f0\u8c03\u5ea6\u4ecd\u7136\u91cd\u8981\uff0c\u4f46\u590d\u6742\u7684\u67b6\u6784\u5728\u5927\u89c4\u6a21\u4e0b\u7684\u6536\u76ca\u6709\u9650\u3002</li>\n    <li>RAE\u5728\u6240\u6709\u6a21\u578b\u89c4\u6a21\u4e0a\u90fd\u4f18\u4e8eVAE\uff0c\u7279\u522b\u662f\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u4e14\u5728\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u66f4\u7a33\u5b9a\u3002</li>\n    <li>RAE\u7684\u6269\u6563\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u751f\u6210\u8d28\u91cf\uff0c\u663e\u793a\u51fa\u5176\u4f5c\u4e3a\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u66f4\u5f3a\u57fa\u7840\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Representation Autoencoders (RAEs) are effective for generating images from text by using high-dimensional spaces.</li>\n  <li>The study tests RAEs on larger datasets beyond ImageNet, finding that while larger models improve quality, the type of training data matters for specific tasks like text-to-image generation.</li>\n  <li>RAEs were compared to a state-of-the-art model (FLUX VAE) and performed better, especially during training and fine-tuning, without overfitting.</li>\n  <li>RAEs showed faster training and better image quality than VAEs across all tested model sizes.</li>\n  <li>The research suggests that RAEs can be a stronger and simpler choice for large-scale text-to-image generation, allowing for new multimodal applications.</li>\n</ul>"}, "publishedAt": "2026-01-22T13:58:16.000Z", "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16208.png", "numComments": 1, "submittedBy": {"_id": "6434226da4c9c55871a78052", "avatarUrl": "/avatars/3309832b3115bc6ad08ae1d10f43118b.svg", "fullname": "BoYang Zheng", "name": "bytetriper", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "662741612ada5b77e310d171", "name": "nyu-visionx", "fullname": "VISIONx @ NYU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.15892", "authors": [{"_id": "6972d788fb12c92b735b7397", "user": {"_id": "641aa5e391e3376a057bbd4c", "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg", "isPro": false, "fullname": "Chenghao Fan", "user": "Facico", "type": "user"}, "name": "Chenghao Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:32.109Z", "hidden": false}, {"_id": "6972d788fb12c92b735b7398", "name": "Wen Heng", "hidden": false}, {"_id": "6972d788fb12c92b735b7399", "name": "Bo Li", "hidden": false}, {"_id": "6972d788fb12c92b735b739a", "name": "Sichen Liu", "hidden": false}, {"_id": "6972d788fb12c92b735b739b", "name": "Yuxuan Song", "hidden": false}, {"_id": "6972d788fb12c92b735b739c", "name": "Jing Su", "hidden": false}, {"_id": "6972d788fb12c92b735b739d", "name": "Xiaoye Qu", "hidden": false}, {"_id": "6972d788fb12c92b735b739e", "name": "Kai Shen", "hidden": false}, {"_id": "6972d788fb12c92b735b739f", "name": "Wei Wei", "hidden": false}], "publishedAt": "2026-01-22T12:13:17.000Z", "submittedOnDailyAt": "2026-01-23T00:09:35.389Z", "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "submittedOnDailyBy": {"_id": "641aa5e391e3376a057bbd4c", "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg", "isPro": false, "fullname": "Chenghao Fan", "user": "Facico", "type": "user"}, "summary": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.", "upvotes": 40, "discussionId": "6972d788fb12c92b735b73a0", "projectPage": "https://bytedance-seed.github.io/Stable-DiffCoder/", "githubRepo": "https://github.com/ByteDance-Seed/Stable-DiffCoder", "githubRepoAddedBy": "user", "ai_summary": "Stable-DiffCoder demonstrates superior code modeling performance compared to autoregressive baselines through block diffusion continual pretraining and efficient training mechanisms.", "ai_keywords": ["diffusion-based language models", "autoregressive models", "block diffusion", "continual pretraining", "warmup", "clipped noise schedule", "supervised fine-tuning", "code modeling", "structured code modeling", "data augmentation"], "githubStars": 16, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08DLLMs\uff09\u80fd\u591f\u5757\u72b6\u751f\u6210\u6570\u636e\uff0c\u5e76\u4e14\u6bd4\u81ea\u56de\u5f52\uff08AR\uff09\u6a21\u578b\u66f4\u597d\u5730\u91cd\u7528\u6570\u636e\u3002</li>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86Stable-DiffCoder\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u4ee3\u7801\u6a21\u578b\uff0c\u91c7\u7528Seed-Coder\u67b6\u6784\u548c\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>\u4e3a\u4e86\u63d0\u9ad8\u77e5\u8bc6\u5b66\u4e60\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u6211\u4eec\u52a0\u5165\u4e86\u5757\u6269\u6563\u6301\u7eed\u9884\u8bad\u7ec3\uff08CPT\uff09\u9636\u6bb5\uff0c\u5e76\u4f18\u5316\u4e86\u9884\u70ed\u548c\u566a\u58f0\u8c03\u5ea6\u3002</li>\n    <li>Stable-DiffCoder\u5728\u591a\u9879\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6574\u4f53\u8868\u73b0\u4f18\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\u3002</li>\n    <li>\u901a\u8fc7\u6269\u6563\u8bad\u7ec3\uff0cStable-DiffCoder\u5728\u4ee3\u7801\u5efa\u6a21\u8d28\u91cf\u4e0a\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u81ea\u56de\u5f52\u8bad\u7ec3\uff0c\u7279\u522b\u662f\u5728\u7f16\u8f91\u548c\u63a8\u7406\u65b9\u9762\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion-based language models (DLLMs) can generate code in blocks rather than one by one, which allows for better data reuse.</li>\n    <li>Stable-DiffCoder is a new model that uses an existing architecture and improves training methods for better performance.</li>\n    <li>This model outperforms traditional autoregressive (AR) models in various coding tasks when using the same data and architecture.</li>\n    <li>Stable-DiffCoder shows better results than many AR and DLLM models, even with limited training stages.</li>\n    <li>Using diffusion methods helps improve code editing and reasoning, and aids low-resource programming languages through data enhancement.</li>\n</ul>"}, "publishedAt": "2026-01-22T07:13:17.000Z", "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "summary": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15892.png", "numComments": 0, "submittedBy": {"_id": "641aa5e391e3376a057bbd4c", "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg", "fullname": "Chenghao Fan", "name": "Facico", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13, "isUserFollowing": false}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.16093", "authors": [{"_id": "6972f00ffb12c92b735b74bb", "user": {"_id": "644d0b5c6dfd5f8240d95f5e", "avatarUrl": "/avatars/c5bcc5691fe8ec3e24558afb80feecab.svg", "isPro": false, "fullname": "zhou yikang", "user": "zhouyik", "type": "user"}, "name": "Yikang Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:04.804Z", "hidden": false}, {"_id": "6972f00ffb12c92b735b74bc", "name": "Tao Zhang", "hidden": false}, {"_id": "6972f00ffb12c92b735b74bd", "user": {"_id": "66350ea032a5f38150f7a82b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66350ea032a5f38150f7a82b/UOrv46KJsQV9Ye_bmyIS6.jpeg", "isPro": false, "fullname": "GongDengxian", "user": "godx7", "type": "user"}, "name": "Dengxian Gong", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:21.767Z", "hidden": false}, {"_id": "6972f00ffb12c92b735b74be", "name": "Yuanzheng Wu", "hidden": false}, {"_id": "6972f00ffb12c92b735b74bf", "name": "Ye Tian", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c0", "name": "Haochen Wang", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c1", "name": "Haobo Yuan", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c2", "name": "Jiacong Wang", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c3", "name": "Lu Qi", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c4", "name": "Hao Fei", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c5", "name": "Anran Wang", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c6", "name": "Zhuochen Wang", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c7", "name": "Yujing Wang", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c8", "name": "Cheng Chen", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c9", "name": "Shunping Ji", "hidden": false}, {"_id": "6972f00ffb12c92b735b74ca", "name": "Xiangtai Li", "hidden": false}], "publishedAt": "2026-01-22T16:44:09.000Z", "submittedOnDailyAt": "2026-01-23T01:22:36.775Z", "title": "SAMTok: Representing Any Mask with Two Words", "submittedOnDailyBy": {"_id": "63958b4414513eaf9029ebf1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png", "isPro": false, "fullname": "Xiangtai Li", "user": "LXT", "type": "user"}, "summary": "Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.", "upvotes": 31, "discussionId": "6972f00ffb12c92b735b74cb", "projectPage": "https://zhouyiks.github.io/projects/SAMTok/", "githubRepo": "https://github.com/bytedance/Sa2VA/tree/main/projects/samtok", "githubRepoAddedBy": "user", "ai_summary": "SAMTok enables pixel-wise capabilities in multi-modal LLMs through discrete mask tokenization and standard training methods, achieving state-of-the-art performance on various vision-language tasks.", "ai_keywords": ["multi-modal LLMs", "region mask", "discrete mask tokenizer", "SAM2", "mask encoder", "residual vector quantizer", "next-token prediction", "reinforcement learning", "region captioning", "region VQA", "grounded conversation", "referring segmentation", "scene graph parsing", "multi-round interactive segmentation", "textual answer-matching reward", "GRES", "GCG"], "githubStars": 1502, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86SAMTok\uff0c\u4e00\u79cd\u79bb\u6563\u7684\u63a9\u7801\u6807\u8bb0\u5668\uff0c\u53ef\u4ee5\u5c06\u533a\u57df\u63a9\u7801\u8f6c\u6362\u4e3a\u4e24\u4e2a\u7279\u6b8a\u7684\u6807\u8bb0\u3002</li>\n    <li>SAMTok\u4f7f\u57fa\u7840\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u80fd\u591f\u901a\u8fc7\u6807\u51c6\u7684\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u5b66\u4e60\u50cf\u7d20\u7ea7\u80fd\u529b\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u65e0\u9700\u6539\u53d8\u6a21\u578b\u67b6\u6784\u6216\u8bbe\u8ba1\u7279\u6b8a\u7684\u635f\u5931\u51fd\u6570\uff0c\u7b80\u5316\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u3002</li>\n    <li>QwenVL-SAMTok\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\uff08\u5982\u533a\u57df\u63cf\u8ff0\u548c\u5bf9\u8bdd\uff09\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6548\u679c\u3002</li>\n    <li>\u5f15\u5165\u4e86\u4e00\u79cd\u6587\u672c\u7b54\u6848\u5339\u914d\u5956\u52b1\uff0c\u589e\u5f3a\u4e86\u63a9\u7801\u751f\u6210\u7684\u5f3a\u5316\u5b66\u4e60\u6548\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>SAMTok is a new tool that helps multi-modal large language models (MLLMs) understand and generate pixel-level information.</li>\n    <li>It simplifies the process by converting region masks into special tokens, allowing models to learn without needing complex changes.</li>\n    <li>SAMTok is built on existing technology and uses a large dataset of diverse masks to create effective and compact tokens.</li>\n    <li>The new approach achieves top results in several tasks like region captioning and interactive segmentation.</li>\n    <li>It also includes a reward system for better learning and performance, making it easier to implement in MLLMs.</li>\n</ul>"}, "publishedAt": "2026-01-22T11:44:09.000Z", "title": "SAMTok: Representing Any Mask with Two Words", "summary": "Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16093.png", "numComments": 1, "submittedBy": {"_id": "63958b4414513eaf9029ebf1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png", "fullname": "Xiangtai Li", "name": "LXT", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.16175", "authors": [{"_id": "6972e02afb12c92b735b73c2", "name": "Mert Yuksekgonul", "hidden": false}, {"_id": "6972e02afb12c92b735b73c3", "name": "Daniel Koceja", "hidden": false}, {"_id": "6972e02afb12c92b735b73c4", "name": "Xinhao Li", "hidden": false}, {"_id": "6972e02afb12c92b735b73c5", "name": "Federico Bianchi", "hidden": false}, {"_id": "6972e02afb12c92b735b73c6", "name": "Jed McCaleb", "hidden": false}, {"_id": "6972e02afb12c92b735b73c7", "name": "Xiaolong Wang", "hidden": false}, {"_id": "6972e02afb12c92b735b73c8", "name": "Jan Kautz", "hidden": false}, {"_id": "6972e02afb12c92b735b73c9", "name": "Yejin Choi", "hidden": false}, {"_id": "6972e02afb12c92b735b73ca", "name": "James Zou", "hidden": false}, {"_id": "6972e02afb12c92b735b73cb", "name": "Carlos Guestrin", "hidden": false}, {"_id": "6972e02afb12c92b735b73cc", "name": "Yu Sun", "hidden": false}], "publishedAt": "2026-01-22T18:24:00.000Z", "submittedOnDailyAt": "2026-01-23T00:15:05.275Z", "title": "Learning to Discover at Test Time", "submittedOnDailyBy": {"_id": "603f7c7af84ebe399f1c85cf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625214886903-603f7c7af84ebe399f1c85cf.jpeg", "isPro": false, "fullname": "Federico Bianchi", "user": "vinid", "type": "user"}, "summary": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erd\u0151s' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to 2times faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.", "upvotes": 24, "discussionId": "6972e02afb12c92b735b73cd", "projectPage": "https://test-time-training.github.io/discover/", "githubRepo": "https://github.com/test-time-training/discover", "githubRepoAddedBy": "user", "ai_summary": "Test-time training enables AI systems to discover optimal solutions for specific scientific problems through continual learning focused on individual challenges rather than generalization.", "ai_keywords": ["reinforcement learning", "test-time training", "continual learning", "search subroutine", "learning objective", "OpenAI gpt-oss-120b", "Tinker API"], "githubStars": 88, "organization": {"_id": "672c672dcf09d152f4da04c4", "name": "StanfordUniversity", "fullname": "Stanford University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/vJI0POlzGMXL2878t1vz2.jpeg"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3a\u201c\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u53d1\u73b0\u201d\uff08TTT-Discover\uff09\uff0c\u5229\u7528\u4eba\u5de5\u667a\u80fd\u5728\u79d1\u5b66\u95ee\u9898\u4e0a\u5bfb\u627e\u6700\u4f73\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u901a\u8fc7\u5728\u6d4b\u8bd5\u65f6\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\uff0c\u6a21\u578b\u53ef\u4ee5\u6839\u636e\u7279\u5b9a\u95ee\u9898\u8fdb\u884c\u6301\u7eed\u5b66\u4e60\uff0c\u800c\u4e0d\u662f\u4ec5\u4ec5\u5e73\u5747\u5bfb\u627e\u591a\u4e2a\u597d\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>TTT-Discover\u5728\u6570\u5b66\u3001GPU\u5185\u6838\u5de5\u7a0b\u3001\u7b97\u6cd5\u8bbe\u8ba1\u548c\u751f\u7269\u5b66\u7b49\u9886\u57df\u7684\u591a\u4e2a\u95ee\u9898\u4e0a\u521b\u4e0b\u4e86\u65b0\u7684\u6700\u4f18\u7eaa\u5f55\u3002</li>\n    <li>\u6211\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u7ecf\u8fc7\u4e13\u5bb6\u5ba1\u67e5\uff0c\u4f7f\u7528\u7684\u662f\u516c\u5f00\u6a21\u578b\uff08OpenAI gpt-oss-120b\uff09\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u516c\u5f00\u4ee3\u7801\u8fdb\u884c\u590d\u73b0\u3002</li>\n    <li>\u6bcf\u4e2a\u95ee\u9898\u7684\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u6210\u672c\u4ec5\u4e3a\u51e0\u767e\u7f8e\u5143\uff0c\u4f7f\u7528\u7684API\u662fThinking Machines\u7684Tinker\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>This study introduces a new method called Test-Time Training to Discover (TTT-Discover) that uses AI to find top solutions for specific scientific problems.</li>\n    <li>TTT-Discover allows a language model (LLM) to learn and adapt during testing, focusing on producing one excellent solution instead of many average ones.</li>\n    <li>The researchers achieved state-of-the-art results in various fields, including mathematics, GPU engineering, algorithm design, and biology.</li>\n    <li>Results include significant improvements, such as doubling the speed of a GPU kernel and solving complex mathematical problems.</li>\n    <li>All findings are reproducible with an open-source model and publicly available code, making the research accessible and cost-effective.</li>\n</ul>"}, "publishedAt": "2026-01-22T13:24:00.000Z", "title": "Learning to Discover at Test Time", "summary": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erd\u0151s' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to 2times faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16175.png", "numComments": 1, "submittedBy": {"_id": "603f7c7af84ebe399f1c85cf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625214886903-603f7c7af84ebe399f1c85cf.jpeg", "fullname": "Federico Bianchi", "name": "vinid", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 21, "isUserFollowing": false}, "organization": {"_id": "672c672dcf09d152f4da04c4", "name": "StanfordUniversity", "fullname": "Stanford University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/vJI0POlzGMXL2878t1vz2.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.15621", "authors": [{"_id": "6972e310fb12c92b735b746a", "name": "Hangrui Hu", "hidden": false}, {"_id": "6972e310fb12c92b735b746b", "name": "Xinfa Zhu", "hidden": false}, {"_id": "6972e310fb12c92b735b746c", "name": "Ting He", "hidden": false}, {"_id": "6972e310fb12c92b735b746d", "name": "Dake Guo", "hidden": false}, {"_id": "6972e310fb12c92b735b746e", "name": "Bin Zhang", "hidden": false}, {"_id": "6972e310fb12c92b735b746f", "user": {"_id": "664eaf0a98e93ef417c3cc42", "avatarUrl": "/avatars/67fb44351cac8964410e5b6549817182.svg", "isPro": false, "fullname": "Xiong Wang", "user": "xiongwang", "type": "user"}, "name": "Xiong Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T20:15:12.488Z", "hidden": false}, {"_id": "6972e310fb12c92b735b7470", "name": "Zhifang Guo", "hidden": false}, {"_id": "6972e310fb12c92b735b7471", "user": {"_id": "67dbdf261956dcedf0f0a7e1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/BRzXe_7jytEYadJ0byMyD.png", "isPro": false, "fullname": "ZiyueJiang", "user": "ZiyueJiang", "type": "user"}, "name": "Ziyue Jiang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-23T09:48:40.953Z", "hidden": false}, {"_id": "6972e310fb12c92b735b7472", "user": {"_id": "666fd382b955b0e655165768", "avatarUrl": "/avatars/66476925471bda2dc9b57f091f245dd9.svg", "isPro": false, "fullname": "hongkun hao", "user": "hongkunhao", "type": "user"}, "name": "Hongkun Hao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-23T09:48:26.167Z", "hidden": false}, {"_id": "6972e310fb12c92b735b7473", "user": {"_id": "6370e58a967e405db11cf788", "avatarUrl": "/avatars/369ed523fd7f9ac08baf2d3b4b2d8426.svg", "isPro": false, "fullname": "guozishan", "user": "jimapple", "type": "user"}, "name": "Zishan Guo", "status": "admin_assigned", "statusLastChangedAt": "2026-01-23T09:48:20.045Z", "hidden": false}, {"_id": "6972e310fb12c92b735b7474", "name": "Xinyu Zhang", "hidden": false}, {"_id": "6972e310fb12c92b735b7475", "name": "Pei Zhang", "hidden": false}, {"_id": "6972e310fb12c92b735b7476", "user": {"_id": "64b0a77df12b47366663884c", "avatarUrl": "/avatars/a212ea862abb5966060e439dd0e7656f.svg", "isPro": false, "fullname": "Baosong Yang", "user": "Baosong", "type": "user"}, "name": "Baosong Yang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-23T09:48:01.836Z", "hidden": false}, {"_id": "6972e310fb12c92b735b7477", "name": "Jin Xu", "hidden": false}, {"_id": "6972e310fb12c92b735b7478", "name": "Jingren Zhou", "hidden": false}, {"_id": "6972e310fb12c92b735b7479", "user": {"_id": "620760a26e3b7210c2ff1943", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg", "isPro": false, "fullname": "Junyang Lin", "user": "JustinLin610", "type": "user"}, "name": "Junyang Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-23T09:47:54.300Z", "hidden": false}], "publishedAt": "2026-01-22T03:51:43.000Z", "submittedOnDailyAt": "2026-01-23T00:25:20.168Z", "title": "Qwen3-TTS Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission (97,ms) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.", "upvotes": 20, "discussionId": "6972e311fb12c92b735b747a", "githubRepo": "https://github.com/QwenLM/Qwen3-TTS", "githubRepoAddedBy": "user", "ai_summary": "The Qwen3-TTS series presents advanced multilingual text-to-speech models with voice cloning and controllable speech generation capabilities, utilizing dual-track LM architecture and specialized speech tokenizers for efficient streaming synthesis.", "ai_keywords": ["text-to-speech", "voice cloning", "dual-track LM architecture", "speech tokenizers", "Qwen-TTS-Tokenizer-25Hz", "Qwen-TTS-Tokenizer-12Hz", "DiT", "ConvNet", "streaming waveform reconstruction", "multilingual", "controllable speech generation"], "githubStars": 2193, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n  <li>\u6211\u4eec\u4ecb\u7ecd\u4e86Qwen3-TTS\u7cfb\u5217\uff0c\u8fd9\u662f\u4e00\u79cd\u5148\u8fdb\u7684\u591a\u8bed\u8a00\u3001\u53ef\u63a7\u3001\u7a33\u5065\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u3002</li>\n  <li>Qwen3-TTS\u652f\u63013\u79d2\u5185\u7684\u8bed\u97f3\u514b\u9686\u548c\u57fa\u4e8e\u63cf\u8ff0\u7684\u63a7\u5236\uff0c\u5141\u8bb8\u521b\u5efa\u65b0\u58f0\u97f3\u5e76\u7cbe\u7ec6\u8c03\u6574\u8f93\u51fa\u8bed\u97f3\u3002</li>\n  <li>\u8be5\u6a21\u578b\u57fa\u4e8e\u8d85\u8fc7500\u4e07\u5c0f\u65f6\u7684\u8bed\u97f3\u6570\u636e\uff0c\u6db5\u76d610\u79cd\u8bed\u8a00\uff0c\u7528\u4e8e\u5b9e\u65f6\u5408\u6210\u3002</li>\n  <li>Qwen3-TTS\u91c7\u7528\u53cc\u8f68\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u5e76\u914d\u5907\u4e24\u79cd\u8bed\u97f3\u5206\u8bcd\u5668\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u6d41\u5f0f\u5904\u7406\u3002</li>\n  <li>\u6211\u4eec\u5c06\u8fd9\u4e9b\u5206\u8bcd\u5668\u548c\u6a21\u578b\u4ee5Apache 2.0\u8bb8\u53ef\u8bc1\u53d1\u5e03\uff0c\u4fc3\u8fdb\u793e\u533a\u7684\u7814\u7a76\u548c\u5f00\u53d1\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Qwen3-TTS is a new series of advanced text-to-speech models that can produce speech in multiple languages.</li>\n    <li>It features voice cloning and allows users to control and manipulate the speech output in detail.</li>\n    <li>The models were trained on over 5 million hours of speech data in 10 languages.</li>\n    <li>There are two types of speech tokenizers: one that focuses on semantic content and another that allows for very low latency streaming.</li>\n    <li>The models and tokenizers are available for public use under the Apache 2.0 license to support further research and development.</li>\n</ul>"}, "publishedAt": "2026-01-21T22:51:43.000Z", "title": "Qwen3-TTS Technical Report", "summary": "In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission (97,ms) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15621.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 214, "isUserFollowing": false}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2601.12538", "authors": [{"_id": "6971913fc1c7409747bf9564", "name": "Tianxin Wei", "hidden": false}, {"_id": "6971913fc1c7409747bf9565", "user": {"_id": "6742eb40924e80c3c80ebe13", "avatarUrl": "/avatars/e6ccb1a89a1ea0bfca70779966f4f429.svg", "isPro": false, "fullname": "Ting-Wei Li", "user": "tingwl0122", "type": "user"}, "name": "Ting-Wei Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T17:12:21.531Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9566", "name": "Zhining Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf9567", "name": "Xuying Ning", "hidden": false}, {"_id": "6971913fc1c7409747bf9568", "name": "Ze Yang", "hidden": false}, {"_id": "6971913fc1c7409747bf9569", "name": "Jiaru Zou", "hidden": false}, {"_id": "6971913fc1c7409747bf956a", "name": "Zhichen Zeng", "hidden": false}, {"_id": "6971913fc1c7409747bf956b", "name": "Ruizhong Qiu", "hidden": false}, {"_id": "6971913fc1c7409747bf956c", "name": "Xiao Lin", "hidden": false}, {"_id": "6971913fc1c7409747bf956d", "name": "Dongqi Fu", "hidden": false}, {"_id": "6971913fc1c7409747bf956e", "name": "Zihao Li", "hidden": false}, {"_id": "6971913fc1c7409747bf956f", "user": {"_id": "653962e75c8e4863e1a2068f", "avatarUrl": "/avatars/d4f5f5da141f37d53ca1986ff17b325e.svg", "isPro": false, "fullname": "Mengting Ai", "user": "famous-blue-raincoat", "type": "user"}, "name": "Mengting Ai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:10.378Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9570", "user": {"_id": "677830bd3f2e3ec475576303", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dhwqUDkk66m4oSGSbcd7j.png", "isPro": false, "fullname": "Duo Zhou", "user": "Claudius7", "type": "user"}, "name": "Duo Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:12.476Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9571", "name": "Wenxuan Bao", "hidden": false}, {"_id": "6971913fc1c7409747bf9572", "user": {"_id": "646323556c27a7e33b23f198", "avatarUrl": "/avatars/17fe142f689ab4be3c2374d1d90393db.svg", "isPro": false, "fullname": "Yunzhe Li", "user": "yunzhel2", "type": "user"}, "name": "Yunzhe Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:14.383Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9573", "name": "Gaotang Li", "hidden": false}, {"_id": "6971913fc1c7409747bf9574", "name": "Cheng Qian", "hidden": false}, {"_id": "6971913fc1c7409747bf9575", "name": "Yu Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf9576", "name": "Xiangru Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf9577", "name": "Yin Xiao", "hidden": false}, {"_id": "6971913fc1c7409747bf9578", "name": "Liri Fang", "hidden": false}, {"_id": "6971913fc1c7409747bf9579", "name": "Hui Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf957a", "name": "Xianfeng Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf957b", "name": "Yuji Zhang", "hidden": false}, {"_id": "6971913fc1c7409747bf957c", "name": "Chi Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf957d", "name": "Jiaxuan You", "hidden": false}, {"_id": "6971913fc1c7409747bf957e", "name": "Heng Ji", "hidden": false}, {"_id": "6971913fc1c7409747bf957f", "name": "Hanghang Tong", "hidden": false}, {"_id": "6971913fc1c7409747bf9580", "name": "Jingrui He", "hidden": false}], "publishedAt": "2026-01-18T18:58:23.000Z", "submittedOnDailyAt": "2026-01-22T00:27:25.162Z", "title": "Agentic Reasoning for Large Language Models", "submittedOnDailyBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "isPro": false, "fullname": "Jiaru Zou", "user": "jiaruz2", "type": "user"}, "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "upvotes": 125, "discussionId": "69719140c1c7409747bf9581", "githubRepo": "https://github.com/weitianxin/Awesome-Agentic-Reasoning", "githubRepoAddedBy": "user", "ai_summary": "Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.", "ai_keywords": ["large language models", "agentic reasoning", "autonomous agents", "planning", "tool use", "search", "feedback", "memory", "adaptation", "collaborative settings", "coordination", "knowledge sharing", "reinforcement learning", "supervised fine-tuning", "in-context reasoning", "post-training reasoning", "real-world applications", "benchmarks", "thought and action", "world modeling", "scalable multi-agent training", "governance"], "githubStars": 105, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "summary_zh": "<ul>\n    <li>\u63a8\u7406\u662f\u63a8\u65ad\u3001\u89e3\u51b3\u95ee\u9898\u548c\u51b3\u7b56\u7684\u57fa\u672c\u8ba4\u77e5\u8fc7\u7a0b\u3002</li>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5c01\u95ed\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5f00\u653e\u548c\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u81ea\u4e3b\u63a8\u7406\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u80fd\u591f\u901a\u8fc7\u6301\u7eed\u4e92\u52a8\u8fdb\u884c\u89c4\u5212\u3001\u884c\u52a8\u548c\u5b66\u4e60\u3002</li>\n    <li>\u81ea\u4e3b\u63a8\u7406\u5206\u4e3a\u4e09\u4e2a\u5c42\u9762\uff1a\u57fa\u7840\u80fd\u529b\u3001\u81ea\u6211\u8fdb\u5316\u80fd\u529b\u548c\u96c6\u4f53\u591a\u4ee3\u7406\u63a8\u7406\u3002</li>\n    <li>\u8be5\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u81ea\u4e3b\u63a8\u7406\u5728\u79d1\u5b66\u3001\u673a\u5668\u4eba\u3001\u533b\u7597\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6846\u67b6\u548c\u672a\u6765\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reasoning is key for tasks like problem-solving and decision-making, but large language models (LLMs) have difficulties in dynamic environments.</li>\n    <li>Agentic reasoning changes the view of LLMs to be autonomous agents that can plan, act, and learn from interactions.</li>\n    <li>The survey describes agentic reasoning in three layers: foundational (basic skills), self-evolving (improving through feedback), and collective (working with others).</li>\n    <li>It differentiates between in-context reasoning (real-time interaction) and post-training reasoning (improving after training).</li>\n    <li>The survey also highlights real-world applications and future challenges like personalization and collaboration for LLMs.</li>\n</ul>"}, "publishedAt": "2026-01-18T13:58:23.000Z", "title": "Agentic Reasoning for Large Language Models", "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12538.png", "numComments": 3, "submittedBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "fullname": "Jiaru Zou", "name": "jiaruz2", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.15876", "authors": [{"_id": "6972d8d5fb12c92b735b73a2", "name": "Taofeng Xue", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a3", "user": {"_id": "6801cbd5f06f2d08f3fd5455", "avatarUrl": "/avatars/c728b48f6938c0d7cb6b14011927ede8.svg", "isPro": false, "fullname": "chong.peng", "user": "KleinChong", "type": "user"}, "name": "Chong Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T20:15:18.698Z", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a4", "name": "Mianqiu Huang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a5", "name": "Linsen Guo", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a6", "user": {"_id": "6764279e684ed3b61b2316a4", "avatarUrl": "/avatars/63a6b6c3b9f442b42480679425951187.svg", "isPro": false, "fullname": "SII-TianchengHAN", "user": "GenSouKai", "type": "user"}, "name": "Tiancheng Han", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:30.106Z", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a7", "name": "Haozhe Wang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a8", "name": "Jianing Wang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a9", "name": "Xiaocheng Zhang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73aa", "name": "Xin Yang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ab", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ac", "name": "Jinrui Ding", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ad", "name": "Xiandi Ma", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ae", "name": "Yuchen Xie", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73af", "name": "Peng Pei", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73b0", "name": "Xunliang Cai", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73b1", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-01-22T11:36:43.000Z", "submittedOnDailyAt": "2026-01-23T07:54:00.525Z", "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience", "submittedOnDailyBy": {"_id": "6459c7c10aba070266e41bb1", "avatarUrl": "/avatars/2178cac69cf4123db5e85191160f3795.svg", "isPro": false, "fullname": "mqhuang", "user": "LutherXD", "type": "user"}, "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.", "upvotes": 62, "discussionId": "6972d8d5fb12c92b735b73b2", "ai_summary": "EvoCUA introduces an evolutionary approach to computer-use agents that combines autonomous task generation with policy optimization to achieve superior performance in complex, long-horizon tasks.", "ai_keywords": ["computer-use agents", "native computer-use agents", "data generation", "policy optimization", "evolutionary cycle", "verifiable synthesis engine", "executable validators", "sandbox rollouts", "iterative evolving learning", "capability boundaries", "error analysis", "self-correction", "OSWorld benchmark", "foundation models"], "summary_zh": "<ul>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u6a21\u578bEvoCUA\uff0c\u80fd\u5728\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u65b9\u9762\u53d6\u5f97\u91cd\u5927\u8fdb\u5c55\u3002</li>\n    <li>EvoCUA\u901a\u8fc7\u81ea\u6211\u751f\u6210\u6570\u636e\u548c\u4f18\u5316\u7b56\u7565\uff0c\u514b\u670d\u4e86\u9759\u6001\u6570\u636e\u7684\u9650\u5236\uff0c\u5f62\u6210\u4e00\u4e2a\u81ea\u6211\u7ef4\u6301\u7684\u8fdb\u5316\u5faa\u73af\u3002</li>\n    <li>\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u7684\u5408\u6210\u5f15\u64ce\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u7684\u4efb\u52a1\u548c\u53ef\u6267\u884c\u7684\u9a8c\u8bc1\u5668\uff0c\u4ee5\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002</li>\n    <li>EvoCUA\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6210\u529f\u7387\u8fbe\u5230\u4e8656.7%\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u5f00\u6e90\u6a21\u578b\u548c\u4e00\u4e9b\u95ed\u6e90\u6a21\u578b\u3002</li>\n    <li>\u8fd9\u79cd\u4ee5\u7ecf\u9a8c\u5b66\u4e60\u9a71\u52a8\u7684\u8fdb\u5316\u8303\u5f0f\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u57fa\u7840\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Native computer-use agents (CUA) are advancing multimodal AI but face limitations due to static data.</li>\n    <li>EvoCUA is a new model that combines data generation and policy improvement in a self-sustaining cycle.</li>\n    <li>A synthesis engine creates diverse tasks and validators to help overcome data shortages.</li>\n    <li>EvoCUA uses a large-scale infrastructure to gather experience from many simulations, improving learning efficiency.</li>\n    <li>Tests show EvoCUA has a 56.7% success rate, outperforming previous models and demonstrating its scalability and effectiveness.</li>\n</ul>"}, "publishedAt": "2026-01-22T06:36:43.000Z", "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience", "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15876.png", "numComments": 1, "submittedBy": {"_id": "6459c7c10aba070266e41bb1", "avatarUrl": "/avatars/2178cac69cf4123db5e85191160f3795.svg", "fullname": "mqhuang", "name": "LutherXD", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.12993", "authors": [{"_id": "69705709a8be625b19c2af1f", "user": {"_id": "6708cbdcf8a1d7b26732c038", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/CN1WHMPKfjQ8wmfwOe0ni.png", "isPro": false, "fullname": "Hao Luo", "user": "Lightet", "type": "user"}, "name": "Hao Luo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T10:15:59.988Z", "hidden": false}, {"_id": "69705709a8be625b19c2af20", "name": "Ye Wang", "hidden": false}, {"_id": "69705709a8be625b19c2af21", "user": {"_id": "640dd700fdeaae139081f598", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg", "isPro": false, "fullname": "Wanpeng Zhang", "user": "zawnpn", "type": "user"}, "name": "Wanpeng Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:19:56.371Z", "hidden": false}, {"_id": "69705709a8be625b19c2af22", "user": {"_id": "64eac1f496f42afd627d439c", "avatarUrl": "/avatars/aa46265122b8a1170f57475494d7922e.svg", "isPro": false, "fullname": "Sipeng Zheng", "user": "sipeng9527", "type": "user"}, "name": "Sipeng Zheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:38.507Z", "hidden": false}, {"_id": "69705709a8be625b19c2af23", "user": {"_id": "66c84a9eab23d3d7dfb2a368", "avatarUrl": "/avatars/b0a50133c6a95ed340dfb462e87820f4.svg", "isPro": false, "fullname": "ziheng xi", "user": "zhenqis123", "type": "user"}, "name": "Ziheng Xi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:45.981Z", "hidden": false}, {"_id": "69705709a8be625b19c2af24", "user": {"_id": "64bdd5cc76a6e2efccb22100", "avatarUrl": "/avatars/5a0edc24283616dafc76ce5ec97ab5a0.svg", "isPro": false, "fullname": "xuchaoyi", "user": "co1one", "type": "user"}, "name": "Chaoyi Xu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:56.014Z", "hidden": false}, {"_id": "69705709a8be625b19c2af25", "user": {"_id": "68872ff6c18b7e1e13115564", "avatarUrl": "/avatars/f908fc3cc89cd81493105359093f299d.svg", "isPro": false, "fullname": "Haiweng Xu", "user": "Seaman05", "type": "user"}, "name": "Haiweng Xu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:50:01.261Z", "hidden": false}, {"_id": "69705709a8be625b19c2af26", "user": {"_id": "644560657a7b94ddc2d445a3", "avatarUrl": "/avatars/09d6447da6ff1bd0b2b00c899c9f1b28.svg", "isPro": false, "fullname": "Haoqi Yuan", "user": "Yaya041", "type": "user"}, "name": "Haoqi Yuan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:50:06.048Z", "hidden": false}, {"_id": "69705709a8be625b19c2af27", "name": "Chi Zhang", "hidden": false}, {"_id": "69705709a8be625b19c2af28", "name": "Yiqing Wang", "hidden": false}, {"_id": "69705709a8be625b19c2af29", "name": "Yicheng Feng", "hidden": false}, {"_id": "69705709a8be625b19c2af2a", "user": {"_id": "67d905c0e27ba28109384f5c", "avatarUrl": "/avatars/26712594ac9d43c8d1a3e75e36b5df16.svg", "isPro": false, "fullname": "Zongqing Lu", "user": "chungtsing", "type": "user"}, "name": "Zongqing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:50:24.833Z", "hidden": false}], "publishedAt": "2026-01-19T12:20:38.000Z", "submittedOnDailyAt": "2026-01-21T02:12:40.880Z", "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization", "submittedOnDailyBy": {"_id": "640dd700fdeaae139081f598", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg", "isPro": false, "fullname": "Wanpeng Zhang", "user": "zawnpn", "type": "user"}, "summary": "We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal \"mother tongue\" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.", "upvotes": 59, "discussionId": "69705709a8be625b19c2af2b", "projectPage": "https://research.beingbeyond.com/being-h05", "githubRepo": "https://github.com/BeingBeyond/Being-H", "githubRepoAddedBy": "user", "ai_summary": "Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.", "ai_keywords": ["Vision-Language-Action", "cross-embodiment generalization", "human-centric learning", "multimodal data", "Unified Action Space", "Mixture-of-Transformers", "Mixture-of-Flow", "manifold-preserving gating", "universal async chunking"], "githubStars": 265, "organization": {"_id": "687a8ba5aedd77694bc94386", "name": "BeingBeyond", "fullname": "BeingBeyond", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/A6vd2S9BqXgtEWHaPy5qW.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86Being-H0.5\uff0c\u4e00\u4e2a\u57fa\u7840\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u7684\u901a\u7528\u6027\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u5f62\u6001\u5dee\u5f02\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u4eba\u7c7b\u4e92\u52a8\u8f68\u8ff9\u89c6\u4e3a\u7269\u7406\u4e92\u52a8\u7684\u201c\u6bcd\u8bed\u201d\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86UniHand-2.0\uff0c\u8fd9\u662f\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc735,000\u5c0f\u65f6\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u6db5\u76d630\u79cd\u4e0d\u540c\u7684\u673a\u5668\u4eba\u5f62\u6001\u3002</li>\n    <li>Being-H0.5\u4f7f\u7528\u4e00\u79cd\u6df7\u5408\u53d8\u6362\u5668\u67b6\u6784\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u6d41\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u5171\u4eab\u8fd0\u52a8\u539f\u7406\u4e0e\u7279\u5b9a\u673a\u5668\u4eba\u7684\u4e13\u5bb6\u89e3\u8026\u3002</li>\n    <li>\u6211\u4eec\u5728\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u7684\u7ed3\u679c\uff0c\u5e76\u5728\u4e94\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u8de8\u5f62\u6001\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Being-H0.5 is a new model that combines vision, language, and actions for better performance across different robots.</li>\n    <li>It addresses issues with existing models that struggle with various robot types and limited data by using human interaction as a learning base.</li>\n    <li>The model is supported by UniHand-2.0, which is the largest dataset for training robots, featuring over 35,000 hours of data from 30 different robotic systems.</li>\n    <li>Being-H0.5 features a design that separates general movement skills from specific robot needs, allowing it to adapt to different robots effectively.</li>\n    <li>The model has shown excellent results in tests and performs well across five different robotic platforms.</li>\n</ul>"}, "publishedAt": "2026-01-19T07:20:38.000Z", "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization", "summary": "We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal \"mother tongue\" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12993.png", "numComments": 1, "submittedBy": {"_id": "640dd700fdeaae139081f598", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg", "fullname": "Wanpeng Zhang", "name": "zawnpn", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11, "isUserFollowing": false}, "organization": {"_id": "687a8ba5aedd77694bc94386", "name": "BeingBeyond", "fullname": "BeingBeyond", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/A6vd2S9BqXgtEWHaPy5qW.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.15165", "authors": [{"_id": "6971933ac1c7409747bf9597", "name": "Zanlin Ni", "hidden": false}, {"_id": "6971933ac1c7409747bf9598", "user": {"_id": "6486dde1f74857df3f1a5828", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg", "isPro": false, "fullname": "Shenzhi Wang", "user": "shenzhi-wang", "type": "user"}, "name": "Shenzhi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:54.254Z", "hidden": false}, {"_id": "6971933ac1c7409747bf9599", "name": "Yang Yue", "hidden": false}, {"_id": "6971933ac1c7409747bf959a", "name": "Tianyu Yu", "hidden": false}, {"_id": "6971933ac1c7409747bf959b", "name": "Weilin Zhao", "hidden": false}, {"_id": "6971933ac1c7409747bf959c", "name": "Yeguo Hua", "hidden": false}, {"_id": "6971933ac1c7409747bf959d", "name": "Tianyi Chen", "hidden": false}, {"_id": "6971933ac1c7409747bf959e", "name": "Jun Song", "hidden": false}, {"_id": "6971933ac1c7409747bf959f", "name": "Cheng Yu", "hidden": false}, {"_id": "6971933ac1c7409747bf95a0", "name": "Bo Zheng", "hidden": false}, {"_id": "6971933ac1c7409747bf95a1", "name": "Gao Huang", "hidden": false}], "publishedAt": "2026-01-21T16:41:58.000Z", "submittedOnDailyAt": "2026-01-23T00:11:51.141Z", "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models", "submittedOnDailyBy": {"_id": "63987ffb2ceb55aabe0852f3", "avatarUrl": "/avatars/343b796ff6b8906203904e8c620d7eb5.svg", "isPro": false, "fullname": "Zanlin Ni", "user": "nzl-thu", "type": "user"}, "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap", "upvotes": 55, "discussionId": "6971933ac1c7409747bf95a2", "projectPage": "https://nzl-thu.github.io/the-flexibility-trap", "githubRepo": "https://github.com/LeapLabTHU/JustGRPO", "githubRepoAddedBy": "user", "ai_summary": "Arbitrary order generation in diffusion large language models limits reasoning capability by causing premature solution space collapse, making standard policy optimization more effective.", "ai_keywords": ["diffusion large language models", "left-to-right constraint", "token generation", "reinforcement learning", "reasoning potential", "mathematical reasoning", "coding tasks", "combinatorial trajectories", "likelihoods", "Group Relative Policy Optimization", "GRPO", "parallel decoding"], "githubStars": 66, "organization": {"_id": "69719700e3846c07669d13ee", "name": "Tsinghua-LeapLab", "fullname": "Tsinghua-LeapLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63987ffb2ceb55aabe0852f3/hflTWNTGxeJx83xNkYrDB.png"}, "summary_zh": "<ul>\n    <li>\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u6253\u7834\u4e86\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u7684\u5de6\u5230\u53f3\u751f\u6210\u9650\u5236\uff0c\u53ef\u4ee5\u4ee5\u4efb\u610f\u987a\u5e8f\u751f\u6210\u6807\u8bb0\u3002</li>\n    <li>\u867d\u7136\u8fd9\u79cd\u7075\u6d3b\u6027\u770b\u4f3c\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b9e\u9645\u4e0a\u5374\u9650\u5236\u4e86 dLLMs \u7684\u63a8\u7406\u8fb9\u754c\u3002</li>\n    <li>dLLMs \u503e\u5411\u4e8e\u5229\u7528\u8fd9\u79cd\u987a\u5e8f\u7075\u6d3b\u6027\uff0c\u8df3\u8fc7\u5bf9\u63a2\u7d22\u81f3\u5173\u91cd\u8981\u7684\u9ad8\u4e0d\u786e\u5b9a\u6027\u6807\u8bb0\uff0c\u4ece\u800c\u5bfc\u81f4\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u8fc7\u65e9\u5d29\u6e83\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u7684 JustGRPO \u65b9\u6cd5\u653e\u5f03\u4e86\u4efb\u610f\u987a\u5e8f\u751f\u6210\uff0c\u91c7\u7528\u6807\u51c6\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u63a8\u7406\u6548\u679c\u3002</li>\n    <li>JustGRPO \u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u5728 GSM8K \u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86 89.1% \u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u7559\u4e86 dLLMs \u7684\u5e76\u884c\u89e3\u7801\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion Large Language Models (dLLMs) can generate text in any order, unlike traditional models that go left to right.</li>\n    <li>This flexibility was thought to improve reasoning for tasks like math and coding.</li>\n    <li>However, the study shows that this order flexibility can actually limit reasoning by avoiding important, uncertain tokens.</li>\n    <li>Current reinforcement learning methods for dLLMs may not work well because they complicate the use of this flexibility.</li>\n    <li>The authors propose a simpler method called JustGRPO, which achieves high accuracy while still using the benefits of dLLMs.</li>\n</ul>"}, "publishedAt": "2026-01-21T11:41:58.000Z", "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models", "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15165.png", "numComments": 1, "submittedBy": {"_id": "63987ffb2ceb55aabe0852f3", "avatarUrl": "/avatars/343b796ff6b8906203904e8c620d7eb5.svg", "fullname": "Zanlin Ni", "name": "nzl-thu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "69719700e3846c07669d13ee", "name": "Tsinghua-LeapLab", "fullname": "Tsinghua-LeapLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63987ffb2ceb55aabe0852f3/hflTWNTGxeJx83xNkYrDB.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.14724", "authors": [{"_id": "6972ee7afb12c92b735b74b4", "user": {"_id": "637169557a5e5d8efdc3e58e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg", "isPro": false, "fullname": "Haowei Zhang", "user": "freesky", "type": "user"}, "name": "Haowei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:23.639Z", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b5", "name": "Shudong Yang", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b6", "name": "Jinlan Fu", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b7", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b8", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-01-21T07:26:15.000Z", "submittedOnDailyAt": "2026-01-23T01:43:37.582Z", "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding", "submittedOnDailyBy": {"_id": "637169557a5e5d8efdc3e58e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg", "isPro": false, "fullname": "Haowei Zhang", "user": "freesky", "type": "user"}, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10times faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.", "upvotes": 52, "discussionId": "6972ee7bfb12c92b735b74b9", "projectPage": "https://hermes-streaming.github.io/", "githubRepo": "https://github.com/haowei-freesky/HERMES", "githubRepoAddedBy": "user", "ai_summary": "HERMES is a training-free architecture that enables real-time video stream understanding by utilizing a hierarchical memory framework based on KV cache reuse, achieving faster response times and maintained accuracy even with reduced video token input.", "ai_keywords": ["Multimodal Large Language Models", "video understanding", "streaming video inputs", "real-time responses", "KV cache", "hierarchical memory framework", "mechanistic attention", "video tokens", "TTFT"], "githubStars": 24, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\uff0c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u79bb\u7ebf\u89c6\u9891\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002</li>\n    <li>\u5c06\u8fd9\u4e9b\u80fd\u529b\u6269\u5c55\u5230\u6d41\u5a92\u4f53\u89c6\u9891\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u6a21\u578b\u5728\u7406\u89e3\u6027\u80fd\u548c\u5b9e\u65f6\u54cd\u5e94\u65b9\u9762\u5b58\u5728\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HERMES\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u8bad\u7ec3\u67b6\u6784\uff0c\u53ef\u4ee5\u5b9e\u65f6\u51c6\u786e\u5730\u7406\u89e3\u89c6\u9891\u6d41\u3002</li>\n    <li>HERMES\u5229\u7528\u7d27\u51d1\u7684KV\u7f13\u5b58\uff0c\u80fd\u591f\u5728\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u9ad8\u6548\u5904\u7406\u6d41\u5a92\u4f53\u89c6\u9891\u3002</li>\n    <li>HERMES\u5728\u7528\u6237\u67e5\u8be2\u5230\u8fbe\u65f6\u65e0\u9700\u989d\u5916\u8ba1\u7b97\uff0c\u4ece\u800c\u786e\u4fdd\u5b9e\u65f6\u54cd\u5e94\uff0c\u5e76\u5728\u6d41\u5a92\u4f53\u6570\u636e\u96c6\u4e0a\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>HERMES is a new system designed to understand video streams in real-time without needing extra training.</    <li>It uses a special memory system called KV cache to manage video information efficiently.</li>\n    <li>HERMES can respond quickly to user queries without needing extra calculations, making it suitable for live video.</li>\n    <li>It is 10 times faster than previous models in terms of time to first response (TTFT).</li>\n    <li>HERMES also reduces video data usage by up to 68% while maintaining or improving accuracy on various tests.</li>\n</ul>"}, "publishedAt": "2026-01-21T02:26:15.000Z", "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10times faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14724.png", "numComments": 2, "submittedBy": {"_id": "637169557a5e5d8efdc3e58e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg", "fullname": "Haowei Zhang", "name": "freesky", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.15197", "authors": [{"_id": "6971c608c1c7409747bf96a5", "user": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "isPro": false, "fullname": "Shijie Lian", "user": "LiamLian0727", "type": "user"}, "name": "Shijie Lian", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:44:25.547Z", "hidden": false}, {"_id": "6971c608c1c7409747bf96a6", "name": "Bin Yu", "hidden": false}, {"_id": "6971c608c1c7409747bf96a7", "name": "Xiaopeng Lin", "hidden": false}, {"_id": "6971c608c1c7409747bf96a8", "name": "Laurence T. Yang", "hidden": false}, {"_id": "6971c608c1c7409747bf96a9", "name": "Zhaolong Shen", "hidden": false}, {"_id": "6971c608c1c7409747bf96aa", "name": "Changti Wu", "hidden": false}, {"_id": "6971c608c1c7409747bf96ab", "name": "Yuzhuo Miao", "hidden": false}, {"_id": "6971c608c1c7409747bf96ac", "name": "Cong Huang", "hidden": false}, {"_id": "6971c608c1c7409747bf96ad", "name": "Kai Chen", "hidden": false}], "publishedAt": "2026-01-21T17:15:22.000Z", "submittedOnDailyAt": "2026-01-23T00:45:20.588Z", "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries", "submittedOnDailyBy": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "isPro": false, "fullname": "Shijie Lian", "user": "LiamLian0727", "type": "user"}, "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior \u03c0(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.", "upvotes": 50, "discussionId": "6971c609c1c7409747bf96ae", "projectPage": "https://github.com/ZGC-EmbodyAI/BayesianVLA", "githubRepo": "https://github.com/ZGC-EmbodyAI/BayesianVLA", "githubRepoAddedBy": "user", "ai_summary": "BayesianVLA addresses language-action grounding issues in robot manipulation by using Bayesian decomposition to prevent information collapse and improve out-of-distribution generalization.", "ai_keywords": ["Vision-Language-Action models", "Information Collapse", "Bayesian decomposition", "latent action queries", "conditional Pointwise Mutual Information", "vision-only policies", "out-of-distribution generalization", "SimplerEnv", "RoboCasa"], "githubStars": 11, "organization": {"_id": "68896d3a716ee5bfb1428441", "name": "ZGCA", "fullname": "Zhongguancun Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\uff08VLA\uff09\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u65b0\u6307\u4ee4\u6216\u590d\u6742\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u5e38\u5e38\u65e0\u6cd5\u6cdb\u5316\u3002</li>\n    <li>\u5f53\u524d\u8bad\u7ec3\u65b9\u6cd5\u5bfc\u81f4\u6570\u636e\u96c6\u504f\u89c1\uff0c\u4f7f\u5f97\u8bed\u8a00\u6307\u4ee4\u53ef\u4ee5\u4ec5\u901a\u8fc7\u89c6\u89c9\u89c2\u5bdf\u6765\u9884\u6d4b\uff0c\u4ece\u800c\u5bfc\u81f4\u4fe1\u606f\u5d29\u6e83\u73b0\u8c61\u3002</li>\n    <li>\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86BayesianVLA\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u5206\u89e3\u6765\u5f3a\u5236\u9075\u5faa\u6307\u4ee4\u3002</li>\n    <li>\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u6f5c\u5728\u52a8\u4f5c\u67e5\u8be2\uff0c\u6784\u5efa\u53cc\u5206\u652f\u7ed3\u6784\u4ee5\u4f30\u8ba1\u89c6\u89c9\u4f18\u5148\u548c\u8bed\u8a00\u6761\u4ef6\u7684\u540e\u9a8c\u3002</li>\n    <li>BayesianVLA\u5728\u4e0d\u9700\u8981\u65b0\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u5404\u7c7b\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-Language-Action (VLA) models help robots understand and follow instructions but often fail with new tasks or complex scenarios.</li>\n    <li>Current training methods create biased datasets, leading to a problem called Information Collapse where models rely too much on visual data and ignore language.</li>\n    <li>To solve this, the authors propose a new method called BayesianVLA that uses a dual-branch system to better connect language instructions and actions.</li>\n    <li>BayesianVLA improves models by maximizing the connection between actions and language instructions, reducing reliance on visual data alone.</li>\n    <li>Tests on different environments show that BayesianVLA significantly enhances the models' ability to follow instructions, especially in challenging situations.</li>\n</ul>"}, "publishedAt": "2026-01-21T12:15:22.000Z", "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries", "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior \u03c0(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15197.png", "numComments": 2, "submittedBy": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "fullname": "Shijie Lian", "name": "LiamLian0727", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "68896d3a716ee5bfb1428441", "name": "ZGCA", "fullname": "Zhongguancun Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16206", "authors": [{"_id": "6972e04dfb12c92b735b73cf", "user": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "isPro": false, "fullname": "Daixuan Cheng", "user": "daixuancheng", "type": "user"}, "name": "Daixuan Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:28.070Z", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d0", "name": "Shaohan Huang", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d1", "name": "Yuxian Gu", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d2", "name": "Huatong Song", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d3", "name": "Guoxin Chen", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d4", "name": "Li Dong", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d5", "name": "Wayne Xin Zhao", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d6", "name": "Ji-Rong Wen", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d7", "name": "Furu Wei", "hidden": false}], "publishedAt": "2026-01-22T18:57:09.000Z", "submittedOnDailyAt": "2026-01-23T00:27:12.305Z", "title": "LLM-in-Sandbox Elicits General Agentic Intelligence", "submittedOnDailyBy": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "isPro": false, "fullname": "Daixuan Cheng", "user": "daixuancheng", "type": "user"}, "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.", "upvotes": 46, "discussionId": "6972e04dfb12c92b735b73d8", "projectPage": "https://llm-in-sandbox.github.io", "githubRepo": "https://github.com/llm-in-sandbox/llm-in-sandbox", "githubRepoAddedBy": "user", "ai_summary": "LLM-in-Sandbox enables large language models to perform general intelligence tasks across diverse domains by allowing them to explore a code sandbox environment, achieving robust generalization without additional training.", "ai_keywords": ["LLM-in-Sandbox", "code sandbox", "virtual computer", "reinforcement learning", "non-agentic data", "sandbox exploration", "general intelligence", "long-context understanding", "instruction following"], "githubStars": 38, "organization": {"_id": "68151d0f51add3813f3f7d1b", "name": "MicrosoftResearch", "fullname": "Microsoft Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86LLM-in-Sandbox\uff0c\u5141\u8bb8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4ee3\u7801\u6c99\u7bb1\u4e2d\u63a2\u7d22\uff0c\u4ee5\u5728\u975e\u4ee3\u7801\u9886\u57df\u5c55\u73b0\u901a\u7528\u667a\u80fd\u3002</li>\n    <li>\u5f3a\u5927\u7684LLM\u80fd\u591f\u81ea\u53d1\u5229\u7528\u4ee3\u7801\u6c99\u7bb1\u5b8c\u6210\u975e\u4ee3\u7801\u4efb\u52a1\uff0c\u4f8b\u5982\u83b7\u53d6\u65b0\u77e5\u8bc6\u3001\u5904\u7406\u957f\u6587\u672c\u548c\u6267\u884c\u811a\u672c\u3002</li>\n    <li>\u901a\u8fc7LLM-in-Sandbox\u5f3a\u5316\u5b66\u4e60\uff08LLM-in-Sandbox-RL\uff09\uff0c\u53ef\u4ee5\u589e\u5f3a\u8fd9\u4e9b\u667a\u80fd\u80fd\u529b\uff0c\u53ea\u4f7f\u7528\u975e\u667a\u80fd\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cLLM-in-Sandbox\u5728\u6570\u5b66\u3001\u7269\u7406\u3001\u5316\u5b66\u3001\u751f\u7269\u533b\u5b66\u7b49\u9886\u57df\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u901a\u7528\u5316\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u5206\u6790\u4e86LLM-in-Sandbox\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u5c06\u5176\u5f00\u6e90\u4e3aPython\u5305\uff0c\u4ee5\u4fbf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4f7f\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LLM-in-Sandbox lets language models (LLMs) work in a virtual environment to improve their abilities in non-code areas.</li>\n    <li>Strong LLMs can use this sandbox without extra training to handle various tasks, like finding information and managing long texts.</li>\n    <li>These models can be made even better with a training method called LLM-in-Sandbox Reinforcement Learning, which focuses on exploring the sandbox.</li>\n    <li>Tests show that LLM-in-Sandbox is effective in many areas, including math, science, and understanding instructions.</li>\n    <li>The system's efficiency has been studied, and it is available as an open-source Python package for practical use.</li>\n</ul>"}, "publishedAt": "2026-01-22T13:57:09.000Z", "title": "LLM-in-Sandbox Elicits General Agentic Intelligence", "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16206.png", "numComments": 2, "submittedBy": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "fullname": "Daixuan Cheng", "name": "daixuancheng", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 16, "isUserFollowing": false}, "organization": {"_id": "68151d0f51add3813f3f7d1b", "name": "MicrosoftResearch", "fullname": "Microsoft Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.12346", "authors": [{"_id": "697145b5c1c7409747bf94c7", "name": "Peizhou Huang", "hidden": false}, {"_id": "697145b5c1c7409747bf94c8", "name": "Zixuan Zhong", "hidden": false}, {"_id": "697145b5c1c7409747bf94c9", "name": "Zhongwei Wan", "hidden": false}, {"_id": "697145b5c1c7409747bf94ca", "user": {"_id": "67136093d2e50f1e8c9fad52", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0q49MyGuav8lJ9CIeyLhu.png", "isPro": false, "fullname": "Donghao Zhou", "user": "donghao-zhou", "type": "user"}, "name": "Donghao Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:36.605Z", "hidden": false}, {"_id": "697145b5c1c7409747bf94cb", "name": "Samiul Alam", "hidden": false}, {"_id": "697145b5c1c7409747bf94cc", "name": "Xin Wang", "hidden": false}, {"_id": "697145b5c1c7409747bf94cd", "name": "Zexin Li", "hidden": false}, {"_id": "697145b5c1c7409747bf94ce", "name": "Zhihao Dou", "hidden": false}, {"_id": "697145b5c1c7409747bf94cf", "name": "Li Zhu", "hidden": false}, {"_id": "697145b5c1c7409747bf94d0", "name": "Jing Xiong", "hidden": false}, {"_id": "697145b5c1c7409747bf94d1", "name": "Chaofan Tao", "hidden": false}, {"_id": "697145b5c1c7409747bf94d2", "name": "Yan Xu", "hidden": false}, {"_id": "697145b5c1c7409747bf94d3", "name": "Dimitrios Dimitriadis", "hidden": false}, {"_id": "697145b5c1c7409747bf94d4", "name": "Tuo Zhang", "hidden": false}, {"_id": "697145b5c1c7409747bf94d5", "name": "Mi Zhang", "hidden": false}], "publishedAt": "2026-01-18T10:41:33.000Z", "submittedOnDailyAt": "2026-01-22T02:19:13.211Z", "title": "MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents", "submittedOnDailyBy": {"_id": "67136093d2e50f1e8c9fad52", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0q49MyGuav8lJ9CIeyLhu.png", "isPro": false, "fullname": "Donghao Zhou", "user": "donghao-zhou", "type": "user"}, "summary": "Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.", "upvotes": 41, "discussionId": "697145b5c1c7409747bf94d6", "projectPage": "https://mmdeepresearch-bench.github.io/", "githubRepo": "https://github.com/AIoT-MLSys-Lab/MMDeepResearch-Bench", "githubRepoAddedBy": "user", "ai_summary": "MMDeepResearch-Bench evaluates multimodal research agents on report generation with visual evidence, revealing trade-offs between prose quality, citation accuracy, and visual grounding.", "ai_keywords": ["multimodal evidence use", "citation-grounded report generation", "multimodal understanding", "deep research agents", "Formula-LLM Adaptive Evaluation", "Trustworthy Retrieval-Aligned Citation Evaluation", "Multimodal Support-Aligned Integrity Check"], "githubStars": 10, "summary_zh": "<ul>\n    <li>\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff08DRA\uff09\u901a\u8fc7\u591a\u6b65\u9aa4\u641c\u7d22\u548c\u7efc\u5408\u751f\u6210\u5f15\u7528\u4e30\u5bcc\u7684\u62a5\u544a\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u96c6\u4e2d\u5728\u7eaf\u6587\u672c\u8bbe\u7f6e\u6216\u77ed\u683c\u5f0f\u7684\u591a\u6a21\u6001\u95ee\u7b54\u4e0a\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86MMDeepResearch-Bench\uff08MMDR-Bench\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u6db5\u76d621\u4e2a\u9886\u57df\u3001\u5305\u542b140\u4e2a\u4e13\u5bb6\u8bbe\u8ba1\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6bcf\u4e2a\u4efb\u52a1\u63d0\u4f9b\u56fe\u50cf-\u6587\u672c\u7ec4\u5408\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u7406\u89e3\u548c\u57fa\u4e8e\u5f15\u7528\u7684\u62a5\u544a\u751f\u6210\u3002</li>\n    <li>MMDR-Bench\u5f3a\u8c03\u62a5\u544a\u5f0f\u7efc\u5408\uff0c\u8981\u6c42\u6a21\u578b\u5c06\u89c6\u89c9\u6750\u6599\u4e0e\u6765\u6e90\u58f0\u660e\u8fde\u63a5\uff0c\u5e76\u5728\u53d9\u8ff0\u3001\u5f15\u7528\u548c\u89c6\u89c9\u53c2\u8003\u4e4b\u95f4\u4fdd\u6301\u4e00\u81f4\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u53ef\u89e3\u91ca\u8bc4\u4ef7\u6d41\u7a0b\uff0c\u5305\u62ec\u62a5\u544a\u8d28\u91cf\u8bc4\u4f30\u7684Formula-LLM\u81ea\u9002\u5e94\u8bc4\u4f30\uff08FLAE\uff09\u3001\u57fa\u4e8e\u5f15\u7528\u7684\u8bc1\u636e\u5bf9\u9f50\u7684\u53ef\u4fe1\u68c0\u7d22\u8bc4\u4f30\uff08TRACE\uff09\u548c\u6587\u672c-\u89c6\u89c9\u5b8c\u6574\u6027\u68c0\u67e5\u7684\u591a\u6a21\u6001\u652f\u6301\u4e00\u81f4\u6027\u68c0\u67e5\uff08MOSAIC\uff09\u3002</li>\n    <li>\u572825\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u7684\u5b9e\u9a8c\u4e2d\uff0c\u53d1\u73b0\u751f\u6210\u8d28\u91cf\u3001\u5f15\u7528\u51c6\u786e\u6027\u548c\u591a\u6a21\u6001\u57fa\u7840\u4e4b\u95f4\u5b58\u5728\u7cfb\u7edf\u6027\u6743\u8861\uff0c\u5f3a\u5927\u7684\u6587\u7b14\u5e76\u4e0d\u4fdd\u8bc1\u8bc1\u636e\u7684\u51c6\u786e\u4f7f\u7528\uff0c\u591a\u6a21\u6001\u5b8c\u6574\u6027\u4ecd\u662f\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u4e3b\u8981\u74f6\u9888\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Deep Research Agents (DRAs) create detailed reports using images and text but current benchmarks often overlook full multimodal evidence use.</li>\n  <li>MMDeepResearch-Bench (MMDR-Bench) is a new benchmark with 140 tasks in 21 areas, focusing on evaluating how well models generate reports using both text and images.</li>\n  <li>The benchmark encourages models to connect visual elements with their sources and maintain consistency in their reports.</li>\n  <li>A new evaluation system is proposed to assess different aspects of report quality and evidence use, providing detailed feedback rather than just a single score.</li>\n  <li>Tests showed that while some models produced high-quality writing, they often struggled with correctly using evidence from multiple sources.</li>\n</ul>"}, "publishedAt": "2026-01-18T05:41:33.000Z", "title": "MMDeepResearch-Bench: A Benchmark for Multimodal Deep Research Agents", "summary": "Deep Research Agents (DRAs) generate citation-rich reports via multi-step search and synthesis, yet existing benchmarks mainly target text-only settings or short-form multimodal QA, missing end-to-end multimodal evidence use. We introduce MMDeepResearch-Bench (MMDR-Bench), a benchmark of 140 expert-crafted tasks across 21 domains, where each task provides an image-text bundle to evaluate multimodal understanding and citation-grounded report generation. Compared to prior setups, MMDR-Bench emphasizes report-style synthesis with explicit evidence use, where models must connect visual artifacts to sourced claims and maintain consistency across narrative, citations, and visual references. We further propose a unified, interpretable evaluation pipeline: Formula-LLM Adaptive Evaluation (FLAE) for report quality, Trustworthy Retrieval-Aligned Citation Evaluation (TRACE) for citation-grounded evidence alignment, and Multimodal Support-Aligned Integrity Check (MOSAIC) for text-visual integrity, each producing fine-grained signals that support error diagnosis beyond a single overall score. Experiments across 25 state-of-the-art models reveal systematic trade-offs between generation quality, citation discipline, and multimodal grounding, highlighting that strong prose alone does not guarantee faithful evidence use and that multimodal integrity remains a key bottleneck for deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12346.png", "numComments": 1, "submittedBy": {"_id": "67136093d2e50f1e8c9fad52", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0q49MyGuav8lJ9CIeyLhu.png", "fullname": "Donghao Zhou", "name": "donghao-zhou", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16208", "authors": [{"_id": "6972ea8bfb12c92b735b74a8", "name": "Shengbang Tong", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74a9", "name": "Boyang Zheng", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74aa", "user": {"_id": "64249f76d476e4ad55665d59", "avatarUrl": "/avatars/a0fec7e423ffae944a874ca267b55c1f.svg", "isPro": false, "fullname": "Ziteng Wang", "user": "AustinWang0330", "type": "user"}, "name": "Ziteng Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:01.136Z", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ab", "name": "Bingda Tang", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ac", "name": "Nanye Ma", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ad", "user": {"_id": "626dc5105f7327906f0b2a4e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626dc5105f7327906f0b2a4e/QCSzuwYqsv8ozRnusVb-F.jpeg", "isPro": true, "fullname": "Ellis Brown", "user": "ellisbrown", "type": "user"}, "name": "Ellis Brown", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T20:15:10.637Z", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ae", "name": "Jihan Yang", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74af", "name": "Rob Fergus", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74b0", "name": "Yann LeCun", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74b1", "name": "Saining Xie", "hidden": false}], "publishedAt": "2026-01-22T18:58:16.000Z", "submittedOnDailyAt": "2026-01-23T00:57:17.761Z", "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "submittedOnDailyBy": {"_id": "6434226da4c9c55871a78052", "avatarUrl": "/avatars/3309832b3115bc6ad08ae1d10f43118b.svg", "isPro": false, "fullname": "BoYang Zheng", "user": "bytetriper", "type": "user"}, "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.", "upvotes": 40, "discussionId": "6972ea8cfb12c92b735b74b2", "projectPage": "https://rae-dit.github.io/scale-rae/", "githubRepo": "https://github.com/ZitengWangNYU/Scale-RAE", "githubRepoAddedBy": "user", "ai_summary": "Representation Autoencoders (RAEs) demonstrate superior performance over VAEs in large-scale text-to-image generation, showing improved stability, faster convergence, and better quality while enabling unified multimodal reasoning in shared representation spaces.", "ai_keywords": ["representation autoencoders", "diffusion modeling", "semantic latent spaces", "text-to-image generation", "frozen representation encoder", "SigLIP-2", "noise scheduling", "diffusion transformers", "pretraining", "finetuning", "catastrophic overfitting", "multimodal model", "shared representation space"], "githubStars": 58, "organization": {"_id": "662741612ada5b77e310d171", "name": "nyu-visionx", "fullname": "VISIONx @ NYU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"}, "summary_zh": "<ul>\n    <li>RAEs\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7ef4\u8bed\u4e49\u6f5c\u5728\u7a7a\u95f4\u4e2d\u3002</li>\n    <li>\u901a\u8fc7\u4f7f\u7528\u7f51\u7edc\u3001\u5408\u6210\u548c\u6587\u672c\u6e32\u67d3\u6570\u636e\uff0c\u5bf9RAE\u89e3\u7801\u5668\u8fdb\u884c\u6269\u5c55\uff0c\u53d1\u73b0\u7279\u5b9a\u9886\u57df\uff08\u5982\u6587\u672c\uff09\u9700\u8981\u7279\u5b9a\u7684\u6570\u636e\u7ec4\u5408\u3002</li>\n    <li>RAE\u8bbe\u8ba1\u7684\u7b80\u5316\u4f7f\u5f97\u67b6\u6784\u66f4\u5bb9\u6613\u4f18\u5316\uff0c\u5c3d\u7ba1\u566a\u58f0\u8c03\u5ea6\u4ecd\u7136\u91cd\u8981\uff0c\u4f46\u590d\u6742\u7684\u67b6\u6784\u5728\u5927\u89c4\u6a21\u4e0b\u7684\u6536\u76ca\u6709\u9650\u3002</li>\n    <li>RAE\u5728\u6240\u6709\u6a21\u578b\u89c4\u6a21\u4e0a\u90fd\u4f18\u4e8eVAE\uff0c\u7279\u522b\u662f\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u4e14\u5728\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u66f4\u7a33\u5b9a\u3002</li>\n    <li>RAE\u7684\u6269\u6563\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u751f\u6210\u8d28\u91cf\uff0c\u663e\u793a\u51fa\u5176\u4f5c\u4e3a\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u66f4\u5f3a\u57fa\u7840\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Representation Autoencoders (RAEs) are effective for generating images from text by using high-dimensional spaces.</li>\n  <li>The study tests RAEs on larger datasets beyond ImageNet, finding that while larger models improve quality, the type of training data matters for specific tasks like text-to-image generation.</li>\n  <li>RAEs were compared to a state-of-the-art model (FLUX VAE) and performed better, especially during training and fine-tuning, without overfitting.</li>\n  <li>RAEs showed faster training and better image quality than VAEs across all tested model sizes.</li>\n  <li>The research suggests that RAEs can be a stronger and simpler choice for large-scale text-to-image generation, allowing for new multimodal applications.</li>\n</ul>"}, "publishedAt": "2026-01-22T13:58:16.000Z", "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16208.png", "numComments": 1, "submittedBy": {"_id": "6434226da4c9c55871a78052", "avatarUrl": "/avatars/3309832b3115bc6ad08ae1d10f43118b.svg", "fullname": "BoYang Zheng", "name": "bytetriper", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "662741612ada5b77e310d171", "name": "nyu-visionx", "fullname": "VISIONx @ NYU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.15892", "authors": [{"_id": "6972d788fb12c92b735b7397", "user": {"_id": "641aa5e391e3376a057bbd4c", "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg", "isPro": false, "fullname": "Chenghao Fan", "user": "Facico", "type": "user"}, "name": "Chenghao Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:32.109Z", "hidden": false}, {"_id": "6972d788fb12c92b735b7398", "name": "Wen Heng", "hidden": false}, {"_id": "6972d788fb12c92b735b7399", "name": "Bo Li", "hidden": false}, {"_id": "6972d788fb12c92b735b739a", "name": "Sichen Liu", "hidden": false}, {"_id": "6972d788fb12c92b735b739b", "name": "Yuxuan Song", "hidden": false}, {"_id": "6972d788fb12c92b735b739c", "name": "Jing Su", "hidden": false}, {"_id": "6972d788fb12c92b735b739d", "name": "Xiaoye Qu", "hidden": false}, {"_id": "6972d788fb12c92b735b739e", "name": "Kai Shen", "hidden": false}, {"_id": "6972d788fb12c92b735b739f", "name": "Wei Wei", "hidden": false}], "publishedAt": "2026-01-22T12:13:17.000Z", "submittedOnDailyAt": "2026-01-23T00:09:35.389Z", "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "submittedOnDailyBy": {"_id": "641aa5e391e3376a057bbd4c", "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg", "isPro": false, "fullname": "Chenghao Fan", "user": "Facico", "type": "user"}, "summary": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.", "upvotes": 40, "discussionId": "6972d788fb12c92b735b73a0", "projectPage": "https://bytedance-seed.github.io/Stable-DiffCoder/", "githubRepo": "https://github.com/ByteDance-Seed/Stable-DiffCoder", "githubRepoAddedBy": "user", "ai_summary": "Stable-DiffCoder demonstrates superior code modeling performance compared to autoregressive baselines through block diffusion continual pretraining and efficient training mechanisms.", "ai_keywords": ["diffusion-based language models", "autoregressive models", "block diffusion", "continual pretraining", "warmup", "clipped noise schedule", "supervised fine-tuning", "code modeling", "structured code modeling", "data augmentation"], "githubStars": 16, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08DLLMs\uff09\u80fd\u591f\u5757\u72b6\u751f\u6210\u6570\u636e\uff0c\u5e76\u4e14\u6bd4\u81ea\u56de\u5f52\uff08AR\uff09\u6a21\u578b\u66f4\u597d\u5730\u91cd\u7528\u6570\u636e\u3002</li>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86Stable-DiffCoder\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u4ee3\u7801\u6a21\u578b\uff0c\u91c7\u7528Seed-Coder\u67b6\u6784\u548c\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>\u4e3a\u4e86\u63d0\u9ad8\u77e5\u8bc6\u5b66\u4e60\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u6211\u4eec\u52a0\u5165\u4e86\u5757\u6269\u6563\u6301\u7eed\u9884\u8bad\u7ec3\uff08CPT\uff09\u9636\u6bb5\uff0c\u5e76\u4f18\u5316\u4e86\u9884\u70ed\u548c\u566a\u58f0\u8c03\u5ea6\u3002</li>\n    <li>Stable-DiffCoder\u5728\u591a\u9879\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6574\u4f53\u8868\u73b0\u4f18\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\u3002</li>\n    <li>\u901a\u8fc7\u6269\u6563\u8bad\u7ec3\uff0cStable-DiffCoder\u5728\u4ee3\u7801\u5efa\u6a21\u8d28\u91cf\u4e0a\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u81ea\u56de\u5f52\u8bad\u7ec3\uff0c\u7279\u522b\u662f\u5728\u7f16\u8f91\u548c\u63a8\u7406\u65b9\u9762\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion-based language models (DLLMs) can generate code in blocks rather than one by one, which allows for better data reuse.</li>\n    <li>Stable-DiffCoder is a new model that uses an existing architecture and improves training methods for better performance.</li>\n    <li>This model outperforms traditional autoregressive (AR) models in various coding tasks when using the same data and architecture.</li>\n    <li>Stable-DiffCoder shows better results than many AR and DLLM models, even with limited training stages.</li>\n    <li>Using diffusion methods helps improve code editing and reasoning, and aids low-resource programming languages through data enhancement.</li>\n</ul>"}, "publishedAt": "2026-01-22T07:13:17.000Z", "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "summary": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15892.png", "numComments": 0, "submittedBy": {"_id": "641aa5e391e3376a057bbd4c", "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg", "fullname": "Chenghao Fan", "name": "Facico", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13, "isUserFollowing": false}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u5728\u89c6\u9891\u95ee\u7b54\u4e2d\uff0c\u89c6\u9891\u5e38\u5e38\u53ea\u63d0\u4f9b\u5c40\u90e8\u89c6\u89c9\u7ebf\u7d22\uff0c\u800c\u7b54\u6848\u5206\u5e03\u5728\u7f51\u4e0a\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0c\u540d\u4e3aVideoDR\uff0c\u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u4e0b\u7684\u5f00\u653e\u57df\u95ee\u7b54\u3002</li>\n    <li>VideoDR\u8981\u6c42\u63d0\u53d6\u8de8\u5e27\u89c6\u89c9\u951a\u70b9\u3001\u8fdb\u884c\u7f51\u7edc\u4e92\u52a8\u68c0\u7d22\u548c\u591a\u8df3\u63a8\u7406\u3002</li>\n    <li>\u8bc4\u4f30\u591a\u79cd\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u679c\u663e\u793aAgentic\u65b9\u6cd5\u5e76\u4e0d\u603b\u662f\u4f18\u4e8eWorkflow\u65b9\u6cd5\u3002</li>\n    <li>\u5206\u6790\u8868\u660e\uff0c\u76ee\u6807\u6f02\u79fb\u548c\u957f\u65f6\u95f4\u4e00\u81f4\u6027\u662f\u4e3b\u8981\u74f6\u9888\uff0cVideoDR\u4e3a\u7814\u7a76\u4e0b\u4e00\u4ee3\u89c6\u9891\u4ee3\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u57fa\u51c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video question answering often relies on visual clues from videos, while answers can be found on the internet.</li>\n    <li>To improve this process, a new benchmark called VideoDR has been created for video-based question answering.</li>\n    <li>VideoDR requires models to extract visual information from videos, retrieve information from the web, and reason through multiple pieces of evidence.</li>\n    <li>Tests on various language models show that their success varies based on how well they track information over long retrieval tasks.</li>\n    <li>VideoDR highlights challenges like maintaining focus and consistency during long searches, which are important for future video research agents.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u73b0\u4ee3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u7406\u89e3\u4e0a\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\uff0c\u8868\u73b0\u4e0d\u4f73\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86BabyVision\u57fa\u51c6\uff0c\u4e13\u95e8\u8bc4\u4f30\u4e0d\u4f9d\u8d56\u8bed\u8a00\u7684\u6838\u5fc3\u89c6\u89c9\u80fd\u529b\u3002</li>\n    <li>BabyVision\u5305\u542b388\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\u548c\u56db\u4e2a\u4e3b\u8981\u7c7b\u522b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709MLLMs\u7684\u8868\u73b0\u660e\u663e\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c6\uff0c\u7279\u522b\u662fGemini3-Pro-Preview\u7684\u5f97\u5206\u8fdc\u4f4e\u4e8e\u6210\u5e74\u4eba\u7684\u5e73\u5747\u5206\u3002</li>\n    <li>BabyVision\u7684\u8fdb\u5c55\u4e3a\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7684\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans develop visual skills early, while Multimodal LLMs (MLLMs) depend on language to understand visuals.</li>\n    <li>State-of-the-art MLLMs struggle with basic visual tasks that even toddlers can do easily.</li>\n    <li>BabyVision is a new benchmark created to test MLLMs' visual abilities without using language.</li>\n    <li>The benchmark includes 388 tasks across 22 classes and shows MLLMs score much lower than humans.</li>\n    <li>The study aims to improve MLLMs' visual perception and proposes new methods for visual reasoning.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u4e30\u5bcc\u7684\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\uff0c\u91cd\u8981\u4e8e\u4ece\u536b\u661f\u56fe\u50cf\u4e2d\u8fdb\u884c\u5206\u5272\u3002</li>\n    <li>\u73b0\u6709\u7684\u5206\u5272\u6a21\u578b\u5bf9\u7269\u7406\u5c5e\u6027\u7684\u5b9e\u4f53\u5206\u5272\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\uff08\u5982\u5b66\u6821\u3001\u516c\u56ed\uff09\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Urban Socio-Semantic Segmentation\u6570\u636e\u96c6SocioSeg\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u793e\u4ea4\u8bed\u4e49\u5b9e\u4f53\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u6846\u67b6SocioReasoner\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u8bc6\u522b\u548c\u591a\u9636\u6bb5\u63a8\u7406\u6765\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u6ce8\u91ca\u7684\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u4e14\u5177\u6709\u5f88\u5f3a\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban areas have many important features that need to be identified from satellite images.</li>\n    <li>Current models can find physical features like buildings but struggle with social features like schools and parks.</li>\n    <li>The researchers created a new dataset called SocioSeg, which includes satellite images, maps, and labels for social features.</li>\n    <li>They developed a framework called SocioReasoner that helps models understand and label these social features more effectively.</li>\n    <li>Tests show that their method works better than existing models and can generalize well to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u5e73\u8861\u7d27\u51d1\u6548\u7387\u548c\u591a\u6a21\u6001\u667a\u80fd\u3002</li>\n    <li>\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7ba1\u9053\u5b9e\u73b0\uff0c\u7ed3\u5408\u4e86\u8bed\u8a00\u5bf9\u9f50\u7684\u611f\u77e5\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u3002</li>\n    <li>\u91c7\u7528\u5e73\u884c\u534f\u540c\u63a8\u7406\uff08PaCoRe\uff09\u6765\u4f18\u5316\u6d4b\u8bd5\u65f6\u7684\u8ba1\u7b97\uff0c\u63d0\u5347\u611f\u77e5\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5c3d\u7ba1\u6a21\u578b\u4f53\u79ef\u5c0f\uff08\u4ec510B\uff09\uff0c\u4f46\u5176\u6027\u80fd\u53ef\u5ab2\u7f8e\u6216\u8d85\u8fc710\u523020\u500d\u66f4\u5927\u7684\u6a21\u578b\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5982MMBench\u5f97\u520692.2%\uff0cAIME2025\u5f97\u520694.43%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a new lightweight open-source model that balances efficiency with advanced multimodal intelligence.</li>\n    <li>It uses a unique training approach with a large dataset of 1.2 trillion multimodal tokens to improve vision and language understanding.</li>\n    <li>The model includes advanced techniques like reinforcement learning and Parallel Coordinated Reasoning (PaCoRe) to enhance its reasoning capabilities.</li>\n    <li>Despite being smaller (10 billion parameters), it competes with much larger models and shows top performance on various benchmarks.</li>\n    <li>The complete model is available to the community for use and further development.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u7684\u76ee\u6807\u662f\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\uff0c\u5229\u7528\u89c6\u89c9\u7ebf\u7d22\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5ffd\u7565\u4e86\u4eba\u7c7b\u5e38\u7528\u7684\u7b56\u7565\u2014\u2014\u4f7f\u7528\u5730\u56fe\u3002</li>\n    <li>\u672c\u7814\u7a76\u4e3a\u6a21\u578b\u589e\u52a0\u4e86\u201c\u601d\u8003\u5730\u56fe\u201d\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u8bbe\u8ba1\u4e3a\u5730\u56fe\u4e2d\u7684\u4ee3\u7406\u5faa\u73af\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u51c6\u786e\u7387\u4ece8.0%\u63d0\u9ad8\u523022.1%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The goal of image geolocalization is to identify where a photo was taken anywhere on Earth using visual information.</li>\n    <li>Current models often ignore the human strategy of using maps, which this study addresses by adding a \"Thinking with Map\" ability.</li>\n    <li>The new approach involves two steps: using reinforcement learning to enhance the model's decision-making and parallel testing to explore different prediction paths.</li>\n    <li>A new benchmark called MAPBench is introduced to test the model with real-world images.</li>\n    <li>Results show this method significantly improves accuracy in geolocalization compared to existing models.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.12538", "authors": [{"_id": "6971913fc1c7409747bf9564", "name": "Tianxin Wei", "hidden": false}, {"_id": "6971913fc1c7409747bf9565", "user": {"_id": "6742eb40924e80c3c80ebe13", "avatarUrl": "/avatars/e6ccb1a89a1ea0bfca70779966f4f429.svg", "isPro": false, "fullname": "Ting-Wei Li", "user": "tingwl0122", "type": "user"}, "name": "Ting-Wei Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T17:12:21.531Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9566", "name": "Zhining Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf9567", "name": "Xuying Ning", "hidden": false}, {"_id": "6971913fc1c7409747bf9568", "name": "Ze Yang", "hidden": false}, {"_id": "6971913fc1c7409747bf9569", "name": "Jiaru Zou", "hidden": false}, {"_id": "6971913fc1c7409747bf956a", "name": "Zhichen Zeng", "hidden": false}, {"_id": "6971913fc1c7409747bf956b", "name": "Ruizhong Qiu", "hidden": false}, {"_id": "6971913fc1c7409747bf956c", "name": "Xiao Lin", "hidden": false}, {"_id": "6971913fc1c7409747bf956d", "name": "Dongqi Fu", "hidden": false}, {"_id": "6971913fc1c7409747bf956e", "name": "Zihao Li", "hidden": false}, {"_id": "6971913fc1c7409747bf956f", "user": {"_id": "653962e75c8e4863e1a2068f", "avatarUrl": "/avatars/d4f5f5da141f37d53ca1986ff17b325e.svg", "isPro": false, "fullname": "Mengting Ai", "user": "famous-blue-raincoat", "type": "user"}, "name": "Mengting Ai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:10.378Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9570", "user": {"_id": "677830bd3f2e3ec475576303", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dhwqUDkk66m4oSGSbcd7j.png", "isPro": false, "fullname": "Duo Zhou", "user": "Claudius7", "type": "user"}, "name": "Duo Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:12.476Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9571", "name": "Wenxuan Bao", "hidden": false}, {"_id": "6971913fc1c7409747bf9572", "user": {"_id": "646323556c27a7e33b23f198", "avatarUrl": "/avatars/17fe142f689ab4be3c2374d1d90393db.svg", "isPro": false, "fullname": "Yunzhe Li", "user": "yunzhel2", "type": "user"}, "name": "Yunzhe Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:14.383Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9573", "name": "Gaotang Li", "hidden": false}, {"_id": "6971913fc1c7409747bf9574", "name": "Cheng Qian", "hidden": false}, {"_id": "6971913fc1c7409747bf9575", "name": "Yu Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf9576", "name": "Xiangru Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf9577", "name": "Yin Xiao", "hidden": false}, {"_id": "6971913fc1c7409747bf9578", "name": "Liri Fang", "hidden": false}, {"_id": "6971913fc1c7409747bf9579", "name": "Hui Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf957a", "name": "Xianfeng Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf957b", "name": "Yuji Zhang", "hidden": false}, {"_id": "6971913fc1c7409747bf957c", "name": "Chi Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf957d", "name": "Jiaxuan You", "hidden": false}, {"_id": "6971913fc1c7409747bf957e", "name": "Heng Ji", "hidden": false}, {"_id": "6971913fc1c7409747bf957f", "name": "Hanghang Tong", "hidden": false}, {"_id": "6971913fc1c7409747bf9580", "name": "Jingrui He", "hidden": false}], "publishedAt": "2026-01-18T18:58:23.000Z", "submittedOnDailyAt": "2026-01-22T00:27:25.162Z", "title": "Agentic Reasoning for Large Language Models", "submittedOnDailyBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "isPro": false, "fullname": "Jiaru Zou", "user": "jiaruz2", "type": "user"}, "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "upvotes": 125, "discussionId": "69719140c1c7409747bf9581", "githubRepo": "https://github.com/weitianxin/Awesome-Agentic-Reasoning", "githubRepoAddedBy": "user", "ai_summary": "Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.", "ai_keywords": ["large language models", "agentic reasoning", "autonomous agents", "planning", "tool use", "search", "feedback", "memory", "adaptation", "collaborative settings", "coordination", "knowledge sharing", "reinforcement learning", "supervised fine-tuning", "in-context reasoning", "post-training reasoning", "real-world applications", "benchmarks", "thought and action", "world modeling", "scalable multi-agent training", "governance"], "githubStars": 105, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "summary_zh": "<ul>\n    <li>\u63a8\u7406\u662f\u63a8\u65ad\u3001\u89e3\u51b3\u95ee\u9898\u548c\u51b3\u7b56\u7684\u57fa\u672c\u8ba4\u77e5\u8fc7\u7a0b\u3002</li>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5c01\u95ed\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5f00\u653e\u548c\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u81ea\u4e3b\u63a8\u7406\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u80fd\u591f\u901a\u8fc7\u6301\u7eed\u4e92\u52a8\u8fdb\u884c\u89c4\u5212\u3001\u884c\u52a8\u548c\u5b66\u4e60\u3002</li>\n    <li>\u81ea\u4e3b\u63a8\u7406\u5206\u4e3a\u4e09\u4e2a\u5c42\u9762\uff1a\u57fa\u7840\u80fd\u529b\u3001\u81ea\u6211\u8fdb\u5316\u80fd\u529b\u548c\u96c6\u4f53\u591a\u4ee3\u7406\u63a8\u7406\u3002</li>\n    <li>\u8be5\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u81ea\u4e3b\u63a8\u7406\u5728\u79d1\u5b66\u3001\u673a\u5668\u4eba\u3001\u533b\u7597\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6846\u67b6\u548c\u672a\u6765\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reasoning is key for tasks like problem-solving and decision-making, but large language models (LLMs) have difficulties in dynamic environments.</li>\n    <li>Agentic reasoning changes the view of LLMs to be autonomous agents that can plan, act, and learn from interactions.</li>\n    <li>The survey describes agentic reasoning in three layers: foundational (basic skills), self-evolving (improving through feedback), and collective (working with others).</li>\n    <li>It differentiates between in-context reasoning (real-time interaction) and post-training reasoning (improving after training).</li>\n    <li>The survey also highlights real-world applications and future challenges like personalization and collaboration for LLMs.</li>\n</ul>"}, "publishedAt": "2026-01-18T13:58:23.000Z", "title": "Agentic Reasoning for Large Language Models", "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12538.png", "numComments": 3, "submittedBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "fullname": "Jiaru Zou", "name": "jiaruz2", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.08763", "authors": [{"_id": "6969b0a232f0333869ff946a", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:38.232Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946b", "user": {"_id": "6891c906f3c31445cc040ab1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891c906f3c31445cc040ab1/NBqxXOY7al4CD0XBj8ke2.jpeg", "isPro": false, "fullname": "Yucheng Wang", "user": "DevilEnfant", "type": "user"}, "name": "Yucheng Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:48.080Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946c", "name": "Yufei He", "hidden": false}, {"_id": "6969b0a232f0333869ff946d", "user": {"_id": "682deb444988bd82847e2b03", "avatarUrl": "/avatars/15da087e84386ea72c6fa2db63571420.svg", "isPro": false, "fullname": "Jia-Ying Wu", "user": "EricaWu", "type": "user"}, "name": "Jiaying Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:59.692Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946e", "name": "Yilun Zhao", "hidden": false}, {"_id": "6969b0a232f0333869ff946f", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0a232f0333869ff9470", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:06.327Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9471", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:12.181Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9472", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:17.979Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9473", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:23.007Z", "hidden": false}], "publishedAt": "2026-01-13T17:48:43.000Z", "submittedOnDailyAt": "2026-01-16T01:00:36.686Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "upvotes": 111, "discussionId": "6969b0a232f0333869ff9474", "ai_summary": "Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.", "ai_keywords": ["reinforcement learning", "large language models", "exploration collapse", "pass@k", "pass@1", "rollout-level objective", "high-level solution strategies", "clustering", "policy advantages", "AUC@K"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u540e\u8bad\u7ec3\u4e2d\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u3002</li>\n    <li>\u76ee\u524d\u7684RL\u65b9\u6cd5\u5e38\u5e38\u5bfc\u81f4\u63a2\u7d22\u5d29\u6e83\uff0c\u7b56\u7565\u8fc7\u65e9\u96c6\u4e2d\u5728\u5c11\u6570\u4e3b\u5bfc\u7684\u63a8\u7406\u6a21\u5f0f\u4e0a\uff0c\u5bfc\u81f4\u591a\u6837\u6027\u4e0d\u8db3\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u2014\u2014\u72ec\u7279\u6027\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\uff0c\u4e13\u6ce8\u4e8e\u5956\u52b1\u5c55\u793a\u7a00\u6709\u9ad8\u5c42\u7b56\u7565\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u805a\u7c7b\u76f8\u540c\u95ee\u9898\u7684\u63a8\u7406\u7ed3\u679c\uff0c\u5ffd\u7565\u8868\u9762\u5dee\u5f02\uff0c\u91cd\u65b0\u8c03\u6574\u7b56\u7565\u4f18\u52bf\uff0c\u4ee5\u652f\u6301\u66f4\u591a\u521b\u65b0\u7684\u89e3\u51b3\u7b56\u7565\u3002</li>\n    <li>\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u533b\u5b66\u63a8\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e0d\u964d\u4f4e\u4f20\u7edf\u51c6\u786e\u7387\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u9ad8\u4e86\u591a\u6837\u6027\u548c\u6574\u4f53\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) is important for improving large language models (LLMs) in complex reasoning tasks.</li>\n    <li>Current RL methods often focus too much on a few common reasoning patterns, which limits the diversity of solutions.</li>\n    <li>We propose a new method called Uniqueness-Aware Reinforcement Learning that rewards unique and correct solutions using an LLM-based judge.</li>\n    <li>This method helps to identify and promote rare high-level strategies rather than just common ones.</li>\n    <li>Our approach improves performance in various subjects without losing accuracy and encourages more diverse solutions.</li>\n</ul>"}, "publishedAt": "2026-01-13T12:48:43.000Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08763.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 109, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 102, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u662f\u4e00\u79cd\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u7b56\u7565\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u8bbe\u8ba1\u901a\u5e38\u53ea\u662f\u88ab\u52a8\u5b58\u50a8\u4fe1\u606f\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u4fe1\u606f\u4e4b\u95f4\u7684\u9ad8\u9636\u5173\u8054\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HGMem\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u80fd\u591f\u52a8\u6001\u8868\u793a\u8bb0\u5fc6\u5e76\u652f\u6301\u590d\u6742\u63a8\u7406\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u56fe\u5c06\u4e8b\u5b9e\u548c\u601d\u7ef4\u8fde\u63a5\u8d77\u6765\uff0c\u5f62\u6210\u4e00\u4e2a\u66f4\u5f3a\u5927\u7684\u77e5\u8bc6\u7ed3\u6784\uff0c\u5e2e\u52a9\u66f4\u6df1\u5165\u7684\u63a8\u7406\u3002</li>\n    <li>\u5728\u591a\u4e2a\u5168\u7403\u7406\u89e3\u7684\u6311\u6218\u6570\u636e\u96c6\u4e0a\uff0cHGMem\u663e\u8457\u6539\u5584\u4e86\u591a\u6b65\u9aa4RAG\u7684\u8868\u73b0\uff0c\u8d85\u8d8a\u4e86\u5f3a\u57fa\u7ebf\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps large language models (LLMs) understand complex tasks better.</li>\n    <li>Current memory systems mainly store facts but do not effectively connect them for deeper reasoning.</li>\n    <li>HGMem is a new memory mechanism that uses a hypergraph to create dynamic connections between facts for enhanced reasoning.</li>\n    <li>This hypergraph structure allows for better organization of knowledge and helps in understanding complex problems.</li>\n    <li>Tests show that HGMem significantly improves performance in multi-step reasoning tasks compared to existing methods.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u63d0\u5347\uff0c\u7528\u6237\u5e0c\u671b\u5b83\u4eec\u63d0\u4f9b\u51c6\u786e\u7684\u56de\u7b54\u548c\u7b26\u5408\u591a\u6837\u5316\u4eba\u7c7b\u504f\u597d\u7684\u884c\u4e3a\u3002</li>\n    <li>\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5f00\u59cb\u4f7f\u7528\u591a\u79cd\u5956\u52b1\u6765\u5f15\u5bfc\u6a21\u578b\u671d\u5411\u671f\u671b\u7684\u884c\u4e3a\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u76f4\u63a5\u4f7f\u7528\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u5728\u591a\u5956\u52b1\u8bbe\u7f6e\u4e2d\u4f1a\u5bfc\u81f4\u5956\u52b1\u7ec4\u5408\u7684\u6b63\u5e38\u5316\u4e0d\u5f53\uff0c\u5f71\u54cd\u8bad\u7ec3\u6548\u679c\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u2014\u2014\u7fa4\u4f53\u5956\u52b1\u89e3\u8026\u6b63\u5e38\u5316\u7b56\u7565\u4f18\u5316\uff08GDPO\uff09\uff0c\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u5956\u52b1\u4e4b\u95f4\u7684\u76f8\u5bf9\u5dee\u5f02\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u5728\u5de5\u5177\u8c03\u7528\u3001\u6570\u5b66\u63a8\u7406\u548c\u7f16\u7801\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\uff0cGDPO\u7684\u8868\u73b0\u59cb\u7ec8\u4f18\u4e8eGRPO\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Users want language models to give accurate answers and behave according to various human preferences.</li>\n    <li>Reinforcement learning (RL) is using multiple rewards to guide models towards these desired behaviors.</li>\n    <li>Applying a method called Group Relative Policy Optimization (GRPO) can lead to problems, as it can make different rewards behave the same, reducing training effectiveness.</li>\n    <li>The paper introduces a new method, Group reward-Decoupled Normalization Policy Optimization (GDPO), which improves training by treating each reward separately and preserving their differences.</li>\n    <li>GDPO outperforms GRPO in different tasks, showing better accuracy and adherence to constraints in language model training.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.08521", "authors": [{"_id": "69674059c5e371f6b235d1d8", "user": {"_id": "68920f91bcf2b25e8e121cf6", "avatarUrl": "/avatars/4bc69f43828a346a3ee24b026e0edbb4.svg", "isPro": false, "fullname": "Fengkai Yang", "user": "ShortCatisLong", "type": "user"}, "name": "Fengkai Yang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-19T14:33:21.899Z", "hidden": false}, {"_id": "69674059c5e371f6b235d1d9", "user": {"_id": "6969715fb2636f5f23a9a8c5", "avatarUrl": "/avatars/5e75043891ee41bb980f71fb9e3a33ab.svg", "isPro": false, "fullname": "Zherui Chen", "user": "chenzherui007", "type": "user"}, "name": "Zherui Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:33:53.078Z", "hidden": false}, {"_id": "69674059c5e371f6b235d1da", "name": "Xiaohan Wang", "hidden": false}, {"_id": "69674059c5e371f6b235d1db", "name": "Xiaodong Lu", "hidden": false}, {"_id": "69674059c5e371f6b235d1dc", "user": {"_id": "666eb642a119281ee0bfa443", "avatarUrl": "/avatars/71317810b00978754ad439837b04faff.svg", "isPro": false, "fullname": "Jiajun Chai", "user": "PandaChai", "type": "user"}, "name": "Jiajun Chai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-19T14:37:17.404Z", "hidden": false}, {"_id": "69674059c5e371f6b235d1dd", "name": "Guojun Yin", "hidden": false}, {"_id": "69674059c5e371f6b235d1de", "name": "Wei Lin", "hidden": false}, {"_id": "69674059c5e371f6b235d1df", "name": "Shuai Ma", "hidden": false}, {"_id": "69674059c5e371f6b235d1e0", "name": "Fuzhen Zhuang", "hidden": false}, {"_id": "69674059c5e371f6b235d1e1", "name": "Deqing Wang", "hidden": false}, {"_id": "69674059c5e371f6b235d1e2", "name": "Yaodong Yang", "hidden": false}, {"_id": "69674059c5e371f6b235d1e3", "name": "Jianxin Li", "hidden": false}, {"_id": "69674059c5e371f6b235d1e4", "user": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "isPro": false, "fullname": "Yikun Ban", "user": "Yikunb", "type": "user"}, "name": "Yikun Ban", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:33:50.655Z", "hidden": false}], "publishedAt": "2026-01-13T13:03:15.000Z", "submittedOnDailyAt": "2026-01-19T00:20:58.837Z", "title": "Your Group-Relative Advantage Is Biased", "submittedOnDailyBy": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "isPro": false, "fullname": "Yikun Ban", "user": "Yikunb", "type": "user"}, "summary": "Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.\n  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.", "upvotes": 95, "discussionId": "6967405ac5e371f6b235d1e5", "ai_summary": "Group-based reinforcement learning from verifier rewards suffers from biased advantage estimation that underestimates hard prompts and overestimates easy prompts, which is addressed through a history-aware adaptive difficulty weighting method that improves performance on mathematical reasoning benchmarks.", "ai_keywords": ["Reinforcement Learning from Verifier Rewards", "group-based methods", "GRPO", "advantage estimation", "bias correction", "adaptive reweighting", "difficulty weighting", "mathematical reasoning", "benchmark evaluation"], "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u9a8c\u8bc1\u8005\u5956\u52b1\uff08RLVR\uff09\u7528\u4e8e\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee5\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u73b0\u6709\u7684\u57fa\u4e8e\u5c0f\u7ec4\u7684\u65b9\u6cd5\uff08\u5982GRPO\uff09\u5728\u4f30\u8ba1\u4f18\u52bf\u65f6\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\u3002</li>\n    <li>\u8fd9\u4e9b\u504f\u5dee\u5bfc\u81f4\u5bf9\u96be\u9898\u7684\u4f18\u52bf\u4f4e\u4f30\uff0c\u5bf9\u7b80\u5355\u95ee\u9898\u7684\u4f18\u52bf\u9ad8\u4f30\uff0c\u4ece\u800c\u5f71\u54cd\u63a2\u7d22\u548c\u5229\u7528\u7684\u5e73\u8861\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u5386\u53f2\u611f\u77e5\u81ea\u9002\u5e94\u96be\u5ea6\u52a0\u6743\uff08HA-DW\uff09\uff0c\u53ef\u4ee5\u6839\u636e\u8bad\u7ec3\u52a8\u6001\u8c03\u6574\u4f18\u52bf\u4f30\u8ba1\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cHA-DW\u80fd\u591f\u6709\u6548\u63d0\u9ad8GRPO\u53ca\u5176\u53d8\u4f53\u7684\u6027\u80fd\uff0c\u7ea0\u6b63\u504f\u5dee\u4f30\u8ba1\u5bf9\u4e8e\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement Learning from Verifier Rewards (RLVR) is popular for training large language models on reasoning tasks.</li>\n    <li>Group-based methods like GRPO estimate advantages based on group performance but have poorly understood theoretical properties.</li>\n    <li>These methods have a bias: they underestimate advantages for difficult prompts and overestimate for easy ones.</li>\n    <li>To fix this, a new method called History-Aware Adaptive Difficulty Weighting (HA-DW) is proposed to adjust advantage estimates based on prompt difficulty.</li>\n    <li>HA-DW improves performance in experiments on reasoning tasks and highlights the importance of accurate advantage estimation in RLVR training.</li>\n</ul>"}, "publishedAt": "2026-01-13T08:03:15.000Z", "title": "Your Group-Relative Advantage Is Biased", "summary": "Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.\n  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08521.png", "numComments": 5, "submittedBy": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "fullname": "Yikun Ban", "name": "Yikunb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Jan 25, 2026";
window.trendingPapers = {
    "today": [{"paper": {"id": "2512.23447", "authors": [{"_id": "69534a9589916ff627aa3f5c", "name": "Ang Lv", "hidden": false}, {"_id": "69534a9589916ff627aa3f5d", "name": "Jin Ma", "hidden": false}, {"_id": "69534a9589916ff627aa3f5e", "name": "Yiyuan Ma", "hidden": false}, {"_id": "69534a9589916ff627aa3f5f", "name": "Siyuan Qiao", "hidden": false}], "publishedAt": "2025-12-29T13:03:18.000Z", "submittedOnDailyAt": "2025-12-30T01:18:40.635Z", "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss", "submittedOnDailyBy": {"_id": "64b8ca3c5067873176d4b436", "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg", "isPro": false, "fullname": "AngLv", "user": "AngLv", "type": "user"}, "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.", "upvotes": 71, "discussionId": "69534a9589916ff627aa3f60", "ai_summary": "An expert-router coupling (ERC) loss aligns router decisions with expert capabilities in Mixture-of-Experts (MoE) models by enforcing constraints on internal activations, improving performance and computational efficiency.", "ai_keywords": ["Mixture-of-Experts (MoE)", "expert-router coupling (ERC) loss", "router embeddings", "proxy tokens", "internal activations", "MoE-LLMs", "expert specialization levels", "n\u00b2 activations"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\u7f3a\u4e4f\u660e\u786e\u7684\u7ea6\u675f\uff0c\u5bfc\u81f4\u8def\u7531\u5668\u7684\u51b3\u7b56\u4e0e\u4e13\u5bb6\u80fd\u529b\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e13\u5bb6-\u8def\u7531\u5668\u8026\u5408\uff08ERC\uff09\u635f\u5931\uff0c\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u8f85\u52a9\u635f\u5931\uff0c\u5c06\u8def\u7531\u5668\u7684\u51b3\u7b56\u4e0e\u4e13\u5bb6\u7684\u80fd\u529b\u7d27\u5bc6\u7ed3\u5408\u3002</li>\n    <li>ERC\u635f\u5931\u5bf9\u4e13\u5bb6\u7684\u5185\u90e8\u6fc0\u6d3b\u65bd\u52a0\u4e24\u4e2a\u7ea6\u675f\uff0c\u786e\u4fdd\u6bcf\u4e2a\u4e13\u5bb6\u5bf9\u81ea\u8eab\u4ee3\u7406token\u7684\u6fc0\u6d3b\u9ad8\u4e8e\u5176\u4ed6\u4e13\u5bb6\u7684\u4ee3\u7406token\u3002</li>\n    <li>ERC\u635f\u5931\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u4ec5\u5728n^2\u6fc0\u6d3b\u4e0a\u64cd\u4f5c\uff0cn\u4e3a\u4e13\u5bb6\u6570\u91cf\uff0c\u56fa\u5b9a\u6210\u672c\u4e0e\u6279\u91cf\u5927\u5c0f\u65e0\u5173\u3002</li>\n    <li>\u901a\u8fc7\u5bf9\u53c2\u6570\u4ece3B\u523015B\u7684MoE-LLM\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u9a8c\u8bc1\u4e86ERC\u635f\u5931\u7684\u6709\u6548\u6027\uff0c\u5e76\u80fd\u7075\u6d3b\u63a7\u5236\u548c\u8ddf\u8e2a\u4e13\u5bb6\u7684\u4e13\u4e1a\u5316\u6c34\u5e73\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Mixture-of-Experts (MoE) models struggle because the router's choices don't always match what the experts can do.</li>\n  <li>We introduce a new ERC loss that connects the router's decisions more closely with the experts' abilities.</li>\n  <li>The ERC loss ensures that each expert responds better to its own assigned tokens than to others, improving specialization.</li>\n  <li>This method is efficient, only needing calculations based on the number of experts, not the number of tokens, making it faster than older methods.</li>\n  <li>Tests on large MoE models show that the ERC loss works well and allows tracking of expert skills during training.</li>\n</ul>"}, "publishedAt": "2025-12-29T08:03:18.000Z", "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss", "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23447.png", "numComments": 1, "submittedBy": {"_id": "64b8ca3c5067873176d4b436", "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg", "fullname": "AngLv", "name": "AngLv", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23576", "authors": [{"_id": "69534f1e89916ff627aa3fe3", "name": "Ethan Chern", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe4", "name": "Zhulin Hu", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe5", "name": "Bohao Tang", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe6", "name": "Jiadi Su", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe7", "name": "Steffi Chern", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe8", "name": "Zhijie Deng", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe9", "name": "Pengfei Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64bb5f9d8e051085bace4d1e/skNa_3Ly0Bg7F6aL0mk92.mp4"], "publishedAt": "2025-12-29T16:17:36.000Z", "submittedOnDailyAt": "2025-12-30T02:36:23.479Z", "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation", "submittedOnDailyBy": {"_id": "64bb5f9d8e051085bace4d1e", "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg", "isPro": false, "fullname": "Ethan Chern", "user": "ethanchern", "type": "user"}, "summary": "Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.", "upvotes": 51, "discussionId": "69534f1e89916ff627aa3fea", "githubRepo": "https://github.com/GAIR-NLP/LiveTalk", "githubRepoAddedBy": "user", "ai_summary": "Real-time multimodal video generation via diffusion is enabled by an improved distillation approach with multimodal conditioning and optimized scheduling, reducing inference latency while maintaining quality for interactive systems.", "ai_keywords": ["diffusion models", "bidirectional attention", "distillation methods", "on-policy distillation", "Self Forcing", "audio language models", "Anchor-Heavy Identity Sinks", "multimodal conditioning", "autoregressive", "on-policy optimization"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u65e8\u5728\u5b9e\u73b0\u5b9e\u65f6\u89c6\u9891\u751f\u6210\uff0c\u4ee5\u652f\u6301\u591a\u6a21\u6001\u4e92\u52a8AI\u7cfb\u7edf\u3002</li>\n    <li>\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u5904\u7406\u591a\u4e2a\u89c6\u9891\u5e27\u65f6\u5b58\u5728\u5ef6\u8fdf\uff0c\u5f71\u54cd\u5b9e\u65f6\u4e92\u52a8\u7684\u6548\u679c\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u8f93\u5165\u8d28\u91cf\u548c\u4f18\u5316\u8fdb\u7a0b\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u89c6\u89c9\u4f2a\u5f71\u95ee\u9898\u3002</li>\n    <li>\u5728\u591a\u6a21\u6001\u6761\u4ef6\u4e0b\u7684\u89c6\u9891\u751f\u6210\u4e2d\uff0c\u6211\u4eec\u7684\u65b0\u6a21\u578b\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u63a8\u7406\u6210\u672c\u548c\u5ef6\u8fdf\u51cf\u5c11\u4e8620\u500d\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u7684\u5b9e\u65f6\u591a\u6a21\u6001\u4e92\u52a8\u7cfb\u7edfLiveTalk\uff0c\u5728\u591a\u8f6e\u4e92\u52a8\u89c6\u9891\u7684\u4e00\u81f4\u6027\u548c\u5185\u5bb9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u54cd\u5e94\u65f6\u95f4\u663e\u8457\u7f29\u77ed\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>This paper focuses on creating a real-time video generation system using diffusion models for better human-AI interaction.</li>\n    <li>Current methods struggle with real-time interactions due to slow denoising processes and quality issues with multimodal inputs.</li>\n    <li>The authors propose an improved distillation method that enhances video quality while significantly reducing processing time and costs.</li>\n    <li>The new model matches the visual quality of larger, more complex models but with 20 times less inference time.</li>\n    <li>The system, called LiveTalk, integrates audio and video for real-time interactions and outperforms existing models in coherence and quality.</li>\n</ul>"}, "publishedAt": "2025-12-29T11:17:36.000Z", "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation", "summary": "Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64bb5f9d8e051085bace4d1e/skNa_3Ly0Bg7F6aL0mk92.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23576.png", "numComments": 1, "submittedBy": {"_id": "64bb5f9d8e051085bace4d1e", "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg", "fullname": "Ethan Chern", "name": "ethanchern", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22096", "authors": [{"_id": "695206a8746a34b55dd548dd", "name": "Xiaofeng Mao", "hidden": false}, {"_id": "695206a8746a34b55dd548de", "name": "Zhen Li", "hidden": false}, {"_id": "695206a8746a34b55dd548df", "name": "Chuanhao Li", "hidden": false}, {"_id": "695206a8746a34b55dd548e0", "name": "Xiaojie Xu", "hidden": false}, {"_id": "695206a8746a34b55dd548e1", "name": "Kaining Ying", "hidden": false}, {"_id": "695206a8746a34b55dd548e2", "name": "Tong He", "hidden": false}, {"_id": "695206a8746a34b55dd548e3", "name": "Jiangmiao Pang", "hidden": false}, {"_id": "695206a8746a34b55dd548e4", "name": "Yu Qiao", "hidden": false}, {"_id": "695206a8746a34b55dd548e5", "name": "Kaipeng Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/NMttpBTOZdYJorkqyoD67.mp4"], "publishedAt": "2025-12-26T17:52:49.000Z", "submittedOnDailyAt": "2025-12-30T01:50:23.447Z", "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model", "submittedOnDailyBy": {"_id": "65f1713552c38a91e0a445e8", "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg", "isPro": false, "fullname": "kaipeng", "user": "kpzhang996", "type": "user"}, "summary": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.", "upvotes": 50, "discussionId": "695206a8746a34b55dd548e6", "projectPage": "https://stdstu12.github.io/YUME-Project/", "githubRepo": "https://github.com/stdstu12/YUME", "githubRepoAddedBy": "user", "githubStars": 426, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u65b9\u6cd5\u663e\u793a\uff0c\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u53ef\u4e92\u52a8\u548c\u53ef\u63a2\u7d22\u7684\u4e16\u754c\u3002</li>\n    <li>\u76ee\u524d\u7684\u65b9\u6cd5\u5b58\u5728\u53c2\u6570\u8fc7\u5927\u3001\u63a8\u7406\u6b65\u9aa4\u5197\u957f\u548c\u5386\u53f2\u4e0a\u4e0b\u6587\u5feb\u901f\u589e\u957f\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u5b9e\u65f6\u6027\u80fd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\\method\uff0c\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u5355\u4e2a\u56fe\u50cf\u6216\u6587\u672c\u63d0\u793a\u751f\u6210\u771f\u5b9e\u3001\u4e92\u52a8\u548c\u8fde\u7eed\u7684\u4e16\u754c\u3002</li>\n    <li>\\method\u7684\u6838\u5fc3\u5305\u62ec\u4e09\u4e2a\u90e8\u5206\uff1a\u751f\u6210\u957f\u89c6\u9891\u7684\u6846\u67b6\u3001\u5b9e\u65f6\u52a0\u901f\u7b56\u7565\u548c\u6587\u672c\u63a7\u5236\u7684\u4e8b\u4ef6\u751f\u6210\u65b9\u6cd5\u3002</li>\n    <li>\u6211\u4eec\u63d0\u4f9b\u4e86\u4ee3\u7801\u5e93\u4f5c\u4e3a\u8865\u5145\u6750\u6599\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New methods using diffusion models can create interactive worlds, but they have issues like large sizes and slow processing.</li>\n    <li>These challenges limit real-time performance and the ability to control generation with text.</li>\n    <li>The proposed framework, called \\method, can generate realistic worlds from just a single image or text prompt.</li>\n    <li>\\method includes three main parts: a long-video generation tool, a fast streaming technique, and a way to create world events using text.</li>\n    <li>The code for this framework is available in the supplementary material.</li>\n</ul>"}, "publishedAt": "2025-12-26T12:52:49.000Z", "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model", "summary": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/NMttpBTOZdYJorkqyoD67.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22096.png", "numComments": 1, "submittedBy": {"_id": "65f1713552c38a91e0a445e8", "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg", "fullname": "kaipeng", "name": "kpzhang996", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22322", "authors": [{"_id": "69533fb889916ff627aa3ecb", "name": "Shaofei Cai", "hidden": false}, {"_id": "69533fb889916ff627aa3ecc", "name": "Yulei Qin", "hidden": false}, {"_id": "69533fb889916ff627aa3ecd", "name": "Haojia Lin", "hidden": false}, {"_id": "69533fb889916ff627aa3ece", "name": "Zihan Xu", "hidden": false}, {"_id": "69533fb889916ff627aa3ecf", "name": "Gang Li", "hidden": false}, {"_id": "69533fb889916ff627aa3ed0", "name": "Yuchen Shi", "hidden": false}, {"_id": "69533fb889916ff627aa3ed1", "name": "Zongyi Li", "hidden": false}, {"_id": "69533fb889916ff627aa3ed2", "name": "Yong Mao", "hidden": false}, {"_id": "69533fb889916ff627aa3ed3", "name": "Siqi Cai", "hidden": false}, {"_id": "69533fb889916ff627aa3ed4", "name": "Xiaoyu Tan", "hidden": false}, {"_id": "69533fb889916ff627aa3ed5", "name": "Yitao Liang", "hidden": false}, {"_id": "69533fb889916ff627aa3ed6", "name": "Ke Li", "hidden": false}, {"_id": "69533fb889916ff627aa3ed7", "name": "Xing Sun", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6390525c00fb8ec4a424e0ff/h0k49_chVHOUTywBgnJR6.gif"], "publishedAt": "2025-12-26T14:51:39.000Z", "submittedOnDailyAt": "2025-12-30T01:07:21.942Z", "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents", "submittedOnDailyBy": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "isPro": false, "fullname": "Yulei Qin", "user": "yolay", "type": "user"}, "summary": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.", "upvotes": 33, "discussionId": "69533fb889916ff627aa3ed8", "projectPage": "https://huggingface.co/collections/yolay/smartsnap", "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742GUI\u4efb\u52a1\u4e2d\u5177\u6709\u5f88\u5927\u6f5c\u529b\uff0c\u4f46\u4efb\u52a1\u5b8c\u6210\u7684\u9a8c\u8bc1\u96be\u5ea6\u5f88\u9ad8\u3002</li>\n    <li>\u73b0\u6709\u7684\u9a8c\u8bc1\u65b9\u5f0f\u662f\u88ab\u52a8\u7684\uff0c\u5206\u6790\u4ee3\u7406\u7684\u6574\u4e2a\u4e92\u52a8\u8fc7\u7a0b\u6765\u5224\u65ad\u6210\u529f\u4e0e\u5426\u3002</li>\n    <li>\u4e3a\u4e86\u63d0\u9ad8\u9a8c\u8bc1\u6548\u7387\uff0c\u63d0\u51fa\u4e86SmartSnap\uff0c\u8f6c\u5411\u4e3b\u52a8\u7684\u3001\u81ea\u6211\u9a8c\u8bc1\u65b9\u5f0f\u3002</li>\n    <li>\u65b0\u8bbe\u8ba1\u7684\u81ea\u6211\u9a8c\u8bc1\u4ee3\u7406\u4e0d\u4ec5\u5b8c\u6210\u4efb\u52a1\uff0c\u8fd8\u63d0\u4f9b\u8bc1\u636e\u6765\u8bc1\u660e\u5176\u6210\u529f\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSmartSnap\u53ef\u4ee5\u5728\u53ef\u6269\u5c55\u6027\u4e0a\u63d0\u5347\u6027\u80fd\uff0c\u6548\u679c\u663e\u8457\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Agentic reinforcement learning helps create autonomous agents for complex GUI tasks, but checking if tasks are completed is difficult.</li>\n    <li>Current task verification is slow and relies on analyzing the agent's entire history, which can be messy and costly.</li>\n    <li>The new SmartSnap approach allows agents to verify their own task completion in real-time using important snapshots of their work.</li>\n    <li>This method uses the 3C Principles (Completeness, Conciseness, Creativity) to gather necessary evidence for verification.</li>\n    <li>Tests show that SmartSnap improves the performance of LLM-driven agents significantly, making them more efficient and competitive.</li>\n</ul>"}, "publishedAt": "2025-12-26T09:51:39.000Z", "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents", "summary": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6390525c00fb8ec4a424e0ff/h0k49_chVHOUTywBgnJR6.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22322.png", "numComments": 2, "submittedBy": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "fullname": "Yulei Qin", "name": "yolay", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23705", "authors": [{"_id": "6953546989916ff627aa4002", "name": "Shaocong Xu", "hidden": false}, {"_id": "6953546989916ff627aa4003", "name": "Songlin Wei", "hidden": false}, {"_id": "6953546989916ff627aa4004", "name": "Qizhe Wei", "hidden": false}, {"_id": "6953546989916ff627aa4005", "name": "Zheng Geng", "hidden": false}, {"_id": "6953546989916ff627aa4006", "name": "Hong Li", "hidden": false}, {"_id": "6953546989916ff627aa4007", "name": "Licheng Shen", "hidden": false}, {"_id": "6953546989916ff627aa4008", "name": "Qianpu Sun", "hidden": false}, {"_id": "6953546989916ff627aa4009", "name": "Shu Han", "hidden": false}, {"_id": "6953546989916ff627aa400a", "name": "Bin Ma", "hidden": false}, {"_id": "6953546989916ff627aa400b", "name": "Bohan Li", "hidden": false}, {"_id": "6953546989916ff627aa400c", "name": "Chongjie Ye", "hidden": false}, {"_id": "6953546989916ff627aa400d", "name": "Yuhang Zheng", "hidden": false}, {"_id": "6953546989916ff627aa400e", "name": "Nan Wang", "hidden": false}, {"_id": "6953546989916ff627aa400f", "name": "Saining Zhang", "hidden": false}, {"_id": "6953546989916ff627aa4010", "name": "Hao Zhao", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/652bd2493a416e1f21beb01a/6NM5vLS_B3DlYtmX1N4A_.gif"], "publishedAt": "2025-12-29T18:59:24.000Z", "submittedOnDailyAt": "2025-12-30T01:56:18.708Z", "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation", "submittedOnDailyBy": {"_id": "652bd2493a416e1f21beb01a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652bd2493a416e1f21beb01a/tKijq1pbjmBZuRm82dNEV.jpeg", "isPro": true, "fullname": "Shaocong.Xu", "user": "Daniellesry", "type": "user"}, "summary": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.", "upvotes": 32, "discussionId": "6953546a89916ff627aa4011", "projectPage": "https://daniellli.github.io/projects/DKT/", "githubRepo": "https://github.com/Daniellli/DKT", "githubRepoAddedBy": "user", "githubStars": 94, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u900f\u660e\u7269\u4f53\u5bf9\u611f\u77e5\u7cfb\u7edf\u6765\u8bf4\u5f88\u96be\u5904\u7406\uff0c\u56e0\u6298\u5c04\u3001\u53cd\u5c04\u548c\u900f\u5c04\u6253\u7834\u4e86\u6df1\u5ea6\u4f30\u8ba1\u7684\u5047\u8bbe\u3002</li>\n    <li>\u6211\u4eec\u89c2\u5bdf\u5230\u73b0\u4ee3\u89c6\u9891\u6269\u6563\u6a21\u578b\u80fd\u591f\u5408\u6210\u771f\u5b9e\u611f\u7684\u900f\u660e\u73b0\u8c61\uff0c\u8bf4\u660e\u5b83\u4eec\u5185\u5316\u4e86\u5149\u5b66\u89c4\u5f8b\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86TransPhy3D\uff0c\u4e00\u4e2a\u5305\u542b\u900f\u660e/\u53cd\u5c04\u573a\u666f\u7684\u5408\u6210\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5171\u670911,000\u4e2a\u5e8f\u5217\u3002</li>\n    <li>\u901a\u8fc7\u4f7f\u7528\u5927\u578b\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u89c6\u9891\u5230\u89c6\u9891\u7684\u8f6c\u6362\u5668\uff0c\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u6df1\u5ea6\u548c\u6cd5\u7ebf\u3002</li>\n    <li>\u6700\u7ec8\u6a21\u578bDKT\u5728\u900f\u660e\u7269\u4f53\u7684\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u5347\u4e86\u6df1\u5ea6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Transparent objects are difficult for perception systems to analyze due to issues with refraction and reflection.</li>\n    <li>The authors created TransPhy3D, a video dataset with 11,000 sequences of transparent and reflective scenes using Blender/Cycles.</li>\n    <li>They developed a model called DKT that learns depth and normals from videos, achieving state-of-the-art results on benchmarks related to transparency.</li>\n    <li>DKT improves accuracy and consistency in depth estimation, especially for challenging surfaces like glass and plastic.</li>\n    <li>The study suggests that modern video diffusion models can effectively handle transparency issues in perception tasks.</li>\n</ul>"}, "publishedAt": "2025-12-29T13:59:24.000Z", "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation", "summary": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/652bd2493a416e1f21beb01a/6NM5vLS_B3DlYtmX1N4A_.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23705.png", "numComments": 1, "submittedBy": {"_id": "652bd2493a416e1f21beb01a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652bd2493a416e1f21beb01a/tKijq1pbjmBZuRm82dNEV.jpeg", "fullname": "Shaocong.Xu", "name": "Daniellesry", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23709", "authors": [{"_id": "69537f4189916ff627aa40c0", "name": "Hau-Shiang Shiu", "hidden": false}, {"_id": "69537f4189916ff627aa40c1", "name": "Chin-Yang Lin", "hidden": false}, {"_id": "69537f4189916ff627aa40c2", "name": "Zhixiang Wang", "hidden": false}, {"_id": "69537f4189916ff627aa40c3", "name": "Chi-Wei Hsiao", "hidden": false}, {"_id": "69537f4189916ff627aa40c4", "name": "Po-Fan Yu", "hidden": false}, {"_id": "69537f4189916ff627aa40c5", "name": "Yu-Chih Chen", "hidden": false}, {"_id": "69537f4189916ff627aa40c6", "name": "Yu-Lun Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/4Urho_F4h3YB3NjJxTgCc.mp4"], "publishedAt": "2025-12-29T18:59:57.000Z", "submittedOnDailyAt": "2025-12-30T05:04:09.292Z", "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion", "submittedOnDailyBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "isPro": false, "fullname": "Yu-Lun Liu", "user": "yulunliu", "type": "user"}, "summary": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/", "upvotes": 29, "discussionId": "69537f4289916ff627aa40c7", "projectPage": "https://jamichss.github.io/stream-diffvsr-project-page/", "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStream-DiffVSR\u7684\u9ad8\u6548\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u4e13\u4e3a\u5728\u7ebf\u5b9e\u65f6\u5904\u7406\u8bbe\u8ba1\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u4ec5\u4f7f\u7528\u8fc7\u53bb\u7684\u5e27\uff0c\u907f\u514d\u4e86\u5bf9\u672a\u6765\u5e27\u7684\u4f9d\u8d56\uff0c\u51cf\u5c11\u4e86\u5ef6\u8fdf\u3002</li>\n    <li>\u7ed3\u5408\u4e86\u5feb\u901f\u63a8\u7406\u7684\u56db\u6b65\u53bb\u566a\u5668\u548c\u8fd0\u52a8\u5bf9\u9f50\u7ebf\u7d22\uff0c\u63d0\u9ad8\u4e86\u56fe\u50cf\u7ec6\u8282\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002</li>\n    <li>\u5728RTX4090 GPU\u4e0a\u5904\u7406720p\u89c6\u9891\u5e27\u4ec5\u97000.328\u79d2\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u6269\u6563\u65b9\u6cd5\u3002</li>\n    <li>\u5728\u4fdd\u6301\u8f83\u9ad8\u611f\u77e5\u8d28\u91cf\u7684\u540c\u65f6\uff0cStream-DiffVSR\u5c06\u5ef6\u8fdf\u4ece4600\u79d2\u51cf\u5c11\u52300.328\u79d2\uff0c\u9002\u5408\u4f4e\u5ef6\u8fdf\u5728\u7ebf\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Stream-DiffVSR is a new method for video super-resolution that works quickly and efficiently.</li>\n  <li>It uses only past video frames to improve quality, making it suitable for real-time applications.</li>\n  <li>The method includes a fast denoiser and a module that helps align motion for better results.</li>\n  <li>Stream-DiffVSR processes 720p video frames in just 0.328 seconds on a powerful GPU.</li>\n  <li>It significantly reduces latency compared to previous methods, making it ideal for low-latency use.</li>\n</ul>"}, "publishedAt": "2025-12-29T13:59:57.000Z", "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion", "summary": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/4Urho_F4h3YB3NjJxTgCc.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23709.png", "numComments": 1, "submittedBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "fullname": "Yu-Lun Liu", "name": "yulunliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22615", "authors": [{"_id": "6953489889916ff627aa3f25", "name": "Jiacheng Ye", "hidden": false}, {"_id": "6953489889916ff627aa3f26", "name": "Shansan Gong", "hidden": false}, {"_id": "6953489889916ff627aa3f27", "name": "Jiahui Gao", "hidden": false}, {"_id": "6953489889916ff627aa3f28", "name": "Junming Fan", "hidden": false}, {"_id": "6953489889916ff627aa3f29", "name": "Shuang Wu", "hidden": false}, {"_id": "6953489889916ff627aa3f2a", "name": "Wei Bi", "hidden": false}, {"_id": "6953489889916ff627aa3f2b", "name": "Haoli Bai", "hidden": false}, {"_id": "6953489889916ff627aa3f2c", "name": "Lifeng Shang", "hidden": false}, {"_id": "6953489889916ff627aa3f2d", "name": "Lingpeng Kong", "hidden": false}], "publishedAt": "2025-12-27T14:46:24.000Z", "submittedOnDailyAt": "2025-12-30T03:42:33.237Z", "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone", "submittedOnDailyBy": {"_id": "628c83d186fc004b14e1ed48", "avatarUrl": "/avatars/05ff943a9b89b5f67c5bc254bf45b8f5.svg", "isPro": false, "fullname": "Shansan Gong", "user": "Sansa", "type": "user"}, "summary": "While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as \u03c0_0 and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.", "upvotes": 27, "discussionId": "6953489889916ff627aa3f2e", "projectPage": "https://hkunlp.github.io/blog/2025/dream-vlx/", "githubRepo": "https://github.com/DreamLM/Dream-VLX", "githubRepoAddedBy": "user", "ai_summary": "Diffusion-based vision-language models and action frameworks demonstrate superior performance in visual planning and robotic control tasks compared to autoregressive baselines.", "ai_keywords": ["diffusion-based large language models (dLLMs)", "Vision-Language Models (VLMs)", "Dream-VL", "Vision-Language-Action model (dVLA)", "Dream-VLA", "action chunking", "parallel generation", "LIBERO", "SimplerEnv-Bridge", "SimplerEnv-Fractal", "continuous pre-training"], "githubStars": 40, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "summary_zh": "<ul>\n    <li>\u81ea\u56de\u5f52\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u590d\u6742\u89c6\u89c9\u89c4\u5212\u548c\u52a8\u6001\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u6548\u679c\u6709\u9650\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6269\u6563\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578bDream-VL\uff0c\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u76f8\u4f3c\u6a21\u578b\u3002</li>\n    <li>Dream-VL\u5728\u89c6\u89c9\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4f18\u4e8e\u9876\u7ea7\u7684\u81ea\u56de\u5f52\u6a21\u578b\u3002</li>\n    <li>\u57fa\u4e8eDream-VL\uff0c\u6211\u4eec\u8fd8\u5f00\u53d1\u4e86Dream-VLA\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u52a8\u4f5c\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002</li>\n    <li>Dream-VLA\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u9886\u5148\u6a21\u578b\uff0c\u5e76\u5c06Dream-VL\u4e0eDream-VLA\u53d1\u5e03\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoregressive Vision-Language Models (VLMs) have limitations in complex tasks like visual planning and robotic control.</li>\n    <li>This study presents Dream-VL, a new diffusion-based VLM that performs better than previous models and is effective for visual planning.</li>\n    <li>Dream-VLA, an extension of Dream-VL, is developed for Vision-Language-Action tasks using robotic data and has better performance due to its design.</li>\n    <li>Dream-VLA achieves impressive success rates in various benchmarks, outperforming leading models.</li>\n    <li>Both Dream-VL and Dream-VLA are made available for further research in the community.</li>\n</ul>"}, "publishedAt": "2025-12-27T09:46:24.000Z", "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone", "summary": "While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as \u03c0_0 and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22615.png", "numComments": 1, "submittedBy": {"_id": "628c83d186fc004b14e1ed48", "avatarUrl": "/avatars/05ff943a9b89b5f67c5bc254bf45b8f5.svg", "fullname": "Shansan Gong", "name": "Sansa", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22323", "authors": [{"_id": "6953692989916ff627aa4065", "name": "Zhibin Qin", "hidden": false}, {"_id": "6953692989916ff627aa4066", "name": "Zhenxiong Tan", "hidden": false}, {"_id": "6953692989916ff627aa4067", "name": "Zeqing Wang", "hidden": false}, {"_id": "6953692989916ff627aa4068", "name": "Songhua Liu", "hidden": false}, {"_id": "6953692989916ff627aa4069", "name": "Xinchao Wang", "hidden": false}], "publishedAt": "2025-12-26T14:59:41.000Z", "submittedOnDailyAt": "2025-12-30T03:43:24.884Z", "title": "SpotEdit: Selective Region Editing in Diffusion Transformers", "submittedOnDailyBy": {"_id": "640ebdfefdeaae139086f4d8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640ebdfefdeaae139086f4d8/2N94gbHubplYD8njmUTPf.jpeg", "isPro": true, "fullname": "Zhenxiong Tan", "user": "Yuanshi", "type": "user"}, "summary": "Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing.", "upvotes": 27, "discussionId": "6953692989916ff627aa406a", "projectPage": "https://biangbiang0321.github.io/SpotEdit.github.io", "githubRepo": "https://github.com/Biangbiang0321/SpotEdit", "githubRepoAddedBy": "user", "githubStars": 36, "organization": {"_id": "6508ab2b349930913196378b", "name": "NationalUniversityofSingapore", "fullname": "National University of Singapore", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"}, "summary_zh": "<ul>\n    <li>Diffusion Transformer\u6a21\u578b\u63d0\u9ad8\u4e86\u56fe\u50cf\u7f16\u8f91\u6280\u672f\uff0c\u901a\u8fc7\u7f16\u7801\u6761\u4ef6\u56fe\u50cf\u5e76\u6574\u5408\u5230\u53d8\u6362\u5c42\u4e2d\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u5728\u7f16\u8f91\u65f6\u5904\u7406\u6240\u6709\u533a\u57df\uff0c\u9020\u6210\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u548c\u672a\u4fee\u6539\u533a\u57df\u7684\u8d28\u91cf\u4e0b\u964d\u3002</li>\n    <li>SpotEdit\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u7f16\u8f91\u6846\u67b6\uff0c\u4ec5\u66f4\u65b0\u88ab\u4fee\u6539\u7684\u533a\u57df\u3002</li>\n    <li>SpotEdit\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u90e8\u5206\uff1aSpotSelector\u8bc6\u522b\u7a33\u5b9a\u533a\u57df\uff0cSpotFusion\u901a\u8fc7\u52a8\u6001\u878d\u5408\u673a\u5236\u4fdd\u7559\u7f16\u8f91\u8d28\u91cf\u3002</li>\n    <li>\u901a\u8fc7\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\uff0cSpotEdit\u5b9e\u73b0\u9ad8\u6548\u548c\u7cbe\u51c6\u7684\u56fe\u50cf\u7f16\u8f91\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion Transformer models improve image editing but often process all areas equally, leading to wasted effort on unchanged parts.</li>\n    <li>SpotEdit is a new framework that focuses only on the parts of an image that need editing, avoiding unnecessary computation.</li>\n    <li>It has two main parts: SpotSelector, which identifies stable areas that don't need changes, and SpotFusion, which blends edited and original features effectively.</li>\n    <li>This approach helps maintain image quality while making the editing process more efficient.</li>\n</ul>"}, "publishedAt": "2025-12-26T09:59:41.000Z", "title": "SpotEdit: Selective Region Editing in Diffusion Transformers", "summary": "Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22323.png", "numComments": 2, "submittedBy": {"_id": "640ebdfefdeaae139086f4d8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640ebdfefdeaae139086f4d8/2N94gbHubplYD8njmUTPf.jpeg", "fullname": "Zhenxiong Tan", "name": "Yuanshi", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 170}, "organization": {"_id": "6508ab2b349930913196378b", "name": "NationalUniversityofSingapore", "fullname": "National University of Singapore", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.15560", "authors": [{"_id": "6953703689916ff627aa407c", "name": "Bozhou Li", "hidden": false}, {"_id": "6953703689916ff627aa407d", "name": "Sihan Yang", "hidden": false}, {"_id": "6953703689916ff627aa407e", "name": "Yushuo Guan", "hidden": false}, {"_id": "6953703689916ff627aa407f", "name": "Ruichuan An", "hidden": false}, {"_id": "6953703689916ff627aa4080", "name": "Xinlong Chen", "hidden": false}, {"_id": "6953703689916ff627aa4081", "name": "Yang Shi", "hidden": false}, {"_id": "6953703689916ff627aa4082", "name": "Pengfei Wan", "hidden": false}, {"_id": "6953703689916ff627aa4083", "name": "Wentao Zhang", "hidden": false}, {"_id": "6953703689916ff627aa4084", "name": "Yuanxing zhang", "hidden": false}], "publishedAt": "2025-12-17T16:09:43.000Z", "submittedOnDailyAt": "2025-12-30T04:19:17.454Z", "title": "GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models", "submittedOnDailyBy": {"_id": "661e62c6bac5d981f886f77b", "avatarUrl": "/avatars/f1eb51ed4499ca434c8939573dfbd5e2.svg", "isPro": false, "fullname": "Bozhou Li", "user": "zooblastlbz", "type": "user"}, "summary": "The text encoder is a critical component of text-to-image and text-to-video diffusion models, fundamentally determining the semantic fidelity of the generated content. However, its development has been hindered by two major challenges: the lack of an efficient evaluation framework that reliably predicts downstream generation performance, and the difficulty of effectively adapting pretrained language models for visual synthesis. To address these issues, we introduce GRAN-TED, a paradigm to Generate Robust, Aligned, and Nuanced Text Embeddings for Diffusion models. Our contribution is twofold. First, we propose TED-6K, a novel text-only benchmark that enables efficient and robust assessment of an encoder's representational quality without requiring costly end-to-end model training. We demonstrate that performance on TED-6K, standardized via a lightweight, unified adapter, strongly correlates with an encoder's effectiveness in downstream generation tasks. Notably, under our experimental setup, compared with training a diffusion model from scratch, evaluating with TED-6K is about 750times faster. Second, guided by this validated framework, we develop a superior text encoder using a novel two-stage training paradigm. This process involves an initial fine-tuning stage on a Multimodal Large Language Model for better visual representation, followed by a layer-wise weighting method to extract more nuanced and potent text features. Our experiments show that the resulting GRAN-TED encoder not only achieves state-of-the-art performance on TED-6K but also leads to demonstrable performance gains in text-to-image and text-to-video generation. Our TED-6K dataset and evaluation code are available at the following link: https://anonymous.4open.science/r/GRAN-TED-4FCC/.", "upvotes": 21, "discussionId": "6953703689916ff627aa4085", "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u6587\u672c\u7f16\u7801\u5668\u5728\u6587\u672c\u751f\u6210\u56fe\u50cf\u548c\u89c6\u9891\u7684\u6269\u6563\u6a21\u578b\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u5f71\u54cd\u751f\u6210\u5185\u5bb9\u7684\u8bed\u4e49\u51c6\u786e\u6027\u3002</li>\n    <li>\u7814\u7a76\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u7f3a\u4e4f\u6709\u6548\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u5bf9\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u9002\u5e94\u89c6\u89c9\u5408\u6210\u7684\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86GRAN-TED\u6846\u67b6\uff0c\u65e8\u5728\u751f\u6210\u5f3a\u5927\u3001\u5bf9\u9f50\u548c\u7ec6\u81f4\u7684\u6587\u672c\u5d4c\u5165\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86TED-6K\uff0c\u4e00\u4e2a\u65b0\u7684\u6587\u672c\u57fa\u51c6\uff0c\u80fd\u9ad8\u6548\u8bc4\u4f30\u7f16\u7801\u5668\u7684\u8868\u73b0\uff0c\u901f\u5ea6\u6bd4\u4ece\u5934\u8bad\u7ec3\u6a21\u578b\u5feb\u7ea6750\u500d\u3002</li>\n    <li>\u901a\u8fc7\u4e24\u9636\u6bb5\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6211\u4eec\u7684GRAN-TED\u7f16\u7801\u5668\u5728TED-6K\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u63d0\u5347\u4e86\u6587\u672c\u751f\u6210\u56fe\u50cf\u548c\u89c6\u9891\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The text encoder is important for creating images and videos from text, but its improvement faces challenges.</li>\n    <li>Two main issues are the lack of a good evaluation system and difficulty in adapting language models for visuals.</li>\n    <li>We introduce GRAN-TED, a new approach for creating better text embeddings for diffusion models.</li>\n    <li>We created TED-6K, a benchmark for evaluating text encoder quality quickly and effectively, showing strong results compared to traditional methods.</li>\n    <li>Using a two-stage training process, we developed a better text encoder that performs well on TED-6K and improves text-to-image and text-to-video generation.</li>\n</ul>"}, "publishedAt": "2025-12-17T11:09:43.000Z", "title": "GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models", "summary": "The text encoder is a critical component of text-to-image and text-to-video diffusion models, fundamentally determining the semantic fidelity of the generated content. However, its development has been hindered by two major challenges: the lack of an efficient evaluation framework that reliably predicts downstream generation performance, and the difficulty of effectively adapting pretrained language models for visual synthesis. To address these issues, we introduce GRAN-TED, a paradigm to Generate Robust, Aligned, and Nuanced Text Embeddings for Diffusion models. Our contribution is twofold. First, we propose TED-6K, a novel text-only benchmark that enables efficient and robust assessment of an encoder's representational quality without requiring costly end-to-end model training. We demonstrate that performance on TED-6K, standardized via a lightweight, unified adapter, strongly correlates with an encoder's effectiveness in downstream generation tasks. Notably, under our experimental setup, compared with training a diffusion model from scratch, evaluating with TED-6K is about 750times faster. Second, guided by this validated framework, we develop a superior text encoder using a novel two-stage training paradigm. This process involves an initial fine-tuning stage on a Multimodal Large Language Model for better visual representation, followed by a layer-wise weighting method to extract more nuanced and potent text features. Our experiments show that the resulting GRAN-TED encoder not only achieves state-of-the-art performance on TED-6K but also leads to demonstrable performance gains in text-to-image and text-to-video generation. Our TED-6K dataset and evaluation code are available at the following link: https://anonymous.4open.science/r/GRAN-TED-4FCC/.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15560.png", "numComments": 1, "submittedBy": {"_id": "661e62c6bac5d981f886f77b", "avatarUrl": "/avatars/f1eb51ed4499ca434c8939573dfbd5e2.svg", "fullname": "Bozhou Li", "name": "zooblastlbz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23541", "authors": [{"_id": "69534cef89916ff627aa3f77", "name": "Pengfei Zhou", "hidden": false}, {"_id": "69534cef89916ff627aa3f78", "name": "Liliang Chen", "hidden": false}, {"_id": "69534cef89916ff627aa3f79", "name": "Shengcong Chen", "hidden": false}, {"_id": "69534cef89916ff627aa3f7a", "name": "Di Chen", "hidden": false}, {"_id": "69534cef89916ff627aa3f7b", "name": "Wenzhi Zhao", "hidden": false}, {"_id": "69534cef89916ff627aa3f7c", "name": "Rongjun Jin", "hidden": false}, {"_id": "69534cef89916ff627aa3f7d", "name": "Guanghui Ren", "hidden": false}, {"_id": "69534cef89916ff627aa3f7e", "name": "Jianlan Luo", "hidden": false}], "publishedAt": "2025-12-29T15:28:42.000Z", "submittedOnDailyAt": "2025-12-30T05:09:42.689Z", "title": "Act2Goal: From World Model To General Goal-conditioned Policy", "submittedOnDailyBy": {"_id": "646ec9b135f55eb49e405faa", "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg", "isPro": false, "fullname": "Guanghui Ren", "user": "sundrops", "type": "user"}, "summary": "Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goal-conditioned policies often struggle with long-horizon manipulation due to their reliance on single-step action prediction without explicit modeling of task progress. We propose Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control. Given a current observation and a target visual goal, the world model generates a plausible sequence of intermediate visual states that captures long-horizon structure. To translate this visual plan into robust execution, we introduce Multi-Scale Temporal Hashing (MSTH), which decomposes the imagined trajectory into dense proximal frames for fine-grained closed-loop control and sparse distal frames that anchor global task consistency. The policy couples these representations with motor control through end-to-end cross-attention, enabling coherent long-horizon behavior while remaining reactive to local disturbances. Act2Goal achieves strong zero-shot generalization to novel objects, spatial layouts, and environments. We further enable reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision. Real-robot experiments demonstrate that Act2Goal improves success rates from 30% to 90% on challenging out-of-distribution tasks within minutes of autonomous interaction, validating that goal-conditioned world models with multi-scale temporal control provide structured guidance necessary for robust long-horizon manipulation. Project page: https://act2goal.github.io/", "upvotes": 19, "discussionId": "69534cf089916ff627aa3f7f", "projectPage": "https://act2goal.github.io/", "ai_summary": "Act2Goal employs a goal-conditioned visual world model with multi-scale temporal control and cross-attention to achieve robust long-horizon robotic manipulation through structured planning and adaptive execution.", "ai_keywords": ["goal-conditioned visual world model", "Multi-Scale Temporal Hashing (MSTH)", "cross-attention", "end-to-end cross-attention", "LoRA-based finetuning", "hindsight goal relabeling"], "organization": {"_id": "676fc7c31c48eff17fac3135", "name": "agibot-world", "fullname": "AgiBot World", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64e57309b78bc92221ce3b70/ewI1QvFVMDgSsShQeSvlX.png"}, "summary_zh": "<ul>\n    <li>\u5f00\u53d1\u7cbe\u786e\u4e14\u5bcc\u6709\u8868\u73b0\u529b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u3002</li>\n    <li>\u63d0\u51fa\u4e86Act2Goal\uff0c\u4e00\u4e2a\u65b0\u7684\u76ee\u6807\u6761\u4ef6\u64cd\u4f5c\u7b56\u7565\uff0c\u7ed3\u5408\u4e86\u89c6\u89c9\u4e16\u754c\u6a21\u578b\u548c\u591a\u5c3a\u5ea6\u65f6\u95f4\u63a7\u5236\u3002</li>\n    <li>\u4e16\u754c\u6a21\u578b\u80fd\u591f\u751f\u6210\u6355\u6349\u957f\u65f6\u95f4\u7ed3\u6784\u7684\u4e2d\u95f4\u89c6\u89c9\u72b6\u6001\u5e8f\u5217\u3002</li>\n    <li>\u4f7f\u7528\u591a\u5c3a\u5ea6\u65f6\u95f4\u54c8\u5e0c\uff08MSTH\uff09\u5c06\u60f3\u8c61\u7684\u8f68\u8ff9\u5206\u89e3\u4e3a\u7ec6\u81f4\u7684\u5e27\u548c\u7a00\u758f\u7684\u5e27\uff0c\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u63a7\u5236\u3002</li>\n    <li>Act2Goal\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0c\u5c06\u6210\u529f\u7387\u4ece30%\u63d0\u9ad8\u523090%\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u56f0\u96be\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Act2Goal is a new method for robotic manipulation that helps robots understand and execute complex tasks over a longer period.</li>\n    <li>It uses a visual world model to create a series of intermediate steps, making it easier for robots to plan their actions toward a visual goal.</li>\n    <li>The method includes Multi-Scale Temporal Hashing (MSTH) to break down the task into detailed actions for precise control while keeping the overall task aligned.</li>\n    <li>Act2Goal can quickly adapt to new objects and environments without needing extra supervision, improving its performance rapidly.</li>\n    <li>In tests, it significantly increased the success rate of robots completing difficult tasks, showing its effectiveness for long-term manipulation. </li>\n</ul>"}, "publishedAt": "2025-12-29T10:28:42.000Z", "title": "Act2Goal: From World Model To General Goal-conditioned Policy", "summary": "Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goal-conditioned policies often struggle with long-horizon manipulation due to their reliance on single-step action prediction without explicit modeling of task progress. We propose Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control. Given a current observation and a target visual goal, the world model generates a plausible sequence of intermediate visual states that captures long-horizon structure. To translate this visual plan into robust execution, we introduce Multi-Scale Temporal Hashing (MSTH), which decomposes the imagined trajectory into dense proximal frames for fine-grained closed-loop control and sparse distal frames that anchor global task consistency. The policy couples these representations with motor control through end-to-end cross-attention, enabling coherent long-horizon behavior while remaining reactive to local disturbances. Act2Goal achieves strong zero-shot generalization to novel objects, spatial layouts, and environments. We further enable reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision. Real-robot experiments demonstrate that Act2Goal improves success rates from 30% to 90% on challenging out-of-distribution tasks within minutes of autonomous interaction, validating that goal-conditioned world models with multi-scale temporal control provide structured guidance necessary for robust long-horizon manipulation. Project page: https://act2goal.github.io/", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23541.png", "numComments": 1, "submittedBy": {"_id": "646ec9b135f55eb49e405faa", "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg", "fullname": "Guanghui Ren", "name": "sundrops", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "676fc7c31c48eff17fac3135", "name": "agibot-world", "fullname": "AgiBot World", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64e57309b78bc92221ce3b70/ewI1QvFVMDgSsShQeSvlX.png"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2512.23447", "authors": [{"_id": "69534a9589916ff627aa3f5c", "name": "Ang Lv", "hidden": false}, {"_id": "69534a9589916ff627aa3f5d", "name": "Jin Ma", "hidden": false}, {"_id": "69534a9589916ff627aa3f5e", "name": "Yiyuan Ma", "hidden": false}, {"_id": "69534a9589916ff627aa3f5f", "name": "Siyuan Qiao", "hidden": false}], "publishedAt": "2025-12-29T13:03:18.000Z", "submittedOnDailyAt": "2025-12-30T01:18:40.635Z", "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss", "submittedOnDailyBy": {"_id": "64b8ca3c5067873176d4b436", "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg", "isPro": false, "fullname": "AngLv", "user": "AngLv", "type": "user"}, "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.", "upvotes": 71, "discussionId": "69534a9589916ff627aa3f60", "ai_summary": "An expert-router coupling (ERC) loss aligns router decisions with expert capabilities in Mixture-of-Experts (MoE) models by enforcing constraints on internal activations, improving performance and computational efficiency.", "ai_keywords": ["Mixture-of-Experts (MoE)", "expert-router coupling (ERC) loss", "router embeddings", "proxy tokens", "internal activations", "MoE-LLMs", "expert specialization levels", "n\u00b2 activations"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\u7f3a\u4e4f\u660e\u786e\u7684\u7ea6\u675f\uff0c\u5bfc\u81f4\u8def\u7531\u5668\u7684\u51b3\u7b56\u4e0e\u4e13\u5bb6\u80fd\u529b\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e13\u5bb6-\u8def\u7531\u5668\u8026\u5408\uff08ERC\uff09\u635f\u5931\uff0c\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u8f85\u52a9\u635f\u5931\uff0c\u5c06\u8def\u7531\u5668\u7684\u51b3\u7b56\u4e0e\u4e13\u5bb6\u7684\u80fd\u529b\u7d27\u5bc6\u7ed3\u5408\u3002</li>\n    <li>ERC\u635f\u5931\u5bf9\u4e13\u5bb6\u7684\u5185\u90e8\u6fc0\u6d3b\u65bd\u52a0\u4e24\u4e2a\u7ea6\u675f\uff0c\u786e\u4fdd\u6bcf\u4e2a\u4e13\u5bb6\u5bf9\u81ea\u8eab\u4ee3\u7406token\u7684\u6fc0\u6d3b\u9ad8\u4e8e\u5176\u4ed6\u4e13\u5bb6\u7684\u4ee3\u7406token\u3002</li>\n    <li>ERC\u635f\u5931\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u4ec5\u5728n^2\u6fc0\u6d3b\u4e0a\u64cd\u4f5c\uff0cn\u4e3a\u4e13\u5bb6\u6570\u91cf\uff0c\u56fa\u5b9a\u6210\u672c\u4e0e\u6279\u91cf\u5927\u5c0f\u65e0\u5173\u3002</li>\n    <li>\u901a\u8fc7\u5bf9\u53c2\u6570\u4ece3B\u523015B\u7684MoE-LLM\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u9a8c\u8bc1\u4e86ERC\u635f\u5931\u7684\u6709\u6548\u6027\uff0c\u5e76\u80fd\u7075\u6d3b\u63a7\u5236\u548c\u8ddf\u8e2a\u4e13\u5bb6\u7684\u4e13\u4e1a\u5316\u6c34\u5e73\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Mixture-of-Experts (MoE) models struggle because the router's choices don't always match what the experts can do.</li>\n  <li>We introduce a new ERC loss that connects the router's decisions more closely with the experts' abilities.</li>\n  <li>The ERC loss ensures that each expert responds better to its own assigned tokens than to others, improving specialization.</li>\n  <li>This method is efficient, only needing calculations based on the number of experts, not the number of tokens, making it faster than older methods.</li>\n  <li>Tests on large MoE models show that the ERC loss works well and allows tracking of expert skills during training.</li>\n</ul>"}, "publishedAt": "2025-12-29T08:03:18.000Z", "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss", "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23447.png", "numComments": 1, "submittedBy": {"_id": "64b8ca3c5067873176d4b436", "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg", "fullname": "AngLv", "name": "AngLv", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.21218", "authors": [{"_id": "694c9c5f746a34b55dd54018", "name": "Kelvin Li", "hidden": false}, {"_id": "694c9c5f746a34b55dd54019", "user": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "isPro": false, "fullname": "Chuyi Shang", "user": "chuyishang", "type": "user"}, "name": "Chuyi Shang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:50.115Z", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401a", "name": "Leonid Karlinsky", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401b", "name": "Rogerio Feris", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401c", "name": "Trevor Darrell", "hidden": false}, {"_id": "694c9c5f746a34b55dd5401d", "name": "Roei Herzig", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65a86fb810125597329a4580/_EqCb0UI7hQNGGGlI4I_J.jpeg"], "publishedAt": "2025-12-24T14:59:49.000Z", "submittedOnDailyAt": "2025-12-26T00:43:27.241Z", "title": "Latent Implicit Visual Reasoning", "submittedOnDailyBy": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "isPro": false, "fullname": "Chuyi Shang", "user": "chuyishang", "type": "user"}, "summary": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.", "upvotes": 61, "discussionId": "694c9c5f746a34b55dd5401e", "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "summary_zh": "<ul>\n    <li>\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\uff0c\u5904\u7406\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u80fd\u529b\u6709\u9650\u3002</li>\n    <li>\u6700\u8fd1\u7684\u65b9\u6cd5\u4f7f\u7528\u8f85\u52a9\u56fe\u50cf\u7b49\u6765\u76d1\u7763\u89c6\u89c9\u6b65\u9aa4\uff0c\u4f46\u5b58\u5728\u4e00\u4e9b\u9650\u5236\u548c\u9ad8\u989d\u6807\u6ce8\u6210\u672c\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u4efb\u52a1\u4f9d\u8d56\u7684\u673a\u5236\uff0c\u53ef\u4ee5\u8ba9LMMs\u5728\u6ca1\u6709\u660e\u786e\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u53d1\u73b0\u5e76\u4f7f\u7528\u89c6\u89c9\u63a8\u7406\u6807\u8bb0\u3002</li>\n    <li>\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u81ea\u9002\u5e94\u5730\u63d0\u53d6\u56fe\u50cf\u4e2d\u7684\u76f8\u5173\u89c6\u89c9\u4fe1\u606f\uff0c\u6548\u679c\u4f18\u4e8e\u76f4\u63a5\u5fae\u8c03\u3002</li>\n    <li>\u6211\u4eec\u7684\u65b9\u6848\u5728\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u9002\u5e94\u591a\u4efb\u52a1\u6307\u4ee4\u8c03\u4f18\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Multimodal Models (LMMs) mainly focus on text, making it hard for them to handle visual reasoning tasks.</li>\n    <li>Current methods try to improve this by using helper images and depth maps, but these methods have limitations like high costs and difficulty in generalizing.</li>\n    <li>The proposed solution allows LMMs to learn visual reasoning without needing specific guidance or supervision.</li>\n    <li>This new method helps the model adaptively understand images better, leading to superior performance on various vision tasks.</li>\n    <li>It also shows good results when applied to different tasks at once, proving its versatility.</li>\n</ul>"}, "publishedAt": "2025-12-24T09:59:49.000Z", "title": "Latent Implicit Visual Reasoning", "summary": "While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what \"useful\" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65a86fb810125597329a4580/_EqCb0UI7hQNGGGlI4I_J.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21218.png", "numComments": 5, "submittedBy": {"_id": "65a86fb810125597329a4580", "avatarUrl": "/avatars/e8704745527060ff48da1cb474ae1424.svg", "fullname": "Chuyi Shang", "name": "chuyishang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.23576", "authors": [{"_id": "69534f1e89916ff627aa3fe3", "name": "Ethan Chern", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe4", "name": "Zhulin Hu", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe5", "name": "Bohao Tang", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe6", "name": "Jiadi Su", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe7", "name": "Steffi Chern", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe8", "name": "Zhijie Deng", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe9", "name": "Pengfei Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64bb5f9d8e051085bace4d1e/skNa_3Ly0Bg7F6aL0mk92.mp4"], "publishedAt": "2025-12-29T16:17:36.000Z", "submittedOnDailyAt": "2025-12-30T02:36:23.479Z", "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation", "submittedOnDailyBy": {"_id": "64bb5f9d8e051085bace4d1e", "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg", "isPro": false, "fullname": "Ethan Chern", "user": "ethanchern", "type": "user"}, "summary": "Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.", "upvotes": 51, "discussionId": "69534f1e89916ff627aa3fea", "githubRepo": "https://github.com/GAIR-NLP/LiveTalk", "githubRepoAddedBy": "user", "ai_summary": "Real-time multimodal video generation via diffusion is enabled by an improved distillation approach with multimodal conditioning and optimized scheduling, reducing inference latency while maintaining quality for interactive systems.", "ai_keywords": ["diffusion models", "bidirectional attention", "distillation methods", "on-policy distillation", "Self Forcing", "audio language models", "Anchor-Heavy Identity Sinks", "multimodal conditioning", "autoregressive", "on-policy optimization"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u65e8\u5728\u5b9e\u73b0\u5b9e\u65f6\u89c6\u9891\u751f\u6210\uff0c\u4ee5\u652f\u6301\u591a\u6a21\u6001\u4e92\u52a8AI\u7cfb\u7edf\u3002</li>\n    <li>\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u5904\u7406\u591a\u4e2a\u89c6\u9891\u5e27\u65f6\u5b58\u5728\u5ef6\u8fdf\uff0c\u5f71\u54cd\u5b9e\u65f6\u4e92\u52a8\u7684\u6548\u679c\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u8f93\u5165\u8d28\u91cf\u548c\u4f18\u5316\u8fdb\u7a0b\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u89c6\u89c9\u4f2a\u5f71\u95ee\u9898\u3002</li>\n    <li>\u5728\u591a\u6a21\u6001\u6761\u4ef6\u4e0b\u7684\u89c6\u9891\u751f\u6210\u4e2d\uff0c\u6211\u4eec\u7684\u65b0\u6a21\u578b\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u63a8\u7406\u6210\u672c\u548c\u5ef6\u8fdf\u51cf\u5c11\u4e8620\u500d\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u7684\u5b9e\u65f6\u591a\u6a21\u6001\u4e92\u52a8\u7cfb\u7edfLiveTalk\uff0c\u5728\u591a\u8f6e\u4e92\u52a8\u89c6\u9891\u7684\u4e00\u81f4\u6027\u548c\u5185\u5bb9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u54cd\u5e94\u65f6\u95f4\u663e\u8457\u7f29\u77ed\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>This paper focuses on creating a real-time video generation system using diffusion models for better human-AI interaction.</li>\n    <li>Current methods struggle with real-time interactions due to slow denoising processes and quality issues with multimodal inputs.</li>\n    <li>The authors propose an improved distillation method that enhances video quality while significantly reducing processing time and costs.</li>\n    <li>The new model matches the visual quality of larger, more complex models but with 20 times less inference time.</li>\n    <li>The system, called LiveTalk, integrates audio and video for real-time interactions and outperforms existing models in coherence and quality.</li>\n</ul>"}, "publishedAt": "2025-12-29T11:17:36.000Z", "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation", "summary": "Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64bb5f9d8e051085bace4d1e/skNa_3Ly0Bg7F6aL0mk92.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23576.png", "numComments": 1, "submittedBy": {"_id": "64bb5f9d8e051085bace4d1e", "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg", "fullname": "Ethan Chern", "name": "ethanchern", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22096", "authors": [{"_id": "695206a8746a34b55dd548dd", "name": "Xiaofeng Mao", "hidden": false}, {"_id": "695206a8746a34b55dd548de", "name": "Zhen Li", "hidden": false}, {"_id": "695206a8746a34b55dd548df", "name": "Chuanhao Li", "hidden": false}, {"_id": "695206a8746a34b55dd548e0", "name": "Xiaojie Xu", "hidden": false}, {"_id": "695206a8746a34b55dd548e1", "name": "Kaining Ying", "hidden": false}, {"_id": "695206a8746a34b55dd548e2", "name": "Tong He", "hidden": false}, {"_id": "695206a8746a34b55dd548e3", "name": "Jiangmiao Pang", "hidden": false}, {"_id": "695206a8746a34b55dd548e4", "name": "Yu Qiao", "hidden": false}, {"_id": "695206a8746a34b55dd548e5", "name": "Kaipeng Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/NMttpBTOZdYJorkqyoD67.mp4"], "publishedAt": "2025-12-26T17:52:49.000Z", "submittedOnDailyAt": "2025-12-30T01:50:23.447Z", "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model", "submittedOnDailyBy": {"_id": "65f1713552c38a91e0a445e8", "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg", "isPro": false, "fullname": "kaipeng", "user": "kpzhang996", "type": "user"}, "summary": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.", "upvotes": 50, "discussionId": "695206a8746a34b55dd548e6", "projectPage": "https://stdstu12.github.io/YUME-Project/", "githubRepo": "https://github.com/stdstu12/YUME", "githubRepoAddedBy": "user", "githubStars": 426, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u65b9\u6cd5\u663e\u793a\uff0c\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u53ef\u4e92\u52a8\u548c\u53ef\u63a2\u7d22\u7684\u4e16\u754c\u3002</li>\n    <li>\u76ee\u524d\u7684\u65b9\u6cd5\u5b58\u5728\u53c2\u6570\u8fc7\u5927\u3001\u63a8\u7406\u6b65\u9aa4\u5197\u957f\u548c\u5386\u53f2\u4e0a\u4e0b\u6587\u5feb\u901f\u589e\u957f\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u5b9e\u65f6\u6027\u80fd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\\method\uff0c\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u5355\u4e2a\u56fe\u50cf\u6216\u6587\u672c\u63d0\u793a\u751f\u6210\u771f\u5b9e\u3001\u4e92\u52a8\u548c\u8fde\u7eed\u7684\u4e16\u754c\u3002</li>\n    <li>\\method\u7684\u6838\u5fc3\u5305\u62ec\u4e09\u4e2a\u90e8\u5206\uff1a\u751f\u6210\u957f\u89c6\u9891\u7684\u6846\u67b6\u3001\u5b9e\u65f6\u52a0\u901f\u7b56\u7565\u548c\u6587\u672c\u63a7\u5236\u7684\u4e8b\u4ef6\u751f\u6210\u65b9\u6cd5\u3002</li>\n    <li>\u6211\u4eec\u63d0\u4f9b\u4e86\u4ee3\u7801\u5e93\u4f5c\u4e3a\u8865\u5145\u6750\u6599\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New methods using diffusion models can create interactive worlds, but they have issues like large sizes and slow processing.</li>\n    <li>These challenges limit real-time performance and the ability to control generation with text.</li>\n    <li>The proposed framework, called \\method, can generate realistic worlds from just a single image or text prompt.</li>\n    <li>\\method includes three main parts: a long-video generation tool, a fast streaming technique, and a way to create world events using text.</li>\n    <li>The code for this framework is available in the supplementary material.</li>\n</ul>"}, "publishedAt": "2025-12-26T12:52:49.000Z", "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model", "summary": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/NMttpBTOZdYJorkqyoD67.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22096.png", "numComments": 1, "submittedBy": {"_id": "65f1713552c38a91e0a445e8", "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg", "fullname": "kaipeng", "name": "kpzhang996", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22322", "authors": [{"_id": "69533fb889916ff627aa3ecb", "name": "Shaofei Cai", "hidden": false}, {"_id": "69533fb889916ff627aa3ecc", "name": "Yulei Qin", "hidden": false}, {"_id": "69533fb889916ff627aa3ecd", "name": "Haojia Lin", "hidden": false}, {"_id": "69533fb889916ff627aa3ece", "name": "Zihan Xu", "hidden": false}, {"_id": "69533fb889916ff627aa3ecf", "name": "Gang Li", "hidden": false}, {"_id": "69533fb889916ff627aa3ed0", "name": "Yuchen Shi", "hidden": false}, {"_id": "69533fb889916ff627aa3ed1", "name": "Zongyi Li", "hidden": false}, {"_id": "69533fb889916ff627aa3ed2", "name": "Yong Mao", "hidden": false}, {"_id": "69533fb889916ff627aa3ed3", "name": "Siqi Cai", "hidden": false}, {"_id": "69533fb889916ff627aa3ed4", "name": "Xiaoyu Tan", "hidden": false}, {"_id": "69533fb889916ff627aa3ed5", "name": "Yitao Liang", "hidden": false}, {"_id": "69533fb889916ff627aa3ed6", "name": "Ke Li", "hidden": false}, {"_id": "69533fb889916ff627aa3ed7", "name": "Xing Sun", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6390525c00fb8ec4a424e0ff/h0k49_chVHOUTywBgnJR6.gif"], "publishedAt": "2025-12-26T14:51:39.000Z", "submittedOnDailyAt": "2025-12-30T01:07:21.942Z", "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents", "submittedOnDailyBy": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "isPro": false, "fullname": "Yulei Qin", "user": "yolay", "type": "user"}, "summary": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.", "upvotes": 33, "discussionId": "69533fb889916ff627aa3ed8", "projectPage": "https://huggingface.co/collections/yolay/smartsnap", "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742GUI\u4efb\u52a1\u4e2d\u5177\u6709\u5f88\u5927\u6f5c\u529b\uff0c\u4f46\u4efb\u52a1\u5b8c\u6210\u7684\u9a8c\u8bc1\u96be\u5ea6\u5f88\u9ad8\u3002</li>\n    <li>\u73b0\u6709\u7684\u9a8c\u8bc1\u65b9\u5f0f\u662f\u88ab\u52a8\u7684\uff0c\u5206\u6790\u4ee3\u7406\u7684\u6574\u4e2a\u4e92\u52a8\u8fc7\u7a0b\u6765\u5224\u65ad\u6210\u529f\u4e0e\u5426\u3002</li>\n    <li>\u4e3a\u4e86\u63d0\u9ad8\u9a8c\u8bc1\u6548\u7387\uff0c\u63d0\u51fa\u4e86SmartSnap\uff0c\u8f6c\u5411\u4e3b\u52a8\u7684\u3001\u81ea\u6211\u9a8c\u8bc1\u65b9\u5f0f\u3002</li>\n    <li>\u65b0\u8bbe\u8ba1\u7684\u81ea\u6211\u9a8c\u8bc1\u4ee3\u7406\u4e0d\u4ec5\u5b8c\u6210\u4efb\u52a1\uff0c\u8fd8\u63d0\u4f9b\u8bc1\u636e\u6765\u8bc1\u660e\u5176\u6210\u529f\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSmartSnap\u53ef\u4ee5\u5728\u53ef\u6269\u5c55\u6027\u4e0a\u63d0\u5347\u6027\u80fd\uff0c\u6548\u679c\u663e\u8457\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Agentic reinforcement learning helps create autonomous agents for complex GUI tasks, but checking if tasks are completed is difficult.</li>\n    <li>Current task verification is slow and relies on analyzing the agent's entire history, which can be messy and costly.</li>\n    <li>The new SmartSnap approach allows agents to verify their own task completion in real-time using important snapshots of their work.</li>\n    <li>This method uses the 3C Principles (Completeness, Conciseness, Creativity) to gather necessary evidence for verification.</li>\n    <li>Tests show that SmartSnap improves the performance of LLM-driven agents significantly, making them more efficient and competitive.</li>\n</ul>"}, "publishedAt": "2025-12-26T09:51:39.000Z", "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents", "summary": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6390525c00fb8ec4a424e0ff/h0k49_chVHOUTywBgnJR6.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22322.png", "numComments": 2, "submittedBy": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "fullname": "Yulei Qin", "name": "yolay", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23705", "authors": [{"_id": "6953546989916ff627aa4002", "name": "Shaocong Xu", "hidden": false}, {"_id": "6953546989916ff627aa4003", "name": "Songlin Wei", "hidden": false}, {"_id": "6953546989916ff627aa4004", "name": "Qizhe Wei", "hidden": false}, {"_id": "6953546989916ff627aa4005", "name": "Zheng Geng", "hidden": false}, {"_id": "6953546989916ff627aa4006", "name": "Hong Li", "hidden": false}, {"_id": "6953546989916ff627aa4007", "name": "Licheng Shen", "hidden": false}, {"_id": "6953546989916ff627aa4008", "name": "Qianpu Sun", "hidden": false}, {"_id": "6953546989916ff627aa4009", "name": "Shu Han", "hidden": false}, {"_id": "6953546989916ff627aa400a", "name": "Bin Ma", "hidden": false}, {"_id": "6953546989916ff627aa400b", "name": "Bohan Li", "hidden": false}, {"_id": "6953546989916ff627aa400c", "name": "Chongjie Ye", "hidden": false}, {"_id": "6953546989916ff627aa400d", "name": "Yuhang Zheng", "hidden": false}, {"_id": "6953546989916ff627aa400e", "name": "Nan Wang", "hidden": false}, {"_id": "6953546989916ff627aa400f", "name": "Saining Zhang", "hidden": false}, {"_id": "6953546989916ff627aa4010", "name": "Hao Zhao", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/652bd2493a416e1f21beb01a/6NM5vLS_B3DlYtmX1N4A_.gif"], "publishedAt": "2025-12-29T18:59:24.000Z", "submittedOnDailyAt": "2025-12-30T01:56:18.708Z", "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation", "submittedOnDailyBy": {"_id": "652bd2493a416e1f21beb01a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652bd2493a416e1f21beb01a/tKijq1pbjmBZuRm82dNEV.jpeg", "isPro": true, "fullname": "Shaocong.Xu", "user": "Daniellesry", "type": "user"}, "summary": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.", "upvotes": 32, "discussionId": "6953546a89916ff627aa4011", "projectPage": "https://daniellli.github.io/projects/DKT/", "githubRepo": "https://github.com/Daniellli/DKT", "githubRepoAddedBy": "user", "githubStars": 94, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u900f\u660e\u7269\u4f53\u5bf9\u611f\u77e5\u7cfb\u7edf\u6765\u8bf4\u5f88\u96be\u5904\u7406\uff0c\u56e0\u6298\u5c04\u3001\u53cd\u5c04\u548c\u900f\u5c04\u6253\u7834\u4e86\u6df1\u5ea6\u4f30\u8ba1\u7684\u5047\u8bbe\u3002</li>\n    <li>\u6211\u4eec\u89c2\u5bdf\u5230\u73b0\u4ee3\u89c6\u9891\u6269\u6563\u6a21\u578b\u80fd\u591f\u5408\u6210\u771f\u5b9e\u611f\u7684\u900f\u660e\u73b0\u8c61\uff0c\u8bf4\u660e\u5b83\u4eec\u5185\u5316\u4e86\u5149\u5b66\u89c4\u5f8b\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86TransPhy3D\uff0c\u4e00\u4e2a\u5305\u542b\u900f\u660e/\u53cd\u5c04\u573a\u666f\u7684\u5408\u6210\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5171\u670911,000\u4e2a\u5e8f\u5217\u3002</li>\n    <li>\u901a\u8fc7\u4f7f\u7528\u5927\u578b\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u89c6\u9891\u5230\u89c6\u9891\u7684\u8f6c\u6362\u5668\uff0c\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u6df1\u5ea6\u548c\u6cd5\u7ebf\u3002</li>\n    <li>\u6700\u7ec8\u6a21\u578bDKT\u5728\u900f\u660e\u7269\u4f53\u7684\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u5347\u4e86\u6df1\u5ea6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Transparent objects are difficult for perception systems to analyze due to issues with refraction and reflection.</li>\n    <li>The authors created TransPhy3D, a video dataset with 11,000 sequences of transparent and reflective scenes using Blender/Cycles.</li>\n    <li>They developed a model called DKT that learns depth and normals from videos, achieving state-of-the-art results on benchmarks related to transparency.</li>\n    <li>DKT improves accuracy and consistency in depth estimation, especially for challenging surfaces like glass and plastic.</li>\n    <li>The study suggests that modern video diffusion models can effectively handle transparency issues in perception tasks.</li>\n</ul>"}, "publishedAt": "2025-12-29T13:59:24.000Z", "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation", "summary": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/652bd2493a416e1f21beb01a/6NM5vLS_B3DlYtmX1N4A_.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23705.png", "numComments": 1, "submittedBy": {"_id": "652bd2493a416e1f21beb01a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652bd2493a416e1f21beb01a/tKijq1pbjmBZuRm82dNEV.jpeg", "fullname": "Shaocong.Xu", "name": "Daniellesry", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23709", "authors": [{"_id": "69537f4189916ff627aa40c0", "name": "Hau-Shiang Shiu", "hidden": false}, {"_id": "69537f4189916ff627aa40c1", "name": "Chin-Yang Lin", "hidden": false}, {"_id": "69537f4189916ff627aa40c2", "name": "Zhixiang Wang", "hidden": false}, {"_id": "69537f4189916ff627aa40c3", "name": "Chi-Wei Hsiao", "hidden": false}, {"_id": "69537f4189916ff627aa40c4", "name": "Po-Fan Yu", "hidden": false}, {"_id": "69537f4189916ff627aa40c5", "name": "Yu-Chih Chen", "hidden": false}, {"_id": "69537f4189916ff627aa40c6", "name": "Yu-Lun Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/4Urho_F4h3YB3NjJxTgCc.mp4"], "publishedAt": "2025-12-29T18:59:57.000Z", "submittedOnDailyAt": "2025-12-30T05:04:09.292Z", "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion", "submittedOnDailyBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "isPro": false, "fullname": "Yu-Lun Liu", "user": "yulunliu", "type": "user"}, "summary": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/", "upvotes": 29, "discussionId": "69537f4289916ff627aa40c7", "projectPage": "https://jamichss.github.io/stream-diffvsr-project-page/", "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStream-DiffVSR\u7684\u9ad8\u6548\u89c6\u9891\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u4e13\u4e3a\u5728\u7ebf\u5b9e\u65f6\u5904\u7406\u8bbe\u8ba1\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u4ec5\u4f7f\u7528\u8fc7\u53bb\u7684\u5e27\uff0c\u907f\u514d\u4e86\u5bf9\u672a\u6765\u5e27\u7684\u4f9d\u8d56\uff0c\u51cf\u5c11\u4e86\u5ef6\u8fdf\u3002</li>\n    <li>\u7ed3\u5408\u4e86\u5feb\u901f\u63a8\u7406\u7684\u56db\u6b65\u53bb\u566a\u5668\u548c\u8fd0\u52a8\u5bf9\u9f50\u7ebf\u7d22\uff0c\u63d0\u9ad8\u4e86\u56fe\u50cf\u7ec6\u8282\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002</li>\n    <li>\u5728RTX4090 GPU\u4e0a\u5904\u7406720p\u89c6\u9891\u5e27\u4ec5\u97000.328\u79d2\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u6269\u6563\u65b9\u6cd5\u3002</li>\n    <li>\u5728\u4fdd\u6301\u8f83\u9ad8\u611f\u77e5\u8d28\u91cf\u7684\u540c\u65f6\uff0cStream-DiffVSR\u5c06\u5ef6\u8fdf\u4ece4600\u79d2\u51cf\u5c11\u52300.328\u79d2\uff0c\u9002\u5408\u4f4e\u5ef6\u8fdf\u5728\u7ebf\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Stream-DiffVSR is a new method for video super-resolution that works quickly and efficiently.</li>\n  <li>It uses only past video frames to improve quality, making it suitable for real-time applications.</li>\n  <li>The method includes a fast denoiser and a module that helps align motion for better results.</li>\n  <li>Stream-DiffVSR processes 720p video frames in just 0.328 seconds on a powerful GPU.</li>\n  <li>It significantly reduces latency compared to previous methods, making it ideal for low-latency use.</li>\n</ul>"}, "publishedAt": "2025-12-29T13:59:57.000Z", "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion", "summary": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/4Urho_F4h3YB3NjJxTgCc.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23709.png", "numComments": 1, "submittedBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "fullname": "Yu-Lun Liu", "name": "yulunliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22615", "authors": [{"_id": "6953489889916ff627aa3f25", "name": "Jiacheng Ye", "hidden": false}, {"_id": "6953489889916ff627aa3f26", "name": "Shansan Gong", "hidden": false}, {"_id": "6953489889916ff627aa3f27", "name": "Jiahui Gao", "hidden": false}, {"_id": "6953489889916ff627aa3f28", "name": "Junming Fan", "hidden": false}, {"_id": "6953489889916ff627aa3f29", "name": "Shuang Wu", "hidden": false}, {"_id": "6953489889916ff627aa3f2a", "name": "Wei Bi", "hidden": false}, {"_id": "6953489889916ff627aa3f2b", "name": "Haoli Bai", "hidden": false}, {"_id": "6953489889916ff627aa3f2c", "name": "Lifeng Shang", "hidden": false}, {"_id": "6953489889916ff627aa3f2d", "name": "Lingpeng Kong", "hidden": false}], "publishedAt": "2025-12-27T14:46:24.000Z", "submittedOnDailyAt": "2025-12-30T03:42:33.237Z", "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone", "submittedOnDailyBy": {"_id": "628c83d186fc004b14e1ed48", "avatarUrl": "/avatars/05ff943a9b89b5f67c5bc254bf45b8f5.svg", "isPro": false, "fullname": "Shansan Gong", "user": "Sansa", "type": "user"}, "summary": "While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as \u03c0_0 and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.", "upvotes": 27, "discussionId": "6953489889916ff627aa3f2e", "projectPage": "https://hkunlp.github.io/blog/2025/dream-vlx/", "githubRepo": "https://github.com/DreamLM/Dream-VLX", "githubRepoAddedBy": "user", "ai_summary": "Diffusion-based vision-language models and action frameworks demonstrate superior performance in visual planning and robotic control tasks compared to autoregressive baselines.", "ai_keywords": ["diffusion-based large language models (dLLMs)", "Vision-Language Models (VLMs)", "Dream-VL", "Vision-Language-Action model (dVLA)", "Dream-VLA", "action chunking", "parallel generation", "LIBERO", "SimplerEnv-Bridge", "SimplerEnv-Fractal", "continuous pre-training"], "githubStars": 40, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "summary_zh": "<ul>\n    <li>\u81ea\u56de\u5f52\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u590d\u6742\u89c6\u89c9\u89c4\u5212\u548c\u52a8\u6001\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u6548\u679c\u6709\u9650\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6269\u6563\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578bDream-VL\uff0c\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u76f8\u4f3c\u6a21\u578b\u3002</li>\n    <li>Dream-VL\u5728\u89c6\u89c9\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4f18\u4e8e\u9876\u7ea7\u7684\u81ea\u56de\u5f52\u6a21\u578b\u3002</li>\n    <li>\u57fa\u4e8eDream-VL\uff0c\u6211\u4eec\u8fd8\u5f00\u53d1\u4e86Dream-VLA\uff0c\u8fd9\u662f\u4e00\u79cd\u7528\u4e8e\u52a8\u4f5c\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002</li>\n    <li>Dream-VLA\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u9886\u5148\u6a21\u578b\uff0c\u5e76\u5c06Dream-VL\u4e0eDream-VLA\u53d1\u5e03\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoregressive Vision-Language Models (VLMs) have limitations in complex tasks like visual planning and robotic control.</li>\n    <li>This study presents Dream-VL, a new diffusion-based VLM that performs better than previous models and is effective for visual planning.</li>\n    <li>Dream-VLA, an extension of Dream-VL, is developed for Vision-Language-Action tasks using robotic data and has better performance due to its design.</li>\n    <li>Dream-VLA achieves impressive success rates in various benchmarks, outperforming leading models.</li>\n    <li>Both Dream-VL and Dream-VLA are made available for further research in the community.</li>\n</ul>"}, "publishedAt": "2025-12-27T09:46:24.000Z", "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone", "summary": "While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as \u03c0_0 and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22615.png", "numComments": 1, "submittedBy": {"_id": "628c83d186fc004b14e1ed48", "avatarUrl": "/avatars/05ff943a9b89b5f67c5bc254bf45b8f5.svg", "fullname": "Shansan Gong", "name": "Sansa", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22323", "authors": [{"_id": "6953692989916ff627aa4065", "name": "Zhibin Qin", "hidden": false}, {"_id": "6953692989916ff627aa4066", "name": "Zhenxiong Tan", "hidden": false}, {"_id": "6953692989916ff627aa4067", "name": "Zeqing Wang", "hidden": false}, {"_id": "6953692989916ff627aa4068", "name": "Songhua Liu", "hidden": false}, {"_id": "6953692989916ff627aa4069", "name": "Xinchao Wang", "hidden": false}], "publishedAt": "2025-12-26T14:59:41.000Z", "submittedOnDailyAt": "2025-12-30T03:43:24.884Z", "title": "SpotEdit: Selective Region Editing in Diffusion Transformers", "submittedOnDailyBy": {"_id": "640ebdfefdeaae139086f4d8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640ebdfefdeaae139086f4d8/2N94gbHubplYD8njmUTPf.jpeg", "isPro": true, "fullname": "Zhenxiong Tan", "user": "Yuanshi", "type": "user"}, "summary": "Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing.", "upvotes": 27, "discussionId": "6953692989916ff627aa406a", "projectPage": "https://biangbiang0321.github.io/SpotEdit.github.io", "githubRepo": "https://github.com/Biangbiang0321/SpotEdit", "githubRepoAddedBy": "user", "githubStars": 36, "organization": {"_id": "6508ab2b349930913196378b", "name": "NationalUniversityofSingapore", "fullname": "National University of Singapore", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"}, "summary_zh": "<ul>\n    <li>Diffusion Transformer\u6a21\u578b\u63d0\u9ad8\u4e86\u56fe\u50cf\u7f16\u8f91\u6280\u672f\uff0c\u901a\u8fc7\u7f16\u7801\u6761\u4ef6\u56fe\u50cf\u5e76\u6574\u5408\u5230\u53d8\u6362\u5c42\u4e2d\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u5728\u7f16\u8f91\u65f6\u5904\u7406\u6240\u6709\u533a\u57df\uff0c\u9020\u6210\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\u548c\u672a\u4fee\u6539\u533a\u57df\u7684\u8d28\u91cf\u4e0b\u964d\u3002</li>\n    <li>SpotEdit\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u7f16\u8f91\u6846\u67b6\uff0c\u4ec5\u66f4\u65b0\u88ab\u4fee\u6539\u7684\u533a\u57df\u3002</li>\n    <li>SpotEdit\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u90e8\u5206\uff1aSpotSelector\u8bc6\u522b\u7a33\u5b9a\u533a\u57df\uff0cSpotFusion\u901a\u8fc7\u52a8\u6001\u878d\u5408\u673a\u5236\u4fdd\u7559\u7f16\u8f91\u8d28\u91cf\u3002</li>\n    <li>\u901a\u8fc7\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\uff0cSpotEdit\u5b9e\u73b0\u9ad8\u6548\u548c\u7cbe\u51c6\u7684\u56fe\u50cf\u7f16\u8f91\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion Transformer models improve image editing but often process all areas equally, leading to wasted effort on unchanged parts.</li>\n    <li>SpotEdit is a new framework that focuses only on the parts of an image that need editing, avoiding unnecessary computation.</li>\n    <li>It has two main parts: SpotSelector, which identifies stable areas that don't need changes, and SpotFusion, which blends edited and original features effectively.</li>\n    <li>This approach helps maintain image quality while making the editing process more efficient.</li>\n</ul>"}, "publishedAt": "2025-12-26T09:59:41.000Z", "title": "SpotEdit: Selective Region Editing in Diffusion Transformers", "summary": "Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22323.png", "numComments": 2, "submittedBy": {"_id": "640ebdfefdeaae139086f4d8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640ebdfefdeaae139086f4d8/2N94gbHubplYD8njmUTPf.jpeg", "fullname": "Zhenxiong Tan", "name": "Yuanshi", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 170}, "organization": {"_id": "6508ab2b349930913196378b", "name": "NationalUniversityofSingapore", "fullname": "National University of Singapore", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.21252", "authors": [{"_id": "694ca90c746a34b55dd542fc", "user": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "isPro": false, "fullname": "Jiawei Liu", "user": "jwliu-cc", "type": "user"}, "name": "Jiawei Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:31.551Z", "hidden": false}, {"_id": "694ca90c746a34b55dd542fd", "name": "Junqiao Li", "hidden": false}, {"_id": "694ca90c746a34b55dd542fe", "user": {"_id": "660abf8c362a1d713adcee60", "avatarUrl": "/avatars/303bb0a2740659bd4121bb318b119163.svg", "isPro": false, "fullname": "Jiangfan Deng", "user": "afanti3", "type": "user"}, "name": "Jiangfan Deng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-25T20:43:34.372Z", "hidden": false}, {"_id": "694ca90c746a34b55dd542ff", "name": "Gen Li", "hidden": false}, {"_id": "694ca90c746a34b55dd54300", "name": "Siyu Zhou", "hidden": false}, {"_id": "694ca90c746a34b55dd54301", "name": "Zetao Fang", "hidden": false}, {"_id": "694ca90c746a34b55dd54302", "name": "Shanshan Lao", "hidden": false}, {"_id": "694ca90c746a34b55dd54303", "name": "Zengde Deng", "hidden": false}, {"_id": "694ca90c746a34b55dd54304", "name": "Jianing Zhu", "hidden": false}, {"_id": "694ca90c746a34b55dd54305", "name": "Tingting Ma", "hidden": false}, {"_id": "694ca90c746a34b55dd54306", "name": "Jiayi Li", "hidden": false}, {"_id": "694ca90c746a34b55dd54307", "name": "Yunqiu Wang", "hidden": false}, {"_id": "694ca90c746a34b55dd54308", "name": "Qian He", "hidden": false}, {"_id": "694ca90c746a34b55dd54309", "name": "Xinglong Wu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63049b95dae2eb7d083f1bf3/605rnyvIr9b5QeEeARAi1.mp4"], "publishedAt": "2025-12-24T16:00:15.000Z", "submittedOnDailyAt": "2025-12-25T03:53:01.476Z", "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation", "submittedOnDailyBy": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "isPro": false, "fullname": "Jiawei Liu", "user": "jwliu-cc", "type": "user"}, "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.", "upvotes": 22, "discussionId": "694ca90c746a34b55dd5430a", "projectPage": "https://dreamontage.github.io/DreaMontage/", "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u201c\u4e00\u955c\u5230\u5e95\u201d\u6280\u672f\u5728\u7535\u5f71\u5236\u4f5c\u4e2d\u5177\u6709\u72ec\u7279\u7684\u7f8e\u5b66\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u5e38\u53d7\u5230\u9ad8\u6210\u672c\u548c\u590d\u6742\u9650\u5236\u7684\u5f71\u54cd\u3002</li>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u7b80\u5355\u7684\u526a\u8f91\u62fc\u63a5\uff0c\u65e0\u6cd5\u4fdd\u6301\u89c6\u89c9\u6d41\u7545\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51faDreaMontage\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u7528\u6237\u63d0\u4f9b\u7684\u591a\u79cd\u8f93\u5165\u4e2d\u5408\u6210\u65e0\u7f1d\u4e14\u5bcc\u6709\u8868\u73b0\u529b\u7684\u957f\u65f6\u6bb5\u4e00\u955c\u89c6\u9891\u3002</li>\n    <li>\u901a\u8fc7\u5f15\u5165\u8f7b\u91cf\u7ea7\u4e2d\u95f4\u6761\u4ef6\u673a\u5236\u3001\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u8bbe\u8ba1\u6bb5\u843d\u81ea\u56de\u5f52\u63a8\u7406\u7b56\u7565\uff0c\u6211\u4eec\u63d0\u5347\u4e86\u751f\u6210\u5185\u5bb9\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u8fde\u8d2f\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u89c6\u89c9\u6548\u679c\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u5c06\u788e\u7247\u5316\u7684\u89c6\u89c9\u6750\u6599\u8f6c\u5316\u4e3a\u751f\u52a8\u7684\u7535\u5f71\u4f53\u9a8c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The \"one-shot\" filmmaking technique is complex and expensive to implement in real life.</li>\n    <li>DreaMontage is a new framework that helps create long, smooth one-shot videos using user-provided inputs.</li>\n    <li>It improves video generation by using a special conditioning method for better frame control.</li>\n    <li>To make videos look better, a high-quality dataset and techniques for visual expression are used.</li>\n    <li>The method is efficient and allows users to turn disjointed clips into cohesive cinematic experiences.</li>\n</ul>"}, "publishedAt": "2025-12-24T11:00:15.000Z", "title": "DreaMontage: Arbitrary Frame-Guided One-Shot Video Generation", "summary": "The \"one-shot\" technique represents a distinct and sophisticated aesthetic in filmmaking. However, its practical realization is often hindered by prohibitive costs and complex real-world constraints. Although emerging video generation models offer a virtual alternative, existing approaches typically rely on naive clip concatenation, which frequently fails to maintain visual smoothness and temporal coherence. In this paper, we introduce DreaMontage, a comprehensive framework designed for arbitrary frame-guided generation, capable of synthesizing seamless, expressive, and long-duration one-shot videos from diverse user-provided inputs. To achieve this, we address the challenge through three primary dimensions. (i) We integrate a lightweight intermediate-conditioning mechanism into the DiT architecture. By employing an Adaptive Tuning strategy that effectively leverages base training data, we unlock robust arbitrary-frame control capabilities. (ii) To enhance visual fidelity and cinematic expressiveness, we curate a high-quality dataset and implement a Visual Expression SFT stage. In addressing critical issues such as subject motion rationality and transition smoothness, we apply a Tailored DPO scheme, which significantly improves the success rate and usability of the generated content. (iii) To facilitate the production of extended sequences, we design a Segment-wise Auto-Regressive (SAR) inference strategy that operates in a memory-efficient manner. Extensive experiments demonstrate that our approach achieves visually striking and seamlessly coherent one-shot effects while maintaining computational efficiency, empowering users to transform fragmented visual materials into vivid, cohesive one-shot cinematic experiences.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63049b95dae2eb7d083f1bf3/605rnyvIr9b5QeEeARAi1.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.21252.png", "numComments": 1, "submittedBy": {"_id": "63049b95dae2eb7d083f1bf3", "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg", "fullname": "Jiawei Liu", "name": "jwliu-cc", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>DeepSeek-V3.2\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u51fa\u8272\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u6df1\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\uff08DSA\uff09\uff0c\u5728\u5904\u7406\u957f\u6587\u672c\u65f6\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002</li>\n    <li>\u901a\u8fc7\u5f3a\u5927\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0cDeepSeek-V3.2\u7684\u8868\u73b0\u4e0eGPT-5\u76f8\u5f53\uff0c\u4e14\u5176\u9ad8\u8ba1\u7b97\u7248\u672c\u8d85\u8fc7\u4e86GPT-5\u3002</li>\n    <li>DeepSeek-V3.2\u57282025\u5e74\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\u548c\u56fd\u9645\u4fe1\u606f\u5b66\u5965\u6797\u5339\u514b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u91d1\u724c\u6c34\u5e73\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u5927\u89c4\u6a21\u7684\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff0c\u4ee5\u7cfb\u7edf\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model designed for efficient computation and strong reasoning abilities.</li>\n    <li>It uses a technology called DeepSeek Sparse Attention (DSA) to make processing faster while maintaining performance, especially with long inputs.</li>\n    <li>The model includes a powerful reinforcement learning system that allows it to perform well, even matching or exceeding the capabilities of GPT-5.</li>\n    <li>DeepSeek-V3.2-Speciale, a high-compute version, has shown exceptional reasoning skills, winning top awards in prestigious math and programming competitions.</li>\n    <li>It features a new pipeline for creating large amounts of training data, improving its ability to follow instructions and generalize in complex tasks.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u9700\u6c42\u7684\u5feb\u901f\u589e\u957f\uff0c\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u9700\u8981\u66f4\u53ef\u9760\u548c\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002</li>\n    <li>\u76ee\u524d\u7684\u6570\u636e\u5904\u7406\u591a\u4f9d\u8d56\u4e34\u65f6\u811a\u672c\uff0c\u7f3a\u4e4f\u89c4\u8303\uff0c\u5f71\u54cd\u53ef\u91cd\u590d\u6027\u548c\u6a21\u578b\u6570\u636e\u751f\u6210\u652f\u6301\u3002</li>\n    <li>DataFlow\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u652f\u6301\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>\u6846\u67b6\u5305\u542b\u8fd1200\u4e2a\u53ef\u91cd\u7528\u7684\u64cd\u4f5c\u7b26\u548c\u516d\u4e2a\u901a\u7528\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u9002\u7528\u4e8e\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u9886\u57df\u3002</li>\n    <li>\u4f7f\u7528DataFlow\u7684\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6570\u636e\u96c6\uff0c\u63d0\u5347\u4e86\u4e0b\u6e38LLM\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>High demand for quality data in Large Language Models (LLMs) requires better data preparation methods.</li>\n    <li>Current methods are often unorganized and lack consistency, making it hard to reproduce results.</li>\n    <li>DataFlow is a new framework that offers a structured way to prepare data for LLMs, making it modular and reusable.</li>\n    <li>It includes nearly 200 tools for various tasks and allows users to build data pipelines easily.</li>\n    <li>DataFlow improves LLM performance significantly compared to existing datasets and lays the groundwork for future AI development.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u5230\u8ba1\u7b97\u987a\u5e8f\u548c\u957f\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u9650\u5236\uff0c\u5f71\u54cd\u4e86\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u5f62\u8c61\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u5b9e\u65f6\u865a\u62df\u5f62\u8c61\u201d\u6846\u67b6\uff0c\u4f7f\u7528\u4e00\u4e2a140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u6e05\u6670\u5ea6\u548c\u65e0\u9650\u957f\u5ea6\u7684\u865a\u62df\u5f62\u8c61\u751f\u6210\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u6d41\u6c34\u7ebf\u5e76\u884c\uff08TPP\uff09\uff0c\u901a\u8fc7\u591aGPU\u5e76\u884c\u5904\u7406\u53bb\u566a\u6b65\u9aa4\uff0c\u964d\u4f4e\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6d41\u5a92\u4f53\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u6eda\u52a8\u6c89\u6d78\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u52a8\u6001\u8c03\u6574\u5916\u89c2\u4ee5\u4fdd\u6301\u5e8f\u5217\u4e00\u81f4\u6027\uff0c\u51cf\u8f7b\u8eab\u4efd\u6f02\u79fb\u548c\u989c\u8272\u4f2a\u5f71\u3002</li>\n    <li>\u5b9e\u65f6\u865a\u62df\u5f62\u8c61\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e86\u6bcf\u79d220\u5e27\u7684\u751f\u6210\u901f\u5ea6\uff0c\u5f00\u521b\u4e86\u5de5\u4e1a\u957f\u89c6\u9891\u5408\u6210\u5e94\u7528\u7684\u65b0\u6a21\u5f0f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new video generation framework that allows for real-time avatar creation using a powerful 14-billion-parameter diffusion model.</li>\n    <li>The method uses Timestep-forcing Pipeline Parallelism (TPP) to speed up processing by using multiple GPUs, which helps avoid delays.</li>\n    <li>It includes a technique called Rolling Sink Frame Mechanism (RSFM) to keep avatars consistent in appearance and avoid errors in color and identity.</li>\n    <li>Live Avatar can generate high-quality video at 20 frames per second on five GPUs, making it suitable for practical use.</li>\n    <li>This work represents a significant advancement in using diffusion models for long videos in real-time applications. </li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp\u662f\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b210\u4e2a\u4efb\u52a1\uff0c\u6a21\u62df\u4f01\u4e1a\u6570\u636e\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u9700\u8981\u5728\u5de5\u4e1a\u6a21\u5f0f\u4e0a\u8fdb\u884c\u590d\u6742\u7684\u8bbe\u8ba1\u548c\u6784\u5efa\uff0c\u5305\u62ec\u4ece\u5934\u521b\u5efa\u591a\u9636\u6bb5SQL\u7ba1\u9053\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\u4efb\u52a1\u8981\u6c42\u89e3\u51b3\u5f00\u653e\u5f0f\u5546\u4e1a\u95ee\u9898\uff0c\u9700\u8fdb\u884c\u6218\u7565\u89c4\u5212\u548c\u63a2\u7d22\u6027\u5206\u6790\u3002</li>\n    <li>\u5b9e\u9a8c\u663e\u793a\uff0c\u76ee\u524d\u7684\u667a\u80fd\u4ee3\u7406\u5728DAComp\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u4e0a\u6210\u529f\u7387\u4f4e\u4e8e20%\u3002</li>\n    <li>DAComp\u5e2e\u52a9\u8bc6\u522b\u5de5\u7a0b\u548c\u5206\u6790\u7684\u4e0d\u540c\u80fd\u529b\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u81ea\u52a8\u5316\u6570\u636e\u4ee3\u7406\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u5e73\u53f0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DAComp is a new benchmark with 210 tasks that reflects real-world data workflows involving data engineering and data analysis.</li>\n    <li>Data engineering tasks involve creating and modifying complex SQL pipelines, while data analysis tasks require solving open-ended business problems.</li>\n    <li>Tasks are evaluated through performance metrics and assessments by a trained LLM-judge based on detailed rubrics.</li>\n    <li>Current technology struggles with DAComp, especially in data engineering tasks with success rates below 20%, indicating significant challenges in creating effective data systems.</li>\n    <li>DAComp helps identify limitations in current data processing capabilities and aims to improve the development of autonomous data agents in business environments.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni\u662f\u4e00\u4e2a\u901a\u7528\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u76f4\u63a5\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u8be5\u7cfb\u7edf\u6574\u5408\u4e86\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u7b49\u591a\u79cd\u529f\u80fd\uff0c\u63d0\u4f9b\u4e00\u4e2a\u6574\u4f53\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>Kling-Omni\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5982\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\uff0c\u5904\u7406\u6210\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\u3002</li>\n    <li>\u6846\u67b6\u4f9d\u8d56\u4e8e\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u5e76\u91c7\u7528\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u3002</li>\n    <li>Kling-Omni\u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u73b0\u51fa\u5411\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u53d1\u5c55\u7684\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new framework that can create high-quality videos from different types of visual language inputs.</li>\n    <li>It combines video generation, editing, and reasoning tasks into one flexible system, unlike previous methods that separated these functions.</li>\n    <li>The framework accepts various user inputs, such as text, images, and videos, and turns them into a single, cohesive output.</li>\n    <li>Kling-Omni is built on a strong data system and uses advanced training methods to improve performance and efficiency.</li>\n    <li>The framework shows excellent ability in creating videos, editing based on reasoning, and following complex instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.08765", "authors": [{"_id": "6938da63dfc35938ba129f3c", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:07.767Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3d", "name": "Yefei He", "hidden": false}, {"_id": "6938da63dfc35938ba129f3e", "user": {"_id": "62d812e143df7719860d05d1", "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg", "isPro": false, "fullname": "zhekai chen", "user": "Azily", "type": "user"}, "name": "Zhekai Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:00.513Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3f", "name": "Shiwei Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f40", "user": {"_id": "637ee45b2438d7485b8d8f6a", "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg", "isPro": false, "fullname": "Xiaogang Xu", "user": "xiaogang00", "type": "user"}, "name": "Xiaogang Xu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:51.241Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f41", "name": "Bin Xia", "hidden": false}, {"_id": "6938da63dfc35938ba129f42", "name": "Dingdong Wang", "hidden": false}, {"_id": "6938da63dfc35938ba129f43", "name": "Hongwei Yi", "hidden": false}, {"_id": "6938da63dfc35938ba129f44", "user": {"_id": "65d5ec74cd05bc1eaa125040", "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg", "isPro": false, "fullname": "Xihui Liu", "user": "XihuiLiu", "type": "user"}, "name": "Xihui Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:32.582Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f45", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:26.372Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f46", "name": "Yu Liu", "hidden": false}, {"_id": "6938da63dfc35938ba129f47", "name": "Yingya Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f48", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:10.566Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "publishedAt": "2025-12-09T16:13:55.000Z", "submittedOnDailyAt": "2025-12-10T00:20:18.797Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "upvotes": 94, "discussionId": "6938da64dfc35938ba129f49", "githubRepo": "https://github.com/ali-vilab/Wan-Move", "githubRepoAddedBy": "user", "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.", "ai_keywords": ["motion control", "video generative models", "dense point trajectories", "latent space", "spatiotemporal feature map", "motion guidance", "image-to-video model", "auxiliary motion encoders", "fine-tuning", "MoveBench", "motion annotations"], "githubStars": 197, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>Wan-Move\u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5\u63a7\u5236\u7cbe\u5ea6\u4f4e\u548c\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\u3002</li>\n    <li>\u901a\u8fc7\u5bc6\u96c6\u7684\u70b9\u8f68\u8ff9\u8868\u793a\u7269\u4f53\u8fd0\u52a8\uff0c\u5b9e\u73b0\u4e86\u5bf9\u573a\u666f\u7684\u7cbe\u7ec6\u63a7\u5236\u3002</li>\n    <li>Wan-Move\u5c06\u8fd0\u52a8\u6307\u5bfc\u96c6\u6210\u5230\u73b0\u6709\u7684\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\u4e2d\uff0c\u7701\u53bb\u4e86\u8f85\u52a9\u8fd0\u52a8\u7f16\u7801\u5668\u7684\u9700\u8981\u3002</li>\n    <li>\u7ecf\u8fc7\u5927\u89c4\u6a21\u8bad\u7ec3\uff0cWan-Move\u751f\u6210\u7684480p\u89c6\u9891\u5728\u8fd0\u52a8\u63a7\u5236\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u5e76\u4e14\u63d0\u4f9b\u4e86\u8bc4\u4f30\u57fa\u51c6MoveBench\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Wan-Move is a new framework that improves motion control in video generation models.</li>\n    <li>It offers precise control over motion, overcoming problems found in existing methods.</li>\n    <li>The framework uses dense point trajectories to guide how scene elements move in videos.</li>\n    <li>Wan-Move can easily integrate with existing video models without changing their architecture.</li>\n    <li>It produces high-quality videos and includes a benchmark called MoveBench for thorough evaluation.</li>\n</ul>"}, "publishedAt": "2025-12-09T11:13:55.000Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.01816", "authors": [{"_id": "692e5c0537312eaa83fd87b8", "user": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "isPro": false, "fullname": "Juanxi Tian", "user": "Juanxi", "type": "user"}, "name": "Juanxi Tian", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:40:43.760Z", "hidden": false}, {"_id": "692e5c0537312eaa83fd87b9", "name": "Siyuan Li", "hidden": false}, {"_id": "692e5c0537312eaa83fd87ba", "name": "Conghui He", "hidden": false}, {"_id": "692e5c0537312eaa83fd87bb", "name": "Lijun Wu", "hidden": false}, {"_id": "692e5c0537312eaa83fd87bc", "user": {"_id": "64be296a46cc3cdfbb057f7e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64be296a46cc3cdfbb057f7e/jSHeNY2AcPifCZzJyFhr4.jpeg", "isPro": false, "fullname": "Cheng Tan", "user": "chengtan9907", "type": "user"}, "name": "Cheng Tan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:40:41.755Z", "hidden": false}], "publishedAt": "2025-12-01T15:52:31.000Z", "submittedOnDailyAt": "2025-12-02T01:31:46.625Z", "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights", "submittedOnDailyBy": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "isPro": false, "fullname": "Juanxi Tian", "user": "Juanxi", "type": "user"}, "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.", "upvotes": 87, "discussionId": "692e5c0537312eaa83fd87bd", "projectPage": "https://opendatalab-raiser.github.io/Envision/", "githubRepo": "https://github.com/opendatalab-raiser/Envision", "ai_summary": "A benchmark for chained text-to-multi-image generation assesses models' ability to model dynamic causal processes and world knowledge, revealing that unified multimodal models outperform specialized ones but still struggle with spatiotemporal consistency.", "ai_keywords": ["multimodal models", "text-to-image (T2I)", "causal event progression", "spatiotemporal causality", "Envision-a", "Envision-Score", "multi-dimensional consistency", "physicality", "aesthetics", "causal narrative coherence", "spatiotemporal consistency", "multi-frame reasoning", "dynamic world modeling"], "githubStars": 27, "organization": {"_id": "66ce9d1f5e180b9b9c8e6f31", "name": "opendatalab", "fullname": "OpenDataLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"}, "summary_zh": "<ul>\n    <li>\u5f53\u524d\u7684\u591a\u6a21\u6001\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u6765\u514b\u670d\u5355\u4e00\u6a21\u6001\u7684\u5c40\u9650\uff0c\u5e38\u4f7f\u7528\u6587\u672c\u5230\u56fe\u50cf\u7684\u4efb\u52a1\u6765\u786e\u4fdd\u8bed\u4e49\u4e00\u81f4\u6027\u3002</li>\n    <li>\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u5355\u56fe\u50cf\u751f\u6210\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u62df\u5408\u9759\u6001\u6a21\u5f0f\u5339\u914d\uff0c\u9650\u5236\u4e86\u5bf9\u52a8\u6001\u8fc7\u7a0b\u7684\u5efa\u6a21\u80fd\u529b\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u201cEnvision\u201d\u57fa\u51c6\uff0c\u7528\u4e8e\u94fe\u5f0f\u6587\u672c\u5230\u591a\u56fe\u50cf\u751f\u6210\uff0c\u5f3a\u8c03\u65f6\u7a7a\u56e0\u679c\u5173\u7cfb\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86\u201cEnvision-Score\u201d\u6307\u6807\uff0c\u7efc\u5408\u8bc4\u4f30\u6a21\u578b\u7684\u591a\u7ef4\u4e00\u81f4\u6027\u3001\u7269\u7406\u6027\u548c\u7f8e\u5b66\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u4e13\u7528\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u7f8e\u5b66\u8868\u73b0\u4e0a\u4f18\u79c0\uff0c\u4f46\u7f3a\u4e4f\u5185\u5728\u7684\u4e16\u754c\u77e5\u8bc6\uff0c\u800c\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578b\u5728\u56e0\u679c\u53d9\u8ff0\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u66f4\u597d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current multimodal models focus on combining understanding and generation, mainly using text-to-image tasks.</li>\n    <li>These models struggle because they train on single images, which makes them good at matching patterns but poor at handling dynamic processes over time.</li>\n    <li>To improve this, a new benchmark called Envision is proposed to test models on generating sequences of images based on causal events.</li>\n    <li>Envision includes 1,000 prompts from various fields and introduces a new metric, Envision-Score, to assess models on consistency and adherence to real-world knowledge.</li>\n    <li>Tests show that while specialized models excel at aesthetics, unified models perform better in creating coherent narratives, but all models still face challenges in maintaining consistency over time.</li>\n</ul>"}, "publishedAt": "2025-12-01T10:52:31.000Z", "title": "Envision: Benchmarking Unified Understanding & Generation for Causal World Process Insights", "summary": "Current multimodal models aim to transcend the limitations of single-modality representations by unifying understanding and generation, often using text-to-image (T2I) tasks to calibrate semantic consistency. However, their reliance on static, single-image generation in training and evaluation leads to overfitting to static pattern matching and semantic fusion, while fundamentally hindering their ability to model dynamic processes that unfold over time. To address these constraints, we propose Envision-a causal event progression benchmark for chained text-to-multi-image generation. Grounded in world knowledge and structured by spatiotemporal causality, it reorganizes existing evaluation dimensions and includes 1,000 four-stage prompts spanning six scientific and humanities domains. To transition evaluation from single images to sequential frames and assess whether models truly internalize world knowledge while adhering to causal-temporal constraints, we introduce Envision-Score, a holistic metric integrating multi-dimensional consistency, physicality, and aesthetics. Comprehensive evaluation of 15 models (10 specialized T2I models, 5 unified models) uncovers: specialized T2I models demonstrate proficiency in aesthetic rendering yet lack intrinsic world knowledge. Unified multimodal models bridge this gap, consistently outperforming specialized counterparts in causal narrative coherence. However, even these unified architectures remain subordinate to closed-source models and struggle to overcome the core challenge of spatiotemporal consistency. This demonstrates that a focus on causally-isolated single images impedes multi-frame reasoning and generation, promoting static pattern matching over dynamic world modeling-ultimately limiting world knowledge internalization, generation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01816.png", "numComments": 4, "submittedBy": {"_id": "670880950e79a8b46f7ff9dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670880950e79a8b46f7ff9dd/hA1TLhwlQblkFsq8wLrkB.jpeg", "fullname": "Juanxi Tian", "name": "Juanxi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13}, "organization": {"_id": "66ce9d1f5e180b9b9c8e6f31", "name": "opendatalab", "fullname": "OpenDataLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/639c3afa7432f2f5d16b7296/yqxxBknyeqkGnYsjoaR4M.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u57fa\u7840\u6a21\u578b\u751f\u6210\u7684\u5185\u5bb9\u5728\u89c6\u89c9\u4e0a\u771f\u5b9e\uff0c\u4f46\u5176\u4f5c\u4e3a\u4e16\u754c\u6a21\u62df\u5668\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u662f\u5426\u6355\u6349\u5230\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7ea6\u675f\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u6307\u6807\uff08\u5982FVD\uff09\u4e3b\u8981\u5173\u6ce8\u611f\u77e5\u8d28\u91cf\uff0c\u5ffd\u89c6\u4e86\u56e0\u679c\u5173\u7cfb\u3001\u7269\u7406\u548c\u5168\u7403\u4e00\u81f4\u6027\u7684\u63a8\u7406\u5931\u8d25\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MMGR\uff08\u591a\u6a21\u6001\u751f\u6210\u63a8\u7406\u8bc4\u4f30\u4e0e\u57fa\u51c6\uff09\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\u6765\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u3002</li>\n    <li>MMGR\u8bc4\u4f30\u4e09\u4e2a\u9886\u57df\u7684\u751f\u6210\u63a8\u7406\uff0c\u5305\u62ec\u62bd\u8c61\u63a8\u7406\u3001\u5177\u4f53\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\uff0c\u5e76\u5e94\u7528\u7ec6\u81f4\u7684\u8bc4\u4f30\u6307\u6807\u3002</li>\n    <li>\u8bc4\u6d4b\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u7269\u7406\u5e38\u8bc6\u4efb\u52a1\u4e0a\u8868\u73b0\u9002\u4e2d\uff0c\u4f46\u5728\u62bd\u8c61\u63a8\u7406\u548c\u957f\u65f6\u95f4\u7a7a\u95f4\u89c4\u5212\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video foundation models can create realistic videos, but they may not always follow real-world rules like physics and logic.</li>\n    <li>Current evaluation methods focus too much on visual quality and miss important reasoning errors.</li>\n    <li>MMGR is a new evaluation system that checks five reasoning skills: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>\n    <li>It tests these reasoning skills in different areas, such as solving puzzles and navigating real environments.</li>\n    <li>Benchmarking shows that while some models perform well in certain areas, they struggle with complex reasoning tasks and maintaining consistency.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.01374", "authors": [{"_id": "692e6bf937312eaa83fd8890", "user": {"_id": "610b70452719facd4ea85e28", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg", "isPro": false, "fullname": "Chujie Zheng", "user": "chujiezheng", "type": "user"}, "name": "Chujie Zheng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:27.206Z", "hidden": false}, {"_id": "692e6bf937312eaa83fd8891", "name": "Kai Dang", "hidden": false}, {"_id": "692e6bf937312eaa83fd8892", "name": "Bowen Yu", "hidden": false}, {"_id": "692e6bf937312eaa83fd8893", "name": "Mingze Li", "hidden": false}, {"_id": "692e6bf937312eaa83fd8894", "user": {"_id": "6278bd42541f3d2dfa77ea70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6278bd42541f3d2dfa77ea70/ejn49eapnB3UXQckAYdTd.jpeg", "isPro": false, "fullname": "Huiqiang Jiang", "user": "iofu728", "type": "user"}, "name": "Huiqiang Jiang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:17:41.367Z", "hidden": false}, {"_id": "692e6bf937312eaa83fd8895", "name": "Junrong Lin", "hidden": false}, {"_id": "692e6bf937312eaa83fd8896", "name": "Yuqiong Liu", "hidden": false}, {"_id": "692e6bf937312eaa83fd8897", "user": {"_id": "62088594a5943c8a8fc94560", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644733028938-62088594a5943c8a8fc94560.png", "isPro": false, "fullname": "An Yang", "user": "yangapku", "type": "user"}, "name": "An Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:25.208Z", "hidden": false}, {"_id": "692e6bf937312eaa83fd8898", "name": "Jingren Zhou", "hidden": false}, {"_id": "692e6bf937312eaa83fd8899", "name": "Junyang Lin", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63d9d68c1cae35c27bf7a6a7/sMajVHMznJ4kLJvdY1HwJ.png"], "publishedAt": "2025-12-01T07:45:39.000Z", "submittedOnDailyAt": "2025-12-02T02:47:49.367Z", "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices", "submittedOnDailyBy": {"_id": "63d9d68c1cae35c27bf7a6a7", "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg", "isPro": false, "fullname": "Bowen Yu", "user": "Tigerph", "type": "user"}, "summary": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.", "upvotes": 78, "discussionId": "692e6bfa37312eaa83fd889a", "ai_summary": "The paper provides a theoretical foundation for optimizing sequence-level rewards in reinforcement learning using token-level objectives, highlighting the importance of techniques like importance sampling correction, clipping, and Routing Replay for stabilizing training, especially with large language models.", "ai_keywords": ["reinforcement learning", "large language models", "sequence-level reward", "token-level objective", "policy gradient methods", "REINFORCE", "first-order approximation", "training-inference discrepancy", "policy staleness", "importance sampling correction", "clipping", "Routing Replay", "Mixture-of-Experts", "on-policy training", "off-policy updates"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u901a\u8fc7\u4e00\u9636\u8fd1\u4f3c\uff0c\u7814\u7a76\u8868\u660e\uff0c\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u4e00\u81f4\u6027\u6700\u597d\u65f6\uff0c\u5e8f\u5217\u7ea7\u5956\u52b1\u53ef\u4ee5\u901a\u8fc7\u66ff\u4ee3\u7684\u6807\u8bb0\u7ea7\u76ee\u6807\u8fdb\u884c\u4f18\u5316\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u591a\u79cd\u7a33\u5b9a\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6280\u672f\uff0c\u5305\u62ec\u91cd\u8981\u6027\u91c7\u6837\u4fee\u6b63\u3001\u526a\u5207\u548c\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u7684\u8def\u7531\u91cd\u653e\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u91cd\u8981\u6027\u91c7\u6837\u4fee\u6b63\u7684\u57fa\u672c\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u5728\u5728\u7ebf\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u4e00\u65e6\u8bad\u7ec3\u7a33\u5b9a\uff0c\u5ef6\u957f\u4f18\u5316\u8fc7\u7a0b\u53ef\u4ee5\u83b7\u5f97\u4e0e\u521d\u59cb\u72b6\u6001\u65e0\u5173\u7684\u76f8\u4f3c\u6700\u7ec8\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>This paper presents a new way to improve reinforcement learning (RL) using large language models.</li>\n    <li>It explains how optimizing rewards at the token level can help achieve better sequence-level rewards.</li>\n    <li>Key techniques like importance sampling, clipping, and Routing Replay are shown to help stabilize RL training.</li>\n    <li>Experiments with a large model demonstrate that using importance sampling leads to better training stability.</li>\n    <li>Once training is stable, performance remains consistent, regardless of the starting conditions.</li>\n</ul>"}, "publishedAt": "2025-12-01T02:45:39.000Z", "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices", "summary": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63d9d68c1cae35c27bf7a6a7/sMajVHMznJ4kLJvdY1HwJ.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.01374.png", "numComments": 2, "submittedBy": {"_id": "63d9d68c1cae35c27bf7a6a7", "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg", "fullname": "Bowen Yu", "name": "Tigerph", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 17}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u79d1\u5b66\u4e00\u822c\u667a\u80fd\uff08SGI\uff09\u662f\u6307\u80fd\u591f\u81ea\u4e3b\u6784\u601d\u3001\u7814\u7a76\u548c\u63a8\u7406\u79d1\u5b66\u9886\u57df\u7684\u80fd\u529b\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\u3002</li>\n    <li>\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b9e\u8df5\u63a2\u7a76\u6a21\u578b\uff08PIM\uff09\u7684SGI\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u4e0e\u79d1\u5b66\u5bb6\u76f8\u5173\u7684\u4efb\u52a1\u8fdb\u884c\u64cd\u4f5c\u5316\u3002</li>\n    <li>SGI-Bench\u5305\u62ec\u8d85\u8fc71000\u4e2a\u8de8\u5b66\u79d1\u7684\u6837\u672c\uff0c\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u6700\u65b0\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\u4e86\u4e00\u4e9b\u4e0d\u8db3\uff1a\u6df1\u5ea6\u7814\u7a76\u7684\u51c6\u786e\u5339\u914d\u7387\u4f4e\uff0810-20%\uff09\uff0c\u521b\u610f\u7f3a\u4e4f\u53ef\u884c\u6027\u548c\u7ec6\u8282\uff0c\u5e72\u5b9e\u9a8c\u7684\u6267\u884c\u7ed3\u679c\u51c6\u786e\u6027\u4f4e\u7b49\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\uff0c\u65e8\u5728\u5728\u63a8\u7406\u65f6\u4f18\u5316\u65b0\u9896\u6027\u5956\u52b1\uff0c\u589e\u5f3a\u5047\u8bbe\u7684\u65b0\u9896\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is no complete framework for Scientific General Intelligence (SGI), which is the ability of AI to think, research, and reason in science on its own.</li>\n    <li>The authors propose a definition of SGI based on a model called the Practical Inquiry Model (PIM), and outline four tasks aligned with how scientists work.</li>\n    <li>They created a benchmark called SGI-Bench, which includes over 1,000 tasks based on major scientific questions to evaluate AI performance.</li>\n    <li>Results show that current AI struggles with deep research accuracy, generating feasible ideas, and executing experiments properly.</li>\n    <li>The authors introduce a new method called Test-Time Reinforcement Learning (TTRL) to improve AI's ability to create novel hypotheses during evaluation.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Dec 31, 2025";
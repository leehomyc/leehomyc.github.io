window.trendingPapers = {
    "today": [{"paper": {"id": "2512.20619", "authors": [{"_id": "694b614d746a34b55dd53d1a", "name": "Jianhong Bai", "hidden": false}, {"_id": "694b614d746a34b55dd53d1b", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "694b614d746a34b55dd53d1c", "name": "Xintao Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1d", "name": "Fu Xiao", "hidden": false}, {"_id": "694b614d746a34b55dd53d1e", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1f", "name": "Qinghe Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d20", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "694b614d746a34b55dd53d21", "name": "Menghan Xia", "hidden": false}, {"_id": "694b614d746a34b55dd53d22", "name": "Zuozhu Liu", "hidden": false}, {"_id": "694b614d746a34b55dd53d23", "name": "Haoji Hu", "hidden": false}, {"_id": "694b614d746a34b55dd53d24", "name": "Pengfei Wan", "hidden": false}, {"_id": "694b614d746a34b55dd53d25", "name": "Kun Gai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "publishedAt": "2025-12-23T18:59:56.000Z", "submittedOnDailyAt": "2025-12-24T01:20:51.117Z", "title": "SemanticGen: Video Generation in Semantic Space", "submittedOnDailyBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "isPro": false, "fullname": "Jianhong Bai", "user": "jianhongbai", "type": "user"}, "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "upvotes": 77, "discussionId": "694b614d746a34b55dd53d26", "projectPage": "https://jianhongbai.github.io/SemanticGen/", "ai_summary": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.", "ai_keywords": ["VAE space", "VAE decoder", "semantic space", "diffusion model", "semantic video features", "bi-directional attention"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>SemanticGen\u662f\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u7684\u6162\u6536\u655b\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002</li>\n    <li>\u8be5\u6a21\u578b\u9996\u5148\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u7279\u5f81\uff0c\u7136\u540e\u518d\u6dfb\u52a0\u9ad8\u9891\u7ec6\u8282\uff0c\u4f7f\u751f\u6210\u8fc7\u7a0b\u66f4\u9ad8\u6548\u3002</li>\n    <li>SemanticGen\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u8fc7\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u7d27\u51d1\u7684\u8bed\u4e49\u89c6\u9891\u7279\u5f81\uff0c\u7b2c\u4e8c\u9636\u6bb5\u6839\u636e\u8fd9\u4e9b\u7279\u5f81\u751f\u6210\u6700\u7ec8\u7684\u89c6\u9891\u3002</li>\n    <li>\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6bd4\u5728VAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u66f4\u5feb\u6536\u655b\uff0c\u4e14\u5bf9\u4e8e\u957f\u89c6\u9891\u751f\u6210\u4e5f\u66f4\u6709\u6548\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSemanticGen\u751f\u6210\u7684\u89c6\u9891\u8d28\u91cf\u9ad8\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u5148\u8fdb\u65b9\u6cd5\u548c\u5f3a\u57fa\u7ebf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Traditional video generative models use a complex approach that can be slow and resource-intensive for long videos.</li>\n    <li>SemanticGen is a new method that generates videos in a simpler, high-level semantic space for better efficiency.</li>\n    <li>It uses a two-stage process: first, it creates a general layout of the video, then adds detailed features.</li>\n    <li>This approach results in faster video generation and works well for longer videos.</li>\n    <li>Tests show that SemanticGen produces high-quality videos and outperforms existing methods.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:56.000Z", "title": "SemanticGen: Video Generation in Semantic Space", "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png", "numComments": 2, "submittedBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "fullname": "Jianhong Bai", "name": "jianhongbai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19673", "authors": [{"_id": "694ac3ad746a34b55dd53b6c", "name": "Yuqiao Tan", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b6d", "name": "Minzheng Wang", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b6e", "name": "Shizhu He", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b6f", "name": "Huanxuan Liao", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b70", "name": "Chengfeng Zhao", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b71", "name": "Qiunan Lu", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b72", "name": "Tian Liang", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b73", "name": "Jun Zhao", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b74", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-22T18:51:48.000Z", "submittedOnDailyAt": "2025-12-24T00:28:45.252Z", "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies", "submittedOnDailyBy": {"_id": "64bcc373ef8c0e42bf16acc5", "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg", "isPro": false, "fullname": "mz.w", "user": "iiiiwis", "type": "user"}, "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.", "upvotes": 49, "discussionId": "694ac3ad746a34b55dd53b75", "githubRepo": "https://github.com/Trae1ounG/BuPO", "githubRepoAddedBy": "user", "ai_summary": "The paper decomposes the policy of large language models into internal layer and modular policies, revealing distinct reasoning patterns across layers and proposing Bottom-up Policy Optimization to enhance performance on complex reasoning tasks.", "ai_keywords": ["reinforcement learning", "large language models", "Transformer residual stream", "unembedding matrix", "Internal Layer Policies", "Internal Modular Policies", "self-attention", "feed-forward network", "entropy", "Bottom-up Policy Optimization"], "githubStars": 22, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u4e00\u4e2a\u7edf\u4e00\u7684\u7b56\u7565\uff0c\u5ffd\u89c6\u4e86\u5176\u5185\u90e8\u673a\u5236\u3002</li>\n    <li>\u672c\u6587\u901a\u8fc7\u5206\u6790Transformer\u7684\u6b8b\u5dee\u6d41\u548c\u9690\u85cf\u72b6\u6001\uff0c\u5206\u89e3\u8bed\u8a00\u6a21\u578b\u7b56\u7565\uff0c\u63ed\u793a\u5185\u90e8\u5c42\u7ea7\u653f\u7b56\u548c\u6a21\u5757\u653f\u7b56\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u65e9\u671f\u5c42\u4fdd\u6301\u9ad8\u71b5\u4ee5\u63a2\u7d22\uff0c\u800c\u9876\u5c42\u8d8b\u5411\u4e8e\u8fd1\u96f6\u71b5\u4ee5\u8fdb\u884c\u7cbe\u7ec6\u5316\u5904\u7406\uff0c\u4e0d\u540c\u6a21\u578b\u7cfb\u5217\u7684\u6536\u655b\u6a21\u5f0f\u4e0d\u540c\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u81ea\u4e0b\u800c\u4e0a\u7684\u653f\u7b56\u4f18\u5316(BuPO)\uff0c\u5728\u65e9\u671f\u8bad\u7ec3\u4e2d\u76f4\u63a5\u4f18\u5316\u5185\u90e8\u5c42\u653f\u7b56\uff0c\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\u548c\u6027\u80fd\u3002</li>\n    <li>\u5728\u590d\u6742\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study breaks down how large language models (LLMs) work internally, focusing on their different layers and components.</li>\n    <li>It introduces Internal Layer Policies and Internal Modular Policies to understand contributions from specific parts of the model.</li>\n    <li>Findings show that early layers of the model explore more, while top layers refine predictions, with different models showing unique patterns.</li>\n    <li>A new reinforcement learning method called Bottom-up Policy Optimization (BuPO) is proposed, which improves training by focusing on lower layers.</li>\n    <li>BuPO has been shown to enhance reasoning capabilities and perform better on complex tasks according to experiments.</li>\n</ul>"}, "publishedAt": "2025-12-22T13:51:48.000Z", "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies", "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19673.png", "numComments": 4, "submittedBy": {"_id": "64bcc373ef8c0e42bf16acc5", "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg", "fullname": "mz.w", "name": "iiiiwis", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.20618", "authors": [{"_id": "694ba02a746a34b55dd53e8b", "name": "Runtao Liu", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8c", "name": "Ziyi Liu", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8d", "name": "Jiaqi Tang", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8e", "name": "Yue Ma", "hidden": false}, {"_id": "694ba02a746a34b55dd53e8f", "name": "Renjie Pi", "hidden": false}, {"_id": "694ba02a746a34b55dd53e90", "name": "Jipeng Zhang", "hidden": false}, {"_id": "694ba02a746a34b55dd53e91", "name": "Qifeng Chen", "hidden": false}], "publishedAt": "2025-12-23T18:59:49.000Z", "submittedOnDailyAt": "2025-12-24T05:57:23.776Z", "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos", "submittedOnDailyBy": {"_id": "642e7a12ccdcf5da7f9657a0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e7a12ccdcf5da7f9657a0/w8jW5BagTuTp6EvC6KEyR.png", "isPro": true, "fullname": "Jiaqi Tang", "user": "Jiaqi-hkust", "type": "user"}, "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.", "upvotes": 38, "discussionId": "694ba02a746a34b55dd53e92", "ai_summary": "A multi-agent framework, involving a master LLM, grounding agent, and vision agent, enhances long-video QA by improving temporal grounding and leveraging visual and textual data.", "ai_keywords": ["multimodal LLMs", "long-video QA", "multi-agent framework", "grounding agent", "vision agent", "reinforcement learning", "temporal grounding", "LongTVQA", "LongTVQA+"], "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u7531\u4e00\u4e2a\u4e3bLLM\u534f\u8c03\u4e00\u4e2a\u5b9a\u4f4d\u4ee3\u7406\u548c\u4e00\u4e2a\u89c6\u89c9\u4ee3\u7406\u3002</li>\n    <li>\u5b9a\u4f4d\u4ee3\u7406\u5e2e\u52a9\u627e\u5230\u4e0e\u95ee\u9898\u76f8\u5173\u7684\u7247\u6bb5\uff0c\u89c6\u89c9\u4ee3\u7406\u63d0\u53d6\u76ee\u6807\u6587\u672c\u89c2\u5bdf\u3002</li>\n    <li>\u4e3b\u4ee3\u7406\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u4ee5\u9f13\u52b1\u7b80\u6d01\u3001\u6b63\u786e\u548c\u9ad8\u6548\u7684\u591a\u4ee3\u7406\u5408\u4f5c\u3002</li>\n    <li>\u5728LongTVQA\u548cLongTVQA+\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u7cfb\u7edf\u663e\u8457\u4f18\u4e8e\u5f3a\u5927\u7684\u975e\u4ee3\u7406\u57fa\u51c6\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u4e86\u4ee3\u7406\u7684\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent developments in multimodal systems allow for better understanding of long videos, but many methods still lose important details.</li>\n    <li>We suggest a new framework using multiple agents: a master agent that coordinates tasks, a grounding agent that finds relevant video sections, and a vision agent that extracts specific information.</li>\n    <li>The master agent uses a step limit and is trained with reinforcement learning to improve teamwork and efficiency.</li>\n    <li>This approach helps focus on important video clips and provides detailed visual context along with subtitles.</li>\n    <li>Our system shows significant improvements in video question answering compared to traditional methods, and reinforcement learning enhances its performance further.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:49.000Z", "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos", "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20618.png", "numComments": 1, "submittedBy": {"_id": "642e7a12ccdcf5da7f9657a0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642e7a12ccdcf5da7f9657a0/w8jW5BagTuTp6EvC6KEyR.png", "fullname": "Jiaqi Tang", "name": "Jiaqi-hkust", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "isAuthorParticipating": false}, {"paper": {"id": "2512.20617", "authors": [{"_id": "694b58e3746a34b55dd53cff", "name": "Yuxi Xiao", "hidden": false}, {"_id": "694b58e3746a34b55dd53d00", "name": "Longfei Li", "hidden": false}, {"_id": "694b58e3746a34b55dd53d01", "name": "Shen Yan", "hidden": false}, {"_id": "694b58e3746a34b55dd53d02", "name": "Xinhang Liu", "hidden": false}, {"_id": "694b58e3746a34b55dd53d03", "name": "Sida Peng", "hidden": false}, {"_id": "694b58e3746a34b55dd53d04", "name": "Yunchao Wei", "hidden": false}, {"_id": "694b58e3746a34b55dd53d05", "name": "Xiaowei Zhou", "hidden": false}, {"_id": "694b58e3746a34b55dd53d06", "name": "Bingyi Kang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hWsrMM0K13mB2Ej9Zwgbp.mp4"], "publishedAt": "2025-12-23T18:59:46.000Z", "submittedOnDailyAt": "2025-12-24T00:38:28.003Z", "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.", "upvotes": 34, "discussionId": "694b58e4746a34b55dd53d07", "projectPage": "https://spatialtree.github.io/", "ai_summary": "SpatialTree, a cognitive-science-inspired hierarchy, evaluates and improves spatial abilities in MLLMs across multiple levels, revealing transfer dynamics and proposing an auto-think strategy for consistent performance enhancement.", "ai_keywords": ["SpatialTree", "low-level perception", "mental mapping", "simulation", "agentic competence", "capability-centric hierarchical benchmark", "targeted supervised fine-tuning", "negative transfer", "cross-level transfer", "naive RL", "auto-think strategy"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86SpatialTree\uff0c\u4e00\u4e2a\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u7684\u7a7a\u95f4\u80fd\u529b\u5c42\u7ea7\u6a21\u578b\uff0c\u5206\u4e3a\u56db\u4e2a\u5c42\u6b21\uff1a\u4f4e\u7ea7\u611f\u77e5\uff08L1\uff09\u3001\u5fc3\u7406\u6620\u5c04\uff08L2\uff09\u3001\u6a21\u62df\uff08L3\uff09\u548c\u4ee3\u7406\u80fd\u529b\uff08L4\uff09\u3002</li>\n    <li>\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u80fd\u529b\u4e2d\u5fc3\u7684\u5c42\u7ea7\u57fa\u51c6\uff0c\u8bc4\u4f3027\u79cd\u7a7a\u95f4\u5b50\u80fd\u529b\uff0c\u5e76\u53d1\u73b0L1\u6280\u80fd\u76f8\u4e92\u72ec\u7acb\uff0c\u800c\u9ad8\u7ea7\u6280\u80fd\u4e4b\u95f4\u76f8\u5173\u6027\u5f3a\u3002</li>\n    <li>\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u53d1\u73b0L1\u5b58\u5728\u8d1f\u8fc1\u79fb\uff0c\u4f46\u4f4e\u7ea7\u5230\u9ad8\u7ea7\u80fd\u529b\u4e4b\u95f4\u7684\u8fc1\u79fb\u8868\u73b0\u51fa\u663e\u8457\u7684\u534f\u540c\u6548\u5e94\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u7b80\u5355\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u53ef\u4ee5\u6539\u5584\u6240\u6709\u5c42\u6b21\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u6291\u5236\u4e0d\u5fc5\u8981\u7684\u601d\u8003\uff0c\u907f\u514d\u5f71\u54cd\u76f4\u89c2\u611f\u77e5\u3002</li>\n    <li>SpatialTree\u4e3a\u7406\u89e3\u548c\u7cfb\u7edf\u6027\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a7a\u95f4\u80fd\u529b\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\u6846\u67b6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Spatial abilities develop in stages, from basic perception to advanced reasoning and interaction.</li>\n    <li>The authors created a framework called SpatialTree, which organizes spatial abilities into four levels: perception, mental mapping, simulation, and agentic competence.</li>\n    <li>They evaluated various multimodal LLMs based on this framework, finding that lower-level skills are mostly independent, while higher-level skills are interconnected.</li>\n    <li>Through fine-tuning, they discovered that while low-level abilities can negatively transfer, higher-level skills benefit from low-level training.</li>\n    <li>The authors propose a new strategy that minimizes unnecessary thinking, improving overall performance across all levels of spatial abilities in MLLMs.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:46.000Z", "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs", "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/hWsrMM0K13mB2Ej9Zwgbp.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20617.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 189}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.18746", "authors": [{"_id": "694b7f3a746a34b55dd53e09", "name": "Guibin Zhang", "hidden": false}, {"_id": "694b7f3a746a34b55dd53e0a", "name": "Haotian Ren", "hidden": false}, {"_id": "694b7f3a746a34b55dd53e0b", "name": "Chong Zhan", "hidden": false}, {"_id": "694b7f3a746a34b55dd53e0c", "name": "Zhenhong Zhou", "hidden": false}, {"_id": "694b7f3a746a34b55dd53e0d", "name": "Junhao Wang", "hidden": false}, {"_id": "694b7f3a746a34b55dd53e0e", "name": "He Zhu", "hidden": false}, {"_id": "694b7f3a746a34b55dd53e0f", "name": "Wangchunshu Zhou", "hidden": false}, {"_id": "694b7f3a746a34b55dd53e10", "name": "Shuicheng Yan", "hidden": false}], "publishedAt": "2025-12-21T14:26:14.000Z", "submittedOnDailyAt": "2025-12-24T03:54:35.992Z", "title": "MemEvolve: Meta-Evolution of Agent Memory Systems", "submittedOnDailyBy": {"_id": "660d17d6c9be0dcd31a30b3d", "avatarUrl": "/avatars/3743fe9b695c488ebe33f0d8fd607a8a.svg", "isPro": false, "fullname": "Zhou Heng", "user": "henggg", "type": "user"}, "summary": "Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to 17.06%; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.", "upvotes": 19, "discussionId": "694b7f3a746a34b55dd53e11", "ai_summary": "MemEvolve, a meta-evolutionary framework, enhances self-evolving memory systems by jointly evolving agents' experiential knowledge and memory architecture, leading to improved performance and generalization.", "ai_keywords": ["self-evolving memory systems", "large language model", "LLM-based agents", "memory architectures", "meta-evolutionary framework", "MemEvolve", "EvolveLab", "modular design space", "encode", "store", "retrieve", "manage", "SmolAgent", "Flash-Searcher"], "summary_zh": "<ul>\n    <li>\u81ea\u6211\u8fdb\u5316\u7684\u8bb0\u5fc6\u7cfb\u7edf\u6b63\u5728\u6539\u53d8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7684\u6f14\u53d8\u8303\u5f0f\u3002</li>\n    <li>\u4ee5\u5f80\u7684\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1\u7684\u8bb0\u5fc6\u67b6\u6784\u6765\u5b58\u50a8\u7ecf\u9a8c\u548c\u5de5\u5177\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u591a\u6837\u5316\u7684\u4efb\u52a1\u73af\u5883\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MemEvolve\uff0c\u4e00\u4e2a\u5143\u8fdb\u5316\u6846\u67b6\uff0c\u53ef\u4ee5\u540c\u65f6\u8fdb\u5316\u4ee3\u7406\u7684\u7ecf\u9a8c\u77e5\u8bc6\u548c\u8bb0\u5fc6\u67b6\u6784\u3002</li>\n    <li>MemEvolve\u7684\u5b9e\u65bd\u57fa\u4e8eEvolveLab\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u81ea\u6211\u8fdb\u5316\u8bb0\u5fc6\u4ee3\u7801\u5e93\uff0c\u5305\u542b\u4e86\u5341\u4e8c\u79cd\u4ee3\u8868\u6027\u7684\u8bb0\u5fc6\u7cfb\u7edf\u3002</li>\n    <li>\u5728\u56db\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8bc4\u4f30\u663e\u793a\uff0cMemEvolve\u5728\u6027\u80fd\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u5e76\u80fd\u6709\u6548\u8de8\u4efb\u52a1\u548c\u8de8\u6a21\u578b\u8fc1\u79fb\u8bb0\u5fc6\u67b6\u6784\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Self-evolving memory systems are changing how large language model-based agents evolve and learn.</li>\n    <li>Previous methods used fixed memory structures that couldn't adapt to different tasks, limiting their effectiveness.</li>\n    <li>MemEvolve is a new framework that improves both the agents' knowledge and their memory systems, allowing better learning and adaptation.</li>\n    <li>EvolveLab is a codebase that organizes different memory systems into a modular format for easier experimentation and improvement.</li>\n    <li>Tests show that MemEvolve significantly boosts performance and allows memory systems to work well across various tasks and models.</li>\n</ul>"}, "publishedAt": "2025-12-21T09:26:14.000Z", "title": "MemEvolve: Meta-Evolution of Agent Memory Systems", "summary": "Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to 17.06%; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.18746.png", "numComments": 1, "submittedBy": {"_id": "660d17d6c9be0dcd31a30b3d", "avatarUrl": "/avatars/3743fe9b695c488ebe33f0d8fd607a8a.svg", "fullname": "Zhou Heng", "name": "henggg", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "isAuthorParticipating": false}, {"paper": {"id": "2512.20491", "authors": [{"_id": "694b5132746a34b55dd53c4e", "name": "Chen Hu", "hidden": false}, {"_id": "694b5132746a34b55dd53c4f", "name": "Haikuo Du", "hidden": false}, {"_id": "694b5132746a34b55dd53c50", "name": "Heng Wang", "hidden": false}, {"_id": "694b5132746a34b55dd53c51", "name": "Lin Lin", "hidden": false}, {"_id": "694b5132746a34b55dd53c52", "name": "Mingrui Chen", "hidden": false}, {"_id": "694b5132746a34b55dd53c53", "name": "Peng Liu", "hidden": false}, {"_id": "694b5132746a34b55dd53c54", "name": "Ruihang Miao", "hidden": false}, {"_id": "694b5132746a34b55dd53c55", "name": "Tianchi Yue", "hidden": false}, {"_id": "694b5132746a34b55dd53c56", "name": "Wang You", "hidden": false}, {"_id": "694b5132746a34b55dd53c57", "name": "Wei Ji", "hidden": false}, {"_id": "694b5132746a34b55dd53c58", "name": "Wei Yuan", "hidden": false}, {"_id": "694b5132746a34b55dd53c59", "name": "Wenjin Deng", "hidden": false}, {"_id": "694b5132746a34b55dd53c5a", "name": "Xiaojian Yuan", "hidden": false}, {"_id": "694b5132746a34b55dd53c5b", "name": "Xiaoyun Zhang", "hidden": false}, {"_id": "694b5132746a34b55dd53c5c", "name": "Xiangyu Liu", "hidden": false}, {"_id": "694b5132746a34b55dd53c5d", "name": "Xikai Liu", "hidden": false}, {"_id": "694b5132746a34b55dd53c5e", "name": "Yanming Xu", "hidden": false}, {"_id": "694b5132746a34b55dd53c5f", "name": "Yicheng Cao", "hidden": false}, {"_id": "694b5132746a34b55dd53c60", "name": "Yifei Zhang", "hidden": false}, {"_id": "694b5132746a34b55dd53c61", "name": "Yongyao Wang", "hidden": false}, {"_id": "694b5132746a34b55dd53c62", "name": "Yubo Shu", "hidden": false}, {"_id": "694b5132746a34b55dd53c63", "name": "Yurong Zhang", "hidden": false}, {"_id": "694b5132746a34b55dd53c64", "name": "Yuxiang Zhang", "hidden": false}, {"_id": "694b5132746a34b55dd53c65", "name": "Zheng Gong", "hidden": false}, {"_id": "694b5132746a34b55dd53c66", "name": "Zhichao Chang", "hidden": false}, {"_id": "694b5132746a34b55dd53c67", "name": "Binyan Li", "hidden": false}, {"_id": "694b5132746a34b55dd53c68", "name": "Dan Ma", "hidden": false}, {"_id": "694b5132746a34b55dd53c69", "name": "Furong Jia", "hidden": false}, {"_id": "694b5132746a34b55dd53c6a", "name": "Hongyuan Wang", "hidden": false}, {"_id": "694b5132746a34b55dd53c6b", "name": "Jiayu Liu", "hidden": false}, {"_id": "694b5132746a34b55dd53c6c", "name": "Jing Bai", "hidden": false}, {"_id": "694b5132746a34b55dd53c6d", "name": "Junlan Liu", "hidden": false}, {"_id": "694b5132746a34b55dd53c6e", "name": "Manjiao Liu", "hidden": false}, {"_id": "694b5132746a34b55dd53c6f", "name": "Na Wang", "hidden": false}, {"_id": "694b5132746a34b55dd53c70", "name": "Qiuping Wu", "hidden": false}, {"_id": "694b5132746a34b55dd53c71", "name": "Qinxin Du", "hidden": false}, {"_id": "694b5132746a34b55dd53c72", "name": "Shiwei Li", "hidden": false}, {"_id": "694b5132746a34b55dd53c73", "name": "Wen Sun", "hidden": false}, {"_id": "694b5132746a34b55dd53c74", "name": "Yifeng Gong", "hidden": false}, {"_id": "694b5132746a34b55dd53c75", "name": "Yonglin Chen", "hidden": false}, {"_id": "694b5132746a34b55dd53c76", "name": "Yuling Zhao", "hidden": false}, {"_id": "694b5132746a34b55dd53c77", "name": "Yuxuan Lin", "hidden": false}, {"_id": "694b5132746a34b55dd53c78", "name": "Ziqi Ren", "hidden": false}, {"_id": "694b5132746a34b55dd53c79", "name": "Zixuan Wang", "hidden": false}, {"_id": "694b5132746a34b55dd53c7a", "name": "Aihu Zhang", "hidden": false}, {"_id": "694b5132746a34b55dd53c7b", "name": "Brian Li", "hidden": false}, {"_id": "694b5132746a34b55dd53c7c", "name": "Buyun Ma", "hidden": false}, {"_id": "694b5132746a34b55dd53c7d", "name": "Kang An", "hidden": false}, {"_id": "694b5132746a34b55dd53c7e", "name": "Li Xie", "hidden": false}, {"_id": "694b5132746a34b55dd53c7f", "name": "Mingliang Li", "hidden": false}, {"_id": "694b5132746a34b55dd53c80", "name": "Pan Li", "hidden": false}, {"_id": "694b5132746a34b55dd53c81", "name": "Shidong Yang", "hidden": false}, {"_id": "694b5132746a34b55dd53c82", "name": "Xi Chen", "hidden": false}, {"_id": "694b5132746a34b55dd53c83", "name": "Xiaojia Liu", "hidden": false}, {"_id": "694b5132746a34b55dd53c84", "name": "Yuchu Luo", "hidden": false}, {"_id": "694b5132746a34b55dd53c85", "name": "Yuan Song", "hidden": false}, {"_id": "694b5132746a34b55dd53c86", "name": "YuanHao Ding", "hidden": false}, {"_id": "694b5132746a34b55dd53c87", "name": "Yuanwei Liang", "hidden": false}, {"_id": "694b5132746a34b55dd53c88", "name": "Zexi Li", "hidden": false}, {"_id": "694b5132746a34b55dd53c89", "name": "Zhaoning Zhang", "hidden": false}, {"_id": "694b5132746a34b55dd53c8a", "name": "Zixin Zhang", "hidden": false}, {"_id": "694b5132746a34b55dd53c8b", "name": "Binxing Jiao", "hidden": false}, {"_id": "694b5132746a34b55dd53c8c", "name": "Daxin Jiang", "hidden": false}, {"_id": "694b5132746a34b55dd53c8d", "name": "Jiansheng Chen", "hidden": false}, {"_id": "694b5132746a34b55dd53c8e", "name": "Jing Li", "hidden": false}, {"_id": "694b5132746a34b55dd53c8f", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "694b5132746a34b55dd53c90", "name": "Yibo Zhu", "hidden": false}], "publishedAt": "2025-12-23T16:32:27.000Z", "submittedOnDailyAt": "2025-12-24T00:04:36.581Z", "title": "Step-DeepResearch Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.", "upvotes": 12, "discussionId": "694b5132746a34b55dd53c91", "ai_summary": "Step-DeepResearch, an end-to-end agent enhanced with a data synthesis strategy and progressive training, achieves expert-level capabilities in deep research scenarios, outperforming established models.", "ai_keywords": ["Deep Research", "BrowseComp", "Step-DeepResearch", "Data Synthesis Strategy", "Atomic Capabilities", "agentic mid-training", "SFT", "RL", "Checklist-style Judger", "ADR-Bench", "Scale AI Research Rubrics", "OpenAI", "Gemini DeepResearch"], "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u6df1\u5ea6\u7814\u7a76\u6210\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u91cd\u8981\u6307\u6807\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u6ee1\u8db3\u771f\u5b9e\u7814\u7a76\u9700\u6c42\u3002</li>\n    <li>\u63d0\u51faStep-DeepResearch\uff0c\u8fd9\u662f\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u667a\u80fd\u4f53\uff0c\u589e\u5f3a\u4e86\u89c4\u5212\u548c\u62a5\u544a\u5199\u4f5c\u80fd\u529b\u3002</li>\n    <li>\u91c7\u7528\u57fa\u4e8e\u539f\u5b50\u80fd\u529b\u7684\u6570\u636e\u5408\u6210\u7b56\u7565\uff0c\u5e76\u5b9e\u65bd\u9010\u6b65\u8bad\u7ec3\u8def\u5f84\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002</li>\n    <li>\u5efa\u7acbADR-Bench\uff0c\u4ee5\u8bc4\u4f30\u4e2d\u6587\u9886\u57df\u7684\u6df1\u5ea6\u7814\u7a76\u573a\u666f\uff0c\u586b\u8865\u8bc4\u4f30\u7a7a\u767d\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cStep-DeepResearch\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8bc1\u660e\u4e2d\u578b\u6a21\u578b\u4e5f\u80fd\u4ee5\u9886\u5148\u6210\u672c\u6548\u7387\u8fbe\u5230\u4e13\u5bb6\u7ea7\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Deep Research is important as LLMs become more like independent agents, but current benchmarks aren't suitable for real-world research tasks.</li>\n    <li>Step-DeepResearch is a new, cost-effective agent designed to improve research skills like understanding intent, making decisions, and verifying information.</li>\n    <li>This approach uses a unique Data Synthesis Strategy and a structured training process to enhance planning and writing abilities.</li>\n    <li>Step-DeepResearch includes a Checklist-style Judger to boost its reliability and effectiveness.</li>\n    <li>It outperforms other models in tests and shows that medium-sized models can achieve high-level research skills at a lower cost.</li>\n</ul>"}, "publishedAt": "2025-12-23T11:32:27.000Z", "title": "Step-DeepResearch Technical Report", "summary": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20491.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 189}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.17102", "authors": [{"_id": "694b5c01746a34b55dd53d0f", "name": "Jiongxiao Wang", "hidden": false}, {"_id": "694b5c01746a34b55dd53d10", "name": "Qiaojing Yan", "hidden": false}, {"_id": "694b5c01746a34b55dd53d11", "name": "Yawei Wang", "hidden": false}, {"_id": "694b5c01746a34b55dd53d12", "name": "Yijun Tian", "hidden": false}, {"_id": "694b5c01746a34b55dd53d13", "name": "Soumya Smruti Mishra", "hidden": false}, {"_id": "694b5c01746a34b55dd53d14", "name": "Zhichao Xu", "hidden": false}, {"_id": "694b5c01746a34b55dd53d15", "name": "Megha Gandhi", "hidden": false}, {"_id": "694b5c01746a34b55dd53d16", "name": "Panpan Xu", "hidden": false}, {"_id": "694b5c01746a34b55dd53d17", "name": "Lin Lee Cheong", "hidden": false}], "publishedAt": "2025-12-18T21:58:19.000Z", "submittedOnDailyAt": "2025-12-24T00:52:22.721Z", "title": "Reinforcement Learning for Self-Improving Agent with Skill Library", "submittedOnDailyBy": {"_id": "638a968ac432da48c7139ccf", "avatarUrl": "/avatars/29d08fc35074654b7f6418667847b206.svg", "isPro": false, "fullname": "Jiongxiao Wang", "user": "Jayfeather1024", "type": "user"}, "summary": "Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.", "upvotes": 12, "discussionId": "694b5c01746a34b55dd53d18", "ai_summary": "A novel RL framework, SAGE, enhances LLM-based agents' self-improvement capabilities by systematically incorporating skills from a skill library, leading to better performance and efficiency in new environments.", "ai_keywords": ["Reinforcement Learning", "RL", "Skill Augmented GRPO for self-Evolution", "SAGE", "Sequential Rollout", "Skill-integrated Reward", "Scenario Goal Completion", "interaction steps", "tokens"], "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u590d\u6742\u63a8\u7406\u548c\u591a\u8f6e\u4e92\u52a8\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u65b0\u73af\u5883\u4e2d\u96be\u4ee5\u6301\u7eed\u6539\u8fdb\u3002</li>\n    <li>\u63d0\u51fa\u4f7f\u7528\u6280\u80fd\u5e93\u6765\u5e2e\u52a9\u4ee3\u7406\u5b66\u4e60\u548c\u5e94\u7528\u65b0\u6280\u80fd\uff0c\u4f46\u73b0\u6709\u7684\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u63d0\u793a\uff0c\u5b9e\u65bd\u8d77\u6765\u6709\u6311\u6218\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u540d\u4e3a\u6280\u80fd\u589e\u5f3a\u81ea\u6211\u8fdb\u5316\uff08SAGE\uff09\uff0c\u4ee5\u63d0\u9ad8\u4ee3\u7406\u7684\u81ea\u6211\u6539\u8fdb\u80fd\u529b\u3002</li>\n    <li>SAGE\u6846\u67b6\u901a\u8fc7\u987a\u5e8f\u56de\u5408\u7684\u65b9\u5f0f\uff0c\u5728\u7c7b\u4f3c\u4efb\u52a1\u7684\u94fe\u6761\u4e2d\u9010\u6b65\u90e8\u7f72\u4ee3\u7406\uff0c\u79ef\u7d2f\u6280\u80fd\u4ee5\u4f9b\u540e\u7eed\u4efb\u52a1\u4f7f\u7528\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cSAGE\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b8c\u6210\u76ee\u6807\u7684\u6210\u529f\u7387\u63d0\u9ad8\u4e868.9%\uff0c\u4e14\u4ea4\u4e92\u6b65\u9aa4\u51cf\u5c11\u4e8626%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Model agents are good at complex tasks but struggle to adapt in new situations.</li>\n    <li>A new method uses skill libraries to help agents learn and apply new skills more effectively.</li>\n    <li>The proposed framework, called SAGE, uses Reinforcement Learning to improve self-learning with skills.</li>\n    <li>SAGE involves a process called Sequential Rollout, where agents learn from similar tasks over time.</li>\n    <li>Tests show SAGE improves task completion rates and efficiency compared to current methods.</li>\n</ul>"}, "publishedAt": "2025-12-18T16:58:19.000Z", "title": "Reinforcement Learning for Self-Improving Agent with Skill Library", "summary": "Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in complex reasoning and multi-turn interactions but struggle to continuously improve and adapt when deployed in new environments. One promising approach is implementing skill libraries that allow agents to learn, validate, and apply new skills. However, current skill library approaches rely primarily on LLM prompting, making consistent skill library implementation challenging. To overcome these challenges, we propose a Reinforcement Learning (RL)-based approach to enhance agents' self-improvement capabilities with a skill library. Specifically, we introduce Skill Augmented GRPO for self-Evolution (SAGE), a novel RL framework that systematically incorporates skills into learning. The framework's key component, Sequential Rollout, iteratively deploys agents across a chain of similar tasks for each rollout. As agents navigate through the task chain, skills generated from previous tasks accumulate in the library and become available for subsequent tasks. Additionally, the framework enhances skill generation and utilization through a Skill-integrated Reward that complements the original outcome-based rewards. Experimental results on AppWorld demonstrate that SAGE, when applied to supervised-finetuned model with expert experience, achieves 8.9% higher Scenario Goal Completion while requiring 26% fewer interaction steps and generating 59% fewer tokens, substantially outperforming existing approaches in both accuracy and efficiency.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17102.png", "numComments": 1, "submittedBy": {"_id": "638a968ac432da48c7139ccf", "avatarUrl": "/avatars/29d08fc35074654b7f6418667847b206.svg", "fullname": "Jiongxiao Wang", "name": "Jayfeather1024", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": false}, {"paper": {"id": "2512.18099", "authors": [{"_id": "694aa9bcd8d7445f420c171d", "name": "Bowen Shi", "hidden": false}, {"_id": "694aa9bcd8d7445f420c171e", "name": "Andros Tjandra", "hidden": false}, {"_id": "694aa9bcd8d7445f420c171f", "name": "John Hoffman", "hidden": false}, {"_id": "694aa9bcd8d7445f420c1720", "name": "Helin Wang", "hidden": false}, {"_id": "694aa9bcd8d7445f420c1721", "name": "Yi-Chiao Wu", "hidden": false}, {"_id": "694aa9bcd8d7445f420c1722", "name": "Luya Gao", "hidden": false}, {"_id": "694aa9bcd8d7445f420c1723", "name": "Julius Richter", "hidden": false}, {"_id": "694aa9bcd8d7445f420c1724", "name": "Matt Le", "hidden": false}, {"_id": "694aa9bcd8d7445f420c1725", "name": "Apoorv Vyas", "hidden": false}, {"_id": "694aa9bcd8d7445f420c1726", "name": "Sanyuan Chen", "hidden": false}, {"_id": "694aa9bcd8d7445f420c1727", "name": "Christoph Feichtenhofer", "hidden": false}, {"_id": "694aa9bcd8d7445f420c1728", "name": "Piotr Doll\u00e1r", "hidden": false}, {"_id": "694aa9bcd8d7445f420c1729", "name": "Wei-Ning Hsu", "hidden": false}, {"_id": "694aa9bcd8d7445f420c172a", "name": "Ann Lee", "hidden": false}], "publishedAt": "2025-12-19T22:14:23.000Z", "submittedOnDailyAt": "2025-12-24T00:06:36.953Z", "title": "SAM Audio: Segment Anything in Audio", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "General audio source separation is a key capability for multimodal AI systems that can perceive and reason about sound. Despite substantial progress in recent years, existing separation models are either domain-specific, designed for fixed categories such as speech or music, or limited in controllability, supporting only a single prompting modality such as text. In this work, we present SAM Audio, a foundation model for general audio separation that unifies text, visual, and temporal span prompting within a single framework. Built on a diffusion transformer architecture, SAM Audio is trained with flow matching on large-scale audio data spanning speech, music, and general sounds, and can flexibly separate target sources described by language, visual masks, or temporal spans. The model achieves state-of-the-art performance across a diverse suite of benchmarks, including general sound, speech, music, and musical instrument separation in both in-the-wild and professionally produced audios, substantially outperforming prior general-purpose and specialized systems. Furthermore, we introduce a new real-world separation benchmark with human-labeled multimodal prompts and a reference-free evaluation model that correlates strongly with human judgment.", "upvotes": 10, "discussionId": "694aa9bdd8d7445f420c172b", "projectPage": "https://ai.meta.com/samaudio/", "githubRepo": "https://github.com/facebookresearch/sam-audio", "githubRepoAddedBy": "user", "ai_summary": "SAM Audio, a diffusion transformer-based foundation model, achieves superior performance in general audio separation using unified text, visual, and temporal span prompts across various audio types.", "ai_keywords": ["diffusion transformer architecture", "flow matching", "audio separation", "general sound", "speech", "music", "musical instrument separation", "in-the-wild audio", "professionally produced audio", "real-world separation benchmark", "human-labeled multimodal prompts", "reference-free evaluation model"], "githubStars": 2494, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "summary_zh": "<ul>\n    <li>SAM Audio\u662f\u4e00\u4e2a\u901a\u7528\u97f3\u9891\u5206\u79bb\u7684\u57fa\u7840\u6a21\u578b\uff0c\u652f\u6301\u6587\u672c\u3001\u89c6\u89c9\u548c\u65f6\u95f4\u8303\u56f4\u7684\u63d0\u793a\u3002</li>\n    <li>\u8be5\u6a21\u578b\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\uff0c\u5e76\u5728\u5927\u91cf\u97f3\u9891\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5305\u62ec\u8bed\u97f3\u3001\u97f3\u4e50\u548c\u4e00\u822c\u58f0\u97f3\u3002</li>\n    <li>SAM Audio\u80fd\u7075\u6d3b\u5730\u5206\u79bb\u7528\u8bed\u8a00\u3001\u89c6\u89c9\u63a9\u7801\u6216\u65f6\u95f4\u8303\u56f4\u63cf\u8ff0\u7684\u76ee\u6807\u97f3\u6e90\u3002</li>\n    <li>\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSAM Audio\u7684\u8868\u73b0\u8d85\u8fc7\u4e86\u4ee5\u524d\u7684\u901a\u7528\u548c\u4e13\u4e1a\u7cfb\u7edf\u3002</li>\n    <li>\u7814\u7a76\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u73b0\u5b9e\u4e16\u754c\u5206\u79bb\u57fa\u51c6\uff0c\u5305\u542b\u4eba\u7c7b\u6807\u8bb0\u7684\u591a\u6a21\u6001\u63d0\u793a\u548c\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u7684\u65e0\u53c2\u8003\u8bc4\u4f30\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>SAM Audio is a new model for separating different audio sources, designed for use in AI systems that understand sound.</li>\n    <li>Unlike previous models, SAM Audio can handle various types of audio, including speech and music, and can be controlled using text, visual cues, and time spans.</li>\n    <li>The model is built on advanced technology and trained with a large amount of audio data, allowing it to perform well in various scenarios.</li>\n    <li>SAM Audio shows better performance than other existing audio separation systems in tests involving different types of audio.</li>\n    <li>The research also includes a new benchmark for evaluating audio separation using human feedback and a method that aligns well with people's opinions.</li>\n</ul>"}, "publishedAt": "2025-12-19T17:14:23.000Z", "title": "SAM Audio: Segment Anything in Audio", "summary": "General audio source separation is a key capability for multimodal AI systems that can perceive and reason about sound. Despite substantial progress in recent years, existing separation models are either domain-specific, designed for fixed categories such as speech or music, or limited in controllability, supporting only a single prompting modality such as text. In this work, we present SAM Audio, a foundation model for general audio separation that unifies text, visual, and temporal span prompting within a single framework. Built on a diffusion transformer architecture, SAM Audio is trained with flow matching on large-scale audio data spanning speech, music, and general sounds, and can flexibly separate target sources described by language, visual masks, or temporal spans. The model achieves state-of-the-art performance across a diverse suite of benchmarks, including general sound, speech, music, and musical instrument separation in both in-the-wild and professionally produced audios, substantially outperforming prior general-purpose and specialized systems. Furthermore, we introduce a new real-world separation benchmark with human-labeled multimodal prompts and a reference-free evaluation model that correlates strongly with human judgment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.18099.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 189}, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16144", "authors": [{"_id": "694b7f94746a34b55dd53e1d", "name": "Prime Intellect Team", "hidden": false}, {"_id": "694b7f94746a34b55dd53e1e", "name": "Mika Senghaas", "hidden": false}, {"_id": "694b7f94746a34b55dd53e1f", "name": "Fares Obeid", "hidden": false}, {"_id": "694b7f94746a34b55dd53e20", "name": "Sami Jaghouar", "hidden": false}, {"_id": "694b7f94746a34b55dd53e21", "name": "William Brown", "hidden": false}, {"_id": "694b7f94746a34b55dd53e22", "name": "Jack Min Ong", "hidden": false}, {"_id": "694b7f94746a34b55dd53e23", "name": "Daniel Auras", "hidden": false}, {"_id": "694b7f94746a34b55dd53e24", "name": "Matej Sirovatka", "hidden": false}, {"_id": "694b7f94746a34b55dd53e25", "name": "Jannik Straube", "hidden": false}, {"_id": "694b7f94746a34b55dd53e26", "name": "Andrew Baker", "hidden": false}, {"_id": "694b7f94746a34b55dd53e27", "name": "Sebastian M\u00fcller", "hidden": false}, {"_id": "694b7f94746a34b55dd53e28", "name": "Justus Mattern", "hidden": false}, {"_id": "694b7f94746a34b55dd53e29", "name": "Manveer Basra", "hidden": false}, {"_id": "694b7f94746a34b55dd53e2a", "name": "Aiman Ismail", "hidden": false}, {"_id": "694b7f94746a34b55dd53e2b", "name": "Dominik Scherm", "hidden": false}, {"_id": "694b7f94746a34b55dd53e2c", "name": "Cooper Miller", "hidden": false}, {"_id": "694b7f94746a34b55dd53e2d", "name": "Ameen Patel", "hidden": false}, {"_id": "694b7f94746a34b55dd53e2e", "name": "Simon Kirsten", "hidden": false}, {"_id": "694b7f94746a34b55dd53e2f", "name": "Mario Sieg", "hidden": false}, {"_id": "694b7f94746a34b55dd53e30", "name": "Christian Reetz", "hidden": false}, {"_id": "694b7f94746a34b55dd53e31", "name": "Kemal Erdem", "hidden": false}, {"_id": "694b7f94746a34b55dd53e32", "name": "Vincent Weisser", "hidden": false}, {"_id": "694b7f94746a34b55dd53e33", "name": "Johannes Hagemann", "hidden": false}], "publishedAt": "2025-12-18T03:57:01.000Z", "submittedOnDailyAt": "2025-12-24T03:23:51.710Z", "title": "INTELLECT-3: Technical Report", "submittedOnDailyBy": {"_id": "631ce4b244503b72277fc89f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677431596830-631ce4b244503b72277fc89f.jpeg", "isPro": true, "fullname": "Quentin Gallou\u00e9dec", "user": "qgallouedec", "type": "user"}, "summary": "We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.", "upvotes": 8, "discussionId": "694b7f94746a34b55dd53e34", "ai_summary": "INTELLECT-3, a large Mixture-of-Experts model trained with reinforcement learning, achieves top performance across various benchmarks and is supported by an open-source RL infrastructure framework.", "ai_keywords": ["Mixture-of-Experts", "reinforcement learning", "RL infrastructure", "prime-rl", "asynchronous reinforcement learning", "GLM-4.5-Air-Base", "SFT", "H200s"], "organization": {"_id": "656ec1d908bd4deb79a0ba70", "name": "PrimeIntellect", "fullname": "Prime Intellect", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61e020e4a343274bb132e138/H2mcdPRWtl4iKLd-OYYBc.jpeg"}, "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86INTELLECT-3\uff0c\u8fd9\u662f\u4e00\u4e2a\u62e5\u67091060\u4ebf\u53c2\u6570\u7684\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0812\u4ebf\u6d3b\u8dc3\u53c2\u6570\uff09\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5728\u6570\u5b66\u3001\u4ee3\u7801\u3001\u79d1\u5b66\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u5353\u8d8a\uff0c\u8d85\u8fc7\u4e86\u8bb8\u591a\u66f4\u5927\u7684\u524d\u6cbf\u6a21\u578b\u3002</li>\n    <li>\u6211\u4eec\u5c06\u8be5\u6a21\u578b\u53ca\u5176\u521b\u5efa\u6240\u9700\u7684\u5b8c\u6574\u57fa\u7840\u8bbe\u65bd\u5f00\u6e90\uff0c\u5305\u62ec\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3001\u5b8c\u6574\u914d\u65b9\u548c\u591a\u79cd\u8bad\u7ec3\u8bc4\u4f30\u73af\u5883\u3002</li>\n    <li>\u63a8\u51fa\u4e86prime-rl\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u89c4\u6a21\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u7684\u5f00\u653e\u6846\u67b6\uff0c\u652f\u6301\u4ece\u5355\u8282\u70b9\u6269\u5c55\u5230\u6570\u5343\u4e2aGPU\uff0c\u9002\u5408\u590d\u6742\u7684\u4ea4\u4e92\u548c\u5de5\u5177\u4f7f\u7528\u3002</li>\n    <li>\u4f7f\u7528\u6b64\u57fa\u7840\u8bbe\u65bd\uff0c\u6211\u4eec\u5728GLM-4.5-Air-Base\u6a21\u578b\u4e0a\u8fdb\u884cSFT\u548cRL\u8bad\u7ec3\uff0c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6548\u7387\u9ad8\uff0c\u6700\u591a\u53ef\u6269\u5c55\u5230512\u4e2aH200s\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>INTELLECT-3 is a new AI model with 106 billion parameters, using a mixture of experts with 12 billion active at a time.</li>\n    <li>This model performs exceptionally well in tasks related to math, code, science, and reasoning, surpassing many larger models.</li>\n    <li>The model and its training framework are open-sourced, including the necessary tools and environments for training and evaluation.</li>\n    <li>Prime-rl is a new framework for large-scale reinforcement learning, designed for efficient training using many GPUs.</li>\n    <li>Training for INTELLECT-3 combines supervised fine-tuning and reinforcement learning, achieving high efficiency with a large number of GPUs.</li>\n</ul>"}, "publishedAt": "2025-12-17T22:57:01.000Z", "title": "INTELLECT-3: Technical Report", "summary": "We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16144.png", "numComments": 1, "submittedBy": {"_id": "631ce4b244503b72277fc89f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677431596830-631ce4b244503b72277fc89f.jpeg", "fullname": "Quentin Gallou\u00e9dec", "name": "qgallouedec", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 560}, "organization": {"_id": "656ec1d908bd4deb79a0ba70", "name": "PrimeIntellect", "fullname": "Prime Intellect", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61e020e4a343274bb132e138/H2mcdPRWtl4iKLd-OYYBc.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.20182", "authors": [{"_id": "694b7923746a34b55dd53df4", "name": "Shuzheng Si", "hidden": false}, {"_id": "694b7923746a34b55dd53df5", "name": "Qingyi Wang", "hidden": false}, {"_id": "694b7923746a34b55dd53df6", "name": "Haozhe Zhao", "hidden": false}, {"_id": "694b7923746a34b55dd53df7", "name": "Yuzhuo Bai", "hidden": false}, {"_id": "694b7923746a34b55dd53df8", "name": "Guanqiao Chen", "hidden": false}, {"_id": "694b7923746a34b55dd53df9", "name": "Kangyang Luo", "hidden": false}, {"_id": "694b7923746a34b55dd53dfa", "name": "Gang Chen", "hidden": false}, {"_id": "694b7923746a34b55dd53dfb", "name": "Fanchao Qi", "hidden": false}, {"_id": "694b7923746a34b55dd53dfc", "name": "Minjia Zhang", "hidden": false}, {"_id": "694b7923746a34b55dd53dfd", "name": "Baobao Chang", "hidden": false}, {"_id": "694b7923746a34b55dd53dfe", "name": "Maosong Sun", "hidden": false}], "publishedAt": "2025-12-23T09:20:32.000Z", "submittedOnDailyAt": "2025-12-24T02:56:50.413Z", "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination", "submittedOnDailyBy": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "summary": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.", "upvotes": 5, "discussionId": "694b7923746a34b55dd53dff", "githubRepo": "https://github.com/S1s-Z/FaithLens", "githubRepoAddedBy": "user", "ai_summary": "FaithLens, a cost-efficient faithfulness hallucination detection model using advanced LLMs for training data synthesis and rule-based reinforcement learning, outperforms models like GPT-4.1 and o3 on 12 tasks with high-quality explanations.", "ai_keywords": ["faithfulness hallucination", "FaithLens", "large language models (LLMs)", "retrieval-augmented generation", "summarization", "binary predictions", "explanations", "data filtering strategy", "cold start", "rule-based reinforcement learning", "prediction correctness", "explanation quality", "data diversity", "trustworthiness", "efficiency", "effectiveness"], "githubStars": 5, "organization": {"_id": "6904703d1c4c3a34bf765739", "name": "TsinghuaNLP", "fullname": "Tsinghua NLP Group", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60965a5d206218bf2b0e00ae/4_JDQKsQDyLK0oGegQCl7.png"}, "summary_zh": "<ul>\n    <li>FaithLens \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u4e2d\u7684\u771f\u5b9e\u5ea6\u5e7b\u89c9\u3002</li>\n    <li>\u8be5\u6a21\u578b\u53ef\u4ee5\u540c\u65f6\u63d0\u4f9b\u4e8c\u5143\u9884\u6d4b\u548c\u76f8\u5e94\u7684\u89e3\u91ca\uff0c\u589e\u5f3a\u53ef\u4fe1\u5ea6\u3002</li>\n    <li>\u901a\u8fc7\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u91c7\u53d6\u6570\u636e\u8fc7\u6ee4\u7b56\u7565\uff0c\u786e\u4fdd\u6807\u7b7e\u51c6\u786e\u6027\u548c\u89e3\u91ca\u8d28\u91cf\u3002</li>\n    <li>\u572812\u4e2a\u4e0d\u540c\u4efb\u52a1\u4e0a\uff0cFaithLens \u7684\u8868\u73b0\u8d85\u8fc7\u4e86GPT-4.1\u7b49\u5148\u8fdb\u6a21\u578b\u3002</li>\n    <li>FaithLens \u80fd\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u89e3\u91ca\uff0c\u5e73\u8861\u4e86\u53ef\u4fe1\u5ea6\u3001\u6548\u7387\u548c\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>FaithLens is a new model that helps detect whether outputs from large language models are trustworthy and accurate.</li>\n    <li>It provides not only a yes or no answer about faithfulness but also explanations for its predictions.</li>\n    <li>The model was trained using high-quality, diverse data created with advanced language models and a careful filtering process.</li>\n    <li>FaithLens was further improved using a method called reinforcement learning to enhance prediction accuracy and explanation quality.</li>\n    <li>In tests, FaithLens performed better than other advanced models and maintained a good balance between trustworthiness, speed, and effectiveness.</li>\n</ul>"}, "publishedAt": "2025-12-23T04:20:32.000Z", "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination", "summary": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20182.png", "numComments": 1, "submittedBy": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "fullname": "ssz", "name": "ssz1111", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "6904703d1c4c3a34bf765739", "name": "TsinghuaNLP", "fullname": "Tsinghua NLP Group", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60965a5d206218bf2b0e00ae/4_JDQKsQDyLK0oGegQCl7.png"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\uff0c\u9700\u8981\u66f4\u53ef\u9760\u548c\u53ef\u6269\u5c55\u7684\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u3002</li>\n    <li>\u76ee\u524d\u7684\u6570\u636e\u51c6\u5907\u5927\u591a\u4f9d\u8d56\u4e34\u65f6\u811a\u672c\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\uff0c\u5f71\u54cd\u4e86\u91cd\u73b0\u6027\u548c\u6570\u636e\u751f\u6210\u7684\u652f\u6301\u3002</li>\n    <li>\u4e3a\u5e94\u5bf9\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86DataFlow\uff0c\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u51c6\u5907\u6846\u67b6\uff0c\u652f\u6301\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>DataFlow\u5305\u542b\u7ea6200\u4e2a\u53ef\u91cd\u7528\u7684\u64cd\u4f5c\u7b26\u548c\u516d\u4e2a\u901a\u7528\u6d41\u7a0b\uff0c\u8986\u76d6\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u9886\u57df\u3002</li>\n    <li>DataFlow\u5728\u591a\u79cd\u5e94\u7528\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38LLM\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6570\u636e\u51c6\u5907\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There's a growing need for high-quality data in Large Language Models (LLMs), but current data preparation methods are often messy and not reliable.</li>\n    <li>DataFlow is a new framework that helps create better data preparation processes, allowing for easier and more efficient data transformations.</li>\n    <li>It includes nearly 200 reusable tools and six pipelines for different tasks like text processing and code generation.</li>\n    <li>DataFlow-Agent can turn simple language instructions into working data pipelines automatically.</li>\n    <li>Using DataFlow has shown to improve LLM performance in various tasks, outperforming existing datasets and methods significantly.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni \u662f\u4e00\u4e2a\u901a\u7528\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u76f4\u63a5\u4ece\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5206\u79bb\u95ee\u9898\u3002</li>\n    <li>Kling-Omni \u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5982\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\uff0c\u5904\u7406\u6210\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5efa\u7acb\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u4f18\u5316\u57fa\u7840\u8bbe\u65bd\u6765\u589e\u5f3a\u80fd\u529b\u3002</li>\n    <li>Kling-Omni \u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6709\u52a9\u4e8e\u53d1\u5c55\u80fd\u591f\u611f\u77e5\u3001\u63a8\u7406\u548c\u4e0e\u590d\u6742\u4e16\u754c\u4e92\u52a8\u7684\u591a\u6a21\u6001\u6a21\u62df\u5668\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system for creating high-quality videos from various inputs like text, images, and videos.</li>\n    <li>It combines video generation, editing, and reasoning into one unified framework, unlike older methods that were separate.</li>\n    <li>The system processes different types of user inputs into a single format to produce impressive video content.</li>\n    <li>Kling-Omni is built on a strong data system and uses advanced training techniques for better performance.</li>\n    <li>It shows excellent results in generating content, editing based on reasoning, and following complex instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u79d1\u5b66\u901a\u7528\u667a\u80fd\uff08SGI\uff09\u5c1a\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\uff0c\u6307\u7684\u662f\u80fd\u591f\u81ea\u4e3b\u6784\u601d\u3001\u7814\u7a76\u548c\u63a8\u7406\u79d1\u5b66\u9886\u57df\u7684\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u57fa\u4e8e\u5b9e\u8df5\u63a2\u7a76\u6a21\u578b\uff08PIM\uff09\u63d0\u51fa\u4e86\u4e00\u79cd\u64cd\u4f5c\u6027SGI\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u79d1\u5b66\u5bb6\u76f8\u5173\u4efb\u52a1\u8fdb\u884c\u5b9e\u73b0\uff1a\u6df1\u5ea6\u7814\u7a76\u3001\u521b\u610f\u751f\u6210\u3001\u5e72\u5b9e\u9a8c/\u6e7f\u5b9e\u9a8c\u548c\u5b9e\u9a8c\u63a8\u7406\u3002</li>\n    <li>SGI-Bench\u5305\u542b1000\u591a\u4e2a\u8de8\u5b66\u79d1\u7684\u6837\u672c\uff0c\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u6700\u65b0\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u6df1\u5ea6\u7814\u7a76\u7684\u51c6\u786e\u5339\u914d\u5ea6\u4f4e\uff0810-20%\uff09\uff0c\u521b\u610f\u7f3a\u4e4f\u53ef\u884c\u6027\u548c\u7ec6\u8282\uff0c\u5e72\u5b9e\u9a8c\u7684\u4ee3\u7801\u53ef\u6267\u884c\u6027\u9ad8\u4f46\u7ed3\u679c\u51c6\u786e\u6027\u4f4e\uff0c\u6e7f\u5b9e\u9a8c\u7684\u8fc7\u7a0b\u4fdd\u771f\u5ea6\u4f4e\u3002</li>\n    <li>\u5f15\u5165\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\uff0c\u4f18\u5316\u68c0\u7d22\u589e\u5f3a\u7684\u65b0\u9896\u6027\u5956\u52b1\uff0c\u63d0\u9ad8\u5047\u8bbe\u7684\u65b0\u9896\u6027\uff0c\u4ece\u800c\u4e3aAI\u7cfb\u7edf\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u63d0\u4f9b\u57fa\u7840\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is currently no clear framework for Scientific General Intelligence (SGI), which is the ability of AI to independently think and work in scientific areas.</li>\n    <li>The authors propose a new definition of SGI based on a model called Practical Inquiry Model (PIM) and suggest four tasks for testing SGI: deep research, idea generation, experiments, and reasoning about experiments.</li>\n    <li>The SGI-Bench includes over 1,000 curated examples from various scientific fields to evaluate advanced AI language models.</li>\n    <li>Results show significant gaps in AI performance, such as low accuracy in deep research and issues with creating practical ideas and executing experiments.</li>\n    <li>The authors introduce a method called Test-Time Reinforcement Learning (TTRL) to improve AI's ability to generate new ideas without needing a reference answer.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20619", "authors": [{"_id": "694b614d746a34b55dd53d1a", "name": "Jianhong Bai", "hidden": false}, {"_id": "694b614d746a34b55dd53d1b", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "694b614d746a34b55dd53d1c", "name": "Xintao Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1d", "name": "Fu Xiao", "hidden": false}, {"_id": "694b614d746a34b55dd53d1e", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1f", "name": "Qinghe Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d20", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "694b614d746a34b55dd53d21", "name": "Menghan Xia", "hidden": false}, {"_id": "694b614d746a34b55dd53d22", "name": "Zuozhu Liu", "hidden": false}, {"_id": "694b614d746a34b55dd53d23", "name": "Haoji Hu", "hidden": false}, {"_id": "694b614d746a34b55dd53d24", "name": "Pengfei Wan", "hidden": false}, {"_id": "694b614d746a34b55dd53d25", "name": "Kun Gai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "publishedAt": "2025-12-23T18:59:56.000Z", "submittedOnDailyAt": "2025-12-24T01:20:51.117Z", "title": "SemanticGen: Video Generation in Semantic Space", "submittedOnDailyBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "isPro": false, "fullname": "Jianhong Bai", "user": "jianhongbai", "type": "user"}, "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "upvotes": 77, "discussionId": "694b614d746a34b55dd53d26", "projectPage": "https://jianhongbai.github.io/SemanticGen/", "ai_summary": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.", "ai_keywords": ["VAE space", "VAE decoder", "semantic space", "diffusion model", "semantic video features", "bi-directional attention"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>SemanticGen\u662f\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u7684\u6162\u6536\u655b\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002</li>\n    <li>\u8be5\u6a21\u578b\u9996\u5148\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u7279\u5f81\uff0c\u7136\u540e\u518d\u6dfb\u52a0\u9ad8\u9891\u7ec6\u8282\uff0c\u4f7f\u751f\u6210\u8fc7\u7a0b\u66f4\u9ad8\u6548\u3002</li>\n    <li>SemanticGen\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u8fc7\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u7d27\u51d1\u7684\u8bed\u4e49\u89c6\u9891\u7279\u5f81\uff0c\u7b2c\u4e8c\u9636\u6bb5\u6839\u636e\u8fd9\u4e9b\u7279\u5f81\u751f\u6210\u6700\u7ec8\u7684\u89c6\u9891\u3002</li>\n    <li>\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6bd4\u5728VAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u66f4\u5feb\u6536\u655b\uff0c\u4e14\u5bf9\u4e8e\u957f\u89c6\u9891\u751f\u6210\u4e5f\u66f4\u6709\u6548\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSemanticGen\u751f\u6210\u7684\u89c6\u9891\u8d28\u91cf\u9ad8\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u5148\u8fdb\u65b9\u6cd5\u548c\u5f3a\u57fa\u7ebf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Traditional video generative models use a complex approach that can be slow and resource-intensive for long videos.</li>\n    <li>SemanticGen is a new method that generates videos in a simpler, high-level semantic space for better efficiency.</li>\n    <li>It uses a two-stage process: first, it creates a general layout of the video, then adds detailed features.</li>\n    <li>This approach results in faster video generation and works well for longer videos.</li>\n    <li>Tests show that SemanticGen produces high-quality videos and outperforms existing methods.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:56.000Z", "title": "SemanticGen: Video Generation in Semantic Space", "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png", "numComments": 2, "submittedBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "fullname": "Jianhong Bai", "name": "jianhongbai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16793", "authors": [{"_id": "6944ba3efbf17e708e185f60", "user": {"_id": "6944c28d3cd5eeb7a7838663", "avatarUrl": "/avatars/63f9f8c4e8462cbc9726bdee249fbcde.svg", "isPro": false, "fullname": "lin", "user": "birdxp", "type": "user"}, "name": "Xiaopeng Lin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:14.947Z", "hidden": false}, {"_id": "6944ba3efbf17e708e185f61", "user": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "isPro": false, "fullname": "Shijie Lian", "user": "LiamLian0727", "type": "user"}, "name": "Shijie Lian", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:18.093Z", "hidden": false}, {"_id": "6944ba3efbf17e708e185f62", "user": {"_id": "63d3b5f1640bb0f77173baea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674819020331-noauth.jpeg", "isPro": false, "fullname": "yubin", "user": "VLyb", "type": "user"}, "name": "Bin Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T11:00:33.691Z", "hidden": false}, {"_id": "6944ba3efbf17e708e185f63", "user": {"_id": "684a7f750fff63d29d18444c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kXTIoUPOM7LDztBcRL6YI.png", "isPro": false, "fullname": "Ruoqi Yang", "user": "lyceen", "type": "user"}, "name": "Ruoqi Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T11:00:35.617Z", "hidden": false}, {"_id": "6944ba3efbf17e708e185f64", "name": "Changti Wu", "hidden": false}, {"_id": "6944ba3efbf17e708e185f65", "name": "Yuzhuo Miao", "hidden": false}, {"_id": "6944ba3efbf17e708e185f66", "name": "Yurun Jin", "hidden": false}, {"_id": "6944ba3efbf17e708e185f67", "name": "Yukun Shi", "hidden": false}, {"_id": "6944ba3efbf17e708e185f68", "name": "Cong Huang", "hidden": false}, {"_id": "6944ba3efbf17e708e185f69", "name": "Bojun Cheng", "hidden": false}, {"_id": "6944ba3efbf17e708e185f6a", "name": "Kai Chen", "hidden": false}], "publishedAt": "2025-12-18T17:27:03.000Z", "submittedOnDailyAt": "2025-12-22T00:59:36.650Z", "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence", "submittedOnDailyBy": {"_id": "6944c28d3cd5eeb7a7838663", "avatarUrl": "/avatars/63f9f8c4e8462cbc9726bdee249fbcde.svg", "isPro": false, "fullname": "lin", "user": "birdxp", "type": "user"}, "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.", "upvotes": 63, "discussionId": "6944ba3efbf17e708e185f6b", "projectPage": "https://zgc-embodyai.github.io/PhysBrain/", "ai_summary": "Proposed Egocentric2Embodiment pipeline translates human egocentric videos into structured training data for robots, enhancing their egocentric understanding and task performance.", "ai_keywords": ["Egocentric2Embodiment", "VQA supervision", "evidence grounding", "temporal consistency", "Egocentric2Embodiment dataset", "E2E-3M", "PhysBrain", "egocentric-aware", "VLA fine-tuning", "SimplerEnv success rates"], "organization": {"_id": "6948d884070dda0c2ae35a78", "name": "DeepCybo", "fullname": "DeepCybo", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65ec01fd770aa0e25d9374dc/QOsz6P_7AxyqGrjsRHTGk.png"}, "summary_zh": "<ul>\n    <li>\u673a\u5668\u4eba\u6cdb\u5316\u4f9d\u8d56\u4e8e\u7269\u7406\u667a\u80fd\uff0c\u5305\u62ec\u7406\u89e3\u72b6\u6001\u53d8\u5316\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u4e92\u52a8\u548c\u957f\u671f\u89c4\u5212\u3002</li>\n    <li>\u76ee\u524d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u7b2c\u4e09\u4eba\u79f0\u6570\u636e\uff0c\u5bfc\u81f4\u4eba\u5f62\u673a\u5668\u4eba\u5b58\u5728\u89c6\u89d2\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002</li>\n    <li>\u6536\u96c6\u673a\u5668\u4eba\u7b2c\u4e00\u4eba\u79f0\u6570\u636e\u6210\u672c\u9ad8\u3001\u79cd\u7c7b\u6709\u9650\uff0c\u800c\u4eba\u7c7b\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u8f6c\u6362\u4e3a\u591a\u5c42\u6b21\u3001\u7ed3\u6784\u5316\u76d1\u7763\u7684Egocentric2Embodiment\u7ffb\u8bd1\u6d41\u7a0b\uff0c\u5e76\u6784\u5efa\u4e86E2E-3M\u6570\u636e\u96c6\u3002</li>\n    <li>\u901a\u8fc7\u5728E2E-3M\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0cPhysBrain\u6a21\u578b\u5728\u7406\u89e3\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u8fc1\u79fb\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Robotic generalization requires understanding physical interactions and long-term planning from a first-person perspective.</li>\n    <li>Most current models are trained on third-person data, which doesn't match how humanoid robots operate.</li>\n    <li>Collecting robot data is costly and lacks variety, while human first-person videos provide a better alternative.</li>\n    <li>The Egocentric2Embodiment pipeline converts raw videos into structured training data for robots.</li>\n    <li>Training robots with this new data improves their ability to plan and succeed in tasks, showing effective learning from human experiences.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:27:03.000Z", "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence", "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16793.png", "numComments": 2, "submittedBy": {"_id": "6944c28d3cd5eeb7a7838663", "avatarUrl": "/avatars/63f9f8c4e8462cbc9726bdee249fbcde.svg", "fullname": "lin", "name": "birdxp", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "6948d884070dda0c2ae35a78", "name": "DeepCybo", "fullname": "DeepCybo", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65ec01fd770aa0e25d9374dc/QOsz6P_7AxyqGrjsRHTGk.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.19693", "authors": [{"_id": "694a0ffa335742716e93227d", "name": "Weichen Fan", "hidden": false}, {"_id": "694a0ffa335742716e93227e", "name": "Haiwen Diao", "hidden": false}, {"_id": "694a0ffa335742716e93227f", "name": "Quan Wang", "hidden": false}, {"_id": "694a0ffa335742716e932280", "name": "Dahua Lin", "hidden": false}, {"_id": "694a0ffa335742716e932281", "name": "Ziwei Liu", "hidden": false}], "publishedAt": "2025-12-22T18:59:57.000Z", "submittedOnDailyAt": "2025-12-23T01:15:14.379Z", "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.", "upvotes": 51, "discussionId": "694a0ffa335742716e932282", "githubRepo": "https://github.com/WeichenFan/UAE", "githubRepoAddedBy": "user", "ai_summary": "Unified Autoencoding combines semantic and pixel-level information through a frequency-band modulator, resulting in a latent space with state-of-the-art performance on image benchmarks.", "ai_keywords": ["spectral characteristics", "semantic encoders", "pixel encoders", "feature spectrum", "low-frequency components", "high-frequency information", "Prism Hypothesis", "Unified Autoencoding", "frequency-band modulator", "ImageNet", "MS-COCO", "latent space"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u4e0d\u540c\u7c7b\u578b\u7684\u7f16\u7801\u5668\u5728\u7279\u5f81\u9891\u8c31\u4e0a\u6709\u72ec\u7279\u7684\u8054\u7cfb\u3002</li>\n    <li>\u8bed\u4e49\u7f16\u7801\u5668\u4e3b\u8981\u6355\u6349\u4f4e\u9891\u6210\u5206\uff0c\u8868\u793a\u62bd\u8c61\u542b\u4e49\uff1b\u50cf\u7d20\u7f16\u7801\u5668\u8fd8\u4fdd\u7559\u9ad8\u9891\u4fe1\u606f\uff0c\u4f20\u8fbe\u7ec6\u8282\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u68f1\u955c\u5047\u8bf4\u201d\uff0c\u8ba4\u4e3a\u6bcf\u79cd\u6570\u636e\u6a21\u5f0f\u53ef\u4ee5\u770b\u4f5c\u662f\u81ea\u7136\u754c\u5728\u5171\u4eab\u7279\u5f81\u9891\u8c31\u4e0a\u7684\u6295\u5f71\u3002</li>\n    <li>\u57fa\u4e8e\u8fd9\u4e00\u7406\u8bba\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u7edf\u4e00\u81ea\u7f16\u7801\u6a21\u578b\uff08UAE\uff09\uff0c\u80fd\u591f\u7ed3\u5408\u8bed\u4e49\u7ed3\u6784\u548c\u50cf\u7d20\u7ec6\u8282\u3002</li>\n    <li>\u5728ImageNet\u548cMS-COCO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUAE\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u6210\u529f\u7edf\u4e00\u4e86\u8bed\u4e49\u548c\u50cf\u7d20\u4fe1\u606f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>This study looks at how different types of data encoders capture information, focusing on their frequency characteristics.</li>\n    <li>It finds that semantic encoders focus on low-frequency information for abstract meanings, while pixel encoders also include high-frequency details for finer visuals.</li>\n    <li>The authors introduce the \"Prism Hypothesis,\" which suggests that different data types project onto a shared feature spectrum, similar to how a prism works.</li>\n    <li>They propose a new model called Unified Autoencoding (UAE) that combines semantic meaning and pixel details effectively.</li>\n    <li>Tests show that UAE performs well on popular datasets like ImageNet and MS-COCO, achieving top-level results in integrating both types of information.</li>\n</ul>"}, "publishedAt": "2025-12-22T13:59:57.000Z", "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding", "summary": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19693.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 189}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19673", "authors": [{"_id": "694ac3ad746a34b55dd53b6c", "name": "Yuqiao Tan", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b6d", "name": "Minzheng Wang", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b6e", "name": "Shizhu He", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b6f", "name": "Huanxuan Liao", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b70", "name": "Chengfeng Zhao", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b71", "name": "Qiunan Lu", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b72", "name": "Tian Liang", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b73", "name": "Jun Zhao", "hidden": false}, {"_id": "694ac3ad746a34b55dd53b74", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-22T18:51:48.000Z", "submittedOnDailyAt": "2025-12-24T00:28:45.252Z", "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies", "submittedOnDailyBy": {"_id": "64bcc373ef8c0e42bf16acc5", "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg", "isPro": false, "fullname": "mz.w", "user": "iiiiwis", "type": "user"}, "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.", "upvotes": 49, "discussionId": "694ac3ad746a34b55dd53b75", "githubRepo": "https://github.com/Trae1ounG/BuPO", "githubRepoAddedBy": "user", "ai_summary": "The paper decomposes the policy of large language models into internal layer and modular policies, revealing distinct reasoning patterns across layers and proposing Bottom-up Policy Optimization to enhance performance on complex reasoning tasks.", "ai_keywords": ["reinforcement learning", "large language models", "Transformer residual stream", "unembedding matrix", "Internal Layer Policies", "Internal Modular Policies", "self-attention", "feed-forward network", "entropy", "Bottom-up Policy Optimization"], "githubStars": 22, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u4e00\u4e2a\u7edf\u4e00\u7684\u7b56\u7565\uff0c\u5ffd\u89c6\u4e86\u5176\u5185\u90e8\u673a\u5236\u3002</li>\n    <li>\u672c\u6587\u901a\u8fc7\u5206\u6790Transformer\u7684\u6b8b\u5dee\u6d41\u548c\u9690\u85cf\u72b6\u6001\uff0c\u5206\u89e3\u8bed\u8a00\u6a21\u578b\u7b56\u7565\uff0c\u63ed\u793a\u5185\u90e8\u5c42\u7ea7\u653f\u7b56\u548c\u6a21\u5757\u653f\u7b56\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u65e9\u671f\u5c42\u4fdd\u6301\u9ad8\u71b5\u4ee5\u63a2\u7d22\uff0c\u800c\u9876\u5c42\u8d8b\u5411\u4e8e\u8fd1\u96f6\u71b5\u4ee5\u8fdb\u884c\u7cbe\u7ec6\u5316\u5904\u7406\uff0c\u4e0d\u540c\u6a21\u578b\u7cfb\u5217\u7684\u6536\u655b\u6a21\u5f0f\u4e0d\u540c\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u81ea\u4e0b\u800c\u4e0a\u7684\u653f\u7b56\u4f18\u5316(BuPO)\uff0c\u5728\u65e9\u671f\u8bad\u7ec3\u4e2d\u76f4\u63a5\u4f18\u5316\u5185\u90e8\u5c42\u653f\u7b56\uff0c\u63d0\u9ad8\u63a8\u7406\u80fd\u529b\u548c\u6027\u80fd\u3002</li>\n    <li>\u5728\u590d\u6742\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study breaks down how large language models (LLMs) work internally, focusing on their different layers and components.</li>\n    <li>It introduces Internal Layer Policies and Internal Modular Policies to understand contributions from specific parts of the model.</li>\n    <li>Findings show that early layers of the model explore more, while top layers refine predictions, with different models showing unique patterns.</li>\n    <li>A new reinforcement learning method called Bottom-up Policy Optimization (BuPO) is proposed, which improves training by focusing on lower layers.</li>\n    <li>BuPO has been shown to enhance reasoning capabilities and perform better on complex tasks according to experiments.</li>\n</ul>"}, "publishedAt": "2025-12-22T13:51:48.000Z", "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies", "summary": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19673.png", "numComments": 4, "submittedBy": {"_id": "64bcc373ef8c0e42bf16acc5", "avatarUrl": "/avatars/873308203d28115ae1a9e4d0e26508f4.svg", "fullname": "mz.w", "name": "iiiiwis", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.17901", "authors": [{"_id": "6948af7534f46eaf46cbb1da", "user": {"_id": "6719bfd07c6e6c83a388aeae", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719bfd07c6e6c83a388aeae/jHxryk04dzHo23TX5F5sz.png", "isPro": false, "fullname": "Junyu Zhang", "user": "jyzhang1208", "type": "user"}, "name": "Junyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:53.009Z", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1db", "name": "Yifan Sun", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1dc", "name": "Tianang Leng", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1dd", "name": "Jingyan Shen", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1de", "name": "Liu Ziyin", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1df", "name": "Paul Pu Liang", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1e0", "name": "Huan Zhang", "hidden": false}], "publishedAt": "2025-12-19T18:59:11.000Z", "submittedOnDailyAt": "2025-12-22T00:10:07.525Z", "title": "When Reasoning Meets Its Laws", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/", "upvotes": 48, "discussionId": "6948af7534f46eaf46cbb1e1", "projectPage": "https://lore-project.github.io/", "githubRepo": "https://github.com/ASTRAL-Group/LoRe", "githubRepoAddedBy": "user", "ai_summary": "A framework called Laws of Reasoning (LoRe) is introduced to theoretically define desired reasoning behaviors in Large Reasoning Models, with a focus on compute and accuracy laws, and a benchmark (LoRe-Bench) to measure these properties.", "ai_keywords": ["Laws of Reasoning", "LoRe", "compute law", "accuracy law", "question complexity", "monotonicity", "compositionality", "LoRe-Bench", "finetuning approach", "compute-law compositionality"], "githubStars": 15, "summary_zh": "<ul>\n    <li>\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u867d\u7136\u8868\u73b0\u4f18\u8d8a\uff0c\u4f46\u63a8\u7406\u884c\u4e3a\u5e38\u5e38\u4e0d\u7b26\u5408\u76f4\u89c9\uff0c\u5bfc\u81f4\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u63a8\u7406\u6cd5\u5219\uff08LoRe\uff09\uff0c\u7528\u4e8e\u63cf\u8ff0LRMs\u4e2d\u56fa\u6709\u7684\u63a8\u7406\u6a21\u5f0f\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u8ba1\u7b97\u6cd5\u5219\uff0c\u5373\u63a8\u7406\u8ba1\u7b97\u5e94\u4e0e\u95ee\u9898\u590d\u6742\u5ea6\u5448\u7ebf\u6027\u5173\u7cfb\u3002</li>\n    <li>\u901a\u8fc7\u5355\u8c03\u6027\u548c\u7ec4\u5408\u6027\u4e24\u4e2a\u7279\u6027\u6765\u68c0\u9a8c\u63a8\u7406\u6cd5\u5219\uff0c\u5e76\u5f15\u5165LoRe-Bench\u4f5c\u4e3a\u57fa\u51c6\u6d4b\u8bd5\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u7edd\u5927\u591a\u6570\u63a8\u7406\u6a21\u578b\u5728\u5355\u8c03\u6027\u65b9\u9762\u8868\u73b0\u5408\u7406\uff0c\u4f46\u5728\u7ec4\u5408\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u6709\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\u4ee5\u6539\u5584\u8fd9\u4e00\u70b9\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Reasoning Models (LRMs) perform well but often have confusing reasoning behaviors.</li>\n    <li>This paper introduces the Laws of Reasoning (LoRe), a framework to define desired reasoning patterns in LRMs.</li>\n    <li>LoRe includes a compute law that suggests reasoning should scale with question complexity, and an accuracy law.</li>\n    <li>The authors created LoRe-Bench to measure two properties of reasoning models: monotonicity and compositionality.</li>\n    <li>Results show that while many models show good monotonicity, they struggle with compositionality, but a new finetuning approach can help improve this.</li>\n</ul>"}, "publishedAt": "2025-12-19T13:59:11.000Z", "title": "When Reasoning Meets Its Laws", "summary": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17901.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16922", "authors": [{"_id": "6944c39ffbf17e708e18605d", "user": {"_id": "63f233820a16587ea967adc2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f233820a16587ea967adc2/1nSoZofPV7UseXzjI2qAH.png", "isPro": false, "fullname": "Sihan XU", "user": "sihanxu", "type": "user"}, "name": "Sihan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:57:39.350Z", "hidden": false}, {"_id": "6944c39ffbf17e708e18605e", "name": "Ziqiao Ma", "hidden": false}, {"_id": "6944c39ffbf17e708e18605f", "name": "Wenhao Chai", "hidden": false}, {"_id": "6944c39ffbf17e708e186060", "name": "Xuweiyi Chen", "hidden": false}, {"_id": "6944c39ffbf17e708e186061", "user": {"_id": "66608add236f958513d21d2e", "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg", "isPro": false, "fullname": "Weiyang Jin", "user": "Wayne-King", "type": "user"}, "name": "Weiyang Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T09:56:57.800Z", "hidden": false}, {"_id": "6944c39ffbf17e708e186062", "name": "Joyce Chai", "hidden": false}, {"_id": "6944c39ffbf17e708e186063", "name": "Saining Xie", "hidden": false}, {"_id": "6944c39ffbf17e708e186064", "name": "Stella X. Yu", "hidden": false}], "publishedAt": "2025-12-18T18:59:58.000Z", "submittedOnDailyAt": "2025-12-19T01:03:18.428Z", "title": "Next-Embedding Prediction Makes Strong Vision Learners", "submittedOnDailyBy": {"_id": "66608add236f958513d21d2e", "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg", "isPro": false, "fullname": "Weiyang Jin", "user": "Wayne-King", "type": "user"}, "summary": "Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.", "upvotes": 47, "discussionId": "6944c3a0fbf17e708e186065", "projectPage": "https://sihanxu.me/nepa", "githubRepo": "https://github.com/SihanXU/nepa", "githubRepoAddedBy": "user", "ai_summary": "Generative pretraining using next embedding prediction outperforms traditional self-supervised methods in visual learning tasks, achieving high accuracy on ImageNet and effective transfer to semantic segmentation.", "ai_keywords": ["generative pretraining", "predictive tasks", "Next-Embedding Predictive Autoregression (NEPA)", "causal masking", "stop gradient", "Transformer", "ImageNet-1k", "top-1 accuracy", "ViT-B", "ViT-L", "semantic segmentation", "ADE20K"], "githubStars": 43, "organization": {"_id": "66df3cb0cf19a8918414cbfe", "name": "SixAILab", "fullname": "SixAILab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f233820a16587ea967adc2/FSRWuJTgSvG0HFKm9_K4Z.jpeg"}, "summary_zh": "<ul>\n    <li>\u7814\u7a76\u5229\u7528\u751f\u6210\u9884\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u63a2\u7d22\u81ea\u76d1\u7763\u89c6\u89c9\u5b66\u4e60\u7684\u53ef\u80fd\u6027\u3002</li>\n    <li>\u6a21\u578b\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u7684\u7279\u5f81\u5d4c\u5165\u6765\u76f4\u63a5\u6267\u884c\u9884\u6d4b\u4efb\u52a1\uff0c\u800c\u4e0d\u662f\u4ec5\u8f93\u51fa\u7279\u5f81\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u79f0\u4e3a\u4e0b\u4e00\u5d4c\u5165\u9884\u6d4b\u81ea\u56de\u5f52\uff08NEPA\uff09\uff0c\u4f7f\u7528\u56e0\u679c\u63a9\u853d\u548c\u505c\u6b62\u68af\u5ea6\u3002</li>\n    <li>\u4f7f\u7528\u7b80\u5355\u7684Transformer\u6a21\u578b\uff0c\u5728ImageNet-1K\u4e0a\u9884\u8bad\u7ec3\u5e76\u4ec5\u4ee5\u4e0b\u4e00\u5d4c\u5165\u9884\u6d4b\u4f5c\u4e3a\u5b66\u4e60\u76ee\u6807\uff0c\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6548\u679c\u3002</li>\n    <li>NEPA\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u5e76\u6210\u529f\u8f6c\u79fb\u5230\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e0a\uff0c\u5c55\u793a\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u7b80\u6d01\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>This research explores using generative pretraining techniques from language to improve visual learning models.</li>\n    <li>Instead of creating features for other tasks, the models are trained to predict future embeddings based on past ones.</li>\n    <li>The method used is called Next-Embedding Predictive Autoregression (NEPA), which involves causal masking and stop gradient techniques.</li>\n    <li>A simple Transformer model trained with NEPA on ImageNet-1k achieved high accuracy without needing complex design elements.</li>\n    <li>NEPA shows strong performance in various tasks, including fine-tuning for image classification and transferring to segmentation tasks.</li>\n</ul>"}, "publishedAt": "2025-12-18T13:59:58.000Z", "title": "Next-Embedding Prediction Makes Strong Vision Learners", "summary": "Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16922.png", "numComments": 1, "submittedBy": {"_id": "66608add236f958513d21d2e", "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg", "fullname": "Weiyang Jin", "name": "Wayne-King", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "66df3cb0cf19a8918414cbfe", "name": "SixAILab", "fullname": "SixAILab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f233820a16587ea967adc2/FSRWuJTgSvG0HFKm9_K4Z.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16301", "authors": [{"_id": "6944be0ffbf17e708e185fde", "user": {"_id": "63724cfada3183d9d53f2009", "avatarUrl": "/avatars/17838fcf244ecf8d139343bb6c6d8562.svg", "isPro": false, "fullname": "Patrick Jiang", "user": "pat-jj", "type": "user"}, "name": "Pengcheng Jiang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:04.988Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fdf", "user": {"_id": "650488e454b989666d042a49", "avatarUrl": "/avatars/3dc79c6f1a9dce872636dddd38a04670.svg", "isPro": false, "fullname": "Jiacheng Lin", "user": "linjc16", "type": "user"}, "name": "Jiacheng Lin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:02.806Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe0", "user": {"_id": "66d4af28033492801d82b890", "avatarUrl": "/avatars/5e8a2dc1b932a679341976d11b22f6c8.svg", "isPro": false, "fullname": "shi", "user": "Gabshi", "type": "user"}, "name": "Zhiyi Shi", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T16:25:52.445Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe1", "name": "Zifeng Wang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe2", "name": "Luxi He", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe3", "name": "Yichen Wu", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe4", "name": "Ming Zhong", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe5", "user": {"_id": "649c5cf5c1ae48cf4d7dda34", "avatarUrl": "/avatars/a2264945f9f876b690017a93f225f937.svg", "isPro": false, "fullname": "Peiyang Song", "user": "p-song1", "type": "user"}, "name": "Peiyang Song", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:06.854Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe6", "name": "Qizheng Zhang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe7", "name": "Heng Wang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe8", "user": {"_id": "66a3f1c4c38ce500371fd8d4", "avatarUrl": "/avatars/381de938091f1a5c179eef72aa247bbf.svg", "isPro": false, "fullname": "Xueqiang Xu", "user": "XueqiangXu", "type": "user"}, "name": "Xueqiang Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:00.653Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe9", "name": "Hanwen Xu", "hidden": false}, {"_id": "6944be0ffbf17e708e185fea", "name": "Pengrui Han", "hidden": false}, {"_id": "6944be0ffbf17e708e185feb", "name": "Dylan Zhang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fec", "name": "Jiashuo Sun", "hidden": false}, {"_id": "6944be0ffbf17e708e185fed", "name": "Chaoqi Yang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fee", "name": "Kun Qian", "hidden": false}, {"_id": "6944be0ffbf17e708e185fef", "name": "Tian Wang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff0", "name": "Changran Hu", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff1", "name": "Manling Li", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff2", "name": "Quanzheng Li", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff3", "name": "Hao Peng", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff4", "name": "Sheng Wang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff5", "name": "Jingbo Shang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff6", "name": "Chao Zhang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff7", "name": "Jiaxuan You", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff8", "name": "Liyuan Liu", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff9", "name": "Pan Lu", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffa", "name": "Yu Zhang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffb", "name": "Heng Ji", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffc", "name": "Yejin Choi", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffd", "name": "Dawn Song", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffe", "name": "Jimeng Sun", "hidden": false}, {"_id": "6944be0ffbf17e708e185fff", "name": "Jiawei Han", "hidden": false}], "publishedAt": "2025-12-18T08:38:51.000Z", "submittedOnDailyAt": "2025-12-19T00:23:16.990Z", "title": "Adaptation of Agentic AI", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.", "upvotes": 43, "discussionId": "6944be10fbf17e708e186000", "githubRepo": "https://github.com/pat-jj/Awesome-Adaptation-of-Agentic-AI", "githubRepoAddedBy": "user", "ai_summary": "This paper presents a framework for agent and tool adaptation in agentic AI systems, clarifying design strategies and identifying open challenges for improving AI capabilities.", "ai_keywords": ["agentic AI systems", "foundation models", "agent adaptations", "tool adaptations", "tool-execution-signaled", "agent-output-signaled", "agent-agnostic", "agent-supervised"], "githubStars": 262, "summary_zh": "<ul>\n    <li>\u524d\u6cbf\u7684\u667a\u80fd\u4ee3\u7406AI\u7cfb\u7edf\u57fa\u4e8e\u53ef\u4ee5\u9002\u5e94\u8ba1\u5212\u3001\u63a8\u7406\u548c\u4e0e\u5916\u90e8\u5de5\u5177\u4e92\u52a8\u7684\u57fa\u7840\u6a21\u578b\u3002</li>\n    <li>\u9002\u5e94\u6027\u6210\u4e3a\u63d0\u9ad8\u6027\u80fd\u3001\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u91cd\u8981\u673a\u5236\u3002</li>\n    <li>\u672c\u6587\u5efa\u7acb\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6846\u67b6\uff0c\u6db5\u76d6\u4ee3\u7406\u9002\u5e94\u548c\u5de5\u5177\u9002\u5e94\u7684\u7814\u7a76\u3002</li>\n    <li>\u6846\u67b6\u5e2e\u52a9\u660e\u786e\u9002\u5e94\u7b56\u7565\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5e76\u63d0\u4f9b\u9009\u62e9\u6216\u5207\u6362\u7b56\u7565\u7684\u5b9e\u7528\u6307\u5bfc\u3002</li>\n    <li>\u6211\u4eec\u56de\u987e\u4e86\u5404\u7c7b\u65b9\u6cd5\uff0c\u5206\u6790\u5b83\u4eec\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u6307\u51fa\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u673a\u4f1a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Advanced AI systems use foundation models that can adapt to perform complex tasks and interact with tools.</li>\n    <li>Adaptation is key to improving AI performance, reliability, and generalization.</li>\n    <li>This paper presents a framework that organizes research into agent and tool adaptations.</li>\n    <li>It categorizes adaptations into different types and provides guidance for selecting strategies in AI design.</li>\n    <li>The paper reviews various approaches, discussing their strengths, weaknesses, and future challenges in AI development.</li>\n</ul>"}, "publishedAt": "2025-12-18T03:38:51.000Z", "title": "Adaptation of Agentic AI", "summary": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16301.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>DeepSeek-V3.2\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u51fa\u8272\u7684\u63a8\u7406\u80fd\u529b\u548c\u4ee3\u7406\u6027\u80fd\u3002</li>\n    <li>\u5f15\u5165\u4e86\u6df1\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff08DSA\uff09\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u60c5\u51b5\u4e0b\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002</li>\n    <li>\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7fDeepSeek-V3.2\u7684\u8868\u73b0\u53ef\u4ee5\u4e0eGPT-5\u76f8\u5ab2\u7f8e\u3002</li>\n    <li>\u9ad8\u8ba1\u7b97\u7248\u672cDeepSeek-V3.2-Speciale\u7684\u8868\u73b0\u8d85\u8fc7\u4e86GPT-5\uff0c\u5e76\u5728\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\u548c\u56fd\u9645\u4fe1\u606f\u5b66\u5965\u6797\u5339\u514b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff0c\u4ee5\u63d0\u9ad8\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e0b\u7684\u8bad\u7ec3\u6570\u636e\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that combines fast performance with strong reasoning abilities.</li>\n    <li>It features a new attention mechanism called DeepSeek Sparse Attention (DSA) that makes it more efficient, especially for long texts.</li>\n    <li>The model uses an improved reinforcement learning approach, allowing it to perform as well as GPT-5 and even better with a special version.</li>\n    <li>DeepSeek-V3.2 has shown exceptional reasoning skills, winning top awards in major math and computer science competitions.</li>\n    <li>A new data generation method helps the model learn to use tools better in complex situations.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.22699", "authors": [{"_id": "692d06234397b1ec214f6788", "name": "Z-Image Team", "hidden": false}, {"_id": "692d06234397b1ec214f6789", "user": {"_id": "692d0e6bb14ceb758205d0dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/692d0e6bb14ceb758205d0dd/gGVq2KSJE11Sr3LkVn-n5.jpeg", "isPro": false, "fullname": "Huanqia Cai", "user": "Orion-Cai", "type": "user"}, "name": "Huanqia Cai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:26.669Z", "hidden": false}, {"_id": "692d06234397b1ec214f678a", "user": {"_id": "67777b7a8376dfe003afa951", "avatarUrl": "/avatars/2af9d3181306d4c53329d047eeadaf1e.svg", "isPro": false, "fullname": "Sihan Cao", "user": "Sihan-Cao", "type": "user"}, "name": "Sihan Cao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:33.191Z", "hidden": false}, {"_id": "692d06234397b1ec214f678b", "user": {"_id": "64a54586c0f13de8e7093314", "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg", "isPro": false, "fullname": "Ruoyi Du", "user": "RuoyiDu", "type": "user"}, "name": "Ruoyi Du", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:18:53.948Z", "hidden": false}, {"_id": "692d06234397b1ec214f678c", "name": "Peng Gao", "hidden": false}, {"_id": "692d06234397b1ec214f678d", "name": "Steven Hoi", "hidden": false}, {"_id": "692d06234397b1ec214f678e", "name": "Shijie Huang", "hidden": false}, {"_id": "692d06234397b1ec214f678f", "name": "Zhaohui Hou", "hidden": false}, {"_id": "692d06234397b1ec214f6790", "user": {"_id": "662a0f2d4bab737c1a279843", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662a0f2d4bab737c1a279843/fC2p3mjMHkVpDQdEqkuR4.png", "isPro": false, "fullname": "Dengyang Jiang", "user": "DyJiang", "type": "user"}, "name": "Dengyang Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:15.555Z", "hidden": false}, {"_id": "692d06234397b1ec214f6791", "user": {"_id": "6537e8eab01250d1d6efed3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gMx73gwdfEhcCFioStGCE.jpeg", "isPro": false, "fullname": "Xin", "user": "Srameo", "type": "user"}, "name": "Xin Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:15.288Z", "hidden": false}, {"_id": "692d06234397b1ec214f6792", "name": "Liangchen Li", "hidden": false}, {"_id": "692d06234397b1ec214f6793", "user": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "name": "Zhen Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:16.899Z", "hidden": false}, {"_id": "692d06234397b1ec214f6794", "user": {"_id": "6740a5730bb4a675446a80ad", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740a5730bb4a675446a80ad/dmruwMdQK3zluJm7YXUtN.jpeg", "isPro": false, "fullname": "Zhong-Yu Li", "user": "lzyhha", "type": "user"}, "name": "Zhong-Yu Li", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:08.972Z", "hidden": false}, {"_id": "692d06234397b1ec214f6795", "name": "David Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6796", "name": "Dongyang Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6797", "user": {"_id": "66332475351231c428653b6b", "avatarUrl": "/avatars/3997bcde54158f7ff9770c85a20875f1.svg", "isPro": false, "fullname": "Junhan Shi", "user": "jshmsjh", "type": "user"}, "name": "Junhan Shi", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:38.865Z", "hidden": false}, {"_id": "692d06234397b1ec214f6798", "user": {"_id": "64379d79fac5ea753f1c10f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png", "isPro": false, "fullname": "Jerry Wu", "user": "QJerry", "type": "user"}, "name": "Qilong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:18.709Z", "hidden": false}, {"_id": "692d06234397b1ec214f6799", "name": "Feng Yu", "hidden": false}, {"_id": "692d06234397b1ec214f679a", "name": "Chi Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679b", "name": "Shifeng Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679c", "user": {"_id": "641988978e0baaeed5a066c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641988978e0baaeed5a066c6/TdCjJ63gw5gdX1RqTvy9a.png", "isPro": false, "fullname": "Shilin", "user": "zsLin", "type": "user"}, "name": "Shilin Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T16:24:44.624Z", "hidden": false}], "publishedAt": "2025-11-27T18:52:07.000Z", "submittedOnDailyAt": "2025-12-01T00:38:17.269Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "submittedOnDailyBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "upvotes": 155, "discussionId": "692d06234397b1ec214f679d", "projectPage": "https://tongyi-mai.github.io/Z-Image-blog/", "githubRepo": "https://github.com/Tongyi-MAI/Z-Image", "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.", "ai_keywords": ["Scalable Single-Stream Diffusion Transformer", "S3-DiT", "diffusion transformer", "omni-pre-training", "instruction-following capabilities", "photorealistic image generation", "bilingual text rendering", "distillation scheme", "reward post-training", "H800 GPU", "VRAM"], "githubStars": 5595, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "summary_zh": "<ul>\n    <li>\u5f53\u524d\u9ad8\u6027\u80fd\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e3b\u8981\u7531\u4e13\u6709\u7cfb\u7edf\u4e3b\u5bfc\uff0c\u5982Nano Banana Pro\u548cSeedream 4.0\u3002</li>\n    <li>\u5f00\u653e\u6e90\u7801\u7684\u66ff\u4ee3\u54c1\u53c2\u6570\u91cf\u5de8\u5927\uff0820\u4ebf\u523080\u4ebf\uff09\uff0c\u4e0d\u9002\u5408\u666e\u901a\u786c\u4ef6\u8fdb\u884c\u63a8\u7406\u548c\u5fae\u8c03\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Z-Image\uff0c\u8fd9\u662f\u4e00\u4e2a\u9ad8\u6548\u76846\u4ebf\u53c2\u6570\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5355\u6d41\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u3002</li>\n    <li>Z-Image\u5728314K H800 GPU\u5c0f\u65f6\u5185\u5b8c\u6210\u5168\u8bad\u7ec3\u6d41\u7a0b\uff0c\u6210\u672c\u7ea6\u4e3a63\u4e07\u7f8e\u5143\uff0c\u540c\u65f6\u63d0\u4f9b\u5feb\u901f\u63a8\u7406\u548c\u517c\u5bb9\u666e\u901a\u786c\u4ef6\u3002</li>\n    <li>Z-Image\u5728\u7167\u7247\u7ea7\u56fe\u50cf\u751f\u6210\u548c\u53cc\u8bed\u6587\u672c\u6e32\u67d3\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u6211\u4eec\u516c\u5f00\u4e86\u4ee3\u7801\u3001\u6743\u91cd\u548c\u5728\u7ebf\u6f14\u793a\uff0c\u4fc3\u8fdb\u53ef\u53ca\u7684\u9ad8\u7aef\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>High-performance image generation models are mostly proprietary, but there are large open-source models that are too heavy for regular use.</li>\n    <li>Z-Image is a new 6B-parameter model that is efficient and designed to challenge the idea that bigger is always better.</li>\n    <li>It was trained quickly and cost-effectively, using only 314K GPU hours and around $630K.</li>\n    <li>Z-Image can run on consumer-grade hardware and has fast inference times, thanks to a special distillation process.</li>\n    <li>The model performs as well as or better than leading competitors in generating realistic images and handling bilingual text, and it is available for public use.</li>\n</ul>"}, "publishedAt": "2025-11-27T13:52:07.000Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png", "numComments": 3, "submittedBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "fullname": "Zhen Li", "name": "Paper99", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 29}, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\uff0c\u9700\u8981\u66f4\u53ef\u9760\u548c\u53ef\u6269\u5c55\u7684\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u3002</li>\n    <li>\u76ee\u524d\u7684\u6570\u636e\u51c6\u5907\u5927\u591a\u4f9d\u8d56\u4e34\u65f6\u811a\u672c\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\uff0c\u5f71\u54cd\u4e86\u91cd\u73b0\u6027\u548c\u6570\u636e\u751f\u6210\u7684\u652f\u6301\u3002</li>\n    <li>\u4e3a\u5e94\u5bf9\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86DataFlow\uff0c\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u51c6\u5907\u6846\u67b6\uff0c\u652f\u6301\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>DataFlow\u5305\u542b\u7ea6200\u4e2a\u53ef\u91cd\u7528\u7684\u64cd\u4f5c\u7b26\u548c\u516d\u4e2a\u901a\u7528\u6d41\u7a0b\uff0c\u8986\u76d6\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u9886\u57df\u3002</li>\n    <li>DataFlow\u5728\u591a\u79cd\u5e94\u7528\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38LLM\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6570\u636e\u51c6\u5907\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There's a growing need for high-quality data in Large Language Models (LLMs), but current data preparation methods are often messy and not reliable.</li>\n    <li>DataFlow is a new framework that helps create better data preparation processes, allowing for easier and more efficient data transformations.</li>\n    <li>It includes nearly 200 reusable tools and six pipelines for different tasks like text processing and code generation.</li>\n    <li>DataFlow-Agent can turn simple language instructions into working data pipelines automatically.</li>\n    <li>Using DataFlow has shown to improve LLM performance in various tasks, outperforming existing datasets and methods significantly.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.20626", "authors": [{"_id": "6927ab26243b2216fb75cd1b", "name": "Wei He", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1c", "user": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "name": "Kai Han", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:11.052Z", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1d", "name": "Hang Zhou", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1e", "name": "Hanting Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1f", "name": "Zhicheng Liu", "hidden": false}, {"_id": "6927ab26243b2216fb75cd20", "name": "Xinghao Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd21", "name": "Yunhe Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "publishedAt": "2025-11-25T18:48:05.000Z", "submittedOnDailyAt": "2025-11-26T23:08:13.066Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "submittedOnDailyBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "upvotes": 154, "discussionId": "6927ab27243b2216fb75cd22", "projectPage": "https://github.com/huawei-noah/noah-research/tree/master/ROOT", "githubRepo": "https://github.com/huawei-noah/noah-research", "ai_summary": "ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.", "ai_keywords": ["large language models", "LLMs", "momentum orthogonalization", "dimensional fragility", "outlier-induced noise", "adaptive Newton iterations", "proximal optimization", "Muon", "Adam-based optimizers", "robust optimizer"], "githubStars": 909, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u52a0\u5267\u4e86\u7b97\u6cd5\u4e0d\u7cbe\u786e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668ROOT\uff0c\u65e8\u5728\u901a\u8fc7\u53cc\u91cd\u7a33\u5065\u673a\u5236\u6765\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>ROOT\u91c7\u7528\u81ea\u9002\u5e94\u725b\u987f\u8fed\u4ee3\u7684\u7ef4\u5ea6\u7a33\u5065\u6b63\u4ea4\u5316\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4e0d\u540c\u67b6\u6784\u914d\u7f6e\u4e0b\u7684\u7cbe\u786e\u5ea6\u3002</li>\n    <li>ROOT\u8fd8\u901a\u8fc7\u90bb\u8fd1\u4f18\u5316\u6846\u67b6\u6291\u5236\u5f02\u5e38\u503c\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u610f\u4e49\u7684\u68af\u5ea6\u65b9\u5411\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cROOT\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e2d\u6bd4Muon\u548c\u57fa\u4e8eAdam\u7684\u4f18\u5316\u5668\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) face challenges with training stability and precision as they grow in size.</li>\n    <li>Recent optimizers have improved efficiency but struggle with robustness against noise and dimensional changes.</li>\n    <li>ROOT is a new optimizer that strengthens training stability using two innovative methods: dimension-robust orthogonalization and noise suppression.</li>\n    <li>Experiments show ROOT outperforms other optimizers like Muon and Adam in tough conditions, achieving faster training and better results.</li>\n    <li>The ROOT code will be available online for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:48:05.000Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20626.png", "numComments": 2, "submittedBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "fullname": "hankai", "name": "hankaixyz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20785", "authors": [{"_id": "692d430f4397b1ec214f696e", "user": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "name": "Zuhao Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:11.311Z", "hidden": false}, {"_id": "692d430f4397b1ec214f696f", "user": {"_id": "6690f58e2f9f6f9c88e91031", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6690f58e2f9f6f9c88e91031/QQ_VoEh7NlE6BUvii08zk.png", "isPro": false, "fullname": "Sudong Wang", "user": "xiao45791", "type": "user"}, "name": "Sudong Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:14.173Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6970", "user": {"_id": "64bb77e786e7fb5b8a317a43", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bb77e786e7fb5b8a317a43/J0jOrlZJ9gazdYaeSH2Bo.png", "isPro": false, "fullname": "kcz", "user": "kcz358", "type": "user"}, "name": "Kaichen Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T10:06:41.343Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6971", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:09.461Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6972", "name": "Sicong Leng", "hidden": false}, {"_id": "692d430f4397b1ec214f6973", "name": "Yifan Zhang", "hidden": false}, {"_id": "692d430f4397b1ec214f6974", "name": "Chengwei Qin", "hidden": false}, {"_id": "692d430f4397b1ec214f6975", "name": "Shijian Lu", "hidden": false}, {"_id": "692d430f4397b1ec214f6976", "name": "Xingxuan Li", "hidden": false}, {"_id": "692d430f4397b1ec214f6977", "user": {"_id": "6454685a548f22be598414c4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg", "isPro": false, "fullname": "Lidong Bing", "user": "LidongBing", "type": "user"}, "name": "Lidong Bing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:49:36.056Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "publishedAt": "2025-11-25T19:22:48.000Z", "submittedOnDailyAt": "2025-12-02T00:35:56.511Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "submittedOnDailyBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "upvotes": 148, "discussionId": "692d430f4397b1ec214f6978", "projectPage": "https://evolvinglmms-lab.github.io/LongVT/", "githubRepo": "https://github.com/EvolvingLMMs-Lab/LongVT", "ai_summary": "LongVT, an end-to-end framework, enhances long video reasoning by interleaving global and local analysis using multimodal tools, outperforming existing methods on challenging benchmarks.", "ai_keywords": ["multimodal models", "video reasoning", "textual Chain-of-Thought", "hallucinations", "long-form videos", "temporal grounding", "video cropping", "fine-grained question-answering", "VideoSIAH", "tool-integrated cold-start supervised fine-tuning", "agentic reinforcement learning", "agentic reinforcement fine-tuning"], "githubStars": 121, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "summary_zh": "<ul>\n    <li>\u957f\u89c6\u9891\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u89c6\u9891\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u5bb9\u6613\u51fa\u73b0\u5e7b\u89c9\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86LongVT\u6846\u67b6\uff0c\u6a21\u4eff\u4eba\u7c7b\u89c2\u770b\u957f\u89c6\u9891\u7684\u65b9\u5f0f\uff0c\u5148\u6d4f\u89c8\u6574\u4f53\u518d\u805a\u7126\u7ec6\u8282\u3002</li>\n    <li>LongVT\u5229\u7528LMMs\u7684\u65f6\u95f4\u57fa\u7840\u80fd\u529b\uff0c\u4ece\u957f\u89c6\u9891\u4e2d\u63d0\u53d6\u76f8\u5173\u7247\u6bb5\u8fdb\u884c\u7ec6\u81f4\u5206\u6790\uff0c\u76f4\u5230\u627e\u5230\u6709\u4f9d\u636e\u7684\u7b54\u6848\u3002</li>\n    <li>\u4e3a\u652f\u6301\u957f\u89c6\u9891\u63a8\u7406\u4efb\u52a1\uff0c\u6211\u4eec\u521b\u5efa\u4e86VideoSIAH\u6570\u636e\u96c6\uff0c\u5305\u62ec247.9K\u4e2a\u8bad\u7ec3\u6837\u672c\u548c1,280\u4e2a\u8bc4\u4f30\u95ee\u7b54\u5bf9\u3002</li>\n    <li>LongVT\u5728\u591a\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u5df2\u516c\u5f00\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>LongVT is a new framework designed to help large multimodal models better understand and reason about long videos.</li>\n    <li>It mimics how humans watch videos by first looking at them globally and then focusing on specific parts for details.</li>\n    <li>The system uses a method of selecting and zooming in on video clips to improve understanding and accuracy.</li>\n    <li>To support this work, a new dataset called VideoSIAH will be released, containing a large number of question-answer pairs for training and evaluation.</li>\n    <li>LongVT has been shown to perform better than existing models on various long-video reasoning tests.</li>\n</ul>"}, "publishedAt": "2025-11-25T14:22:48.000Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20785.png", "numComments": 3, "submittedBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "fullname": "Zuhao Yang", "name": "mwxely", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u5230\u8ba1\u7b97\u987a\u5e8f\u548c\u957f\u671f\u4e0d\u4e00\u81f4\u6027\u7684\u9650\u5236\uff0c\u5f71\u54cd\u4e86\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u5934\u50cf\u5408\u6210\u7684\u5e94\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201cLive Avatar\u201d\uff0c\u4e00\u4e2a\u9ad8\u6548\u3001\u9ad8\u6e05\u6670\u5ea6\u3001\u65e0\u9650\u957f\u5ea6\u7684\u5934\u50cf\u751f\u6210\u6846\u67b6\uff0c\u57fa\u4e8e140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c(TPP)\uff0c\u901a\u8fc7\u591a\u4e2aGPU\u5e76\u884c\u5904\u7406\u53bb\u566a\u6b65\u9aa4\uff0c\u63d0\u9ad8\u5b9e\u65f6\u6d41\u5a92\u4f53\u7684\u7a33\u5b9a\u6027\u548c\u4f4e\u5ef6\u8fdf\u3002</li>\n    <li>\u4e3a\u63d0\u9ad8\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u63d0\u51fa\u4e86\u6eda\u52a8\u53c2\u8003\u5e27\u673a\u5236(RSFM)\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5916\u89c2\u4fdd\u6301\u5e8f\u5217\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>Live Avatar\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e8620 FPS\u7684\u751f\u6210\u901f\u5ea6\uff0c\u662f\u9996\u4e2a\u5728\u6b64\u89c4\u6a21\u4e0b\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u4fdd\u771f\u5934\u50cf\u751f\u6210\u7684\u6280\u672f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new framework for creating high-quality animated avatars in real-time using a powerful 14-billion-parameter model.</li>\n    <li>It uses a technique called Timestep-forcing Pipeline Parallelism (TPP) to speed up the process by distributing work across multiple GPUs.</li>\n    <li>The framework includes a method called Rolling Sink Frame Mechanism (RSFM) to improve visual consistency and reduce errors in appearance.</li>\n    <li>Live Avatar can generate avatars at 20 frames per second using 5 GPUs, making it suitable for real-time applications.</li>\n    <li>This work represents a significant advancement in using diffusion models for long video synthesis in practical settings.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp \u662f\u4e00\u4e2a\u5305\u542b 210 \u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6a21\u62df\u4e86\u771f\u5b9e\u4f01\u4e1a\u6570\u636e\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u9700\u8981\u5728\u5de5\u4e1a\u6a21\u5f0f\u4e0a\u8fdb\u884c\u5b58\u50a8\u5e93\u7ea7\u7684\u5de5\u7a0b\u8bbe\u8ba1\u4e0e\u6784\u5efa\uff0c\u5305\u62ec\u4ece\u5934\u5f00\u59cb\u8bbe\u8ba1\u591a\u9636\u6bb5 SQL \u7ba1\u9053\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\u4efb\u52a1\u6d89\u53ca\u5f00\u653e\u5f0f\u5546\u4e1a\u95ee\u9898\uff0c\u9700\u8981\u6218\u7565\u89c4\u5212\u548c\u63a2\u7d22\u6027\u5206\u6790\uff0c\u751f\u6210\u53ef\u64cd\u4f5c\u7684\u5efa\u8bae\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u5148\u8fdb\u7684\u667a\u80fd\u4f53\u5728 DAComp \u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u4e0a\u6210\u529f\u7387\u4f4e\u4e8e 20%\u3002</li>\n    <li>DAComp \u4e3a\u5f00\u53d1\u771f\u6b63\u6709\u6548\u7684\u81ea\u4e3b\u6570\u636e\u667a\u80fd\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u660e\u786e\u4e86\u5de5\u7a0b\u548c\u5206\u6790\u80fd\u529b\u7684\u533a\u522b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DAComp is a benchmark made up of 210 tasks that represent complex real-world data workflows in businesses.</li>\n    <li>Data engineering tasks involve creating and improving SQL data pipelines, while data analysis tasks focus on solving open-ended business problems.</li>\n    <li>Tasks are evaluated using various methods, including execution-based scoring for engineering tasks and a well-designed rubric for analysis tasks.</li>\n    <li>Current advanced AI systems perform poorly on these tasks, especially in data engineering, with success rates below 20%, and below 40% in data analysis.</li>\n    <li>DAComp highlights the separate challenges of engineering and analysis, aiming to improve the development of autonomous data agents for businesses.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni \u662f\u4e00\u4e2a\u901a\u7528\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u76f4\u63a5\u4ece\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5206\u79bb\u95ee\u9898\u3002</li>\n    <li>Kling-Omni \u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5982\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\uff0c\u5904\u7406\u6210\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5efa\u7acb\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u4f18\u5316\u57fa\u7840\u8bbe\u65bd\u6765\u589e\u5f3a\u80fd\u529b\u3002</li>\n    <li>Kling-Omni \u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6709\u52a9\u4e8e\u53d1\u5c55\u80fd\u591f\u611f\u77e5\u3001\u63a8\u7406\u548c\u4e0e\u590d\u6742\u4e16\u754c\u4e92\u52a8\u7684\u591a\u6a21\u6001\u6a21\u62df\u5668\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system for creating high-quality videos from various inputs like text, images, and videos.</li>\n    <li>It combines video generation, editing, and reasoning into one unified framework, unlike older methods that were separate.</li>\n    <li>The system processes different types of user inputs into a single format to produce impressive video content.</li>\n    <li>Kling-Omni is built on a strong data system and uses advanced training techniques for better performance.</li>\n    <li>It shows excellent results in generating content, editing based on reasoning, and following complex instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.21631", "authors": [{"_id": "692ffb1a26742347f61daf38", "user": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "isPro": false, "fullname": "shuai bai", "user": "ShuaiBai623", "type": "user"}, "name": "Shuai Bai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:34:29.118Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf39", "name": "Yuxuan Cai", "hidden": false}, {"_id": "692ffb1a26742347f61daf3a", "name": "Ruizhe Chen", "hidden": false}, {"_id": "692ffb1a26742347f61daf3b", "name": "Keqin Chen", "hidden": false}, {"_id": "692ffb1a26742347f61daf3c", "user": {"_id": "63f30b870a16587ea970edfe", "avatarUrl": "/avatars/b58ab2d8a85a6d83462c297de2714ce4.svg", "isPro": false, "fullname": "Xiong-Hui Chen", "user": "xionghuichen", "type": "user"}, "name": "Xionghui Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:35:42.689Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf3d", "user": {"_id": "65b2529285b6c21448a10d65", "avatarUrl": "/avatars/1b09e2742aecce1bbdc57f0c4504cf38.svg", "isPro": false, "fullname": "Zesen Cheng", "user": "ClownRat", "type": "user"}, "name": "Zesen Cheng", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:35:51.365Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf3e", "name": "Lianghao Deng", "hidden": false}, {"_id": "692ffb1a26742347f61daf3f", "name": "Wei Ding", "hidden": false}, {"_id": "692ffb1a26742347f61daf40", "name": "Chang Gao", "hidden": false}, {"_id": "692ffb1a26742347f61daf41", "name": "Chunjiang Ge", "hidden": false}, {"_id": "692ffb1a26742347f61daf42", "name": "Wenbin Ge", "hidden": false}, {"_id": "692ffb1a26742347f61daf43", "name": "Zhifang Guo", "hidden": false}, {"_id": "692ffb1a26742347f61daf44", "user": {"_id": "656f1b21b075b63c90ba02ee", "avatarUrl": "/avatars/d6856815ef06261394178161e4d511b4.svg", "isPro": false, "fullname": "Huang Qidong", "user": "shikiw", "type": "user"}, "name": "Qidong Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:49.065Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf45", "name": "Jie Huang", "hidden": false}, {"_id": "692ffb1a26742347f61daf46", "name": "Fei Huang", "hidden": false}, {"_id": "692ffb1a26742347f61daf47", "name": "Binyuan Hui", "hidden": false}, {"_id": "692ffb1a26742347f61daf48", "name": "Shutong Jiang", "hidden": false}, {"_id": "692ffb1a26742347f61daf49", "name": "Zhaohai Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4a", "name": "Mingsheng Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4b", "name": "Mei Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4c", "user": {"_id": "6346be8f7fb9f11870c63998", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6346be8f7fb9f11870c63998/tFWawSkXL6bv1zgvzFWQd.png", "isPro": false, "fullname": "Kaixin Li", "user": "likaixin", "type": "user"}, "name": "Kaixin Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:53.648Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf4d", "user": {"_id": "67a31313cf9d856beb7f9afb", "avatarUrl": "/avatars/69395b134716f750545eab35a164e51f.svg", "isPro": false, "fullname": "Zicheng Lin", "user": "etonlin", "type": "user"}, "name": "Zicheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:36:50.803Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf4e", "name": "Junyang Lin", "hidden": false}, {"_id": "692ffb1a26742347f61daf4f", "name": "Xuejing Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf50", "name": "Jiawei Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf51", "name": "Chenglong Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf52", "name": "Yang Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf53", "name": "Dayiheng Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf54", "user": {"_id": "64e72776e9fc9d0475ef5188", "avatarUrl": "/avatars/d32b9d4e1da5486c3d5f9b04fa29d167.svg", "isPro": false, "fullname": "Shixuan Liu", "user": "liusx", "type": "user"}, "name": "Shixuan Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:58.470Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf55", "name": "Dunjie Lu", "hidden": false}, {"_id": "692ffb1a26742347f61daf56", "name": "Ruilin Luo", "hidden": false}, {"_id": "692ffb1a26742347f61daf57", "name": "Chenxu Lv", "hidden": false}, {"_id": "692ffb1a26742347f61daf58", "name": "Rui Men", "hidden": false}, {"_id": "692ffb1a26742347f61daf59", "name": "Lingchen Meng", "hidden": false}, {"_id": "692ffb1a26742347f61daf5a", "name": "Xuancheng Ren", "hidden": false}, {"_id": "692ffb1a26742347f61daf5b", "name": "Xingzhang Ren", "hidden": false}, {"_id": "692ffb1a26742347f61daf5c", "name": "Sibo Song", "hidden": false}, {"_id": "692ffb1a26742347f61daf5d", "name": "Yuchong Sun", "hidden": false}, {"_id": "692ffb1a26742347f61daf5e", "name": "Jun Tang", "hidden": false}, {"_id": "692ffb1a26742347f61daf5f", "name": "Jianhong Tu", "hidden": false}, {"_id": "692ffb1a26742347f61daf60", "name": "Jianqiang Wan", "hidden": false}, {"_id": "692ffb1a26742347f61daf61", "name": "Peng Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf62", "name": "Pengfei Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf63", "name": "Qiuyue Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf64", "name": "Yuxuan Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf65", "name": "Tianbao Xie", "hidden": false}, {"_id": "692ffb1a26742347f61daf66", "name": "Yiheng Xu", "hidden": false}, {"_id": "692ffb1a26742347f61daf67", "user": {"_id": "645b10e80c73ea27d13f7aca", "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg", "isPro": false, "fullname": "xuhaiyang", "user": "xhyandwyy", "type": "user"}, "name": "Haiyang Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:51.583Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf68", "name": "Jin Xu", "hidden": false}, {"_id": "692ffb1a26742347f61daf69", "name": "Zhibo Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6a", "name": "Mingkun Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6b", "name": "Jianxin Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6c", "name": "An Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6d", "name": "Bowen Yu", "hidden": false}, {"_id": "692ffb1a26742347f61daf6e", "name": "Fei Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6f", "name": "Hang Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf70", "name": "Xi Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf71", "name": "Bo Zheng", "hidden": false}, {"_id": "692ffb1a26742347f61daf72", "name": "Humen Zhong", "hidden": false}, {"_id": "692ffb1a26742347f61daf73", "name": "Jingren Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf74", "name": "Fan Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf75", "name": "Jing Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf76", "user": {"_id": "627d2723401f42c57b6b7c0c", "avatarUrl": "/avatars/6ff754e56aaee63d8572881a6a966171.svg", "isPro": false, "fullname": "Yuanzhi Zhu", "user": "Yuanzhi", "type": "user"}, "name": "Yuanzhi Zhu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:36:59.879Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf77", "name": "Ke Zhu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63451cf0a05b51f7ded25505/kVvzrCGrjwoK0CYh4DKif.jpeg"], "publishedAt": "2025-11-26T17:59:08.000Z", "submittedOnDailyAt": "2025-12-04T01:02:46.772Z", "title": "Qwen3-VL Technical Report", "submittedOnDailyBy": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "isPro": false, "fullname": "shuai bai", "user": "ShuaiBai623", "type": "user"}, "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.", "upvotes": 110, "discussionId": "692ffb1b26742347f61daf78", "ai_summary": "Qwen3-VL, a vision-language model, excels in text and multimodal understanding through advanced architectures and larger contexts, achieving superior performance across benchmarks.", "ai_keywords": ["vision-language model", "interleaved contexts", "multimodal benchmarks", "dense variants", "mixture-of-experts", "pure-text understanding", "long-context comprehension", "multimodal reasoning", "MMMU", "visual-math benchmarks", "interleaved-MRoPE", "DeepStack", "text-based time alignment", "T-RoPE", "explicit textual timestamp alignment", "vision-language alignment", "image-grounded reasoning", "agentic decision-making", "multimodal code intelligence"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>Qwen3-VL\u662fQwen\u7cfb\u5217\u4e2d\u6700\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u8d8a\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u3002</li>\n    <li>\u8be5\u6a21\u578b\u652f\u6301\u6700\u591a256K\u4e2a\u4ee4\u724c\u7684\u4ea4\u9519\u4e0a\u4e0b\u6587\uff0c\u53ef\u65e0\u7f1d\u6574\u5408\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u3002</li>\n    <li>Qwen3-VL\u63d0\u4f9b\u5f3a\u5927\u7684\u6587\u672c\u7406\u89e3\u80fd\u529b\u548c\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u80fd\u6709\u6548\u5904\u7406\u957f\u6587\u6863\u548c\u89c6\u9891\u3002</li>\n    <li>\u5177\u5907\u5148\u8fdb\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u5728\u56fe\u50cf\u548c\u89c6\u9891\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002</li>\n    <li>\u6a21\u578b\u67b6\u6784\u8fdb\u884c\u4e86\u4e09\u9879\u5173\u952e\u5347\u7ea7\uff0c\u63d0\u5347\u4e86\u7a7a\u95f4\u65f6\u95f4\u5efa\u6a21\u548c\u89c6\u89c9\u8bed\u8a00\u5bf9\u9f50\u7684\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Qwen3-VL is the latest and most advanced model in the Qwen series, excelling in various multimodal tasks involving text, images, and video.</li>\n    <li>It can handle up to 256,000 tokens at once, allowing for better understanding and integration of long documents and videos.</li>\n    <li>The model comes in different sizes and types to balance performance and processing speed.</li>\n    <li>It offers improved understanding of text, robust long-context comprehension, and strong reasoning abilities across images and videos.</li>\n    <li>Key upgrades include better modeling of spatial and temporal information, improved vision-language alignment, and precise timing for video content.</li>\n</ul>"}, "publishedAt": "2025-11-26T12:59:08.000Z", "title": "Qwen3-VL Technical Report", "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63451cf0a05b51f7ded25505/kVvzrCGrjwoK0CYh4DKif.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21631.png", "numComments": 3, "submittedBy": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "fullname": "shuai bai", "name": "ShuaiBai623", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 42}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.21689", "authors": [{"_id": "692f11ffbfc6eea1b6fb6900", "name": "Hongjin Su", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6901", "name": "Shizhe Diao", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6902", "name": "Ximing Lu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6903", "name": "Mingjie Liu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6904", "name": "Jiacheng Xu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6905", "name": "Xin Dong", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6906", "name": "Yonggan Fu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6907", "name": "Peter Belcak", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6908", "name": "Hanrong Ye", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6909", "name": "Hongxu Yin", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690a", "name": "Yi Dong", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690b", "name": "Evelina Bakhturina", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690c", "name": "Tao Yu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690d", "name": "Yejin Choi", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690e", "name": "Jan Kautz", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690f", "name": "Pavlo Molchanov", "hidden": false}], "publishedAt": "2025-11-26T18:59:46.000Z", "submittedOnDailyAt": "2025-12-03T13:46:43.945Z", "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration", "submittedOnDailyBy": {"_id": "633bd54b00732349209a18fe", "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg", "isPro": false, "fullname": "Shizhe Diao", "user": "shizhediao", "type": "user"}, "summary": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.", "upvotes": 96, "discussionId": "692f11ffbfc6eea1b6fb6910", "projectPage": "https://research.nvidia.com/labs/lpr/ToolOrchestra/", "githubRepo": "https://github.com/NVlabs/ToolOrchestra/", "ai_summary": "A small orchestrator using ToolOrchestra method coordinates various intelligent tools with reinforcement learning, achieving higher accuracy and efficiency in solving complex tasks like Humanity's Last Exam compared to larger models.", "ai_keywords": ["large language models", "ToolOrchestra", "reinforcement learning", "outcome-aware rewards", "efficiency-aware rewards", "user-preference-aware rewards", "Orchestrator", "tool-use agents", "humanity's last exam", "tau2-bench", "FRAMES", "tool-augmented reasoning systems"], "githubStars": 297, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u5728\u89e3\u51b3\u590d\u6742\u95ee\u9898\u65f6\u4ecd\u9762\u4e34\u6311\u6218\u548c\u9ad8\u6210\u672c\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86ToolOrchestra\uff0c\u4e00\u79cd\u8bad\u7ec3\u5c0f\u578b\u534f\u8c03\u8005\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5730\u7ba1\u7406\u667a\u80fd\u5de5\u5177\u3002</li>\n    <li>ToolOrchestra\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff0c\u8003\u8651\u7ed3\u679c\u3001\u6548\u7387\u548c\u7528\u6237\u504f\u597d\u8fdb\u884c\u5956\u52b1\u3002</li>\n    <li>\u65b0\u6a21\u578bOrchestrator\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u51c6\u786e\u6027\u66f4\u9ad8\uff0c\u6210\u672c\u66f4\u4f4e\uff0c\u4e14\u7b26\u5408\u7528\u6237\u504f\u597d\u3002</li>\n    <li>Orchestrator\u5728\u591a\u4e2a\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8eGPT-5\uff0c\u540c\u65f6\u66f4\u8282\u7701\u8d44\u6e90\uff0c\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u6548\u7387\u548c\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models are good at many tasks but struggle with very complex problems.</li>\n    <li>ToolOrchestra is a new method that trains small models to manage and coordinate various tools to solve difficult tasks more efficiently.</li>\n    <li>The new model, Orchestrator, is smaller (8 billion parameters) but performs better and costs less than previous models when using tools.</li>\n    <li>Orchestrator scores 37.1% on a complex test, beating GPT-5\u2019s score of 35.1% while being 2.5 times more efficient.</li>\n    <li>This approach shows that using a simple model to manage different tools is more effective and efficient than current methods.</li>\n</ul>"}, "publishedAt": "2025-11-26T13:59:46.000Z", "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration", "summary": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21689.png", "numComments": 3, "submittedBy": {"_id": "633bd54b00732349209a18fe", "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg", "fullname": "Shizhe Diao", "name": "shizhediao", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Dec 25, 2025";
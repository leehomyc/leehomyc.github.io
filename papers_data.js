window.trendingPapers = {
    "today": [{"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\uff0c\u5229\u7528\u89c6\u89c9\u7ebf\u7d22\u8fdb\u884c\u5b9a\u4f4d\u3002</li>\n    <li>\u5f53\u524d\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5ffd\u89c6\u4e86\u4eba\u7c7b\u5e38\u7528\u7684\u7b56\u7565\u2014\u2014\u4f7f\u7528\u5730\u56fe\u3002</li>\n    <li>\u672c\u7814\u7a76\u4e3a\u6a21\u578b\u6dfb\u52a0\u4e86\u201c\u601d\u8003\u5730\u56fe\u201d\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u8bbe\u8ba1\u4e3a\u4e00\u4e2a\u201c\u5730\u56fe\u4e2d\u7684\u667a\u80fd\u4f53\u201d\u5faa\u73af\u3002</li>\n    <li>\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u589e\u5f3a\u5b66\u4e60\uff08RL\uff09\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08TTS\uff09\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6548\u7387\u548c\u63a2\u7d22\u80fd\u529b\u3002</li>\n    <li>\u5728\u6700\u65b0\u7684\u771f\u5b9e\u56fe\u50cf\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u7279\u522b\u662f\u51c6\u786e\u5ea6\u663e\u8457\u63d0\u9ad8\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>The goal of image geolocalization is to predict where a photo was taken using visual clues.</li>\n    <li>Current models use advanced techniques but often ignore the human strategy of using maps.</li>\n    <li>This study introduces a new model that incorporates map-thinking into its process.</li>\n    <li>The model improves its decision-making through reinforcement learning and explores multiple options before making a prediction.</li>\n    <li>Tests show this new method significantly outperforms previous models on real-world images.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03017", "authors": [{"_id": "696488cc138cc47cbd765365", "name": "Jing Xiong", "hidden": false}, {"_id": "696488cc138cc47cbd765366", "name": "Qi Han", "hidden": false}, {"_id": "696488cc138cc47cbd765367", "name": "Yunta Hsieh", "hidden": false}, {"_id": "696488cc138cc47cbd765368", "name": "Hui Shen", "hidden": false}, {"_id": "696488cc138cc47cbd765369", "name": "Huajian Xin", "hidden": false}, {"_id": "696488cc138cc47cbd76536a", "name": "Chaofan Tao", "hidden": false}, {"_id": "696488cc138cc47cbd76536b", "name": "Chenyang Zhao", "hidden": false}, {"_id": "696488cc138cc47cbd76536c", "name": "Hengyuan Zhang", "hidden": false}, {"_id": "696488cc138cc47cbd76536d", "name": "Taiqiang Wu", "hidden": false}, {"_id": "696488cc138cc47cbd76536e", "name": "Zhen Zhang", "hidden": false}, {"_id": "696488cc138cc47cbd76536f", "name": "Haochen Wang", "hidden": false}, {"_id": "696488cc138cc47cbd765370", "name": "Zhongwei Wan", "hidden": false}, {"_id": "696488cc138cc47cbd765371", "name": "Lingpeng Kong", "hidden": false}, {"_id": "696488cc138cc47cbd765372", "name": "Ngai Wong", "hidden": false}], "publishedAt": "2026-01-06T13:42:51.000Z", "submittedOnDailyAt": "2026-01-12T03:10:40.203Z", "title": "MMFormalizer: Multimodal Autoformalization in the Wild", "submittedOnDailyBy": {"_id": "60851545a5da133ac6c38686", "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg", "isPro": false, "fullname": "Jing Xiong", "user": "menik1126", "type": "user"}, "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io", "upvotes": 94, "discussionId": "696488cc138cc47cbd765373", "projectPage": "https://mmformalizer.github.io/", "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.", "ai_keywords": ["autoformalization", "multimodal", "perceptually grounded primitives", "recursive grounding", "axiom composition", "adaptive recursive termination", "dimensional grounding", "axiomatic grounding", "PhyX-AF", "MathVerse", "PhyX", "Synthetic Geometry", "Analytic Geometry", "GPT-5", "Gemini-3-Pro", "classical mechanics", "relativity", "quantum mechanics", "thermodynamics"], "summary_zh": "<ul>\n    <li>Autoformalization \u662f\u5c06\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u8f6c\u6362\u4e3a\u6b63\u5f0f\u9648\u8ff0\u4ee5\u652f\u6301\u673a\u5668\u63a8\u7406\u7684\u8fc7\u7a0b\uff0c\u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u9762\u4e34\u6311\u6218\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86 MMFormalizer\uff0c\u5b83\u901a\u8fc7\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u5b66\u548c\u7269\u7406\u9886\u57df\u7684\u5b9e\u4f53\u7ed3\u5408\uff0c\u6269\u5c55\u4e86 autoformalization \u7684\u5e94\u7528\u3002</li>\n    <li>MMFormalizer \u901a\u8fc7\u9012\u5f52\u6784\u5efa\u5f62\u5f0f\u547d\u9898\uff0c\u786e\u4fdd\u6bcf\u4e2a\u62bd\u8c61\u90fd\u6709\u89c6\u89c9\u8bc1\u636e\u652f\u6301\u3002</li>\n    <li>\u5728\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5 PhyX-AF \u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a GPT-5 \u5728\u7269\u7406\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u51e0\u4f55\u9886\u57df\u4ecd\u7136\u662f\u6700\u5177\u6311\u6218\u6027\u7684\u3002</li>\n    <li>MMFormalizer \u662f\u9996\u4e2a\u80fd\u591f\u5904\u7406\u7ecf\u5178\u529b\u5b66\u3001\u76f8\u5bf9\u8bba\u3001\u91cf\u5b50\u529b\u5b66\u548c\u70ed\u529b\u5b66\u7684\u591a\u6a21\u6001 autoformalization \u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoformalization helps translate natural language math into formal statements for machine reasoning, but it struggles with real-world complexities.</li>\n    <li>MMFormalizer is a new method that integrates visual elements with mathematics and physics to improve this process.</li>\n    <li>It builds formal propositions from basic visual concepts, ensuring that every idea is backed by visual proof and relevant principles.</li>\n    <li>MMFormalizer was tested on a new benchmark called PhyX-AF, which includes a variety of multimodal tasks, showing strong results with advanced models like GPT-5.</li>\n    <li>This method is the first to effectively handle complex topics like classical mechanics, relativity, quantum mechanics, and thermodynamics in an integrated way.</li>\n</ul>"}, "publishedAt": "2026-01-06T08:42:51.000Z", "title": "MMFormalizer: Multimodal Autoformalization in the Wild", "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png", "numComments": 1, "submittedBy": {"_id": "60851545a5da133ac6c38686", "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg", "fullname": "Jing Xiong", "name": "menik1126", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.03319", "authors": [{"_id": "6960e7365b7998385e6396e3", "user": {"_id": "6761f183e5b85d453550147a", "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg", "isPro": false, "fullname": "EldadMat", "user": "eldad929", "type": "user"}, "name": "Eldad Matmon", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T15:45:52.792Z", "hidden": false}, {"_id": "6960e7365b7998385e6396e4", "name": "Amit Bracha", "hidden": false}, {"_id": "6960e7365b7998385e6396e5", "user": {"_id": "62b3e85bcbd2a402fc7804b1", "avatarUrl": "/avatars/63125ce8a1e20b8c6e836f223d24284f.svg", "isPro": false, "fullname": "noam rotstein", "user": "noamrot", "type": "user"}, "name": "Noam Rotstein", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:36:06.891Z", "hidden": false}, {"_id": "6960e7365b7998385e6396e6", "name": "Ron Kimmel", "hidden": false}], "publishedAt": "2026-01-06T13:56:28.000Z", "submittedOnDailyAt": "2026-01-12T03:43:15.850Z", "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature", "submittedOnDailyBy": {"_id": "6761f183e5b85d453550147a", "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg", "isPro": false, "fullname": "EldadMat", "user": "eldad929", "type": "user"}, "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.", "upvotes": 45, "discussionId": "6960e7365b7998385e6396e7", "projectPage": "https://c4ricaturegs.github.io/", "ai_summary": "A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.", "ai_keywords": ["3D Gaussian Splatting", "FLAME mesh", "curvature-weighted Poisson equation", "pseudo-ground-truth caricature images", "local affine transformations", "real-time deformations", "closed-form solutions"], "organization": {"_id": "6393322be2364bc1eea56e45", "name": "Technion", "fullname": "Technion Israel institute of technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u63a7\u76843D\u9762\u90e8\u6f2b\u753b\u5316\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u903c\u771f\u7684\u6548\u679c\u3002</li>\n    <li>\u91c7\u7528\u57fa\u4e8e\u9ad8\u65af\u66f2\u7387\u7684\u8868\u9762\u5938\u5f20\u6280\u672f\uff0c\u5e76\u7ed3\u54083D\u9ad8\u65af\u70b9\u4e91\u6280\u672f\uff0c\u4ee5\u6539\u5584\u6e32\u67d3\u8d28\u91cf\u3002</li>\n    <li>\u901a\u8fc7\u63d0\u53d6FLAME\u7f51\u683c\uff0c\u89e3\u51b3\u4e86\u66f2\u7387\u52a0\u6743\u7684\u6cca\u677e\u65b9\u7a0b\uff0c\u5f97\u5230\u4e86\u5938\u5f20\u7684\u9762\u90e8\u6a21\u578b\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e00\u79cd\u8bad\u7ec3\u65b9\u6848\uff0c\u7ed3\u5408\u771f\u5b9e\u4e0e\u5408\u6210\u7684\u76d1\u7763\uff0c\u4f7f\u5f97\u9ad8\u65af\u96c6\u5408\u80fd\u591f\u8868\u793a\u81ea\u7136\u548c\u5938\u5f20\u7684\u5934\u50cf\u3002</li>\n    <li>\u5b9e\u73b0\u4e86\u5b9e\u65f6\u53d8\u5f62\uff0c\u5e76\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u5de5\u4f5c\uff0c\u751f\u6210\u4e86\u903c\u771f\u7684\u6f2b\u753b\u5934\u50cf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>A new framework creates realistic 3D caricatures of faces that can be easily controlled.</li>\n    <li>The method uses a special technique to exaggerate facial features while addressing smoothing issues with advanced 3D Gaussian Splatting.</li>\n    <li>It involves extracting a mesh from multiple views of a face and applying a mathematical approach to enhance its shape.</li>\n    <li>The framework synthesizes caricature images for training, allowing for better representation of both normal and exaggerated faces.</li>\n    <li>It enables real-time adjustments and consistently produces high-quality caricature avatars that outperform previous methods.</li>\n</ul>"}, "publishedAt": "2026-01-06T08:56:28.000Z", "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature", "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03319.png", "numComments": 1, "submittedBy": {"_id": "6761f183e5b85d453550147a", "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg", "fullname": "EldadMat", "name": "eldad929", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "6393322be2364bc1eea56e45", "name": "Technion", "fullname": "Technion Israel institute of technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06002", "authors": [{"_id": "6964644c138cc47cbd76529b", "user": {"_id": "636f526a6cd69d9a36ff2b53", "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg", "isPro": false, "fullname": "Qiguang Chen", "user": "LightChen2333", "type": "user"}, "name": "Qiguang Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:33:48.803Z", "hidden": false}, {"_id": "6964644c138cc47cbd76529c", "name": "Yantao Du", "hidden": false}, {"_id": "6964644c138cc47cbd76529d", "name": "Ziniu Li", "hidden": false}, {"_id": "6964644c138cc47cbd76529e", "name": "Jinhao Liu", "hidden": false}, {"_id": "6964644c138cc47cbd76529f", "name": "Songyao Duan", "hidden": false}, {"_id": "6964644c138cc47cbd7652a0", "name": "Jiarui Guo", "hidden": false}, {"_id": "6964644c138cc47cbd7652a1", "name": "Minghao Liu", "hidden": false}, {"_id": "6964644c138cc47cbd7652a2", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6964644c138cc47cbd7652a3", "name": "Tong Yang", "hidden": false}, {"_id": "6964644c138cc47cbd7652a4", "user": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "name": "Ge Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:33:51.170Z", "hidden": false}, {"_id": "6964644c138cc47cbd7652a5", "name": "Libo Qin", "hidden": false}, {"_id": "6964644c138cc47cbd7652a6", "name": "Wanxiang Che", "hidden": false}, {"_id": "6964644c138cc47cbd7652a7", "name": "Wenhao Huang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"], "publishedAt": "2026-01-09T18:39:01.000Z", "submittedOnDailyAt": "2026-01-12T01:02:27.368Z", "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning", "submittedOnDailyBy": {"_id": "636f526a6cd69d9a36ff2b53", "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg", "isPro": false, "fullname": "Qiguang Chen", "user": "LightChen2333", "type": "user"}, "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.", "upvotes": 38, "discussionId": "6964644c138cc47cbd7652a8", "ai_summary": "Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.", "ai_keywords": ["chain-of-thought", "large language models", "Long CoT", "fine-tuning", "entropy convergence", "semantic isomers", "distribution-transfer-graph", "molecular-like structures", "deep reasoning", "self-reflection", "self-exploration"], "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5b66\u4e60\u6709\u6548\u7684\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u6709\u7a33\u5b9a\u7684\u5206\u5b50\u7ed3\u6784\uff0c\u7531\u4e09\u79cd\u76f8\u4e92\u4f5c\u7528\u7c7b\u578b\u6784\u6210\u3002</li>\n    <li>\u5206\u6790\u663e\u793a\uff0c\u8fd9\u4e9b\u7ed3\u6784\u662f\u901a\u8fc7\u957f\u94fe\u601d\u7ef4\u7684\u5fae\u8c03\u5f62\u6210\u7684\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u6a21\u4eff\u5173\u952e\u8bcd\u3002</li>\n    <li>\u5f15\u5165\u6709\u6548\u7684\u8bed\u4e49\u5f02\u6784\u4f53\uff0c\u5f3a\u8c03\u4fc3\u8fdb\u5feb\u901f\u71b5\u6536\u655b\u7684\u7ed3\u5408\u6709\u52a9\u4e8e\u7a33\u5b9a\u5b66\u4e60\u3002</li>\n    <li>\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u63d0\u51faMole-Syn\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u957f\u94fe\u601d\u7ef4\u7ed3\u6784\u7684\u5408\u6210\u6548\u679c\u548c\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models struggle with learning long chain-of-thought reasoning from other models or humans.</li>\n    <li>Effective long reasoning patterns have stable structures formed by three types of interactions: Deep-Reasoning, Self-Reflection, and Self-Exploration.</li>\n    <li>These structures are developed through fine-tuning, rather than just imitating keywords.</li>\n    <li>We introduce a method called Mole-Syn that helps create effective long reasoning structures, improving model performance and stability.</li>\n    <li>Only certain types of interactions support successful long reasoning learning, while competition between structures can hinder it.</li>\n</ul>"}, "publishedAt": "2026-01-09T13:39:01.000Z", "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning", "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06002.png", "numComments": 1, "submittedBy": {"_id": "636f526a6cd69d9a36ff2b53", "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg", "fullname": "Qiguang Chen", "name": "LightChen2333", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06021", "authors": [{"_id": "69645f30138cc47cbd765248", "name": "Jiajie Zhang", "hidden": false}, {"_id": "69645f30138cc47cbd765249", "name": "Xin Lv", "hidden": false}, {"_id": "69645f30138cc47cbd76524a", "name": "Ling Feng", "hidden": false}, {"_id": "69645f30138cc47cbd76524b", "name": "Lei Hou", "hidden": false}, {"_id": "69645f30138cc47cbd76524c", "name": "Juanzi Li", "hidden": false}], "publishedAt": "2026-01-09T18:57:53.000Z", "submittedOnDailyAt": "2026-01-12T00:13:19.034Z", "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards", "submittedOnDailyBy": {"_id": "66cdd285c51a915bd5f2d017", "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg", "isPro": false, "fullname": "Jiajie Zhang", "user": "NeoZ123", "type": "user"}, "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.", "upvotes": 30, "discussionId": "69645f30138cc47cbd76524d", "githubRepo": "https://github.com/THUDM/CaRR", "githubRepoAddedBy": "user", "ai_summary": "A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.", "ai_keywords": ["reinforcement learning", "deep search agents", "fine-grained reward framework", "reasoning comprehensiveness", "factual grounding", "evidence connectivity", "verifiable single-hop rubrics", "citation-aware group relative policy optimization", "outcome rewards", "shortcut exploitation", "hallucinations"], "githubStars": 15, "organization": {"_id": "62ad27f19096e7f9ecb1853a", "name": "zai-org", "fullname": "Z.ai", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u88ab\u7528\u6765\u63d0\u5347\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u7684\u6027\u80fd\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u4e8c\u5143\u7ed3\u679c\u5956\u52b1\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u4ee3\u7406\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u4e0d\u826f\u884c\u4e3a\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u5f15\u7528\u610f\u8bc6\u8bc4\u5206\u5956\u52b1\uff08CaRR\uff09\uff0c\u5f3a\u8c03\u63a8\u7406\u7684\u5168\u9762\u6027\u3001\u4e8b\u5b9e\u57fa\u7840\u548c\u8bc1\u636e\u8fde\u63a5\u3002</li>\n    <li>CaRR \u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u53ef\u9a8c\u8bc1\u7684\u5355\u6b65\u8bc4\u5206\uff0c\u8981\u6c42\u4ee3\u7406\u8bc6\u522b\u9690\u85cf\u5b9e\u4f53\u5e76\u63d0\u4f9b\u6b63\u786e\u7684\u5f15\u7528\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408CaRR\u548c\u7ed3\u679c\u5956\u52b1\u7684\u5f15\u7528\u610f\u8bc6\u7ec4\u76f8\u5bf9\u653f\u7b56\u4f18\u5316\uff08C-GRPO\uff09\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) is important for improving deep search agents that use large language models (LLMs).</li>\n    <li>Current methods mostly use simple rewards based on yes/no outcomes, which can lead to poor reasoning and incorrect answers.</li>\n    <li>The new Citation-aware Rubric Rewards (CaRR) framework encourages better reasoning by breaking down questions and requiring evidence and citations.</li>\n    <li>We also developed Citation-aware Group Relative Policy Optimization (C-GRPO) to integrate CaRR with traditional rewards for better training results.</li>\n    <li>Tests show C-GRPO is more effective than existing methods, reducing mistakes and improving reasoning quality in search tasks.</li>\n</ul>"}, "publishedAt": "2026-01-09T13:57:53.000Z", "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards", "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06021.png", "numComments": 1, "submittedBy": {"_id": "66cdd285c51a915bd5f2d017", "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg", "fullname": "Jiajie Zhang", "name": "NeoZ123", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "62ad27f19096e7f9ecb1853a", "name": "zai-org", "fullname": "Z.ai", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.05808", "authors": [{"_id": "696462fd138cc47cbd765289", "name": "Xiaoshuai Song", "hidden": false}, {"_id": "696462fd138cc47cbd76528a", "name": "Haofei Chang", "hidden": false}, {"_id": "696462fd138cc47cbd76528b", "user": {"_id": "61cd4b833dd34ba1985e0753", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61cd4b833dd34ba1985e0753/BfHfrwotoMESpXZOHiIe4.png", "isPro": false, "fullname": "KABI", "user": "dongguanting", "type": "user"}, "name": "Guanting Dong", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:24.231Z", "hidden": false}, {"_id": "696462fd138cc47cbd76528c", "name": "Yutao Zhu", "hidden": false}, {"_id": "696462fd138cc47cbd76528d", "name": "Zhicheng Dou", "hidden": false}, {"_id": "696462fd138cc47cbd76528e", "name": "Ji-Rong Wen", "hidden": false}], "publishedAt": "2026-01-09T14:32:06.000Z", "submittedOnDailyAt": "2026-01-12T00:32:15.159Z", "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis", "submittedOnDailyBy": {"_id": "6621ec2524eb2673fe0790fc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg", "isPro": false, "fullname": "Ania Forge", "user": "zhangboguodong", "type": "user"}, "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.", "upvotes": 24, "discussionId": "696462fe138cc47cbd76528f", "ai_summary": "EnvScaler automates the creation of tool-interaction environments through programmatic synthesis, enhancing LLM performance in complex multi-turn, multi-tool tasks via supervised fine-tuning and reinforcement learning.", "ai_keywords": ["tool-interaction environments", "programmatic synthesis", "environment skeletons", "task scenarios", "rule-based trajectory validation", "supervised fine-tuning", "reinforcement learning", "multi-turn interactions", "multi-tool interactions"], "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aEnvScaler\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u521b\u5efa\u53ef\u6269\u5c55\u7684\u5de5\u5177\u4e92\u52a8\u73af\u5883\u3002</li>\n    <li>EnvScaler\u7531\u4e24\u4e2a\u90e8\u5206\u7ec4\u6210\uff1aSkelBuilder\u548cScenGenerator\uff0c\u524d\u8005\u6784\u5efa\u73af\u5883\u6846\u67b6\uff0c\u540e\u8005\u751f\u6210\u4efb\u52a1\u573a\u666f\u3002</li>\n    <li>\u901a\u8fc7EnvScaler\uff0c\u5408\u6210\u4e86191\u4e2a\u73af\u5883\u548c\u7ea67000\u4e2a\u573a\u666f\uff0c\u5e94\u7528\u4e8eQwen3\u7cfb\u5217\u6a21\u578b\u7684\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u3002</li>\n    <li>\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEnvScaler\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u89e3\u51b3\u4efb\u52a1\u7684\u80fd\u529b\u3002</li>\n    <li>\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u5df2\u53d1\u5e03\u5728GitHub\u4e0a\uff0c\u4f9b\u5927\u5bb6\u4f7f\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) need rich environments to learn how to interact with tools in the real world.</li>\n    <li>Access to real systems is limited, and creating these environments manually is difficult and not scalable.</li>\n    <li>The paper introduces EnvScaler, an automated system that creates scalable tool-interaction environments.</li>\n    <li>EnvScaler has two main parts: SkelBuilder makes diverse environment frameworks, and ScenGenerator creates task scenarios and validation functions.</li>\n    <li>Using EnvScaler, the authors created 191 environments and around 7,000 scenarios, improving LLMs' performance in complex tasks significantly.</li>\n</ul>"}, "publishedAt": "2026-01-09T09:32:06.000Z", "title": "EnvScaler: Scaling Tool-Interactive Environments for LLM Agent via Programmatic Synthesis", "summary": "Large language models (LLMs) are expected to be trained to act as agents in various real-world environments, but this process relies on rich and varied tool-interaction sandboxes. However, access to real systems is often restricted; LLM-simulated environments are prone to hallucinations and inconsistencies; and manually built sandboxes are hard to scale. In this paper, we propose EnvScaler, an automated framework for scalable tool-interaction environments via programmatic synthesis. EnvScaler comprises two components. First, SkelBuilder constructs diverse environment skeletons through topic mining, logic modeling, and quality evaluation. Then, ScenGenerator generates multiple task scenarios and rule-based trajectory validation functions for each environment. With EnvScaler, we synthesize 191 environments and about 7K scenarios, and apply them to Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for Qwen3 series models. Results on three benchmarks show that EnvScaler significantly improves LLMs' ability to solve tasks in complex environments involving multi-turn, multi-tool interactions. We release our code and data at https://github.com/RUC-NLPIR/EnvScaler.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05808.png", "numComments": 4, "submittedBy": {"_id": "6621ec2524eb2673fe0790fc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6621ec2524eb2673fe0790fc/cooTXi12eRWFiSSIj_nA-.jpeg", "fullname": "Ania Forge", "name": "zhangboguodong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.04720", "authors": [{"_id": "69647212138cc47cbd765311", "name": "Mingxin Li", "hidden": false}, {"_id": "69647212138cc47cbd765312", "name": "Yanzhao Zhang", "hidden": false}, {"_id": "69647212138cc47cbd765313", "user": {"_id": "616adb8578833ce5997e441a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg", "isPro": false, "fullname": "Dingkun Long", "user": "thenlper", "type": "user"}, "name": "Dingkun Long", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:33:42.987Z", "hidden": false}, {"_id": "69647212138cc47cbd765314", "name": "Keqin Chen", "hidden": false}, {"_id": "69647212138cc47cbd765315", "name": "Sibo Song", "hidden": false}, {"_id": "69647212138cc47cbd765316", "name": "Shuai Bai", "hidden": false}, {"_id": "69647212138cc47cbd765317", "name": "Zhibo Yang", "hidden": false}, {"_id": "69647212138cc47cbd765318", "name": "Pengjun Xie", "hidden": false}, {"_id": "69647212138cc47cbd765319", "name": "An Yang", "hidden": false}, {"_id": "69647212138cc47cbd76531a", "name": "Dayiheng Liu", "hidden": false}, {"_id": "69647212138cc47cbd76531b", "name": "Jingren Zhou", "hidden": false}, {"_id": "69647212138cc47cbd76531c", "name": "Junyang Lin", "hidden": false}], "publishedAt": "2026-01-08T08:36:06.000Z", "submittedOnDailyAt": "2026-01-12T02:49:49.573Z", "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "submittedOnDailyBy": {"_id": "616adb8578833ce5997e441a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg", "isPro": false, "fullname": "Dingkun Long", "user": "thenlper", "type": "user"}, "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.", "upvotes": 20, "discussionId": "69647212138cc47cbd76531d", "githubRepo": "https://github.com/QwenLM/Qwen3-VL-Embedding", "githubRepoAddedBy": "auto", "ai_summary": "The Qwen3-VL-Embedding and Qwen3-VL-Reranker models form an end-to-end multimodal search pipeline, leveraging multi-stage training and cross-attention mechanisms to achieve high-precision retrieval across diverse modalities.", "ai_keywords": ["Qwen3-VL-Embedding", "Qwen3-VL-Reranker", "multimodal search", "multi-stage training", "contrastive pre-training", "model distillation", "Matryoshka Representation Learning", "cross-encoder architecture", "cross-attention mechanisms", "multimodal embedding", "visual question answering", "video-text matching"], "githubStars": 620, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86Qwen3-VL-Embedding\u548cQwen3-VL-Reranker\u6a21\u578b\u7cfb\u5217\uff0c\u57fa\u4e8eQwen3-VL\u57fa\u7840\u6a21\u578b\uff0c\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u7684\u591a\u6a21\u6001\u641c\u7d22\u3002</li>\n    <li>Qwen3-VL-Embedding\u6a21\u578b\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\u751f\u6210\u4e30\u5bcc\u7684\u9ad8\u7ef4\u5411\u91cf\uff0c\u652f\u6301\u7075\u6d3b\u7684\u5d4c\u5165\u7ef4\u5ea6\u548c\u6700\u591a32k\u4e2a\u4ee4\u724c\u7684\u8f93\u5165\u3002</li>\n    <li>Qwen3-VL-Reranker\u4f7f\u7528\u4ea4\u53c9\u7f16\u7801\u67b6\u6784\u8fdb\u884c\u67e5\u8be2\u548c\u6587\u6863\u5bf9\u7684\u76f8\u5173\u6027\u4f30\u8ba1\u3002</li>\n    <li>\u8fd9\u4e24\u4e2a\u6a21\u578b\u7cfb\u5217\u652f\u6301\u8d85\u8fc730\u79cd\u8bed\u8a00\uff0c\u5e76\u5206\u4e3a2B\u548c8B\u53c2\u6570\u5927\u5c0f\uff0c\u9002\u5e94\u4e0d\u540c\u7684\u90e8\u7f72\u9700\u6c42\u3002</li>\n    <li>Qwen3-VL-Embedding\u7cfb\u5217\u5728\u591a\u6a21\u6001\u5d4c\u5165\u8bc4\u4f30\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u7ed3\u679c\uff0c\u5c24\u5176\u662fQwen3-VL-Embedding-8B\u5728MMEB-V2\u4e2d\u6392\u540d\u7b2c\u4e00\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The report introduces two new models, Qwen3-VL-Embedding and Qwen3-VL-Reranker, for advanced multimodal search.</li>\n    <li>These models can process various types of data, like text, images, and videos, and represent them in a unified way.</li>\n    <li>Qwen3-VL-Embedding uses a multi-step training approach to create detailed high-dimensional vectors, supporting up to 32,000 tokens.</li>\n    <li>Qwen3-VL-Reranker assesses how relevant different query-document pairs are using a specialized architecture.</li>\n    <li>Both models support over 30 languages, come in different sizes, and have achieved top results in multimodal evaluation tests.</li>\n</ul>"}, "publishedAt": "2026-01-08T03:36:06.000Z", "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking", "summary": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in 2B and 8B parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of 77.8 on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04720.png", "numComments": 1, "submittedBy": {"_id": "616adb8578833ce5997e441a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616adb8578833ce5997e441a/Oy5YkTaCMq-FEc0k3FTMd.jpeg", "fullname": "Dingkun Long", "name": "thenlper", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 121, "isUserFollowing": false}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05930", "authors": [{"_id": "69646f2d138cc47cbd7652db", "name": "Jingsheng Zheng", "hidden": false}, {"_id": "69646f2d138cc47cbd7652dc", "name": "Jintian Zhang", "hidden": false}, {"_id": "69646f2d138cc47cbd7652dd", "name": "Yujie Luo", "hidden": false}, {"_id": "69646f2d138cc47cbd7652de", "name": "Yuren Mao", "hidden": false}, {"_id": "69646f2d138cc47cbd7652df", "name": "Yunjun Gao", "hidden": false}, {"_id": "69646f2d138cc47cbd7652e0", "name": "Lun Du", "hidden": false}, {"_id": "69646f2d138cc47cbd7652e1", "name": "Huajun Chen", "hidden": false}, {"_id": "69646f2d138cc47cbd7652e2", "user": {"_id": "620b3bbb0668e435407c8d0a", "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg", "isPro": true, "fullname": "Ningyu Zhang", "user": "Ningyu", "type": "user"}, "name": "Ningyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T15:19:07.432Z", "hidden": false}], "publishedAt": "2026-01-09T16:44:17.000Z", "submittedOnDailyAt": "2026-01-12T01:19:32.404Z", "title": "Can We Predict Before Executing Machine Learning Agents?", "submittedOnDailyBy": {"_id": "620b3bbb0668e435407c8d0a", "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg", "isPro": true, "fullname": "Ningyu Zhang", "user": "Ningyu", "type": "user"}, "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.", "upvotes": 19, "discussionId": "69646f2d138cc47cbd7652e3", "githubRepo": "https://github.com/zjunlp/predict-before-execute", "githubRepoAddedBy": "user", "ai_summary": "Autonomous machine learning agents overcome execution bottlenecks by predicting outcomes before physical execution, achieving faster convergence and improved performance through a predict-then-verify approach.", "ai_keywords": ["autonomous machine learning agents", "Generate-Execute-Feedback paradigm", "Execution Bottleneck", "World Models", "Data-centric Solution Preference", "LLMs", "Predict-then-Verify loop", "convergence acceleration"], "githubStars": 7, "organization": {"_id": "67c1d682826160b28f778510", "name": "antgroup", "fullname": "Ant Group", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"}, "summary_zh": "<ul>\n    <li>\u81ea\u4e3b\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u6539\u53d8\u4e86\u79d1\u5b66\u53d1\u73b0\uff0c\u4f46\u4ecd\u53d7\u9650\u4e8e\u751f\u6210-\u6267\u884c-\u53cd\u9988\u7684\u6a21\u5f0f\u3002</li>\n    <li>\u4e4b\u524d\u7684\u65b9\u6cd5\u5728\u5047\u8bbe\u8bc4\u4f30\u65f6\u9762\u4e34\u4e25\u91cd\u7684\u6267\u884c\u74f6\u9888\uff0c\u4f9d\u8d56\u6602\u8d35\u7684\u7269\u7406\u6267\u884c\u3002</li>\n    <li>\u901a\u8fc7\u5f15\u5165\u6267\u884c\u5148\u9a8c\uff0c\u6211\u4eec\u7528\u5373\u65f6\u7684\u9884\u6d4b\u63a8\u7406\u66ff\u4ee3\u4e86\u6210\u672c\u9ad8\u6602\u7684\u8fd0\u884c\u65f6\u68c0\u67e5\u3002</li>\n    <li>\u6211\u4eec\u5efa\u7acb\u4e86\u4e00\u4e2a\u5305\u542b18,438\u5bf9\u6bd4\u8f83\u7684\u6570\u636e\u96c6\uff0c\u8bc1\u660e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u3002</li>\n    <li>FOREAGENT\u4ee3\u7406\u91c7\u7528\u9884\u6d4b-\u9a8c\u8bc1\u5faa\u73af\uff0c\u5b9e\u73b0\u4e86\u6536\u655b\u901f\u5ea6\u63d0\u9ad86\u500d\uff0c\u5e76\u8d85\u8d8a\u4e86\u57fa\u4e8e\u6267\u884c\u7684\u57fa\u51c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autonomous machine learning agents are changing how we discover new scientific knowledge, but they face limits due to a Generate-Execute-Feedback system.</li>\n    <li>Previous methods struggle because they need expensive physical tests to evaluate hypotheses, creating an Execution Bottleneck.</li>\n    <li>To overcome this, the authors use predictive reasoning instead of costly physical checks, inspired by World Models.</li>\n    <li>They introduce a new task called Data-centric Solution Preference and create a dataset of 18,438 comparisons.</li>\n    <li>The new system, called FOREAGENT, uses a Predict-then-Verify loop, speeding up processes by six times and improving accuracy over traditional methods.</li>\n</ul>"}, "publishedAt": "2026-01-09T11:44:17.000Z", "title": "Can We Predict Before Executing Machine Learning Agents?", "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05930.png", "numComments": 1, "submittedBy": {"_id": "620b3bbb0668e435407c8d0a", "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg", "fullname": "Ningyu Zhang", "name": "Ningyu", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 35, "isUserFollowing": false}, "organization": {"_id": "67c1d682826160b28f778510", "name": "antgroup", "fullname": "Ant Group", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/7VcPHdLSGlged3ixK1dys.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05882", "authors": [{"_id": "6964d4daba573026e23e33fd", "name": "Constantinos Karouzos", "hidden": false}, {"_id": "6964d4daba573026e23e33fe", "user": {"_id": "643d0a4d8a55b2bbf4f2a90e", "avatarUrl": "/avatars/9534aaf81cbf12f015c6826b682fdb84.svg", "isPro": false, "fullname": "Xingwei Tan", "user": "XingweiT", "type": "user"}, "name": "Xingwei Tan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T14:16:37.605Z", "hidden": false}, {"_id": "6964d4daba573026e23e33ff", "name": "Nikolaos Aletras", "hidden": false}], "publishedAt": "2026-01-09T15:56:55.000Z", "submittedOnDailyAt": "2026-01-12T12:00:15.855Z", "title": "An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift", "submittedOnDailyBy": {"_id": "643d0a4d8a55b2bbf4f2a90e", "avatarUrl": "/avatars/9534aaf81cbf12f015c6826b682fdb84.svg", "isPro": false, "fullname": "Xingwei Tan", "user": "XingweiT", "type": "user"}, "summary": "Preference tuning aligns pretrained language models to human judgments of quality, helpfulness, or safety by optimizing over explicit preference signals rather than likelihood alone. Prior work has shown that preference-tuning degrades performance and reduces helpfulness when evaluated outside the training domain. However, the extent to which adaptation strategies mitigate this domain shift remains unexplored. We address this challenge by conducting a comprehensive and systematic study of alignment generalization under domain shift. We compare five popular alignment objectives and various adaptation strategies from source to target, including target-domain supervised fine-tuning and pseudo-labeling, across summarization and question-answering helpfulness tasks. Our findings reveal systematic differences in generalization across alignment objectives under domain shift. We show that adaptation strategies based on pseudo-labeling can substantially reduce domain-shift degradation", "upvotes": 18, "discussionId": "6964d4daba573026e23e3400", "githubRepo": "https://github.com/ckarouzos/prefadap", "githubRepoAddedBy": "user", "ai_summary": "Preference tuning of language models shows varying generalization capabilities under domain shift, with pseudo-labeling adaptation strategies effectively reducing performance degradation in summarization and question-answering tasks.", "ai_keywords": ["preference tuning", "language models", "domain shift", "alignment objectives", "adaptation strategies", "supervised fine-tuning", "pseudo-labeling", "summarization", "question-answering"], "githubStars": 0, "summary_zh": "<ul>\n    <li>\u504f\u597d\u8c03\u4f18\u901a\u8fc7\u4f18\u5316\u660e\u786e\u7684\u504f\u597d\u4fe1\u53f7\u6765\u8c03\u6574\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u7b26\u5408\u4eba\u7c7b\u5bf9\u8d28\u91cf\u3001\u5e2e\u52a9\u6027\u6216\u5b89\u5168\u6027\u7684\u5224\u65ad\u3002</li>\n    <li>\u4e4b\u524d\u7684\u7814\u7a76\u8868\u660e\uff0c\u504f\u597d\u8c03\u4f18\u5728\u8bad\u7ec3\u57df\u5916\u4f1a\u964d\u4f4e\u6027\u80fd\u548c\u5e2e\u52a9\u6027\u3002</li>\n    <li>\u672c\u7814\u7a76\u7cfb\u7edf\u5730\u63a2\u8ba8\u4e86\u5728\u9886\u57df\u8f6c\u79fb\u4e0b\u7684\u5bf9\u9f50\u6cdb\u5316\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u6bd4\u8f83\u4e86\u4e94\u79cd\u6d41\u884c\u7684\u5bf9\u9f50\u76ee\u6807\u548c\u591a\u79cd\u9002\u5e94\u7b56\u7565\uff0c\u5305\u62ec\u76ee\u6807\u57df\u7684\u76d1\u7763\u5fae\u8c03\u548c\u4f2a\u6807\u8bb0\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8e\u4f2a\u6807\u8bb0\u7684\u9002\u5e94\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u9886\u57df\u8f6c\u79fb\u5e26\u6765\u7684\u6027\u80fd\u4e0b\u964d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Preference tuning helps language models match human ideas about quality and safety by focusing on preferences, not just likelihood.</li>\n    <li>Previous research found that preference tuning can lower performance and helpfulness when used outside the original training area.</li>\n    <li>This study examines how different adaptation methods can help improve performance when moving to new domains.</li>\n    <li>We tested five alignment goals and several adaptation strategies like fine-tuning and pseudo-labeling in summarization and question-answering tasks.</li>\n    <li>The results show that some adaptation methods, especially pseudo-labeling, can significantly improve performance when facing domain shifts.</li>\n</ul>"}, "publishedAt": "2026-01-09T10:56:55.000Z", "title": "An Empirical Study on Preference Tuning Generalization and Diversity Under Domain Shift", "summary": "Preference tuning aligns pretrained language models to human judgments of quality, helpfulness, or safety by optimizing over explicit preference signals rather than likelihood alone. Prior work has shown that preference-tuning degrades performance and reduces helpfulness when evaluated outside the training domain. However, the extent to which adaptation strategies mitigate this domain shift remains unexplored. We address this challenge by conducting a comprehensive and systematic study of alignment generalization under domain shift. We compare five popular alignment objectives and various adaptation strategies from source to target, including target-domain supervised fine-tuning and pseudo-labeling, across summarization and question-answering helpfulness tasks. Our findings reveal systematic differences in generalization across alignment objectives under domain shift. We show that adaptation strategies based on pseudo-labeling can substantially reduce domain-shift degradation", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05882.png", "numComments": 1, "submittedBy": {"_id": "643d0a4d8a55b2bbf4f2a90e", "avatarUrl": "/avatars/9534aaf81cbf12f015c6826b682fdb84.svg", "fullname": "Xingwei Tan", "name": "XingweiT", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.04786", "authors": [{"_id": "6964788b138cc47cbd76533c", "user": {"_id": "66ba29dd59e8e7a957154c5f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png", "isPro": false, "fullname": "Lang Feng", "user": "langfeng01", "type": "user"}, "name": "Lang Feng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:33:41.004Z", "hidden": false}, {"_id": "6964788b138cc47cbd76533d", "name": "Fuchao Yang", "hidden": false}, {"_id": "6964788b138cc47cbd76533e", "name": "Feng Chen", "hidden": false}, {"_id": "6964788b138cc47cbd76533f", "name": "Xin Cheng", "hidden": false}, {"_id": "6964788b138cc47cbd765340", "user": {"_id": "645b10e80c73ea27d13f7aca", "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg", "isPro": false, "fullname": "xuhaiyang", "user": "xhyandwyy", "type": "user"}, "name": "Haiyang Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:33:39.142Z", "hidden": false}, {"_id": "6964788b138cc47cbd765341", "name": "Zhenglin Wan", "hidden": false}, {"_id": "6964788b138cc47cbd765342", "name": "Ming Yan", "hidden": false}, {"_id": "6964788b138cc47cbd765343", "name": "Bo An", "hidden": false}], "publishedAt": "2026-01-08T10:10:20.000Z", "submittedOnDailyAt": "2026-01-12T02:18:55.645Z", "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression", "submittedOnDailyBy": {"_id": "66ba29dd59e8e7a957154c5f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png", "isPro": false, "fullname": "Lang Feng", "user": "langfeng01", "type": "user"}, "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.", "upvotes": 18, "discussionId": "6964788b138cc47cbd765344", "ai_summary": "AgentOCR reduces token consumption in agentic systems by representing interaction history as visual tokens and employing visual caching and self-compression techniques.", "ai_keywords": ["large language models", "reinforcement learning", "multi-turn interaction trajectories", "visual tokens", "segment optical caching", "agentic self-compression", "token efficiency", "rendering speedup"], "organization": {"_id": "6508b28cf36bb51c50faad98", "name": "NanyangTechnologicalUniversity", "fullname": "Nanyang Technological University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\u4f7f\u5f97\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u7cfb\u7edf\u80fd\u591f\u5904\u7406\u591a\u8f6e\u4ea4\u4e92\uff0c\u4f46\u6587\u672c\u5386\u53f2\u7684\u5feb\u901f\u589e\u957f\u5bfc\u81f4\u5185\u5b58\u548c\u4ee4\u724c\u9884\u7b97\u7684\u95ee\u9898\u3002</li>\n    <li>AgentOCR\u6846\u67b6\u901a\u8fc7\u5c06\u89c2\u5bdf-\u52a8\u4f5c\u5386\u53f2\u8868\u793a\u4e3a\u538b\u7f29\u7684\u56fe\u50cf\uff0c\u5229\u7528\u89c6\u89c9\u4ee4\u724c\u7684\u4fe1\u606f\u5bc6\u5ea6\u3002</li>\n    <li>\u8be5\u6846\u67b6\u4f7f\u7528\u201c\u5149\u5b66\u7f13\u5b58\u201d\u673a\u5236\uff0c\u901a\u8fc7\u5c06\u5386\u53f2\u5206\u89e3\u4e3a\u53ef\u54c8\u5e0c\u7684\u6bb5\u6765\u6d88\u9664\u91cd\u590d\u6e32\u67d3\uff0c\u4ece\u800c\u63d0\u9ad8\u591a\u8f6e\u4ea4\u4e92\u7684\u53ef\u6269\u5c55\u6027\u3002</li>\n    <li>AgentOCR\u8fd8\u5f15\u5165\u4e86\u667a\u80fd\u81ea\u538b\u7f29\u6280\u672f\uff0c\u4f7f\u5f97\u667a\u80fd\u4f53\u80fd\u591f\u4e3b\u52a8\u8c03\u6574\u538b\u7f29\u7387\uff0c\u4ee5\u5e73\u8861\u4efb\u52a1\u6210\u529f\u7387\u548c\u4ee4\u724c\u4f7f\u7528\u6548\u7387\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cAgentOCR\u5728\u4fdd\u6301\u8d85\u8fc795%\u7684\u6587\u672c\u57fa\u7840\u667a\u80fd\u4f53\u6027\u80fd\u7684\u540c\u65f6\uff0c\u4ee4\u724c\u6d88\u8017\u51cf\u5c11\u8d85\u8fc750%\uff0c\u5e76\u6709\u6548\u63d0\u9ad8\u4e86\u6e32\u67d3\u901f\u5ea6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>AgentOCR is a new framework designed to improve the efficiency of large language models in handling long interaction histories.</li>\n    <li>It uses visual tokens to represent past actions and observations as compact images, which saves memory and reduces token usage.</li>\n    <li>AgentOCR includes a feature called segment optical caching that avoids unnecessary re-rendering of visual data, speeding up processing.</li>\n    <li>It allows the agent to self-compress its output, balancing task success with efficient use of tokens.</li>\n    <li>Experiments show that AgentOCR maintains high performance while cutting token usage by over 50% and dramatically speeding up rendering times.</li>\n</ul>"}, "publishedAt": "2026-01-08T05:10:20.000Z", "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression", "summary": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.04786.png", "numComments": 1, "submittedBy": {"_id": "66ba29dd59e8e7a957154c5f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ba29dd59e8e7a957154c5f/VvVS7IZNPUIB023GAEf5u.png", "fullname": "Lang Feng", "name": "langfeng01", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "6508b28cf36bb51c50faad98", "name": "NanyangTechnologicalUniversity", "fullname": "Nanyang Technological University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZPD1fvei0bcIGeDXxeSkn.png"}, "isAuthorParticipating": true}],
    "week": [{"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\uff0c\u5229\u7528\u89c6\u89c9\u7ebf\u7d22\u8fdb\u884c\u5b9a\u4f4d\u3002</li>\n    <li>\u5f53\u524d\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5ffd\u89c6\u4e86\u4eba\u7c7b\u5e38\u7528\u7684\u7b56\u7565\u2014\u2014\u4f7f\u7528\u5730\u56fe\u3002</li>\n    <li>\u672c\u7814\u7a76\u4e3a\u6a21\u578b\u6dfb\u52a0\u4e86\u201c\u601d\u8003\u5730\u56fe\u201d\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u8bbe\u8ba1\u4e3a\u4e00\u4e2a\u201c\u5730\u56fe\u4e2d\u7684\u667a\u80fd\u4f53\u201d\u5faa\u73af\u3002</li>\n    <li>\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u589e\u5f3a\u5b66\u4e60\uff08RL\uff09\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08TTS\uff09\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6548\u7387\u548c\u63a2\u7d22\u80fd\u529b\u3002</li>\n    <li>\u5728\u6700\u65b0\u7684\u771f\u5b9e\u56fe\u50cf\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u7279\u522b\u662f\u51c6\u786e\u5ea6\u663e\u8457\u63d0\u9ad8\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>The goal of image geolocalization is to predict where a photo was taken using visual clues.</li>\n    <li>Current models use advanced techniques but often ignore the human strategy of using maps.</li>\n    <li>This study introduces a new model that incorporates map-thinking into its process.</li>\n    <li>The model improves its decision-making through reinforcement learning and explores multiple options before making a prediction.</li>\n    <li>Tests show this new method significantly outperforms previous models on real-world images.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u63d0\u5347\uff0c\u7528\u6237\u671f\u671b\u6a21\u578b\u63d0\u4f9b\u51c6\u786e\u7684\u56de\u590d\u548c\u7b26\u5408\u591a\u6837\u5316\u4eba\u7c7b\u504f\u597d\u7684\u884c\u4e3a\u3002</li>\n    <li>\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5f00\u59cb\u4f7f\u7528\u591a\u4e2a\u5956\u52b1\u6765\u5f15\u5bfc\u6a21\u578b\u7684\u884c\u4e3a\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\u76f4\u63a5\u5e94\u7528\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u4f1a\u5bfc\u81f4\u5956\u52b1\u7ec4\u5408\u7684\u6b63\u5e38\u5316\u51fa\u73b0\u95ee\u9898\uff0c\u4ece\u800c\u5f71\u54cd\u8bad\u7ec3\u6548\u679c\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u2014\u2014\u7fa4\u4f53\u5956\u52b1\u89e3\u8026\u6b63\u5e38\u5316\u7b56\u7565\u4f18\u5316\uff08GDPO\uff09\uff0c\u4ee5\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\uff0c\u4fdd\u6301\u5956\u52b1\u7684\u76f8\u5bf9\u5dee\u5f02\u3002</li>\n    <li>\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\uff0cGDPO\u7684\u8868\u73b0\u4f18\u4e8eGRPO\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Users expect language models to give accurate answers and behave according to different human preferences.</li>\n    <li>Reinforcement learning (RL) is using multiple rewards to help models achieve these behaviors.</li>\n    <li>Existing methods like Group Relative Policy Optimization (GRPO) can lead to problems by normalizing rewards too much.</li>\n    <li>This paper introduces a new method called Group reward-Decoupled Normalization Policy Optimization (GDPO) that keeps the individual rewards separate, improving training stability and performance.</li>\n    <li>GDPO outperforms GRPO in tasks like tool calling, math reasoning, and coding reasoning based on accuracy and adherence to constraints.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03017", "authors": [{"_id": "696488cc138cc47cbd765365", "name": "Jing Xiong", "hidden": false}, {"_id": "696488cc138cc47cbd765366", "name": "Qi Han", "hidden": false}, {"_id": "696488cc138cc47cbd765367", "name": "Yunta Hsieh", "hidden": false}, {"_id": "696488cc138cc47cbd765368", "name": "Hui Shen", "hidden": false}, {"_id": "696488cc138cc47cbd765369", "name": "Huajian Xin", "hidden": false}, {"_id": "696488cc138cc47cbd76536a", "name": "Chaofan Tao", "hidden": false}, {"_id": "696488cc138cc47cbd76536b", "name": "Chenyang Zhao", "hidden": false}, {"_id": "696488cc138cc47cbd76536c", "name": "Hengyuan Zhang", "hidden": false}, {"_id": "696488cc138cc47cbd76536d", "name": "Taiqiang Wu", "hidden": false}, {"_id": "696488cc138cc47cbd76536e", "name": "Zhen Zhang", "hidden": false}, {"_id": "696488cc138cc47cbd76536f", "name": "Haochen Wang", "hidden": false}, {"_id": "696488cc138cc47cbd765370", "name": "Zhongwei Wan", "hidden": false}, {"_id": "696488cc138cc47cbd765371", "name": "Lingpeng Kong", "hidden": false}, {"_id": "696488cc138cc47cbd765372", "name": "Ngai Wong", "hidden": false}], "publishedAt": "2026-01-06T13:42:51.000Z", "submittedOnDailyAt": "2026-01-12T03:10:40.203Z", "title": "MMFormalizer: Multimodal Autoformalization in the Wild", "submittedOnDailyBy": {"_id": "60851545a5da133ac6c38686", "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg", "isPro": false, "fullname": "Jing Xiong", "user": "menik1126", "type": "user"}, "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io", "upvotes": 94, "discussionId": "696488cc138cc47cbd765373", "projectPage": "https://mmformalizer.github.io/", "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.", "ai_keywords": ["autoformalization", "multimodal", "perceptually grounded primitives", "recursive grounding", "axiom composition", "adaptive recursive termination", "dimensional grounding", "axiomatic grounding", "PhyX-AF", "MathVerse", "PhyX", "Synthetic Geometry", "Analytic Geometry", "GPT-5", "Gemini-3-Pro", "classical mechanics", "relativity", "quantum mechanics", "thermodynamics"], "summary_zh": "<ul>\n    <li>Autoformalization \u662f\u5c06\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u8f6c\u6362\u4e3a\u6b63\u5f0f\u9648\u8ff0\u4ee5\u652f\u6301\u673a\u5668\u63a8\u7406\u7684\u8fc7\u7a0b\uff0c\u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u9762\u4e34\u6311\u6218\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86 MMFormalizer\uff0c\u5b83\u901a\u8fc7\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u5b66\u548c\u7269\u7406\u9886\u57df\u7684\u5b9e\u4f53\u7ed3\u5408\uff0c\u6269\u5c55\u4e86 autoformalization \u7684\u5e94\u7528\u3002</li>\n    <li>MMFormalizer \u901a\u8fc7\u9012\u5f52\u6784\u5efa\u5f62\u5f0f\u547d\u9898\uff0c\u786e\u4fdd\u6bcf\u4e2a\u62bd\u8c61\u90fd\u6709\u89c6\u89c9\u8bc1\u636e\u652f\u6301\u3002</li>\n    <li>\u5728\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5 PhyX-AF \u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a GPT-5 \u5728\u7269\u7406\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u51e0\u4f55\u9886\u57df\u4ecd\u7136\u662f\u6700\u5177\u6311\u6218\u6027\u7684\u3002</li>\n    <li>MMFormalizer \u662f\u9996\u4e2a\u80fd\u591f\u5904\u7406\u7ecf\u5178\u529b\u5b66\u3001\u76f8\u5bf9\u8bba\u3001\u91cf\u5b50\u529b\u5b66\u548c\u70ed\u529b\u5b66\u7684\u591a\u6a21\u6001 autoformalization \u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoformalization helps translate natural language math into formal statements for machine reasoning, but it struggles with real-world complexities.</li>\n    <li>MMFormalizer is a new method that integrates visual elements with mathematics and physics to improve this process.</li>\n    <li>It builds formal propositions from basic visual concepts, ensuring that every idea is backed by visual proof and relevant principles.</li>\n    <li>MMFormalizer was tested on a new benchmark called PhyX-AF, which includes a variety of multimodal tasks, showing strong results with advanced models like GPT-5.</li>\n    <li>This method is the first to effectively handle complex topics like classical mechanics, relativity, quantum mechanics, and thermodynamics in an integrated way.</li>\n</ul>"}, "publishedAt": "2026-01-06T08:42:51.000Z", "title": "MMFormalizer: Multimodal Autoformalization in the Wild", "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png", "numComments": 1, "submittedBy": {"_id": "60851545a5da133ac6c38686", "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg", "fullname": "Jing Xiong", "name": "menik1126", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.03252", "authors": [{"_id": "695dc956c03d6d81e4399ea4", "name": "Hao Yu", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea5", "user": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "isPro": false, "fullname": "Haotong Lin", "user": "haotongl", "type": "user"}, "name": "Haotong Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:15:04.783Z", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea6", "name": "Jiawei Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea7", "name": "Jiaxin Li", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea8", "name": "Yida Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399ea9", "user": {"_id": "6791a6c19ce382eae861ed61", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6791a6c19ce382eae861ed61/zerctN-RdeP4hSrWidtyN.jpeg", "isPro": false, "fullname": "Xueyang Zhang", "user": "zhangxueyang001", "type": "user"}, "name": "Xueyang Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:15:39.946Z", "hidden": false}, {"_id": "695dc956c03d6d81e4399eaa", "name": "Yue Wang", "hidden": false}, {"_id": "695dc956c03d6d81e4399eab", "name": "Xiaowei Zhou", "hidden": false}, {"_id": "695dc956c03d6d81e4399eac", "name": "Ruizhen Hu", "hidden": false}, {"_id": "695dc956c03d6d81e4399ead", "user": {"_id": "62986ca2b58e71e2ac9b8f01", "avatarUrl": "/avatars/83944db5f3dbb6f47c47c46fb2cb2849.svg", "isPro": false, "fullname": "Sida Peng", "user": "pengsida", "type": "user"}, "name": "Sida Peng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:16:12.074Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6489a01b8de3f9d810b0154f/XlXUo1VjGVhePk-xHRmkj.mp4"], "publishedAt": "2026-01-06T18:57:06.000Z", "submittedOnDailyAt": "2026-01-07T00:26:37.060Z", "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields", "submittedOnDailyBy": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "isPro": false, "fullname": "Haotong Lin", "user": "haotongl", "type": "user"}, "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.", "upvotes": 71, "discussionId": "695dc956c03d6d81e4399eae", "ai_summary": "InfiniDepth represents depth as neural implicit fields using a local implicit decoder, enabling continuous 2D coordinate querying for arbitrary-resolution depth estimation and superior performance in fine-detail regions.", "ai_keywords": ["neural implicit fields", "local implicit decoder", "continuous 2D coordinates", "arbitrary-resolution depth estimation", "synthetic benchmark", "4K synthetic benchmark", "novel view synthesis", "viewpoint shifts"], "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u53ea\u80fd\u5728\u79bb\u6563\u56fe\u50cf\u7f51\u683c\u4e0a\u9884\u6d4b\u6df1\u5ea6\uff0c\u9650\u5236\u4e86\u8f93\u51fa\u5206\u8fa8\u7387\u7684\u7075\u6d3b\u6027\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86InfiniDepth\uff0c\u4f7f\u7528\u795e\u7ecf\u9690\u5f0f\u573a\u8868\u793a\u6df1\u5ea6\uff0c\u53ef\u4ee5\u5728\u8fde\u7eed\u76842D\u5750\u6807\u4e0a\u67e5\u8be2\u6df1\u5ea6\u3002</li>\n    <li>InfiniDepth\u53ef\u4ee5\u5b9e\u73b0\u4efb\u610f\u5206\u8fa8\u7387\u548c\u7ec6\u81f4\u7684\u6df1\u5ea6\u4f30\u8ba1\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u76844K\u5408\u6210\u57fa\u51c6\uff0c\u6db5\u76d6\u6765\u81ea\u4e94\u4e2a\u4e0d\u540c\u6e38\u620f\u7684\u591a\u6837\u573a\u666f\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cInfiniDepth\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u7279\u522b\u662f\u5728\u7ec6\u8282\u4e30\u5bcc\u7684\u533a\u57df\u8868\u73b0\u7a81\u51fa\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Most existing depth estimation methods only work on fixed image grids, which limits their detail and flexibility.</li>\n    <li>The new method called InfiniDepth uses neural implicit fields to predict depth at any point in an image, allowing for high-resolution and detailed depth estimation.</li>\n    <li>InfiniDepth has been tested on a high-quality 4K benchmark created from five different video games, showcasing a variety of detailed scenes.</li>\n    <li>Results show that InfiniDepth outperforms other methods in both synthetic and real-world depth estimation tasks, especially in areas with fine details.</li>\n    <li>This method also improves the quality of new image views from different angles, reducing errors like holes and artifacts.</li>\n</ul>"}, "publishedAt": "2026-01-06T13:57:06.000Z", "title": "InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields", "summary": "Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method's capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6489a01b8de3f9d810b0154f/XlXUo1VjGVhePk-xHRmkj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03252.png", "numComments": 8, "submittedBy": {"_id": "6489a01b8de3f9d810b0154f", "avatarUrl": "/avatars/f7a0fc6816535945e11bac1212dd7b57.svg", "fullname": "Haotong Lin", "name": "haotongl", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03509", "authors": [{"_id": "695fbc7d5b7998385e639349", "name": "Haochen Shi", "hidden": false}, {"_id": "695fbc7d5b7998385e63934a", "name": "Xingdi Yuan", "hidden": false}, {"_id": "695fbc7d5b7998385e63934b", "name": "Bang Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/654a97282d2fcd6bf2851173/sAKIzhLgfcEhgVZMBkHRW.png"], "publishedAt": "2026-01-07T01:43:25.000Z", "submittedOnDailyAt": "2026-01-08T11:50:05.687Z", "title": "Evolving Programmatic Skill Networks", "submittedOnDailyBy": {"_id": "654a97282d2fcd6bf2851173", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png", "isPro": false, "fullname": "Bang Liu", "user": "Bang-UdeM-Mila", "type": "user"}, "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.", "upvotes": 54, "discussionId": "695fbc7e5b7998385e63934c", "ai_summary": "Programmatic Skill Network enables continual skill acquisition through executable symbolic programs that evolve via reflection, progressive optimization, and structural refactoring mechanisms.", "ai_keywords": ["Programmatic Skill Network", "executable symbolic programs", "skill composition", "structured fault localization", "progressive optimization", "maturity-aware update gating", "canonical structural refactoring", "rollback validation", "neural network training", "skill reuse", "rapid adaptation", "generalization"], "organization": {"_id": "636e93488ba65db4a0987ab4", "name": "Universite-de-Montreal", "fullname": "Universit\u00e9 de Montr\u00e9al"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u7814\u7a76\u5728\u5f00\u653e\u5f0f\u73af\u5883\u4e2d\u6301\u7eed\u83b7\u5f97\u6280\u80fd\uff0c\u4ee3\u7406\u9700\u8981\u6784\u5efa\u3001\u5b8c\u5584\u548c\u91cd\u7528\u6280\u80fd\u5e93\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86\u7a0b\u5e8f\u6280\u80fd\u7f51\u7edc\uff08PSN\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u8fc7\u7ecf\u9a8c\u6f14\u53d8\u7684\u7b26\u53f7\u7a0b\u5e8f\u6280\u80fd\u7f51\u7edc\u3002</li>\n    <li>PSN\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u673a\u5236\uff1a\u6545\u969c\u5b9a\u4f4d\u3001\u6210\u719f\u5ea6\u611f\u77e5\u7684\u4f18\u5316\u66f4\u65b0\u548c\u7ed3\u6784\u91cd\u6784\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cPSN\u5728\u6280\u80fd\u91cd\u7528\u3001\u5feb\u901f\u9002\u5e94\u548c\u5f3a\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002</li>\n    <li>\u6211\u4eec\u8ba1\u5212\u5f00\u6e90\u8fd9\u9879\u7814\u7a76\u7684\u4ee3\u7801\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study focuses on how agents can learn and improve skills in complex environments.</li>\n    <li>It introduces a new system called Programmatic Skill Network (PSN), which allows skills to be represented as executable programs.</li>\n    <li>PSN includes three main features: a method for identifying errors in skills, a way to improve skills while keeping reliable ones stable, and a system for organizing skills efficiently.</li>\n    <li>The learning process in PSN is similar to how neural networks are trained.</li>\n    <li>Tests show that PSN can effectively reuse skills, adapt quickly, and perform well on various tasks.</li>\n</ul>"}, "publishedAt": "2026-01-06T20:43:25.000Z", "title": "Evolving Programmatic Skill Networks", "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/654a97282d2fcd6bf2851173/sAKIzhLgfcEhgVZMBkHRW.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03509.png", "numComments": 1, "submittedBy": {"_id": "654a97282d2fcd6bf2851173", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png", "fullname": "Bang Liu", "name": "Bang-UdeM-Mila", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10, "isUserFollowing": false}, "organization": {"_id": "636e93488ba65db4a0987ab4", "name": "Universite-de-Montreal", "fullname": "Universit\u00e9 de Montr\u00e9al"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.03319", "authors": [{"_id": "6960e7365b7998385e6396e3", "user": {"_id": "6761f183e5b85d453550147a", "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg", "isPro": false, "fullname": "EldadMat", "user": "eldad929", "type": "user"}, "name": "Eldad Matmon", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T15:45:52.792Z", "hidden": false}, {"_id": "6960e7365b7998385e6396e4", "name": "Amit Bracha", "hidden": false}, {"_id": "6960e7365b7998385e6396e5", "user": {"_id": "62b3e85bcbd2a402fc7804b1", "avatarUrl": "/avatars/63125ce8a1e20b8c6e836f223d24284f.svg", "isPro": false, "fullname": "noam rotstein", "user": "noamrot", "type": "user"}, "name": "Noam Rotstein", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:36:06.891Z", "hidden": false}, {"_id": "6960e7365b7998385e6396e6", "name": "Ron Kimmel", "hidden": false}], "publishedAt": "2026-01-06T13:56:28.000Z", "submittedOnDailyAt": "2026-01-12T03:43:15.850Z", "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature", "submittedOnDailyBy": {"_id": "6761f183e5b85d453550147a", "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg", "isPro": false, "fullname": "EldadMat", "user": "eldad929", "type": "user"}, "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.", "upvotes": 45, "discussionId": "6960e7365b7998385e6396e7", "projectPage": "https://c4ricaturegs.github.io/", "ai_summary": "A photorealistic 3D caricaturization framework combines Gaussian curvature-based surface exaggeration with 3D Gaussian Splatting to create controllable, realistic avatars with improved fidelity and real-time deformation capabilities.", "ai_keywords": ["3D Gaussian Splatting", "FLAME mesh", "curvature-weighted Poisson equation", "pseudo-ground-truth caricature images", "local affine transformations", "real-time deformations", "closed-form solutions"], "organization": {"_id": "6393322be2364bc1eea56e45", "name": "Technion", "fullname": "Technion Israel institute of technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u63a7\u76843D\u9762\u90e8\u6f2b\u753b\u5316\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u903c\u771f\u7684\u6548\u679c\u3002</li>\n    <li>\u91c7\u7528\u57fa\u4e8e\u9ad8\u65af\u66f2\u7387\u7684\u8868\u9762\u5938\u5f20\u6280\u672f\uff0c\u5e76\u7ed3\u54083D\u9ad8\u65af\u70b9\u4e91\u6280\u672f\uff0c\u4ee5\u6539\u5584\u6e32\u67d3\u8d28\u91cf\u3002</li>\n    <li>\u901a\u8fc7\u63d0\u53d6FLAME\u7f51\u683c\uff0c\u89e3\u51b3\u4e86\u66f2\u7387\u52a0\u6743\u7684\u6cca\u677e\u65b9\u7a0b\uff0c\u5f97\u5230\u4e86\u5938\u5f20\u7684\u9762\u90e8\u6a21\u578b\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e00\u79cd\u8bad\u7ec3\u65b9\u6848\uff0c\u7ed3\u5408\u771f\u5b9e\u4e0e\u5408\u6210\u7684\u76d1\u7763\uff0c\u4f7f\u5f97\u9ad8\u65af\u96c6\u5408\u80fd\u591f\u8868\u793a\u81ea\u7136\u548c\u5938\u5f20\u7684\u5934\u50cf\u3002</li>\n    <li>\u5b9e\u73b0\u4e86\u5b9e\u65f6\u53d8\u5f62\uff0c\u5e76\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u5de5\u4f5c\uff0c\u751f\u6210\u4e86\u903c\u771f\u7684\u6f2b\u753b\u5934\u50cf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>A new framework creates realistic 3D caricatures of faces that can be easily controlled.</li>\n    <li>The method uses a special technique to exaggerate facial features while addressing smoothing issues with advanced 3D Gaussian Splatting.</li>\n    <li>It involves extracting a mesh from multiple views of a face and applying a mathematical approach to enhance its shape.</li>\n    <li>The framework synthesizes caricature images for training, allowing for better representation of both normal and exaggerated faces.</li>\n    <li>It enables real-time adjustments and consistently produces high-quality caricature avatars that outperform previous methods.</li>\n</ul>"}, "publishedAt": "2026-01-06T08:56:28.000Z", "title": "CaricatureGS: Exaggerating 3D Gaussian Splatting Faces With Gaussian Curvature", "summary": "A photorealistic and controllable 3D caricaturization framework for faces is introduced. We start with an intrinsic Gaussian curvature-based surface exaggeration technique, which, when coupled with texture, tends to produce over-smoothed renders. To address this, we resort to 3D Gaussian Splatting (3DGS), which has recently been shown to produce realistic free-viewpoint avatars. Given a multiview sequence, we extract a FLAME mesh, solve a curvature-weighted Poisson equation, and obtain its exaggerated form. However, directly deforming the Gaussians yields poor results, necessitating the synthesis of pseudo-ground-truth caricature images by warping each frame to its exaggerated 2D representation using local affine transformations. We then devise a training scheme that alternates real and synthesized supervision, enabling a single Gaussian collection to represent both natural and exaggerated avatars. This scheme improves fidelity, supports local edits, and allows continuous control over the intensity of the caricature. In order to achieve real-time deformations, an efficient interpolation between the original and exaggerated surfaces is introduced. We further analyze and show that it has a bounded deviation from closed-form solutions. In both quantitative and qualitative evaluations, our results outperform prior work, delivering photorealistic, geometry-controlled caricature avatars.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03319.png", "numComments": 1, "submittedBy": {"_id": "6761f183e5b85d453550147a", "avatarUrl": "/avatars/b071c0b1ee46955ee5ff38a3c3df457f.svg", "fullname": "EldadMat", "name": "eldad929", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "6393322be2364bc1eea56e45", "name": "Technion", "fullname": "Technion Israel institute of technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03233", "authors": [{"_id": "695dc6d9c03d6d81e4399e85", "user": {"_id": "6303cc5e0547362a22a51af0", "avatarUrl": "/avatars/8f3348f121565bf6c5e1af0e559a43a3.svg", "isPro": false, "fullname": "Yoav HaCohen", "user": "yoavhacohen", "type": "user"}, "name": "Yoav HaCohen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:29.722Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e86", "user": {"_id": "6489c487b9e9258ba065418f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6489c487b9e9258ba065418f/6rzmV3bQ3YxswG6NP2hDW.png", "isPro": false, "fullname": "Benny Brazowski", "user": "benibraz", "type": "user"}, "name": "Benny Brazowski", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:35.981Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e87", "user": {"_id": "62dd30a8d43078cd49ac8ad8", "avatarUrl": "/avatars/ad599719290637f7817b7508a91c2e2c.svg", "isPro": false, "fullname": "Nisan Chiprut", "user": "nisan", "type": "user"}, "name": "Nisan Chiprut", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:41.634Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e88", "user": {"_id": "64a7adc087cbd4dc7301fdd6", "avatarUrl": "/avatars/b4ec4c3a0409af8ec4a5de05db453034.svg", "isPro": false, "fullname": "Yaki Bitterman", "user": "jacobitterman", "type": "user"}, "name": "Yaki Bitterman", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:46.749Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e89", "user": {"_id": "65897258509bcae23fa162c9", "avatarUrl": "/avatars/29d277a0c425c936e25e82e79caa10a4.svg", "isPro": false, "fullname": "Andrew Kvochko", "user": "kvochko", "type": "user"}, "name": "Andrew Kvochko", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:19:51.722Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8a", "name": "Avishai Berkowitz", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8b", "name": "Daniel Shalem", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8c", "user": {"_id": "681af83e2f4aaa88639e703d", "avatarUrl": "/avatars/d69b664daad0afb529440c14fdb9bc3a.svg", "isPro": false, "fullname": "Daphna Lifschitz", "user": "Daphnal", "type": "user"}, "name": "Daphna Lifschitz", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:04.180Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8d", "user": {"_id": "636b97a57631fe5e86fe1fa2", "avatarUrl": "/avatars/c568ae26fd4fc2655cd12f15d539db58.svg", "isPro": false, "fullname": "Dudu Moshe", "user": "dudumoshe", "type": "user"}, "name": "Dudu Moshe", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:13.512Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8e", "name": "Eitan Porat", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e8f", "user": {"_id": "677a422979d3c32a5dd87a0a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WUa6E68GpnT2mEMJ41nDd.png", "isPro": false, "fullname": "Eitan Richardson", "user": "eitanrich", "type": "user"}, "name": "Eitan Richardson", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:22.677Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e90", "user": {"_id": "673f6911d83832a6ce15e7bf", "avatarUrl": "/avatars/0da6cded3b0e785241a6ba5fdb5d8ceb.svg", "isPro": false, "fullname": "Guy Shiran", "user": "guysrn", "type": "user"}, "name": "Guy Shiran", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:28.250Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e91", "user": {"_id": "65744a2fe09de6aa74026d80", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65744a2fe09de6aa74026d80/kCxIKdeBJwAPKmvlm7fDP.jpeg", "isPro": false, "fullname": "Itay Chachy", "user": "ItayChachy", "type": "user"}, "name": "Itay Chachy", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:36.781Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e92", "name": "Jonathan Chetboun", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e93", "user": {"_id": "6678365ac411b340b32d6148", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6678365ac411b340b32d6148/7OhHzbu65pa95eYrAbbLW.jpeg", "isPro": false, "fullname": "Michael Finkelson", "user": "MichaelFinkelson", "type": "user"}, "name": "Michael Finkelson", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:53.574Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e94", "user": {"_id": "6318aa43cb4ca740c4c55651", "avatarUrl": "/avatars/24082c776d284393a5a38a99e5c0bab8.svg", "isPro": false, "fullname": "michael kupchick", "user": "michaellightricks", "type": "user"}, "name": "Michael Kupchick", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:20:59.945Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e95", "user": {"_id": "673f29b568595672b8d3e90e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673f29b568595672b8d3e90e/4sYADg3mpqMKmJ4fQwaTl.png", "isPro": false, "fullname": "Nir Zabari", "user": "NirZabariLTX", "type": "user"}, "name": "Nir Zabari", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:07.297Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e96", "user": {"_id": "64ae89c043dda9449a1eb1ba", "avatarUrl": "/avatars/12cf3de929d38ddd92cc3f3337dc2ed2.svg", "isPro": false, "fullname": "Nitzan Guetta", "user": "nitzanguetta", "type": "user"}, "name": "Nitzan Guetta", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:14.712Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e97", "name": "Noa Kotler", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e98", "user": {"_id": "631f58935ba8c026340b377c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631f58935ba8c026340b377c/4yoHLdNE99VBb7ji_Mzzj.jpeg", "isPro": false, "fullname": "Ofir Bibi", "user": "ofirbibi", "type": "user"}, "name": "Ofir Bibi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:27.196Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e99", "user": {"_id": "674348b46215a2c0878e219b", "avatarUrl": "/avatars/8a213e431a1583d1a93377410907c059.svg", "isPro": false, "fullname": "Ori Gordon", "user": "origordon", "type": "user"}, "name": "Ori Gordon", "status": "admin_assigned", "statusLastChangedAt": "2026-01-07T13:21:34.878Z", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9a", "name": "Poriya Panet", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9b", "name": "Roi Benita", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9c", "name": "Shahar Armon", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9d", "name": "Victor Kulikov", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9e", "name": "Yaron Inger", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399e9f", "name": "Yonatan Shiftan", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399ea0", "name": "Zeev Melumian", "hidden": false}, {"_id": "695dc6d9c03d6d81e4399ea1", "name": "Zeev Farbman", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cYoXYuK3pjt85pl5-fvUv.mp4"], "publishedAt": "2026-01-06T18:24:41.000Z", "submittedOnDailyAt": "2026-01-07T00:07:29.528Z", "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.", "upvotes": 40, "discussionId": "695dc6d9c03d6d81e4399ea2", "projectPage": "https://app.ltx.studio/ltx-2-playground/i2v", "githubRepo": "https://github.com/Lightricks/LTX-2", "githubRepoAddedBy": "user", "ai_summary": "LTX-2 is an open-source audiovisual diffusion model that generates synchronized video and audio content using a dual-stream transformer architecture with cross-modal attention and classifier-free guidance.", "ai_keywords": ["text-to-video diffusion models", "audiovisual content", "dual-stream transformer", "cross-attention layers", "temporal positional embeddings", "AdaLN", "classifier-free guidance", "modality-aware classifier-free guidance", "multilingual text encoder", "diffusion models"], "githubStars": 922, "summary_zh": "<ul>\n    <li>LTX-2\u662f\u4e00\u4e2a\u5f00\u6e90\u6a21\u578b\uff0c\u53ef\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u540c\u6b65\u89c6\u542c\u5185\u5bb9\u3002</li>\n    <li>\u5b83\u7ed3\u5408\u4e8614B\u53c2\u6570\u7684\u89c6\u9891\u6d41\u548c5B\u53c2\u6570\u7684\u97f3\u9891\u6d41\uff0c\u4f7f\u7528\u53cc\u6d41\u53d8\u6362\u5668\u67b6\u6784\u3002</li>\n    <li>\u8be5\u6a21\u578b\u80fd\u591f\u751f\u6210\u4e0e\u573a\u666f\u7684\u89d2\u8272\u3001\u73af\u5883\u3001\u98ce\u683c\u548c\u60c5\u611f\u76f8\u7b26\u7684\u4e30\u5bcc\u97f3\u8f68\u3002</li>\n    <li>LTX-2\u5728\u5f00\u6e90\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u4e86\u9886\u5148\u7684\u89c6\u542c\u8d28\u91cf\u548c\u5bf9\u63d0\u793a\u7684\u9075\u5faa\u3002</li>\n    <li>\u6240\u6709\u6a21\u578b\u6743\u91cd\u548c\u4ee3\u7801\u90fd\u5df2\u516c\u5f00\u53d1\u5e03\uff0c\u4f7f\u7528\u6210\u672c\u548c\u63a8\u7406\u65f6\u95f4\u6bd4\u4e13\u6709\u6a21\u578b\u4f4e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LTX-2 is a new open-source model that creates high-quality videos with synchronized audio.</li>\n    <li>It uses a dual-stream transformer with a large video stream and a smaller audio stream to improve the quality of both.</li>\n    <li>The model includes features for better understanding of text prompts and enhances the alignment between audio and video.</li>\n    <li>LTX-2 generates natural-sounding audio that matches the scenes, characters, and emotions in the video.</li>\n    <li>It outperforms other open-source systems in audiovisual quality and is more efficient than proprietary models, with all resources publicly available.</li>\n</ul>"}, "publishedAt": "2026-01-06T13:24:41.000Z", "title": "LTX-2: Efficient Joint Audio-Visual Foundation Model", "summary": "Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/cYoXYuK3pjt85pl5-fvUv.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03233.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 202, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.06002", "authors": [{"_id": "6964644c138cc47cbd76529b", "user": {"_id": "636f526a6cd69d9a36ff2b53", "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg", "isPro": false, "fullname": "Qiguang Chen", "user": "LightChen2333", "type": "user"}, "name": "Qiguang Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:33:48.803Z", "hidden": false}, {"_id": "6964644c138cc47cbd76529c", "name": "Yantao Du", "hidden": false}, {"_id": "6964644c138cc47cbd76529d", "name": "Ziniu Li", "hidden": false}, {"_id": "6964644c138cc47cbd76529e", "name": "Jinhao Liu", "hidden": false}, {"_id": "6964644c138cc47cbd76529f", "name": "Songyao Duan", "hidden": false}, {"_id": "6964644c138cc47cbd7652a0", "name": "Jiarui Guo", "hidden": false}, {"_id": "6964644c138cc47cbd7652a1", "name": "Minghao Liu", "hidden": false}, {"_id": "6964644c138cc47cbd7652a2", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6964644c138cc47cbd7652a3", "name": "Tong Yang", "hidden": false}, {"_id": "6964644c138cc47cbd7652a4", "user": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "name": "Ge Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:33:51.170Z", "hidden": false}, {"_id": "6964644c138cc47cbd7652a5", "name": "Libo Qin", "hidden": false}, {"_id": "6964644c138cc47cbd7652a6", "name": "Wanxiang Che", "hidden": false}, {"_id": "6964644c138cc47cbd7652a7", "name": "Wenhao Huang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"], "publishedAt": "2026-01-09T18:39:01.000Z", "submittedOnDailyAt": "2026-01-12T01:02:27.368Z", "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning", "submittedOnDailyBy": {"_id": "636f526a6cd69d9a36ff2b53", "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg", "isPro": false, "fullname": "Qiguang Chen", "user": "LightChen2333", "type": "user"}, "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.", "upvotes": 38, "discussionId": "6964644c138cc47cbd7652a8", "ai_summary": "Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.", "ai_keywords": ["chain-of-thought", "large language models", "Long CoT", "fine-tuning", "entropy convergence", "semantic isomers", "distribution-transfer-graph", "molecular-like structures", "deep reasoning", "self-reflection", "self-exploration"], "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5b66\u4e60\u6709\u6548\u7684\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u6709\u7a33\u5b9a\u7684\u5206\u5b50\u7ed3\u6784\uff0c\u7531\u4e09\u79cd\u76f8\u4e92\u4f5c\u7528\u7c7b\u578b\u6784\u6210\u3002</li>\n    <li>\u5206\u6790\u663e\u793a\uff0c\u8fd9\u4e9b\u7ed3\u6784\u662f\u901a\u8fc7\u957f\u94fe\u601d\u7ef4\u7684\u5fae\u8c03\u5f62\u6210\u7684\uff0c\u800c\u4e0d\u662f\u7b80\u5355\u6a21\u4eff\u5173\u952e\u8bcd\u3002</li>\n    <li>\u5f15\u5165\u6709\u6548\u7684\u8bed\u4e49\u5f02\u6784\u4f53\uff0c\u5f3a\u8c03\u4fc3\u8fdb\u5feb\u901f\u71b5\u6536\u655b\u7684\u7ed3\u5408\u6709\u52a9\u4e8e\u7a33\u5b9a\u5b66\u4e60\u3002</li>\n    <li>\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u63d0\u51faMole-Syn\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u957f\u94fe\u601d\u7ef4\u7ed3\u6784\u7684\u5408\u6210\u6548\u679c\u548c\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models struggle with learning long chain-of-thought reasoning from other models or humans.</li>\n    <li>Effective long reasoning patterns have stable structures formed by three types of interactions: Deep-Reasoning, Self-Reflection, and Self-Exploration.</li>\n    <li>These structures are developed through fine-tuning, rather than just imitating keywords.</li>\n    <li>We introduce a method called Mole-Syn that helps create effective long reasoning structures, improving model performance and stability.</li>\n    <li>Only certain types of interactions support successful long reasoning learning, while competition between structures can hinder it.</li>\n</ul>"}, "publishedAt": "2026-01-09T13:39:01.000Z", "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning", "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06002.png", "numComments": 1, "submittedBy": {"_id": "636f526a6cd69d9a36ff2b53", "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg", "fullname": "Qiguang Chen", "name": "LightChen2333", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03872", "authors": [{"_id": "695f1a475fa3847525c41d06", "user": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "name": "Jinyang Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:32:36.055Z", "hidden": false}, {"_id": "695f1a475fa3847525c41d07", "name": "Guocheng Zhai", "hidden": false}, {"_id": "695f1a475fa3847525c41d08", "name": "Ruihan Jin", "hidden": false}, {"_id": "695f1a475fa3847525c41d09", "name": "Jiahao Yuan", "hidden": false}, {"_id": "695f1a475fa3847525c41d0a", "name": "Yuhao Shen", "hidden": false}, {"_id": "695f1a475fa3847525c41d0b", "name": "Shuai Zhang", "hidden": false}, {"_id": "695f1a475fa3847525c41d0c", "name": "Zhengqi Wen", "hidden": false}, {"_id": "695f1a475fa3847525c41d0d", "name": "Jianhua Tao", "hidden": false}], "publishedAt": "2026-01-07T12:38:33.000Z", "submittedOnDailyAt": "2026-01-08T04:50:03.287Z", "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning", "submittedOnDailyBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "summary": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.", "upvotes": 30, "discussionId": "695f1a475fa3847525c41d0e", "ai_summary": "ATLAS is a dual-path framework that dynamically selects optimal model-tool combinations for cross-domain reasoning through cluster-based routing and reinforcement learning-based multi-step routing, achieving superior performance on complex reasoning tasks.", "ai_keywords": ["large language models", "external tools", "model-tool combination", "high-dimensional optimization", "dual-path framework", "training-free cluster-based routing", "RL-based multi-step routing", "cross-domain complex reasoning", "domain-specific alignment", "out-of-distribution generalization"], "summary_zh": "<ul>\n    <li>\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u5916\u90e8\u5de5\u5177\u7ed3\u5408\uff0c\u589e\u5f3a\u4e86\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u80fd\u529b\u3002</li>\n    <li>\u9009\u62e9\u6700\u4f73\u7684\u6a21\u578b-\u5de5\u5177\u7ec4\u5408\u662f\u4e00\u4e2a\u590d\u6742\u7684\u9ad8\u7ef4\u4f18\u5316\u95ee\u9898\u3002</li>\n    <li>\u63d0\u51fa\u4e86ATLAS\u6846\u67b6\uff0c\u652f\u6301\u8de8\u9886\u57df\u590d\u6742\u63a8\u7406\u4e2d\u7684\u52a8\u6001\u5de5\u5177\u4f7f\u7528\u3002</li>\n    <li>ATLAS\u91c7\u7528\u53cc\u8def\u5f84\u65b9\u6cd5\uff0c\u7ed3\u5408\u57fa\u4e8e\u805a\u7c7b\u7684\u8def\u7531\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u6b65\u8def\u7531\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cATLAS\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5c24\u5176\u5728\u89c6\u89c9\u63a8\u7406\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) are becoming more powerful by integrating with different tools.</li>\n    <li>Choosing the best combination of model and tool is a complex problem due to their variety.</li>\n    <li>The paper introduces ATLAS, a new framework that improves how models and tools work together.</li>\n    <li>ATLAS uses two main methods: one that aligns tools with specific domains without training, and another that learns to use tools better over time.</li>\n    <li>Tests show ATLAS performs better than existing models and methods, especially in visual reasoning tasks.</li>\n</ul>"}, "publishedAt": "2026-01-07T07:38:33.000Z", "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning", "summary": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03872.png", "numComments": 1, "submittedBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "fullname": "Jinyang Wu", "name": "Jinyang23", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06021", "authors": [{"_id": "69645f30138cc47cbd765248", "name": "Jiajie Zhang", "hidden": false}, {"_id": "69645f30138cc47cbd765249", "name": "Xin Lv", "hidden": false}, {"_id": "69645f30138cc47cbd76524a", "name": "Ling Feng", "hidden": false}, {"_id": "69645f30138cc47cbd76524b", "name": "Lei Hou", "hidden": false}, {"_id": "69645f30138cc47cbd76524c", "name": "Juanzi Li", "hidden": false}], "publishedAt": "2026-01-09T18:57:53.000Z", "submittedOnDailyAt": "2026-01-12T00:13:19.034Z", "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards", "submittedOnDailyBy": {"_id": "66cdd285c51a915bd5f2d017", "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg", "isPro": false, "fullname": "Jiajie Zhang", "user": "NeoZ123", "type": "user"}, "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.", "upvotes": 30, "discussionId": "69645f30138cc47cbd76524d", "githubRepo": "https://github.com/THUDM/CaRR", "githubRepoAddedBy": "user", "ai_summary": "A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.", "ai_keywords": ["reinforcement learning", "deep search agents", "fine-grained reward framework", "reasoning comprehensiveness", "factual grounding", "evidence connectivity", "verifiable single-hop rubrics", "citation-aware group relative policy optimization", "outcome rewards", "shortcut exploitation", "hallucinations"], "githubStars": 15, "organization": {"_id": "62ad27f19096e7f9ecb1853a", "name": "zai-org", "fullname": "Z.ai", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u88ab\u7528\u6765\u63d0\u5347\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u7684\u6027\u80fd\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u4e8c\u5143\u7ed3\u679c\u5956\u52b1\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u4ee3\u7406\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u4e0d\u826f\u884c\u4e3a\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u5f15\u7528\u610f\u8bc6\u8bc4\u5206\u5956\u52b1\uff08CaRR\uff09\uff0c\u5f3a\u8c03\u63a8\u7406\u7684\u5168\u9762\u6027\u3001\u4e8b\u5b9e\u57fa\u7840\u548c\u8bc1\u636e\u8fde\u63a5\u3002</li>\n    <li>CaRR \u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u53ef\u9a8c\u8bc1\u7684\u5355\u6b65\u8bc4\u5206\uff0c\u8981\u6c42\u4ee3\u7406\u8bc6\u522b\u9690\u85cf\u5b9e\u4f53\u5e76\u63d0\u4f9b\u6b63\u786e\u7684\u5f15\u7528\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408CaRR\u548c\u7ed3\u679c\u5956\u52b1\u7684\u5f15\u7528\u610f\u8bc6\u7ec4\u76f8\u5bf9\u653f\u7b56\u4f18\u5316\uff08C-GRPO\uff09\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) is important for improving deep search agents that use large language models (LLMs).</li>\n    <li>Current methods mostly use simple rewards based on yes/no outcomes, which can lead to poor reasoning and incorrect answers.</li>\n    <li>The new Citation-aware Rubric Rewards (CaRR) framework encourages better reasoning by breaking down questions and requiring evidence and citations.</li>\n    <li>We also developed Citation-aware Group Relative Policy Optimization (C-GRPO) to integrate CaRR with traditional rewards for better training results.</li>\n    <li>Tests show C-GRPO is more effective than existing methods, reducing mistakes and improving reasoning quality in search tasks.</li>\n</ul>"}, "publishedAt": "2026-01-09T13:57:53.000Z", "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards", "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06021.png", "numComments": 1, "submittedBy": {"_id": "66cdd285c51a915bd5f2d017", "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg", "fullname": "Jiajie Zhang", "name": "NeoZ123", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "62ad27f19096e7f9ecb1853a", "name": "zai-org", "fullname": "Z.ai", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u9700\u6c42\u7684\u589e\u957f\uff0c\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u53d8\u5f97\u66f4\u52a0\u91cd\u8981\u3002</li>\n    <li>\u76ee\u524d\u7684\u6570\u636e\u51c6\u5907\u65b9\u6cd5\u591a\u4e3a\u4e34\u65f6\u811a\u672c\uff0c\u7f3a\u4e4f\u89c4\u8303\uff0c\u5f71\u54cd\u4e86\u53ef\u91cd\u590d\u6027\u548c\u6570\u636e\u751f\u6210\u652f\u6301\u3002</li>\n    <li>DataFlow\u662f\u4e00\u4e2a\u7edf\u4e00\u7684LLM\u9a71\u52a8\u7684\u6570\u636e\u51c6\u5907\u6846\u67b6\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5305\u542b\u8fd1200\u4e2a\u53ef\u91cd\u7528\u64cd\u4f5c\u7b26\u548c\u516d\u4e2a\u901a\u7528\u7ba1\u9053\uff0c\u8986\u76d6\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u9886\u57df\u3002</li>\n    <li>DataFlow\u5728\u516d\u4e2a\u4f7f\u7528\u6848\u4f8b\u4e2d consistently \u63d0\u5347LLM\u6027\u80fd\uff0c\u4e14\u751f\u6210\u7684\u7edf\u4e00\u6570\u636e\u96c6\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u8bad\u7ec3\u6570\u636e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is a growing need for high-quality data in Large Language Models (LLMs), but current data preparation methods are inconsistent and hard to reproduce.</li>\n    <li>DataFlow is a new framework that helps create reliable and efficient data preparation pipelines for LLMs, using modular and reusable components.</li>\n    <li>The framework includes nearly 200 reusable tools and covers various tasks like text processing, mathematical reasoning, and code generation.</li>\n    <li>DataFlow-Agent can automatically create data pipelines from simple language instructions, making it easier to use.</li>\n    <li>DataFlow has shown to significantly improve LLM performance in several tests, outperforming existing datasets and benchmarks.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\uff0c\u5229\u7528\u89c6\u89c9\u7ebf\u7d22\u8fdb\u884c\u5b9a\u4f4d\u3002</li>\n    <li>\u5f53\u524d\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5ffd\u89c6\u4e86\u4eba\u7c7b\u5e38\u7528\u7684\u7b56\u7565\u2014\u2014\u4f7f\u7528\u5730\u56fe\u3002</li>\n    <li>\u672c\u7814\u7a76\u4e3a\u6a21\u578b\u6dfb\u52a0\u4e86\u201c\u601d\u8003\u5730\u56fe\u201d\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u8bbe\u8ba1\u4e3a\u4e00\u4e2a\u201c\u5730\u56fe\u4e2d\u7684\u667a\u80fd\u4f53\u201d\u5faa\u73af\u3002</li>\n    <li>\u91c7\u7528\u4e86\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u589e\u5f3a\u5b66\u4e60\uff08RL\uff09\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08TTS\uff09\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6548\u7387\u548c\u63a2\u7d22\u80fd\u529b\u3002</li>\n    <li>\u5728\u6700\u65b0\u7684\u771f\u5b9e\u56fe\u50cf\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u7279\u522b\u662f\u51c6\u786e\u5ea6\u663e\u8457\u63d0\u9ad8\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>The goal of image geolocalization is to predict where a photo was taken using visual clues.</li>\n    <li>Current models use advanced techniques but often ignore the human strategy of using maps.</li>\n    <li>This study introduces a new model that incorporates map-thinking into its process.</li>\n    <li>The model improves its decision-making through reinforcement learning and explores multiple options before making a prediction.</li>\n    <li>Tests show this new method significantly outperforms previous models on real-world images.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni\u662f\u4e00\u4e2a\u901a\u7528\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u5b8c\u6574\u7684\u7cfb\u7edf\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5206\u79bb\u7ba1\u9053\u7684\u65b9\u6cd5\u3002</li>\n    <li>Kling-Omni\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5305\u62ec\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\uff0c\u80fd\u521b\u5efa\u7535\u5f71\u8d28\u91cf\u7684\u89c6\u9891\u5185\u5bb9\u3002</li>\n    <li>\u6846\u67b6\u5efa\u7acb\u5728\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\u4e0a\uff0c\u91c7\u7528\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u3002</li>\n    <li>Kling-Omni\u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u6267\u884c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6709\u671b\u6210\u4e3a\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u91cd\u8981\u8fdb\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new framework that creates high-quality videos from various inputs like text, images, and videos.</li>\n    <li>It combines different tasks related to video generation, editing, and reasoning into one system.</li>\n    <li>The framework processes multiple types of user inputs to produce cinematic-quality videos.</li>\n    <li>Kling-Omni is built on a strong data system and benefits from advanced training and optimization techniques.</li>\n    <li>It shows impressive performance in generating content, editing based on reasoning, and following complex instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 96, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 83, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u662f\u4e00\u79cd\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5168\u7403\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u7b56\u7565\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u8bbe\u8ba1\u4e3b\u8981\u662f\u88ab\u52a8\u5b58\u50a8\uff0c\u65e0\u6cd5\u6709\u6548\u6574\u5408\u4fe1\u606f\uff0c\u5bfc\u81f4\u63a8\u7406\u8fc7\u7a0b\u788e\u7247\u5316\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HGMem\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u80fd\u591f\u52a8\u6001\u5730\u8868\u793a\u590d\u6742\u63a8\u7406\u548c\u5168\u7403\u7406\u89e3\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u8fb9\u5c06\u8bb0\u5fc6\u5355\u5143\u8fde\u63a5\u8d77\u6765\uff0c\u5f62\u6210\u66f4\u9ad8\u9636\u7684\u4e92\u52a8\uff0c\u4ece\u800c\u589e\u5f3a\u8bb0\u5fc6\u7684\u8868\u73b0\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHGMem\u5728\u591a\u4e2a\u590d\u6742\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6b65\u9aa4RAG\u7684\u8868\u73b0\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u57fa\u7ebf\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Multi-step retrieval-augmented generation (RAG) helps improve large language models for complex tasks requiring understanding and reasoning.</li>\n  <li>Current memory systems mainly store isolated facts, limiting their ability to connect information and reason effectively.</li>\n  <li>HGMem is a new memory mechanism that uses hypergraphs to create a dynamic structure for better reasoning and understanding.</li>\n  <li>This approach allows for richer connections between facts, leading to improved multi-step reasoning and knowledge integration.</li>\n  <li>Tests show that HGMem significantly enhances performance on various challenging tasks compared to existing methods.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u63d0\u5347\uff0c\u7528\u6237\u671f\u671b\u6a21\u578b\u63d0\u4f9b\u51c6\u786e\u7684\u56de\u590d\u548c\u7b26\u5408\u591a\u6837\u5316\u4eba\u7c7b\u504f\u597d\u7684\u884c\u4e3a\u3002</li>\n    <li>\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5f00\u59cb\u4f7f\u7528\u591a\u4e2a\u5956\u52b1\u6765\u5f15\u5bfc\u6a21\u578b\u7684\u884c\u4e3a\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\u76f4\u63a5\u5e94\u7528\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u4f1a\u5bfc\u81f4\u5956\u52b1\u7ec4\u5408\u7684\u6b63\u5e38\u5316\u51fa\u73b0\u95ee\u9898\uff0c\u4ece\u800c\u5f71\u54cd\u8bad\u7ec3\u6548\u679c\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\u2014\u2014\u7fa4\u4f53\u5956\u52b1\u89e3\u8026\u6b63\u5e38\u5316\u7b56\u7565\u4f18\u5316\uff08GDPO\uff09\uff0c\u4ee5\u89e3\u51b3\u4ee5\u4e0a\u95ee\u9898\uff0c\u4fdd\u6301\u5956\u52b1\u7684\u76f8\u5bf9\u5dee\u5f02\u3002</li>\n    <li>\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\uff0cGDPO\u7684\u8868\u73b0\u4f18\u4e8eGRPO\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Users expect language models to give accurate answers and behave according to different human preferences.</li>\n    <li>Reinforcement learning (RL) is using multiple rewards to help models achieve these behaviors.</li>\n    <li>Existing methods like Group Relative Policy Optimization (GRPO) can lead to problems by normalizing rewards too much.</li>\n    <li>This paper introduces a new method called Group reward-Decoupled Normalization Policy Optimization (GDPO) that keeps the individual rewards separate, improving training stability and performance.</li>\n    <li>GDPO outperforms GRPO in tasks like tool calling, math reasoning, and coding reasoning based on accuracy and adherence to constraints.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03017", "authors": [{"_id": "696488cc138cc47cbd765365", "name": "Jing Xiong", "hidden": false}, {"_id": "696488cc138cc47cbd765366", "name": "Qi Han", "hidden": false}, {"_id": "696488cc138cc47cbd765367", "name": "Yunta Hsieh", "hidden": false}, {"_id": "696488cc138cc47cbd765368", "name": "Hui Shen", "hidden": false}, {"_id": "696488cc138cc47cbd765369", "name": "Huajian Xin", "hidden": false}, {"_id": "696488cc138cc47cbd76536a", "name": "Chaofan Tao", "hidden": false}, {"_id": "696488cc138cc47cbd76536b", "name": "Chenyang Zhao", "hidden": false}, {"_id": "696488cc138cc47cbd76536c", "name": "Hengyuan Zhang", "hidden": false}, {"_id": "696488cc138cc47cbd76536d", "name": "Taiqiang Wu", "hidden": false}, {"_id": "696488cc138cc47cbd76536e", "name": "Zhen Zhang", "hidden": false}, {"_id": "696488cc138cc47cbd76536f", "name": "Haochen Wang", "hidden": false}, {"_id": "696488cc138cc47cbd765370", "name": "Zhongwei Wan", "hidden": false}, {"_id": "696488cc138cc47cbd765371", "name": "Lingpeng Kong", "hidden": false}, {"_id": "696488cc138cc47cbd765372", "name": "Ngai Wong", "hidden": false}], "publishedAt": "2026-01-06T13:42:51.000Z", "submittedOnDailyAt": "2026-01-12T03:10:40.203Z", "title": "MMFormalizer: Multimodal Autoformalization in the Wild", "submittedOnDailyBy": {"_id": "60851545a5da133ac6c38686", "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg", "isPro": false, "fullname": "Jing Xiong", "user": "menik1126", "type": "user"}, "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io", "upvotes": 94, "discussionId": "696488cc138cc47cbd765373", "projectPage": "https://mmformalizer.github.io/", "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.", "ai_keywords": ["autoformalization", "multimodal", "perceptually grounded primitives", "recursive grounding", "axiom composition", "adaptive recursive termination", "dimensional grounding", "axiomatic grounding", "PhyX-AF", "MathVerse", "PhyX", "Synthetic Geometry", "Analytic Geometry", "GPT-5", "Gemini-3-Pro", "classical mechanics", "relativity", "quantum mechanics", "thermodynamics"], "summary_zh": "<ul>\n    <li>Autoformalization \u662f\u5c06\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u8f6c\u6362\u4e3a\u6b63\u5f0f\u9648\u8ff0\u4ee5\u652f\u6301\u673a\u5668\u63a8\u7406\u7684\u8fc7\u7a0b\uff0c\u4f46\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u9762\u4e34\u6311\u6218\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86 MMFormalizer\uff0c\u5b83\u901a\u8fc7\u4e0e\u771f\u5b9e\u4e16\u754c\u6570\u5b66\u548c\u7269\u7406\u9886\u57df\u7684\u5b9e\u4f53\u7ed3\u5408\uff0c\u6269\u5c55\u4e86 autoformalization \u7684\u5e94\u7528\u3002</li>\n    <li>MMFormalizer \u901a\u8fc7\u9012\u5f52\u6784\u5efa\u5f62\u5f0f\u547d\u9898\uff0c\u786e\u4fdd\u6bcf\u4e2a\u62bd\u8c61\u90fd\u6709\u89c6\u89c9\u8bc1\u636e\u652f\u6301\u3002</li>\n    <li>\u5728\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5 PhyX-AF \u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a GPT-5 \u5728\u7269\u7406\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u51e0\u4f55\u9886\u57df\u4ecd\u7136\u662f\u6700\u5177\u6311\u6218\u6027\u7684\u3002</li>\n    <li>MMFormalizer \u662f\u9996\u4e2a\u80fd\u591f\u5904\u7406\u7ecf\u5178\u529b\u5b66\u3001\u76f8\u5bf9\u8bba\u3001\u91cf\u5b50\u529b\u5b66\u548c\u70ed\u529b\u5b66\u7684\u591a\u6a21\u6001 autoformalization \u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoformalization helps translate natural language math into formal statements for machine reasoning, but it struggles with real-world complexities.</li>\n    <li>MMFormalizer is a new method that integrates visual elements with mathematics and physics to improve this process.</li>\n    <li>It builds formal propositions from basic visual concepts, ensuring that every idea is backed by visual proof and relevant principles.</li>\n    <li>MMFormalizer was tested on a new benchmark called PhyX-AF, which includes a variety of multimodal tasks, showing strong results with advanced models like GPT-5.</li>\n    <li>This method is the first to effectively handle complex topics like classical mechanics, relativity, quantum mechanics, and thermodynamics in an integrated way.</li>\n</ul>"}, "publishedAt": "2026-01-06T08:42:51.000Z", "title": "MMFormalizer: Multimodal Autoformalization in the Wild", "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png", "numComments": 1, "submittedBy": {"_id": "60851545a5da133ac6c38686", "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg", "fullname": "Jing Xiong", "name": "menik1126", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.00393", "authors": [{"_id": "695b2297832867f253525d68", "user": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "name": "Yuxue Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:23.295Z", "hidden": false}, {"_id": "695b2297832867f253525d69", "user": {"_id": "649ecf9827145c4463240177", "avatarUrl": "/avatars/27696cf31790a3d58d8be2e0c983800e.svg", "isPro": false, "fullname": "Lue Fan", "user": "Abyssaledge", "type": "user"}, "name": "Lue Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T13:49:26.330Z", "hidden": false}, {"_id": "695b2297832867f253525d6a", "user": {"_id": "644cc2c36dfd5f8240d76a52", "avatarUrl": "/avatars/dcd9279af1c6d8535e48dc6e3e6511cd.svg", "isPro": false, "fullname": "Ziqi Shi", "user": "renshengjihe", "type": "user"}, "name": "Ziqi Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:21.077Z", "hidden": false}, {"_id": "695b2297832867f253525d6b", "name": "Junran Peng", "hidden": false}, {"_id": "695b2297832867f253525d6c", "name": "Feng Wang", "hidden": false}, {"_id": "695b2297832867f253525d6d", "name": "Zhaoxiang Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "publishedAt": "2026-01-01T17:07:30.000Z", "submittedOnDailyAt": "2026-01-05T02:49:46.994Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "submittedOnDailyBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "upvotes": 83, "discussionId": "695b2297832867f253525d6e", "projectPage": "https://neoverse-4d.github.io/", "githubRepo": "https://github.com/IamCreateAI/NeoVerse", "githubRepoAddedBy": "user", "ai_summary": "NeoVerse is a scalable 4D world model that enables pose-free reconstruction and novel-trajectory video generation from monocular videos with state-of-the-art performance.", "ai_keywords": ["4D world model", "4D reconstruction", "novel-trajectory video generation", "monocular videos", "pose-free", "feed-forward", "degradation pattern simulation"], "githubStars": 107, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86NeoVerse\uff0c\u4e00\u4e2a\u591a\u529f\u80fd\u76844D\u4e16\u754c\u6a21\u578b\u3002</li>\n    <li>NeoVerse\u89e3\u51b3\u4e86\u73b0\u67094D\u5efa\u6a21\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002</li>\n    <li>\u5b83\u652f\u6301\u65e0\u59ff\u6001\u524d\u9988\u76844D\u91cd\u5efa\u548c\u5728\u7ebf\u5355\u76ee\u964d\u7ea7\u6a21\u5f0f\u6a21\u62df\u3002</li>\n    <li>NeoVerse\u5728\u6807\u51c6\u91cd\u5efa\u548c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002</li>\n    <li>\u9879\u76ee\u9875\u9762\u53ef\u8bbf\u95ee\uff1ahttps://neoverse-4d.github.io</li>\n</ul>", "summary_simple": "<ul>\n    <li>NeoVerse is a new 4D world model that can create 4D reconstructions and generate videos from different angles.</li>\n    <li>Current 4D modeling methods struggle with scalability due to expensive data and complicated training processes.</li>\n    <li>NeoVerse allows for easy use with regular videos, making it more accessible.</li>\n    <li>It includes advanced features for 4D reconstruction and simulates how video quality can change.</li>\n    <li>NeoVerse achieves top performance in tests for reconstruction and video generation.</li>\n</ul>"}, "publishedAt": "2026-01-01T12:07:30.000Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00393.png", "numComments": 1, "submittedBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "fullname": "Yuxue Yang", "name": "Yuppie1204", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24615", "authors": [{"_id": "69564d96832867f2535257af", "user": {"_id": "622b00a776c20fee5d14501b", "avatarUrl": "/avatars/e00496dda1e309548e7b5b437839bb65.svg", "isPro": false, "fullname": "Eason shi", "user": "Easonshi", "type": "user"}, "name": "Yuchen Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:50.111Z", "hidden": false}, {"_id": "69564d96832867f2535257b0", "user": {"_id": "66e258bdc70c02b46dfed6e3", "avatarUrl": "/avatars/ccc2d604616c018f45a268a610472cac.svg", "isPro": false, "fullname": "Yuzheng Cai", "user": "Ucreate", "type": "user"}, "name": "Yuzheng Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:50.884Z", "hidden": false}, {"_id": "69564d96832867f2535257b1", "name": "Siqi Cai", "hidden": false}, {"_id": "69564d96832867f2535257b2", "name": "Zihan Xu", "hidden": false}, {"_id": "69564d96832867f2535257b3", "user": {"_id": "64154bfa385a75d7790f80e8", "avatarUrl": "/avatars/9e22f54b5eb7c4ebedad99a9a92c4b6a.svg", "isPro": false, "fullname": "Lichao Chen", "user": "nth233", "type": "user"}, "name": "Lichao Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:46.825Z", "hidden": false}, {"_id": "69564d96832867f2535257b4", "user": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "isPro": false, "fullname": "Yulei Qin", "user": "yolay", "type": "user"}, "name": "Yulei Qin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:48.064Z", "hidden": false}, {"_id": "69564d96832867f2535257b5", "name": "Zhijian Zhou", "hidden": false}, {"_id": "69564d96832867f2535257b6", "name": "Xiang Fei", "hidden": false}, {"_id": "69564d96832867f2535257b7", "user": {"_id": "6604e43869c47cd78fdebd08", "avatarUrl": "/avatars/4c11f5e1aeae3c5eb213f6ec6d5bfe72.svg", "isPro": false, "fullname": "Qiu", "user": "ChaofanDFG", "type": "user"}, "name": "Chaofan Qiu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:48.910Z", "hidden": false}, {"_id": "69564d96832867f2535257b8", "user": {"_id": "637af0a7bdf7309aa6da1c36", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637af0a7bdf7309aa6da1c36/NHZ-09otVCfbpXVxm8f-e.png", "isPro": false, "fullname": "Xiaoyu Tan", "user": "WIlliam1900", "type": "user"}, "name": "Xiaoyu Tan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:52.763Z", "hidden": false}, {"_id": "69564d96832867f2535257b9", "name": "Gang Li", "hidden": false}, {"_id": "69564d96832867f2535257ba", "name": "Zongyi Li", "hidden": false}, {"_id": "69564d96832867f2535257bb", "name": "Haojia Lin", "hidden": false}, {"_id": "69564d96832867f2535257bc", "name": "Guocan Cai", "hidden": false}, {"_id": "69564d96832867f2535257bd", "name": "Yong Mao", "hidden": false}, {"_id": "69564d96832867f2535257be", "name": "Yunsheng Wu", "hidden": false}, {"_id": "69564d96832867f2535257bf", "name": "Ke Li", "hidden": false}, {"_id": "69564d96832867f2535257c0", "user": {"_id": "647401e50da364bd0d002f2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png", "isPro": false, "fullname": "XING SUN", "user": "tedsun", "type": "user"}, "name": "Xing Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:39.390Z", "hidden": false}], "publishedAt": "2025-12-31T04:17:36.000Z", "submittedOnDailyAt": "2026-01-05T00:21:56.456Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "submittedOnDailyBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "isPro": false, "fullname": "Ke Li", "user": "tristanli", "type": "user"}, "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "upvotes": 82, "discussionId": "69564d96832867f2535257c1", "projectPage": "https://tencentcloudadp.github.io/youtu-agent/", "githubRepo": "https://github.com/TencentCloudADP/youtu-agent", "githubRepoAddedBy": "user", "githubStars": 4095, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u6846\u67b6\u9762\u4e34\u914d\u7f6e\u6210\u672c\u9ad8\u548c\u80fd\u529b\u9759\u6001\u7684\u6311\u6218\u3002</li>\n    <li>Youtu-Agent\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u65e8\u5728\u81ea\u52a8\u751f\u6210\u548c\u6301\u7eed\u6f14\u5316LLM\u4ee3\u7406\u3002</li>\n    <li>\u8be5\u6846\u67b6\u63d0\u4f9b\u7ed3\u6784\u5316\u914d\u7f6e\u7cfb\u7edf\uff0c\u652f\u6301\u7075\u6d3b\u91cd\u7528\u548c\u81ea\u52a8\u5408\u6210\u3002</li>\n    <li>\u5f15\u5165\u5de5\u4f5c\u6d41\u6a21\u5f0f\u548c\u5143\u4ee3\u7406\u6a21\u5f0f\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u5de5\u5177\u4ee3\u7801\u3001\u63d0\u793a\u548c\u914d\u7f6e\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cYoutu-Agent\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u4e14\u4f18\u5316\u5b66\u4e60\u901f\u5ea6\u63d0\u534740%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Existing LLM agent frameworks are expensive to set up and lack flexibility in adapting to new tasks.</li>\n    <li>Youtu-Agent is a new framework that allows for easier creation and ongoing improvement of LLM agents.</li>\n    <li>It uses a structured system to separate different parts of the agent, which makes it more adaptable and easier to update.</li>\n    <li>Youtu-Agent can automatically generate code and configurations for both simple and complex tasks.</li>\n    <li>Tests show Youtu-Agent performs exceptionally well and improves learning speed and task performance significantly.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:17:36.000Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24615.png", "numComments": 1, "submittedBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "fullname": "Ke Li", "name": "tristanli", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u751f\u6210\u6a21\u578b\u80fd\u591f\u751f\u6210\u89c6\u89c9\u771f\u5b9e\u548c\u65f6\u95f4\u8fde\u8d2f\u7684\u5185\u5bb9\uff0c\u4f46\u5176\u4f5c\u4e3a\u4e16\u754c\u6a21\u62df\u5668\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u662f\u5426\u80fd\u6355\u6349\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7ea6\u675f\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u6807\u51c6\u5982FVD\u6ce8\u91cd\u611f\u77e5\u8d28\u91cf\uff0c\u800c\u5ffd\u89c6\u4e86\u56e0\u679c\u5173\u7cfb\u3001\u7269\u7406\u548c\u5168\u5c40\u4e00\u81f4\u6027\u65b9\u9762\u7684\u63a8\u7406\u5931\u8d25\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86MMGR\uff08\u591a\u6a21\u6001\u751f\u6210\u63a8\u7406\u8bc4\u4f30\u4e0e\u57fa\u51c6\uff09\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8bc4\u4f30\uff1a\u7269\u7406\u3001\u903b\u8f91\u3001\u4e09\u7ef4\u7a7a\u95f4\u3001\u4e8c\u7ef4\u7a7a\u95f4\u548c\u65f6\u95f4\u3002</li>\n    <li>MMGR\u5728\u62bd\u8c61\u63a8\u7406\u3001\u5177\u8eab\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\u7b49\u4e09\u4e2a\u9886\u57df\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u6a21\u578b\u5728\u5404\u4e2a\u9886\u57df\u7684\u8868\u73b0\u5dee\u8ddd\u660e\u663e\u3002</li>\n    <li>\u5206\u6790\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u4e3b\u8981\u5c40\u9650\u6027\uff0c\u5982\u8fc7\u4e8e\u4f9d\u8d56\u611f\u77e5\u6570\u636e\u3001\u5168\u5c40\u72b6\u6001\u4e00\u81f4\u6027\u5dee\u548c\u5956\u52b1\u89c6\u89c9\u53ef\u4fe1\u5ea6\u800c\u975e\u56e0\u679c\u6b63\u786e\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video models can create realistic videos, but they need to accurately reflect physical and logical rules to be reliable.</li>\n    <li>Current evaluation methods focus too much on visual quality and ignore reasoning mistakes like breaking physics or logic.</li>\n    <li>MMGR is a new framework that assesses five types of reasoning abilities and tests models in areas like abstract reasoning and navigation.</li>\n    <li>Benchmarking showed gaps in performance among various video and image models, especially in abstract reasoning and long-plan navigation.</li>\n    <li>MMGR aims to improve model evaluation by focusing on both visual and logical correctness, helping develop better-generating models.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u5c3d\u7ba1\u79d1\u5b66\u4eba\u5de5\u667a\u80fd\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u5c1a\u7f3a\u4e4f\u4e00\u4e2a\u5b8c\u6574\u7684\u79d1\u5b66\u901a\u7528\u667a\u80fd\uff08SGI\uff09\u6846\u67b6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b9e\u9645\u63a2\u7a76\u6a21\u578b\uff08PIM\uff09\u7684SGI\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u4e0e\u79d1\u5b66\u5bb6\u76f8\u5173\u7684\u4efb\u52a1\u8fdb\u884c\u64cd\u4f5c\u5316\u3002</li>\n    <li>SGI-Bench\u5305\u542b\u8d85\u8fc71,000\u4e2a\u8de8\u5b66\u79d1\u7684\u6837\u672c\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u6700\u65b0\u7684\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u6df1\u5ea6\u7814\u7a76\u7684\u51c6\u786e\u5339\u914d\u7387\u4f4e\uff0810-20%\uff09\uff0c\u521b\u610f\u7f3a\u4e4f\u53ef\u884c\u6027\u548c\u7ec6\u8282\uff0c\u5e72\u5b9e\u9a8c\u7684\u6267\u884c\u7ed3\u679c\u51c6\u786e\u6027\u4f4e\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\uff0c\u4f18\u5316\u63a8\u7406\u4e2d\u7684\u65b0\u9896\u6027\u5956\u52b1\uff0c\u4ee5\u589e\u5f3a\u5047\u8bbe\u7684\u65b0\u9896\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is currently no clear framework for Scientific General Intelligence (SGI), which is the ability of AI to independently think and work in scientific areas.</li>\n    <li>The authors propose a definition of SGI based on a Practical Inquiry Model, focusing on tasks like research, generating ideas, conducting experiments, and reasoning.</li>\n    <li>They created SGI-Bench, a tool with over 1,000 curated examples to evaluate the performance of advanced AI models in scientific contexts.</li>\n    <li>Results showed limitations in AI's ability to conduct deep research, generate feasible ideas, and accurately execute experiments, along with challenges in reasoning across different types of data.</li>\n    <li>The authors introduced a method called Test-Time Reinforcement Learning to improve AI's ability to generate new hypotheses without relying on previous answers.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Jan 13, 2026";
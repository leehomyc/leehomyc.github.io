window.trendingPapers = {
    "today": [{"paper": {"id": "2601.00393", "authors": [{"_id": "695b2297832867f253525d68", "user": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "name": "Yuxue Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:23.295Z", "hidden": false}, {"_id": "695b2297832867f253525d69", "user": {"_id": "649ecf9827145c4463240177", "avatarUrl": "/avatars/27696cf31790a3d58d8be2e0c983800e.svg", "isPro": false, "fullname": "Lue Fan", "user": "Abyssaledge", "type": "user"}, "name": "Lue Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T13:49:26.330Z", "hidden": false}, {"_id": "695b2297832867f253525d6a", "user": {"_id": "644cc2c36dfd5f8240d76a52", "avatarUrl": "/avatars/dcd9279af1c6d8535e48dc6e3e6511cd.svg", "isPro": false, "fullname": "Ziqi Shi", "user": "renshengjihe", "type": "user"}, "name": "Ziqi Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:21.077Z", "hidden": false}, {"_id": "695b2297832867f253525d6b", "name": "Junran Peng", "hidden": false}, {"_id": "695b2297832867f253525d6c", "name": "Feng Wang", "hidden": false}, {"_id": "695b2297832867f253525d6d", "name": "Zhaoxiang Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "publishedAt": "2026-01-01T17:07:30.000Z", "submittedOnDailyAt": "2026-01-05T02:49:46.994Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "submittedOnDailyBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "upvotes": 83, "discussionId": "695b2297832867f253525d6e", "projectPage": "https://neoverse-4d.github.io/", "githubRepo": "https://github.com/IamCreateAI/NeoVerse", "githubRepoAddedBy": "user", "ai_summary": "NeoVerse is a scalable 4D world model that enables pose-free reconstruction and novel-trajectory video generation from monocular videos with state-of-the-art performance.", "ai_keywords": ["4D world model", "4D reconstruction", "novel-trajectory video generation", "monocular videos", "pose-free", "feed-forward", "degradation pattern simulation"], "githubStars": 107, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86NeoVerse\uff0c\u4e00\u4e2a\u591a\u529f\u80fd\u76844D\u4e16\u754c\u6a21\u578b\u3002</li>\n    <li>\u89e3\u51b3\u4e86\u73b0\u67094D\u5efa\u6a21\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u9650\u5236\u3002</li>\n    <li>NeoVerse\u652f\u6301\u65e0\u59ff\u6001\u524d\u99884D\u91cd\u5efa\u548c\u5355\u76ee\u89c6\u9891\u751f\u6210\u3002</li>\n    <li>\u5177\u5907\u5728\u4e0d\u540c\u9886\u57df\u7684\u901a\u7528\u6027\u548c\u591a\u6837\u6027\u3002</li>\n    <li>\u5728\u91cd\u5efa\u548c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NeoVerse is a new model for creating 4D worlds, which includes 4D reconstruction and video generation.</li>\n    <li>Current methods of 4D modeling struggle with scalability due to expensive data and complicated training processes.</li>\n    <li>NeoVerse uses a simple and flexible approach that works with regular monocular videos.</li>\n    <li>It includes advanced techniques like pose-free reconstruction and online simulation for better performance.</li>\n    <li>NeoVerse achieves top results in benchmarks for 4D reconstruction and video generation.</li>\n</ul>"}, "publishedAt": "2026-01-01T12:07:30.000Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00393.png", "numComments": 1, "submittedBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "fullname": "Yuxue Yang", "name": "Yuppie1204", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24615", "authors": [{"_id": "69564d96832867f2535257af", "user": {"_id": "622b00a776c20fee5d14501b", "avatarUrl": "/avatars/e00496dda1e309548e7b5b437839bb65.svg", "isPro": false, "fullname": "Eason shi", "user": "Easonshi", "type": "user"}, "name": "Yuchen Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:50.111Z", "hidden": false}, {"_id": "69564d96832867f2535257b0", "user": {"_id": "66e258bdc70c02b46dfed6e3", "avatarUrl": "/avatars/ccc2d604616c018f45a268a610472cac.svg", "isPro": false, "fullname": "Yuzheng Cai", "user": "Ucreate", "type": "user"}, "name": "Yuzheng Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:50.884Z", "hidden": false}, {"_id": "69564d96832867f2535257b1", "name": "Siqi Cai", "hidden": false}, {"_id": "69564d96832867f2535257b2", "name": "Zihan Xu", "hidden": false}, {"_id": "69564d96832867f2535257b3", "user": {"_id": "64154bfa385a75d7790f80e8", "avatarUrl": "/avatars/9e22f54b5eb7c4ebedad99a9a92c4b6a.svg", "isPro": false, "fullname": "Lichao Chen", "user": "nth233", "type": "user"}, "name": "Lichao Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:46.825Z", "hidden": false}, {"_id": "69564d96832867f2535257b4", "user": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "isPro": false, "fullname": "Yulei Qin", "user": "yolay", "type": "user"}, "name": "Yulei Qin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:48.064Z", "hidden": false}, {"_id": "69564d96832867f2535257b5", "name": "Zhijian Zhou", "hidden": false}, {"_id": "69564d96832867f2535257b6", "name": "Xiang Fei", "hidden": false}, {"_id": "69564d96832867f2535257b7", "user": {"_id": "6604e43869c47cd78fdebd08", "avatarUrl": "/avatars/4c11f5e1aeae3c5eb213f6ec6d5bfe72.svg", "isPro": false, "fullname": "Qiu", "user": "ChaofanDFG", "type": "user"}, "name": "Chaofan Qiu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:48.910Z", "hidden": false}, {"_id": "69564d96832867f2535257b8", "user": {"_id": "637af0a7bdf7309aa6da1c36", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637af0a7bdf7309aa6da1c36/NHZ-09otVCfbpXVxm8f-e.png", "isPro": false, "fullname": "Xiaoyu Tan", "user": "WIlliam1900", "type": "user"}, "name": "Xiaoyu Tan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:52.763Z", "hidden": false}, {"_id": "69564d96832867f2535257b9", "name": "Gang Li", "hidden": false}, {"_id": "69564d96832867f2535257ba", "name": "Zongyi Li", "hidden": false}, {"_id": "69564d96832867f2535257bb", "name": "Haojia Lin", "hidden": false}, {"_id": "69564d96832867f2535257bc", "name": "Guocan Cai", "hidden": false}, {"_id": "69564d96832867f2535257bd", "name": "Yong Mao", "hidden": false}, {"_id": "69564d96832867f2535257be", "name": "Yunsheng Wu", "hidden": false}, {"_id": "69564d96832867f2535257bf", "name": "Ke Li", "hidden": false}, {"_id": "69564d96832867f2535257c0", "user": {"_id": "647401e50da364bd0d002f2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png", "isPro": false, "fullname": "XING SUN", "user": "tedsun", "type": "user"}, "name": "Xing Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:39.390Z", "hidden": false}], "publishedAt": "2025-12-31T04:17:36.000Z", "submittedOnDailyAt": "2026-01-05T00:21:56.456Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "submittedOnDailyBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "isPro": false, "fullname": "Ke Li", "user": "tristanli", "type": "user"}, "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "upvotes": 82, "discussionId": "69564d96832867f2535257c1", "projectPage": "https://tencentcloudadp.github.io/youtu-agent/", "githubRepo": "https://github.com/TencentCloudADP/youtu-agent", "githubRepoAddedBy": "user", "githubStars": 4095, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u6846\u67b6\u9762\u4e34\u9ad8\u914d\u7f6e\u6210\u672c\u548c\u9759\u6001\u80fd\u529b\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Youtu-Agent\uff0c\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u65e8\u5728\u81ea\u52a8\u751f\u6210\u548c\u6301\u7eed\u8fdb\u5316LLM\u4ee3\u7406\u3002</li>\n    <li>Youtu-Agent\u5177\u6709\u7ed3\u6784\u5316\u914d\u7f6e\u7cfb\u7edf\uff0c\u53ef\u7075\u6d3b\u91cd\u7528\u548c\u81ea\u52a8\u5408\u6210\u5de5\u5177\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u3002</li>\n    <li>\u5f15\u5165\u4e86\u5de5\u4f5c\u6d41\u6a21\u5f0f\u548c\u5143\u4ee3\u7406\u6a21\u5f0f\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u5de5\u5177\u4ee3\u7801\u3001\u63d0\u793a\u548c\u914d\u7f6e\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cYoutu-Agent\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u5e76\u4e14\u5728\u5de5\u5177\u5408\u6210\u548c\u6027\u80fd\u63d0\u5347\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Youtu-Agent is a new framework that helps create and improve Large Language Model (LLM) agents more easily and efficiently.</li>\n    <li>It addresses issues like high setup costs and the inability of agents to adapt to changes without expensive adjustments.</li>\n    <li>The framework allows for flexible use of tools and environments, with two ways to generate agents: one for standard tasks and another for complex tasks.</li>\n    <li>Youtu-Agent includes features that help agents learn from experience and improve their performance without needing to change their core settings.</li>\n    <li>Tests show Youtu-Agent performs very well, with significant improvements in tasks like question answering and coding, along with faster training times.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:17:36.000Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24615.png", "numComments": 1, "submittedBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "fullname": "Ke Li", "name": "tristanli", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.00664", "authors": [{"_id": "695b237a832867f253525d70", "user": {"_id": "66b57c77778c98d29446c8ec", "avatarUrl": "/avatars/c176bb7c072f3093f6a0786c87d384d8.svg", "isPro": false, "fullname": "Taekyung Ki", "user": "taekyungki", "type": "user"}, "name": "Taekyung Ki", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:19.100Z", "hidden": false}, {"_id": "695b237a832867f253525d71", "name": "Sangwon Jang", "hidden": false}, {"_id": "695b237a832867f253525d72", "name": "Jaehyeong Jo", "hidden": false}, {"_id": "695b237a832867f253525d73", "user": {"_id": "652066649004117947e46ed6", "avatarUrl": "/avatars/972c97df6f26d2c3d6ce71ec579984bb.svg", "isPro": false, "fullname": "Jaehong Yoon", "user": "jaehong31", "type": "user"}, "name": "Jaehong Yoon", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:17.228Z", "hidden": false}, {"_id": "695b237a832867f253525d74", "name": "Sung Ju Hwang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/OjpAmq7fuwGa-ZxL3KbSY.mp4"], "publishedAt": "2026-01-02T11:58:48.000Z", "submittedOnDailyAt": "2026-01-05T00:05:44.498Z", "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.", "upvotes": 43, "discussionId": "695b237a832867f253525d75", "projectPage": "https://taekyungki.github.io/AvatarForcing/", "githubRepo": "https://github.com/TaekyungKi/AvatarForcing", "githubRepoAddedBy": "user", "ai_summary": "Avatar Forcing framework enables real-time interactive head avatar generation with low latency and expressive motion through diffusion forcing and label-free preference optimization.", "ai_keywords": ["diffusion forcing", "real-time interaction", "multimodal inputs", "audio", "motion", "causal constraints", "synthetic losing samples", "direct preference optimization", "interactive head avatar generation"], "githubStars": 65, "summary_zh": "<ul>\n    <li>\u8c08\u8bdd\u5934\u50cf\u751f\u6210\u6280\u672f\u53ef\u4ee5\u5c06\u9759\u6001\u8096\u50cf\u8f6c\u5316\u4e3a\u903c\u771f\u7684\u865a\u62df\u5934\u50cf\uff0c\u7528\u4e8e\u4ea4\u6d41\u548c\u5185\u5bb9\u521b\u4f5c\u3002</li>\n    <li>\u76ee\u524d\u7684\u6a21\u578b\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u4e92\u52a8\uff0c\u901a\u5e38\u53ea\u80fd\u751f\u6210\u5355\u5411\u7684\u3001\u7f3a\u4e4f\u60c5\u611f\u7684\u56de\u5e94\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201cAvatar Forcing\u201d\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u5b9e\u65f6\u751f\u6210\u5934\u50cf\u8fd0\u52a8\u548c\u65e0\u6807\u7b7e\u6570\u636e\u5b66\u4e60\u60c5\u611f\u53cd\u5e94\u7684\u6311\u6218\u3002</li>\n    <li>\u8be5\u6846\u67b6\u53ef\u4ee5\u5b9e\u65f6\u5904\u7406\u7528\u6237\u7684\u97f3\u9891\u548c\u52a8\u4f5c\u8f93\u5165\uff0c\u53cd\u5e94\u5ef6\u8fdf\u4f4e\u81f3500\u6beb\u79d2\uff0c\u652f\u6301\u591a\u79cd\u4ea4\u6d41\u65b9\u5f0f\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u5feb6.8\u500d\uff0c\u5e76\u4e14\u751f\u6210\u7684\u5934\u50cf\u8fd0\u52a8\u66f4\u5177\u53cd\u5e94\u6027\u548c\u8868\u73b0\u529b\uff0c\u8d85\u8fc780%\u7684\u4eba\u9009\u62e9\u8be5\u65b9\u6cd5\u3002 </li>\n</ul>", "summary_simple": "<ul>\n  <li>The study focuses on creating lifelike avatars for virtual communication from static images.</li>\n  <li>Current models struggle with making interactions feel truly engaging and often respond in a one-way manner.</li>\n  <li>Two main challenges identified are generating real-time motion and learning expressive reactions without needing extra labeled data.</li>\n  <li>The new framework called Avatar Forcing allows avatars to quickly respond to user inputs like audio and movements using a method that reacts in about 500ms.</li>\n  <li>Experimental results show a significant improvement in interaction speed and expressiveness of the avatars, with over 80% preference compared to previous models.</li>\n</ul>"}, "publishedAt": "2026-01-02T06:58:48.000Z", "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation", "summary": "Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/OjpAmq7fuwGa-ZxL3KbSY.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00664.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 198}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24330", "authors": [{"_id": "69560bcd832867f2535256fc", "name": "Yong Xien Chng", "hidden": false}, {"_id": "69560bcd832867f2535256fd", "name": "Tao Hu", "hidden": false}, {"_id": "69560bcd832867f2535256fe", "name": "Wenwen Tong", "hidden": false}, {"_id": "69560bcd832867f2535256ff", "name": "Xueheng Li", "hidden": false}, {"_id": "69560bcd832867f253525700", "name": "Jiandong Chen", "hidden": false}, {"_id": "69560bcd832867f253525701", "name": "Haojia Yu", "hidden": false}, {"_id": "69560bcd832867f253525702", "name": "Jiefan Lu", "hidden": false}, {"_id": "69560bcd832867f253525703", "name": "Hewei Guo", "hidden": false}, {"_id": "69560bcd832867f253525704", "name": "Hanming Deng", "hidden": false}, {"_id": "69560bcd832867f253525705", "name": "Chengjun Xie", "hidden": false}, {"_id": "69560bcd832867f253525706", "name": "Gao Huang", "hidden": false}, {"_id": "69560bcd832867f253525707", "name": "Dahua Lin", "hidden": false}, {"_id": "69560bcd832867f253525708", "name": "Lewei Lu", "hidden": false}], "publishedAt": "2025-12-30T16:31:45.000Z", "submittedOnDailyAt": "2026-01-05T00:14:32.572Z", "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning", "submittedOnDailyBy": {"_id": "647d4f1236e109abce409c3b", "avatarUrl": "/avatars/d166f5f8be666e96b522a0a0effd21c4.svg", "isPro": false, "fullname": "Wenwen Tong", "user": "tongww", "type": "user"}, "summary": "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.", "upvotes": 30, "discussionId": "69560bcd832867f253525709", "githubRepo": "https://github.com/OpenSenseNova/SenseNova-MARS", "githubRepoAddedBy": "user", "githubStars": 24, "organization": {"_id": "64f0405f8a4cf3e5e6b38f9c", "name": "sensenova", "fullname": "SenseNova", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652d06833b5997ed71ce5c46/k66xcOMf4NVbMSFulUjHY.png"}, "summary_zh": "<ul>\n    <li>Vision-Language Models (VLMs) \u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\uff0c\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u63a8\u7406\u548c\u5355\u4e00\u5de5\u5177\u8c03\u7528\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u822c\u7684\u52a8\u6001\u5de5\u5177\u64cd\u4f5c\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86 SenseNova-MARS\uff0c\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u4ee3\u7406\u63a8\u7406\u548c\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u589e\u5f3a VLMs \u7684\u89c6\u89c9\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002</li>\n    <li>SenseNova-MARS \u52a8\u6001\u6574\u5408\u56fe\u50cf\u641c\u7d22\u3001\u6587\u672c\u641c\u7d22\u548c\u56fe\u50cf\u88c1\u526a\u5de5\u5177\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u7684\u89c6\u89c9\u7406\u89e3\u6311\u6218\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86 Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) \u7b97\u6cd5\uff0c\u6539\u5584\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u5de5\u5177\u8c03\u7528\u548c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSenseNova-MARS \u5728\u641c\u7d22\u548c\u7ec6\u7c92\u5ea6\u56fe\u50cf\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u5176\u4ed6\u5546\u4e1a\u6a21\u578b\uff0c\u5e76\u5c06\u5f00\u653e\u6240\u6709\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6570\u636e\u96c6\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-Language Models (VLMs) struggle with combining tool use and reasoning in complex visual tasks.</li>\n    <li>SenseNova-MARS is a new framework that enhances VLMs by allowing them to use tools like image search and cropping while reasoning.</li>\n    <li>It uses reinforcement learning and a new training algorithm to improve how VLMs perform tasks.</li>\n    <li>The HR-MMSearch benchmark is introduced to evaluate VLMs on complex visual tasks with high-resolution images and detailed questions.</li>\n    <li>SenseNova-MARS outperforms other models in tests, and the developers plan to share their code, models, and datasets for further study.</li>\n</ul>"}, "publishedAt": "2025-12-30T11:31:45.000Z", "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning", "summary": "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24330.png", "numComments": 1, "submittedBy": {"_id": "647d4f1236e109abce409c3b", "avatarUrl": "/avatars/d166f5f8be666e96b522a0a0effd21c4.svg", "fullname": "Wenwen Tong", "name": "tongww", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "64f0405f8a4cf3e5e6b38f9c", "name": "sensenova", "fullname": "SenseNova", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652d06833b5997ed71ce5c46/k66xcOMf4NVbMSFulUjHY.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24271", "authors": [{"_id": "695b1d02832867f253525d50", "name": "Zhe Huang", "hidden": false}, {"_id": "695b1d02832867f253525d51", "name": "Hao Wen", "hidden": false}, {"_id": "695b1d02832867f253525d52", "user": {"_id": "6583a8bdaa85c512dac3be51", "avatarUrl": "/avatars/5ece6d142e85b58ea5f59b9af251ee02.svg", "isPro": false, "fullname": "aiming hao", "user": "HamHugging", "type": "user"}, "name": "Aiming Hao", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:32.637Z", "hidden": false}, {"_id": "695b1d02832867f253525d53", "name": "Bingze Song", "hidden": false}, {"_id": "695b1d02832867f253525d54", "name": "Meiqi Wu", "hidden": false}, {"_id": "695b1d02832867f253525d55", "name": "Jiahong Wu", "hidden": false}, {"_id": "695b1d02832867f253525d56", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "695b1d02832867f253525d57", "name": "Sheng Lu", "hidden": false}, {"_id": "695b1d02832867f253525d58", "name": "Haoqian Wang", "hidden": false}], "publishedAt": "2025-12-30T14:53:33.000Z", "submittedOnDailyAt": "2026-01-05T01:14:54.357Z", "title": "Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "Multimodal Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to transform real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise ell_1 advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.", "upvotes": 25, "discussionId": "695b1d02832867f253525d59", "projectPage": "https://amap-ml.github.io/Taming-Hallucinations/", "githubRepo": "https://github.com/AMAP-ML/Taming-Hallucinations", "githubRepoAddedBy": "user", "githubStars": 29, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u9891\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b58\u5728\u4f9d\u8d56\u8bed\u8a00\u80cc\u666f\u7684\u95ee\u9898\uff0c\u53ef\u80fd\u5bfc\u81f4\u89c6\u89c9\u4e0a\u4e0d\u771f\u5b9e\u7684\u5e7b\u89c9\u3002</li>\n    <li>\u8fd9\u79cd\u95ee\u9898\u6e90\u4e8e\u6587\u672c\u548c\u89c6\u9891\u6570\u636e\u4e4b\u95f4\u7684\u4e0d\u5e73\u8861\uff0c\u6536\u96c6\u548c\u6807\u6ce8\u53cd\u4e8b\u5b9e\u6570\u636e\u7684\u6210\u672c\u5f88\u9ad8\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86DualityForge\uff0c\u4e00\u4e2a\u65b0\u9896\u7684\u53cd\u4e8b\u5b9e\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u63a7\u7684\u6269\u6563\u89c6\u9891\u7f16\u8f91\u5c06\u771f\u5b9e\u89c6\u9891\u8f6c\u6362\u4e3a\u53cd\u4e8b\u5b9e\u573a\u666f\u3002</li>\n    <li>\u8be5\u6846\u67b6\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u95ee\u7b54\u5bf9\u548c\u539f\u59cb\u7f16\u8f91\u89c6\u9891\u5bf9\uff0c\u7528\u4e8e\u5bf9\u6bd4\u8bad\u7ec3\uff0c\u4ece\u800c\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u578b\u89c6\u9891\u6570\u636e\u96c6DualityVidQA\uff0c\u65e8\u5728\u51cf\u5c11MLLM\u7684\u5e7b\u89c9\u73b0\u8c61\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u53cd\u4e8b\u5b9e\u89c6\u9891\u4e0a\u663e\u8457\u51cf\u5c11\u6a21\u578b\u5e7b\u89c9\uff0c\u76f8\u8f83\u4e8e\u57fa\u7ebf\u6a21\u578b\u63d0\u5347\u4e8624.0%\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multimodal Large Language Models (MLLMs) have improved video understanding but often make mistakes by relying too much on language instead of visuals.</li>\n    <li>This problem is worse with counterfactual videos, which show scenarios that don\u2019t make sense, and fixing it is hard due to the high cost of creating and labeling such data.</li>\n    <li>The authors introduce DualityForge, a new tool that uses video editing to create counterfactual scenarios from real videos and generates question-answer pairs for training.</li>\n    <li>They also developed DualityVidQA, a large dataset aimed at reducing MLLM errors in understanding counterfactual videos.</li>\n    <li>Tests show that their method significantly lowers errors in MLLMs when dealing with counterfactual videos and performs well across various benchmarks; they plan to share their dataset and code publicly.</li>\n</ul>"}, "publishedAt": "2025-12-30T09:53:33.000Z", "title": "Taming Hallucinations: Boosting MLLMs' Video Understanding via Counterfactual Video Generation", "summary": "Multimodal Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to transform real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise ell_1 advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24271.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.00796", "authors": [{"_id": "695b646f832867f253525e0d", "name": "Jiewen Chan", "hidden": false}, {"_id": "695b646f832867f253525e0e", "name": "Zhenjun Zhao", "hidden": false}, {"_id": "695b646f832867f253525e0f", "name": "Yu-Lun Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/enxEXrkIl4E0MXNfFVAK4.qt"], "publishedAt": "2026-01-02T18:59:55.000Z", "submittedOnDailyAt": "2026-01-05T04:47:40.380Z", "title": "AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction", "submittedOnDailyBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "isPro": false, "fullname": "Yu-Lun Liu", "user": "yulunliu", "type": "user"}, "summary": "Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/", "upvotes": 22, "discussionId": "695b646f832867f253525e10", "projectPage": "https://jiewenchan.github.io/AdaGaR/", "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86AdaGaR\uff0c\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u52a8\u60013D\u573a\u666f\u3002</li>\n    <li>AdaGaR\u901a\u8fc7\u81ea\u9002\u5e94Gabor\u8868\u793a\u6765\u5e73\u8861\u7ec6\u8282\u6355\u6349\u548c\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u4f7f\u7528\u4e09\u6b21Hermite\u6837\u6761\u548c\u65f6\u95f4\u66f2\u7387\u6b63\u5219\u5316\uff0c\u786e\u4fdd\u8fd0\u52a8\u7684\u5e73\u6ed1\u6f14\u53d8\u3002</li>\n    <li>\u81ea\u9002\u5e94\u521d\u59cb\u5316\u673a\u5236\u7ed3\u5408\u6df1\u5ea6\u4f30\u8ba1\u3001\u70b9\u8ffd\u8e2a\u548c\u524d\u666f\u906e\u7f69\uff0c\u5efa\u7acb\u7a33\u5b9a\u7684\u70b9\u4e91\u5206\u5e03\u3002</li>\n    <li>\u5728Tap-Vid DAVIS\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reconstructing 3D scenes from single videos needs to capture both details and smooth movement.</li>\n    <li>Current methods struggle with detail and can cause motion issues due to lack of continuity.</li>\n    <li>The new method, AdaGaR, improves scene modeling by using adjustable frequency and energy settings for better detail and stability.</li>\n    <li>It ensures smooth motion by using a special technique called Cubic Hermite Splines.</li>\n    <li>Tests show AdaGaR performs very well in various tasks like video editing and frame interpolation.</li>\n</ul>"}, "publishedAt": "2026-01-02T13:59:55.000Z", "title": "AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction", "summary": "Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/enxEXrkIl4E0MXNfFVAK4.qt"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00796.png", "numComments": 1, "submittedBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "fullname": "Yu-Lun Liu", "name": "yulunliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "isAuthorParticipating": false}, {"paper": {"id": "2601.00417", "authors": [{"_id": "695b2238832867f253525d62", "user": {"_id": "647bf082aba7062fe5c51ca9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/VvKAhQC_LxBcBuy3XROSX.jpeg", "isPro": false, "fullname": "Yifan Zhang", "user": "yifAI", "type": "user"}, "name": "Yifan Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T13:49:28.701Z", "hidden": false}, {"_id": "695b2238832867f253525d63", "user": {"_id": "653d276681f52ceb4d12bd85", "avatarUrl": "/avatars/56601a25e5f883a8f6dc15f6fd9dcc57.svg", "isPro": false, "fullname": "Yifeng Liu", "user": "Lewis-Lau", "type": "user"}, "name": "Yifeng Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:25.264Z", "hidden": false}, {"_id": "695b2238832867f253525d64", "name": "Mengdi Wang", "hidden": false}, {"_id": "695b2238832867f253525d65", "name": "Quanquan Gu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/BQO8lEQ-MwHpWvjNsJo6d.png"], "publishedAt": "2026-01-01T18:11:38.000Z", "submittedOnDailyAt": "2026-01-05T00:00:42.191Z", "title": "Deep Delta Learning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector k(X) and a gating scalar \u03b2(X). We provide a spectral analysis of this operator, demonstrating that the gate \u03b2(X) enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.", "upvotes": 19, "discussionId": "695b2238832867f253525d66", "projectPage": "https://yifanzhang-pro.github.io/deep-delta-learning", "githubRepo": "https://github.com/yifanzhang-pro/deep-delta-learning", "githubRepoAddedBy": "user", "ai_summary": "Deep Delta Learning introduces a novel residual connection mechanism that uses a learnable geometric transformation to generalize identity shortcuts, enabling more flexible feature modeling while maintaining stable training properties.", "ai_keywords": ["deep residual networks", "identity shortcut connection", "vanishing gradient problem", "residual connection", "Delta Operator", "rank-1 perturbation", "reflection direction vector", "gating scalar", "spectral analysis", "geometric reflection", "synchronous rank-1 injection", "layer-wise transition operator", "non-monotonic dynamics", "gated residual architectures"], "githubStars": 234, "organization": {"_id": "659e8e0b336c5a4647897ca4", "name": "math-ai", "fullname": "math-ai", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647bf082aba7062fe5c51ca9/UT7gQJZxvgq1eoG8j1Fg2.jpeg"}, "summary_zh": "<ul>\n    <li>\u6df1\u6b8b\u5dee\u7f51\u7edc\u7684\u6709\u6548\u6027\u4f9d\u8d56\u4e8e\u8eab\u4efd\u5feb\u6377\u8fde\u63a5\uff0c\u8fd9\u6709\u52a9\u4e8e\u89e3\u51b3\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u3002</li>\n    <li>\u8fd9\u79cd\u673a\u5236\u5bf9\u7279\u5f81\u8f6c\u6362\u65bd\u52a0\u4e86\u4e25\u683c\u7684\u52a0\u6027\u504f\u7f6e\uff0c\u9650\u5236\u4e86\u7f51\u7edc\u5efa\u6a21\u590d\u6742\u72b6\u6001\u8f6c\u6362\u7684\u80fd\u529b\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u67b6\u6784\uff0c\u79f0\u4e3a\u6df1\u5ea6\u589e\u91cf\u5b66\u4e60\uff08DDL\uff09\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u51e0\u4f55\u53d8\u6362\u6269\u5c55\u4e86\u6807\u51c6\u6b8b\u5dee\u8fde\u63a5\u3002</li>\n    <li>\u8fd9\u79cd\u53d8\u6362\u79f0\u4e3a\u589e\u91cf\u7b97\u5b50\uff0c\u53ef\u4ee5\u52a8\u6001\u8c03\u6574\u8eab\u4efd\u6620\u5c04\u3001\u6b63\u4ea4\u6295\u5f71\u548c\u51e0\u4f55\u53cd\u5c04\u4e4b\u95f4\u7684\u5173\u7cfb\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5141\u8bb8\u7f51\u7edc\u66f4\u597d\u5730\u63a7\u5236\u5c42\u95f4\u8f6c\u79fb\u64cd\u4f5c\u7684\u8c31\uff0c\u4ece\u800c\u5efa\u6a21\u590d\u6742\u7684\u975e\u5355\u8c03\u52a8\u6001\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u7684\u8bad\u7ec3\u7279\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Deep residual networks use identity shortcut connections to solve the vanishing gradient problem but can limit how they model complex changes.</li>\n    <li>The paper introduces Deep Delta Learning (DDL), which improves on standard residual connections by adding a learnable transformation called the Delta Operator.</li>\n    <li>The Delta Operator allows the network to adjust how it combines old and new information using a dynamic gating mechanism.</li>\n    <li>This new structure helps the network handle more complex behaviors while maintaining stable training like traditional gated structures.</li>\n    <li>The research includes a detailed analysis showing how this approach can enhance the network's adaptability and performance.</li>\n</ul>"}, "publishedAt": "2026-01-01T13:11:38.000Z", "title": "Deep Delta Learning", "summary": "The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector k(X) and a gating scalar \u03b2(X). We provide a spectral analysis of this operator, demonstrating that the gate \u03b2(X) enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/BQO8lEQ-MwHpWvjNsJo6d.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00417.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 198}, "organization": {"_id": "659e8e0b336c5a4647897ca4", "name": "math-ai", "fullname": "math-ai", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647bf082aba7062fe5c51ca9/UT7gQJZxvgq1eoG8j1Fg2.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24695", "authors": [{"_id": "695b4caa832867f253525dcd", "user": {"_id": "65cccd5134a5d74cbaa9446c", "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg", "isPro": false, "fullname": "Ali Behrouz", "user": "AliBehrouz", "type": "user"}, "name": "Ali Behrouz", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:10.014Z", "hidden": false}, {"_id": "695b4caa832867f253525dce", "name": "Meisam Razaviyayn", "hidden": false}, {"_id": "695b4caa832867f253525dcf", "name": "Peilin Zhong", "hidden": false}, {"_id": "695b4caa832867f253525dd0", "name": "Vahab Mirrokni", "hidden": false}], "publishedAt": "2025-12-31T07:59:43.000Z", "submittedOnDailyAt": "2026-01-05T03:02:31.820Z", "title": "Nested Learning: The Illusion of Deep Learning Architectures", "submittedOnDailyBy": {"_id": "65cccd5134a5d74cbaa9446c", "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg", "isPro": false, "fullname": "Ali Behrouz", "user": "AliBehrouz", "type": "user"}, "summary": "Despite the recent progresses, particularly in developing Language Models, there are fundamental challenges and unanswered questions about how such models can continually learn/memorize, self-improve, and find effective solutions. In this paper, we present a new learning paradigm, called Nested Learning (NL), that coherently represents a machine learning model with a set of nested, multi-level, and/or parallel optimization problems, each of which with its own context flow. Through the lenses of NL, existing deep learning methods learns from data through compressing their own context flow, and in-context learning naturally emerges in large models. NL suggests a philosophy to design more expressive learning algorithms with more levels, resulting in higher-order in-context learning and potentially unlocking effective continual learning capabilities. We advocate for NL by presenting three core contributions: (1) Expressive Optimizers: We show that known gradient-based optimizers, such as Adam, SGD with Momentum, etc., are in fact associative memory modules that aim to compress the gradients' information (by gradient descent). Building on this insight, we present other more expressive optimizers with deep memory and/or more powerful learning rules; (2) Self-Modifying Learning Module: Taking advantage of NL's insights on learning algorithms, we present a sequence model that learns how to modify itself by learning its own update algorithm; and (3) Continuum Memory System: We present a new formulation for memory system that generalizes the traditional viewpoint of long/short-term memory. Combining our self-modifying sequence model with the continuum memory system, we present a continual learning module, called Hope, showing promising results in language modeling, knowledge incorporation, and few-shot generalization tasks, continual learning, and long-context reasoning tasks.", "upvotes": 19, "discussionId": "695b4cab832867f253525dd1", "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u8303\u5f0f\uff0c\u79f0\u4e3a\u5d4c\u5957\u5b66\u4e60\uff08Nested Learning\uff0cNL\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5b66\u4e60\u548c\u81ea\u6211\u63d0\u5347\u80fd\u529b\u3002</li>\n    <li>NL \u901a\u8fc7\u591a\u5c42\u6b21\u548c\u5e76\u884c\u4f18\u5316\u95ee\u9898\u7684\u65b9\u5f0f\uff0c\u5e2e\u52a9\u6a21\u578b\u66f4\u52a0\u6709\u6548\u5730\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u3002</li>\n    <li>\u5c55\u793a\u4e86\u5df2\u77e5\u7684\u68af\u5ea6\u4f18\u5316\u5668\uff08\u5982 Adam \u548c\u5e26\u52a8\u91cf\u7684 SGD\uff09\u5b9e\u9645\u4e0a\u662f\u538b\u7f29\u68af\u5ea6\u4fe1\u606f\u7684\u8bb0\u5fc6\u6a21\u5757\u3002</li>\n    <li>\u4ecb\u7ecd\u4e86\u4e00\u79cd\u81ea\u6211\u4fee\u6539\u7684\u5b66\u4e60\u6a21\u5757\uff0c\u53ef\u4ee5\u5b66\u4e60\u5982\u4f55\u8c03\u6574\u81ea\u5df1\u7684\u66f4\u65b0\u7b97\u6cd5\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u4f7f\u5176\u66f4\u597d\u5730\u652f\u6301\u6301\u7eed\u5b66\u4e60\u548c\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4efb\u52a1\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper introduces a new learning approach called Nested Learning (NL) to address challenges in how language models learn and self-improve.</li>\n    <li>NL represents machine learning as a series of interconnected optimization problems, which enhances the model's ability to learn from context.</li>\n    <li>Three main contributions of NL are: \n        <ul>\n            <li>Development of more expressive optimizers that enhance how models process gradient information.</li>\n            <li>A self-modifying learning module that allows models to improve their own update algorithms.</li>\n            <li>A continuum memory system that provides a new way to understand memory in models, leading to better continual learning.</li>\n        </ul>\n    </li>\n    <li>The proposed continual learning module, called Hope, shows promise in various tasks such as language modeling and long-context reasoning.</li>\n</ul>"}, "publishedAt": "2025-12-31T02:59:43.000Z", "title": "Nested Learning: The Illusion of Deep Learning Architectures", "summary": "Despite the recent progresses, particularly in developing Language Models, there are fundamental challenges and unanswered questions about how such models can continually learn/memorize, self-improve, and find effective solutions. In this paper, we present a new learning paradigm, called Nested Learning (NL), that coherently represents a machine learning model with a set of nested, multi-level, and/or parallel optimization problems, each of which with its own context flow. Through the lenses of NL, existing deep learning methods learns from data through compressing their own context flow, and in-context learning naturally emerges in large models. NL suggests a philosophy to design more expressive learning algorithms with more levels, resulting in higher-order in-context learning and potentially unlocking effective continual learning capabilities. We advocate for NL by presenting three core contributions: (1) Expressive Optimizers: We show that known gradient-based optimizers, such as Adam, SGD with Momentum, etc., are in fact associative memory modules that aim to compress the gradients' information (by gradient descent). Building on this insight, we present other more expressive optimizers with deep memory and/or more powerful learning rules; (2) Self-Modifying Learning Module: Taking advantage of NL's insights on learning algorithms, we present a sequence model that learns how to modify itself by learning its own update algorithm; and (3) Continuum Memory System: We present a new formulation for memory system that generalizes the traditional viewpoint of long/short-term memory. Combining our self-modifying sequence model with the continuum memory system, we present a continual learning module, called Hope, showing promising results in language modeling, knowledge incorporation, and few-shot generalization tasks, continual learning, and long-context reasoning tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24695.png", "numComments": 3, "submittedBy": {"_id": "65cccd5134a5d74cbaa9446c", "avatarUrl": "/avatars/5255b734628992106598eae4f2c5848f.svg", "fullname": "Ali Behrouz", "name": "AliBehrouz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "isAuthorParticipating": true}, {"paper": {"id": "2601.00747", "authors": [{"_id": "695bd6ef6aa73bc11f09130c", "name": "Max Ruiz Luyten", "hidden": false}, {"_id": "695bd6ef6aa73bc11f09130d", "name": "Mihaela van der Schaar", "hidden": false}], "publishedAt": "2026-01-02T17:10:31.000Z", "submittedOnDailyAt": "2026-01-05T12:56:27.215Z", "title": "The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving", "submittedOnDailyBy": {"_id": "64b666ceb59ced6b452ffb09", "avatarUrl": "/avatars/3114cc44273944a6923c33f94aa93ea7.svg", "isPro": false, "fullname": "Max Ruiz Luyten", "user": "maxruizluyten", "type": "user"}, "summary": "State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.", "upvotes": 10, "discussionId": "695bd6ef6aa73bc11f09130e", "ai_summary": "Large language model training methods that optimize for correctness can cause reasoning path diversity collapse, but a new variational framework provides principled solutions to maintain both accuracy and creativity.", "ai_keywords": ["large language models", "bootstrapped reasoning loops", "chains of thought", "reasoning paths", "semantic entropy", "Distributional Creative Reasoning", "variational objective", "gradient flow", "probability measures", "solution traces", "STaR", "GRPO", "DPO", "entropy bonuses"], "organization": {"_id": "65f9e02087d1c912d985eebf", "name": "CambUni", "fullname": "University of Cambridge", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/gdCou-0KoYsLPYqPrz6xn.png"}, "summary_zh": "<ul>\n    <li>\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u4e8e\u81ea\u6211\u589e\u5f3a\u7684\u63a8\u7406\u5faa\u73af\uff0c\u4ee5\u4f18\u5316\u6b63\u786e\u6027\u3002</li>\n    <li>\u8fd9\u79cd\u8bbe\u8ba1\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u63a8\u7406\u8def\u5f84\u7684\u5206\u5e03\u5d29\u6e83\uff0c\u4ece\u800c\u964d\u4f4e\u521b\u9020\u6027\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86\u5206\u5e03\u521b\u610f\u63a8\u7406\uff08DCR\uff09\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u76ee\u6807\u6765\u8bad\u7ec3\u6a21\u578b\u3002</li>\n    <li>DCR\u6846\u67b6\u5f97\u51fa\u4e86\u4e09\u4e2a\u4e3b\u8981\u7ed3\u679c\uff0c\u5305\u62ec\u591a\u6837\u6027\u8870\u9000\u5b9a\u7406\u548c\u786e\u4fdd\u653f\u7b56\u7a33\u5b9a\u591a\u6837\u5316\u7684\u8bbe\u8ba1\u3002</li>\n    <li>DCR\u4e3a\u4fdd\u6301\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6b63\u786e\u6027\u548c\u521b\u9020\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current large language models (LLMs) focus on finding and reinforcing the most correct reasoning paths, which can limit their creativity.</li>\n    <li>The study introduces a new method called Distributional Creative Reasoning (DCR) to help improve how LLMs learn and generate solutions.</li>\n    <li>DCR shows how focusing too much on correctness can reduce diversity in reasoning, which is important for creative problem-solving.</li>\n    <li>The framework provides three main findings: how diversity decays with different methods, ways to maintain a diverse and stable approach, and practical steps to achieve this.</li>\n    <li>DCR aims to help LLMs be both accurate and creative in their reasoning processes.</li>\n</ul>"}, "publishedAt": "2026-01-02T12:10:31.000Z", "title": "The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving", "summary": "State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00747.png", "numComments": 1, "submittedBy": {"_id": "64b666ceb59ced6b452ffb09", "avatarUrl": "/avatars/3114cc44273944a6923c33f94aa93ea7.svg", "fullname": "Max Ruiz Luyten", "name": "maxruizluyten", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "65f9e02087d1c912d985eebf", "name": "CambUni", "fullname": "University of Cambridge", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/gdCou-0KoYsLPYqPrz6xn.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22955", "authors": [{"_id": "695b3555832867f253525d92", "user": {"_id": "6445e7b1b272430bdbf64e80", "avatarUrl": "/avatars/d3e59a3b488f8539966c944bb16f7b90.svg", "isPro": false, "fullname": "Haoyuan WU", "user": "hywu", "type": "user"}, "name": "Haoyuan Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:15.168Z", "hidden": false}, {"_id": "695b3555832867f253525d93", "name": "Hai Wang", "hidden": false}, {"_id": "695b3555832867f253525d94", "name": "Jiajia Wu", "hidden": false}, {"_id": "695b3555832867f253525d95", "name": "Jinxiang Ou", "hidden": false}, {"_id": "695b3555832867f253525d96", "name": "Keyao Wang", "hidden": false}, {"_id": "695b3555832867f253525d97", "name": "Weile Chen", "hidden": false}, {"_id": "695b3555832867f253525d98", "name": "Zihao Zheng", "hidden": false}, {"_id": "695b3555832867f253525d99", "name": "Bei Yu", "hidden": false}], "publishedAt": "2025-12-28T14:53:24.000Z", "submittedOnDailyAt": "2026-01-05T06:31:03.112Z", "title": "Diversity or Precision? A Deep Dive into Next Token Prediction", "submittedOnDailyBy": {"_id": "6445e7b1b272430bdbf64e80", "avatarUrl": "/avatars/d3e59a3b488f8539966c944bb16f7b90.svg", "isPro": false, "fullname": "Haoyuan WU", "user": "hywu", "type": "user"}, "summary": "Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs). The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode. To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning. By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision. Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL.", "upvotes": 4, "discussionId": "695b3556832867f253525d9a", "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u80fd\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>RL\u8bad\u7ec3\u7684\u6709\u6548\u6027\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8f93\u51fa\u5206\u5e03\u5bc6\u5207\u76f8\u5173\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u5c06RL\u539f\u5219\u5e94\u7528\u4e8e\u76d1\u7763\u5b66\u4e60\u3002</li>\n    <li>\u5f15\u5165\u5956\u52b1\u5851\u9020\u7b56\u7565\uff0c\u5e73\u8861\u591a\u6837\u6027\u548c\u7cbe\u786e\u6027\uff0c\u4ee5\u6539\u5584\u63a2\u7d22\u7a7a\u95f4\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u6ce8\u91cd\u7cbe\u786e\u6027\u7684\u5148\u9a8c\u5206\u5e03\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u63a2\u7d22\u7a7a\u95f4\uff0c\u63d0\u5347\u63a8\u7406\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) can significantly boost the reasoning skills of large language models (LLMs).</li>\n    <li>The success of RL training relies on how the pre-trained model defines its output options.</li>\n    <li>This paper reinterprets cross-entropy loss as a method for improving RL through policy gradient optimization.</li>\n    <li>A new approach is introduced that balances diversity and accuracy in token prediction for better RL training.</li>\n    <li>Surprisingly, focusing on precision rather than randomness leads to more effective exploration in RL.</li>\n</ul>"}, "publishedAt": "2025-12-28T09:53:24.000Z", "title": "Diversity or Precision? A Deep Dive into Next Token Prediction", "summary": "Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs). The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode. To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning. By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision. Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22955.png", "numComments": 1, "submittedBy": {"_id": "6445e7b1b272430bdbf64e80", "avatarUrl": "/avatars/d3e59a3b488f8539966c944bb16f7b90.svg", "fullname": "Haoyuan WU", "name": "hywu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 23}, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "isAuthorParticipating": true}],
    "week": [{"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 87, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 51, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7528\u4e8e\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u73b0\u6709\u7684\u5185\u5b58\u8bbe\u8ba1\u4e3b\u8981\u662f\u88ab\u52a8\u5b58\u50a8\uff0c\u4e0d\u80fd\u6709\u6548\u5904\u7406\u4e8b\u5b9e\u4e4b\u95f4\u7684\u9ad8\u9636\u5173\u8054\u3002</li>\n    <li>HGMem\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u5185\u5b58\u673a\u5236\uff0c\u8d85\u8d8a\u4e86\u7b80\u5355\u5b58\u50a8\uff0c\u652f\u6301\u590d\u6742\u63a8\u7406\u548c\u5168\u9762\u7406\u89e3\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u56fe\u8868\u793a\uff0c\u4fc3\u8fdb\u5185\u5b58\u4e2d\u66f4\u9ad8\u9636\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5f62\u6210\u7efc\u5408\u77e5\u8bc6\u7ed3\u6784\u3002</li>\n    <li>\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\uff0cHGMem\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6b65RAG\u7684\u8868\u73b0\uff0c\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps large language models (LLMs) with complex reasoning tasks.</li>\n    <li>Current memory systems mainly store isolated facts and do not effectively connect related information.</li>\n    <li>This lack of connection limits reasoning ability and understanding in longer contexts.</li>\n    <li>HGMem is a new memory system that uses a hypergraph to create dynamic connections between facts for better reasoning.</li>\n    <li>Tests show that HGMem improves performance in multi-step RAG tasks compared to existing systems.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.00393", "authors": [{"_id": "695b2297832867f253525d68", "user": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "name": "Yuxue Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:23.295Z", "hidden": false}, {"_id": "695b2297832867f253525d69", "user": {"_id": "649ecf9827145c4463240177", "avatarUrl": "/avatars/27696cf31790a3d58d8be2e0c983800e.svg", "isPro": false, "fullname": "Lue Fan", "user": "Abyssaledge", "type": "user"}, "name": "Lue Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T13:49:26.330Z", "hidden": false}, {"_id": "695b2297832867f253525d6a", "user": {"_id": "644cc2c36dfd5f8240d76a52", "avatarUrl": "/avatars/dcd9279af1c6d8535e48dc6e3e6511cd.svg", "isPro": false, "fullname": "Ziqi Shi", "user": "renshengjihe", "type": "user"}, "name": "Ziqi Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:21.077Z", "hidden": false}, {"_id": "695b2297832867f253525d6b", "name": "Junran Peng", "hidden": false}, {"_id": "695b2297832867f253525d6c", "name": "Feng Wang", "hidden": false}, {"_id": "695b2297832867f253525d6d", "name": "Zhaoxiang Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "publishedAt": "2026-01-01T17:07:30.000Z", "submittedOnDailyAt": "2026-01-05T02:49:46.994Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "submittedOnDailyBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "upvotes": 83, "discussionId": "695b2297832867f253525d6e", "projectPage": "https://neoverse-4d.github.io/", "githubRepo": "https://github.com/IamCreateAI/NeoVerse", "githubRepoAddedBy": "user", "ai_summary": "NeoVerse is a scalable 4D world model that enables pose-free reconstruction and novel-trajectory video generation from monocular videos with state-of-the-art performance.", "ai_keywords": ["4D world model", "4D reconstruction", "novel-trajectory video generation", "monocular videos", "pose-free", "feed-forward", "degradation pattern simulation"], "githubStars": 107, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86NeoVerse\uff0c\u4e00\u4e2a\u591a\u529f\u80fd\u76844D\u4e16\u754c\u6a21\u578b\u3002</li>\n    <li>\u89e3\u51b3\u4e86\u73b0\u67094D\u5efa\u6a21\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u9650\u5236\u3002</li>\n    <li>NeoVerse\u652f\u6301\u65e0\u59ff\u6001\u524d\u99884D\u91cd\u5efa\u548c\u5355\u76ee\u89c6\u9891\u751f\u6210\u3002</li>\n    <li>\u5177\u5907\u5728\u4e0d\u540c\u9886\u57df\u7684\u901a\u7528\u6027\u548c\u591a\u6837\u6027\u3002</li>\n    <li>\u5728\u91cd\u5efa\u548c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NeoVerse is a new model for creating 4D worlds, which includes 4D reconstruction and video generation.</li>\n    <li>Current methods of 4D modeling struggle with scalability due to expensive data and complicated training processes.</li>\n    <li>NeoVerse uses a simple and flexible approach that works with regular monocular videos.</li>\n    <li>It includes advanced techniques like pose-free reconstruction and online simulation for better performance.</li>\n    <li>NeoVerse achieves top results in benchmarks for 4D reconstruction and video generation.</li>\n</ul>"}, "publishedAt": "2026-01-01T12:07:30.000Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00393.png", "numComments": 1, "submittedBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "fullname": "Yuxue Yang", "name": "Yuppie1204", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24615", "authors": [{"_id": "69564d96832867f2535257af", "user": {"_id": "622b00a776c20fee5d14501b", "avatarUrl": "/avatars/e00496dda1e309548e7b5b437839bb65.svg", "isPro": false, "fullname": "Eason shi", "user": "Easonshi", "type": "user"}, "name": "Yuchen Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:50.111Z", "hidden": false}, {"_id": "69564d96832867f2535257b0", "user": {"_id": "66e258bdc70c02b46dfed6e3", "avatarUrl": "/avatars/ccc2d604616c018f45a268a610472cac.svg", "isPro": false, "fullname": "Yuzheng Cai", "user": "Ucreate", "type": "user"}, "name": "Yuzheng Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:50.884Z", "hidden": false}, {"_id": "69564d96832867f2535257b1", "name": "Siqi Cai", "hidden": false}, {"_id": "69564d96832867f2535257b2", "name": "Zihan Xu", "hidden": false}, {"_id": "69564d96832867f2535257b3", "user": {"_id": "64154bfa385a75d7790f80e8", "avatarUrl": "/avatars/9e22f54b5eb7c4ebedad99a9a92c4b6a.svg", "isPro": false, "fullname": "Lichao Chen", "user": "nth233", "type": "user"}, "name": "Lichao Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:46.825Z", "hidden": false}, {"_id": "69564d96832867f2535257b4", "user": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "isPro": false, "fullname": "Yulei Qin", "user": "yolay", "type": "user"}, "name": "Yulei Qin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:48.064Z", "hidden": false}, {"_id": "69564d96832867f2535257b5", "name": "Zhijian Zhou", "hidden": false}, {"_id": "69564d96832867f2535257b6", "name": "Xiang Fei", "hidden": false}, {"_id": "69564d96832867f2535257b7", "user": {"_id": "6604e43869c47cd78fdebd08", "avatarUrl": "/avatars/4c11f5e1aeae3c5eb213f6ec6d5bfe72.svg", "isPro": false, "fullname": "Qiu", "user": "ChaofanDFG", "type": "user"}, "name": "Chaofan Qiu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:48.910Z", "hidden": false}, {"_id": "69564d96832867f2535257b8", "user": {"_id": "637af0a7bdf7309aa6da1c36", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637af0a7bdf7309aa6da1c36/NHZ-09otVCfbpXVxm8f-e.png", "isPro": false, "fullname": "Xiaoyu Tan", "user": "WIlliam1900", "type": "user"}, "name": "Xiaoyu Tan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:52.763Z", "hidden": false}, {"_id": "69564d96832867f2535257b9", "name": "Gang Li", "hidden": false}, {"_id": "69564d96832867f2535257ba", "name": "Zongyi Li", "hidden": false}, {"_id": "69564d96832867f2535257bb", "name": "Haojia Lin", "hidden": false}, {"_id": "69564d96832867f2535257bc", "name": "Guocan Cai", "hidden": false}, {"_id": "69564d96832867f2535257bd", "name": "Yong Mao", "hidden": false}, {"_id": "69564d96832867f2535257be", "name": "Yunsheng Wu", "hidden": false}, {"_id": "69564d96832867f2535257bf", "name": "Ke Li", "hidden": false}, {"_id": "69564d96832867f2535257c0", "user": {"_id": "647401e50da364bd0d002f2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png", "isPro": false, "fullname": "XING SUN", "user": "tedsun", "type": "user"}, "name": "Xing Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:39.390Z", "hidden": false}], "publishedAt": "2025-12-31T04:17:36.000Z", "submittedOnDailyAt": "2026-01-05T00:21:56.456Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "submittedOnDailyBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "isPro": false, "fullname": "Ke Li", "user": "tristanli", "type": "user"}, "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "upvotes": 82, "discussionId": "69564d96832867f2535257c1", "projectPage": "https://tencentcloudadp.github.io/youtu-agent/", "githubRepo": "https://github.com/TencentCloudADP/youtu-agent", "githubRepoAddedBy": "user", "githubStars": 4095, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u6846\u67b6\u9762\u4e34\u9ad8\u914d\u7f6e\u6210\u672c\u548c\u9759\u6001\u80fd\u529b\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Youtu-Agent\uff0c\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u65e8\u5728\u81ea\u52a8\u751f\u6210\u548c\u6301\u7eed\u8fdb\u5316LLM\u4ee3\u7406\u3002</li>\n    <li>Youtu-Agent\u5177\u6709\u7ed3\u6784\u5316\u914d\u7f6e\u7cfb\u7edf\uff0c\u53ef\u7075\u6d3b\u91cd\u7528\u548c\u81ea\u52a8\u5408\u6210\u5de5\u5177\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u3002</li>\n    <li>\u5f15\u5165\u4e86\u5de5\u4f5c\u6d41\u6a21\u5f0f\u548c\u5143\u4ee3\u7406\u6a21\u5f0f\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u5de5\u5177\u4ee3\u7801\u3001\u63d0\u793a\u548c\u914d\u7f6e\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cYoutu-Agent\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u5e76\u4e14\u5728\u5de5\u5177\u5408\u6210\u548c\u6027\u80fd\u63d0\u5347\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Youtu-Agent is a new framework that helps create and improve Large Language Model (LLM) agents more easily and efficiently.</li>\n    <li>It addresses issues like high setup costs and the inability of agents to adapt to changes without expensive adjustments.</li>\n    <li>The framework allows for flexible use of tools and environments, with two ways to generate agents: one for standard tasks and another for complex tasks.</li>\n    <li>Youtu-Agent includes features that help agents learn from experience and improve their performance without needing to change their core settings.</li>\n    <li>Tests show Youtu-Agent performs very well, with significant improvements in tasks like question answering and coding, along with faster training times.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:17:36.000Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24615.png", "numComments": 1, "submittedBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "fullname": "Ke Li", "name": "tristanli", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24880", "authors": [{"_id": "69561fbf832867f253525726", "name": "Zhenda Xie", "hidden": false}, {"_id": "69561fbf832867f253525727", "name": "Yixuan Wei", "hidden": false}, {"_id": "69561fbf832867f253525728", "name": "Huanqi Cao", "hidden": false}, {"_id": "69561fbf832867f253525729", "name": "Chenggang Zhao", "hidden": false}, {"_id": "69561fbf832867f25352572a", "name": "Chengqi Deng", "hidden": false}, {"_id": "69561fbf832867f25352572b", "name": "Jiashi Li", "hidden": false}, {"_id": "69561fbf832867f25352572c", "name": "Damai Dai", "hidden": false}, {"_id": "69561fbf832867f25352572d", "name": "Huazuo Gao", "hidden": false}, {"_id": "69561fbf832867f25352572e", "name": "Jiang Chang", "hidden": false}, {"_id": "69561fbf832867f25352572f", "name": "Liang Zhao", "hidden": false}, {"_id": "69561fbf832867f253525730", "name": "Shangyan Zhou", "hidden": false}, {"_id": "69561fbf832867f253525731", "name": "Zhean Xu", "hidden": false}, {"_id": "69561fbf832867f253525732", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "69561fbf832867f253525733", "name": "Wangding Zeng", "hidden": false}, {"_id": "69561fbf832867f253525734", "name": "Shengding Hu", "hidden": false}, {"_id": "69561fbf832867f253525735", "name": "Yuqing Wang", "hidden": false}, {"_id": "69561fbf832867f253525736", "name": "Jingyang Yuan", "hidden": false}, {"_id": "69561fbf832867f253525737", "name": "Lean Wang", "hidden": false}, {"_id": "69561fbf832867f253525738", "name": "Wenfeng Liang", "hidden": false}], "publishedAt": "2025-12-31T14:16:26.000Z", "submittedOnDailyAt": "2026-01-01T07:30:32.169Z", "title": "mHC: Manifold-Constrained Hyper-Connections", "submittedOnDailyBy": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "isPro": false, "fullname": "Adina Yakefu", "user": "AdinaY", "type": "user"}, "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.", "upvotes": 59, "discussionId": "69561fc0832867f253525739", "ai_summary": "Manifold-Constrained Hyper-Connections (mHC) stabilize and scale residual connection architectures by restoring identity mapping properties through manifold projection and infrastructure optimization.", "ai_keywords": ["Hyper-Connections (HC)", "Manifold-Constrained Hyper-Connections (mHC)", "residual connections", "residual stream width", "connectivity patterns", "identity mapping property", "training instability", "memory access overhead", "manifold projection", "infrastructure optimization", "scalability"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53eb\u505a\u201c\u6d41\u5f62\u7ea6\u675f\u8d85\u8fde\u63a5\u201d\uff08mHC\uff09\u7684\u65b0\u6846\u67b6\u3002</li>\n    <li>mHC\u65e8\u5728\u89e3\u51b3\u8d85\u8fde\u63a5\uff08HC\uff09\u4e2d\u7684\u8eab\u4efd\u6620\u5c04\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002</li>\n    <li>\u8be5\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u57fa\u7840\u8bbe\u65bd\u6765\u63d0\u5347\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u63d0\u5347\u3002</li>\n    <li>\u5b9e\u9a8c\u8bc1\u660e\uff0cmHC\u5728\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\u6709\u6548\uff0c\u5e26\u6765\u4e86\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\u3002</li>\n    <li>mHC\u4e3a\u9876\u5c42\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u89e3\u548c\u53d1\u5c55\u57fa\u7840\u6a21\u578b\u7684\u65b9\u5411\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent studies have improved residual connections in neural networks with a method called Hyper-Connections (HC), which increases connection diversity and stream width.</li>\n    <li>However, these improvements can lead to issues like instability during training and higher memory usage due to the loss of the identity mapping property.</li>\n    <li>To solve these problems, a new approach called Manifold-Constrained Hyper-Connections (mHC) is proposed, which restores the identity mapping while optimizing efficiency.</li>\n    <li>Tests show that mHC allows for better performance and scalability when training large models.</li>\n    <li>mHC could help advance the design of neural networks and guide future improvements in foundational models.</li>\n</ul>"}, "publishedAt": "2025-12-31T09:16:26.000Z", "title": "mHC: Manifold-Constrained Hyper-Connections", "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24880.png", "numComments": 1, "submittedBy": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "fullname": "Adina Yakefu", "name": "AdinaY", "type": "user", "isPro": false, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 1140}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24617", "authors": [{"_id": "69573165832867f253525871", "user": {"_id": "646b4c9fdf2609a541c0866e", "avatarUrl": "/avatars/7ec6c709017dcf32d4ac49d1e3820328.svg", "isPro": false, "fullname": "Qu", "user": "ScottQu", "type": "user"}, "name": "Xingwei Qu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:07.662Z", "hidden": false}, {"_id": "69573165832867f253525872", "name": "Shaowen Wang", "hidden": false}, {"_id": "69573165832867f253525873", "user": {"_id": "65a62085576772f531e13856", "avatarUrl": "/avatars/72c67a60422e333ea4e323f7480ae0b7.svg", "isPro": false, "fullname": "Huang Zihao", "user": "FetchFortune", "type": "user"}, "name": "Zihao Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:46.182Z", "hidden": false}, {"_id": "69573165832867f253525874", "user": {"_id": "64e851825ddcace745ba15bd", "avatarUrl": "/avatars/7b6612c411222974d9ea181784eef915.svg", "isPro": false, "fullname": "Kai Hua", "user": "kkish", "type": "user"}, "name": "Kai Hua", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:53.467Z", "hidden": false}, {"_id": "69573165832867f253525875", "name": "Fan Yin", "hidden": false}, {"_id": "69573165832867f253525876", "name": "Rui-Jie Zhu", "hidden": false}, {"_id": "69573165832867f253525877", "name": "Jundong Zhou", "hidden": false}, {"_id": "69573165832867f253525878", "name": "Qiyang Min", "hidden": false}, {"_id": "69573165832867f253525879", "name": "Zihao Wang", "hidden": false}, {"_id": "69573165832867f25352587a", "name": "Yizhi Li", "hidden": false}, {"_id": "69573165832867f25352587b", "name": "Tianyu Zhang", "hidden": false}, {"_id": "69573165832867f25352587c", "name": "He Xing", "hidden": false}, {"_id": "69573165832867f25352587d", "name": "Zheng Zhang", "hidden": false}, {"_id": "69573165832867f25352587e", "name": "Yuxuan Song", "hidden": false}, {"_id": "69573165832867f25352587f", "name": "Tianyu Zheng", "hidden": false}, {"_id": "69573165832867f253525880", "name": "Zhiyuan Zeng", "hidden": false}, {"_id": "69573165832867f253525881", "name": "Chenghua Lin", "hidden": false}, {"_id": "69573165832867f253525882", "name": "Ge Zhang", "hidden": false}, {"_id": "69573165832867f253525883", "name": "Wenhao Huang", "hidden": false}], "publishedAt": "2025-12-31T04:19:33.000Z", "submittedOnDailyAt": "2026-01-02T00:17:55.450Z", "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space", "submittedOnDailyBy": {"_id": "646b4c9fdf2609a541c0866e", "avatarUrl": "/avatars/7ec6c709017dcf32d4ac49d1e3820328.svg", "isPro": false, "fullname": "Qu", "user": "ScottQu", "type": "user"}, "summary": "Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first compression-aware scaling law, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a decoupled \u03bcP parametrization that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting (R=4, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a +2.69\\% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.", "upvotes": 50, "discussionId": "69573165832867f253525884", "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u6240\u6709\u8bcd\u5143\u5e94\u7528\u7edf\u4e00\u8ba1\u7b97\uff0c\u4f46\u8bed\u8a00\u7684\u4fe1\u606f\u5bc6\u5ea6\u662f\u4e0d\u5747\u5300\u7684\u3002</li>\n    <li>\u63d0\u51fa\u52a8\u6001\u5927\u6982\u5ff5\u6a21\u578b\uff08DLCM\uff09\uff0c\u901a\u8fc7\u5b66\u4e60\u8bed\u4e49\u8fb9\u754c\u5e76\u5c06\u8ba1\u7b97\u4ece\u8bcd\u5143\u8f6c\u79fb\u5230\u538b\u7f29\u7684\u6982\u5ff5\u7a7a\u95f4\uff0c\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002</li>\n    <li>DLCM\u53ef\u4ee5\u7aef\u5230\u7aef\u53d1\u73b0\u53ef\u53d8\u957f\u5ea6\u7684\u6982\u5ff5\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u9884\u5b9a\u4e49\u7684\u8bed\u8a00\u5355\u4f4d\u3002</li>\n    <li>\u5f15\u5165\u4e86\u7b2c\u4e00\u4e2a\u5173\u6ce8\u538b\u7f29\u7684\u6269\u5c55\u6cd5\u5219\uff0c\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u7684\u5206\u914d\u3002</li>\n    <li>DLCM\u5728\u5b9e\u9645\u8bbe\u7f6e\u4e2d\u5c06\u5927\u7ea6\u4e09\u5206\u4e4b\u4e00\u7684\u63a8\u7406\u8ba1\u7b97\u91cd\u65b0\u5206\u914d\uff0c\u5e73\u5747\u572812\u4e2a\u96f6-shot\u57fa\u51c6\u4e0a\u63d0\u9ad8\u4e862.69%\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Large Language Models use the same processing for all words, which isn't efficient because language has different levels of complexity.</li>\n  <li>Dynamic Large Concept Models (DLCM) focus on key ideas rather than individual words, making processing more efficient.</li>\n  <li>DLCM identifies important concepts and can adapt to different lengths of ideas without needing fixed language rules.</li>\n  <li>The framework includes a new scaling law that helps allocate computing power wisely based on different levels of processing needs.</li>\n  <li>In tests, DLCM improved performance by about 2.69% while using the same amount of computing resources.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:19:33.000Z", "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space", "summary": "Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose Dynamic Large Concept Models (DLCM), a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first compression-aware scaling law, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a decoupled \u03bcP parametrization that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting (R=4, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a +2.69\\% average improvement across 12 zero-shot benchmarks under matched inference FLOPs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24617.png", "numComments": 4, "submittedBy": {"_id": "646b4c9fdf2609a541c0866e", "avatarUrl": "/avatars/7ec6c709017dcf32d4ac49d1e3820328.svg", "fullname": "Qu", "name": "ScottQu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24618", "authors": [{"_id": "6955d543832867f25352555d", "name": "Junru Lu", "hidden": false}, {"_id": "6955d543832867f25352555e", "name": "Jiarui Qin", "hidden": false}, {"_id": "6955d543832867f25352555f", "name": "Lingfeng Qiao", "hidden": false}, {"_id": "6955d543832867f253525560", "name": "Yinghui Li", "hidden": false}, {"_id": "6955d543832867f253525561", "name": "Xinyi Dai", "hidden": false}, {"_id": "6955d543832867f253525562", "name": "Bo Ke", "hidden": false}, {"_id": "6955d543832867f253525563", "name": "Jianfeng He", "hidden": false}, {"_id": "6955d543832867f253525564", "name": "Ruizhi Qiao", "hidden": false}, {"_id": "6955d543832867f253525565", "name": "Di Yin", "hidden": false}, {"_id": "6955d543832867f253525566", "name": "Xing Sun", "hidden": false}, {"_id": "6955d543832867f253525567", "name": "Yunsheng Wu", "hidden": false}, {"_id": "6955d543832867f253525568", "name": "Yinsong Liu", "hidden": false}, {"_id": "6955d543832867f253525569", "name": "Shuangyin Liu", "hidden": false}, {"_id": "6955d543832867f25352556a", "name": "Mingkong Tang", "hidden": false}, {"_id": "6955d543832867f25352556b", "name": "Haodong Lin", "hidden": false}, {"_id": "6955d543832867f25352556c", "name": "Jiayi Kuang", "hidden": false}, {"_id": "6955d543832867f25352556d", "name": "Fanxu Meng", "hidden": false}, {"_id": "6955d543832867f25352556e", "name": "Xiaojuan Tang", "hidden": false}, {"_id": "6955d543832867f25352556f", "name": "Yunjia Xi", "hidden": false}, {"_id": "6955d543832867f253525570", "name": "Junjie Huang", "hidden": false}, {"_id": "6955d543832867f253525571", "name": "Haotong Yang", "hidden": false}, {"_id": "6955d543832867f253525572", "name": "Zhenyi Shen", "hidden": false}, {"_id": "6955d543832867f253525573", "name": "Yangning Li", "hidden": false}, {"_id": "6955d543832867f253525574", "name": "Qianwen Zhang", "hidden": false}, {"_id": "6955d543832867f253525575", "name": "Yifei Yu", "hidden": false}, {"_id": "6955d543832867f253525576", "name": "Siyu An", "hidden": false}, {"_id": "6955d543832867f253525577", "name": "Junnan Dong", "hidden": false}, {"_id": "6955d543832867f253525578", "name": "Qiufeng Wang", "hidden": false}, {"_id": "6955d543832867f253525579", "name": "Jie Wang", "hidden": false}, {"_id": "6955d543832867f25352557a", "name": "Keyu Chen", "hidden": false}, {"_id": "6955d543832867f25352557b", "name": "Wei Wen", "hidden": false}, {"_id": "6955d543832867f25352557c", "name": "Taian Guo", "hidden": false}, {"_id": "6955d543832867f25352557d", "name": "Zhifeng Shen", "hidden": false}, {"_id": "6955d543832867f25352557e", "name": "Daohai Yu", "hidden": false}, {"_id": "6955d543832867f25352557f", "name": "Jiahao Li", "hidden": false}, {"_id": "6955d543832867f253525580", "name": "Ke Li", "hidden": false}, {"_id": "6955d543832867f253525581", "name": "Zongyi Li", "hidden": false}, {"_id": "6955d543832867f253525582", "name": "Xiaoyu Tan", "hidden": false}], "publishedAt": "2025-12-31T04:25:11.000Z", "submittedOnDailyAt": "2026-01-01T00:33:09.720Z", "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "upvotes": 43, "discussionId": "6955d543832867f253525583", "ai_summary": "Youtu-LLM is a lightweight language model optimized for computational efficiency and agentic intelligence through a compact architecture, STEM-focused training curriculum, and scalable mid-training strategies for planning and reasoning tasks.", "ai_keywords": ["Multi-Latent Attention (MLA) architecture", "STEM-oriented vocabulary", "128k context window", "Commonsense-STEM-Agent Curriculum", "multi-stage training strategy", "agentic mid-training", "data construction schemes", "planning and reflection behaviors", "long-context reasoning", "state tracking", "agentic capabilities"], "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86Youtu-LLM\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4f46\u5f3a\u5927\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u9ad8\u6548\u8ba1\u7b97\u5e76\u5177\u5907\u667a\u80fd\u80fd\u529b\u3002</li>\n    <li>Youtu-LLM\u662f\u5168\u65b0\u9884\u8bad\u7ec3\u7684\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u84b8\u998f\uff0c\u4e13\u6ce8\u4e8e\u57f9\u517b\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u3002</li>\n    <li>\u5177\u6709128k\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u7d27\u51d1\u67b6\u6784\uff0c\u9002\u5408\u957f\u65f6\u95f4\u7684\u63a8\u7406\u548c\u72b6\u6001\u8ddf\u8e2a\u3002</li>\n    <li>\u91c7\u7528\u5206\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u4e30\u5bcc\u7684\u6570\u636e\u96c6\u6765\u57f9\u517b\u6df1\u5ea6\u8ba4\u77e5\u80fd\u529b\u3002</li>\n    <li>\u5728\u8bc4\u4f30\u4e2d\uff0cYoutu-LLM\u5728\u5c0f\u4e8e2B\u53c2\u6570\u7684\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u4f73\u57fa\u51c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Youtu-LLM is a new lightweight language model designed for high efficiency and intelligent reasoning.</li>\n    <li>It has a unique architecture that allows it to handle long contexts (up to 128k tokens) while using minimal memory.</li>\n    <li>The model is trained on a large dataset (11 trillion tokens) with a focus on developing deep reasoning skills in STEM and agentic tasks.</li>\n    <li>It uses diverse training methods to improve its planning and problem-solving abilities in areas like math and coding.</li>\n    <li>Youtu-LLM outperforms other small models and even competes well with larger models in various tasks.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:25:11.000Z", "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24618.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 197}, "isAuthorParticipating": false}, {"paper": {"id": "2601.00664", "authors": [{"_id": "695b237a832867f253525d70", "user": {"_id": "66b57c77778c98d29446c8ec", "avatarUrl": "/avatars/c176bb7c072f3093f6a0786c87d384d8.svg", "isPro": false, "fullname": "Taekyung Ki", "user": "taekyungki", "type": "user"}, "name": "Taekyung Ki", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:19.100Z", "hidden": false}, {"_id": "695b237a832867f253525d71", "name": "Sangwon Jang", "hidden": false}, {"_id": "695b237a832867f253525d72", "name": "Jaehyeong Jo", "hidden": false}, {"_id": "695b237a832867f253525d73", "user": {"_id": "652066649004117947e46ed6", "avatarUrl": "/avatars/972c97df6f26d2c3d6ce71ec579984bb.svg", "isPro": false, "fullname": "Jaehong Yoon", "user": "jaehong31", "type": "user"}, "name": "Jaehong Yoon", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:17.228Z", "hidden": false}, {"_id": "695b237a832867f253525d74", "name": "Sung Ju Hwang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/OjpAmq7fuwGa-ZxL3KbSY.mp4"], "publishedAt": "2026-01-02T11:58:48.000Z", "submittedOnDailyAt": "2026-01-05T00:05:44.498Z", "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.", "upvotes": 43, "discussionId": "695b237a832867f253525d75", "projectPage": "https://taekyungki.github.io/AvatarForcing/", "githubRepo": "https://github.com/TaekyungKi/AvatarForcing", "githubRepoAddedBy": "user", "ai_summary": "Avatar Forcing framework enables real-time interactive head avatar generation with low latency and expressive motion through diffusion forcing and label-free preference optimization.", "ai_keywords": ["diffusion forcing", "real-time interaction", "multimodal inputs", "audio", "motion", "causal constraints", "synthetic losing samples", "direct preference optimization", "interactive head avatar generation"], "githubStars": 65, "summary_zh": "<ul>\n    <li>\u8c08\u8bdd\u5934\u50cf\u751f\u6210\u6280\u672f\u53ef\u4ee5\u5c06\u9759\u6001\u8096\u50cf\u8f6c\u5316\u4e3a\u903c\u771f\u7684\u865a\u62df\u5934\u50cf\uff0c\u7528\u4e8e\u4ea4\u6d41\u548c\u5185\u5bb9\u521b\u4f5c\u3002</li>\n    <li>\u76ee\u524d\u7684\u6a21\u578b\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u4e92\u52a8\uff0c\u901a\u5e38\u53ea\u80fd\u751f\u6210\u5355\u5411\u7684\u3001\u7f3a\u4e4f\u60c5\u611f\u7684\u56de\u5e94\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201cAvatar Forcing\u201d\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u5b9e\u65f6\u751f\u6210\u5934\u50cf\u8fd0\u52a8\u548c\u65e0\u6807\u7b7e\u6570\u636e\u5b66\u4e60\u60c5\u611f\u53cd\u5e94\u7684\u6311\u6218\u3002</li>\n    <li>\u8be5\u6846\u67b6\u53ef\u4ee5\u5b9e\u65f6\u5904\u7406\u7528\u6237\u7684\u97f3\u9891\u548c\u52a8\u4f5c\u8f93\u5165\uff0c\u53cd\u5e94\u5ef6\u8fdf\u4f4e\u81f3500\u6beb\u79d2\uff0c\u652f\u6301\u591a\u79cd\u4ea4\u6d41\u65b9\u5f0f\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u5feb6.8\u500d\uff0c\u5e76\u4e14\u751f\u6210\u7684\u5934\u50cf\u8fd0\u52a8\u66f4\u5177\u53cd\u5e94\u6027\u548c\u8868\u73b0\u529b\uff0c\u8d85\u8fc780%\u7684\u4eba\u9009\u62e9\u8be5\u65b9\u6cd5\u3002 </li>\n</ul>", "summary_simple": "<ul>\n  <li>The study focuses on creating lifelike avatars for virtual communication from static images.</li>\n  <li>Current models struggle with making interactions feel truly engaging and often respond in a one-way manner.</li>\n  <li>Two main challenges identified are generating real-time motion and learning expressive reactions without needing extra labeled data.</li>\n  <li>The new framework called Avatar Forcing allows avatars to quickly respond to user inputs like audio and movements using a method that reacts in about 500ms.</li>\n  <li>Experimental results show a significant improvement in interaction speed and expressiveness of the avatars, with over 80% preference compared to previous models.</li>\n</ul>"}, "publishedAt": "2026-01-02T06:58:48.000Z", "title": "Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation", "summary": "Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/OjpAmq7fuwGa-ZxL3KbSY.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00664.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 198}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24165", "authors": [{"_id": "69571b38832867f25352584d", "user": {"_id": "67247adb73d1eb17b6bfd27c", "avatarUrl": "/avatars/57bdbb7362f9854c87dd0a71ae071652.svg", "isPro": false, "fullname": "Zefeng He", "user": "yhx12", "type": "user"}, "name": "Zefeng He", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:20.236Z", "hidden": false}, {"_id": "69571b38832867f25352584e", "name": "Xiaoye Qu", "hidden": false}, {"_id": "69571b38832867f25352584f", "name": "Yafu Li", "hidden": false}, {"_id": "69571b38832867f253525850", "user": {"_id": "629454301ae2138079f7ff31", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629454301ae2138079f7ff31/rVtbF-j06gDiYzomTeVTc.jpeg", "isPro": false, "fullname": "Tong Zhu", "user": "Spico", "type": "user"}, "name": "Tong Zhu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:16.903Z", "hidden": false}, {"_id": "69571b38832867f253525851", "name": "Siyuan Huang", "hidden": false}, {"_id": "69571b38832867f253525852", "name": "Yu Cheng", "hidden": false}], "publishedAt": "2025-12-30T11:51:18.000Z", "submittedOnDailyAt": "2026-01-02T03:34:41.232Z", "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models", "submittedOnDailyBy": {"_id": "629454301ae2138079f7ff31", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629454301ae2138079f7ff31/rVtbF-j06gDiYzomTeVTc.jpeg", "isPro": false, "fullname": "Tong Zhu", "user": "Spico", "type": "user"}, "summary": "While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\\%) and Gemini-3-Flash (+111.6\\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.", "upvotes": 43, "discussionId": "69571b38832867f253525853", "projectPage": "https://diffthinker-project.github.io/", "githubRepo": "https://github.com/lcqysl/DiffThinker", "githubRepoAddedBy": "user", "githubStars": 95, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e0a\u53d6\u5f97\u4e86\u5f88\u5927\u8fdb\u5c55\uff0c\u4f46\u4e3b\u8981\u8fd8\u662f\u4ee5\u6587\u672c\u4e3a\u4e2d\u5fc3\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u7684\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4e0d\u4f73\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u591a\u6a21\u6001\u63a8\u7406\u8303\u5f0f\uff0c\u5e76\u4ecb\u7ecd\u4e86DiffThinker\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u63a8\u7406\u6846\u67b6\u3002</li>\n    <li>DiffThinker\u5c06\u591a\u6a21\u6001\u63a8\u7406\u91cd\u65b0\u5b9a\u4e49\u4e3a\u751f\u6210\u7684\u56fe\u50cf\u5230\u56fe\u50cf\u4efb\u52a1\uff0c\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u903b\u8f91\u4e00\u81f4\u6027\u548c\u7a7a\u95f4\u7cbe\u5ea6\u3002</li>\n    <li>\u901a\u8fc7\u4e0e\u5176\u4ed6\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6bd4\u8f83\uff0c\u63ed\u793a\u4e86DiffThinker\u7684\u56db\u4e2a\u6838\u5fc3\u7279\u6027\uff1a\u6548\u7387\u3001\u53ef\u63a7\u6027\u3001\u539f\u751f\u5e76\u884c\u6027\u548c\u534f\u4f5c\u3002</li>\n    <li>\u5728\u591a\u4e2a\u9886\u57df\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDiffThinker\u663e\u8457\u8d85\u8d8a\u4e86\u5305\u62ecGPT-5\u548cGemini-3-Flash\u5728\u5185\u7684\u9886\u5148\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u751f\u6210\u591a\u6a21\u6001\u63a8\u7406\u5728\u89c6\u89c9\u63a8\u7406\u4e2d\u7684\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent models for multimodal reasoning mainly focus on text, which can limit their effectiveness in complex visual tasks.</li>\n    <li>This paper introduces DiffThinker, a new framework that improves multimodal reasoning by treating it as an image-to-image task.</li>\n    <li>DiffThinker shows better logical consistency and spatial accuracy in visual tasks compared to existing models.</li>\n    <li>The study reveals four key features of DiffThinker: efficiency, controllability, native parallelism, and collaboration.</li>\n    <li>Experiments demonstrate that DiffThinker significantly outperforms leading models like GPT-5 and Gemini-3-Flash in various complex reasoning tasks.</li>\n</ul>"}, "publishedAt": "2025-12-30T06:51:18.000Z", "title": "DiffThinker: Towards Generative Multimodal Reasoning with Diffusion Models", "summary": "While recent Multimodal Large Language Models (MLLMs) have attained significant strides in multimodal reasoning, their reasoning processes remain predominantly text-centric, leading to suboptimal performance in complex long-horizon, vision-centric tasks. In this paper, we establish a novel Generative Multimodal Reasoning paradigm and introduce DiffThinker, a diffusion-based reasoning framework. Conceptually, DiffThinker reformulates multimodal reasoning as a native generative image-to-image task, achieving superior logical consistency and spatial precision in vision-centric tasks. We perform a systematic comparison between DiffThinker and MLLMs, providing the first in-depth investigation into the intrinsic characteristics of this paradigm, revealing four core properties: efficiency, controllability, native parallelism, and collaboration. Extensive experiments across four domains (sequential planning, combinatorial optimization, constraint satisfaction, and spatial configuration) demonstrate that DiffThinker significantly outperforms leading closed source models including GPT-5 (+314.2\\%) and Gemini-3-Flash (+111.6\\%), as well as the fine-tuned Qwen3-VL-32B baseline (+39.0\\%), highlighting generative multimodal reasoning as a promising approach for vision-centric reasoning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24165.png", "numComments": 5, "submittedBy": {"_id": "629454301ae2138079f7ff31", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629454301ae2138079f7ff31/rVtbF-j06gDiYzomTeVTc.jpeg", "fullname": "Tong Zhu", "name": "Spico", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 21}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24873", "authors": [{"_id": "6955e3f8832867f2535255cd", "name": "Weixun Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255ce", "name": "XiaoXiao Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255cf", "name": "Wanhe An", "hidden": false}, {"_id": "6955e3f8832867f2535255d0", "name": "Fangwen Dai", "hidden": false}, {"_id": "6955e3f8832867f2535255d1", "name": "Wei Gao", "hidden": false}, {"_id": "6955e3f8832867f2535255d2", "name": "Yancheng He", "hidden": false}, {"_id": "6955e3f8832867f2535255d3", "name": "Ju Huang", "hidden": false}, {"_id": "6955e3f8832867f2535255d4", "name": "Qiang Ji", "hidden": false}, {"_id": "6955e3f8832867f2535255d5", "name": "Hanqi Jin", "hidden": false}, {"_id": "6955e3f8832867f2535255d6", "name": "Xiaoyang Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d7", "name": "Yang Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d8", "name": "Zhongwen Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d9", "name": "Shirong Lin", "hidden": false}, {"_id": "6955e3f8832867f2535255da", "name": "Jiashun Liu", "hidden": false}, {"_id": "6955e3f8832867f2535255db", "name": "Zenan Liu", "hidden": false}, {"_id": "6955e3f8832867f2535255dc", "name": "Tao Luo", "hidden": false}, {"_id": "6955e3f8832867f2535255dd", "name": "Dilxat Muhtar", "hidden": false}, {"_id": "6955e3f8832867f2535255de", "name": "Yuanbin Qu", "hidden": false}, {"_id": "6955e3f8832867f2535255df", "name": "Jiaqiang Shi", "hidden": false}, {"_id": "6955e3f8832867f2535255e0", "name": "Qinghui Sun", "hidden": false}, {"_id": "6955e3f8832867f2535255e1", "name": "Yingshui Tan", "hidden": false}, {"_id": "6955e3f8832867f2535255e2", "name": "Hao Tang", "hidden": false}, {"_id": "6955e3f8832867f2535255e3", "name": "Runze Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e4", "name": "Yi Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e5", "name": "Zhaoguo Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e6", "name": "Yanan Wu", "hidden": false}, {"_id": "6955e3f8832867f2535255e7", "name": "Shaopan Xiong", "hidden": false}, {"_id": "6955e3f8832867f2535255e8", "name": "Binchen Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255e9", "name": "Xander Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255ea", "name": "Yuchi Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255eb", "name": "Qipeng Zhang", "hidden": false}, {"_id": "6955e3f8832867f2535255ec", "name": "Xixia Zhang", "hidden": false}, {"_id": "6955e3f8832867f2535255ed", "name": "Haizhou Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255ee", "name": "Jie Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255ef", "name": "Shuaibing Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255f0", "name": "Baihui Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f1", "name": "Jianhui Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f2", "name": "Suhang Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f3", "name": "Yanni Zhu", "hidden": false}, {"_id": "6955e3f8832867f2535255f4", "name": "Mengze Cai", "hidden": false}, {"_id": "6955e3f8832867f2535255f5", "name": "Kerui Cao", "hidden": false}, {"_id": "6955e3f8832867f2535255f6", "name": "Xitong Chen", "hidden": false}, {"_id": "6955e3f8832867f2535255f7", "name": "Yue Dai", "hidden": false}, {"_id": "6955e3f8832867f2535255f8", "name": "Lifan Du", "hidden": false}, {"_id": "6955e3f8832867f2535255f9", "name": "Tao Feng", "hidden": false}, {"_id": "6955e3f8832867f2535255fa", "name": "Tao He", "hidden": false}, {"_id": "6955e3f8832867f2535255fb", "name": "Jin Hu", "hidden": false}, {"_id": "6955e3f8832867f2535255fc", "name": "Yijie Hu", "hidden": false}, {"_id": "6955e3f8832867f2535255fd", "name": "Ziyu Jiang", "hidden": false}, {"_id": "6955e3f8832867f2535255fe", "name": "Cheng Li", "hidden": false}, {"_id": "6955e3f8832867f2535255ff", "name": "Xiang Li", "hidden": false}, {"_id": "6955e3f8832867f253525600", "name": "Jing Liang", "hidden": false}, {"_id": "6955e3f8832867f253525601", "name": "Chonghuan Liu", "hidden": false}, {"_id": "6955e3f8832867f253525602", "name": "ZhenDong Liu", "hidden": false}, {"_id": "6955e3f8832867f253525603", "name": "Haodong Mi", "hidden": false}, {"_id": "6955e3f8832867f253525604", "name": "Yanhu Mo", "hidden": false}, {"_id": "6955e3f8832867f253525605", "name": "Junjia Ni", "hidden": false}, {"_id": "6955e3f8832867f253525606", "name": "Shixin Pei", "hidden": false}, {"_id": "6955e3f8832867f253525607", "name": "Jingyu Shen", "hidden": false}, {"_id": "6955e3f8832867f253525608", "name": "XiaoShuai Song", "hidden": false}, {"_id": "6955e3f8832867f253525609", "name": "Cecilia Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560a", "name": "Chaofan Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560b", "name": "Kangyu Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560c", "name": "Pei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560d", "name": "Tao Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560e", "name": "Wei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560f", "name": "Ke Xiao", "hidden": false}, {"_id": "6955e3f8832867f253525610", "name": "Mingyu Xu", "hidden": false}, {"_id": "6955e3f8832867f253525611", "name": "Tiange Xu", "hidden": false}, {"_id": "6955e3f8832867f253525612", "name": "Nan Ya", "hidden": false}, {"_id": "6955e3f8832867f253525613", "name": "Siran Yang", "hidden": false}, {"_id": "6955e3f8832867f253525614", "name": "Jianan Ye", "hidden": false}, {"_id": "6955e3f8832867f253525615", "name": "Yaxing Zang", "hidden": false}, {"_id": "6955e3f8832867f253525616", "name": "Duo Zhang", "hidden": false}, {"_id": "6955e3f8832867f253525617", "name": "Junbo Zhang", "hidden": false}, {"_id": "6955e3f8832867f253525618", "name": "Boren Zheng", "hidden": false}, {"_id": "6955e3f8832867f253525619", "name": "Wanxi Deng", "hidden": false}, {"_id": "6955e3f8832867f25352561a", "name": "Ling Pan", "hidden": false}, {"_id": "6955e3f8832867f25352561b", "name": "Lin Qu", "hidden": false}, {"_id": "6955e3f8832867f25352561c", "name": "Wenbo Su", "hidden": false}, {"_id": "6955e3f8832867f25352561d", "name": "Jiamang Wang", "hidden": false}, {"_id": "6955e3f8832867f25352561e", "name": "Wei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352561f", "name": "Hu Wei", "hidden": false}, {"_id": "6955e3f8832867f253525620", "name": "Minggang Wu", "hidden": false}, {"_id": "6955e3f8832867f253525621", "name": "Cheng Yu", "hidden": false}, {"_id": "6955e3f8832867f253525622", "name": "Bing Zhao", "hidden": false}, {"_id": "6955e3f8832867f253525623", "name": "Zhicheng Zheng", "hidden": false}, {"_id": "6955e3f8832867f253525624", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-12-31T14:03:39.000Z", "submittedOnDailyAt": "2026-01-01T00:33:23.374Z", "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.", "upvotes": 33, "discussionId": "6955e3f8832867f253525625", "ai_summary": "The Agentic Learning Ecosystem (ALE) introduces a principled infrastructure for agent development, combining post-training optimization, sandbox environments, and policy alignment to enhance long-horizon training stability and performance in real-world tasks.", "ai_keywords": ["ROLL (post-training framework)", "ROCK (sandbox environment manager)", "iFlow CLI (agent framework)", "ROME (agentic model)", "data composition protocols", "Interaction-based Policy Alignment (IPA)", "semantic interaction chunks", "Terminal Bench Pro", "SWE-bench Verified"], "summary_zh": "<ul>\n  <li>Agentic crafting \u9700\u8981 LLM\uff08\u5927\u8bed\u8a00\u6a21\u578b\uff09\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u591a\u6b21\u64cd\u4f5c\uff0c\u901a\u8fc7\u884c\u52a8\u89c2\u5bdf\u7ed3\u679c\u5e76\u4e0d\u65ad\u6539\u8fdb\u3002</li>\n  <li>\u5f00\u6e90\u793e\u533a\u7f3a\u4e4f\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u5de5\u5177\u6765\u7b80\u5316\u667a\u80fd\u4f53\u7684\u5f00\u53d1\u3002</li>\n  <li>\u6211\u4eec\u4ecb\u7ecd\u4e86 Agentic Learning Ecosystem (ALE)\uff0c\u8fd9\u662f\u4e00\u4e2a\u4f18\u5316\u667a\u80fd\u4f53 LLM \u751f\u4ea7\u6d41\u7a0b\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u5305\u542b ROLL\u3001ROCK \u548c iFlow CLI \u4e09\u4e2a\u7ec4\u4ef6\u3002</li>\n  <li>\u6211\u4eec\u53d1\u5e03\u4e86 ROME\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e ALE \u7684\u5f00\u6e90\u667a\u80fd\u4f53\uff0c\u7ecf\u8fc7\u8d85\u8fc7\u4e00\u767e\u4e07\u4e2a\u8f68\u8ff9\u7684\u8bad\u7ec3\u3002</li>\n  <li>ROME \u5728\u5404\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86 ALE \u57fa\u7840\u8bbe\u65bd\u7684\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Agentic crafting helps large language models (LLMs) work in real-world situations by taking actions and learning from outcomes.</li>\n    <li>The open-source community lacks a comprehensive system for developing these agent LLMs efficiently.</li>\n    <li>We present the Agentic Learning Ecosystem (ALE), which includes three main tools: ROLL for optimizing model weights, ROCK for managing simulation environments, and iFlow CLI for improving context management.</li>\n    <li>We also introduce ROME, an open-source agent model trained on over one million experiences, which uses a new approach for training stability called Interaction-based Policy Alignment (IPA).</li>\n    <li>ROME has shown strong performance in various benchmarks, demonstrating the effectiveness of the ALE system.</li>\n</ul>"}, "publishedAt": "2025-12-31T09:03:39.000Z", "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem", "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24873.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 197}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24330", "authors": [{"_id": "69560bcd832867f2535256fc", "name": "Yong Xien Chng", "hidden": false}, {"_id": "69560bcd832867f2535256fd", "name": "Tao Hu", "hidden": false}, {"_id": "69560bcd832867f2535256fe", "name": "Wenwen Tong", "hidden": false}, {"_id": "69560bcd832867f2535256ff", "name": "Xueheng Li", "hidden": false}, {"_id": "69560bcd832867f253525700", "name": "Jiandong Chen", "hidden": false}, {"_id": "69560bcd832867f253525701", "name": "Haojia Yu", "hidden": false}, {"_id": "69560bcd832867f253525702", "name": "Jiefan Lu", "hidden": false}, {"_id": "69560bcd832867f253525703", "name": "Hewei Guo", "hidden": false}, {"_id": "69560bcd832867f253525704", "name": "Hanming Deng", "hidden": false}, {"_id": "69560bcd832867f253525705", "name": "Chengjun Xie", "hidden": false}, {"_id": "69560bcd832867f253525706", "name": "Gao Huang", "hidden": false}, {"_id": "69560bcd832867f253525707", "name": "Dahua Lin", "hidden": false}, {"_id": "69560bcd832867f253525708", "name": "Lewei Lu", "hidden": false}], "publishedAt": "2025-12-30T16:31:45.000Z", "submittedOnDailyAt": "2026-01-05T00:14:32.572Z", "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning", "submittedOnDailyBy": {"_id": "647d4f1236e109abce409c3b", "avatarUrl": "/avatars/d166f5f8be666e96b522a0a0effd21c4.svg", "isPro": false, "fullname": "Wenwen Tong", "user": "tongww", "type": "user"}, "summary": "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.", "upvotes": 30, "discussionId": "69560bcd832867f253525709", "githubRepo": "https://github.com/OpenSenseNova/SenseNova-MARS", "githubRepoAddedBy": "user", "githubStars": 24, "organization": {"_id": "64f0405f8a4cf3e5e6b38f9c", "name": "sensenova", "fullname": "SenseNova", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652d06833b5997ed71ce5c46/k66xcOMf4NVbMSFulUjHY.png"}, "summary_zh": "<ul>\n    <li>Vision-Language Models (VLMs) \u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\uff0c\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u63a8\u7406\u548c\u5355\u4e00\u5de5\u5177\u8c03\u7528\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u822c\u7684\u52a8\u6001\u5de5\u5177\u64cd\u4f5c\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86 SenseNova-MARS\uff0c\u4e00\u4e2a\u65b0\u7684\u591a\u6a21\u6001\u4ee3\u7406\u63a8\u7406\u548c\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u589e\u5f3a VLMs \u7684\u89c6\u89c9\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002</li>\n    <li>SenseNova-MARS \u52a8\u6001\u6574\u5408\u56fe\u50cf\u641c\u7d22\u3001\u6587\u672c\u641c\u7d22\u548c\u56fe\u50cf\u88c1\u526a\u5de5\u5177\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u7684\u89c6\u89c9\u7406\u89e3\u6311\u6218\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86 Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) \u7b97\u6cd5\uff0c\u6539\u5584\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u5de5\u5177\u8c03\u7528\u548c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSenseNova-MARS \u5728\u641c\u7d22\u548c\u7ec6\u7c92\u5ea6\u56fe\u50cf\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u5176\u4ed6\u5546\u4e1a\u6a21\u578b\uff0c\u5e76\u5c06\u5f00\u653e\u6240\u6709\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6570\u636e\u96c6\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-Language Models (VLMs) struggle with combining tool use and reasoning in complex visual tasks.</li>\n    <li>SenseNova-MARS is a new framework that enhances VLMs by allowing them to use tools like image search and cropping while reasoning.</li>\n    <li>It uses reinforcement learning and a new training algorithm to improve how VLMs perform tasks.</li>\n    <li>The HR-MMSearch benchmark is introduced to evaluate VLMs on complex visual tasks with high-resolution images and detailed questions.</li>\n    <li>SenseNova-MARS outperforms other models in tests, and the developers plan to share their code, models, and datasets for further study.</li>\n</ul>"}, "publishedAt": "2025-12-30T11:31:45.000Z", "title": "SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning", "summary": "While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model's ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-8B scores 67.84 on MMSearch and 41.64 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Flash and GPT-5. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24330.png", "numComments": 1, "submittedBy": {"_id": "647d4f1236e109abce409c3b", "avatarUrl": "/avatars/d166f5f8be666e96b522a0a0effd21c4.svg", "fullname": "Wenwen Tong", "name": "tongww", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "64f0405f8a4cf3e5e6b38f9c", "name": "sensenova", "fullname": "SenseNova", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/652d06833b5997ed71ce5c46/k66xcOMf4NVbMSFulUjHY.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\uff0c\u73b0\u6709\u7684\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u6548\u7387\u4f4e\u4e0b\uff0c\u96be\u4ee5\u590d\u73b0\u3002</li>\n    <li>\u63d0\u51fa\u4e86DataFlow\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u4f9b\u6a21\u5757\u5316\u3001\u53ef\u91cd\u7528\u548c\u53ef\u7ec4\u5408\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>DataFlow\u5305\u542b\u8fd1200\u4e2a\u53ef\u91cd\u7528\u7684\u64cd\u4f5c\u7b26\u548c\u516d\u79cd\u901a\u7528\u7ba1\u9053\uff0c\u6db5\u76d6\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u7b49\u9886\u57df\u3002</li>\n    <li>\u5f15\u5165DataFlow-Agent\uff0c\u53ef\u5c06\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u81ea\u52a8\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u7ba1\u9053\u3002</li>\n    <li>\u5728\u516d\u4e2a\u5e94\u7528\u6848\u4f8b\u4e2d\uff0cDataFlow\u663e\u8457\u63d0\u9ad8\u4e86\u4e0b\u6e38\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6570\u636e\u96c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The demand for high-quality data in Large Language Models (LLMs) is increasing, highlighting the need for better data preparation processes.</li>\n    <li>Current data preparation methods are often messy and lack effective structures, making them hard to reproduce and optimize.</li>\n    <li>DataFlow is a new framework that improves data preparation for LLMs by using organized and reusable data transformation methods.</li>\n    <li>It includes nearly 200 reusable components and supports various tasks like text processing, coding, and knowledge extraction.</li>\n    <li>DataFlow has shown to significantly enhance LLM performance across multiple tasks, outperforming existing datasets and providing a strong foundation for future AI development.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni\u662f\u4e00\u4e2a\u901a\u7528\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5982\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u5185\u5bb9\u3002</li>\n    <li>Kling-Omni\u80fd\u591f\u5904\u7406\u8fd9\u4e9b\u8f93\u5165\uff0c\u751f\u6210\u7535\u5f71\u8d28\u91cf\u548c\u667a\u80fd\u7684\u89c6\u9891\u5185\u5bb9\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5efa\u7acb\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u5e76\u91c7\u7528\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u63a8\u7406\u4f18\u5316\u3002</li>\n    <li>\u8bc4\u4f30\u8868\u660e\uff0cKling-Omni\u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u662f\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u91cd\u8981\u8fdb\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new framework that creates high-quality videos from various types of input like text, images, and existing videos.</li>\n    <li>It combines different video tasks (generation, editing, reasoning) into one system, unlike traditional methods that separate them.</li>\n    <li>The system processes multiple inputs into a single format to produce cinematic-quality videos.</li>\n    <li>Kling-Omni is built on a strong data foundation and uses advanced training and optimization methods for better performance.</li>\n    <li>It shows great abilities in generating content, editing based on reasoning, and following complex instructions, aiming to be a key tool for creating interactive simulations of the world.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.08765", "authors": [{"_id": "6938da63dfc35938ba129f3c", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:07.767Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3d", "name": "Yefei He", "hidden": false}, {"_id": "6938da63dfc35938ba129f3e", "user": {"_id": "62d812e143df7719860d05d1", "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg", "isPro": false, "fullname": "zhekai chen", "user": "Azily", "type": "user"}, "name": "Zhekai Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:00.513Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3f", "name": "Shiwei Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f40", "user": {"_id": "637ee45b2438d7485b8d8f6a", "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg", "isPro": false, "fullname": "Xiaogang Xu", "user": "xiaogang00", "type": "user"}, "name": "Xiaogang Xu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:51.241Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f41", "name": "Bin Xia", "hidden": false}, {"_id": "6938da63dfc35938ba129f42", "name": "Dingdong Wang", "hidden": false}, {"_id": "6938da63dfc35938ba129f43", "name": "Hongwei Yi", "hidden": false}, {"_id": "6938da63dfc35938ba129f44", "user": {"_id": "65d5ec74cd05bc1eaa125040", "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg", "isPro": false, "fullname": "Xihui Liu", "user": "XihuiLiu", "type": "user"}, "name": "Xihui Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:32.582Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f45", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:26.372Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f46", "name": "Yu Liu", "hidden": false}, {"_id": "6938da63dfc35938ba129f47", "name": "Yingya Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f48", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:10.566Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "publishedAt": "2025-12-09T16:13:55.000Z", "submittedOnDailyAt": "2025-12-10T00:20:18.797Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "upvotes": 94, "discussionId": "6938da64dfc35938ba129f49", "githubRepo": "https://github.com/ali-vilab/Wan-Move", "githubRepoAddedBy": "user", "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.", "ai_keywords": ["motion control", "video generative models", "dense point trajectories", "latent space", "spatiotemporal feature map", "motion guidance", "image-to-video model", "auxiliary motion encoders", "fine-tuning", "MoveBench", "motion annotations"], "githubStars": 197, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>Wan-Move \u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u73b0\u6709\u7684\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5\u63a7\u5236\u7cbe\u5ea6\u4f4e\u4e14\u6269\u5c55\u6027\u5dee\uff0c\u800c Wan-Move \u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u901a\u8fc7\u5bc6\u96c6\u70b9\u8f68\u8ff9\u8868\u793a\u7269\u4f53\u8fd0\u52a8\uff0cWan-Move \u5b9e\u73b0\u4e86\u5bf9\u573a\u666f\u7684\u7cbe\u7ec6\u63a7\u5236\u3002</li>\n    <li>\u8be5\u6846\u67b6\u53ef\u4ee5\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u7684\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\u4e2d\uff0c\u65e0\u9700\u66f4\u6539\u67b6\u6784\u3002</li>\n    <li>\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cWan-Move \u7684\u8fd0\u52a8\u53ef\u63a7\u6027\u4e0e\u5546\u4e1a\u8f6f\u4ef6 Kling 1.5 Pro \u7684 Motion Brush \u76f8\u5f53\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Wan-Move is a new framework that improves motion control in video generation models.</li>\n    <li>It allows for precise and high-quality control over video motion, overcoming issues found in existing methods.</li>\n    <li>The framework uses dense point trajectories to guide how objects move in the video scene.</li>\n    <li>Wan-Move can be easily integrated into existing video models without changing their architecture.</li>\n    <li>It has been tested extensively and shows better motion quality compared to other methods, with its code and data publicly available.</li>\n</ul>"}, "publishedAt": "2025-12-09T11:13:55.000Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 87, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 51, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7528\u4e8e\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u73b0\u6709\u7684\u5185\u5b58\u8bbe\u8ba1\u4e3b\u8981\u662f\u88ab\u52a8\u5b58\u50a8\uff0c\u4e0d\u80fd\u6709\u6548\u5904\u7406\u4e8b\u5b9e\u4e4b\u95f4\u7684\u9ad8\u9636\u5173\u8054\u3002</li>\n    <li>HGMem\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u5185\u5b58\u673a\u5236\uff0c\u8d85\u8d8a\u4e86\u7b80\u5355\u5b58\u50a8\uff0c\u652f\u6301\u590d\u6742\u63a8\u7406\u548c\u5168\u9762\u7406\u89e3\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u56fe\u8868\u793a\uff0c\u4fc3\u8fdb\u5185\u5b58\u4e2d\u66f4\u9ad8\u9636\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5f62\u6210\u7efc\u5408\u77e5\u8bc6\u7ed3\u6784\u3002</li>\n    <li>\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\uff0cHGMem\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6b65RAG\u7684\u8868\u73b0\uff0c\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps large language models (LLMs) with complex reasoning tasks.</li>\n    <li>Current memory systems mainly store isolated facts and do not effectively connect related information.</li>\n    <li>This lack of connection limits reasoning ability and understanding in longer contexts.</li>\n    <li>HGMem is a new memory system that uses a hypergraph to create dynamic connections between facts for better reasoning.</li>\n    <li>Tests show that HGMem improves performance in multi-step RAG tasks compared to existing systems.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.00393", "authors": [{"_id": "695b2297832867f253525d68", "user": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "name": "Yuxue Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:23.295Z", "hidden": false}, {"_id": "695b2297832867f253525d69", "user": {"_id": "649ecf9827145c4463240177", "avatarUrl": "/avatars/27696cf31790a3d58d8be2e0c983800e.svg", "isPro": false, "fullname": "Lue Fan", "user": "Abyssaledge", "type": "user"}, "name": "Lue Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T13:49:26.330Z", "hidden": false}, {"_id": "695b2297832867f253525d6a", "user": {"_id": "644cc2c36dfd5f8240d76a52", "avatarUrl": "/avatars/dcd9279af1c6d8535e48dc6e3e6511cd.svg", "isPro": false, "fullname": "Ziqi Shi", "user": "renshengjihe", "type": "user"}, "name": "Ziqi Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:21.077Z", "hidden": false}, {"_id": "695b2297832867f253525d6b", "name": "Junran Peng", "hidden": false}, {"_id": "695b2297832867f253525d6c", "name": "Feng Wang", "hidden": false}, {"_id": "695b2297832867f253525d6d", "name": "Zhaoxiang Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "publishedAt": "2026-01-01T17:07:30.000Z", "submittedOnDailyAt": "2026-01-05T02:49:46.994Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "submittedOnDailyBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "upvotes": 83, "discussionId": "695b2297832867f253525d6e", "projectPage": "https://neoverse-4d.github.io/", "githubRepo": "https://github.com/IamCreateAI/NeoVerse", "githubRepoAddedBy": "user", "ai_summary": "NeoVerse is a scalable 4D world model that enables pose-free reconstruction and novel-trajectory video generation from monocular videos with state-of-the-art performance.", "ai_keywords": ["4D world model", "4D reconstruction", "novel-trajectory video generation", "monocular videos", "pose-free", "feed-forward", "degradation pattern simulation"], "githubStars": 107, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86NeoVerse\uff0c\u4e00\u4e2a\u591a\u529f\u80fd\u76844D\u4e16\u754c\u6a21\u578b\u3002</li>\n    <li>\u89e3\u51b3\u4e86\u73b0\u67094D\u5efa\u6a21\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u9650\u5236\u3002</li>\n    <li>NeoVerse\u652f\u6301\u65e0\u59ff\u6001\u524d\u99884D\u91cd\u5efa\u548c\u5355\u76ee\u89c6\u9891\u751f\u6210\u3002</li>\n    <li>\u5177\u5907\u5728\u4e0d\u540c\u9886\u57df\u7684\u901a\u7528\u6027\u548c\u591a\u6837\u6027\u3002</li>\n    <li>\u5728\u91cd\u5efa\u548c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NeoVerse is a new model for creating 4D worlds, which includes 4D reconstruction and video generation.</li>\n    <li>Current methods of 4D modeling struggle with scalability due to expensive data and complicated training processes.</li>\n    <li>NeoVerse uses a simple and flexible approach that works with regular monocular videos.</li>\n    <li>It includes advanced techniques like pose-free reconstruction and online simulation for better performance.</li>\n    <li>NeoVerse achieves top results in benchmarks for 4D reconstruction and video generation.</li>\n</ul>"}, "publishedAt": "2026-01-01T12:07:30.000Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00393.png", "numComments": 1, "submittedBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "fullname": "Yuxue Yang", "name": "Yuppie1204", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24615", "authors": [{"_id": "69564d96832867f2535257af", "user": {"_id": "622b00a776c20fee5d14501b", "avatarUrl": "/avatars/e00496dda1e309548e7b5b437839bb65.svg", "isPro": false, "fullname": "Eason shi", "user": "Easonshi", "type": "user"}, "name": "Yuchen Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:50.111Z", "hidden": false}, {"_id": "69564d96832867f2535257b0", "user": {"_id": "66e258bdc70c02b46dfed6e3", "avatarUrl": "/avatars/ccc2d604616c018f45a268a610472cac.svg", "isPro": false, "fullname": "Yuzheng Cai", "user": "Ucreate", "type": "user"}, "name": "Yuzheng Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:50.884Z", "hidden": false}, {"_id": "69564d96832867f2535257b1", "name": "Siqi Cai", "hidden": false}, {"_id": "69564d96832867f2535257b2", "name": "Zihan Xu", "hidden": false}, {"_id": "69564d96832867f2535257b3", "user": {"_id": "64154bfa385a75d7790f80e8", "avatarUrl": "/avatars/9e22f54b5eb7c4ebedad99a9a92c4b6a.svg", "isPro": false, "fullname": "Lichao Chen", "user": "nth233", "type": "user"}, "name": "Lichao Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:46.825Z", "hidden": false}, {"_id": "69564d96832867f2535257b4", "user": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "isPro": false, "fullname": "Yulei Qin", "user": "yolay", "type": "user"}, "name": "Yulei Qin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:48.064Z", "hidden": false}, {"_id": "69564d96832867f2535257b5", "name": "Zhijian Zhou", "hidden": false}, {"_id": "69564d96832867f2535257b6", "name": "Xiang Fei", "hidden": false}, {"_id": "69564d96832867f2535257b7", "user": {"_id": "6604e43869c47cd78fdebd08", "avatarUrl": "/avatars/4c11f5e1aeae3c5eb213f6ec6d5bfe72.svg", "isPro": false, "fullname": "Qiu", "user": "ChaofanDFG", "type": "user"}, "name": "Chaofan Qiu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:48.910Z", "hidden": false}, {"_id": "69564d96832867f2535257b8", "user": {"_id": "637af0a7bdf7309aa6da1c36", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637af0a7bdf7309aa6da1c36/NHZ-09otVCfbpXVxm8f-e.png", "isPro": false, "fullname": "Xiaoyu Tan", "user": "WIlliam1900", "type": "user"}, "name": "Xiaoyu Tan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:52.763Z", "hidden": false}, {"_id": "69564d96832867f2535257b9", "name": "Gang Li", "hidden": false}, {"_id": "69564d96832867f2535257ba", "name": "Zongyi Li", "hidden": false}, {"_id": "69564d96832867f2535257bb", "name": "Haojia Lin", "hidden": false}, {"_id": "69564d96832867f2535257bc", "name": "Guocan Cai", "hidden": false}, {"_id": "69564d96832867f2535257bd", "name": "Yong Mao", "hidden": false}, {"_id": "69564d96832867f2535257be", "name": "Yunsheng Wu", "hidden": false}, {"_id": "69564d96832867f2535257bf", "name": "Ke Li", "hidden": false}, {"_id": "69564d96832867f2535257c0", "user": {"_id": "647401e50da364bd0d002f2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png", "isPro": false, "fullname": "XING SUN", "user": "tedsun", "type": "user"}, "name": "Xing Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:39.390Z", "hidden": false}], "publishedAt": "2025-12-31T04:17:36.000Z", "submittedOnDailyAt": "2026-01-05T00:21:56.456Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "submittedOnDailyBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "isPro": false, "fullname": "Ke Li", "user": "tristanli", "type": "user"}, "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "upvotes": 82, "discussionId": "69564d96832867f2535257c1", "projectPage": "https://tencentcloudadp.github.io/youtu-agent/", "githubRepo": "https://github.com/TencentCloudADP/youtu-agent", "githubRepoAddedBy": "user", "githubStars": 4095, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u6846\u67b6\u9762\u4e34\u9ad8\u914d\u7f6e\u6210\u672c\u548c\u9759\u6001\u80fd\u529b\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Youtu-Agent\uff0c\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u65e8\u5728\u81ea\u52a8\u751f\u6210\u548c\u6301\u7eed\u8fdb\u5316LLM\u4ee3\u7406\u3002</li>\n    <li>Youtu-Agent\u5177\u6709\u7ed3\u6784\u5316\u914d\u7f6e\u7cfb\u7edf\uff0c\u53ef\u7075\u6d3b\u91cd\u7528\u548c\u81ea\u52a8\u5408\u6210\u5de5\u5177\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u3002</li>\n    <li>\u5f15\u5165\u4e86\u5de5\u4f5c\u6d41\u6a21\u5f0f\u548c\u5143\u4ee3\u7406\u6a21\u5f0f\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u5de5\u5177\u4ee3\u7801\u3001\u63d0\u793a\u548c\u914d\u7f6e\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cYoutu-Agent\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u5e76\u4e14\u5728\u5de5\u5177\u5408\u6210\u548c\u6027\u80fd\u63d0\u5347\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Youtu-Agent is a new framework that helps create and improve Large Language Model (LLM) agents more easily and efficiently.</li>\n    <li>It addresses issues like high setup costs and the inability of agents to adapt to changes without expensive adjustments.</li>\n    <li>The framework allows for flexible use of tools and environments, with two ways to generate agents: one for standard tasks and another for complex tasks.</li>\n    <li>Youtu-Agent includes features that help agents learn from experience and improve their performance without needing to change their core settings.</li>\n    <li>Tests show Youtu-Agent performs very well, with significant improvements in tasks like question answering and coding, along with faster training times.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:17:36.000Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24615.png", "numComments": 1, "submittedBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "fullname": "Ke Li", "name": "tristanli", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u89c6\u89c9\u4e0a\u771f\u5b9e\u4e14\u65f6\u95f4\u4e0a\u8fde\u8d2f\u7684\u5185\u5bb9\uff0c\u4f46\u5176\u4f5c\u4e3a\u4e16\u754c\u6a21\u62df\u5668\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u5b83\u4eec\u662f\u5426\u6355\u83b7\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7ea6\u675f\u3002</li>\n    <li>\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u5982FVD\u4fa7\u91cd\u4e8e\u611f\u77e5\u8d28\u91cf\uff0c\u800c\u5ffd\u89c6\u63a8\u7406\u5931\u8d25\uff0c\u4f8b\u5982\u56e0\u679c\u5173\u7cfb\u3001\u7269\u7406\u548c\u5168\u5c40\u4e00\u81f4\u6027\u7684\u8fdd\u53cd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MMGR\uff08\u591a\u6a21\u6001\u751f\u6210\u63a8\u7406\u8bc4\u4f30\u57fa\u51c6\uff09\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8bc4\u4f30\uff1a\u7269\u7406\u3001\u903b\u8f91\u30013D\u7a7a\u95f4\u30012D\u7a7a\u95f4\u548c\u65f6\u95f4\u3002</li>\n    <li>MMGR\u5728\u4e09\u4e2a\u9886\u57df\u8fdb\u884c\u8bc4\u4f30\uff1a\u62bd\u8c61\u63a8\u7406\u3001\u5177\u8eab\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\uff0c\u663e\u793a\u51fa\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002</li>\n    <li>\u5f53\u524d\u6a21\u578b\u5b58\u5728\u4e00\u4e9b\u5173\u952e\u5c40\u9650\u6027\uff0c\u5305\u62ec\u5bf9\u611f\u77e5\u6570\u636e\u7684\u8fc7\u5ea6\u4f9d\u8d56\u3001\u5168\u7403\u72b6\u6001\u4e00\u81f4\u6027\u5f31\uff0c\u4ee5\u53ca\u5956\u52b1\u89c6\u89c9\u5408\u7406\u6027\u800c\u975e\u56e0\u679c\u6b63\u786e\u6027\u7684\u76ee\u6807\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video foundation models can create realistic content, but their accuracy as world simulators is in question.</li>\n    <li>Current evaluation methods focus on visual quality and miss issues like causality and physics violations.</li>\n    <li>The new MMGR framework assesses generative reasoning through five key abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>\n    <li>MMGR tests models in three areas: Abstract Reasoning, Embodied Navigation, and Physical Commonsense.</li>\n    <li>Results show significant performance gaps, especially in Abstract Reasoning, highlighting weaknesses in existing models.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u79d1\u5b66\u901a\u7528\u667a\u80fd\uff08SGI\uff09\u6307\u7684\u662f\u81ea\u4e3b\u5730\u6784\u601d\u3001\u8c03\u67e5\u548c\u63a8\u7406\u79d1\u5b66\u9886\u57df\u7684\u80fd\u529b\uff0c\u76ee\u524d\u5c1a\u7f3a\u4e4f\u4e00\u4e2a\u8fde\u8d2f\u7684\u6846\u67b6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b9e\u9645\u63a2\u7a76\u6a21\u578b\uff08PIM\uff09\u7684SGI\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u79d1\u5b66\u5bb6\u76f8\u5173\u7684\u4efb\u52a1\u8fdb\u884c\u64cd\u4f5c\u5316\uff1a\u6df1\u5ea6\u7814\u7a76\u3001\u521b\u610f\u751f\u6210\u3001\u5e72\u5b9e\u9a8c\u548c\u6e7f\u5b9e\u9a8c\u7684\u63a8\u7406\u3002</li>\n    <li>SGI-Bench\u5305\u542b\u8d85\u8fc71000\u4e2a\u4e13\u5bb6\u7b56\u5212\u7684\u8de8\u5b66\u79d1\u6837\u672c\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u6700\u65b0\u7684\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u663e\u793a\u5728\u6df1\u5ea6\u7814\u7a76\u4e2d\u51c6\u786e\u5339\u914d\u7387\u4f4e\uff0810-20%\uff09\uff0c\u521b\u610f\u7f3a\u4e4f\u53ef\u884c\u6027\u548c\u7ec6\u8282\uff0c\u5e72\u5b9e\u9a8c\u7684\u4ee3\u7801\u53ef\u6267\u884c\u6027\u9ad8\u4f46\u7ed3\u679c\u51c6\u786e\u6027\u4f4e\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\uff0c\u4f18\u5316\u63a8\u7406\u4e2d\u7684\u68c0\u7d22\u589e\u5f3a\u65b0\u9896\u6027\u5956\u52b1\uff0c\u63d0\u5347\u5047\u8bbe\u7684\u65b0\u9896\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is no clear framework for Scientific General Intelligence (SGI), which is the ability of AI to think and work like a scientist.</li>\n    <li>The authors propose a new definition of SGI based on a model that includes steps like deliberation and action, and they test it with four scientific tasks.</li>\n    <li>They created SGI-Bench, a benchmark with over 1,000 examples to evaluate AI capabilities in scientific research.</li>\n    <li>Results showed significant gaps in AI performance, such as poor accuracy in deep research and challenges in executing experiments correctly.</li>\n    <li>An approach called Test-Time Reinforcement Learning (TTRL) was introduced to improve AI's ability to generate new ideas during testing without needing answers to compare against.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20619", "authors": [{"_id": "694b614d746a34b55dd53d1a", "name": "Jianhong Bai", "hidden": false}, {"_id": "694b614d746a34b55dd53d1b", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "694b614d746a34b55dd53d1c", "name": "Xintao Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1d", "name": "Fu Xiao", "hidden": false}, {"_id": "694b614d746a34b55dd53d1e", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1f", "name": "Qinghe Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d20", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "694b614d746a34b55dd53d21", "name": "Menghan Xia", "hidden": false}, {"_id": "694b614d746a34b55dd53d22", "name": "Zuozhu Liu", "hidden": false}, {"_id": "694b614d746a34b55dd53d23", "name": "Haoji Hu", "hidden": false}, {"_id": "694b614d746a34b55dd53d24", "name": "Pengfei Wan", "hidden": false}, {"_id": "694b614d746a34b55dd53d25", "name": "Kun Gai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "publishedAt": "2025-12-23T18:59:56.000Z", "submittedOnDailyAt": "2025-12-24T01:20:51.117Z", "title": "SemanticGen: Video Generation in Semantic Space", "submittedOnDailyBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "isPro": false, "fullname": "Jianhong Bai", "user": "jianhongbai", "type": "user"}, "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "upvotes": 77, "discussionId": "694b614d746a34b55dd53d26", "projectPage": "https://jianhongbai.github.io/SemanticGen/", "ai_summary": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.", "ai_keywords": ["VAE space", "VAE decoder", "semantic space", "diffusion model", "semantic video features", "bi-directional attention"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u4f20\u7edf\u89c6\u9891\u751f\u6210\u6a21\u578b\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b66\u4e60\u89c6\u9891\u5206\u5e03\uff0c\u4f46\u751f\u6210\u957f\u89c6\u9891\u65f6\u901f\u5ea6\u6162\u4e14\u8ba1\u7b97\u5f00\u9500\u5927\u3002</li>\n    <li>SemanticGen\u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u751f\u6210\u7d27\u51d1\u7684\u8bed\u4e49\u89c6\u9891\u7279\u5f81\uff0c\u5b9a\u4e49\u89c6\u9891\u7684\u6574\u4f53\u5e03\u5c40\uff1b\u7b2c\u4e8c\u9636\u6bb5\u6839\u636e\u8fd9\u4e9b\u7279\u5f81\u751f\u6210VAE\u6f5c\u5728\u53d8\u91cf\uff0c\u8f93\u51fa\u6700\u7ec8\u89c6\u9891\u3002</li>\n    <li>\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u751f\u6210\u89c6\u9891\u6bd4\u5728VAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u66f4\u5feb\u6536\u655b\uff0c\u5e76\u4e14\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u8ba1\u7b97\u6548\u7387\u9ad8\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSemanticGen\u751f\u6210\u7684\u89c6\u9891\u8d28\u91cf\u9ad8\uff0c\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u548c\u57fa\u51c6\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>SemanticGen is a new method for generating videos that improves speed and reduces computational costs.</li>\n    <li>It starts by creating a basic layout of the video in a compact semantic space before adding detailed features.</li>\n    <li>There are two stages in the generation process: first, it creates semantic video features, and then it uses these features to generate the final video.</li>\n    <li>This approach leads to faster results compared to traditional methods using VAE latents.</li>\n    <li>Tests show that SemanticGen produces high-quality videos and outperforms existing techniques.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:56.000Z", "title": "SemanticGen: Video Generation in Semantic Space", "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png", "numComments": 2, "submittedBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "fullname": "Jianhong Bai", "name": "jianhongbai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.15431", "authors": [{"_id": "69437417542d62d58a7bf6c4", "name": "Haolong Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6c5", "name": "Jia Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c6", "name": "Xin Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c7", "name": "Yeqing Shen", "hidden": false}, {"_id": "69437417542d62d58a7bf6c8", "user": {"_id": "653614073f4248157d60ccdc", "avatarUrl": "/avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg", "isPro": false, "fullname": "mengziyang", "user": "zylate", "type": "user"}, "name": "Ziyang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:53.033Z", "hidden": false}, {"_id": "69437417542d62d58a7bf6c9", "name": "Zhimin Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6ca", "name": "Kaijun Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6cb", "name": "Jin Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf6cc", "name": "Lieyu Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6cd", "name": "Mi Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6ce", "name": "Shiliang Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6cf", "name": "Zhirui Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d0", "name": "Brian Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d1", "name": "Kang An", "hidden": false}, {"_id": "69437417542d62d58a7bf6d2", "name": "Chenyang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d3", "name": "Lei Lei", "hidden": false}, {"_id": "69437417542d62d58a7bf6d4", "name": "Mengmeng Duan", "hidden": false}, {"_id": "69437417542d62d58a7bf6d5", "name": "Danxun Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d6", "name": "Guodong Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d7", "name": "Hang Cheng", "hidden": false}, {"_id": "69437417542d62d58a7bf6d8", "name": "Hao Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d9", "name": "Jie Dong", "hidden": false}, {"_id": "69437417542d62d58a7bf6da", "name": "Junhao Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6db", "name": "Mei Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6dc", "name": "Renjie Yu", "hidden": false}, {"_id": "69437417542d62d58a7bf6dd", "name": "Shunshan Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6de", "name": "Xu Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf6df", "name": "Yiting Dai", "hidden": false}, {"_id": "69437417542d62d58a7bf6e0", "name": "Yineng Deng", "hidden": false}, {"_id": "69437417542d62d58a7bf6e1", "name": "Yingdan Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6e2", "name": "Zelin Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6e3", "name": "Wen Sun", "hidden": false}, {"_id": "69437417542d62d58a7bf6e4", "name": "Chengxu Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e5", "name": "Chunqin Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6e6", "name": "Dong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6e7", "name": "Fengqiong Xiao", "hidden": false}, {"_id": "69437417542d62d58a7bf6e8", "name": "Guanghao Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e9", "name": "Guopeng Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ea", "name": "Guozhen Peng", "hidden": false}, {"_id": "69437417542d62d58a7bf6eb", "name": "Hongbing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ec", "name": "Hang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ed", "name": "Hongming Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6ee", "name": "Jingjing Xie", "hidden": false}, {"_id": "69437417542d62d58a7bf6ef", "name": "Jianyong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6f0", "name": "Jingyang Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf6f1", "name": "Jiaju Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f2", "name": "Jiayu Yuan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f3", "name": "Jianpeng Yin", "hidden": false}, {"_id": "69437417542d62d58a7bf6f4", "name": "Kai Cao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f5", "name": "Liang Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f6", "name": "Liguo Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f7", "name": "Liying Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6f8", "name": "Mengqiang Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f9", "name": "Min Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fa", "name": "Manjiao Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fb", "name": "Mao Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf6fc", "name": "Mingxin Wan", "hidden": false}, {"_id": "69437417542d62d58a7bf6fd", "name": "Na Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6fe", "name": "Nan Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6ff", "name": "Ning Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf700", "name": "Peiyao Ma", "hidden": false}, {"_id": "69437417542d62d58a7bf701", "name": "Qingzhou Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf702", "name": "Qiao Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf703", "name": "Qinlin Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf704", "name": "Qiong Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf705", "name": "Qiongyao Li", "hidden": false}, {"_id": "69437417542d62d58a7bf706", "name": "Shangwu Zhong", "hidden": false}, {"_id": "69437417542d62d58a7bf707", "name": "Shuli Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf708", "name": "Shaofan Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf709", "name": "Shisi Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf70a", "name": "Shuang Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf70b", "name": "Xingbin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70c", "name": "Xiaojia Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70d", "name": "Xiaojie Hou", "hidden": false}, {"_id": "69437417542d62d58a7bf70e", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70f", "name": "Xuanti Feng", "hidden": false}, {"_id": "69437417542d62d58a7bf710", "name": "Xuedan Cai", "hidden": false}, {"_id": "69437417542d62d58a7bf711", "name": "Xuan Wen", "hidden": false}, {"_id": "69437417542d62d58a7bf712", "name": "Xianwei Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf713", "name": "Xin Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf714", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf715", "name": "Xin Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf716", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf717", "name": "Yukang Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf718", "name": "Yunfang Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf719", "name": "Yuqing Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf71a", "name": "Yixun Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf71b", "name": "Zejia Weng", "hidden": false}, {"_id": "69437417542d62d58a7bf71c", "name": "Zhonghao Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf71d", "name": "Zhiguo Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf71e", "name": "Zhuoyu Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf71f", "name": "Zheng Ge", "hidden": false}, {"_id": "69437417542d62d58a7bf720", "name": "Jing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf721", "name": "Yibo Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf722", "name": "Binxing Jiao", "hidden": false}, {"_id": "69437417542d62d58a7bf723", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf724", "name": "Daxin Jiang", "hidden": false}], "publishedAt": "2025-12-17T13:26:30.000Z", "submittedOnDailyAt": "2025-12-18T00:55:26.804Z", "title": "Step-GUI Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "upvotes": 77, "discussionId": "69437418542d62d58a7bf725", "projectPage": "https://opengelab.github.io/", "githubRepo": "https://github.com/stepfun-ai/gelab-zero", "githubRepoAddedBy": "user", "ai_summary": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.", "ai_keywords": ["multimodal large language models", "GUI automation", "self-evolving training pipeline", "Calibrated Step Reward System", "trajectory-level calibration", "Step-GUI", "GUI performance", "GUI-MCP", "Model Context Protocol", "AndroidWorld", "OSWorld", "ScreenShot-Pro", "AndroidDaily", "real-world mobile usage patterns", "hierarchical architecture", "low-level atomic operations", "high-level task delegation", "local specialist models", "high-privacy execution"], "githubStars": 1417, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6211\u6f14\u53d8\u7684\u8bad\u7ec3\u7ba1\u9053\uff0c\u901a\u8fc7\u6821\u51c6\u6b65\u9aa4\u5956\u52b1\u7cfb\u7edf\u63d0\u9ad8\u6570\u636e\u6807\u6ce8\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002</li>\n    <li>\u5f00\u53d1\u4e86Step-GUI\u6a21\u578b\uff0c\u5c55\u73b0\u51fa\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u6027\u80fd\u4e0a\u7684\u9886\u5148\u8868\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5e7f\u6cdb\u7684\u901a\u7528\u80fd\u529b\u3002</li>\n    <li>\u63d0\u51fa\u4e86GUI-MCP\uff0c\u8fd9\u662f\u9996\u4e2a\u7528\u4e8eGUI\u81ea\u52a8\u5316\u7684\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff0c\u517c\u987e\u9690\u79c1\u4fdd\u62a4\u548c\u8bbe\u5907\u95f4\u7684\u6807\u51c6\u5316\u63a5\u53e3\u3002</li>\n    <li>\u5f15\u5165\u4e86AndroidDaily\u57fa\u51c6\uff0c\u8bc4\u4f30\u4ee3\u7406\u5728\u771f\u5b9e\u65e5\u5e38\u4f7f\u7528\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u542b3146\u4e2a\u9759\u6001\u52a8\u4f5c\u548c235\u4e2a\u7aef\u5230\u7aef\u4efb\u52a1\u3002</li>\n    <li>\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u5b9e\u7528GUI\u4ee3\u7406\u7684\u5f00\u53d1\uff0c\u5c55\u793a\u4e86\u5728\u65e5\u5e38\u6570\u5b57\u4ea4\u4e92\u4e2d\u5b9e\u9645\u5e94\u7528\u7684\u5f3a\u5927\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New models for automating graphical user interfaces (GUIs) can now be trained more efficiently with high-quality data.</li>\n    <li>A new training system called the Calibrated Step Reward System improves the accuracy of model-generated data, achieving over 90% accuracy at a much lower cost.</li>\n    <li>The Step-GUI models show excellent performance on GUI tasks, achieving top scores on various benchmarks.</li>\n    <li>To ensure user privacy, a new Model Context Protocol (GUI-MCP) allows sensitive data to remain on devices while automating tasks.</li>\n    <li>A new benchmark, AndroidDaily, evaluates how well these models perform in real-life mobile scenarios, showing strong results for common tasks.</li>\n</ul>"}, "publishedAt": "2025-12-17T08:26:30.000Z", "title": "Step-GUI Technical Report", "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15431.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Jan 06, 2026";
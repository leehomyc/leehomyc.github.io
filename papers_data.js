window.trendingPapers = {
    "today": [{"paper": {"id": "2601.21558", "authors": [{"_id": "697c279ea67238fac88cc104", "user": {"_id": "621499d72be42a56cca7afad", "avatarUrl": "/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg", "isPro": false, "fullname": "TianXiaoyu", "user": "Emperorizzis", "type": "user"}, "name": "Xiaoyu Tian", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T17:00:01.968Z", "hidden": false}, {"_id": "697c279ea67238fac88cc105", "name": "Haotian Wang", "hidden": false}, {"_id": "697c279ea67238fac88cc106", "name": "Shuaiting Chen", "hidden": false}, {"_id": "697c279ea67238fac88cc107", "name": "Hao Zhou", "hidden": false}, {"_id": "697c279ea67238fac88cc108", "name": "Kaichi Yu", "hidden": false}, {"_id": "697c279ea67238fac88cc109", "name": "Yudian Zhang", "hidden": false}, {"_id": "697c279ea67238fac88cc10a", "user": {"_id": "690189db2b5a1d242306b77f", "avatarUrl": "/avatars/b0297c57395029da2725758671952bb6.svg", "isPro": false, "fullname": "jade ouyang", "user": "jade0101", "type": "user"}, "name": "Jade Ouyang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T16:59:59.965Z", "hidden": false}, {"_id": "697c279ea67238fac88cc10b", "name": "Junxi Yin", "hidden": false}, {"_id": "697c279ea67238fac88cc10c", "name": "Jiong Chen", "hidden": false}, {"_id": "697c279ea67238fac88cc10d", "name": "Baoyan Guo", "hidden": false}, {"_id": "697c279ea67238fac88cc10e", "name": "Lei Zhang", "hidden": false}, {"_id": "697c279ea67238fac88cc10f", "name": "Junjie Tao", "hidden": false}, {"_id": "697c279ea67238fac88cc110", "name": "Yuansheng Song", "hidden": false}, {"_id": "697c279ea67238fac88cc111", "name": "Ming Cui", "hidden": false}, {"_id": "697c279ea67238fac88cc112", "name": "Chengwei Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/621499d72be42a56cca7afad/C3KRF5SVsqevuZy7nGJfm.png"], "publishedAt": "2026-01-29T11:22:23.000Z", "submittedOnDailyAt": "2026-02-02T00:08:03.322Z", "title": "ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas", "submittedOnDailyBy": {"_id": "621499d72be42a56cca7afad", "avatarUrl": "/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg", "isPro": false, "fullname": "TianXiaoyu", "user": "Emperorizzis", "type": "user"}, "summary": "Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.", "upvotes": 45, "discussionId": "697c279fa67238fac88cc113", "githubRepo": "https://github.com/LianjiaTech/astra", "githubRepoAddedBy": "user", "ai_summary": "ASTRA is an automated framework that trains tool-augmented language models using synthetic data and verifiable reinforcement learning to improve multi-step decision-making capabilities.", "ai_keywords": ["tool-call graphs", "trajectory-level rewards", "supervised fine-tuning", "reinforcement learning", "agent training", "multi-step decision making", "verifiable environments", "compositional topology", "semantic reasoning", "tool-augmented language models"], "githubStars": 84, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u6b65\u9aa4\u51b3\u7b56\u4e2d\u9010\u6e10\u88ab\u7528\u4f5c\u589e\u5f3a\u5de5\u5177\u7684\u667a\u80fd\u4f53\uff0c\u4f46\u8bad\u7ec3\u8fd9\u4e9b\u667a\u80fd\u4f53\u4ecd\u7136\u5b58\u5728\u6311\u6218\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u4eba\u5de5\u5e72\u9884\uff0c\u4f9d\u8d56\u4e0d\u53ef\u9a8c\u8bc1\u7684\u6a21\u62df\u73af\u5883\uff0c\u5e76\u4e14\u96be\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u7684\u957f\u671f\u5b66\u4e60\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86ASTRA\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u81ea\u52a8\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u53ef\u6269\u5c55\u6570\u636e\u5408\u6210\u548c\u53ef\u9a8c\u8bc1\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u589e\u5f3a\u5de5\u5177\u7684\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u3002</li>\n    <li>ASTRA\u7ed3\u5408\u4e86\u4e24\u5927\u7ec4\u4ef6\uff1a\u4e00\u662f\u5229\u7528\u5de5\u5177\u8c03\u7528\u56fe\u7684\u9759\u6001\u62d3\u6251\u5408\u6210\u591a\u6837\u5316\u7684\u8f68\u8ff9\uff0c\u4e8c\u662f\u73af\u5883\u5408\u6210\u6846\u67b6\u5c06\u95ee\u9898-\u7b54\u6848\u8f68\u8ff9\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u548c\u53ef\u9a8c\u8bc1\u7684\u73af\u5883\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cASTRA\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63a5\u8fd1\u5c01\u95ed\u6e90\u7cfb\u7edf\uff0c\u540c\u65f6\u4fdd\u6301\u6838\u5fc3\u63a8\u7406\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models are used for complex decision making but training them to use tools effectively is difficult.</li>\n    <li>Current methods need manual help, work in unverified settings, and struggle with long-term learning.</li>\n    <li>ASTRA is a new automated framework that helps train these models using synthesized data and verified reinforcement learning.</li>\n    <li>It combines two parts: one that creates diverse training scenarios and another that builds environments for testing reasoning skills.</li>\n    <li>ASTRA shows strong performance in benchmarks and offers resources for others to use at their GitHub page.</li>\n</ul>"}, "publishedAt": "2026-01-29T06:22:23.000Z", "title": "ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas", "summary": "Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/621499d72be42a56cca7afad/C3KRF5SVsqevuZy7nGJfm.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21558.png", "numComments": 3, "submittedBy": {"_id": "621499d72be42a56cca7afad", "avatarUrl": "/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg", "fullname": "TianXiaoyu", "name": "Emperorizzis", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.22813", "authors": [{"_id": "6980c289020e48d648c5d3ca", "user": {"_id": "623753b5eddd7763adc9346a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/-iLaQXQ1FOuJr0sfqulBA.jpeg", "isPro": false, "fullname": "Andrei Panferov", "user": "BlackSamorez", "type": "user"}, "name": "Andrei Panferov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T16:52:03.673Z", "hidden": false}, {"_id": "6980c289020e48d648c5d3cb", "name": "Erik Schultheis", "hidden": false}, {"_id": "6980c289020e48d648c5d3cc", "user": {"_id": "632a2e325f2ff1958c0103be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632a2e325f2ff1958c0103be/Tb0ql9e4LcaFktTK1hzqe.jpeg", "isPro": false, "fullname": "Rush Tabesh", "user": "soroushtabesh", "type": "user"}, "name": "Soroush Tabesh", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T15:43:33.368Z", "hidden": false}, {"_id": "6980c289020e48d648c5d3cd", "name": "Dan Alistarh", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/623753b5eddd7763adc9346a/ABDboVhzjTdRyUdiu_quW.png"], "publishedAt": "2026-01-30T10:39:11.000Z", "submittedOnDailyAt": "2026-02-02T13:00:36.911Z", "title": "Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation", "submittedOnDailyBy": {"_id": "623753b5eddd7763adc9346a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/-iLaQXQ1FOuJr0sfqulBA.jpeg", "isPro": false, "fullname": "Andrei Panferov", "user": "BlackSamorez", "type": "user"}, "summary": "The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .", "upvotes": 41, "discussionId": "6980c289020e48d648c5d3ce", "githubRepo": "https://github.com/IST-DASLab/Quartet-II", "githubRepoAddedBy": "user", "ai_summary": "Quantized training method Quartet II improves NVFP4 format utilization for large language model pre-training through enhanced gradient estimation and faster GPU execution.", "ai_keywords": ["NVFP4", "stochastic rounding", "quantized training", "micro-scaled formats", "MS-EDEN", "Quartet II", "linear layers", "gradient estimation", "matrix multiplications", "LLM training", "NVIDIA Blackwell GPUs"], "githubStars": 5, "organization": {"_id": "64d0ffde9cff738203a50e9b", "name": "ISTA-DASLab", "fullname": " IST Austria Distributed Algorithms and Systems Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/628e0ce4e53bbd334577fcb0/TRPtgtSavYjDJOK3S1I8M.png"}, "summary_zh": "<ul>\n    <li>NVIDIA\u7684Blackwell GPU\u652f\u6301NVFP4\u4f4e\u7cbe\u5ea6\u683c\u5f0f\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21\u6a21\u578b\u7684\u5b8c\u5168\u91cf\u5316\u9884\u8bad\u7ec3\u3002</li>\n    <li>\u73b0\u6709\u7684\u91cf\u5316\u8bad\u7ec3\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u4e0a\u6709\u6240\u727a\u7272\uff0c\u4f7f\u7528\u968f\u673a\u820d\u5165\uff08SR\uff09\u5bfc\u81f4\u4e0e\u6807\u51c6FP16\u548cFP8\u8bad\u7ec3\u76f8\u6bd4\u51c6\u786e\u6027\u4e0b\u964d\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u504f\u91cf\u5316\u65b9\u6cd5MS-EDEN\uff0c\u91cf\u5316\u8bef\u5dee\u6bd4SR\u4f4e\u8d85\u8fc72\u500d\u3002</li>\n    <li>\u6211\u4eec\u5c06MS-EDEN\u6574\u5408\u8fdb\u4e00\u79cd\u65b0\u7684\u5b8c\u5168NVFP4\u91cf\u5316\u65b9\u6848Quartet II\uff0c\u63d0\u5347\u4e86\u77e9\u9635\u4e58\u6cd5\u7684\u68af\u5ea6\u4f30\u8ba1\u51c6\u786e\u6027\u3002</li>\n    <li>Quartet II\u5728\u8bad\u7ec3\u6700\u5927\u4e3a1.9B\u53c2\u6570\u7684LLM\u65f6\uff0c\u63d0\u4f9b\u4e86\u6bd4BF16\u5feb4.2\u500d\u7684\u6267\u884c\u901f\u5ea6\uff0c\u4ee3\u7801\u53ef\u5728GitHub\u4e0a\u83b7\u53d6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NVIDIA Blackwell GPUs support a new lower-precision format called NVFP4, which allows for fully quantized pre-training of large models.</li>\n    <li>Current methods for quantized training lose some accuracy due to using stochastic rounding for gradient estimation.</li>\n    <li>This paper introduces a new method called MS-EDEN, which reduces quantization error significantly compared to stochastic rounding.</li>\n    <li>The authors present a new quantization scheme called Quartet II that improves gradient estimation for matrix multiplications in NVFP4.</li>\n    <li>Quartet II shows better performance on large language model training and offers up to 4.2x speedup on NVIDIA Blackwell GPUs.</li>\n</ul>"}, "publishedAt": "2026-01-30T05:39:11.000Z", "title": "Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation", "summary": "The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/623753b5eddd7763adc9346a/ABDboVhzjTdRyUdiu_quW.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22813.png", "numComments": 1, "submittedBy": {"_id": "623753b5eddd7763adc9346a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/-iLaQXQ1FOuJr0sfqulBA.jpeg", "fullname": "Andrei Panferov", "name": "BlackSamorez", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 41, "isUserFollowing": false}, "organization": {"_id": "64d0ffde9cff738203a50e9b", "name": "ISTA-DASLab", "fullname": " IST Austria Distributed Algorithms and Systems Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/628e0ce4e53bbd334577fcb0/TRPtgtSavYjDJOK3S1I8M.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.22975", "authors": [{"_id": "6980a86d6676f9332270678f", "name": "Ximing Lu", "hidden": false}, {"_id": "6980a86d6676f93322706790", "name": "David Acuna", "hidden": false}, {"_id": "6980a86d6676f93322706791", "name": "Jaehun Jung", "hidden": false}, {"_id": "6980a86d6676f93322706792", "name": "Jian Hu", "hidden": false}, {"_id": "6980a86d6676f93322706793", "user": {"_id": "64bce15bafd1e46c5504ad38", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png", "isPro": false, "fullname": "Di Zhang", "user": "di-zhang-fdu", "type": "user"}, "name": "Di Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T16:52:05.846Z", "hidden": false}, {"_id": "6980a86d6676f93322706794", "name": "Shizhe Diao", "hidden": false}, {"_id": "6980a86d6676f93322706795", "name": "Yunheng Zou", "hidden": false}, {"_id": "6980a86d6676f93322706796", "name": "Shaokun Zhang", "hidden": false}, {"_id": "6980a86d6676f93322706797", "name": "Brandon Cui", "hidden": false}, {"_id": "6980a86d6676f93322706798", "name": "Mingjie Liu", "hidden": false}, {"_id": "6980a86d6676f93322706799", "name": "Hyunwoo Kim", "hidden": false}, {"_id": "6980a86d6676f9332270679a", "name": "Prithviraj Ammanabrolu", "hidden": false}, {"_id": "6980a86d6676f9332270679b", "name": "Jan Kautz", "hidden": false}, {"_id": "6980a86d6676f9332270679c", "name": "Yi Dong", "hidden": false}, {"_id": "6980a86d6676f9332270679d", "name": "Yejin Choi", "hidden": false}], "publishedAt": "2026-01-30T13:39:11.000Z", "submittedOnDailyAt": "2026-02-02T11:10:15.313Z", "title": "Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text", "submittedOnDailyBy": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a cornerstone for unlocking complex reasoning in Large Language Models (LLMs). Yet, scaling up RL is bottlenecked by limited existing verifiable data, where improvements increasingly saturate over prolonged training. To overcome this, we propose Golden Goose, a simple trick to synthesize unlimited RLVR tasks from unverifiable internet text by constructing a multiple-choice question-answering version of the fill-in-the-middle task. Given a source text, we prompt an LLM to identify and mask key reasoning steps, then generate a set of diverse, plausible distractors. This enables us to leverage reasoning-rich unverifiable corpora typically excluded from prior RLVR data construction (e.g., science textbooks) to synthesize GooseReason-0.7M, a large-scale RLVR dataset with over 0.7 million tasks spanning mathematics, programming, and general scientific domains. Empirically, GooseReason effectively revives models saturated on existing RLVR data, yielding robust, sustained gains under continuous RL and achieving new state-of-the-art results for 1.5B and 4B-Instruct models across 15 diverse benchmarks. Finally, we deploy Golden Goose in a real-world setting, synthesizing RLVR tasks from raw FineWeb scrapes for the cybersecurity domain, where no prior RLVR data exists. Training Qwen3-4B-Instruct on the resulting data GooseReason-Cyber sets a new state-of-the-art in cybersecurity, surpassing a 7B domain-specialized model with extensive domain-specific pre-training and post-training. This highlights the potential of automatically scaling up RLVR data by exploiting abundant, reasoning-rich, unverifiable internet text.", "upvotes": 34, "discussionId": "6980a86d6676f9332270679e", "ai_summary": "Golden Goose synthesizes unlimited RLVR tasks from unverifiable internet text by creating multiple-choice question-answering versions of fill-in-the-middle tasks, enabling large-scale training and achieving state-of-the-art results in cybersecurity and other domains.", "ai_keywords": ["Reinforcement Learning with Verifiable Rewards", "Large Language Models", "fill-in-the-middle task", "multiple-choice question-answering", "unverifiable corpora", "GooseReason-0.7M", "continuous RL", "Qwen3-4B-Instruct", "GooseReason-Cyber"], "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u9ec4\u91d1\u9e45\uff08Golden Goose\uff09\u65b9\u6cd5\uff0c\u4ece\u4e0d\u53ef\u9a8c\u8bc1\u7684\u4e92\u8054\u7f51\u6587\u672c\u4e2d\u5408\u6210\u65e0\u9650\u7684\u5f3a\u5316\u5b66\u4e60\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u4efb\u52a1\u3002</li>\n    <li>\u901a\u8fc7\u591a\u9879\u9009\u62e9\u9898\u7684\u65b9\u5f0f\uff0c\u8bc6\u522b\u548c\u9690\u85cf\u5173\u952e\u63a8\u7406\u6b65\u9aa4\uff0c\u5e76\u751f\u6210\u591a\u6837\u7684\u5e72\u6270\u9879\u3002</li>\n    <li>\u5408\u6210\u4e86\u4e00\u4e2a\u5927\u578bRLVR\u6570\u636e\u96c6GooseReason\uff0c\u5305\u542b\u8d85\u8fc770\u4e07\u9053\u4efb\u52a1\uff0c\u6db5\u76d6\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u79d1\u5b66\u9886\u57df\u3002</li>\n    <li>GooseReason\u6709\u6548\u63d0\u5347\u4e86\u73b0\u6709\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002</li>\n    <li>\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\uff0c\u5229\u7528\u9ec4\u91d1\u9e45\u5408\u6210\u4e86RLVR\u4efb\u52a1\uff0c\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u8d85\u8fc7\u4e86\u4e00\u4e2a\u7ecf\u8fc7\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\u76847B\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement Learning with Verifiable Rewards (RLVR) helps Large Language Models (LLMs) improve their reasoning skills, but has limited data for training.</li>\n    <li>The Golden Goose method creates unlimited RLVR tasks by using internet text to build multiple-choice questions, which helps overcome data limitations.</li>\n    <li>GooseReason-0.7M is a new dataset with over 0.7 million reasoning tasks from subjects like math and science, made possible by this method.</li>\n    <li>Using GooseReason, models that were previously stuck on existing data showed significant improvements in performance across various benchmarks.</li>\n    <li>Golden Goose was also applied in cybersecurity, creating a new dataset that outperformed a larger, specialized model, demonstrating the effectiveness of using online text for RLVR tasks.</li>\n</ul>"}, "publishedAt": "2026-01-30T08:39:11.000Z", "title": "Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a cornerstone for unlocking complex reasoning in Large Language Models (LLMs). Yet, scaling up RL is bottlenecked by limited existing verifiable data, where improvements increasingly saturate over prolonged training. To overcome this, we propose Golden Goose, a simple trick to synthesize unlimited RLVR tasks from unverifiable internet text by constructing a multiple-choice question-answering version of the fill-in-the-middle task. Given a source text, we prompt an LLM to identify and mask key reasoning steps, then generate a set of diverse, plausible distractors. This enables us to leverage reasoning-rich unverifiable corpora typically excluded from prior RLVR data construction (e.g., science textbooks) to synthesize GooseReason-0.7M, a large-scale RLVR dataset with over 0.7 million tasks spanning mathematics, programming, and general scientific domains. Empirically, GooseReason effectively revives models saturated on existing RLVR data, yielding robust, sustained gains under continuous RL and achieving new state-of-the-art results for 1.5B and 4B-Instruct models across 15 diverse benchmarks. Finally, we deploy Golden Goose in a real-world setting, synthesizing RLVR tasks from raw FineWeb scrapes for the cybersecurity domain, where no prior RLVR data exists. Training Qwen3-4B-Instruct on the resulting data GooseReason-Cyber sets a new state-of-the-art in cybersecurity, surpassing a 7B domain-specialized model with extensive domain-specific pre-training and post-training. This highlights the potential of automatically scaling up RLVR data by exploiting abundant, reasoning-rich, unverifiable internet text.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22975.png", "numComments": 2, "submittedBy": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "fullname": "Ximing Lu", "name": "Ximing", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.23143", "authors": [{"_id": "69801ae26676f9332270659b", "user": {"_id": "64ad5f59b7e4b2c1ce47eb43", "avatarUrl": "/avatars/1f13ebe21a90d8c99920aa2c8cd9ac45.svg", "isPro": false, "fullname": "Seanie Lee", "user": "Seanie-lee", "type": "user"}, "name": "Seanie Lee", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T16:52:56.623Z", "hidden": false}, {"_id": "69801ae26676f9332270659c", "user": {"_id": "638716c14e00d7fc0902fef4", "avatarUrl": "/avatars/5fa8152f8c0e4e600d1a64802c3e0103.svg", "isPro": false, "fullname": "Sangwoo Park", "user": "Sangsang", "type": "user"}, "name": "Sangwoo Park", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T16:52:59.260Z", "hidden": false}, {"_id": "69801ae26676f9332270659d", "user": {"_id": "64cfa0b9749587dbe01d0079", "avatarUrl": "/avatars/93ca0a1d9c5578d052c5af0d4d1a0252.svg", "isPro": false, "fullname": "Yumin Choi", "user": "YuminChoi", "type": "user"}, "name": "Yumin Choi", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T16:52:54.423Z", "hidden": false}, {"_id": "69801ae26676f9332270659e", "name": "Gyeongman Kim", "hidden": false}, {"_id": "69801ae26676f9332270659f", "name": "Minki Kang", "hidden": false}, {"_id": "69801ae26676f933227065a0", "name": "Jihun Yun", "hidden": false}, {"_id": "69801ae26676f933227065a1", "name": "Dongmin Park", "hidden": false}, {"_id": "69801ae26676f933227065a2", "name": "Jongho Park", "hidden": false}, {"_id": "69801ae26676f933227065a3", "name": "Sung Ju Hwang", "hidden": false}], "publishedAt": "2026-01-30T16:31:02.000Z", "submittedOnDailyAt": "2026-02-02T02:09:13.735Z", "title": "THINKSAFE: Self-Generated Safety Alignment for Reasoning Models", "submittedOnDailyBy": {"_id": "638716c14e00d7fc0902fef4", "avatarUrl": "/avatars/5fa8152f8c0e4e600d1a64802c3e0103.svg", "isPro": false, "fullname": "Sangwoo Park", "user": "Sangsang", "type": "user"}, "summary": "Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.", "upvotes": 32, "discussionId": "69801ae26676f933227065a4", "githubRepo": "https://github.com/seanie12/ThinkSafe.git", "githubRepoAddedBy": "user", "ai_summary": "ThinkSafe is a self-aligned framework that enhances safety in large reasoning models through lightweight refusal steering and fine-tuning on self-generated responses, maintaining reasoning performance while reducing computational costs.", "ai_keywords": ["reinforcement learning", "chain-of-thought reasoning", "external teacher distillation", "distributional discrepancy", "lightweight refusal steering", "self-generated alignment", "safety alignment", "reasoning proficiency", "computational cost"], "githubStars": 3, "summary_zh": "<ul>\n    <li>\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5f88\u597d\u7684\u6210\u7ee9\uff0c\u4f46\u8fc7\u5ea6\u4f18\u5316\u4f7f\u5176\u5bb9\u6613\u53d7\u5230\u6709\u5bb3\u63d0\u793a\u7684\u5f71\u54cd\u3002</li>\n    <li>\u4e3a\u4e86\u63d0\u9ad8\u5b89\u5168\u6027\uff0c\u6700\u8fd1\u7684\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u6559\u5e08\u84b8\u998f\uff0c\u4f46\u8fd9\u4f1a\u5bfc\u81f4\u63a8\u7406\u80fd\u529b\u4e0b\u964d\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86ThinkSafe\uff0c\u4e00\u4e2a\u81ea\u6211\u751f\u6210\u7684\u5bf9\u9f50\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u6ca1\u6709\u5916\u90e8\u6559\u5e08\u7684\u60c5\u51b5\u4e0b\u6062\u590d\u5b89\u5168\u6027\u3002</li>\n    <li>ThinkSafe\u901a\u8fc7\u8f7b\u91cf\u7ea7\u62d2\u7edd\u5f15\u5bfc\uff0c\u5e2e\u52a9\u6a21\u578b\u751f\u6210\u5b89\u5168\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4ece\u800c\u6709\u6548\u5730\u91cd\u65b0\u5bf9\u9f50\u6a21\u578b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cThinkSafe\u5728\u4fdd\u6301\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\uff0c\u5e76\u4e14\u8ba1\u7b97\u6210\u672c\u5927\u5927\u964d\u4f4e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large reasoning models use reinforcement learning to improve their reasoning abilities but can become vulnerable to harmful prompts.</li>\n    <li>Recent methods to improve safety rely on external teachers, which can hurt the model's original reasoning skills.</li>\n    <li>ThinkSafe is a new framework that enhances safety without needing external teachers.</li>\n    <li>It uses a technique called lightweight refusal steering to help models recognize harmful content while maintaining their reasoning abilities.</li>\n    <li>Tests show that ThinkSafe improves safety and keeps reasoning skills strong, all while being more cost-effective.</li>\n</ul>"}, "publishedAt": "2026-01-30T11:31:02.000Z", "title": "THINKSAFE: Self-Generated Safety Alignment for Reasoning Models", "summary": "Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23143.png", "numComments": 2, "submittedBy": {"_id": "638716c14e00d7fc0902fef4", "avatarUrl": "/avatars/5fa8152f8c0e4e600d1a64802c3e0103.svg", "fullname": "Sangwoo Park", "name": "Sangsang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.22628", "authors": [{"_id": "6980103b6676f9332270654a", "user": {"_id": "681ab9d3d7dbd87287875667", "avatarUrl": "/avatars/1a2785d7a250c4988b1c1c5cc78e53fc.svg", "isPro": false, "fullname": "ChengyiYang", "user": "ChengyiYang", "type": "user"}, "name": "Chengyi Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T16:56:38.598Z", "hidden": false}, {"_id": "6980103b6676f9332270654b", "name": "Zhishang Xiang", "hidden": false}, {"_id": "6980103b6676f9332270654c", "name": "Yunbo Tang", "hidden": false}, {"_id": "6980103b6676f9332270654d", "name": "Zongpei Teng", "hidden": false}, {"_id": "6980103b6676f9332270654e", "name": "Chengsong Huang", "hidden": false}, {"_id": "6980103b6676f9332270654f", "name": "Fei Long", "hidden": false}, {"_id": "6980103b6676f93322706550", "name": "Yuhan Liu", "hidden": false}, {"_id": "6980103b6676f93322706551", "name": "Jinsong Su", "hidden": false}], "publishedAt": "2026-01-30T06:38:02.000Z", "submittedOnDailyAt": "2026-02-02T00:37:03.238Z", "title": "TTCS: Test-Time Curriculum Synthesis for Self-Evolving", "submittedOnDailyBy": {"_id": "62ea79dd01ed9b0e8f61ccd3", "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg", "isPro": false, "fullname": "Chengsong Huang", "user": "ChengsongHuang", "type": "user"}, "summary": "Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS.", "upvotes": 24, "discussionId": "6980103b6676f93322706552", "githubRepo": "https://github.com/XMUDeepLIT/TTCS", "githubRepoAddedBy": "user", "ai_summary": "TTCS is a co-evolving test-time training framework that enhances LLM reasoning abilities by iteratively generating challenging question variants and updating a reasoning solver through self-consistency rewards.", "ai_keywords": ["test-time training", "large language models", "pseudo-labels", "self-consistency rewards", "question synthesizer", "reasoning solver", "iterative optimization", "test-time curricula", "mathematical benchmarks", "general-domain tasks"], "githubStars": 19, "summary_zh": "<ul>\n    <li>\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff08TTCS\uff09\u662f\u4e00\u79cd\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u53ea\u4f7f\u7528\u6d4b\u8bd5\u95ee\u9898\u8fdb\u884c\u8c03\u6574\u3002</li>\n    <li>\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u63a8\u7406\u95ee\u9898\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u4e3a\u539f\u59cb\u6d4b\u8bd5\u95ee\u9898\u592a\u96be\uff0c\u96be\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u4f2a\u6807\u7b7e\u3002</li>\n    <li>TTCS\u901a\u8fc7\u4e24\u4e2a\u653f\u7b56\u5171\u540c\u8fdb\u5316\uff1a\u95ee\u9898\u5408\u6210\u5668\u548c\u63a8\u7406\u6c42\u89e3\u5668\uff0c\u9010\u6b65\u4f18\u5316\u4ee5\u9002\u5e94\u6a21\u578b\u80fd\u529b\u3002</li>\n    <li>\u95ee\u9898\u5408\u6210\u5668\u6839\u636e\u6d4b\u8bd5\u95ee\u9898\u751f\u6210\u66f4\u5177\u6311\u6218\u6027\u7684\u53d8\u4f53\uff0c\u800c\u63a8\u7406\u6c42\u89e3\u5668\u6839\u636e\u591a\u6b21\u91c7\u6837\u7684\u54cd\u5e94\u8fdb\u884c\u81ea\u6211\u66f4\u65b0\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cTTCS\u5728\u590d\u6742\u6570\u5b66\u57fa\u51c6\u4e0a\u6709\u6548\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u80fd\u8f6c\u79fb\u5230\u4e0d\u540c\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\u4e2d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Test-Time Training (TTT) helps improve the reasoning skills of large language models (LLMs) by adapting them to test questions.</li>\n    <li>Current TTT methods face challenges with hard reasoning tasks and small test sets, leading to poor results and instability.</li>\n    <li>We introduced TTCS, a framework that creates two evolving components: a question synthesizer and a reasoning solver.</li>\n    <li>The synthesizer generates increasingly difficult questions based on the test questions, while the solver improves using feedback from its own answers.</li>\n    <li>TTCS has been shown to enhance reasoning abilities on tough math problems and performs well on general tasks with various LLMs.</li>\n</ul>"}, "publishedAt": "2026-01-30T01:38:02.000Z", "title": "TTCS: Test-Time Curriculum Synthesis for Self-Evolving", "summary": "Test-Time Training offers a promising way to improve the reasoning ability of large language models (LLMs) by adapting the model using only the test questions. However, existing methods struggle with difficult reasoning problems for two reasons: raw test questions are often too difficult to yield high-quality pseudo-labels, and the limited size of test sets makes continuous online updates prone to instability. To address these limitations, we propose TTCS, a co-evolving test-time training framework. Specifically, TTCS initializes two policies from the same pretrained model: a question synthesizer and a reasoning solver. These policies evolve through iterative optimization: the synthesizer generates progressively challenging question variants conditioned on the test questions, creating a structured curriculum tailored to the solver's current capability, while the solver updates itself using self-consistency rewards computed from multiple sampled responses on both original test and synthetic questions. Crucially, the solver's feedback guides the synthesizer to generate questions aligned with the model's current capability, and the generated question variants in turn stabilize the solver's test-time training. Experiments show that TTCS consistently strengthens the reasoning ability on challenging mathematical benchmarks and transfers to general-domain tasks across different LLM backbones, highlighting a scalable path towards dynamically constructing test-time curricula for self-evolving. Our code and implementation details are available at https://github.com/XMUDeepLIT/TTCS.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22628.png", "numComments": 2, "submittedBy": {"_id": "62ea79dd01ed9b0e8f61ccd3", "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg", "fullname": "Chengsong Huang", "name": "ChengsongHuang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.21192", "authors": [{"_id": "697cae786676f933227060de", "user": {"_id": "66a36281e5b14ef0e6d3befa", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/gejfR0L2VGYWBvVfHTyC6.png", "isPro": false, "fullname": "Lucas Chan", "user": "lucaswychan", "type": "user"}, "name": "Wun Yu Chan", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T16:58:57.137Z", "hidden": false}, {"_id": "697cae786676f933227060df", "name": "Shaojin Chen", "hidden": false}, {"_id": "697cae786676f933227060e0", "name": "Huihao Jing", "hidden": false}, {"_id": "697cae786676f933227060e1", "name": "Kwun Hang Lau", "hidden": false}, {"_id": "697cae786676f933227060e2", "name": "Elton Chun-Chai Li", "hidden": false}, {"_id": "697cae786676f933227060e3", "name": "Zihao Wang", "hidden": false}, {"_id": "697cae786676f933227060e4", "name": "Haoran Li", "hidden": false}, {"_id": "697cae786676f933227060e5", "name": "Yangqiu Song", "hidden": false}], "publishedAt": "2026-01-29T02:48:34.000Z", "submittedOnDailyAt": "2026-02-02T14:43:15.103Z", "title": "Do Reasoning Models Enhance Embedding Models?", "submittedOnDailyBy": {"_id": "66a36281e5b14ef0e6d3befa", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/gejfR0L2VGYWBvVfHTyC6.png", "isPro": false, "fullname": "Lucas Chan", "user": "lucaswychan", "type": "user"}, "summary": "State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.", "upvotes": 22, "discussionId": "697cae796676f933227060e6", "githubRepo": "https://github.com/HKUST-KnowComp/Reasoning-Embedding", "githubRepoAddedBy": "user", "ai_summary": "Embedding models initialized from RLVR-tuned reasoning models show no performance advantage over base models, with HRSA revealing preserved global geometry and linear readout despite local geometric reorganization.", "ai_keywords": ["embedding models", "decoder-only LLM", "contrastive learning", "Reinforcement Learning with Verifiable Rewards", "MTEB", "BRIGHT", "hierarchical representation similarity analysis", "latent manifold", "global manifold geometry", "local geometry reorganization", "coordinate basis drift", "Manifold Realignment", "supervised fine-tuning"], "githubStars": 5, "summary_zh": "<ul>\n    <li>\u6700\u65b0\u7684\u5d4c\u5165\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u91c7\u7528\u4ec5\u89e3\u7801\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u57fa\u7840\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u8c03\u6574\u3002</li>\n    <li>\u7814\u7a76\u4e86\u901a\u8fc7\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u8bad\u7ec3\u7684\u63a8\u7406\u6a21\u578b\u662f\u5426\u80fd\u63d0\u9ad8\u8bed\u4e49\u8868\u793a\u7684\u6548\u679c\u3002</li>\n    <li>\u7ed3\u679c\u663e\u793a\uff0c\u4eceRLVR\u8c03\u4f18\u7684\u57fa\u7840\u6a21\u578b\u521d\u59cb\u5316\u7684\u5d4c\u5165\u6a21\u578b\u5728\u76f8\u540c\u8bad\u7ec3\u6761\u4ef6\u4e0b\u5e76\u6ca1\u6709\u660e\u663e\u7684\u6027\u80fd\u4f18\u52bf\u3002</li>\n    <li>\u5f15\u5165\u4e86\u5206\u5c42\u8868\u793a\u76f8\u4f3c\u6027\u5206\u6790\uff08HRSA\uff09\u6846\u67b6\uff0c\u5206\u6790\u4e86\u8868\u793a\u3001\u51e0\u4f55\u548c\u529f\u80fd\u5c42\u9762\u7684\u76f8\u4f3c\u6027\u3002</li>\n    <li>\u53d1\u73b0RLVR\u4f18\u5316\u4e86\u73b0\u6709\u8bed\u4e49\u7a7a\u95f4\u4e2d\u7684\u8f68\u8ff9\uff0c\u800c\u4e0d\u662f\u6839\u672c\u6539\u53d8\u8bed\u4e49\u7a7a\u95f4\u7684\u7ed3\u6784\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New embedding models are based on large language models and use contrastive learning.</li>\n    <li>We investigated if reasoning models trained with Reinforcement Learning with Verifiable Rewards (RLVR) improve semantic representations when used as embeddings.</li>\n    <li>Surprisingly, models initialized from RLVR-tuned backbones did not perform better than the original models in tests.</li>\n    <li>We developed a framework called Hierarchical Representation Similarity Analysis (HRSA) to analyze the similarity of representations.</li>\n    <li>HRSA shows that RLVR changes some local structures but keeps the overall geometry, leading to similar performance between base and RLVR models.</li>\n</ul>"}, "publishedAt": "2026-01-28T21:48:34.000Z", "title": "Do Reasoning Models Enhance Embedding Models?", "summary": "State-of-the-art embedding models are increasingly derived from decoder-only Large Language Model (LLM) backbones adapted via contrastive learning. Given the emergence of reasoning models trained via Reinforcement Learning with Verifiable Rewards (RLVR), a natural question arises: do enhanced reasoning translate to superior semantic representations when these models serve as embedding initializations? Contrary to expectation, our evaluation on MTEB and BRIGHT reveals a **null effect**: embedding models initialized from RLVR-tuned backbones yield no consistent performance advantage over their base counterparts when subjected to identical training recipes. To unpack this paradox, we introduce **H**ierarchical **R**epresentation **S**imilarity **A**nalysis (HRSA), a framework that decomposes similarity across representation, geometry, and function levels. HRSA reveals that while RLVR induces irreversible latent manifold's local geometry reorganization and reversible coordinate basis drift, it preserves the global manifold geometry and linear readout. Consequently, subsequent contrastive learning drives strong alignment between base- and reasoning-initialized models, a phenomenon we term **Manifold Realignment**. Empirically, our findings suggest that unlike Supervised Fine-Tuning (SFT), RLVR optimizes trajectories within an existing semantic landscape rather than fundamentally restructuring the landscape itself.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21192.png", "numComments": 1, "submittedBy": {"_id": "66a36281e5b14ef0e6d3befa", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/gejfR0L2VGYWBvVfHTyC6.png", "fullname": "Lucas Chan", "name": "lucaswychan", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.23265", "authors": [{"_id": "698024266676f933227065e5", "name": "Dawei Zhu", "hidden": false}, {"_id": "698024266676f933227065e6", "name": "Rui Meng", "hidden": false}, {"_id": "698024266676f933227065e7", "name": "Yale Song", "hidden": false}, {"_id": "698024266676f933227065e8", "name": "Xiyu Wei", "hidden": false}, {"_id": "698024266676f933227065e9", "name": "Sujian Li", "hidden": false}, {"_id": "698024266676f933227065ea", "name": "Tomas Pfister", "hidden": false}, {"_id": "698024266676f933227065eb", "name": "Jinsung Yoon", "hidden": false}], "publishedAt": "2026-01-30T18:33:37.000Z", "submittedOnDailyAt": "2026-02-02T01:42:31.514Z", "title": "PaperBanana: Automating Academic Illustration for AI Scientists", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.", "upvotes": 19, "discussionId": "698024276676f933227065ec", "projectPage": "https://dwzhu-pku.github.io/PaperBanana/", "ai_summary": "_paperbanana is an agentic framework that automates the creation of publication-ready academic illustrations using advanced vision-language models and image generation techniques.", "ai_keywords": ["VLMs", "image generation models", "agentic framework", "publication-ready illustrations", "methodology diagrams", "PaperBananaBench", "self-critique", "statistical plots"], "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "summary_zh": "<ul>\n    <li>PaperBanana\u662f\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u5b66\u672f\u63d2\u56fe\u7684\u6846\u67b6\uff0c\u65e8\u5728\u51cf\u8f7b\u7814\u7a76\u5de5\u4f5c\u4e2d\u7684\u63d2\u56fe\u5236\u4f5c\u8d1f\u62c5\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5229\u7528\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u534f\u8c03\u591a\u4e2a\u4e13\u95e8\u7684\u4ee3\u7406\u8fdb\u884c\u63d2\u56fe\u751f\u6210\u3002</li>\n    <li>\u4e3a\u4e86\u8bc4\u4f30PaperBanana\u7684\u6548\u679c\uff0c\u7814\u7a76\u56e2\u961f\u521b\u5efa\u4e86PaperBananaBench\uff0c\u5305\u542b292\u4e2a\u6d4b\u8bd5\u6848\u4f8b\uff0c\u6d89\u53ca\u4e0d\u540c\u7684\u7814\u7a76\u9886\u57df\u548c\u63d2\u56fe\u98ce\u683c\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPaperBanana\u5728\u51c6\u786e\u6027\u3001\u7b80\u6d01\u6027\u3001\u53ef\u8bfb\u6027\u548c\u7f8e\u89c2\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u4e3b\u6d41\u65b9\u6cd5\u3002</li>\n    <li>\u6b64\u5916\uff0cPaperBanana\u8fd8\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u7edf\u8ba1\u56fe\u8868\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8\u5316\u63d2\u56fe\u751f\u6210\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>PaperBanana is a new tool that helps create illustrations for academic publications automatically.</li>\n    <li>It uses advanced AI models to gather information, plan designs, and refine images through self-review.</li>\n    <li>The tool has been tested with 292 examples from research papers to ensure it works well across different topics and styles.</li>\n    <li>Results show that PaperBanana is better than existing methods in terms of accuracy, clarity, and visual appeal.</li>\n    <li>It can also create high-quality statistical plots, making it useful for more types of illustrations.</li>\n</ul>"}, "publishedAt": "2026-01-30T13:33:37.000Z", "title": "PaperBanana: Automating Academic Illustration for AI Scientists", "summary": "Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23265.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 224, "isUserFollowing": false}, "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.23182", "authors": [{"_id": "6980487a6676f933227066aa", "name": "Siyang He", "hidden": false}, {"_id": "6980487a6676f933227066ab", "name": "Qiqi Wang", "hidden": false}, {"_id": "6980487a6676f933227066ac", "user": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "isPro": false, "fullname": "Xiaoran Liu (SII)", "user": "SII-xrliu", "type": "user"}, "name": "Xiaoran Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T16:52:14.672Z", "hidden": false}, {"_id": "6980487a6676f933227066ad", "name": "Hongnan Ma", "hidden": false}, {"_id": "6980487a6676f933227066ae", "name": "Yiwei Shi", "hidden": false}, {"_id": "6980487a6676f933227066af", "name": "Yuerong Song", "hidden": false}, {"_id": "6980487a6676f933227066b0", "name": "Ying Zhu", "hidden": false}, {"_id": "6980487a6676f933227066b1", "name": "Tianyi Liang", "hidden": false}, {"_id": "6980487a6676f933227066b2", "name": "Zengfeng Huang", "hidden": false}, {"_id": "6980487a6676f933227066b3", "name": "Ziwei He", "hidden": false}, {"_id": "6980487a6676f933227066b4", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-01-30T17:06:41.000Z", "submittedOnDailyAt": "2026-02-02T04:18:39.975Z", "title": "FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation", "submittedOnDailyBy": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "isPro": false, "fullname": "Xiaoran Liu (SII)", "user": "SII-xrliu", "type": "user"}, "summary": "Despite the non-autoregressive potential of diffusion language models (dLLMs), existing decoding strategies demonstrate positional bias, failing to fully unlock the potential of arbitrary generation. In this work, we delve into the inherent spectral characteristics of dLLMs and present the first frequency-domain analysis showing that low-frequency components in hidden states primarily encode global structural information and long-range dependencies, while high-frequency components are responsible for characterizing local details. Based on this observation, we propose FourierSampler, which leverages a frequency-domain sliding window mechanism to dynamically guide the model to achieve a \"structure-to-detail\" generation. FourierSampler outperforms other inference enhancement strategies on LLADA and SDAR, achieving relative improvements of 20.4% on LLaDA1.5-8B and 16.0% on LLaDA-8B-Instruct. It notably surpasses similarly sized autoregressive models like Llama3.1-8B-Instruct.", "upvotes": 18, "discussionId": "6980487b6676f933227066b5", "githubRepo": "https://github.com/ShirleYoung/FourierSampler", "githubRepoAddedBy": "user", "ai_summary": "Frequency-domain analysis of diffusion language models reveals that low-frequency components encode global structure while high-frequency components capture local details, enabling improved generation through FourierSampler's dynamic frequency-domain sliding window mechanism.", "ai_keywords": ["diffusion language models", "spectral characteristics", "frequency-domain analysis", "hidden states", "low-frequency components", "high-frequency components", "FourierSampler", "frequency-domain sliding window mechanism", "arbitrary generation", "positional bias"], "githubStars": 2, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u89e3\u7801\u7b56\u7565\u4e0a\u5b58\u5728\u4f4d\u7f6e\u504f\u5dee\uff0c\u672a\u80fd\u5145\u5206\u53d1\u6325\u5176\u751f\u6210\u6f5c\u529b\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u4f4e\u9891\u6210\u5206\u7f16\u7801\u5168\u5c40\u7ed3\u6784\u4fe1\u606f\u548c\u957f\u7a0b\u4f9d\u8d56\uff0c\u800c\u9ad8\u9891\u6210\u5206\u5219\u8d1f\u8d23\u5c40\u90e8\u7ec6\u8282\u3002</li>\n    <li>\u63d0\u51fa\u4e86 FourierSampler\uff0c\u5b83\u4f7f\u7528\u9891\u57df\u6ed1\u52a8\u7a97\u53e3\u673a\u5236\u6765\u52a8\u6001\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u7ed3\u6784\u5230\u7ec6\u8282\u3002</li>\n    <li>\u5728 LLADA \u548c SDAR \u4e0a\uff0cFourierSampler \u7684\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u63a8\u7406\u589e\u5f3a\u7b56\u7565\uff0cLLaDA1.5-8B \u63d0\u9ad8\u4e86 20.4%\u3002</li>\n    <li>FourierSampler \u7684\u8868\u73b0\u8d85\u8d8a\u4e86\u7c7b\u4f3c\u89c4\u6a21\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u5982 Llama3.1-8B-Instruct\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion language models (dLLMs) have potential but current decoding methods show positional bias, limiting their effectiveness.</li>\n    <li>This research analyzes the frequency characteristics of dLLMs, finding that low frequencies capture overall structure while high frequencies capture local details.</li>\n    <li>Based on this analysis, the authors introduce a new method called FourierSampler that helps the model generate content by focusing on structure first and then details.</li>\n    <li>FourierSampler significantly outperforms other strategies, improving results by over 20% on some models compared to existing methods.</li>\n    <li>It also performs better than similar autoregressive models, demonstrating its effectiveness in generating high-quality outputs.</li>\n</ul>"}, "publishedAt": "2026-01-30T12:06:41.000Z", "title": "FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation", "summary": "Despite the non-autoregressive potential of diffusion language models (dLLMs), existing decoding strategies demonstrate positional bias, failing to fully unlock the potential of arbitrary generation. In this work, we delve into the inherent spectral characteristics of dLLMs and present the first frequency-domain analysis showing that low-frequency components in hidden states primarily encode global structural information and long-range dependencies, while high-frequency components are responsible for characterizing local details. Based on this observation, we propose FourierSampler, which leverages a frequency-domain sliding window mechanism to dynamically guide the model to achieve a \"structure-to-detail\" generation. FourierSampler outperforms other inference enhancement strategies on LLADA and SDAR, achieving relative improvements of 20.4% on LLaDA1.5-8B and 16.0% on LLaDA-8B-Instruct. It notably surpasses similarly sized autoregressive models like Llama3.1-8B-Instruct.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23182.png", "numComments": 1, "submittedBy": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "fullname": "Xiaoran Liu (SII)", "name": "SII-xrliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.23184", "authors": [{"_id": "6980247b6676f933227065ee", "user": {"_id": "6912d19a10fbcbf70dedf126", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6912d19a10fbcbf70dedf126/XYstDl622FMSbFRQ9Ab6o.jpeg", "isPro": false, "fullname": "Fanmeng Wang", "user": "FanmengWang", "type": "user"}, "name": "Fanmeng Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T16:52:37.097Z", "hidden": false}, {"_id": "6980247b6676f933227065ef", "name": "Haotian Liu", "hidden": false}, {"_id": "6980247b6676f933227065f0", "name": "Guojiang Zhao", "hidden": false}, {"_id": "6980247b6676f933227065f1", "name": "Hongteng Xu", "hidden": false}, {"_id": "6980247b6676f933227065f2", "name": "Zhifeng Gao", "hidden": false}], "publishedAt": "2026-01-30T17:08:06.000Z", "submittedOnDailyAt": "2026-02-02T01:44:01.004Z", "title": "ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While Chain-of-Thought (CoT) significantly enhances the performance of Large Language Models (LLMs), explicit reasoning chains introduce substantial computational redundancy. Recent latent reasoning methods attempt to mitigate this by compressing reasoning processes into latent space, but often suffer from severe performance degradation due to the lack of appropriate compression guidance. In this study, we propose Rendered CoT-Guided variational Latent Reasoning (ReGuLaR), a simple yet novel latent learning paradigm resolving this issue. Fundamentally, we formulate latent reasoning within the Variational Auto-Encoding (VAE) framework, sampling the current latent reasoning state from the posterior distribution conditioned on previous ones. Specifically, when learning this variational latent reasoning model, we render explicit reasoning chains as images, from which we extract dense visual-semantic representations to regularize the posterior distribution, thereby achieving efficient compression with minimal information loss. Extensive experiments demonstrate that ReGuLaR significantly outperforms existing latent reasoning methods across both computational efficiency and reasoning effectiveness, and even surpasses CoT through multi-modal reasoning, providing a new and insightful solution to latent reasoning. Code: https://github.com/FanmengWang/ReGuLaR.", "upvotes": 12, "discussionId": "6980247b6676f933227065f3", "githubRepo": "https://github.com/FanmengWang/ReGuLaR", "githubRepoAddedBy": "user", "ai_summary": "ReGuLaR introduces a variational auto-encoding framework that compresses reasoning processes into latent space while maintaining performance through image-rendered explicit reasoning chains for guidance.", "ai_keywords": ["Chain-of-Thought", "Large Language Models", "latent reasoning", "Variational Auto-Encoding", "posterior distribution", "visual-semantic representations", "multi-modal reasoning"], "githubStars": 15, "summary_zh": "<ul>\n    <li>Chain-of-Thought (CoT) \u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f46\u4f1a\u5bfc\u81f4\u8ba1\u7b97\u5197\u4f59\u3002</li>\n    <li>\u73b0\u6709\u7684\u6f5c\u5728\u63a8\u7406\u65b9\u6cd5\u5c1d\u8bd5\u538b\u7f29\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f46\u901a\u5e38\u4f1a\u964d\u4f4e\u6027\u80fd\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5 ReGuLaR\uff0c\u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u6846\u67b6\u8fdb\u884c\u6f5c\u5728\u63a8\u7406\u3002</li>\n    <li>ReGuLaR \u5c06\u63a8\u7406\u94fe\u6e32\u67d3\u4e3a\u56fe\u50cf\uff0c\u4ee5\u63d0\u53d6\u89c6\u89c9-\u8bed\u4e49\u8868\u793a\uff0c\u4f18\u5316\u4fe1\u606f\u538b\u7f29\u3002</li>\n    <li>\u5b9e\u9a8c\u8bc1\u660e ReGuLaR \u5728\u8ba1\u7b97\u6548\u7387\u548c\u63a8\u7406\u6709\u6548\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e0a\u8d85\u8d8a\u4e86 CoT\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Chain-of-Thought (CoT) improves Large Language Models (LLMs) but can be very computationally heavy.</li>\n    <li>New methods try to reduce this redundancy by compressing reasoning into a simpler form, but they often lose performance.</li>\n    <li>The study introduces a new method called ReGuLaR, which uses a Variational Auto-Encoding framework for better latent reasoning.</li>\n    <li>ReGuLaR creates images of reasoning chains to help maintain information while compressing the data.</li>\n    <li>Tests show that ReGuLaR is more efficient and effective than other methods and even better than traditional CoT in multi-modal reasoning.</li>\n</ul>"}, "publishedAt": "2026-01-30T12:08:06.000Z", "title": "ReGuLaR: Variational Latent Reasoning Guided by Rendered Chain-of-Thought", "summary": "While Chain-of-Thought (CoT) significantly enhances the performance of Large Language Models (LLMs), explicit reasoning chains introduce substantial computational redundancy. Recent latent reasoning methods attempt to mitigate this by compressing reasoning processes into latent space, but often suffer from severe performance degradation due to the lack of appropriate compression guidance. In this study, we propose Rendered CoT-Guided variational Latent Reasoning (ReGuLaR), a simple yet novel latent learning paradigm resolving this issue. Fundamentally, we formulate latent reasoning within the Variational Auto-Encoding (VAE) framework, sampling the current latent reasoning state from the posterior distribution conditioned on previous ones. Specifically, when learning this variational latent reasoning model, we render explicit reasoning chains as images, from which we extract dense visual-semantic representations to regularize the posterior distribution, thereby achieving efficient compression with minimal information loss. Extensive experiments demonstrate that ReGuLaR significantly outperforms existing latent reasoning methods across both computational efficiency and reasoning effectiveness, and even surpasses CoT through multi-modal reasoning, providing a new and insightful solution to latent reasoning. Code: https://github.com/FanmengWang/ReGuLaR.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.23184.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 224, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.21998", "authors": [{"_id": "6980d46983fdbbe1963c2ce6", "name": "Lin Li", "hidden": false}, {"_id": "6980d46983fdbbe1963c2ce7", "name": "Qihang Zhang", "hidden": false}, {"_id": "6980d46983fdbbe1963c2ce8", "name": "Yiming Luo", "hidden": false}, {"_id": "6980d46983fdbbe1963c2ce9", "name": "Shuai Yang", "hidden": false}, {"_id": "6980d46983fdbbe1963c2cea", "name": "Ruilin Wang", "hidden": false}, {"_id": "6980d46983fdbbe1963c2ceb", "name": "Fei Han", "hidden": false}, {"_id": "6980d46983fdbbe1963c2cec", "name": "Mingrui Yu", "hidden": false}, {"_id": "6980d46983fdbbe1963c2ced", "name": "Zelin Gao", "hidden": false}, {"_id": "6980d46983fdbbe1963c2cee", "name": "Nan Xue", "hidden": false}, {"_id": "6980d46983fdbbe1963c2cef", "name": "Xing Zhu", "hidden": false}, {"_id": "6980d46983fdbbe1963c2cf0", "name": "Yujun Shen", "hidden": false}, {"_id": "6980d46983fdbbe1963c2cf1", "name": "Yinghao Xu", "hidden": false}], "publishedAt": "2026-01-29T17:07:43.000Z", "submittedOnDailyAt": "2026-02-02T14:14:54.399Z", "title": "Causal World Modeling for Robot Control", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.", "upvotes": 12, "discussionId": "6980d46983fdbbe1963c2cf2", "ai_summary": "Video world modeling enables robot learning through a unified framework that predicts frames and executes policies simultaneously using a shared latent space and closed-loop feedback mechanisms.", "ai_keywords": ["video world modeling", "autoregressive diffusion framework", "frame prediction", "policy execution", "shared latent space", "Mixture-of-Transformers", "closed-loop rollout mechanism", "asynchronous inference pipeline", "long-horizon manipulation", "data efficiency", "generalizability"], "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u5f3a\u8c03\u89c6\u9891\u4e16\u754c\u5efa\u6a21\u4e0e\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u7840\u3002</li>\n    <li>\u89c6\u9891\u4e16\u754c\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u7406\u89e3\u52a8\u4f5c\u4e0e\u89c6\u89c9\u52a8\u6001\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u6765\u60f3\u8c61\u672a\u6765\u3002</li>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86LingBot-VA\uff0c\u8fd9\u662f\u4e00\u79cd\u81ea\u56de\u5f52\u6269\u6563\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u5b66\u4e60\u5e27\u9884\u6d4b\u548c\u7b56\u7565\u6267\u884c\u3002</li>\n    <li>\u6a21\u578b\u8bbe\u8ba1\u5305\u62ec\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u3001\u95ed\u73af\u56de\u653e\u673a\u5236\uff0c\u4ee5\u53ca\u5f02\u6b65\u63a8\u7406\u7ba1\u9053\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u3002</li>\n    <li>\u5728\u6a21\u62df\u57fa\u51c6\u548c\u5b9e\u9645\u573a\u666f\u4e2d\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u957f\u65f6\u95f4\u64cd\u4f5c\u548c\u6570\u636e\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video world modeling helps robots learn by predicting the future based on actions and visual changes.</li>\n    <li>We introduced LingBot-VA, a new framework that learns to predict video frames and execute actions at the same time.</li>\n    <li>Key features of our model include a shared space for vision and actions, a system for continuous feedback, and a fast way to predict actions and move the robot simultaneously.</li>\n    <li>Our model performs well in both simulations and real-life tests, showing good skills in complex tasks and adaptability to new situations.</li>\n    <li>We have made our code and model available for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2026-01-29T12:07:43.000Z", "title": "Causal World Modeling for Robot Control", "summary": "This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21998.png", "numComments": 1, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 9203, "isUserFollowing": false}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2601.20833", "authors": [{"_id": "697b9192a67238fac88cbee8", "name": "Tengyue Xu", "hidden": false}, {"_id": "697b9192a67238fac88cbee9", "name": "Zhuoyang Qian", "hidden": false}, {"_id": "697b9192a67238fac88cbeea", "name": "Gaoge Liu", "hidden": false}, {"_id": "697b9192a67238fac88cbeeb", "name": "Li Ling", "hidden": false}, {"_id": "697b9192a67238fac88cbeec", "name": "Zhentao Zhang", "hidden": false}, {"_id": "697b9192a67238fac88cbeed", "name": "Biao Wu", "hidden": false}, {"_id": "697b9192a67238fac88cbeee", "name": "Shuo Zhang", "hidden": false}, {"_id": "697b9192a67238fac88cbeef", "name": "Ke Lu", "hidden": false}, {"_id": "697b9192a67238fac88cbef0", "name": "Wei Shi", "hidden": false}, {"_id": "697b9192a67238fac88cbef1", "name": "Ziqi Wang", "hidden": false}, {"_id": "697b9192a67238fac88cbef2", "name": "Zheng Feng", "hidden": false}, {"_id": "697b9192a67238fac88cbef3", "name": "Yan Luo", "hidden": false}, {"_id": "697b9192a67238fac88cbef4", "name": "Shu Xu", "hidden": false}, {"_id": "697b9192a67238fac88cbef5", "name": "Yongjin Chen", "hidden": false}, {"_id": "697b9192a67238fac88cbef6", "name": "Zhibo Feng", "hidden": false}, {"_id": "697b9192a67238fac88cbef7", "name": "Zhuo Chen", "hidden": false}, {"_id": "697b9192a67238fac88cbef8", "name": "Bruce Yuan", "hidden": false}, {"_id": "697b9192a67238fac88cbef9", "name": "Harry Wang", "hidden": false}, {"_id": "697b9192a67238fac88cbefa", "name": "Kris Chen", "hidden": false}], "publishedAt": "2026-01-28T18:31:54.000Z", "submittedOnDailyAt": "2026-01-30T03:32:00.106Z", "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives", "submittedOnDailyBy": {"_id": "62baa0d6dd02fbf607ce97be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62baa0d6dd02fbf607ce97be/V0I6pANlLEf2YDd9ZLZgi.jpeg", "isPro": false, "fullname": "Wendy", "user": "Wendy-Fly", "type": "user"}, "summary": "Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.", "upvotes": 113, "discussionId": "697b9192a67238fac88cbefb", "githubRepo": "https://github.com/AgentAlphaAGI/Idea2Paper", "githubRepoAddedBy": "user", "ai_summary": "Offline knowledge construction through structured methodological graphs enables more reliable and scalable autonomous scientific discovery by reducing reliance on real-time literature processing.", "ai_keywords": ["large language model", "autonomous scientific discovery", "runtime-centric execution", "context window limitations", "hallucination", "pre-computation-driven framework", "peer-reviewed papers", "research patterns", "methodological knowledge graph", "end-to-end research workflows"], "githubStars": 54, "organization": {"_id": "69542731e1200d74c1c053d1", "name": "AgentAlphaAGI", "fullname": "AgentAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b78eb76ab5d14ca7faac87/TbMZ3y00APtRzHEfTSR7I.jpeg"}, "summary_zh": "<ul>\n    <li>Idea2Story\u662f\u4e00\u4e2a\u57fa\u4e8e\u9884\u8ba1\u7b97\u7684\u6846\u67b6\uff0c\u65e8\u5728\u5b9e\u73b0\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5c06\u6587\u732e\u7406\u89e3\u4ece\u5728\u7ebf\u63a8\u7406\u8f6c\u53d8\u4e3a\u79bb\u7ebf\u77e5\u8bc6\u6784\u5efa\uff0c\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u548c\u4e0a\u4e0b\u6587\u9650\u5236\u3002</li>\n    <li>Idea2Story\u6536\u96c6\u540c\u884c\u8bc4\u5ba1\u8bba\u6587\u53ca\u5176\u53cd\u9988\uff0c\u63d0\u53d6\u6838\u5fc3\u65b9\u6cd5\u5355\u5143\uff0c\u5e76\u7ec4\u7ec7\u6210\u7ed3\u6784\u5316\u7684\u77e5\u8bc6\u56fe\u8c31\u3002</li>\n    <li>\u5728\u8fd0\u884c\u65f6\uff0c\u7528\u6237\u7684\u7814\u7a76\u610f\u56fe\u4e0e\u5df2\u6709\u7814\u7a76\u8303\u5f0f\u5bf9\u9f50\uff0c\u4ece\u800c\u9ad8\u6548\u68c0\u7d22\u548c\u91cd\u7528\u7814\u7a76\u6a21\u5f0f\u3002</li>\n    <li>\u521d\u6b65\u7814\u7a76\u8868\u660e\uff0cIdea2Story\u80fd\u591f\u751f\u6210\u8fde\u8d2f\u4e14\u521b\u65b0\u7684\u7814\u7a76\u6a21\u5f0f\uff0c\u652f\u6301\u53ef\u9760\u7684\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Idea2Story is a new framework for scientific discovery that moves from online reasoning to offline knowledge building.</li>\n    <li>It collects peer-reviewed papers and feedback, extracts key methods, and organizes them into a knowledge graph.</li>\n    <li>This system allows researchers to efficiently find and reuse established research patterns instead of starting from scratch.</li>\n    <li>By using a knowledge graph, Idea2Story reduces the need for constant online reasoning and allows for more reliable outcomes.</li>\n    <li>Early studies show that Idea2Story can create high-quality, coherent research ideas effectively.</li>\n</ul>"}, "publishedAt": "2026-01-28T13:31:54.000Z", "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives", "summary": "Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20833.png", "numComments": 1, "submittedBy": {"_id": "62baa0d6dd02fbf607ce97be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62baa0d6dd02fbf607ce97be/V0I6pANlLEf2YDd9ZLZgi.jpeg", "fullname": "Wendy", "name": "Wendy-Fly", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "69542731e1200d74c1c053d1", "name": "AgentAlphaAGI", "fullname": "AgentAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b78eb76ab5d14ca7faac87/TbMZ3y00APtRzHEfTSR7I.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20354", "authors": [{"_id": "697c1857a67238fac88cc06e", "user": {"_id": "656c9cfef7be0986b49934ea", "avatarUrl": "/avatars/2030e77c28fb4c518b692cd9a20de665.svg", "isPro": false, "fullname": "MuMing", "user": "ZengbinWang", "type": "user"}, "name": "Zengbin Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T09:36:35.067Z", "hidden": false}, {"_id": "697c1857a67238fac88cc06f", "name": "Xuecai Hu", "hidden": false}, {"_id": "697c1857a67238fac88cc070", "name": "Yong Wang", "hidden": false}, {"_id": "697c1857a67238fac88cc071", "name": "Feng Xiong", "hidden": false}, {"_id": "697c1857a67238fac88cc072", "name": "Man Zhang", "hidden": false}, {"_id": "697c1857a67238fac88cc073", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-28T08:15:00.000Z", "submittedOnDailyAt": "2026-01-30T05:01:51.614Z", "title": "Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.", "upvotes": 96, "discussionId": "697c1857a67238fac88cc074", "githubRepo": "https://github.com/AMAP-ML/SpatialGenEval", "githubRepoAddedBy": "user", "ai_summary": "A new benchmark and dataset are introduced to evaluate and improve spatial reasoning capabilities in text-to-image models through information-dense prompts and fine-tuning.", "ai_keywords": ["text-to-image models", "spatial intelligence", "benchmark", "long prompts", "information-dense prompts", "spatial reasoning", "Stable Diffusion-XL", "Uniworld-V1", "OmniGen2", "fine-tuning"], "githubStars": 93, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u65f6\u5e38\u5e38\u8868\u73b0\u4e0d\u4f73\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6SpatialGenEval\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u7a7a\u95f4\u667a\u80fd\uff0c\u5305\u542b1230\u4e2a\u957f\u63d0\u793a\u548c25\u4e2a\u771f\u5b9e\u573a\u666f\u3002</li>\n    <li>\u6bcf\u4e2a\u63d0\u793a\u5305\u542b10\u4e2a\u7a7a\u95f4\u5b50\u9886\u57df\u548c\u5bf9\u5e94\u7684\u591a\u9009\u9898\uff0c\u6d89\u53ca\u5bf9\u8c61\u4f4d\u7f6e\u3001\u5e03\u5c40\u3001\u906e\u6321\u548c\u56e0\u679c\u5173\u7cfb\u7b49\u5185\u5bb9\u3002</li>\n    <li>\u5bf921\u4e2a\u5148\u8fdb\u6a21\u578b\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u9ad8\u9636\u7a7a\u95f4\u63a8\u7406\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u74f6\u9888\u3002</li>\n    <li>\u6784\u5efa\u4e86SpatialT2I\u6570\u636e\u96c6\uff0c\u5305\u542b15400\u5bf9\u6587\u672c-\u56fe\u50cf\uff0c\u786e\u4fdd\u56fe\u50cf\u4e00\u81f4\u6027\u5e76\u63d0\u9ad8\u4fe1\u606f\u5bc6\u5ea6\uff0c\u6539\u8fdb\u4e86\u6a21\u578b\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Text-to-image models can create high-quality images but struggle with complex spatial relationships.</li>\n    <li>SpatialGenEval is a new benchmark that tests the spatial understanding of these models using 1,230 detailed prompts from 25 real-world scenes.</li>\n    <li>The prompts include questions about object positions, layouts, and interactions, revealing that advanced spatial reasoning is a major challenge for current models.</li>\n    <li>The researchers also created the SpatialT2I dataset with 15,400 text-image pairs to improve image consistency and information density.</li>\n    <li>Fine-tuning on leading models showed performance improvements, demonstrating the importance of data quality for enhancing spatial intelligence in T2I models.</li>\n</ul>"}, "publishedAt": "2026-01-28T03:15:00.000Z", "title": "Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models", "summary": "Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20354.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20614", "authors": [{"_id": "697ac91bdf3e800774f13c12", "name": "Yanqi Dai", "hidden": false}, {"_id": "697ac91bdf3e800774f13c13", "name": "Yuxiang Ji", "hidden": false}, {"_id": "697ac91bdf3e800774f13c14", "name": "Xiao Zhang", "hidden": false}, {"_id": "697ac91bdf3e800774f13c15", "name": "Yong Wang", "hidden": false}, {"_id": "697ac91bdf3e800774f13c16", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "697ac91bdf3e800774f13c17", "name": "Zhiwu Lu", "hidden": false}], "publishedAt": "2026-01-28T13:49:23.000Z", "submittedOnDailyAt": "2026-01-29T00:15:35.371Z", "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation", "submittedOnDailyBy": {"_id": "66cde57cb1fe4c78fe3ab770", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66cde57cb1fe4c78fe3ab770/0R1aA-f_XLjCfy1HwqZ-p.jpeg", "isPro": false, "fullname": "Yanqi Dai", "user": "YanqiDai", "type": "user"}, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.", "upvotes": 93, "discussionId": "697ac91bdf3e800774f13c18", "githubRepo": "https://github.com/AMAP-ML/MathForge", "githubRepoAddedBy": "user", "ai_summary": "MathForge enhances mathematical reasoning in large models through a dual framework combining difficulty-aware policy optimization and multi-aspect question reformulation to address limitations in existing reinforcement learning methods.", "ai_keywords": ["Reinforcement Learning with Verifiable Rewards", "Group Relative Policy Optimization", "Difficulty-Aware Group Policy Optimization", "Multi-Aspect Question Reformulation", "mathematical reasoning", "policy updates", "group advantage estimation", "question-level weighting", "data augmentation"], "githubStars": 84, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\u4e0e\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\u80fd\u591f\u589e\u5f3a\u5927\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u76ee\u524d\u7684\u65b9\u6cd5\u5728\u7b97\u6cd5\u548c\u6570\u636e\u65b9\u9762\u90fd\u7f3a\u4e4f\u5bf9\u66f4\u5177\u6311\u6218\u6027\u95ee\u9898\u7684\u91cd\u89c6\uff0c\u8fd9\u5f71\u54cd\u4e86\u80fd\u529b\u7684\u63d0\u5347\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMathForge\u7684\u6846\u67b6\uff0c\u5305\u62ec\u96be\u5ea6\u611f\u77e5\u7684\u56e2\u4f53\u7b56\u7565\u4f18\u5316\uff08DGPO\uff09\u7b97\u6cd5\u548c\u591a\u65b9\u9762\u95ee\u9898\u91cd\u6784\uff08MQR\uff09\u7b56\u7565\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n    <li>DGPO\u901a\u8fc7\u96be\u5ea6\u5e73\u8861\u7684\u4f18\u52bf\u4f30\u8ba1\u4fee\u6b63\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u4f18\u5148\u5904\u7406\u66f4\u96be\u7684\u95ee\u9898\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMathForge\u5728\u591a\u79cd\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement Learning with Verifiable Rewards (RLVR) helps improve mathematical reasoning in large models.</li>\n    <li>Current methods do not focus enough on harder questions, which are important for improving skills.</li>\n    <li>A new framework called MathForge includes two parts: Difficulty-Aware Group Policy Optimization (DGPO) and Multi-Aspect Question Reformulation (MQR).</li>\n    <li>DGPO fixes issues in existing methods by balancing question difficulty and focusing on harder questions.</li>\n    <li>MQR changes questions to make them harder while keeping the correct answers, leading to better performance in math tasks.</li>\n</ul>"}, "publishedAt": "2026-01-28T08:49:23.000Z", "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20614.png", "numComments": 12, "submittedBy": {"_id": "66cde57cb1fe4c78fe3ab770", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66cde57cb1fe4c78fe3ab770/0R1aA-f_XLjCfy1HwqZ-p.jpeg", "fullname": "Yanqi Dai", "name": "YanqiDai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.21204", "authors": [{"_id": "697c3801a67238fac88cc1b1", "name": "Hong Liu", "hidden": false}, {"_id": "697c3801a67238fac88cc1b2", "name": "Jiaqi Zhang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b3", "name": "Chao Wang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b4", "name": "Xing Hu", "hidden": false}, {"_id": "697c3801a67238fac88cc1b5", "name": "Linkun Lyu", "hidden": false}, {"_id": "697c3801a67238fac88cc1b6", "name": "Jiaqi Sun", "hidden": false}, {"_id": "697c3801a67238fac88cc1b7", "name": "Xurui Yang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b8", "name": "Bo Wang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b9", "name": "Fengcun Li", "hidden": false}, {"_id": "697c3801a67238fac88cc1ba", "name": "Yulei Qian", "hidden": false}, {"_id": "697c3801a67238fac88cc1bb", "name": "Lingtong Si", "hidden": false}, {"_id": "697c3801a67238fac88cc1bc", "name": "Yerui Sun", "hidden": false}, {"_id": "697c3801a67238fac88cc1bd", "name": "Rumei Li", "hidden": false}, {"_id": "697c3801a67238fac88cc1be", "name": "Peng Pei", "hidden": false}, {"_id": "697c3801a67238fac88cc1bf", "name": "Yuchen Xie", "hidden": false}, {"_id": "697c3801a67238fac88cc1c0", "name": "Xunliang Cai", "hidden": false}], "publishedAt": "2026-01-29T03:11:19.000Z", "submittedOnDailyAt": "2026-01-30T02:18:11.112Z", "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.", "upvotes": 76, "discussionId": "697c3801a67238fac88cc1c1", "ai_summary": "Embedding scaling offers superior sparsity scaling compared to expert scaling in large language models, enabling efficient inference through system optimizations and speculative decoding.", "ai_keywords": ["Mixture-of-Experts", "sparsity scaling", "embedding scaling", "Pareto frontier", "parameter budgeting", "model width", "model depth", "system optimizations", "speculative decoding", "LongCat-Flash-Lite"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5e38\u7528\u4e8e\u7a00\u758f\u6027\u6269\u5c55\uff0c\u4f46\u9762\u4e34\u6536\u76ca\u9012\u51cf\u548c\u7cfb\u7edf\u74f6\u9888\u95ee\u9898\u3002</li>\n    <li>\u672c\u6587\u63a2\u8ba8\u4e86\u5d4c\u5165\u6269\u5c55\u4f5c\u4e3a\u53e6\u4e00\u79cd\u6709\u6548\u7684\u7a00\u758f\u6027\u6269\u5c55\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u548c\u5b9e\u9a8c\u627e\u5230\u4e86\u5176\u4f18\u52bf\u533a\u57df\u3002</li>\n    <li>\u6211\u4eec\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u5f71\u54cd\u5d4c\u5165\u6269\u5c55\u6709\u6548\u6027\u7684\u5173\u952e\u67b6\u6784\u56e0\u7d20\uff0c\u5305\u62ec\u53c2\u6570\u9884\u7b97\u3001\u6a21\u578b\u5bbd\u5ea6\u548c\u6df1\u5ea6\u7684\u76f8\u4e92\u4f5c\u7528\u3002</li>\n    <li>\u901a\u8fc7\u7cfb\u7edf\u4f18\u5316\u548c\u63a8\u6d4b\u89e3\u7801\uff0c\u6211\u4eec\u5c06\u7a00\u758f\u6027\u8f6c\u5316\u4e3a\u5b9e\u9645\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86LongCat-Flash-Lite\uff0c\u4e00\u4e2a685\u4ebf\u53c2\u6570\u7684\u6a21\u578b\uff0c\u5c3d\u7ba1\u5d4c\u5165\u5360\u7528\u8d85\u8fc7300\u4ebf\u53c2\u6570\uff0c\u4f46\u5176\u6027\u80fd\u8d85\u8fc7\u4e86\u540c\u7b49\u53c2\u6570\u7684MoE\u57fa\u51c6\uff0c\u5e76\u5728\u7279\u5b9a\u9886\u57df\u8868\u73b0\u51fa\u8272\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Mixture-of-Experts (MoE) models are popular for handling large language models but have limitations in performance and system efficiency.</li>\n    <li>This study looks at using embedding scaling as an alternative way to improve sparsity in models.</li>\n    <li>Research shows that embedding scaling can lead to better performance in certain situations compared to expert scaling.</li>\n    <li>Key factors like model size and structure influence how well embedding scaling works.</li>\n    <li>The new model, LongCat-Flash-Lite, is highly efficient, using fewer active parameters while outperforming similar models, especially in coding and agent-related tasks.</li>\n</ul>"}, "publishedAt": "2026-01-28T22:11:19.000Z", "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "summary": "While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21204.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 221, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20540", "authors": [{"_id": "697ac48cdf3e800774f13bc1", "name": "Robbyant Team", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc2", "name": "Zelin Gao", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc3", "user": {"_id": "64981bea09cea550852652af", "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg", "isPro": false, "fullname": "Qiuyu Wang", "user": "qiuyuu", "type": "user"}, "name": "Qiuyu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:16:29.190Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc4", "name": "Yanhong Zeng", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc5", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc6", "user": {"_id": "64acd2ec39fcfebff8c79c00", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64acd2ec39fcfebff8c79c00/Avq66l5hO-aggNtk4Y1ss.png", "isPro": false, "fullname": "Ka Leong Cheng", "user": "felixcheng97", "type": "user"}, "name": "Ka Leong Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T13:56:36.041Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc7", "name": "Yixuan Li", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc8", "user": {"_id": "665f059a8947302aa2c63afe", "avatarUrl": "/avatars/50f560285946532321a0bd526494148d.svg", "isPro": false, "fullname": "hanlin wang", "user": "hlwang06", "type": "user"}, "name": "Hanlin Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:18:10.952Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc9", "name": "Yinghao Xu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bca", "name": "Shuailei Ma", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcb", "name": "Yihang Chen", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcc", "name": "Jie Liu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcd", "name": "Yansong Cheng", "hidden": false}, {"_id": "697ac48cdf3e800774f13bce", "name": "Yao Yao", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcf", "name": "Jiayi Zhu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd0", "user": {"_id": "656084f44e8918182d4f07c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/akAvCUCi7eR31PWOXrVPw.jpeg", "isPro": false, "fullname": "Yihao Meng", "user": "Yhmeng1106", "type": "user"}, "name": "Yihao Meng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T13:56:37.840Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd1", "name": "Kecheng Zheng", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd2", "user": {"_id": "63f0baf66309c84d5f4a2226", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f0baf66309c84d5f4a2226/ihOgtwseRkfP1t-60IgyT.jpeg", "isPro": true, "fullname": "Qingyan", "user": "QingyanBai", "type": "user"}, "name": "Qingyan Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:16:24.535Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd3", "user": {"_id": "6478a982256b62e219917d67", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/PUJ-N2cQxgEmDGfyjajyA.jpeg", "isPro": false, "fullname": "JingyeChen22", "user": "JingyeChen22", "type": "user"}, "name": "Jingye Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:17:37.828Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd4", "name": "Zehong Shen", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd5", "user": {"_id": "662128ec9ca2cd4e6db2fb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662128ec9ca2cd4e6db2fb44/uUg1V-pVfxT3mLuFgJuAN.jpeg", "isPro": false, "fullname": "Bruce Yu", "user": "bruceyyu", "type": "user"}, "name": "Yue Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:16:27.115Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd6", "name": "Xing Zhu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd7", "name": "Yujun Shen", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd8", "name": "Hao Ouyang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64981bea09cea550852652af/HObcL400nFnYaw2kOcjor.mp4"], "publishedAt": "2026-01-28T12:37:01.000Z", "submittedOnDailyAt": "2026-01-29T00:09:39.166Z", "title": "Advancing Open-source World Models", "submittedOnDailyBy": {"_id": "64981bea09cea550852652af", "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg", "isPro": false, "fullname": "Qiuyu Wang", "user": "qiuyuu", "type": "user"}, "summary": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.", "upvotes": 66, "discussionId": "697ac48cdf3e800774f13bd9", "projectPage": "https://technology.robbyant.com/lingbot-world", "githubRepo": "https://github.com/Robbyant/lingbot-world/", "githubRepoAddedBy": "user", "ai_summary": "LingBot-World is an open-source world simulator with high-fidelity dynamics, long-term memory capabilities, and real-time interactivity for diverse environments.", "ai_keywords": ["world simulator", "video generation", "world model", "long-term memory", "real-time interactivity"], "githubStars": 756, "organization": {"_id": "69709f892cd08371c1011a2e", "name": "robbyant", "fullname": "Robbyant", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"}, "summary_zh": "<ul>\n    <li>LingBot-World \u662f\u4e00\u4e2a\u5f00\u653e\u6e90\u4ee3\u7801\u7684\u4e16\u754c\u6a21\u62df\u5668\uff0c\u57fa\u4e8e\u89c6\u9891\u751f\u6210\u6280\u672f\u3002</li>\n    <li>\u5b83\u80fd\u591f\u5728\u591a\u79cd\u73af\u5883\u4e2d\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u548c\u5f3a\u5927\u7684\u52a8\u6001\u6548\u679c\uff0c\u5305\u62ec\u73b0\u5b9e\u4e3b\u4e49\u3001\u79d1\u5b66\u80cc\u666f\u548c\u5361\u901a\u98ce\u683c\u7b49\u3002</li>\n    <li>\u8be5\u6a21\u62df\u5668\u652f\u6301\u5206\u949f\u7ea7\u7684\u89c6\u91ce\uff0c\u5e76\u80fd\u4fdd\u6301\u65f6\u95f4\u4e0a\u7684\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\uff0c\u5373\u201c\u957f\u671f\u8bb0\u5fc6\u201d\u3002</li>\n    <li>\u5b83\u5b9e\u73b0\u4e86\u5b9e\u65f6\u4ea4\u4e92\uff0c\u751f\u621016\u5e27\u6bcf\u79d2\u7684\u753b\u9762\u65f6\u5ef6\u8fdf\u4f4e\u4e8e1\u79d2\u3002</li>\n    <li>\u53d1\u5e03\u4ee3\u7801\u548c\u6a21\u578b\u65e8\u5728\u7f29\u5c0f\u5f00\u653e\u6e90\u548c\u5c01\u95ed\u6e90\u6280\u672f\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4fc3\u8fdb\u5185\u5bb9\u521b\u4f5c\u3001\u6e38\u620f\u548c\u673a\u5668\u4eba\u5b66\u4e60\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LingBot-World is an open-source world simulator that generates videos.</li>\n    <li>It features high-quality visuals and realistic dynamics in various styles, including realistic and cartoonish environments.</li>\n    <li>It has long-term memory, allowing it to keep track of context over time.</li>\n    <li>It can interact in real-time with a response time of less than 1 second while generating 16 frames per second.</li>\n    <li>The code and model are publicly available to help bridge the gap between open-source and closed-source technologies, benefiting fields like content creation, gaming, and robot learning.</li>\n</ul>"}, "publishedAt": "2026-01-28T07:37:01.000Z", "title": "Advancing Open-source World Models", "summary": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64981bea09cea550852652af/HObcL400nFnYaw2kOcjor.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20540.png", "numComments": 1, "submittedBy": {"_id": "64981bea09cea550852652af", "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg", "fullname": "Qiuyu Wang", "name": "qiuyuu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "69709f892cd08371c1011a2e", "name": "robbyant", "fullname": "Robbyant", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.19325", "authors": [{"_id": "69798298df44b75fa47e47a9", "name": "Zichen Wen", "hidden": false}, {"_id": "69798298df44b75fa47e47aa", "user": {"_id": "688c72c011ef3399b561dee7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/688c72c011ef3399b561dee7/puhgnTOAfZYetsC46hqGm.jpeg", "isPro": false, "fullname": "BoxueYang", "user": "Boxue", "type": "user"}, "name": "Boxue Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:17:12.982Z", "hidden": false}, {"_id": "69798298df44b75fa47e47ab", "name": "Shuang Chen", "hidden": false}, {"_id": "69798298df44b75fa47e47ac", "name": "Yaojie Zhang", "hidden": false}, {"_id": "69798298df44b75fa47e47ad", "name": "Yuhang Han", "hidden": false}, {"_id": "69798298df44b75fa47e47ae", "name": "Junlong Ke", "hidden": false}, {"_id": "69798298df44b75fa47e47af", "name": "Cong Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47b0", "name": "Yicheng Fu", "hidden": false}, {"_id": "69798298df44b75fa47e47b1", "name": "Jiawang Zhao", "hidden": false}, {"_id": "69798298df44b75fa47e47b2", "name": "Jiangchao Yao", "hidden": false}, {"_id": "69798298df44b75fa47e47b3", "name": "Xi Fang", "hidden": false}, {"_id": "69798298df44b75fa47e47b4", "name": "Zhen Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47b5", "name": "Henxing Cai", "hidden": false}, {"_id": "69798298df44b75fa47e47b6", "name": "Lin Yao", "hidden": false}, {"_id": "69798298df44b75fa47e47b7", "name": "Zhifeng Gao", "hidden": false}, {"_id": "69798298df44b75fa47e47b8", "name": "Yanhui Hong", "hidden": false}, {"_id": "69798298df44b75fa47e47b9", "name": "Nang Yuan", "hidden": false}, {"_id": "69798298df44b75fa47e47ba", "name": "Yixuan Li", "hidden": false}, {"_id": "69798298df44b75fa47e47bb", "name": "Guojiang Zhao", "hidden": false}, {"_id": "69798298df44b75fa47e47bc", "name": "Haoyi Tao", "hidden": false}, {"_id": "69798298df44b75fa47e47bd", "name": "Nan Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47be", "name": "Han Lyu", "hidden": false}, {"_id": "69798298df44b75fa47e47bf", "name": "Guolin Ke", "hidden": false}, {"_id": "69798298df44b75fa47e47c0", "name": "Ning Liao", "hidden": false}, {"_id": "69798298df44b75fa47e47c1", "name": "Xiaoxing Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47c2", "name": "Kai Chen", "hidden": false}, {"_id": "69798298df44b75fa47e47c3", "name": "Zhiyu Li", "hidden": false}, {"_id": "69798298df44b75fa47e47c4", "name": "Feiyu Xiong", "hidden": false}, {"_id": "69798298df44b75fa47e47c5", "name": "Sihan Hu", "hidden": false}, {"_id": "69798298df44b75fa47e47c6", "name": "Kun Chen", "hidden": false}, {"_id": "69798298df44b75fa47e47c7", "name": "Yanfeng Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47c8", "name": "Weinan E", "hidden": false}, {"_id": "69798298df44b75fa47e47c9", "name": "Linfeng Zhang", "hidden": false}, {"_id": "69798298df44b75fa47e47ca", "name": "Linfeng Zhang", "hidden": false}], "publishedAt": "2026-01-27T08:12:18.000Z", "submittedOnDailyAt": "2026-01-29T01:20:58.570Z", "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery", "submittedOnDailyBy": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "isPro": false, "fullname": "Zichen Wen", "user": "zichenwen", "type": "user"}, "summary": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.", "upvotes": 53, "discussionId": "69798298df44b75fa47e47cb", "projectPage": "https://innovatorlm.github.io/Innovator-VL", "githubRepo": "https://github.com/InnovatorLM/Innovator-VL", "githubRepoAddedBy": "user", "ai_summary": "Innovator-VL demonstrates that principled training design and transparent methodology can achieve strong scientific intelligence with reduced data requirements while maintaining general vision performance.", "ai_keywords": ["multimodal large language model", "scientific multimodal large language model", "end-to-end reproducible training pipeline", "supervised fine-tuning", "reinforcement learning", "scientific reasoning", "data efficiency", "principled data selection", "generalization", "scientific alignment"], "githubStars": 70, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>Innovator-VL \u662f\u4e00\u4e2a\u79d1\u5b66\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u5347\u5bf9\u4e0d\u540c\u79d1\u5b66\u9886\u57df\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u900f\u660e\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u63d0\u4f9b\u5b8c\u6574\u7684\u53ef\u590d\u73b0\u8bad\u7ec3\u6d41\u7a0b\uff0c\u65b9\u4fbf\u793e\u533a\u8fdb\u884c\u7cfb\u7edf\u6027\u6269\u5c55\u3002</li>\n    <li>Innovator-VL \u6570\u636e\u6548\u7387\u9ad8\uff0c\u4f7f\u7528\u4e0d\u5230\u4e94\u767e\u4e07\u4e2a\u6837\u672c\u5c31\u80fd\u5728\u5404\u79cd\u79d1\u5b66\u4efb\u52a1\u4e0a\u53d6\u5f97\u7ade\u4e89\u6027\u8868\u73b0\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u3002</li>\n    <li>\u6a21\u578b\u5728\u901a\u7528\u89c6\u89c9\u3001\u591a\u6a21\u6001\u63a8\u7406\u548c\u79d1\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u6ca1\u6709\u5927\u89c4\u6a21\u6570\u636e\uff0c\u4e5f\u80fd\u6784\u5efa\u9ad8\u6548\u3001\u53ef\u590d\u73b0\u7684\u79d1\u5b66\u591a\u6a21\u6001\u6a21\u578b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Innovator-VL is a new model that helps with understanding and reasoning in various scientific fields while also performing well in general vision tasks.</li>\n    <li>The model uses a clear and reproducible training method that allows others to build upon it easily.</li>\n    <li>It achieves good results using fewer than five million samples, showing that careful data selection is more effective than just using a lot of data.</li>\n    <li>Innovator-VL performs well in both scientific tasks and general vision tasks, showing it can handle multiple types of challenges without losing effectiveness.</li>\n    <li>This work shows that strong scientific models can be created efficiently without needing large amounts of data, paving the way for future research.</li>\n</ul>"}, "publishedAt": "2026-01-27T03:12:18.000Z", "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery", "summary": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19325.png", "numComments": 1, "submittedBy": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "fullname": "Zichen Wen", "name": "zichenwen", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.22153", "authors": [{"_id": "697c2899a67238fac88cc115", "user": {"_id": "63f47b5321eb234ab739e91a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg", "isPro": false, "fullname": "Haozhe Xie", "user": "hzxie", "type": "user"}, "name": "Haozhe Xie", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T13:31:48.996Z", "hidden": false}, {"_id": "697c2899a67238fac88cc116", "user": {"_id": "672392c4a4c4381cefc06416", "avatarUrl": "/avatars/8ee84a7e3e91e5d13074bc3c407ff75d.svg", "isPro": false, "fullname": "Wen Beichen", "user": "wenbc21", "type": "user"}, "name": "Beichen Wen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T13:31:52.487Z", "hidden": false}, {"_id": "697c2899a67238fac88cc117", "user": {"_id": "6899ff3f4c5ca50a326bb456", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Nuqof2ofdaQUD5b07cDnG.png", "isPro": false, "fullname": "Zheng Jiarui", "user": "zghtyarecrenj", "type": "user"}, "name": "Jiarui Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T13:31:46.030Z", "hidden": false}, {"_id": "697c2899a67238fac88cc118", "name": "Zhaoxi Chen", "hidden": false}, {"_id": "697c2899a67238fac88cc119", "name": "Fangzhou Hong", "hidden": false}, {"_id": "697c2899a67238fac88cc11a", "name": "Haiwen Diao", "hidden": false}, {"_id": "697c2899a67238fac88cc11b", "name": "Ziwei Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/p9cPxETttQUS23woFb14M.mp4"], "publishedAt": "2026-01-29T18:59:51.000Z", "submittedOnDailyAt": "2026-01-30T01:46:39.673Z", "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation", "submittedOnDailyBy": {"_id": "63f47b5321eb234ab739e91a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg", "isPro": false, "fullname": "Haozhe Xie", "user": "hzxie", "type": "user"}, "summary": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.", "upvotes": 45, "discussionId": "697c2899a67238fac88cc11c", "projectPage": "https://haozhexie.com/project/dynamic-vla", "githubRepo": "https://github.com/hzxie/DynamicVLA", "githubRepoAddedBy": "user", "ai_summary": "DynamicVLA addresses dynamic object manipulation challenges through a compact vision-language-action model with temporal reasoning and closed-loop adaptation, supported by a new benchmark for dynamic manipulation tasks.", "ai_keywords": ["Vision-Language-Action models", "temporal reasoning", "closed-loop adaptation", "convolutional vision encoder", "multimodal inference", "Continuous Inference", "Latent-aware Action Streaming", "Dynamic Object Manipulation benchmark", "synthetic episodes", "real-world episodes"], "githubStars": 48, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "summary_zh": "<ul>\n    <li>\u52a8\u6001\u7269\u4f53\u64cd\u63a7\u4ecd\u7136\u662f\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u9762\u4e34\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u573a\u666f\u4e2d\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DynamicVLA\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u4e09\u9879\u5173\u952e\u8bbe\u8ba1\u63d0\u9ad8\u52a8\u6001\u7269\u4f53\u64cd\u63a7\u80fd\u529b\u3002</li>\n    <li>\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u7d27\u51d1\u76840.4B VLA\u548c\u8fde\u7eed\u63a8\u7406\u529f\u80fd\uff0c\u4ee5\u964d\u4f4e\u5ef6\u8fdf\u5e76\u9002\u5e94\u7269\u4f53\u8fd0\u52a8\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86\u52a8\u6001\u7269\u4f53\u64cd\u63a7\uff08DOM\uff09\u57fa\u51c6\uff0c\u6536\u96c6\u4e86200K\u5408\u6210\u6848\u4f8b\u548c2000\u4e2a\u771f\u5b9e\u6848\u4f8b\uff0c\u4ee5\u652f\u6301\u7814\u7a76\u3002</li>\n    <li>\u8bc4\u4f30\u663e\u793aDynamicVLA\u5728\u54cd\u5e94\u901f\u5ea6\u3001\u611f\u77e5\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u52a8\u6001\u7269\u4f53\u64cd\u63a7\u573a\u666f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DynamicVLA is a new framework designed to improve how machines manipulate moving objects.</li>\n    <li>It combines fast visual processing, continuous reasoning, and better action timing for quicker responses.</li>\n    <li>The framework includes a new benchmark called Dynamic Object Manipulation (DOM) that provides a large dataset for training, consisting of 200,000 synthetic and 2,000 real-world episodes.</li>\n    <li>DynamicVLA shows significant improvements in speed, perception, and adaptability in dynamic environments.</li>\n    <li>This framework aims to unify various methods for dealing with dynamic object manipulation challenges.</li>\n</ul>"}, "publishedAt": "2026-01-29T13:59:51.000Z", "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation", "summary": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/p9cPxETttQUS23woFb14M.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22153.png", "numComments": 2, "submittedBy": {"_id": "63f47b5321eb234ab739e91a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg", "fullname": "Haozhe Xie", "name": "hzxie", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 19, "isUserFollowing": false}, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.21558", "authors": [{"_id": "697c279ea67238fac88cc104", "user": {"_id": "621499d72be42a56cca7afad", "avatarUrl": "/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg", "isPro": false, "fullname": "TianXiaoyu", "user": "Emperorizzis", "type": "user"}, "name": "Xiaoyu Tian", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T17:00:01.968Z", "hidden": false}, {"_id": "697c279ea67238fac88cc105", "name": "Haotian Wang", "hidden": false}, {"_id": "697c279ea67238fac88cc106", "name": "Shuaiting Chen", "hidden": false}, {"_id": "697c279ea67238fac88cc107", "name": "Hao Zhou", "hidden": false}, {"_id": "697c279ea67238fac88cc108", "name": "Kaichi Yu", "hidden": false}, {"_id": "697c279ea67238fac88cc109", "name": "Yudian Zhang", "hidden": false}, {"_id": "697c279ea67238fac88cc10a", "user": {"_id": "690189db2b5a1d242306b77f", "avatarUrl": "/avatars/b0297c57395029da2725758671952bb6.svg", "isPro": false, "fullname": "jade ouyang", "user": "jade0101", "type": "user"}, "name": "Jade Ouyang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T16:59:59.965Z", "hidden": false}, {"_id": "697c279ea67238fac88cc10b", "name": "Junxi Yin", "hidden": false}, {"_id": "697c279ea67238fac88cc10c", "name": "Jiong Chen", "hidden": false}, {"_id": "697c279ea67238fac88cc10d", "name": "Baoyan Guo", "hidden": false}, {"_id": "697c279ea67238fac88cc10e", "name": "Lei Zhang", "hidden": false}, {"_id": "697c279ea67238fac88cc10f", "name": "Junjie Tao", "hidden": false}, {"_id": "697c279ea67238fac88cc110", "name": "Yuansheng Song", "hidden": false}, {"_id": "697c279ea67238fac88cc111", "name": "Ming Cui", "hidden": false}, {"_id": "697c279ea67238fac88cc112", "name": "Chengwei Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/621499d72be42a56cca7afad/C3KRF5SVsqevuZy7nGJfm.png"], "publishedAt": "2026-01-29T11:22:23.000Z", "submittedOnDailyAt": "2026-02-02T00:08:03.322Z", "title": "ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas", "submittedOnDailyBy": {"_id": "621499d72be42a56cca7afad", "avatarUrl": "/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg", "isPro": false, "fullname": "TianXiaoyu", "user": "Emperorizzis", "type": "user"}, "summary": "Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.", "upvotes": 45, "discussionId": "697c279fa67238fac88cc113", "githubRepo": "https://github.com/LianjiaTech/astra", "githubRepoAddedBy": "user", "ai_summary": "ASTRA is an automated framework that trains tool-augmented language models using synthetic data and verifiable reinforcement learning to improve multi-step decision-making capabilities.", "ai_keywords": ["tool-call graphs", "trajectory-level rewards", "supervised fine-tuning", "reinforcement learning", "agent training", "multi-step decision making", "verifiable environments", "compositional topology", "semantic reasoning", "tool-augmented language models"], "githubStars": 84, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u6b65\u9aa4\u51b3\u7b56\u4e2d\u9010\u6e10\u88ab\u7528\u4f5c\u589e\u5f3a\u5de5\u5177\u7684\u667a\u80fd\u4f53\uff0c\u4f46\u8bad\u7ec3\u8fd9\u4e9b\u667a\u80fd\u4f53\u4ecd\u7136\u5b58\u5728\u6311\u6218\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u4eba\u5de5\u5e72\u9884\uff0c\u4f9d\u8d56\u4e0d\u53ef\u9a8c\u8bc1\u7684\u6a21\u62df\u73af\u5883\uff0c\u5e76\u4e14\u96be\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u7684\u957f\u671f\u5b66\u4e60\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86ASTRA\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u81ea\u52a8\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u53ef\u6269\u5c55\u6570\u636e\u5408\u6210\u548c\u53ef\u9a8c\u8bc1\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u589e\u5f3a\u5de5\u5177\u7684\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u3002</li>\n    <li>ASTRA\u7ed3\u5408\u4e86\u4e24\u5927\u7ec4\u4ef6\uff1a\u4e00\u662f\u5229\u7528\u5de5\u5177\u8c03\u7528\u56fe\u7684\u9759\u6001\u62d3\u6251\u5408\u6210\u591a\u6837\u5316\u7684\u8f68\u8ff9\uff0c\u4e8c\u662f\u73af\u5883\u5408\u6210\u6846\u67b6\u5c06\u95ee\u9898-\u7b54\u6848\u8f68\u8ff9\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u548c\u53ef\u9a8c\u8bc1\u7684\u73af\u5883\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cASTRA\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63a5\u8fd1\u5c01\u95ed\u6e90\u7cfb\u7edf\uff0c\u540c\u65f6\u4fdd\u6301\u6838\u5fc3\u63a8\u7406\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models are used for complex decision making but training them to use tools effectively is difficult.</li>\n    <li>Current methods need manual help, work in unverified settings, and struggle with long-term learning.</li>\n    <li>ASTRA is a new automated framework that helps train these models using synthesized data and verified reinforcement learning.</li>\n    <li>It combines two parts: one that creates diverse training scenarios and another that builds environments for testing reasoning skills.</li>\n    <li>ASTRA shows strong performance in benchmarks and offers resources for others to use at their GitHub page.</li>\n</ul>"}, "publishedAt": "2026-01-29T06:22:23.000Z", "title": "ASTRA: Automated Synthesis of agentic Trajectories and Reinforcement Arenas", "summary": "Large language models (LLMs) are increasingly used as tool-augmented agents for multi-step decision making, yet training robust tool-using agents remains challenging. Existing methods still require manual intervention, depend on non-verifiable simulated environments, rely exclusively on either supervised fine-tuning (SFT) or reinforcement learning (RL), and struggle with stable long-horizon, multi-turn learning. To address these challenges, we introduce ASTRA, a fully automated end-to-end framework for training tool-augmented language model agents via scalable data synthesis and verifiable reinforcement learning. ASTRA integrates two complementary components. First, a pipeline that leverages the static topology of tool-call graphs synthesizes diverse, structurally grounded trajectories, instilling broad and transferable tool-use competence. Second, an environment synthesis framework that captures the rich, compositional topology of human semantic reasoning converts decomposed question-answer traces into independent, code-executable, and rule-verifiable environments, enabling deterministic multi-turn RL. Based on this method, we develop a unified training methodology that integrates SFT with online RL using trajectory-level rewards to balance task completion and interaction efficiency. Experiments on multiple agentic tool-use benchmarks demonstrate that ASTRA-trained models achieve state-of-the-art performance at comparable scales, approaching closed-source systems while preserving core reasoning ability. We release the full pipelines, environments, and trained models at https://github.com/LianjiaTech/astra.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/621499d72be42a56cca7afad/C3KRF5SVsqevuZy7nGJfm.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21558.png", "numComments": 3, "submittedBy": {"_id": "621499d72be42a56cca7afad", "avatarUrl": "/avatars/3cea70d1a55c8096b4270093c69e4a5e.svg", "fullname": "TianXiaoyu", "name": "Emperorizzis", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.21639", "authors": [{"_id": "697c5270a67238fac88cc226", "user": {"_id": "693f91d7ed7d40c019934508", "avatarUrl": "/avatars/0d73f098627c9ebd2ae7d90e693a34f6.svg", "isPro": false, "fullname": "Yufeng Zhong", "user": "Albert-Zhong", "type": "user"}, "name": "Yufeng Zhong", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T09:35:23.719Z", "hidden": false}, {"_id": "697c5270a67238fac88cc227", "name": "Lei Chen", "hidden": false}, {"_id": "697c5270a67238fac88cc228", "name": "Xuanle Zhao", "hidden": false}, {"_id": "697c5270a67238fac88cc229", "name": "Wenkang Han", "hidden": false}, {"_id": "697c5270a67238fac88cc22a", "name": "Liming Zheng", "hidden": false}, {"_id": "697c5270a67238fac88cc22b", "name": "Jing Huang", "hidden": false}, {"_id": "697c5270a67238fac88cc22c", "name": "Deyang Jiang", "hidden": false}, {"_id": "697c5270a67238fac88cc22d", "name": "Yilin Cao", "hidden": false}, {"_id": "697c5270a67238fac88cc22e", "name": "Lin Ma", "hidden": false}, {"_id": "697c5270a67238fac88cc22f", "name": "Zhixiong Zeng", "hidden": false}], "publishedAt": "2026-01-29T12:43:02.000Z", "submittedOnDailyAt": "2026-01-30T04:33:06.261Z", "title": "OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models", "submittedOnDailyBy": {"_id": "6572cbc42bb242937c0a1101", "avatarUrl": "/avatars/f2af45e6b242aa47578fe3f60e97ca86.svg", "isPro": false, "fullname": "Xuanle Zhao", "user": "xxxllz", "type": "user"}, "summary": "The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (Text-centric OCR), neglecting the identification of visual elements from visually information-dense image sources (Vision-centric OCR), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose OCRVerse, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.", "upvotes": 41, "discussionId": "697c5270a67238fac88cc230", "githubRepo": "https://github.com/DocTron-hub/OCRVerse", "githubRepoAddedBy": "user", "ai_summary": "OCRVerse is a novel end-to-end OCR method that unifies text-centric and vision-centric approaches through comprehensive data engineering and a two-stage SFT-RL training framework with domain-specific reward strategies.", "ai_keywords": ["OCR", "vision-centric OCR", "text-centric OCR", "end-to-end OCR", "data engineering", "SFT-RL training", "cross-domain training", "reward strategies", "domain-specific customization", "cross-domain fusion"], "githubStars": 13, "summary_zh": "<ul>\n    <li>OCR\u6280\u672f\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u89c6\u89c9\u56fe\u50cf\u4e2d\u7684\u4fe1\u606f\u63d0\u53d6\u65b9\u9762\u3002</li>\n    <li>\u73b0\u6709\u7684OCR\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u6587\u672c\u8bc6\u522b\uff0c\u5ffd\u89c6\u4e86\u4ece\u4fe1\u606f\u5bc6\u96c6\u7684\u89c6\u89c9\u56fe\u50cf\u4e2d\u8bc6\u522b\u89c6\u89c9\u5143\u7d20\u3002</li>\n    <li>\u63d0\u51fa\u4e86OCRVerse\uff0c\u8fd9\u662f\u9996\u4e2a\u7edf\u4e00\u6587\u672c\u4e2d\u5fc3\u548c\u89c6\u89c9\u4e2d\u5fc3OCR\u7684\u6574\u4f53\u65b9\u6cd5\u3002</li>\n    <li>\u6784\u5efa\u4e86\u5168\u9762\u7684\u6570\u636e\u5de5\u7a0b\uff0c\u6db5\u76d6\u4e86\u5404\u79cd\u6587\u672c\u548c\u89c6\u89c9\u6587\u6863\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOCRVerse\u5728\u6587\u672c\u548c\u89c6\u89c9\u6570\u636e\u7c7b\u578b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u6027\u80fd\u4e0d\u900a\u8272\u4e8e\u5927\u578b\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The demand for managing multimodal data is increasing due to large vision language models.</li>\n    <li>Current OCR methods mainly focus on extracting text from images, ignoring other visual elements.</li>\n    <li>OCRVerse is a new OCR method that combines text recognition and visual element identification.</li>\n    <li>It uses a two-stage training method to adapt to different types of documents and images.</li>\n    <li>Experiments show that OCRVerse performs well, rivaling both open-source and closed-source models.</li>\n</ul>"}, "publishedAt": "2026-01-29T07:43:02.000Z", "title": "OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models", "summary": "The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (Text-centric OCR), neglecting the identification of visual elements from visually information-dense image sources (Vision-centric OCR), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose OCRVerse, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21639.png", "numComments": 2, "submittedBy": {"_id": "6572cbc42bb242937c0a1101", "avatarUrl": "/avatars/f2af45e6b242aa47578fe3f60e97ca86.svg", "fullname": "Xuanle Zhao", "name": "xxxllz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.22813", "authors": [{"_id": "6980c289020e48d648c5d3ca", "user": {"_id": "623753b5eddd7763adc9346a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/-iLaQXQ1FOuJr0sfqulBA.jpeg", "isPro": false, "fullname": "Andrei Panferov", "user": "BlackSamorez", "type": "user"}, "name": "Andrei Panferov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T16:52:03.673Z", "hidden": false}, {"_id": "6980c289020e48d648c5d3cb", "name": "Erik Schultheis", "hidden": false}, {"_id": "6980c289020e48d648c5d3cc", "user": {"_id": "632a2e325f2ff1958c0103be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632a2e325f2ff1958c0103be/Tb0ql9e4LcaFktTK1hzqe.jpeg", "isPro": false, "fullname": "Rush Tabesh", "user": "soroushtabesh", "type": "user"}, "name": "Soroush Tabesh", "status": "claimed_verified", "statusLastChangedAt": "2026-02-02T15:43:33.368Z", "hidden": false}, {"_id": "6980c289020e48d648c5d3cd", "name": "Dan Alistarh", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/623753b5eddd7763adc9346a/ABDboVhzjTdRyUdiu_quW.png"], "publishedAt": "2026-01-30T10:39:11.000Z", "submittedOnDailyAt": "2026-02-02T13:00:36.911Z", "title": "Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation", "submittedOnDailyBy": {"_id": "623753b5eddd7763adc9346a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/-iLaQXQ1FOuJr0sfqulBA.jpeg", "isPro": false, "fullname": "Andrei Panferov", "user": "BlackSamorez", "type": "user"}, "summary": "The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .", "upvotes": 41, "discussionId": "6980c289020e48d648c5d3ce", "githubRepo": "https://github.com/IST-DASLab/Quartet-II", "githubRepoAddedBy": "user", "ai_summary": "Quantized training method Quartet II improves NVFP4 format utilization for large language model pre-training through enhanced gradient estimation and faster GPU execution.", "ai_keywords": ["NVFP4", "stochastic rounding", "quantized training", "micro-scaled formats", "MS-EDEN", "Quartet II", "linear layers", "gradient estimation", "matrix multiplications", "LLM training", "NVIDIA Blackwell GPUs"], "githubStars": 5, "organization": {"_id": "64d0ffde9cff738203a50e9b", "name": "ISTA-DASLab", "fullname": " IST Austria Distributed Algorithms and Systems Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/628e0ce4e53bbd334577fcb0/TRPtgtSavYjDJOK3S1I8M.png"}, "summary_zh": "<ul>\n    <li>NVIDIA\u7684Blackwell GPU\u652f\u6301NVFP4\u4f4e\u7cbe\u5ea6\u683c\u5f0f\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5927\u89c4\u6a21\u6a21\u578b\u7684\u5b8c\u5168\u91cf\u5316\u9884\u8bad\u7ec3\u3002</li>\n    <li>\u73b0\u6709\u7684\u91cf\u5316\u8bad\u7ec3\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u4e0a\u6709\u6240\u727a\u7272\uff0c\u4f7f\u7528\u968f\u673a\u820d\u5165\uff08SR\uff09\u5bfc\u81f4\u4e0e\u6807\u51c6FP16\u548cFP8\u8bad\u7ec3\u76f8\u6bd4\u51c6\u786e\u6027\u4e0b\u964d\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u504f\u91cf\u5316\u65b9\u6cd5MS-EDEN\uff0c\u91cf\u5316\u8bef\u5dee\u6bd4SR\u4f4e\u8d85\u8fc72\u500d\u3002</li>\n    <li>\u6211\u4eec\u5c06MS-EDEN\u6574\u5408\u8fdb\u4e00\u79cd\u65b0\u7684\u5b8c\u5168NVFP4\u91cf\u5316\u65b9\u6848Quartet II\uff0c\u63d0\u5347\u4e86\u77e9\u9635\u4e58\u6cd5\u7684\u68af\u5ea6\u4f30\u8ba1\u51c6\u786e\u6027\u3002</li>\n    <li>Quartet II\u5728\u8bad\u7ec3\u6700\u5927\u4e3a1.9B\u53c2\u6570\u7684LLM\u65f6\uff0c\u63d0\u4f9b\u4e86\u6bd4BF16\u5feb4.2\u500d\u7684\u6267\u884c\u901f\u5ea6\uff0c\u4ee3\u7801\u53ef\u5728GitHub\u4e0a\u83b7\u53d6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NVIDIA Blackwell GPUs support a new lower-precision format called NVFP4, which allows for fully quantized pre-training of large models.</li>\n    <li>Current methods for quantized training lose some accuracy due to using stochastic rounding for gradient estimation.</li>\n    <li>This paper introduces a new method called MS-EDEN, which reduces quantization error significantly compared to stochastic rounding.</li>\n    <li>The authors present a new quantization scheme called Quartet II that improves gradient estimation for matrix multiplications in NVFP4.</li>\n    <li>Quartet II shows better performance on large language model training and offers up to 4.2x speedup on NVIDIA Blackwell GPUs.</li>\n</ul>"}, "publishedAt": "2026-01-30T05:39:11.000Z", "title": "Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation", "summary": "The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/623753b5eddd7763adc9346a/ABDboVhzjTdRyUdiu_quW.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22813.png", "numComments": 1, "submittedBy": {"_id": "623753b5eddd7763adc9346a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/-iLaQXQ1FOuJr0sfqulBA.jpeg", "fullname": "Andrei Panferov", "name": "BlackSamorez", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 41, "isUserFollowing": false}, "organization": {"_id": "64d0ffde9cff738203a50e9b", "name": "ISTA-DASLab", "fullname": " IST Austria Distributed Algorithms and Systems Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/628e0ce4e53bbd334577fcb0/TRPtgtSavYjDJOK3S1I8M.png"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u4e3a\u4e86\u63d0\u9ad8\u89c6\u9891\u95ee\u7b54\u7684\u51c6\u786e\u6027\uff0c\u7814\u7a76\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0c\u79f0\u4e3aVideoDR\u3002</li>\n    <li>VideoDR\u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u4e0b\u7684\u5f00\u653e\u9886\u57df\u89c6\u9891\u95ee\u7b54\uff0c\u9700\u8981\u63d0\u53d6\u8de8\u5e27\u89c6\u89c9\u7ebf\u7d22\u548c\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u3002</li>\n    <li>\u901a\u8fc7\u4e25\u683c\u7684\u4eba\u4e3a\u6807\u6ce8\u548c\u8d28\u91cf\u63a7\u5236\uff0c\u83b7\u5f97\u4e86\u6db5\u76d6\u516d\u4e2a\u8bed\u4e49\u9886\u57df\u7684\u9ad8\u8d28\u91cf\u6837\u672c\u3002</li>\n    <li>\u5728\u8bc4\u4f30\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u53d1\u73b0\u201cAgentic\u201d\u65b9\u6cd5\u5e76\u4e0d\u603b\u662f\u4f18\u4e8e\u201cWorkflow\u201d\uff0c\u5176\u6548\u679c\u53d6\u51b3\u4e8e\u6a21\u578b\u4fdd\u6301\u89c6\u9891\u7ebf\u7d22\u7684\u80fd\u529b\u3002</li>\n    <li>\u7814\u7a76\u6307\u51fa\uff0c\u76ee\u6807\u6f02\u79fb\u548c\u957f\u65f6\u95f4\u4e00\u81f4\u6027\u662f\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u7684\u6838\u5fc3\u74f6\u9888\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Video question answering often relies on visual clues from videos and requires answers from the web.</li>\n  <li>To address this, a new benchmark called VideoDR has been created for video-based research.</li>\n  <li>VideoDR involves finding visual cues, retrieving information from the web, and reasoning across multiple sources.</li>\n  <li>The study evaluates several large language models and compares two approaches: Workflow and Agentic.</li>\n  <li>Challenges identified include maintaining focus on video clues and consistency over long retrieval processes.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u4eba\u7c7b\u5728\u638c\u63e1\u8bed\u8a00\u4e4b\u524d\u5c31\u80fd\u53d1\u5c55\u57fa\u672c\u7684\u89c6\u89c9\u6280\u80fd\uff0c\u4f46\u73b0\u4ee3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4ecd\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u6765\u5f25\u8865\u89c6\u89c9\u7406\u89e3\u7684\u4e0d\u8db3\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684MLLMs\u5728\u57fa\u7840\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u751a\u81f33\u5c81\u7684\u5b69\u5b50\u4e5f\u80fd\u8f7b\u677e\u5b8c\u6210\u8fd9\u4e9b\u4efb\u52a1\u3002</li>\n    <li>\u4e3a\u6b64\uff0c\u6211\u4eec\u63a8\u51fa\u4e86BabyVision\uff0c\u4e00\u4e2a\u8bc4\u4f30MLLMs\u6838\u5fc3\u89c6\u89c9\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b388\u4e2a\u9879\u76ee\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\u548c\u56db\u4e2a\u4e3b\u8981\u7c7b\u522b\u3002</li>\n    <li>\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u9886\u5148\u7684MLLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5f97\u5206\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\uff0cGemini3-Pro-Preview\u7684\u5f97\u5206\u4e3a49.7\uff0c\u8fdc\u4f4e\u4e8e6\u5c81\u513f\u7ae5\u7684\u6c34\u5e73\u548c94.1\u7684\u6210\u5e74\u5e73\u5747\u5206\u3002</li>\n    <li>BabyVision\u7684\u8fdb\u5c55\u6807\u5fd7\u7740\u5411\u4eba\u7c7b\u6c34\u5e73\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u8fc8\u8fdb\u4e86\u4e00\u6b65\uff0c\u540c\u65f6\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86BabyVision-Gen\u548c\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\u5305\u6765\u89e3\u51b3\u89c6\u89c9\u63a8\u7406\u95ee\u9898\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans learn basic visual skills before language, but modern Multimodal LLMs (MLLMs) depend on language to understand visuals.</li>\n    <li>Research shows that top MLLMs struggle with simple visual tasks that even young children can easily do.</li>\n    <li>BabyVision is a new benchmark created to test visual abilities without relying on language, featuring 388 tasks in 22 subclasses.</li>\n    <li>Results show that leading MLLMs score much lower than humans, with Gemini3-Pro-Preview scoring 49.7 compared to an average adult score of 94.1.</li>\n    <li>BabyVision aims to improve MLLMs' visual perception and reasoning skills, and additional resources are available for public use.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u4e30\u5bcc\u7684\u8bed\u4e49\u5b9e\u4f53\uff0c\u536b\u661f\u56fe\u50cf\u4e2d\u5206\u5272\u8fd9\u4e9b\u5b9e\u4f53\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u76ee\u524d\u7684\u5206\u5272\u6a21\u578b\u80fd\u5904\u7406\u7269\u7406\u5c5e\u6027\u7684\u5b9e\u4f53\uff0c\u4f46\u5bf9\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\uff08\u5982\u5b66\u6821\u3001\u516c\u56ed\uff09\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6SocioSeg\uff0c\u5305\u62ec\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u6846\u67b6SocioReasoner\uff0c\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u6807\u6ce8\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u4e14\u5177\u5907\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban areas have many different entities, and identifying them from satellite images is important for various applications.</li>\n    <li>Current models can segment physical features like buildings, but struggle with socially defined categories such as schools and parks.</li>\n    <li>We created a new dataset called SocioSeg that includes satellite images, maps, and detailed labels for social entities.</li>\n    <li>We developed a new framework, SocioReasoner, which uses vision-language reasoning to identify and label these social entities more effectively.</li>\n    <li>Our experiments show that this approach outperforms existing models and can generalize well to new cases.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.16725", "authors": [{"_id": "6976d5405d41524304c13537", "name": "Meituan LongCat Team", "hidden": false}, {"_id": "6976d5405d41524304c13538", "name": "Anchun Gui", "hidden": false}, {"_id": "6976d5405d41524304c13539", "name": "Bei Li", "hidden": false}, {"_id": "6976d5405d41524304c1353a", "name": "Bingyang Tao", "hidden": false}, {"_id": "6976d5405d41524304c1353b", "name": "Bole Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1353c", "name": "Borun Chen", "hidden": false}, {"_id": "6976d5405d41524304c1353e", "name": "Chao Zhang", "hidden": false}, {"_id": "69772bc15d41524304c13739", "name": "Chao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1353f", "name": "Chen Gao", "hidden": false}, {"_id": "6976d5405d41524304c13540", "name": "Chen Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13541", "name": "Chengcheng Han", "hidden": false}, {"_id": "6976d5405d41524304c13542", "name": "Chenhui Yang", "hidden": false}, {"_id": "6976d5405d41524304c13543", "name": "Chuyu Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13544", "name": "Cong Chen", "hidden": false}, {"_id": "6976d5405d41524304c13545", "name": "Cunguang Wang", "hidden": false}, {"_id": "6976d5405d41524304c13546", "name": "Daoru Pan", "hidden": false}, {"_id": "6976d5405d41524304c13547", "name": "Defei Bu", "hidden": false}, {"_id": "6976d5405d41524304c13548", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13549", "name": "Di Xiu", "hidden": false}, {"_id": "6976d5405d41524304c1354a", "name": "Dishan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1354b", "name": "Dongyu Ru", "hidden": false}, {"_id": "6976d5405d41524304c1354c", "name": "Dunwei Tu", "hidden": false}, {"_id": "6976d5405d41524304c1354d", "name": "Fan Wu", "hidden": false}, {"_id": "6976d5405d41524304c1354e", "name": "Fengcheng Yuan", "hidden": false}, {"_id": "6976d5405d41524304c1354f", "name": "Fengcun Li", "hidden": false}, {"_id": "6976d5405d41524304c13550", "name": "Gang Xu", "hidden": false}, {"_id": "6976d5405d41524304c13551", "name": "Guanyu Wu", "hidden": false}, {"_id": "6976d5405d41524304c13552", "name": "Guoyuan Lin", "hidden": false}, {"_id": "6976d5405d41524304c13553", "name": "Haibin Wang", "hidden": false}, {"_id": "6976d5405d41524304c13554", "name": "Hansi Yang", "hidden": false}, {"_id": "6976d5405d41524304c13555", "name": "Hao Yang", "hidden": false}, {"_id": "6976d5405d41524304c13556", "name": "Haonan Yan", "hidden": false}, {"_id": "6976d5405d41524304c13557", "name": "Haoxiang Ma", "hidden": false}, {"_id": "6976d5405d41524304c13558", "name": "Haoxing Wen", "hidden": false}, {"_id": "6976d5405d41524304c13559", "name": "Hongyan Hao", "hidden": false}, {"_id": "6976d5405d41524304c1355a", "name": "Hongyin Tang", "hidden": false}, {"_id": "6976d5405d41524304c1355b", "name": "Hongyu Zang", "hidden": false}, {"_id": "6976d5405d41524304c1355c", "name": "Hongzhi Ni", "hidden": false}, {"_id": "6976d5405d41524304c1355d", "name": "Hui Su", "hidden": false}, {"_id": "6976d5405d41524304c1355e", "name": "Jiacheng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1355f", "name": "Jiahong Zhou", "hidden": false}, {"_id": "6976d5405d41524304c13560", "name": "Jiahuan Li", "hidden": false}, {"_id": "6976d5405d41524304c13561", "name": "Jiaming Wang", "hidden": false}, {"_id": "6976d5405d41524304c13562", "name": "Jian Yang", "hidden": false}, {"_id": "6976d5405d41524304c13563", "user": {"_id": "64008a0af4ff62c2616d8858", "avatarUrl": "/avatars/b52c98857916fba5377ace8089d658b2.svg", "isPro": false, "fullname": "zhangjf", "user": "zhangjf", "type": "user"}, "name": "Jianfei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:09.272Z", "hidden": false}, {"_id": "6976d5405d41524304c13564", "name": "Jianhao Xu", "hidden": false}, {"_id": "6976d5405d41524304c13565", "name": "Jianing Wang", "hidden": false}, {"_id": "6976d5405d41524304c13566", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13567", "name": "Jiaqi Sun", "hidden": false}, {"_id": "6976d5405d41524304c13568", "name": "Jiarong Shi", "hidden": false}, {"_id": "6976d5405d41524304c13569", "name": "Jiarui Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1356a", "name": "Jingang Wang", "hidden": false}, {"_id": "6976d5405d41524304c1356b", "user": {"_id": "6592472fccbc1e2cc7250903", "avatarUrl": "/avatars/6f04ae66944eb2ce65c5aca7927bab10.svg", "isPro": false, "fullname": "Jinluan Yang", "user": "Jinluan", "type": "user"}, "name": "Jinluan Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:47.175Z", "hidden": false}, {"_id": "6976d5405d41524304c1356c", "name": "Jinrui Ding", "hidden": false}, {"_id": "6976d5405d41524304c1356d", "name": "Jinwei Xiao", "hidden": false}, {"_id": "6976d5405d41524304c1356e", "name": "Jiyuan He", "hidden": false}, {"_id": "6976d5405d41524304c1356f", "name": "Juncan Xu", "hidden": false}, {"_id": "6976d5405d41524304c13570", "name": "Kefeng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13571", "name": "Keheng Wang", "hidden": false}, {"_id": "6976d5405d41524304c13572", "name": "Li Wei", "hidden": false}, {"_id": "6976d5405d41524304c13573", "name": "Lianhui Ma", "hidden": false}, {"_id": "6976d5405d41524304c13574", "name": "Lin Qiu", "hidden": false}, {"_id": "6976d5405d41524304c13575", "name": "Lingbing Kong", "hidden": false}, {"_id": "6976d5405d41524304c13576", "name": "Lingchuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c13577", "name": "Linsen Guo", "hidden": false}, {"_id": "6976d5405d41524304c13578", "name": "Mengshen Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13579", "name": "Mengxia Shen", "hidden": false}, {"_id": "6976d5405d41524304c1357a", "name": "Mingyang Zhu", "hidden": false}, {"_id": "6976d5405d41524304c1357b", "name": "Peiguang Li", "hidden": false}, {"_id": "6976d5405d41524304c1357c", "name": "Peng Pei", "hidden": false}, {"_id": "6976d5405d41524304c1357d", "name": "Pengcheng Jia", "hidden": false}, {"_id": "6976d5405d41524304c1357e", "name": "Pengtao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1357f", "name": "Peng Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13580", "name": "Qi Gu", "hidden": false}, {"_id": "6976d5405d41524304c13581", "name": "Qiong Huang", "hidden": false}, {"_id": "6976d5405d41524304c13582", "name": "Qiyuan Duan", "hidden": false}, {"_id": "6976d5405d41524304c13583", "name": "Quanchi Weng", "hidden": false}, {"_id": "6976d5405d41524304c13584", "name": "Rongxiang Weng", "hidden": false}, {"_id": "6976d5405d41524304c13585", "name": "Rongzhi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13586", "name": "Rumei Li", "hidden": false}, {"_id": "6976d5405d41524304c13587", "name": "Shanglin Lei", "hidden": false}, {"_id": "6976d5405d41524304c13588", "user": {"_id": "64db5f5dd68a6ddcc7bd89e9", "avatarUrl": "/avatars/69375ec915927b855813df8a6d486837.svg", "isPro": false, "fullname": "Shengnan An", "user": "ShengnanAn", "type": "user"}, "name": "Shengnan An", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:11.410Z", "hidden": false}, {"_id": "6976d5405d41524304c13589", "name": "Shijun Dai", "hidden": false}, {"_id": "6976d5405d41524304c1358a", "name": "Shuaikang Liu", "hidden": false}, {"_id": "6976d5405d41524304c1358b", "name": "Shuang Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1358c", "name": "Shuo Wang", "hidden": false}, {"_id": "6976d5405d41524304c1358d", "name": "Songyuan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1358e", "name": "Tao Liang", "hidden": false}, {"_id": "6976d5405d41524304c1358f", "name": "Tianhao Hu", "hidden": false}, {"_id": "6976d5405d41524304c13590", "name": "Tianze Chen", "hidden": false}, {"_id": "6976d5405d41524304c13591", "name": "Wei Liu", "hidden": false}, {"_id": "6976d5405d41524304c13592", "name": "Wei Shi", "hidden": false}, {"_id": "6976d5405d41524304c13593", "name": "Wei Wang", "hidden": false}, {"_id": "6976d5405d41524304c13594", "name": "Weifeng Tang", "hidden": false}, {"_id": "6976d5405d41524304c13595", "name": "Wenjie Shi", "hidden": false}, {"_id": "6976d5405d41524304c13596", "name": "Wenlong Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13597", "name": "Wentao Chen", "hidden": false}, {"_id": "6976d5405d41524304c13598", "name": "Wentao Shi", "hidden": false}, {"_id": "6976d5405d41524304c13599", "name": "Xi Su", "hidden": false}, {"_id": "6976d5405d41524304c1359a", "name": "Xiangcheng Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359b", "name": "Xiandi Ma", "hidden": false}, {"_id": "6976d5405d41524304c1359c", "user": {"_id": "63edb098679c2cc40abc6c2e", "avatarUrl": "/avatars/288c7229937c2c3f29fda6d17c7df2eb.svg", "isPro": false, "fullname": "Xiangyu", "user": "xixy", "type": "user"}, "name": "Xiangyu Xi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:13.312Z", "hidden": false}, {"_id": "6976d5405d41524304c1359d", "name": "Xiangyuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359e", "name": "Xiangzhou Huang", "hidden": false}, {"_id": "6976d5405d41524304c1359f", "name": "Xiao Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a0", "name": "Xiaodong Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a1", "name": "Xiaolong Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a2", "name": "Xiaowei Shi", "hidden": false}, {"_id": "6976d5405d41524304c135a3", "name": "Xiaoyu Li", "hidden": false}, {"_id": "6976d5405d41524304c135a4", "name": "Xin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a5", "name": "Xingchen Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a6", "name": "Xuan Huang", "hidden": false}, {"_id": "6976d5405d41524304c135a7", "name": "Xuezhi Cao", "hidden": false}, {"_id": "6976d5405d41524304c135a8", "name": "Xunliang Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a9", "name": "Yan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135aa", "user": {"_id": "63fc1c420aab06079200c15c", "avatarUrl": "/avatars/8e8e82a9a6552848581ca9f65011263c.svg", "isPro": false, "fullname": "yang bai", "user": "byang", "type": "user"}, "name": "Yang Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:07.036Z", "hidden": false}, {"_id": "6976d5405d41524304c135ab", "name": "Yang Liu", "hidden": false}, {"_id": "6976d5405d41524304c135ac", "name": "Yang Yang", "hidden": false}, {"_id": "6976d5405d41524304c135ad", "name": "Yang Zheng", "hidden": false}, {"_id": "6976d5405d41524304c135ae", "name": "Yaoming Wang", "hidden": false}, {"_id": "6976d5405d41524304c135af", "name": "Yaoming Zhu", "hidden": false}, {"_id": "6976d5405d41524304c135b0", "name": "Yaqi Huo", "hidden": false}, {"_id": "6976d5405d41524304c135b1", "name": "Yanyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b2", "name": "Yaorui Shi", "hidden": false}, {"_id": "6976d5405d41524304c135b3", "name": "Yerui Sun", "hidden": false}, {"_id": "6976d5405d41524304c135b4", "name": "Yi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b5", "name": "Yihao Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b6", "name": "Yi-Kai Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b7", "name": "Yifan Lu", "hidden": false}, {"_id": "6976d5405d41524304c135b8", "name": "Yifan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135b9", "name": "Yitao Zhai", "hidden": false}, {"_id": "6976d5405d41524304c135ba", "name": "Yongjing Yin", "hidden": false}, {"_id": "6976d5405d41524304c135bb", "name": "Yongwei Zhou", "hidden": false}, {"_id": "6976d5405d41524304c135bc", "name": "Youshao Xiao", "hidden": false}, {"_id": "6976d5405d41524304c135bd", "name": "Yuchuan Dai", "hidden": false}, {"_id": "6976d5405d41524304c135be", "name": "Yuchen Xie", "hidden": false}, {"_id": "6976d5405d41524304c135bf", "name": "Yuchen Yu", "hidden": false}, {"_id": "6976d5405d41524304c135c0", "name": "Yufei Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135c1", "name": "Yuhuai Wei", "hidden": false}, {"_id": "6976d5405d41524304c135c2", "name": "Yulei Qian", "hidden": false}, {"_id": "6976d5405d41524304c135c3", "name": "Yunfan Liang", "hidden": false}, {"_id": "6976d5405d41524304c135c4", "name": "Yunke Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135c5", "name": "Yuwei Jiang", "hidden": false}, {"_id": "6976d5405d41524304c135c6", "name": "Yuxin Bian", "hidden": false}, {"_id": "6976d5405d41524304c135c7", "name": "Yuxin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135c8", "name": "Yuxin Liu", "hidden": false}, {"_id": "6976d5405d41524304c135c9", "name": "Yue Xu", "hidden": false}, {"_id": "6976d5405d41524304c135ca", "name": "Yueqing Sun", "hidden": false}, {"_id": "6976d5405d41524304c135cb", "name": "Zeyang Yu", "hidden": false}, {"_id": "6976d5405d41524304c135cc", "name": "Zhao Yang", "hidden": false}, {"_id": "6976d5405d41524304c135cd", "name": "Zhengsheng Huang", "hidden": false}, {"_id": "6976d5405d41524304c135ce", "name": "Zhengyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135cf", "name": "Zhijian Liu", "hidden": false}, {"_id": "6976d5405d41524304c135d0", "name": "Zhikang Xia", "hidden": false}, {"_id": "6976d5405d41524304c135d1", "name": "Zhimin Lin", "hidden": false}, {"_id": "6976d5405d41524304c135d2", "name": "Zhiyuan Yao", "hidden": false}, {"_id": "6976d5405d41524304c135d3", "name": "Zhuofan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135d4", "name": "Zhuowen Han", "hidden": false}, {"_id": "6976d5405d41524304c135d5", "name": "Zijian Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135d6", "name": "Ziran Li", "hidden": false}, {"_id": "6976d5405d41524304c135d7", "name": "Ziwen Wang", "hidden": false}, {"_id": "6976d5405d41524304c135d8", "name": "Ziyuan Zhuang", "hidden": false}], "publishedAt": "2026-01-23T13:20:09.000Z", "submittedOnDailyAt": "2026-01-26T00:15:28.340Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "upvotes": 136, "discussionId": "6976d5405d41524304c135d9", "ai_summary": "A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for real-world robustness and complex reasoning.", "ai_keywords": ["Mixture-of-Experts", "agentic reasoning", "domain-parallel expert training", "fusion", "asynchronous reinforcement learning", "DORA", "long-tailed generation", "multi-turn interactions", "real-world noise patterns", "test-time scaling", "reasoning depth", "reasoning width", "parallel thinking"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86\u540d\u4e3a LongCat-Flash-Thinking-2601 \u76845600\u4ebf\u53c2\u6570\u5f00\u6e90\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u63a8\u7406\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u591a\u9879\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5305\u62ec\u4ee3\u7406\u641c\u7d22\u548c\u5de5\u5177\u4f7f\u7528\u7b49\u3002</li>\n    <li>\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u5728\u590d\u6742\u5de5\u5177\u4e92\u52a8\u548c\u566a\u58f0\u73af\u5883\u4e0b\u5c55\u73b0\u7a33\u5065\u8868\u73b0\u3002</li>\n    <li>\u91c7\u7528\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u4e13\u5bb6\u8bad\u7ec3\u548c\u6570\u636e\u6784\u5efa\uff0c\u652f\u6301\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u591a\u73af\u5883\u8bad\u7ec3\u3002</li>\n    <li>\u5f15\u5165\u4e86\u91cd\u601d\u6a21\u5f0f\u4ee5\u589e\u5f3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u5b9e\u73b0\u66f4\u6df1\u548c\u66f4\u5e7f\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongCat-Flash-Thinking-2601 is a new open-source reasoning model with 560 billion parameters, designed for advanced logical reasoning.</li>\n    <li>The model outperforms other open-source models in various reasoning tasks, such as searching and using tools effectively.</li>\n    <li>It shows great ability to handle complex interactions with tools and performs well in noisy real-world situations.</li>\n    <li>The model's strong performance comes from a unique training approach that combines different training methods and focuses on real-world challenges.</li>\n    <li>It includes a new feature called Heavy Thinking mode, which helps improve reasoning by allowing deeper and broader thinking during tests.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:20:09.000Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16725.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u7d27\u51d1\u6548\u7387\u548c\u591a\u6a21\u6001\u667a\u80fd\u7684\u5e73\u8861\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\u5b9e\u73b0\uff0c\u5177\u67091.2T\u591a\u6a21\u6001\u6570\u636e\u7684\u8bad\u7ec3\u57fa\u7840\u3002</li>\n    <li>\u6a21\u578b\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u5e76\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u8d44\u6e90\u7684\u4f7f\u7528\u6548\u7387\u3002</li>\n    <li>\u5c3d\u7ba1\u6a21\u578b\u4f53\u79ef\u5c0f\uff0810B\uff09\uff0c\u4f46\u5176\u6027\u80fd\u53ef\u4ee5\u4e0e10\u523020\u500d\u66f4\u5927\u7684\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u751a\u81f3\u8d85\u8fc7\u4e00\u4e9b\u9876\u7ea7\u4e13\u6709\u6a21\u578b\u3002</li>\n    <li>\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSTEP3-VL-10B \u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u7ee9\uff0c\u5e76\u5411\u793e\u533a\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u6a21\u578b\u5957\u4ef6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a compact, open-source model that combines efficiency with advanced multimodal intelligence.</li>\n    <li>It uses a unique training approach that merges language and vision capabilities using a large dataset of multimodal tokens.</li>\n    <li>The model features advanced reasoning techniques to improve its performance during testing.</li>\n    <li>Despite being smaller than many other models, STEP3-VL-10B performs at a high level, often exceeding larger models.</li>\n    <li>The team has made the full model available for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u662f\u901a\u8fc7\u89c6\u89c9\u7ebf\u7d22\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u672a\u5145\u5206\u5229\u7528\u4eba\u7c7b\u5e38\u7528\u7684\u7b56\u7565\u2014\u2014\u4f7f\u7528\u5730\u56fe\u3002</li>\n    <li>\u672c\u7814\u7a76\u4e3a\u6a21\u578b\u6dfb\u52a0\u4e86\u201c\u601d\u8003\u5730\u56fe\u201d\u7684\u80fd\u529b\uff0c\u91c7\u7528\u4e86\u4e00\u4e2a\u4ee3\u7406-\u5730\u56fe\u5faa\u73af\u7684\u6846\u67b6\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5c24\u5176\u5728500\u7c73\u7cbe\u5ea6\u4e0a\u4ece8.0%\u63d0\u5347\u523022.1%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The image geolocalization task predicts where an image was taken on Earth using visual clues.</li>\n    <li>Current models use world knowledge and reasoning but don't use maps, a common strategy for humans.</li>\n    <li>This work introduces a \"Thinking with Map\" ability, using a new approach called agent-in-the-map loop.</li>\n    <li>A two-stage optimization is developed, which includes reinforcement learning to improve the model's decision-making and a method to explore multiple paths before making a prediction.</li>\n    <li>The new method, tested on real-world images, significantly outperforms existing models, improving accuracy from 8.0% to 22.1% for a key metric.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.17058", "authors": [{"_id": "69782c96026bdf0473116e06", "user": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "name": "Wei Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:49.701Z", "hidden": false}, {"_id": "69782c96026bdf0473116e07", "name": "Jun Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e08", "name": "Haoyu Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e09", "name": "Zhenghao Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0a", "name": "Qikang He", "hidden": false}, {"_id": "69782c96026bdf0473116e0b", "name": "Shaokun Han", "hidden": false}, {"_id": "69782c96026bdf0473116e0c", "name": "Guoliang Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0d", "user": {"_id": "64ef522242da8d2a897d62da", "avatarUrl": "/avatars/03611010d247da66696ac8976d4d3ed3.svg", "isPro": false, "fullname": "xuanhe zhou", "user": "zhouxh19", "type": "user"}, "name": "Xuanhe Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:19.930Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0e", "user": {"_id": "674fa2f067c963c50a066594", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674fa2f067c963c50a066594/hKZ46Mwm_UEguzBt63ys_.jpeg", "isPro": false, "fullname": "yeye he", "user": "yeyehe", "type": "user"}, "name": "Yeye He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:27.638Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0f", "name": "Chunwei Liu", "hidden": false}, {"_id": "69782c96026bdf0473116e10", "user": {"_id": "66724ce47e7ff5d8bd069c7c", "avatarUrl": "/avatars/953f66585390dbdb202c1d7b7250d7bd.svg", "isPro": false, "fullname": "Zirui Tang", "user": "TerryTang", "type": "user"}, "name": "Zirui Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:49.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e11", "name": "Bin Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e12", "user": {"_id": "695612aabf3c8959a3a05f9c", "avatarUrl": "/avatars/c18885f6dea6f3ee019405cd8cf6f484.svg", "isPro": false, "fullname": "ShenTang990", "user": "shentang", "type": "user"}, "name": "Shen Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:56.579Z", "hidden": false}, {"_id": "69782c96026bdf0473116e13", "name": "Kai Zuo", "hidden": false}, {"_id": "69782c96026bdf0473116e14", "user": {"_id": "67efa8a2ed790a2e999dc216", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0S4lQCJX61uCF8EkSLMkk.png", "isPro": false, "fullname": "Yuyu Luo", "user": "luoyuyu", "type": "user"}, "name": "Yuyu Luo", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:02.233Z", "hidden": false}, {"_id": "69782c96026bdf0473116e15", "name": "Zhenzhe Zheng", "hidden": false}, {"_id": "69782c96026bdf0473116e16", "user": {"_id": "63f9fca8d4349b157a109eec", "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg", "isPro": false, "fullname": "Conghui He", "user": "conghui", "type": "user"}, "name": "Conghui He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:57:14.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e17", "name": "Jingren Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e18", "name": "Fan Wu", "hidden": false}], "publishedAt": "2026-01-22T12:02:45.000Z", "submittedOnDailyAt": "2026-01-27T00:42:38.464Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "submittedOnDailyBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "upvotes": 127, "discussionId": "69782c97026bdf0473116e19", "projectPage": "https://github.com/weAIDB/awesome-data-llm", "githubRepo": "https://github.com/weAIDB/awesome-data-llm", "githubRepoAddedBy": "user", "ai_summary": "LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.", "ai_keywords": ["data preparation", "large language models", "prompt-driven workflows", "agentic workflows", "data cleaning", "data integration", "data enrichment", "entity matching", "schema matching", "data annotation", "data profiling"], "githubStars": 644, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u6570\u636e\u51c6\u5907\u7684\u76ee\u7684\u662f\u53bb\u566a\u539f\u59cb\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u8de8\u6570\u636e\u96c6\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u6570\u636e\u5e94\u7528\u3002</li>\n    <li>\u968f\u7740\u5bf9\u5e94\u7528\u5c31\u7eea\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\uff0cLLM\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u4ee5\u53ca\u7075\u6d3b\u7684\u4ee3\u7406\u6784\u5efa\u57fa\u7840\u8bbe\u65bd\u7684\u51fa\u73b0\uff0cLLM\u589e\u5f3a\u7684\u65b9\u6cd5\u6b63\u5728\u5feb\u901f\u6210\u4e3a\u6570\u636e\u51c6\u5907\u7684\u4e3b\u6d41\u3002</li>\n    <li>\u672c\u6587\u901a\u8fc7\u7814\u7a76\u5927\u91cf\u6587\u732e\uff0c\u7cfb\u7edf\u56de\u987e\u4e86LLM\u6280\u672f\u5728\u6570\u636e\u51c6\u5907\u4e2d\u7684\u5e94\u7528\uff0c\u5f3a\u8c03\u4e86\u4ece\u57fa\u4e8e\u89c4\u5219\u7684\u6d41\u7a0b\u8f6c\u5411\u4ee5\u63d0\u793a\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u4ee5\u4efb\u52a1\u4e3a\u4e2d\u5fc3\u7684\u5206\u7c7b\u6cd5\uff0c\u5c06\u6570\u636e\u51c6\u5907\u5206\u4e3a\u6570\u636e\u6e05\u6d17\u3001\u6570\u636e\u96c6\u6210\u548c\u6570\u636e\u4e30\u5bcc\u4e09\u4e2a\u4e3b\u8981\u4efb\u52a1\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u4efb\u52a1\u7684\u4ee3\u8868\u6027\u6280\u672f\u8fdb\u884c\u4e86\u8c03\u67e5\u3002</li>\n    <li>\u8ba8\u8bba\u4e86\u5f53\u524d\u7684\u7814\u7a76\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f3a\u8c03\u53ef\u6269\u5c55\u7684LLM-\u6570\u636e\u7cfb\u7edf\u548c\u53ef\u9760\u5de5\u4f5c\u6d41\u7a0b\u8bbe\u8ba1\u7684\u672a\u6765\u53d1\u5c55\u8def\u7ebf\u56fe\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Data preparation helps improve raw datasets by reducing noise, finding relationships, and extracting insights, which is important for various applications.</li>\n    <li>Advancements in LLM techniques and new infrastructures (like Databricks Unity Catalog) are changing how data is prepared, making it more efficient and effective.</li>\n    <li>This paper reviews recent studies on how LLM techniques are used for data preparation, focusing on three main tasks: cleaning, integration, and enrichment of data.</li>\n    <li>It highlights the strengths and weaknesses of different methods, such as better understanding of data but challenges with scaling and evaluation.</li>\n    <li>The paper also identifies research challenges and suggests future directions for developing scalable and reliable data preparation systems.</li>\n</ul>"}, "publishedAt": "2026-01-22T07:02:45.000Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17058.png", "numComments": 2, "submittedBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "fullname": "Wei Zhou", "name": "weizhoudb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.12538", "authors": [{"_id": "6971913fc1c7409747bf9564", "name": "Tianxin Wei", "hidden": false}, {"_id": "6971913fc1c7409747bf9565", "user": {"_id": "6742eb40924e80c3c80ebe13", "avatarUrl": "/avatars/e6ccb1a89a1ea0bfca70779966f4f429.svg", "isPro": false, "fullname": "Ting-Wei Li", "user": "tingwl0122", "type": "user"}, "name": "Ting-Wei Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T17:12:21.531Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9566", "name": "Zhining Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf9567", "name": "Xuying Ning", "hidden": false}, {"_id": "6971913fc1c7409747bf9568", "name": "Ze Yang", "hidden": false}, {"_id": "6971913fc1c7409747bf9569", "name": "Jiaru Zou", "hidden": false}, {"_id": "6971913fc1c7409747bf956a", "name": "Zhichen Zeng", "hidden": false}, {"_id": "6971913fc1c7409747bf956b", "name": "Ruizhong Qiu", "hidden": false}, {"_id": "6971913fc1c7409747bf956c", "name": "Xiao Lin", "hidden": false}, {"_id": "6971913fc1c7409747bf956d", "name": "Dongqi Fu", "hidden": false}, {"_id": "6971913fc1c7409747bf956e", "name": "Zihao Li", "hidden": false}, {"_id": "6971913fc1c7409747bf956f", "user": {"_id": "653962e75c8e4863e1a2068f", "avatarUrl": "/avatars/d4f5f5da141f37d53ca1986ff17b325e.svg", "isPro": false, "fullname": "Mengting Ai", "user": "famous-blue-raincoat", "type": "user"}, "name": "Mengting Ai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:10.378Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9570", "user": {"_id": "677830bd3f2e3ec475576303", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dhwqUDkk66m4oSGSbcd7j.png", "isPro": false, "fullname": "Duo Zhou", "user": "Claudius7", "type": "user"}, "name": "Duo Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:12.476Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9571", "name": "Wenxuan Bao", "hidden": false}, {"_id": "6971913fc1c7409747bf9572", "user": {"_id": "646323556c27a7e33b23f198", "avatarUrl": "/avatars/17fe142f689ab4be3c2374d1d90393db.svg", "isPro": false, "fullname": "Yunzhe Li", "user": "yunzhel2", "type": "user"}, "name": "Yunzhe Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:14.383Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9573", "name": "Gaotang Li", "hidden": false}, {"_id": "6971913fc1c7409747bf9574", "name": "Cheng Qian", "hidden": false}, {"_id": "6971913fc1c7409747bf9575", "name": "Yu Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf9576", "name": "Xiangru Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf9577", "name": "Yin Xiao", "hidden": false}, {"_id": "6971913fc1c7409747bf9578", "name": "Liri Fang", "hidden": false}, {"_id": "6971913fc1c7409747bf9579", "name": "Hui Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf957a", "name": "Xianfeng Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf957b", "name": "Yuji Zhang", "hidden": false}, {"_id": "6971913fc1c7409747bf957c", "name": "Chi Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf957d", "name": "Jiaxuan You", "hidden": false}, {"_id": "6971913fc1c7409747bf957e", "name": "Heng Ji", "hidden": false}, {"_id": "6971913fc1c7409747bf957f", "name": "Hanghang Tong", "hidden": false}, {"_id": "6971913fc1c7409747bf9580", "name": "Jingrui He", "hidden": false}], "publishedAt": "2026-01-18T18:58:23.000Z", "submittedOnDailyAt": "2026-01-22T00:27:25.162Z", "title": "Agentic Reasoning for Large Language Models", "submittedOnDailyBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "isPro": false, "fullname": "Jiaru Zou", "user": "jiaruz2", "type": "user"}, "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "upvotes": 125, "discussionId": "69719140c1c7409747bf9581", "githubRepo": "https://github.com/weitianxin/Awesome-Agentic-Reasoning", "githubRepoAddedBy": "user", "ai_summary": "Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.", "ai_keywords": ["large language models", "agentic reasoning", "autonomous agents", "planning", "tool use", "search", "feedback", "memory", "adaptation", "collaborative settings", "coordination", "knowledge sharing", "reinforcement learning", "supervised fine-tuning", "in-context reasoning", "post-training reasoning", "real-world applications", "benchmarks", "thought and action", "world modeling", "scalable multi-agent training", "governance"], "githubStars": 105, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "summary_zh": "<ul>\n    <li>\u63a8\u7406\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u6d89\u53ca\u63a8\u65ad\u3001\u89e3\u51b3\u95ee\u9898\u548c\u51b3\u7b56\u3002</li>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5c01\u95ed\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5f00\u653e\u548c\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u80fd\u591f\u901a\u8fc7\u6301\u7eed\u4e92\u52a8\u8fdb\u884c\u8ba1\u5212\u3001\u884c\u52a8\u548c\u5b66\u4e60\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u5206\u4e3a\u4e09\u4e2a\u5c42\u6b21\uff1a\u57fa\u7840\u4ee3\u7406\u63a8\u7406\u3001\u81ea\u6211\u6f14\u5316\u4ee3\u7406\u63a8\u7406\u548c\u96c6\u4f53\u591a\u4ee3\u7406\u63a8\u7406\u3002</li>\n    <li>\u672c\u6587\u56de\u987e\u4e86\u4ee3\u7406\u63a8\u7406\u7684\u6846\u67b6\uff0c\u5e76\u8ba8\u8bba\u4e86\u672a\u6765\u7684\u6311\u6218\u548c\u53d1\u5c55\u65b9\u5411\uff0c\u5305\u62ec\u4e2a\u6027\u5316\u548c\u591a\u4ee3\u7406\u8bad\u7ec3\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Reasoning is essential for making decisions, solving problems, and drawing conclusions.</li>\n    <li>While large language models (LLMs) are good at reasoning in controlled settings, they have difficulties in unpredictable environments.</li>\n    <li>Agentic reasoning treats LLMs as independent agents that can learn and adapt through ongoing interactions.</li>\n    <li>The survey discusses three levels of agentic reasoning: basic abilities like planning, improving through feedback, and working together with other agents.</li>\n    <li>It highlights real-world applications of agentic reasoning and outlines future challenges, such as personalization and training multiple agents together.</li>\n</ul>"}, "publishedAt": "2026-01-18T13:58:23.000Z", "title": "Agentic Reasoning for Large Language Models", "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12538.png", "numComments": 3, "submittedBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "fullname": "Jiaru Zou", "name": "jiaruz2", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20833", "authors": [{"_id": "697b9192a67238fac88cbee8", "name": "Tengyue Xu", "hidden": false}, {"_id": "697b9192a67238fac88cbee9", "name": "Zhuoyang Qian", "hidden": false}, {"_id": "697b9192a67238fac88cbeea", "name": "Gaoge Liu", "hidden": false}, {"_id": "697b9192a67238fac88cbeeb", "name": "Li Ling", "hidden": false}, {"_id": "697b9192a67238fac88cbeec", "name": "Zhentao Zhang", "hidden": false}, {"_id": "697b9192a67238fac88cbeed", "name": "Biao Wu", "hidden": false}, {"_id": "697b9192a67238fac88cbeee", "name": "Shuo Zhang", "hidden": false}, {"_id": "697b9192a67238fac88cbeef", "name": "Ke Lu", "hidden": false}, {"_id": "697b9192a67238fac88cbef0", "name": "Wei Shi", "hidden": false}, {"_id": "697b9192a67238fac88cbef1", "name": "Ziqi Wang", "hidden": false}, {"_id": "697b9192a67238fac88cbef2", "name": "Zheng Feng", "hidden": false}, {"_id": "697b9192a67238fac88cbef3", "name": "Yan Luo", "hidden": false}, {"_id": "697b9192a67238fac88cbef4", "name": "Shu Xu", "hidden": false}, {"_id": "697b9192a67238fac88cbef5", "name": "Yongjin Chen", "hidden": false}, {"_id": "697b9192a67238fac88cbef6", "name": "Zhibo Feng", "hidden": false}, {"_id": "697b9192a67238fac88cbef7", "name": "Zhuo Chen", "hidden": false}, {"_id": "697b9192a67238fac88cbef8", "name": "Bruce Yuan", "hidden": false}, {"_id": "697b9192a67238fac88cbef9", "name": "Harry Wang", "hidden": false}, {"_id": "697b9192a67238fac88cbefa", "name": "Kris Chen", "hidden": false}], "publishedAt": "2026-01-28T18:31:54.000Z", "submittedOnDailyAt": "2026-01-30T03:32:00.106Z", "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives", "submittedOnDailyBy": {"_id": "62baa0d6dd02fbf607ce97be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62baa0d6dd02fbf607ce97be/V0I6pANlLEf2YDd9ZLZgi.jpeg", "isPro": false, "fullname": "Wendy", "user": "Wendy-Fly", "type": "user"}, "summary": "Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.", "upvotes": 113, "discussionId": "697b9192a67238fac88cbefb", "githubRepo": "https://github.com/AgentAlphaAGI/Idea2Paper", "githubRepoAddedBy": "user", "ai_summary": "Offline knowledge construction through structured methodological graphs enables more reliable and scalable autonomous scientific discovery by reducing reliance on real-time literature processing.", "ai_keywords": ["large language model", "autonomous scientific discovery", "runtime-centric execution", "context window limitations", "hallucination", "pre-computation-driven framework", "peer-reviewed papers", "research patterns", "methodological knowledge graph", "end-to-end research workflows"], "githubStars": 54, "organization": {"_id": "69542731e1200d74c1c053d1", "name": "AgentAlphaAGI", "fullname": "AgentAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b78eb76ab5d14ca7faac87/TbMZ3y00APtRzHEfTSR7I.jpeg"}, "summary_zh": "<ul>\n    <li>Idea2Story\u662f\u4e00\u4e2a\u57fa\u4e8e\u9884\u8ba1\u7b97\u7684\u6846\u67b6\uff0c\u65e8\u5728\u5b9e\u73b0\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5c06\u6587\u732e\u7406\u89e3\u4ece\u5728\u7ebf\u63a8\u7406\u8f6c\u53d8\u4e3a\u79bb\u7ebf\u77e5\u8bc6\u6784\u5efa\uff0c\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u548c\u4e0a\u4e0b\u6587\u9650\u5236\u3002</li>\n    <li>Idea2Story\u6536\u96c6\u540c\u884c\u8bc4\u5ba1\u8bba\u6587\u53ca\u5176\u53cd\u9988\uff0c\u63d0\u53d6\u6838\u5fc3\u65b9\u6cd5\u5355\u5143\uff0c\u5e76\u7ec4\u7ec7\u6210\u7ed3\u6784\u5316\u7684\u77e5\u8bc6\u56fe\u8c31\u3002</li>\n    <li>\u5728\u8fd0\u884c\u65f6\uff0c\u7528\u6237\u7684\u7814\u7a76\u610f\u56fe\u4e0e\u5df2\u6709\u7814\u7a76\u8303\u5f0f\u5bf9\u9f50\uff0c\u4ece\u800c\u9ad8\u6548\u68c0\u7d22\u548c\u91cd\u7528\u7814\u7a76\u6a21\u5f0f\u3002</li>\n    <li>\u521d\u6b65\u7814\u7a76\u8868\u660e\uff0cIdea2Story\u80fd\u591f\u751f\u6210\u8fde\u8d2f\u4e14\u521b\u65b0\u7684\u7814\u7a76\u6a21\u5f0f\uff0c\u652f\u6301\u53ef\u9760\u7684\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Idea2Story is a new framework for scientific discovery that moves from online reasoning to offline knowledge building.</li>\n    <li>It collects peer-reviewed papers and feedback, extracts key methods, and organizes them into a knowledge graph.</li>\n    <li>This system allows researchers to efficiently find and reuse established research patterns instead of starting from scratch.</li>\n    <li>By using a knowledge graph, Idea2Story reduces the need for constant online reasoning and allows for more reliable outcomes.</li>\n    <li>Early studies show that Idea2Story can create high-quality, coherent research ideas effectively.</li>\n</ul>"}, "publishedAt": "2026-01-28T13:31:54.000Z", "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives", "summary": "Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20833.png", "numComments": 1, "submittedBy": {"_id": "62baa0d6dd02fbf607ce97be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62baa0d6dd02fbf607ce97be/V0I6pANlLEf2YDd9ZLZgi.jpeg", "fullname": "Wendy", "name": "Wendy-Fly", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "69542731e1200d74c1c053d1", "name": "AgentAlphaAGI", "fullname": "AgentAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b78eb76ab5d14ca7faac87/TbMZ3y00APtRzHEfTSR7I.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.08763", "authors": [{"_id": "6969b0a232f0333869ff946a", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:38.232Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946b", "user": {"_id": "6891c906f3c31445cc040ab1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891c906f3c31445cc040ab1/NBqxXOY7al4CD0XBj8ke2.jpeg", "isPro": false, "fullname": "Yucheng Wang", "user": "DevilEnfant", "type": "user"}, "name": "Yucheng Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:48.080Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946c", "name": "Yufei He", "hidden": false}, {"_id": "6969b0a232f0333869ff946d", "user": {"_id": "682deb444988bd82847e2b03", "avatarUrl": "/avatars/15da087e84386ea72c6fa2db63571420.svg", "isPro": false, "fullname": "Jia-Ying Wu", "user": "EricaWu", "type": "user"}, "name": "Jiaying Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:59.692Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946e", "name": "Yilun Zhao", "hidden": false}, {"_id": "6969b0a232f0333869ff946f", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0a232f0333869ff9470", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:06.327Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9471", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:12.181Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9472", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:17.979Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9473", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:23.007Z", "hidden": false}], "publishedAt": "2026-01-13T17:48:43.000Z", "submittedOnDailyAt": "2026-01-16T01:00:36.686Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "upvotes": 111, "discussionId": "6969b0a232f0333869ff9474", "ai_summary": "Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.", "ai_keywords": ["reinforcement learning", "large language models", "exploration collapse", "pass@k", "pass@1", "rollout-level objective", "high-level solution strategies", "clustering", "policy advantages", "AUC@K"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u540e\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u53d8\u5f97\u91cd\u8981\uff0c\u4f46\u5e38\u5e38\u9762\u4e34\u63a2\u7d22\u5d29\u6e83\u7684\u95ee\u9898\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u8fc7\u4e8e\u96c6\u4e2d\u4e8e\u5c40\u90e8\u7684\u884c\u4e3a\uff0c\u5bfc\u81f4\u7b56\u7565\u591a\u6837\u6027\u4e0d\u8db3\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3a\u72ec\u7279\u6027\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\uff0c\u5956\u52b1\u5c55\u793a\u7a00\u6709\u9ad8\u5c42\u7b56\u7565\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u805a\u7c7b\u76f8\u540c\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u91cd\u65b0\u52a0\u6743\u7b56\u7565\u4f18\u52bf\uff0c\u4ee5\u9f13\u52b1\u65b0\u9896\u7684\u7b56\u7565\u3002</li>\n    <li>\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u533b\u5b66\u63a8\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u591a\u6837\u6027\u548c\u89e3\u51b3\u65b9\u6848\u7684\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e00\u5b9a\u7684\u51c6\u786e\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) helps improve large language models (LLMs) but often focuses too much on a few common reasoning patterns.</li>\n    <li>This focus can lead to limited exploration and diversity in solutions, which affects overall performance.</li>\n    <li>The authors propose a new approach called Uniqueness-Aware Reinforcement Learning that rewards unique and rare successful strategies.</li>\n    <li>This method uses an LLM-based judge to group similar solutions and adjust rewards based on the uniqueness of the strategies.</li>\n    <li>The approach shows better performance in various reasoning tasks without sacrificing the ability to achieve high-level accuracy.</li>\n</ul>"}, "publishedAt": "2026-01-13T12:48:43.000Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08763.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Feb 03, 2026";
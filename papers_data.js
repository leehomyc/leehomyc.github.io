window.trendingPapers = {
    "today": [{"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u89c6\u89c9\u4e0a\u771f\u5b9e\u548c\u65f6\u95f4\u4e0a\u8fde\u8d2f\u7684\u5185\u5bb9\uff0c\u4f46\u5b83\u4eec\u4f5c\u4e3a\u4e16\u754c\u6a21\u62df\u5668\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u662f\u5426\u6355\u6349\u5230\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7ea6\u675f\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u6307\u6807\u5982FVD\u5173\u6ce8\u611f\u77e5\u8d28\u91cf\uff0c\u4f46\u5ffd\u89c6\u4e86\u56e0\u679c\u5173\u7cfb\u3001\u7269\u7406\u548c\u5168\u5c40\u4e00\u81f4\u6027\u7b49\u63a8\u7406\u5931\u8d25\u3002</li>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86MMGR\uff08\u591a\u6a21\u6001\u751f\u6210\u63a8\u7406\u8bc4\u4f30\u548c\u57fa\u51c6\uff09\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8bc4\u4f30\uff1a\u7269\u7406\u3001\u903b\u8f91\u3001\u4e09\u7ef4\u7a7a\u95f4\u3001\u4e8c\u7ef4\u7a7a\u95f4\u548c\u65f6\u95f4\u3002</li>\n    <li>MMGR\u5728\u62bd\u8c61\u63a8\u7406\u3001\u5177\u8eab\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\u4e09\u4e2a\u9886\u57df\u8fdb\u884c\u8bc4\u4f30\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u62bd\u8c61\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u5dee\u3002</li>\n    <li>\u6211\u4eec\u7684\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u5bf9\u611f\u77e5\u6570\u636e\u7684\u8fc7\u5ea6\u4f9d\u8d56\u548c\u5728\u56e0\u679c\u6b63\u786e\u6027\u4e0a\u7684\u4e0d\u8db3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video foundation models can create realistic video content, but their accuracy in simulating the real world is questionable.</li>\n    <li>Current evaluation methods mainly focus on visual quality and ignore important reasoning issues like causality and physics.</li>\n    <li>The new MMGR framework assesses models based on five reasoning skills: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>\n    <li>MMGR tests models across three areas: Abstract Reasoning, Embodied Navigation, and Physical Commonsense, revealing performance gaps.</li>\n    <li>Models perform well in Physical Commonsense but poorly in Abstract Reasoning and long-term planning, highlighting major limitations.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13281", "authors": [{"_id": "6940d82465f1e24a117805c2", "name": "Jiaqi Wang", "hidden": false}, {"_id": "6940d82465f1e24a117805c3", "name": "Weijia Wu", "hidden": false}, {"_id": "6940d82465f1e24a117805c4", "name": "Yi Zhan", "hidden": false}, {"_id": "6940d82465f1e24a117805c5", "name": "Rui Zhao", "hidden": false}, {"_id": "6940d82465f1e24a117805c6", "name": "Ming Hu", "hidden": false}, {"_id": "6940d82465f1e24a117805c7", "name": "James Cheng", "hidden": false}, {"_id": "6940d82465f1e24a117805c8", "name": "Wei Liu", "hidden": false}, {"_id": "6940d82465f1e24a117805c9", "name": "Philip Torr", "hidden": false}, {"_id": "6940d82465f1e24a117805ca", "user": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "isPro": false, "fullname": "Qinghong (Kevin) Lin", "user": "KevinQHLin", "type": "user"}, "name": "Kevin Qinghong Lin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T14:04:37.407Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/g2dRdpFY8mVSrBnrK3C0q.mp4"], "publishedAt": "2025-12-15T12:41:23.000Z", "submittedOnDailyAt": "2025-12-17T08:26:01.077Z", "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?", "submittedOnDailyBy": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "isPro": false, "fullname": "Qinghong (Kevin) Lin", "user": "KevinQHLin", "type": "user"}, "summary": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: (i) Immersive ASMR video-audio sources. Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. (ii) Peer-Review evaluation. An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\\% accuracy (random 50\\%), far below that of human experts (81.25\\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.", "upvotes": 50, "discussionId": "6940d82565f1e24a117805cb", "projectPage": "https://video-reality-test.github.io/", "githubRepo": "https://github.com/video-reality-test/video-reality-test", "githubRepoAddedBy": "user", "ai_summary": "The Video Reality Test benchmark evaluates the realism and detection of AI-generated ASMR videos with audio, revealing that even the best models can deceive VLMs and humans, highlighting limitations in perceptual fidelity and audio-visual consistency.", "ai_keywords": ["ASMR", "audio-visual coupling", "Veo3.1-Fast", "Gemini 2.5-Pro", "perceptual realism", "real-fake discrimination", "audio-visual consistency"], "githubStars": 13, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u751f\u6210\u6280\u672f\u7684\u8fdb\u6b65\u4f7f\u5f97AI\u751f\u6210\u7684\u89c6\u9891\u4e0e\u771f\u5b9e\u89c6\u9891\u51e0\u4e4e\u65e0\u6cd5\u533a\u5206\uff0c\u8fd9\u7ed9\u68c0\u6d4b\u5e26\u6765\u4e86\u6311\u6218\u3002</li>\n    <li>\u73b0\u6709\u7684\u68c0\u6d4b\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u6ca1\u6709\u97f3\u9891\u7684\u89c6\u9891\uff0c\u5173\u6ce8\u5e7f\u6cdb\u7684\u53d9\u4e8b\u9886\u57df\uff0c\u5e76\u4e14\u4e3b\u8981\u4e13\u6ce8\u4e8e\u5206\u7c7b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u89c6\u9891\u73b0\u5b9e\u6d4b\u8bd5\u201d\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8eASMR\u89c6\u9891\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u6d4b\u8bd5\u97f3\u89c6\u9891\u7ed3\u5408\u4e0b\u7684\u611f\u77e5\u771f\u5b9e\u5ea6\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6700\u597d\u7684\u751f\u6210\u6a21\u578bVeo3.1-Fast\u80fd\u591f\u6b3a\u9a97\u5927\u591a\u6570VLMs\uff08\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\uff0c\u5176\u51c6\u786e\u7387\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u3002</li>\n    <li>\u6dfb\u52a0\u97f3\u9891\u53ef\u4ee5\u63d0\u9ad8\u771f\u5b9e\u4e0e\u865a\u5047\u89c6\u9891\u7684\u533a\u5206\u80fd\u529b\uff0c\u4f46\u67d0\u4e9b\u8868\u9762\u7ebf\u7d22\uff08\u5982\u6c34\u5370\uff09\u4ecd\u7136\u4f1a\u8bef\u5bfc\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>AI-generated videos are becoming so realistic that it is hard to tell them apart from real videos, creating challenges in detection.</li>\n    <li>Previous tests for detecting AI-generated content mostly focused on videos without sound and didn't assess how well audio and visuals work together.</li>\n    <li>The new benchmark, called Video Reality Test, uses ASMR videos to evaluate how convincing audio-visual videos are.</li>\n    <li>In tests, the best AI video generator fooled most models that try to detect fakes, with only 56% accuracy compared to 81.25% for human experts.</li>\n    <li>While adding audio helps in detecting fake videos, some features like watermarks can still trick detection models.</li>\n</ul>"}, "publishedAt": "2025-12-15T07:41:23.000Z", "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?", "summary": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: (i) Immersive ASMR video-audio sources. Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. (ii) Peer-Review evaluation. An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\\% accuracy (random 50\\%), far below that of human experts (81.25\\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/g2dRdpFY8mVSrBnrK3C0q.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13281.png", "numComments": 2, "submittedBy": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "fullname": "Qinghong (Kevin) Lin", "name": "KevinQHLin", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 41}, "isAuthorParticipating": true}, {"paper": {"id": "2512.14614", "authors": [{"_id": "694219f25d5b2dc1052747ff", "user": {"_id": "64897b1f0ec897cfe579a399", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64897b1f0ec897cfe579a399/ICR_75b877BaSE94gjBuj.jpeg", "isPro": false, "fullname": "wenq", "user": "wenqsun", "type": "user"}, "name": "Wenqiang Sun", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:38.348Z", "hidden": false}, {"_id": "694219f25d5b2dc105274800", "name": "Haiyu Zhang", "hidden": false}, {"_id": "694219f25d5b2dc105274801", "name": "Haoyuan Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274802", "name": "Junta Wu", "hidden": false}, {"_id": "694219f25d5b2dc105274803", "name": "Zehan Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274804", "name": "Zhenwei Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274805", "name": "Yunhong Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274806", "name": "Jun Zhang", "hidden": false}, {"_id": "694219f25d5b2dc105274807", "name": "Tengfei Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274808", "name": "Chunchao Guo", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/z23kgc1LgY1qvs-Tfp4zj.mp4"], "publishedAt": "2025-12-16T17:22:46.000Z", "submittedOnDailyAt": "2025-12-17T00:24:30.301Z", "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.", "upvotes": 48, "discussionId": "694219f35d5b2dc105274809", "projectPage": "https://3d-models.hunyuan.tencent.com/world/", "githubRepo": "https://github.com/Tencent-Hunyuan/HY-WorldPlay", "githubRepoAddedBy": "user", "ai_summary": "WorldPlay is a streaming video diffusion model that achieves real-time, interactive world modeling with long-term geometric consistency by using a Dual Action Representation, Reconstituted Context Memory, and Context Forcing.", "ai_keywords": ["Dual Action Representation", "Reconstituted Context Memory", "temporal reframing", "Context Forcing", "memory-aware model", "long-horizon streaming video"], "githubStars": 302, "summary_zh": "<ul>\n    <li>WorldPlay \u662f\u4e00\u79cd\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u7684\u4e92\u52a8\u4e16\u754c\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u901f\u5ea6\u548c\u5185\u5b58\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u4e86\u53cc\u91cd\u52a8\u4f5c\u8868\u793a\uff0c\u80fd\u591f\u5728\u7528\u6237\u8f93\u5165\u65f6\u63d0\u4f9b\u5f3a\u5927\u7684\u52a8\u4f5c\u63a7\u5236\u3002</li>\n    <li>\u901a\u8fc7\u91cd\u6784\u4e0a\u4e0b\u6587\u8bb0\u5fc6\uff0cWorldPlay \u52a8\u6001\u91cd\u5efa\u8fc7\u53bb\u7684\u5e27\uff0c\u4fdd\u6301\u51e0\u4f55\u91cd\u8981\u7684\u957f\u65f6\u95f4\u5e27\u53ef\u8bbf\u95ee\uff0c\u4ece\u800c\u786e\u4fdd\u957f\u671f\u4e00\u81f4\u6027\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u5f3a\u5236\u7684\u65b9\u6cd5\uff0c\u4ee5\u4fdd\u6301\u6a21\u578b\u5728\u5b9e\u65f6\u901f\u5ea6\u4e0b\u7684\u957f\u7a0b\u4fe1\u606f\u4f7f\u7528\u80fd\u529b\uff0c\u9632\u6b62\u9519\u8bef\u6f02\u79fb\u3002</li>\n    <li>WorldPlay \u53ef\u4ee5\u751f\u6210720p\u300124\u5e27\u6bcf\u79d2\u7684\u9ad8\u8d28\u91cf\u6d41\u89c6\u9891\uff0c\u5e76\u4e14\u5728\u591a\u79cd\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>WorldPlay is a new video streaming model that allows for real-time interactive world modeling with consistent geometric accuracy.</li>\n    <li>It features a Dual Action Representation for better control based on user inputs from the keyboard and mouse.</li>\n    <li>The model uses a Reconstituted Context Memory to maintain important past information, helping with long-term consistency.</li>\n    <li>Context Forcing is a new method that helps align memory between parts of the model to enable fast processing while reducing errors.</li>\n    <li>WorldPlay can produce high-quality 720p video at 24 frames per second, showing improved performance compared to current methods.</li>\n</ul>"}, "publishedAt": "2025-12-16T12:22:46.000Z", "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/z23kgc1LgY1qvs-Tfp4zj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14614.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.12675", "authors": [{"_id": "6940c8af65f1e24a1177fbe2", "user": {"_id": "65e71ef39cf349af2940b317", "avatarUrl": "/avatars/fc1cd8d3510946fc947d67b16b51834b.svg", "isPro": false, "fullname": "Yuran Wang", "user": "Ryann829", "type": "user"}, "name": "Yuran Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:09:32.675Z", "hidden": false}, {"_id": "6940c8af65f1e24a1177fbe3", "user": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "name": "Bohan Zeng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:09:30.756Z", "hidden": false}, {"_id": "6940c8af65f1e24a1177fbe4", "name": "Chengzhuo Tong", "hidden": false}, {"_id": "6940c8af65f1e24a1177fbe5", "name": "Wenxuan Liu", "hidden": false}, {"_id": "6940c8af65f1e24a1177fbe6", "name": "Yang Shi", "hidden": false}, {"_id": "6940c8af65f1e24a1177fbe7", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6940c8af65f1e24a1177fbe8", "name": "Hao Liang", "hidden": false}, {"_id": "6940c8af65f1e24a1177fbe9", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6940c8af65f1e24a1177fbea", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-14T12:58:19.000Z", "submittedOnDailyAt": "2025-12-17T00:18:03.394Z", "title": "Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.", "upvotes": 38, "discussionId": "6940c8b065f1e24a1177fbeb", "githubRepo": "https://github.com/Ryann-Ran/Scone", "githubRepoAddedBy": "user", "ai_summary": "Scone integrates composition and distinction in image generation by using a two-stage training scheme with semantic alignment and attention-based masking, outperforming existing models on benchmarks.", "ai_keywords": ["composition", "distinction", "semantic bridge", "semantic alignment", "attention-based masking", "SconeEval"], "githubStars": 19, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u7814\u7a76\u4e86\u591a\u4e3b\u4f53\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u4e3b\u4f53\u533a\u5206\u95ee\u9898\u3002</li>\n    <li>\u63d0\u51fa\u4e86Scone\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u56fe\u50cf\u7684\u7ec4\u6210\u548c\u533a\u5206\u80fd\u529b\u3002</li>\n    <li>Scone\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u548c\u6ce8\u610f\u529b\u63a9\u853d\u6765\u63d0\u9ad8\u4e3b\u4f53\u533a\u5206\u6027\u3002</li>\n    <li>\u63a8\u51fa\u4e86SconeEval\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u7ec4\u6210\u548c\u533a\u5206\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cScone\u5728\u73b0\u6709\u6a21\u578b\u4e2d\u8868\u73b0\u66f4\u597d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current image generation methods struggle to identify and generate the correct subjects when multiple candidates are present.</li>\n    <li>Scone is a new method that improves both the composition of images and the ability to distinguish subjects.</li>\n    <li>It uses a two-step training process to first learn how to compose images, then focuses on distinguishing subjects accurately.</li>\n    <li>SconeEval is a new benchmark for testing how well models can compose and distinguish subjects in images.</li>\n    <li>Tests show that Scone performs better than existing models in these tasks, and its resources are available online.</li>\n</ul>"}, "publishedAt": "2025-12-14T07:58:19.000Z", "title": "Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling", "summary": "Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12675.png", "numComments": 1, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.13660", "authors": [{"_id": "694220325d5b2dc105274831", "user": {"_id": "63f08dc79cf89c9ed1bb89cd", "avatarUrl": "/avatars/37290358ad00bbd752f519cfdec02f3e.svg", "isPro": false, "fullname": "Zhoues", "user": "Zhoues", "type": "user"}, "name": "Enshen Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:27.449Z", "hidden": false}, {"_id": "694220325d5b2dc105274832", "name": "Cheng Chi", "hidden": false}, {"_id": "694220325d5b2dc105274833", "name": "Yibo Li", "hidden": false}, {"_id": "694220325d5b2dc105274834", "name": "Jingkun An", "hidden": false}, {"_id": "694220325d5b2dc105274835", "name": "Jiayuan Zhang", "hidden": false}, {"_id": "694220325d5b2dc105274836", "name": "Shanyu Rong", "hidden": false}, {"_id": "694220325d5b2dc105274837", "name": "Yi Han", "hidden": false}, {"_id": "694220325d5b2dc105274838", "name": "Yuheng Ji", "hidden": false}, {"_id": "694220325d5b2dc105274839", "name": "Mengzhen Liu", "hidden": false}, {"_id": "694220325d5b2dc10527483a", "name": "Pengwei Wang", "hidden": false}, {"_id": "694220325d5b2dc10527483b", "name": "Zhongyuan Wang", "hidden": false}, {"_id": "694220325d5b2dc10527483c", "name": "Lu Sheng", "hidden": false}, {"_id": "694220325d5b2dc10527483d", "name": "Shanghang Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63f08dc79cf89c9ed1bb89cd/oBxP3i_7vhT9DXbEAa7rd.mp4"], "publishedAt": "2025-12-15T18:52:43.000Z", "submittedOnDailyAt": "2025-12-17T01:02:34.029Z", "title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics", "submittedOnDailyBy": {"_id": "63f08dc79cf89c9ed1bb89cd", "avatarUrl": "/avatars/37290358ad00bbd752f519cfdec02f3e.svg", "isPro": false, "fullname": "Zhoues", "user": "Zhoues", "type": "user"}, "summary": "Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.", "upvotes": 31, "discussionId": "694220325d5b2dc10527483e", "projectPage": "https://zhoues.github.io/RoboTracer/", "githubRepo": "https://github.com/Zhoues/RoboTracer", "githubRepoAddedBy": "user", "ai_summary": "RoboTracer, a 3D-aware visual language model, enhances spatial tracing by combining supervised and reinforcement fine-tuning with a universal spatial encoder and regression-supervised decoder, achieving state-of-the-art performance on TraceSpatial-Bench.", "ai_keywords": ["3D-aware VLM", "universal spatial encoder", "regression-supervised decoder", "supervised fine-tuning", "reinforcement fine-tuning", "metric-sensitive process rewards", "TraceSpatial", "TraceSpatial-Bench", "spatial tracing", "spatial understanding", "spatial referring", "UR5", "G1 humanoid"], "githubStars": 14, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>RoboTracer \u662f\u4e00\u79cd\u65b0\u578b\u7684\u673a\u5668\u4eba\u7a7a\u95f4\u8ffd\u8e2a\u7cfb\u7edf\uff0c\u80fd\u591f\u8fdb\u884c\u4e09\u7ef4\u7a7a\u95f4\u7684\u5f15\u7528\u548c\u6d4b\u91cf\u3002</li>\n    <li>\u5b83\u4f7f\u7528\u901a\u7528\u7a7a\u95f4\u7f16\u7801\u5668\u548c\u56de\u5f52\u76d1\u7763\u89e3\u7801\u5668\u6765\u589e\u5f3a\u5bf9\u5c3a\u5ea6\u7684\u7406\u89e3\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u591a\u6b65\u9aa4\u7684\u63a8\u7406\u3002</li>\n    <li>\u4e3a\u4e86\u652f\u6301\u8bad\u7ec3\uff0c\u7814\u7a76\u56e2\u961f\u63a8\u51fa\u4e86\u5305\u542b3000\u4e07\u4e2a\u95ee\u7b54\u5bf9\u7684 TraceSpatial \u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5404\u79cd\u573a\u666f\u5e76\u652f\u6301\u590d\u6742\u7684\u63a8\u7406\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cRoboTracer \u5728\u7a7a\u95f4\u7406\u89e3\u3001\u6d4b\u91cf\u548c\u5f15\u7528\u65b9\u9762\u7684\u6210\u529f\u7387\u8fbe\u523079.1%\uff0c\u5e76\u5728\u8bc4\u4f30\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002</li>\n    <li>RoboTracer \u53ef\u4ee5\u4e0e\u591a\u79cd\u63a7\u5236\u7b56\u7565\u7ed3\u5408\uff0c\u6267\u884c\u590d\u6742\u7684\u52a8\u6001\u4efb\u52a1\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>RoboTracer is a new robot system designed to improve spatial tracing, which is challenging due to the need for complex reasoning and real-world measurements.</li>\n    <li>It uses a special encoder and a decoder to help robots understand and measure 3D spaces better during training.</li>\n    <li>RoboTracer employs reinforcement fine-tuning to enhance its reasoning skills by rewarding accurate intermediate steps in the tracing process.</li>\n    <li>A large dataset called TraceSpatial, with 30 million question-answer pairs, was created to support the training of RoboTracer and test its capabilities.</li>\n    <li>RoboTracer outperforms existing systems in spatial tasks, achieving a 79.1% success rate and significantly better accuracy on a new evaluation benchmark.</li>\n</ul>"}, "publishedAt": "2025-12-15T13:52:43.000Z", "title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics", "summary": "Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63f08dc79cf89c9ed1bb89cd/oBxP3i_7vhT9DXbEAa7rd.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13660.png", "numComments": 1, "submittedBy": {"_id": "63f08dc79cf89c9ed1bb89cd", "avatarUrl": "/avatars/37290358ad00bbd752f519cfdec02f3e.svg", "fullname": "Zhoues", "name": "Zhoues", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.14051", "authors": [{"_id": "6942480b5d5b2dc105274934", "name": "Mengzhang Cai", "hidden": false}, {"_id": "6942480b5d5b2dc105274935", "name": "Xin Gao", "hidden": false}, {"_id": "6942480b5d5b2dc105274936", "name": "Yu Li", "hidden": false}, {"_id": "6942480b5d5b2dc105274937", "name": "Honglin Lin", "hidden": false}, {"_id": "6942480b5d5b2dc105274938", "name": "Zheng Liu", "hidden": false}, {"_id": "6942480b5d5b2dc105274939", "name": "Zhuoshi Pan", "hidden": false}, {"_id": "6942480b5d5b2dc10527493a", "name": "Qizhi Pei", "hidden": false}, {"_id": "6942480b5d5b2dc10527493b", "name": "Xiaoran Shang", "hidden": false}, {"_id": "6942480b5d5b2dc10527493c", "name": "Mengyuan Sun", "hidden": false}, {"_id": "6942480b5d5b2dc10527493d", "user": {"_id": "66580d3d80ee5b1e11a94e57", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66580d3d80ee5b1e11a94e57/aHaPqrV5vNefFktYRsiGf.jpeg", "isPro": false, "fullname": "Zinan Tang", "user": "Word2Li", "type": "user"}, "name": "Zinan Tang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T14:04:26.917Z", "hidden": false}, {"_id": "6942480b5d5b2dc10527493e", "user": {"_id": "6567f597d0a121b8e803b47a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/9vFjVzdWgMmwACAmJIngA.png", "isPro": false, "fullname": "xiaoyang wang", "user": "Xiaoyang318", "type": "user"}, "name": "Xiaoyang Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T14:04:28.799Z", "hidden": false}, {"_id": "6942480b5d5b2dc10527493f", "user": {"_id": "6875f5b55096cad81398a5af", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6875f5b55096cad81398a5af/CwXl53Jdp3LsBuRudT_CM.jpeg", "isPro": false, "fullname": "Zhanping Zhong", "user": "ChampionZhong", "type": "user"}, "name": "Zhanping Zhong", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:07:51.225Z", "hidden": false}, {"_id": "6942480b5d5b2dc105274940", "name": "Yun Zhu", "hidden": false}, {"_id": "6942480b5d5b2dc105274941", "name": "Dahua Lin", "hidden": false}, {"_id": "6942480b5d5b2dc105274942", "name": "Conghui He", "hidden": false}, {"_id": "6942480b5d5b2dc105274943", "name": "Lijun Wu", "hidden": false}], "publishedAt": "2025-12-16T03:33:24.000Z", "submittedOnDailyAt": "2025-12-17T03:37:20.056Z", "title": "OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value", "submittedOnDailyBy": {"_id": "643e60d96db6ba8c5ee177ad", "avatarUrl": "/avatars/73ac7740e462ba0b53a2f2480d9f1e3e.svg", "isPro": false, "fullname": "Lijun Wu", "user": "apeters", "type": "user"}, "summary": "The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.", "upvotes": 27, "discussionId": "6942480b5d5b2dc105274944", "projectPage": "https://opendataarena.github.io", "githubRepo": "https://github.com/OpenDataArena/OpenDataArena-Tool", "githubRepoAddedBy": "user", "ai_summary": "OpenDataArena (ODA) is an open platform that benchmarks post-training datasets for Large Language Models (LLMs) using a unified pipeline, multi-dimensional scoring, and data lineage exploration to enhance reproducibility and understanding of data impacts on model behavior.", "ai_keywords": ["Large Language Models", "post-training datasets", "benchmarking", "OpenDataArena", "unified training-evaluation pipeline", "multi-dimensional scoring framework", "data lineage explorer", "data-centric AI"], "githubStars": 79, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u53d1\u5c55\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u548c\u591a\u6837\u5316\u7684\u540e\u671f\u8bad\u7ec3\u6570\u636e\u3002</li>\n    <li>\u76ee\u524d\u7684\u6570\u636e\u6765\u6e90\u4e0d\u900f\u660e\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5f71\u54cd\u6a21\u578b\u7684\u53ef\u590d\u73b0\u6027\u3002</li>\n    <li>\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u6211\u4eec\u63a8\u51fa\u4e86OpenDataArena\uff08ODA\uff09\uff0c\u4e00\u4e2a\u5f00\u653e\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u8bc4\u4f30\u540e\u671f\u6570\u636e\u7684\u4ef7\u503c\u3002</li>\n    <li>ODA\u5305\u62ec\u7edf\u4e00\u7684\u8bad\u7ec3\u8bc4\u4f30\u6d41\u7a0b\u3001\u591a\u7ef4\u8bc4\u5206\u6846\u67b6\u3001\u4ea4\u4e92\u5f0f\u6570\u636e\u6765\u6e90\u63a2\u7d22\u5de5\u5177\uff0c\u4ee5\u53ca\u5f00\u6e90\u5de5\u5177\u5305\u3002</li>\n    <li>\u901a\u8fc7\u5728ODA\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6211\u4eec\u53d1\u73b0\u6570\u636e\u590d\u6742\u6027\u4e0e\u4efb\u52a1\u8868\u73b0\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u53d1\u5e03\u6240\u6709\u7ed3\u679c\u548c\u5de5\u5177\uff0c\u4fc3\u8fdb\u9ad8\u8d28\u91cf\u6570\u636e\u8bc4\u4f30\u7684\u666e\u53ca\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) rely on the quality and variety of their training datasets, which are often not well understood.</li>\n    <li>OpenDataArena (ODA) is a new platform aimed at improving the evaluation of post-training data for LLMs.</li>\n    <li>ODA includes a system for comparing models, scoring data quality, exploring dataset origins, and providing open tools for research.</li>\n    <li>Research using ODA has examined over 120 datasets and found important insights about data quality and performance trade-offs.</li>\n    <li>ODA aims to improve data evaluation methods and support more scientific approaches to developing AI models.</li>\n</ul>"}, "publishedAt": "2025-12-15T22:33:24.000Z", "title": "OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value", "summary": "The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14051.png", "numComments": 2, "submittedBy": {"_id": "643e60d96db6ba8c5ee177ad", "avatarUrl": "/avatars/73ac7740e462ba0b53a2f2480d9f1e3e.svg", "fullname": "Lijun Wu", "name": "apeters", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10}, "isAuthorParticipating": false}, {"paper": {"id": "2512.12980", "authors": [{"_id": "6942299d5d5b2dc105274890", "user": {"_id": "68cb7d70de299da181a5ec4c", "avatarUrl": "/avatars/3a8e87a807592e176347bc4e6f58a686.svg", "isPro": false, "fullname": "CHEN TINGYANG", "user": "Tingyang-Chen", "type": "user"}, "name": "Tingyang Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:12.437Z", "hidden": false}, {"_id": "6942299d5d5b2dc105274891", "name": "Cong Fu", "hidden": false}, {"_id": "6942299d5d5b2dc105274892", "name": "Jiahua Wu", "hidden": false}, {"_id": "6942299d5d5b2dc105274893", "name": "Haotian Wu", "hidden": false}, {"_id": "6942299d5d5b2dc105274894", "name": "Hua Fan", "hidden": false}, {"_id": "6942299d5d5b2dc105274895", "name": "Xiangyu Ke", "hidden": false}, {"_id": "6942299d5d5b2dc105274896", "name": "Yunjun Gao", "hidden": false}, {"_id": "6942299d5d5b2dc105274897", "name": "Yabo Ni", "hidden": false}, {"_id": "6942299d5d5b2dc105274898", "name": "Anxiang Zeng", "hidden": false}], "publishedAt": "2025-12-15T04:49:33.000Z", "submittedOnDailyAt": "2025-12-17T06:57:54.379Z", "title": "Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views", "submittedOnDailyBy": {"_id": "665e8515045fcbf12b99558a", "avatarUrl": "/avatars/e4ea26703d9dd2c8f19412ccd81d2cae.svg", "isPro": false, "fullname": "Fu Cong", "user": "fcthebrave", "type": "user"}, "summary": "Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services -- from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recall-latency trade-off against a ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice.\n  We present Iceberg, a holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From a task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For a more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face recognition, text retrieval, and recommendation systems. Each dataset, ranging from 1M to 100M vectors, includes rich, task-specific labels and evaluation metrics, enabling assessment of retrieval algorithms within the full application pipeline rather than in isolation. Iceberg benchmarks 13 state-of-the-art VSS methods and re-ranks them based on application-level metrics, revealing substantial deviations from traditional rankings derived purely from recall-latency evaluations. Building on these insights, we define a set of task-centric meta-features and derive an interpretable decision tree to guide practitioners in selecting and tuning VSS methods for their specific workloads.", "upvotes": 23, "discussionId": "6942299e5d5b2dc105274899", "githubRepo": "https://github.com/ZJU-DAILY/Iceberg", "githubRepoAddedBy": "user", "ai_summary": "Iceberg is a benchmark suite that evaluates vector similarity search methods in real-world contexts, identifying key sources of performance degradation and providing guidance for selecting and tuning these methods.", "ai_keywords": ["Vector Similarity Search", "VSS", "embedding lookups", "large language models", "semantic information retrieval", "recommendation engines", "recall-latency trade-off", "Information Loss Funnel", "Embedding Loss", "Metric Misuse", "Data Distribution Sensitivity", "task-specific labels", "evaluation metrics", "application pipeline", "state-of-the-art VSS methods", "task-centric meta-features", "decision tree"], "githubStars": 1, "summary_zh": "<ul>\n    <li>\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7684\u5411\u91cf\u76f8\u4f3c\u6027\u641c\u7d22(VSS)\u6b63\u5728\u6210\u4e3a\u6570\u636e\u5e93\u7cfb\u7edf\u7684\u91cd\u8981\u529f\u80fd\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u4fe1\u606f\u68c0\u7d22\u548c\u63a8\u8350\u7cfb\u7edf\u3002</li>\n    <li>\u76ee\u524d\u7684\u8bc4\u4f30\u6807\u51c6\u4e3b\u8981\u5173\u6ce8\u53ec\u56de\u7387\u4e0e\u5ef6\u8fdf\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5ffd\u89c6\u4e86\u68c0\u7d22\u8d28\u91cf\u5bf9\u540e\u7eed\u4efb\u52a1\u7684\u5f71\u54cd\u3002</li>\n    <li>Iceberg\u662f\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u5957\u4ef6\uff0c\u7528\u4e8e\u5728\u5b9e\u9645\u5e94\u7528\u80cc\u666f\u4e0b\u8bc4\u4f30VSS\u65b9\u6cd5\uff0c\u63ed\u793a\u4fe1\u606f\u635f\u5931\u7684\u4e09\u4e2a\u4e3b\u8981\u6765\u6e90\u3002</li>\n    <li>Iceberg\u6db5\u76d6\u516b\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u8303\u56f4\u5305\u62ec\u56fe\u50cf\u5206\u7c7b\u3001\u4eba\u8138\u8bc6\u522b\u3001\u6587\u672c\u68c0\u7d22\u548c\u63a8\u8350\u7cfb\u7edf\uff0c\u6570\u636e\u96c6\u89c4\u6a21\u4ece100\u4e07\u52301\u4ebf\u4e2a\u5411\u91cf\u4e0d\u7b49\u3002</li>\n    <li>\u57fa\u4e8e\u5e94\u7528\u7ea7\u6307\u6807\u91cd\u65b0\u6392\u540d\u768413\u79cd\u6700\u5148\u8fdb\u7684VSS\u65b9\u6cd5\u663e\u793a\uff0c\u4e0e\u4f20\u7edf\u7684\u53ec\u56de\u7387-\u5ef6\u8fdf\u8bc4\u4f30\u76f8\u6bd4\uff0c\u7ed3\u679c\u6709\u663e\u8457\u5dee\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vector Similarity Search (VSS) is becoming essential for modern databases used in services like language models and recommendations.</li>\n    <li>Current evaluations of VSS mainly focus on recall and speed, ignoring how retrieval quality affects actual tasks.</li>\n    <li>Iceberg is a new benchmark that evaluates VSS methods in real application contexts and identifies key performance issues.</li>\n    <li>It uses eight diverse datasets to assess VSS methods and provides task-specific evaluation metrics.</li>\n    <li>Iceberg helps users choose and optimize VSS methods by offering insights and a decision tree based on task-centric features.</li>\n</ul>"}, "publishedAt": "2025-12-14T23:49:33.000Z", "title": "Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views", "summary": "Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services -- from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recall-latency trade-off against a ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice.\n  We present Iceberg, a holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From a task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For a more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face recognition, text retrieval, and recommendation systems. Each dataset, ranging from 1M to 100M vectors, includes rich, task-specific labels and evaluation metrics, enabling assessment of retrieval algorithms within the full application pipeline rather than in isolation. Iceberg benchmarks 13 state-of-the-art VSS methods and re-ranks them based on application-level metrics, revealing substantial deviations from traditional rankings derived purely from recall-latency evaluations. Building on these insights, we define a set of task-centric meta-features and derive an interpretable decision tree to guide practitioners in selecting and tuning VSS methods for their specific workloads.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12980.png", "numComments": 1, "submittedBy": {"_id": "665e8515045fcbf12b99558a", "avatarUrl": "/avatars/e4ea26703d9dd2c8f19412ccd81d2cae.svg", "fullname": "Fu Cong", "name": "fcthebrave", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": false}, {"paper": {"id": "2512.14336", "authors": [{"_id": "69423f565d5b2dc105274912", "name": "Jooyeol Yun", "hidden": false}, {"_id": "69423f565d5b2dc105274913", "name": "Jaegul Choo", "hidden": false}], "publishedAt": "2025-12-16T12:03:46.000Z", "submittedOnDailyAt": "2025-12-17T03:00:58.491Z", "title": "Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure", "submittedOnDailyBy": {"_id": "6369f693bf21b20c5692937b", "avatarUrl": "/avatars/e937dc8234b3e456149882bfce34841f.svg", "isPro": false, "fullname": "Jooyeol Yun", "user": "YeolJoo", "type": "user"}, "summary": "Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics.", "upvotes": 22, "discussionId": "69423f565d5b2dc105274914", "projectPage": "https://yeolj00.github.io/personal-projects/vector-prism/", "githubRepo": "https://github.com/YeolJ00/vector-prism", "githubRepoAddedBy": "user", "ai_summary": "A framework aggregates weak predictions to recover semantic structure, enabling coherent SVG animations and improving VLM interactions with vector graphics.", "ai_keywords": ["vision-language models", "VLMs", "SVG animation", "semantic structure", "statistical aggregation", "weak part predictions", "semantic recovery"], "githubStars": 2, "organization": {"_id": "6475760c33192631bad2bb38", "name": "kaist-ai", "fullname": "KAIST AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"}, "summary_zh": "<ul>\n    <li>\u53ef\u7f29\u653e\u77e2\u91cf\u56fe\u5f62\uff08SVG\uff09\u5728\u73b0\u4ee3\u7f51\u9875\u8bbe\u8ba1\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u52a8\u753b\u9700\u6c42\u4e0d\u65ad\u589e\u957f\u3002</li>\n    <li>\u5c3d\u7ba1\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u548c\u8fd0\u52a8\u89c4\u5212\u4e0a\u6709\u8fdb\u5c55\uff0c\u4f46\u81ea\u52a8\u5316SVG\u52a8\u753b\u4ecd\u7136\u5f88\u6709\u6311\u6218\u6027\u3002</li>\n    <li>VLMs\u901a\u5e38\u9519\u8bef\u5904\u7406SVG\uff0c\u56e0\u4e3a\u89c6\u89c9\u4e0a\u76f8\u5173\u7684\u90e8\u5206\u5e38\u5e38\u88ab\u5206\u5272\u6210\u4f4e\u7ea7\u5f62\u72b6\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u8fd0\u52a8\u6307\u5f15\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u8ba1\u591a\u6b21\u5f31\u9884\u6d4b\u7684\u805a\u5408\uff0c\u6062\u590dSVG\u52a8\u753b\u6240\u9700\u7684\u8bed\u4e49\u7ed3\u6784\u3002</li>\n    <li>\u6211\u4eec\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8bed\u4e49\u6062\u590d\u663e\u8457\u63d0\u9ad8\u4e86\u52a8\u753b\u7684\u4e00\u81f4\u6027\uff0c\u4f7fVLMs\u4e0e\u77e2\u91cf\u56fe\u5f62\u7684\u4ea4\u4e92\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>SVGs are important for web design, and there is a growing interest in animating them.</li>\n    <li>Current vision-language models (VLMs) struggle to automate SVG animations because they often misinterpret the graphics.</li>\n    <li>This paper presents a new framework that helps VLMs understand the structure of SVGs better.</li>\n    <li>By grouping SVG elements semantically, the framework allows for more coherent animations.</li>\n    <li>Experiments show that this method significantly improves SVG animation quality and VLM interactions.</li>\n</ul>"}, "publishedAt": "2025-12-16T07:03:46.000Z", "title": "Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure", "summary": "Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14336.png", "numComments": 1, "submittedBy": {"_id": "6369f693bf21b20c5692937b", "avatarUrl": "/avatars/e937dc8234b3e456149882bfce34841f.svg", "fullname": "Jooyeol Yun", "name": "YeolJoo", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "6475760c33192631bad2bb38", "name": "kaist-ai", "fullname": "KAIST AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.14699", "authors": [{"_id": "69421f525d5b2dc10527481f", "name": "Sihui Ji", "hidden": false}, {"_id": "69421f525d5b2dc105274820", "name": "Xi Chen", "hidden": false}, {"_id": "69421f525d5b2dc105274821", "name": "Shuai Yang", "hidden": false}, {"_id": "69421f525d5b2dc105274822", "name": "Xin Tao", "hidden": false}, {"_id": "69421f525d5b2dc105274823", "name": "Pengfei Wan", "hidden": false}, {"_id": "69421f525d5b2dc105274824", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-17T10:13:28.014Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xvSTre7TiA3JDn2hTfpan.mp4"], "publishedAt": "2025-12-16T18:59:59.000Z", "submittedOnDailyAt": "2025-12-17T00:41:26.687Z", "title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.", "upvotes": 14, "discussionId": "69421f525d5b2dc105274825", "ai_summary": "MemFlow dynamically updates a memory bank by retrieving relevant historical frames for each video chunk, ensuring narrative coherence and generation efficiency with minimal computational overhead.", "ai_keywords": ["memory bank", "historical frames", "text prompt", "attention layers", "KV cache", "computational overhead"], "summary_zh": "<ul>\n    <li>\u6d41\u5a92\u4f53\u89c6\u9891\u751f\u6210\u7684\u6838\u5fc3\u6311\u6218\u662f\u4fdd\u6301\u957f\u65f6\u95f4\u5185\u5bb9\u7684\u4e00\u81f4\u6027\uff0c\u9700\u8981\u9ad8\u6548\u7684\u5185\u5b58\u8bbe\u8ba1\u3002</li>\n    <li>\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u901a\u8fc7\u9884\u5b9a\u4e49\u7b56\u7565\u538b\u7f29\u5386\u53f2\u5e27\u6765\u7ef4\u62a4\u5185\u5b58\uff0c\u4f46\u8fd9\u79cd\u56fa\u5b9a\u7b56\u7565\u96be\u4ee5\u6ee1\u8db3\u4e0d\u540c\u89c6\u9891\u7247\u6bb5\u7684\u9700\u6c42\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MemFlow\uff0c\u901a\u8fc7\u6839\u636e\u5373\u5c06\u751f\u6210\u7684\u7247\u6bb5\u7684\u6587\u672c\u63d0\u793a\u52a8\u6001\u66f4\u65b0\u5185\u5b58\u5e93\uff0c\u68c0\u7d22\u6700\u76f8\u5173\u7684\u5386\u53f2\u5e27\u3002</li>\n    <li>\u8fd9\u79cd\u8bbe\u8ba1\u786e\u4fdd\u4e86\u53d9\u4e8b\u7684\u4e00\u81f4\u6027\uff0c\u5373\u4f7f\u672a\u6765\u5e27\u4e2d\u51fa\u73b0\u65b0\u4e8b\u4ef6\u6216\u573a\u666f\u5207\u6362\u3002</li>\n    <li>MemFlow\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u53ea\u6fc0\u6d3b\u5185\u5b58\u5e93\u4e2d\u4e0e\u6bcf\u4e2a\u67e5\u8be2\u6700\u76f8\u5173\u7684\u4ee4\u724c\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u6548\u7387\uff0c\u5e76\u4e14\u8ba1\u7b97\u8d1f\u62c5\u5f88\u5c0f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The main challenge in streaming video generation is keeping the content consistent over long periods, which requires effective memory management.</li>\n    <li>Current methods often use fixed strategies to compress historical video frames, but these don't adapt well to different parts of the video.</li>\n    <li>This work introduces a new method called MemFlow that updates memory dynamically based on the specific text prompt for each video chunk.</li>\n    <li>MemFlow allows for coherent storytelling even when new events or changes occur in the video.</li>\n    <li>It efficiently activates only the most relevant information during video generation, leading to better consistency with minimal processing delay.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:59:59.000Z", "title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives", "summary": "The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xvSTre7TiA3JDn2hTfpan.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14699.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.14503", "authors": [{"_id": "694220f85d5b2dc105274859", "name": "Chao Yi", "hidden": false}, {"_id": "694220f85d5b2dc10527485a", "name": "Dian Chen", "hidden": false}, {"_id": "694220f85d5b2dc10527485b", "user": {"_id": "645469c7363bb3aaf9ca9caf", "avatarUrl": "/avatars/0a456a16c1447dd1dcd8d45b807af77c.svg", "isPro": false, "fullname": "Gaoyang Guo", "user": "hairlatic", "type": "user"}, "name": "Gaoyang Guo", "status": "admin_assigned", "statusLastChangedAt": "2025-12-17T10:13:52.444Z", "hidden": false}, {"_id": "694220f85d5b2dc10527485c", "user": {"_id": "65acfb3a14e6582c30b4ce76", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65acfb3a14e6582c30b4ce76/RhEhePggBtyM0RIIqXQen.jpeg", "isPro": false, "fullname": "TangJiakai", "user": "TangJiakai5704", "type": "user"}, "name": "Jiakai Tang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:21.661Z", "hidden": false}, {"_id": "694220f85d5b2dc10527485d", "name": "Jian Wu", "hidden": false}, {"_id": "694220f85d5b2dc10527485e", "name": "Jing Yu", "hidden": false}, {"_id": "694220f85d5b2dc10527485f", "name": "Mao Zhang", "hidden": false}, {"_id": "694220f85d5b2dc105274860", "name": "Wen Chen", "hidden": false}, {"_id": "694220f85d5b2dc105274861", "name": "Wenjun Yang", "hidden": false}, {"_id": "694220f85d5b2dc105274862", "name": "Yujie Luo", "hidden": false}, {"_id": "694220f85d5b2dc105274863", "name": "Yuning Jiang", "hidden": false}, {"_id": "694220f85d5b2dc105274864", "user": {"_id": "6942695b2057100e28ce0200", "avatarUrl": "/avatars/48986efcbfd004c0b6eb866f9cbdd7c1.svg", "isPro": false, "fullname": "Zhujin Gao", "user": "zhjgao", "type": "user"}, "name": "Zhujin Gao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-17T10:14:05.884Z", "hidden": false}, {"_id": "694220f85d5b2dc105274865", "name": "Bo Zheng", "hidden": false}, {"_id": "694220f85d5b2dc105274866", "name": "Binbin Cao", "hidden": false}, {"_id": "694220f85d5b2dc105274867", "name": "Changfa Wu", "hidden": false}, {"_id": "694220f85d5b2dc105274868", "name": "Dixuan Wang", "hidden": false}, {"_id": "694220f85d5b2dc105274869", "name": "Han Wu", "hidden": false}, {"_id": "694220f85d5b2dc10527486a", "name": "Haoyi Hu", "hidden": false}, {"_id": "694220f85d5b2dc10527486b", "name": "Kewei Zhu", "hidden": false}, {"_id": "694220f85d5b2dc10527486c", "name": "Lang Tian", "hidden": false}, {"_id": "694220f85d5b2dc10527486d", "name": "Lin Yang", "hidden": false}, {"_id": "694220f85d5b2dc10527486e", "name": "Qiqi Huang", "hidden": false}, {"_id": "694220f85d5b2dc10527486f", "name": "Siqi Yang", "hidden": false}, {"_id": "694220f85d5b2dc105274870", "name": "Wenbo Su", "hidden": false}, {"_id": "694220f85d5b2dc105274871", "name": "Xiaoxiao He", "hidden": false}, {"_id": "694220f85d5b2dc105274872", "name": "Xin Tong", "hidden": false}, {"_id": "694220f85d5b2dc105274873", "name": "Xu Chen", "hidden": false}, {"_id": "694220f85d5b2dc105274874", "name": "Xunke Xi", "hidden": false}, {"_id": "694220f85d5b2dc105274875", "name": "Xiaowei Huang", "hidden": false}, {"_id": "694220f85d5b2dc105274876", "name": "Yaxuan Wu", "hidden": false}, {"_id": "694220f85d5b2dc105274877", "name": "Yeqiu Yang", "hidden": false}, {"_id": "694220f85d5b2dc105274878", "name": "Yi Hu", "hidden": false}, {"_id": "694220f85d5b2dc105274879", "name": "Yujin Yuan", "hidden": false}, {"_id": "694220f85d5b2dc10527487a", "name": "Yuliang Yan", "hidden": false}, {"_id": "694220f85d5b2dc10527487b", "name": "Zile Zhou", "hidden": false}], "publishedAt": "2025-12-16T15:40:44.000Z", "submittedOnDailyAt": "2025-12-17T00:48:26.159Z", "title": "RecGPT-V2 Technical Report", "submittedOnDailyBy": {"_id": "65acfb3a14e6582c30b4ce76", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65acfb3a14e6582c30b4ce76/RhEhePggBtyM0RIIqXQen.jpeg", "isPro": false, "fullname": "TangJiakai", "user": "TangJiakai5704", "type": "user"}, "summary": "Large language models (LLMs) have demonstrated remarkable potential in transforming recommender systems from implicit behavioral pattern matching to explicit intent reasoning. While RecGPT-V1 successfully pioneered this paradigm by integrating LLM-based reasoning into user interest mining and item tag prediction, it suffers from four fundamental limitations: (1) computational inefficiency and cognitive redundancy across multiple reasoning routes; (2) insufficient explanation diversity in fixed-template generation; (3) limited generalization under supervised learning paradigms; and (4) simplistic outcome-focused evaluation that fails to match human standards.\n  To address these challenges, we present RecGPT-V2 with four key innovations. First, a Hierarchical Multi-Agent System restructures intent reasoning through coordinated collaboration, eliminating cognitive duplication while enabling diverse intent coverage. Combined with Hybrid Representation Inference that compresses user-behavior contexts, our framework reduces GPU consumption by 60% and improves exclusive recall from 9.39% to 10.99%. Second, a Meta-Prompting framework dynamically generates contextually adaptive prompts, improving explanation diversity by +7.3%. Third, constrained reinforcement learning mitigates multi-reward conflicts, achieving +24.1% improvement in tag prediction and +13.0% in explanation acceptance. Fourth, an Agent-as-a-Judge framework decomposes assessment into multi-step reasoning, improving human preference alignment. Online A/B tests on Taobao demonstrate significant improvements: +2.98% CTR, +3.71% IPV, +2.19% TV, and +11.46% NER. RecGPT-V2 establishes both the technical feasibility and commercial viability of deploying LLM-powered intent reasoning at scale, bridging the gap between cognitive exploration and industrial utility.", "upvotes": 14, "discussionId": "694220f95d5b2dc10527487c", "ai_summary": "RecGPT-V2 enhances recommender systems by integrating a Hierarchical Multi-Agent System, Hybrid Representation Inference, Meta-Prompting, constrained reinforcement learning, and an Agent-as-a-Judge framework to improve efficiency, explanation diversity, generalization, and human preference alignment.", "ai_keywords": ["Hierarchical Multi-Agent System", "Hybrid Representation Inference", "Meta-Prompting", "constrained reinforcement learning", "Agent-as-a-Judge", "intent reasoning", "cognitive duplication", "explanation diversity", "tag prediction", "explanation acceptance", "CTR", "IPV", "TV", "NER"], "summary_zh": "<ul>\n  <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u4ece\u884c\u4e3a\u6a21\u5f0f\u5339\u914d\u8f6c\u5411\u610f\u56fe\u63a8\u7406\u3002</li>\n  <li>RecGPT-V1 \u6709\u56db\u4e2a\u4e3b\u8981\u95ee\u9898\uff0c\u5305\u62ec\u8ba1\u7b97\u6548\u7387\u4f4e\u3001\u89e3\u91ca\u591a\u6837\u6027\u4e0d\u8db3\u3001\u76d1\u7763\u5b66\u4e60\u6cdb\u5316\u80fd\u529b\u6709\u9650\u548c\u8bc4\u4f30\u65b9\u6cd5\u7b80\u5355\u3002</li>\n  <li>RecGPT-V2 \u901a\u8fc7\u56db\u9879\u521b\u65b0\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002</li>\n  <li>\u91c7\u7528\u5206\u5c42\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u91cd\u590d\uff0c\u63d0\u9ad8\u4e86\u610f\u56fe\u8986\u76d6\u7387\uff0cGPU\u6d88\u8017\u964d\u4f4e60%\u3002</li>\n  <li>\u5728\u7ebf\u6d4b\u8bd5\u663e\u793a\uff0cRecGPT-V2 \u5728\u591a\u4e2a\u6307\u6807\u4e0a\u90fd\u6709\u663e\u8457\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u6280\u672f\u548c\u5546\u4e1a\u53ef\u884c\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) can improve recommender systems by focusing on user intent rather than just behavior.</li>\n    <li>RecGPT-V1 had limitations, such as being inefficient, lacking diverse explanations, and not matching human evaluation standards.</li>\n    <li>RecGPT-V2 introduces a new system that reduces computation needs by 60% and improves user recall rates.</li>\n    <li>It uses adaptive prompts for better explanations and improves prediction accuracy significantly through new learning methods.</li>\n    <li>Real-world tests show that RecGPT-V2 significantly boosts user engagement metrics on platforms like Taobao.</li>\n</ul>"}, "publishedAt": "2025-12-16T10:40:44.000Z", "title": "RecGPT-V2 Technical Report", "summary": "Large language models (LLMs) have demonstrated remarkable potential in transforming recommender systems from implicit behavioral pattern matching to explicit intent reasoning. While RecGPT-V1 successfully pioneered this paradigm by integrating LLM-based reasoning into user interest mining and item tag prediction, it suffers from four fundamental limitations: (1) computational inefficiency and cognitive redundancy across multiple reasoning routes; (2) insufficient explanation diversity in fixed-template generation; (3) limited generalization under supervised learning paradigms; and (4) simplistic outcome-focused evaluation that fails to match human standards.\n  To address these challenges, we present RecGPT-V2 with four key innovations. First, a Hierarchical Multi-Agent System restructures intent reasoning through coordinated collaboration, eliminating cognitive duplication while enabling diverse intent coverage. Combined with Hybrid Representation Inference that compresses user-behavior contexts, our framework reduces GPU consumption by 60% and improves exclusive recall from 9.39% to 10.99%. Second, a Meta-Prompting framework dynamically generates contextually adaptive prompts, improving explanation diversity by +7.3%. Third, constrained reinforcement learning mitigates multi-reward conflicts, achieving +24.1% improvement in tag prediction and +13.0% in explanation acceptance. Fourth, an Agent-as-a-Judge framework decomposes assessment into multi-step reasoning, improving human preference alignment. Online A/B tests on Taobao demonstrate significant improvements: +2.98% CTR, +3.71% IPV, +2.19% TV, and +11.46% NER. RecGPT-V2 establishes both the technical feasibility and commercial viability of deploying LLM-powered intent reasoning at scale, bridging the gap between cognitive exploration and industrial utility.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14503.png", "numComments": 1, "submittedBy": {"_id": "65acfb3a14e6582c30b4ce76", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65acfb3a14e6582c30b4ce76/RhEhePggBtyM0RIIqXQen.jpeg", "fullname": "TangJiakai", "name": "TangJiakai5704", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": true}],
    "week": [{"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u89c6\u89c9\u4e0a\u771f\u5b9e\u548c\u65f6\u95f4\u4e0a\u8fde\u8d2f\u7684\u5185\u5bb9\uff0c\u4f46\u5b83\u4eec\u4f5c\u4e3a\u4e16\u754c\u6a21\u62df\u5668\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u662f\u5426\u6355\u6349\u5230\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7ea6\u675f\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u6307\u6807\u5982FVD\u5173\u6ce8\u611f\u77e5\u8d28\u91cf\uff0c\u4f46\u5ffd\u89c6\u4e86\u56e0\u679c\u5173\u7cfb\u3001\u7269\u7406\u548c\u5168\u5c40\u4e00\u81f4\u6027\u7b49\u63a8\u7406\u5931\u8d25\u3002</li>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86MMGR\uff08\u591a\u6a21\u6001\u751f\u6210\u63a8\u7406\u8bc4\u4f30\u548c\u57fa\u51c6\uff09\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8bc4\u4f30\uff1a\u7269\u7406\u3001\u903b\u8f91\u3001\u4e09\u7ef4\u7a7a\u95f4\u3001\u4e8c\u7ef4\u7a7a\u95f4\u548c\u65f6\u95f4\u3002</li>\n    <li>MMGR\u5728\u62bd\u8c61\u63a8\u7406\u3001\u5177\u8eab\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\u4e09\u4e2a\u9886\u57df\u8fdb\u884c\u8bc4\u4f30\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u62bd\u8c61\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u5dee\u3002</li>\n    <li>\u6211\u4eec\u7684\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u5bf9\u611f\u77e5\u6570\u636e\u7684\u8fc7\u5ea6\u4f9d\u8d56\u548c\u5728\u56e0\u679c\u6b63\u786e\u6027\u4e0a\u7684\u4e0d\u8db3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video foundation models can create realistic video content, but their accuracy in simulating the real world is questionable.</li>\n    <li>Current evaluation methods mainly focus on visual quality and ignore important reasoning issues like causality and physics.</li>\n    <li>The new MMGR framework assesses models based on five reasoning skills: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>\n    <li>MMGR tests models across three areas: Abstract Reasoning, Embodied Navigation, and Physical Commonsense, revealing performance gaps.</li>\n    <li>Models perform well in Physical Commonsense but poorly in Abstract Reasoning and long-term planning, highlighting major limitations.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13586", "authors": [{"_id": "6940d86d65f1e24a117805cd", "user": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "isPro": false, "fullname": "Jia-Nan Li", "user": "JinaLeejnl", "type": "user"}, "name": "Jia-Nan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:38:49.797Z", "hidden": false}, {"_id": "6940d86d65f1e24a117805ce", "name": "Jian Guan", "hidden": false}, {"_id": "6940d86d65f1e24a117805cf", "name": "Wei Wu", "hidden": false}, {"_id": "6940d86d65f1e24a117805d0", "name": "Chongxuan Li", "hidden": false}], "publishedAt": "2025-12-15T17:41:19.000Z", "submittedOnDailyAt": "2025-12-16T01:39:12.101Z", "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "submittedOnDailyBy": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "isPro": false, "fullname": "Jia-Nan Li", "user": "JinaLeejnl", "type": "user"}, "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.", "upvotes": 74, "discussionId": "6940d86d65f1e24a117805d1", "githubRepo": "https://github.com/ML-GSAI/ReFusion", "githubRepoAddedBy": "user", "ai_summary": "ReFusion, a novel masked diffusion model, improves performance and efficiency by using slot-based parallel decoding, achieving superior results compared to autoregressive models and traditional masked diffusion models.", "ai_keywords": ["autoregressive models", "masked diffusion models", "Key-Value caching", "parallel decoding", "token level", "slot level", "diffusion-based planning", "autoregressive infilling", "slot-based design", "slot-level permutation space"], "githubStars": 18, "summary_zh": "<ul>\n    <li>\u81ea\u56de\u5f52\u6a21\u578b\uff08ARMs\uff09\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u800c\u63a9\u853d\u6269\u6563\u6a21\u578b\uff08MDMs\uff09\u6709\u9ad8\u8ba1\u7b97\u5f00\u9500\u548c\u751f\u6210\u4e0d\u8fde\u8d2f\u7684\u95ee\u9898\u3002</li>\n    <li>ReFusion\u662f\u4e00\u79cd\u65b0\u578b\u63a9\u853d\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u5e76\u884c\u89e3\u7801\u63d0\u5347\u5230\u66f4\u9ad8\u7684\u69fd\u7ea7\u522b\u6765\u63d0\u9ad8\u6548\u7387\u3002</li>\n    <li>ReFusion\u4f7f\u7528\u201c\u89c4\u5212\u548c\u586b\u5145\u201d\u7684\u8fed\u4ee3\u89e3\u7801\u8fc7\u7a0b\uff0c\u5148\u8bc6\u522b\u5f31\u4f9d\u8d56\u7684\u69fd\uff0c\u7136\u540e\u5e76\u884c\u89e3\u7801\u8fd9\u4e9b\u69fd\u3002</li>\n    <li>\u8fd9\u79cd\u69fd\u4e3a\u57fa\u7840\u7684\u8bbe\u8ba1\u53ef\u4ee5\u5b8c\u5168\u91cd\u7528KV\u7f13\u5b58\uff0c\u5e76\u7b80\u5316\u5b66\u4e60\u590d\u6742\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cReFusion\u5728\u6027\u80fd\u548c\u901f\u5ea6\u4e0a\u663e\u8457\u8d85\u8d8a\u4e86\u4ee5\u524d\u7684MDMs\uff0c\u5e76\u7f29\u5c0f\u4e86\u4e0e\u5f3aARMs\u7684\u6027\u80fd\u5dee\u8ddd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoregressive models are slow at generating sequences one step at a time.</li>\n    <li>Masked diffusion models are faster but have issues with high computational costs and generating inconsistent outputs.</li>\n    <li>ReFusion is a new model that improves efficiency by decoding at a higher level, using fixed-length sequences called slots.</li>\n    <li>It uses a two-step process: first planning which slots to fill, then filling them in parallel, allowing for better use of resources.</li>\n    <li>ReFusion has shown significant improvements, outperforming previous models by 34% and being over 18 times faster on average.</li>\n</ul>"}, "publishedAt": "2025-12-15T12:41:19.000Z", "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13586.png", "numComments": 2, "submittedBy": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "fullname": "Jia-Nan Li", "name": "JinaLeejnl", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "isAuthorParticipating": true}, {"paper": {"id": "2512.13687", "authors": [{"_id": "6940ee0665f1e24a1178066d", "user": {"_id": "67756c9c846a267749304255", "avatarUrl": "/avatars/01f09805b561887c55d1b9ad4e96b461.svg", "isPro": false, "fullname": "Jingfeng Yao", "user": "MapleF9", "type": "user"}, "name": "Jingfeng Yao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T10:34:03.365Z", "hidden": false}, {"_id": "6940ee0665f1e24a1178066e", "user": {"_id": "6264bf5a1ed8d81e47ae3a62", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1650769741842-noauth.jpeg", "isPro": false, "fullname": "Yuda Song", "user": "IDKiro", "type": "user"}, "name": "Yuda Song", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:38.071Z", "hidden": false}, {"_id": "6940ee0665f1e24a1178066f", "user": {"_id": "64192280d459c9e7fbb03aa1", "avatarUrl": "/avatars/89935de92f3f9107d7b768b82fb27e70.svg", "isPro": false, "fullname": "Zhou", "user": "Yucong", "type": "user"}, "name": "Yucong Zhou", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T10:34:50.881Z", "hidden": false}, {"_id": "6940ee0665f1e24a11780670", "user": {"_id": "62600de6d47e3dbae32ce1ce", "avatarUrl": "/avatars/a536417cfec6e10ac415091bd1829426.svg", "isPro": false, "fullname": "Xinggang Wang", "user": "xinggangw", "type": "user"}, "name": "Xinggang Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T10:34:40.199Z", "hidden": false}], "publishedAt": "2025-12-15T18:59:54.000Z", "submittedOnDailyAt": "2025-12-16T07:11:50.997Z", "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation", "submittedOnDailyBy": {"_id": "67756c9c846a267749304255", "avatarUrl": "/avatars/01f09805b561887c55d1b9ad4e96b461.svg", "isPro": false, "fullname": "Jingfeng Yao", "user": "MapleF9", "type": "user"}, "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.", "upvotes": 66, "discussionId": "6940ee0665f1e24a11780671", "githubRepo": "https://github.com/MiniMax-AI/VTP", "githubRepoAddedBy": "user", "ai_summary": "A unified visual tokenizer pre-training framework (VTP) improves generative performance by optimizing image-text contrastive, self-supervised, and reconstruction losses, leading to better scaling properties and higher zero-shot accuracy and faster convergence.", "ai_keywords": ["latent space", "visual tokenizers", "VAEs", "reconstruction-based training", "low-level information", "pre-training scaling problem", "high-level semantics", "VTP", "image-text contrastive", "self-supervised", "reconstruction losses", "generative performance", "ImageNet", "zero-shot accuracy", "rFID", "FLOPS", "DiT", "advanced distillation methods"], "githubStars": 92, "organization": {"_id": "6778fc29920093dbc0c24917", "name": "MiniMaxAI", "fullname": "MiniMax", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/676e38ad04af5bec20bc9faf/dUd-LsZEX0H_d4qefO_g6.jpeg"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u6807\u8bb0\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\u8d28\u91cf\u5bf9\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u4f20\u7edf\u7684\u57fa\u4e8e\u91cd\u5efa\u7684\u8bad\u7ec3\u65b9\u6cd5\u5bfc\u81f4\u6f5c\u5728\u7a7a\u95f4\u504f\u5411\u4f4e\u7ea7\u4fe1\u606f\uff0c\u5f71\u54cd\u751f\u6210\u8d28\u91cf\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51faVTP\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u56fe\u50cf-\u6587\u672c\u5bf9\u6bd4\u3001\u81ea\u76d1\u7763\u548c\u91cd\u5efa\u635f\u5931\uff0c\u6539\u5584\u89c6\u89c9\u6807\u8bb0\u5668\u7684\u9884\u8bad\u7ec3\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\u7406\u89e3\u80fd\u529b\u662f\u751f\u6210\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u4e14\u751f\u6210\u6027\u80fd\u66f4\u597d\u5730\u4e0e\u8ba1\u7b97\u8d44\u6e90\u548c\u6570\u636e\u89c4\u6a21\u76f8\u5173\u3002</li>\n    <li>\u6211\u4eec\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u751f\u6210\u8d28\u91cf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The quality of visual tokenizers' latent space is important for generative models, but standard training methods focus too much on low-level details.</li>\n    <li>This focus leads to a \"pre-training scaling problem,\" where more computing power doesn't significantly improve generation quality.</li>\n    <li>To improve generation, the latent space should represent high-level concepts better.</li>\n    <li>The new VTP framework combines different training methods to optimize performance and shows better scaling with increased resources.</li>\n    <li>After pre-training, VTP achieves high accuracy and faster generation compared to existing methods, with significant improvements seen with more computing resources.</li>\n</ul>"}, "publishedAt": "2025-12-15T13:59:54.000Z", "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation", "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13687.png", "numComments": 1, "submittedBy": {"_id": "67756c9c846a267749304255", "avatarUrl": "/avatars/01f09805b561887c55d1b9ad4e96b461.svg", "fullname": "Jingfeng Yao", "name": "MapleF9", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "6778fc29920093dbc0c24917", "name": "MiniMaxAI", "fullname": "MiniMax", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/676e38ad04af5bec20bc9faf/dUd-LsZEX0H_d4qefO_g6.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.13564", "authors": [{"_id": "6940d68565f1e24a1178056c", "user": {"_id": "6544b9b646dbdeca34ee5f52", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6544b9b646dbdeca34ee5f52/nRx6m1C4wfZ_xSWoBUNJf.png", "isPro": false, "fullname": "Yuyang Hu", "user": "namespace-ERI", "type": "user"}, "name": "Yuyang Hu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:40.040Z", "hidden": false}, {"_id": "6940d68565f1e24a1178056d", "user": {"_id": "65435cad429b80b14922ab8d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/N8oWq4ZZn3dRxmXi18FrA.jpeg", "isPro": false, "fullname": "Shichun Liu", "user": "Liusc2020", "type": "user"}, "name": "Shichun Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:40.844Z", "hidden": false}, {"_id": "6940d68565f1e24a1178056e", "name": "Yanwei Yue", "hidden": false}, {"_id": "6940d68565f1e24a1178056f", "name": "Guibin Zhang", "hidden": false}, {"_id": "6940d68565f1e24a11780570", "name": "Boyang Liu", "hidden": false}, {"_id": "6940d68565f1e24a11780571", "name": "Fangyi Zhu", "hidden": false}, {"_id": "6940d68565f1e24a11780572", "name": "Jiahang Lin", "hidden": false}, {"_id": "6940d68565f1e24a11780573", "user": {"_id": "638ef0b0c67af472d31674a6", "avatarUrl": "/avatars/02df97d15a0f46b47f9162221733b121.svg", "isPro": false, "fullname": "Honglin Guo", "user": "KYLN24", "type": "user"}, "name": "Honglin Guo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:38.698Z", "hidden": false}, {"_id": "6940d68565f1e24a11780574", "name": "Shihan Dou", "hidden": false}, {"_id": "6940d68565f1e24a11780575", "name": "Zhiheng Xi", "hidden": false}, {"_id": "6940d68565f1e24a11780576", "name": "Senjie Jin", "hidden": false}, {"_id": "6940d68565f1e24a11780577", "user": {"_id": "62e52483a944e2a56cd2c6ca", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg", "isPro": false, "fullname": "Jiejun Tan", "user": "zstanjj", "type": "user"}, "name": "Jiejun Tan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:42.726Z", "hidden": false}, {"_id": "6940d68565f1e24a11780578", "name": "Yanbin Yin", "hidden": false}, {"_id": "6940d68565f1e24a11780579", "name": "Jiongnan Liu", "hidden": false}, {"_id": "6940d68565f1e24a1178057a", "name": "Zeyu Zhang", "hidden": false}, {"_id": "6940d68565f1e24a1178057b", "user": {"_id": "6309bfdab8d7b3889319b588", "avatarUrl": "/avatars/572acdad470f765ef2e058ead3741e24.svg", "isPro": false, "fullname": "SunZX", "user": "Jeryi", "type": "user"}, "name": "Zhongxiang Sun", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T22:32:57.018Z", "hidden": false}, {"_id": "6940d68565f1e24a1178057c", "name": "Yutao Zhu", "hidden": false}, {"_id": "6940d68565f1e24a1178057d", "name": "Hao Sun", "hidden": false}, {"_id": "6940d68565f1e24a1178057e", "name": "Boci Peng", "hidden": false}, {"_id": "6940d68565f1e24a1178057f", "name": "Zhenrong Cheng", "hidden": false}, {"_id": "6940d68565f1e24a11780580", "name": "Xuanbo Fan", "hidden": false}, {"_id": "6940d68565f1e24a11780581", "name": "Jiaxin Guo", "hidden": false}, {"_id": "6940d68565f1e24a11780582", "name": "Xinlei Yu", "hidden": false}, {"_id": "6940d68565f1e24a11780583", "name": "Zhenhong Zhou", "hidden": false}, {"_id": "6940d68565f1e24a11780584", "name": "Zewen Hu", "hidden": false}, {"_id": "6940d68565f1e24a11780585", "name": "Jiahao Huo", "hidden": false}, {"_id": "6940d68565f1e24a11780586", "name": "Junhao Wang", "hidden": false}, {"_id": "6940d68565f1e24a11780587", "name": "Yuwei Niu", "hidden": false}, {"_id": "6940d68565f1e24a11780588", "name": "Yu Wang", "hidden": false}, {"_id": "6940d68565f1e24a11780589", "name": "Zhenfei Yin", "hidden": false}, {"_id": "6940d68565f1e24a1178058a", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6940d68565f1e24a1178058b", "name": "Yue Liao", "hidden": false}, {"_id": "6940d68565f1e24a1178058c", "name": "Qiankun Li", "hidden": false}, {"_id": "6940d68565f1e24a1178058d", "name": "Kun Wang", "hidden": false}, {"_id": "6940d68565f1e24a1178058e", "name": "Wangchunshu Zhou", "hidden": false}, {"_id": "6940d68565f1e24a1178058f", "name": "Yixin Liu", "hidden": false}, {"_id": "6940d68565f1e24a11780590", "name": "Dawei Cheng", "hidden": false}, {"_id": "6940d68565f1e24a11780591", "name": "Qi Zhang", "hidden": false}, {"_id": "6940d68565f1e24a11780592", "name": "Tao Gui", "hidden": false}, {"_id": "6940d68565f1e24a11780593", "name": "Shirui Pan", "hidden": false}, {"_id": "6940d68565f1e24a11780594", "name": "Yan Zhang", "hidden": false}, {"_id": "6940d68565f1e24a11780595", "name": "Philip Torr", "hidden": false}, {"_id": "6940d68565f1e24a11780596", "name": "Zhicheng Dou", "hidden": false}, {"_id": "6940d68565f1e24a11780597", "name": "Ji-Rong Wen", "hidden": false}, {"_id": "6940d68565f1e24a11780598", "name": "Xuanjing Huang", "hidden": false}, {"_id": "6940d68565f1e24a11780599", "name": "Yu-Gang Jiang", "hidden": false}, {"_id": "6940d68565f1e24a1178059a", "name": "Shuicheng Yan", "hidden": false}], "publishedAt": "2025-12-15T17:22:34.000Z", "submittedOnDailyAt": "2025-12-16T01:18:34.363Z", "title": "Memory in the Age of AI Agents", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.", "upvotes": 62, "discussionId": "6940d68565f1e24a1178059b", "githubRepo": "https://github.com/Shichun-Liu/Agent-Memory-Paper-List", "githubRepoAddedBy": "user", "ai_summary": "This survey provides an updated overview of agent memory research, distinguishing its forms, functions, and dynamics, and highlights emerging research directions.", "ai_keywords": ["agent memory", "LLM memory", "retrieval augmented generation (RAG)", "context engineering", "token-level memory", "parametric memory", "latent memory", "factual memory", "experiential memory", "working memory", "memory benchmarks", "open-source frameworks", "memory automation", "reinforcement learning integration", "multimodal memory", "multi-agent memory", "trustworthiness issues"], "githubStars": 115, "summary_zh": "<ul>\n    <li>\u8bb0\u5fc6\u662f\u57fa\u7840\u6a21\u578b\u4ee3\u7406\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u7814\u7a76\u5728\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u9886\u57df\u53d8\u5f97\u8d8a\u6765\u8d8a\u5206\u6563\u3002</li>\n    <li>\u73b0\u6709\u7684\u4ee3\u7406\u8bb0\u5fc6\u7814\u7a76\u5728\u52a8\u673a\u3001\u5b9e\u73b0\u548c\u8bc4\u4f30\u4e0a\u5dee\u5f02\u5f88\u5927\uff0c\u672f\u8bed\u4f7f\u7528\u4e0d\u591f\u6e05\u6670\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u5f53\u524d\u4ee3\u7406\u8bb0\u5fc6\u7814\u7a76\u7684\u6700\u65b0\u6982\u51b5\uff0c\u5e76\u660e\u786e\u533a\u5206\u4ee3\u7406\u8bb0\u5fc6\u4e0e\u76f8\u5173\u6982\u5ff5\u3002</li>\n    <li>\u4ece\u5f62\u5f0f\u3001\u529f\u80fd\u548c\u52a8\u6001\u4e09\u4e2a\u89d2\u5ea6\u5206\u6790\u4ee3\u7406\u8bb0\u5fc6\uff0c\u8bc6\u522b\u51fa\u4e09\u79cd\u4e3b\u8981\u5f62\u5f0f\uff1a\u4ee4\u724c\u7ea7\u3001\u53c2\u6570\u5316\u548c\u6f5c\u5728\u8bb0\u5fc6\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u65b0\u5174\u7814\u7a76\u524d\u6cbf\u7684\u5c55\u671b\uff0c\u5305\u62ec\u8bb0\u5fc6\u81ea\u52a8\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u96c6\u6210\u548c\u591a\u6a21\u6001\u8bb0\u5fc6\u7b49\u95ee\u9898\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Memory is a key feature of agents based on foundation models and is gaining more attention in research.</li>\n    <li>The field of agent memory research is fragmented, with differences in motivations, implementations, and evaluation methods.</li>\n    <li>Traditional classifications of memory (like long/short-term) are not enough to cover the variety of agent memory systems today.</li>\n    <li>This work outlines the current state of agent memory research, defining it clearly and distinguishing it from similar concepts.</li>\n    <li>It identifies types of memory, proposes a detailed taxonomy, and discusses future research areas such as memory automation and multimodal memory.</li>\n</ul>"}, "publishedAt": "2025-12-15T12:22:34.000Z", "title": "Memory in the Age of AI Agents", "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13564.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 185}, "isAuthorParticipating": false}, {"paper": {"id": "2512.10430", "authors": [{"_id": "693bba8d9874a2a5e4ffb3ab", "user": {"_id": "64f4c8739ee58d48e8507e0e", "avatarUrl": "/avatars/4be540dfb4a949f37cba2d3c3729fbde.svg", "isPro": false, "fullname": "Dmitrii Stoianov", "user": "heylimon", "type": "user"}, "name": "Dmitrii Stoianov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:40.198Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ac", "user": {"_id": "64fb054ebb362cbf2fe53159", "avatarUrl": "/avatars/936c37a77d46d0ea579d2f8a9aea9284.svg", "isPro": false, "fullname": "Danil Taranets", "user": "taranetsdan", "type": "user"}, "name": "Danil Taranets", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:29.281Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ad", "user": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "isPro": false, "fullname": "Olga Tsymboi", "user": "oltsy", "type": "user"}, "name": "Olga Tsymboi", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:38.180Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3ae", "user": {"_id": "6780dcd6acf8d824c03864da", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6PeN6OXbSq0M-L4OxFTrn.png", "isPro": false, "fullname": "Ramil Latypov", "user": "kylecr4ne", "type": "user"}, "name": "Ramil Latypov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:23.437Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3af", "user": {"_id": "6513f03e86d74f32ed65e3b8", "avatarUrl": "/avatars/c327966623f775a2d1f3d984ca162ef6.svg", "isPro": false, "fullname": "Almaz Dautov", "user": "the-hir0", "type": "user"}, "name": "Almaz Dautov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:30.990Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b0", "user": {"_id": "621a8daf325b927e60fcef08", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621a8daf325b927e60fcef08/bM8W-of2u0yvL8FHeY2ra.jpeg", "isPro": false, "fullname": "Vladislav Kruglikov", "user": "vladislavkruglikov", "type": "user"}, "name": "Vladislav Kruglikov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:21.170Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b1", "name": "Nikita Surkov", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b2", "user": {"_id": "63188c428d698d8c1642a0d8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63188c428d698d8c1642a0d8/MlcBnU7CmnKdRF053JcY4.jpeg", "isPro": false, "fullname": "German Abramov", "user": "germanjke", "type": "user"}, "name": "German Abramov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T12:59:28.579Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b3", "name": "Pavel Gein", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b4", "user": {"_id": "636a9a07e3ad78bc68b1a5a2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668020490988-636a9a07e3ad78bc68b1a5a2.jpeg", "isPro": false, "fullname": "Dmitry Abulkhanov", "user": "mponty", "type": "user"}, "name": "Dmitry Abulkhanov", "status": "admin_assigned", "statusLastChangedAt": "2025-12-12T13:02:42.107Z", "hidden": true}, {"_id": "693bba8d9874a2a5e4ffb3b5", "user": {"_id": "658bc20cfdf2279d4721f218", "avatarUrl": "/avatars/5f1cb94373fbbbcfed9b848c5ebdd1ad.svg", "isPro": false, "fullname": "Mikhail Gashkov", "user": "MikeGashkov", "type": "user"}, "name": "Mikhail Gashkov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:32.809Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b6", "name": "Viktor Zelenkovskiy", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b7", "user": {"_id": "644f64bc17b6189cda54cae8", "avatarUrl": "/avatars/f684a65b35a9be06cbb16fb8f44a4782.svg", "isPro": false, "fullname": "Artem Batalov", "user": "batalovme", "type": "user"}, "name": "Artem Batalov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:27.497Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b8", "user": {"_id": "62609d224e6e4b84475eb8d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62609d224e6e4b84475eb8d9/PKQvuLm40PGg91VKRVSIb.jpeg", "isPro": false, "fullname": "Alex Medvedev", "user": "kenkaneki", "type": "user"}, "name": "Aleksandr Medvedev", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:36.035Z", "hidden": false}, {"_id": "693bba8d9874a2a5e4ffb3b9", "user": {"_id": "63f26358be95ed4c9a9b0583", "avatarUrl": "/avatars/133f2d28c5e5139d61048dfef5e9f4ff.svg", "isPro": false, "fullname": "Anatoly Potapov", "user": "AnatoliiPotapov", "type": "user"}, "name": "Anatolii Potapov", "status": "claimed_verified", "statusLastChangedAt": "2025-12-12T09:14:25.666Z", "hidden": false}], "publishedAt": "2025-12-11T08:40:10.000Z", "submittedOnDailyAt": "2025-12-12T08:33:32.798Z", "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground", "submittedOnDailyBy": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "isPro": false, "fullname": "Olga Tsymboi", "user": "oltsy", "type": "user"}, "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.", "upvotes": 60, "discussionId": "693bba8e9874a2a5e4ffb3ba", "ai_summary": "T-pro 2.0 is an open-weight Russian LLM for hybrid reasoning and efficient inference, using a Cyrillic-dense tokenizer and EAGLE speculative-decoding pipeline.", "ai_keywords": ["Cyrillic-dense tokenizer", "EAGLE speculative-decoding pipeline", "hybrid reasoning", "efficient inference", "reasoning-trace generation"], "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86T-pro 2.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u4fc4\u7f57\u65af\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u6df7\u5408\u63a8\u7406\u548c\u9ad8\u6548\u63a8\u7406\u3002</li>\n    <li>\u8be5\u6a21\u578b\u53ef\u4ee5\u76f4\u63a5\u56de\u7b54\u95ee\u9898\uff0c\u5e76\u751f\u6210\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f7f\u7528\u4e86\u5bc6\u96c6\u7684\u897f\u91cc\u5c14\u5b57\u7b26\u6807\u8bb0\u5668\u548c\u6539\u8fdb\u7684EAGLE\u89e3\u7801\u6d41\u7a0b\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u3002</li>\n    <li>\u4e3a\u4fbf\u4e8e\u7814\u7a76\uff0c\u53d1\u5e03\u4e86\u6a21\u578b\u6743\u91cd\u3001T-Wix 500k\u6307\u4ee4\u8bed\u6599\u5e93\u3001T-Math\u63a8\u7406\u57fa\u51c6\u548cEAGLE\u6743\u91cd\uff0c\u5747\u53ef\u5728Hugging Face\u4e0a\u83b7\u53d6\u3002</li>\n    <li>\u63d0\u4f9b\u4e86\u4e00\u4e2a\u516c\u5171\u7f51\u7edc\u6f14\u793a\uff0c\u5c55\u793a\u4e86\u63a8\u7406\u548c\u975e\u63a8\u7406\u6a21\u5f0f\uff0c\u4ee5\u53ca\u5728\u4e0d\u540c\u9886\u57df\u4e2d\u5b9e\u73b0\u7684\u901f\u5ea6\u63d0\u5347\u3002</li>\n    <li>T-pro 2.0\u4e3a\u6784\u5efa\u548c\u8bc4\u4f30\u9ad8\u6548\u5b9e\u7528\u7684\u4fc4\u7f57\u65af\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u8bbf\u95ee\u7684\u5f00\u653e\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>T-pro 2.0 is a new Russian language model designed for quick reasoning and answering.</li>\n    <li>It uses a special tokenizer and a fast decoding method to improve performance.</li>\n    <li>The model, along with additional resources, is available on Hugging Face for research and development.</li>\n    <li>A public demo shows how the model can reason and work quickly in different areas.</li>\n    <li>T-pro 2.0 is aimed at helping users create and test efficient applications in Russian.</li>\n</ul>"}, "publishedAt": "2025-12-11T03:40:10.000Z", "title": "T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground", "summary": "We introduce T-pro 2.0, an open-weight Russian LLM for hybrid reasoning and efficient inference. The model supports direct answering and reasoning-trace generation, using a Cyrillic-dense tokenizer and an adapted EAGLE speculative-decoding pipeline to reduce latency. To enable reproducible and extensible research, we release the model weights, the T-Wix 500k instruction corpus, the T-Math reasoning benchmark, and the EAGLE weights on Hugging Face. These resources allow users to study Russian-language reasoning and to extend or adapt both the model and the inference pipeline. A public web demo exposes reasoning and non-reasoning modes and illustrates the speedups achieved by our inference stack across domains. T-pro 2.0 thus serves as an accessible open system for building and evaluating efficient, practical Russian LLM applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.10430.png", "numComments": 1, "submittedBy": {"_id": "6612fe63da0c53de48c7ce3b", "avatarUrl": "/avatars/207c80b5da078239371a31b17f63ccfd.svg", "fullname": "Olga Tsymboi", "name": "oltsy", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "organization": {"_id": "675861e944dbb69c2673c71c", "name": "t-tech", "fullname": "T-Tech", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/674ea07d320a043daeb2d98b/IwSCMolFY4Otk7sFXzWhi.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.12967", "authors": [{"_id": "6940d25d65f1e24a11780417", "user": {"_id": "64777a346e6c7ac608c1e9bf", "avatarUrl": "/avatars/b0e65ba781c90c2560606eb5467101eb.svg", "isPro": false, "fullname": "Weizhou Shen", "user": "shenwzh3", "type": "user"}, "name": "Weizhou Shen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:42.079Z", "hidden": false}, {"_id": "6940d25d65f1e24a11780418", "user": {"_id": "64c9b0f28d2d187c24d1e6c1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1CPnAaB3gsupdpiNWaoDc.png", "isPro": false, "fullname": "ZiYi Yang", "user": "AALF", "type": "user"}, "name": "Ziyi Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:56.062Z", "hidden": false}, {"_id": "6940d25d65f1e24a11780419", "name": "Chenliang Li", "hidden": false}, {"_id": "6940d25d65f1e24a1178041a", "name": "Zhiyuan Lu", "hidden": false}, {"_id": "6940d25d65f1e24a1178041b", "name": "Miao Peng", "hidden": false}, {"_id": "6940d25d65f1e24a1178041c", "name": "Huashan Sun", "hidden": false}, {"_id": "6940d25d65f1e24a1178041d", "name": "Yingcheng Shi", "hidden": false}, {"_id": "6940d25d65f1e24a1178041e", "name": "Shengyi Liao", "hidden": false}, {"_id": "6940d25d65f1e24a1178041f", "name": "Shaopeng Lai", "hidden": false}, {"_id": "6940d25d65f1e24a11780420", "name": "Bo Zhang", "hidden": false}, {"_id": "6940d25d65f1e24a11780421", "name": "Dayiheng Liu", "hidden": false}, {"_id": "6940d25d65f1e24a11780422", "name": "Fei Huang", "hidden": false}, {"_id": "6940d25d65f1e24a11780423", "name": "Jingren Zhou", "hidden": false}, {"_id": "6940d25d65f1e24a11780424", "name": "Ming Yan", "hidden": false}], "publishedAt": "2025-12-15T04:11:11.000Z", "submittedOnDailyAt": "2025-12-16T01:29:41.007Z", "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.", "upvotes": 54, "discussionId": "6940d25d65f1e24a11780425", "githubRepo": "https://github.com/Tongyi-Zhiwen/Qwen-Doc", "githubRepoAddedBy": "user", "ai_summary": "QwenLong-L1.5 enhances long-context reasoning through data synthesis, stabilized reinforcement learning, and memory-augmented architecture, achieving superior performance on benchmarks and general domains.", "ai_keywords": ["Long-Context Data Synthesis Pipeline", "atomic facts", "verifiable reasoning questions", "Stabilized Reinforcement Learning", "task-balanced sampling", "task-specific advantage estimation", "Adaptive Entropy-Controlled Policy Optimization", "Memory-Augmented Architecture", "multi-stage fusion RL training", "single-pass reasoning", "iterative memory-based processing", "memory-agent framework"], "githubStars": 311, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>QwenLong-L1.5\u6a21\u578b\u901a\u8fc7\u521b\u65b0\u7684\u540e\u8bad\u7ec3\u6280\u672f\uff0c\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u4e86\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u751f\u6210\u9700\u8981\u591a\u6b65\u63a8\u7406\u7684\u6311\u6218\u6027\u4efb\u52a1\uff0c\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u3002</li>\n    <li>\u901a\u8fc7\u4efb\u52a1\u5e73\u8861\u91c7\u6837\u548c\u81ea\u9002\u5e94\u71b5\u63a7\u5236\u7b56\u7565\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u5185\u5b58\u589e\u5f3a\u67b6\u6784\uff0c\u80fd\u591f\u5904\u7406\u8d85\u8fc7400\u4e07\u6807\u8bb0\u7684\u8d85\u957f\u4e0a\u4e0b\u6587\uff0c\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cQwenLong-L1.5\u7684\u8868\u73b0\u4e0eGPT-5\u548cGemini-2.5-Pro\u76f8\u5f53\uff0c\u5e73\u5747\u63d0\u9ad8\u4e869.90\u5206\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>QwenLong-L1.5 is a new model designed for better reasoning over long pieces of text.</li>\n    <li>It uses a special method to create challenging questions based on facts and their relationships, which helps improve its training data.</li>\n    <li>The model includes a new approach to reinforcement learning that helps stabilize training when dealing with long contexts.</li>\n    <li>It has a memory management system that allows it to handle very long text sequences (over 4 million tokens) effectively.</li>\n    <li>QwenLong-L1.5 performs comparably to other advanced models like GPT-5 and shows improvements in various reasoning tasks.</li>\n</ul>"}, "publishedAt": "2025-12-14T23:11:11.000Z", "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management", "summary": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.12967.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 185}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.13604", "authors": [{"_id": "6940d44165f1e24a11780535", "name": "Jianxiong Gao", "hidden": false}, {"_id": "6940d44165f1e24a11780536", "name": "Zhaoxi Chen", "hidden": false}, {"_id": "6940d44165f1e24a11780537", "name": "Xian Liu", "hidden": false}, {"_id": "6940d44165f1e24a11780538", "user": {"_id": "64970d3d9c3b29dca8633f87", "avatarUrl": "/avatars/11e3c9c66d28490d6d09925f9aa47cd1.svg", "isPro": false, "fullname": "JunhaoZhuang", "user": "JunhaoZhuang", "type": "user"}, "name": "Junhao Zhuang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:51.620Z", "hidden": false}, {"_id": "6940d44165f1e24a11780539", "name": "Chengming Xu", "hidden": false}, {"_id": "6940d44165f1e24a1178053a", "name": "Jianfeng Feng", "hidden": false}, {"_id": "6940d44165f1e24a1178053b", "name": "Yu Qiao", "hidden": false}, {"_id": "6940d44165f1e24a1178053c", "name": "Yanwei Fu", "hidden": false}, {"_id": "6940d44165f1e24a1178053d", "user": {"_id": "635f8ed47c05eb9f59963d3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg", "isPro": false, "fullname": "ChenyangSi", "user": "ChenyangSi", "type": "user"}, "name": "Chenyang Si", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:39:49.025Z", "hidden": false}, {"_id": "6940d44165f1e24a1178053e", "name": "Ziwei Liu", "hidden": false}], "publishedAt": "2025-12-15T17:59:58.000Z", "submittedOnDailyAt": "2025-12-16T01:12:24.486Z", "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model", "submittedOnDailyBy": {"_id": "643815c4961bb61e463c5896", "avatarUrl": "/avatars/3b44592472f16c56105bff8c314d9939.svg", "isPro": false, "fullname": "Jianxiong Gao", "user": "Jianxiong", "type": "user"}, "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.", "upvotes": 52, "discussionId": "6940d44165f1e24a1178053f", "projectPage": "https://vchitect.github.io/LongVie2-project/", "ai_summary": "LongVie 2, an end-to-end autoregressive framework, enhances controllability, visual quality, and temporal consistency in video world models through three progressive training stages.", "ai_keywords": ["autoregressive framework", "multi-modal guidance", "degradation-aware training", "history-context guidance", "video world models", "controllability", "visual quality", "temporal consistency", "LongVGenBench", "state-of-the-art performance", "temporal coherence", "visual fidelity"], "summary_zh": "<ul>\n    <li>\u6784\u5efa\u57fa\u4e8e\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u7cfb\u7edf\u7684\u89c6\u9891\u4e16\u754c\u6a21\u578b\u662f\u5b9e\u73b0\u7a7a\u95f4\u65f6\u95f4\u667a\u80fd\u7684\u91cd\u8981\u4e00\u6b65\u3002</li>\n    <li>\u6709\u6548\u7684\u4e16\u754c\u6a21\u578b\u5e94\u5177\u5907\u53ef\u63a7\u6027\u3001\u957f\u671f\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e09\u4e2a\u57fa\u672c\u7279\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86LongVie 2\uff0c\u4e00\u4e2a\u7ecf\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u7684\u7aef\u5230\u7aef\u81ea\u56de\u5f52\u6846\u67b6\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5305\u542b\u591a\u6a21\u6001\u6307\u5bfc\u3001\u964d\u8d28\u611f\u77e5\u8bad\u7ec3\u548c\u5386\u53f2\u4e0a\u4e0b\u6587\u6307\u5bfc\uff0c\u4ee5\u63d0\u5347\u63a7\u5236\u80fd\u529b\u548c\u89c6\u89c9\u8d28\u91cf\u3002</li>\n    <li>LongVie 2\u5728\u957f\u7a0b\u53ef\u63a7\u6027\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u5e76\u652f\u6301\u6700\u957f\u4e94\u5206\u949f\u7684\u89c6\u9891\u8fde\u7eed\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongVie 2 is a new system designed to improve video generation and understanding over time.</li>\n    <li>It focuses on three key features: control over video output, maintaining high visual quality, and keeping a consistent look across frames.</li>\n    <li>The system is developed in three stages to enhance these features effectively.</li>\n    <li>It includes a new benchmark, LongVGenBench, which tests video generation using various real and fake video environments.</li>\n    <li>LongVie 2 outperforms existing methods in controllability, visual quality, and can create videos of up to five minutes long.</li>\n</ul>"}, "publishedAt": "2025-12-15T12:59:58.000Z", "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model", "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13604.png", "numComments": 1, "submittedBy": {"_id": "643815c4961bb61e463c5896", "avatarUrl": "/avatars/3b44592472f16c56105bff8c314d9939.svg", "fullname": "Jianxiong Gao", "name": "Jianxiong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13281", "authors": [{"_id": "6940d82465f1e24a117805c2", "name": "Jiaqi Wang", "hidden": false}, {"_id": "6940d82465f1e24a117805c3", "name": "Weijia Wu", "hidden": false}, {"_id": "6940d82465f1e24a117805c4", "name": "Yi Zhan", "hidden": false}, {"_id": "6940d82465f1e24a117805c5", "name": "Rui Zhao", "hidden": false}, {"_id": "6940d82465f1e24a117805c6", "name": "Ming Hu", "hidden": false}, {"_id": "6940d82465f1e24a117805c7", "name": "James Cheng", "hidden": false}, {"_id": "6940d82465f1e24a117805c8", "name": "Wei Liu", "hidden": false}, {"_id": "6940d82465f1e24a117805c9", "name": "Philip Torr", "hidden": false}, {"_id": "6940d82465f1e24a117805ca", "user": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "isPro": false, "fullname": "Qinghong (Kevin) Lin", "user": "KevinQHLin", "type": "user"}, "name": "Kevin Qinghong Lin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T14:04:37.407Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/g2dRdpFY8mVSrBnrK3C0q.mp4"], "publishedAt": "2025-12-15T12:41:23.000Z", "submittedOnDailyAt": "2025-12-17T08:26:01.077Z", "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?", "submittedOnDailyBy": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "isPro": false, "fullname": "Qinghong (Kevin) Lin", "user": "KevinQHLin", "type": "user"}, "summary": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: (i) Immersive ASMR video-audio sources. Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. (ii) Peer-Review evaluation. An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\\% accuracy (random 50\\%), far below that of human experts (81.25\\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.", "upvotes": 50, "discussionId": "6940d82565f1e24a117805cb", "projectPage": "https://video-reality-test.github.io/", "githubRepo": "https://github.com/video-reality-test/video-reality-test", "githubRepoAddedBy": "user", "ai_summary": "The Video Reality Test benchmark evaluates the realism and detection of AI-generated ASMR videos with audio, revealing that even the best models can deceive VLMs and humans, highlighting limitations in perceptual fidelity and audio-visual consistency.", "ai_keywords": ["ASMR", "audio-visual coupling", "Veo3.1-Fast", "Gemini 2.5-Pro", "perceptual realism", "real-fake discrimination", "audio-visual consistency"], "githubStars": 13, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u751f\u6210\u6280\u672f\u7684\u8fdb\u6b65\u4f7f\u5f97AI\u751f\u6210\u7684\u89c6\u9891\u4e0e\u771f\u5b9e\u89c6\u9891\u51e0\u4e4e\u65e0\u6cd5\u533a\u5206\uff0c\u8fd9\u7ed9\u68c0\u6d4b\u5e26\u6765\u4e86\u6311\u6218\u3002</li>\n    <li>\u73b0\u6709\u7684\u68c0\u6d4b\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u6ca1\u6709\u97f3\u9891\u7684\u89c6\u9891\uff0c\u5173\u6ce8\u5e7f\u6cdb\u7684\u53d9\u4e8b\u9886\u57df\uff0c\u5e76\u4e14\u4e3b\u8981\u4e13\u6ce8\u4e8e\u5206\u7c7b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u89c6\u9891\u73b0\u5b9e\u6d4b\u8bd5\u201d\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8eASMR\u89c6\u9891\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u6d4b\u8bd5\u97f3\u89c6\u9891\u7ed3\u5408\u4e0b\u7684\u611f\u77e5\u771f\u5b9e\u5ea6\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6700\u597d\u7684\u751f\u6210\u6a21\u578bVeo3.1-Fast\u80fd\u591f\u6b3a\u9a97\u5927\u591a\u6570VLMs\uff08\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\uff0c\u5176\u51c6\u786e\u7387\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u3002</li>\n    <li>\u6dfb\u52a0\u97f3\u9891\u53ef\u4ee5\u63d0\u9ad8\u771f\u5b9e\u4e0e\u865a\u5047\u89c6\u9891\u7684\u533a\u5206\u80fd\u529b\uff0c\u4f46\u67d0\u4e9b\u8868\u9762\u7ebf\u7d22\uff08\u5982\u6c34\u5370\uff09\u4ecd\u7136\u4f1a\u8bef\u5bfc\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>AI-generated videos are becoming so realistic that it is hard to tell them apart from real videos, creating challenges in detection.</li>\n    <li>Previous tests for detecting AI-generated content mostly focused on videos without sound and didn't assess how well audio and visuals work together.</li>\n    <li>The new benchmark, called Video Reality Test, uses ASMR videos to evaluate how convincing audio-visual videos are.</li>\n    <li>In tests, the best AI video generator fooled most models that try to detect fakes, with only 56% accuracy compared to 81.25% for human experts.</li>\n    <li>While adding audio helps in detecting fake videos, some features like watermarks can still trick detection models.</li>\n</ul>"}, "publishedAt": "2025-12-15T07:41:23.000Z", "title": "Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?", "summary": "Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: (i) Immersive ASMR video-audio sources. Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. (ii) Peer-Review evaluation. An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\\% accuracy (random 50\\%), far below that of human experts (81.25\\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64440be5af034cdfd69ca3a7/g2dRdpFY8mVSrBnrK3C0q.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13281.png", "numComments": 2, "submittedBy": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "fullname": "Qinghong (Kevin) Lin", "name": "KevinQHLin", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 41}, "isAuthorParticipating": true}, {"paper": {"id": "2512.14614", "authors": [{"_id": "694219f25d5b2dc1052747ff", "user": {"_id": "64897b1f0ec897cfe579a399", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64897b1f0ec897cfe579a399/ICR_75b877BaSE94gjBuj.jpeg", "isPro": false, "fullname": "wenq", "user": "wenqsun", "type": "user"}, "name": "Wenqiang Sun", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:38.348Z", "hidden": false}, {"_id": "694219f25d5b2dc105274800", "name": "Haiyu Zhang", "hidden": false}, {"_id": "694219f25d5b2dc105274801", "name": "Haoyuan Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274802", "name": "Junta Wu", "hidden": false}, {"_id": "694219f25d5b2dc105274803", "name": "Zehan Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274804", "name": "Zhenwei Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274805", "name": "Yunhong Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274806", "name": "Jun Zhang", "hidden": false}, {"_id": "694219f25d5b2dc105274807", "name": "Tengfei Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274808", "name": "Chunchao Guo", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/z23kgc1LgY1qvs-Tfp4zj.mp4"], "publishedAt": "2025-12-16T17:22:46.000Z", "submittedOnDailyAt": "2025-12-17T00:24:30.301Z", "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.", "upvotes": 48, "discussionId": "694219f35d5b2dc105274809", "projectPage": "https://3d-models.hunyuan.tencent.com/world/", "githubRepo": "https://github.com/Tencent-Hunyuan/HY-WorldPlay", "githubRepoAddedBy": "user", "ai_summary": "WorldPlay is a streaming video diffusion model that achieves real-time, interactive world modeling with long-term geometric consistency by using a Dual Action Representation, Reconstituted Context Memory, and Context Forcing.", "ai_keywords": ["Dual Action Representation", "Reconstituted Context Memory", "temporal reframing", "Context Forcing", "memory-aware model", "long-horizon streaming video"], "githubStars": 302, "summary_zh": "<ul>\n    <li>WorldPlay \u662f\u4e00\u79cd\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u7684\u4e92\u52a8\u4e16\u754c\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u901f\u5ea6\u548c\u5185\u5b58\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u4e86\u53cc\u91cd\u52a8\u4f5c\u8868\u793a\uff0c\u80fd\u591f\u5728\u7528\u6237\u8f93\u5165\u65f6\u63d0\u4f9b\u5f3a\u5927\u7684\u52a8\u4f5c\u63a7\u5236\u3002</li>\n    <li>\u901a\u8fc7\u91cd\u6784\u4e0a\u4e0b\u6587\u8bb0\u5fc6\uff0cWorldPlay \u52a8\u6001\u91cd\u5efa\u8fc7\u53bb\u7684\u5e27\uff0c\u4fdd\u6301\u51e0\u4f55\u91cd\u8981\u7684\u957f\u65f6\u95f4\u5e27\u53ef\u8bbf\u95ee\uff0c\u4ece\u800c\u786e\u4fdd\u957f\u671f\u4e00\u81f4\u6027\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u5f3a\u5236\u7684\u65b9\u6cd5\uff0c\u4ee5\u4fdd\u6301\u6a21\u578b\u5728\u5b9e\u65f6\u901f\u5ea6\u4e0b\u7684\u957f\u7a0b\u4fe1\u606f\u4f7f\u7528\u80fd\u529b\uff0c\u9632\u6b62\u9519\u8bef\u6f02\u79fb\u3002</li>\n    <li>WorldPlay \u53ef\u4ee5\u751f\u6210720p\u300124\u5e27\u6bcf\u79d2\u7684\u9ad8\u8d28\u91cf\u6d41\u89c6\u9891\uff0c\u5e76\u4e14\u5728\u591a\u79cd\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>WorldPlay is a new video streaming model that allows for real-time interactive world modeling with consistent geometric accuracy.</li>\n    <li>It features a Dual Action Representation for better control based on user inputs from the keyboard and mouse.</li>\n    <li>The model uses a Reconstituted Context Memory to maintain important past information, helping with long-term consistency.</li>\n    <li>Context Forcing is a new method that helps align memory between parts of the model to enable fast processing while reducing errors.</li>\n    <li>WorldPlay can produce high-quality 720p video at 24 frames per second, showing improved performance compared to current methods.</li>\n</ul>"}, "publishedAt": "2025-12-16T12:22:46.000Z", "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/z23kgc1LgY1qvs-Tfp4zj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14614.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13168", "authors": [{"_id": "6940eedd65f1e24a11780673", "user": {"_id": "637b08057ce76c3b834da15d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PFmI1HNcZlHF0RoLjP3c2.png", "isPro": false, "fullname": "Haoyu Dong", "user": "HaoyuDong", "type": "user"}, "name": "Haoyu Dong", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:37:23.964Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780674", "user": {"_id": "684ae4a3a6d604475ef72f22", "avatarUrl": "/avatars/70e61614ab115f07361c0baec351255a.svg", "isPro": false, "fullname": "Pengkun Zhang", "user": "logicluo", "type": "user"}, "name": "Pengkun Zhang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T15:09:52.673Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780675", "user": {"_id": "69369fc780cb6886b08dcf5b", "avatarUrl": "/avatars/1159e73666a55706e551afd4518adf0b.svg", "isPro": false, "fullname": "Yan Gao", "user": "gaoyansdyt", "type": "user"}, "name": "Yan Gao", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:47:22.721Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780676", "user": {"_id": "69377b51e5af592ad600e1c5", "avatarUrl": "/avatars/72612a4a69da2bd565ed4777287cff86.svg", "isPro": false, "fullname": "Xuanyu Dong", "user": "KcNcooo", "type": "user"}, "name": "Xuanyu Dong", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T14:13:14.674Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780677", "user": {"_id": "6777bb751ee6b20009098ab6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6777bb751ee6b20009098ab6/OumKsg54DNDH8qkslg95x.jpeg", "isPro": false, "fullname": "Yilin Cheng", "user": "Yilin-Cheng24", "type": "user"}, "name": "Yilin Cheng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T10:18:24.145Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780678", "user": {"_id": "694135abb4ad5b6ca6c4c9fe", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/694135abb4ad5b6ca6c4c9fe/ZFl3u8k69DZpyErVLYRBk.jpeg", "isPro": false, "fullname": "Mingzhe Lu", "user": "metaphor2ml", "type": "user"}, "name": "Mingzhe Lu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T14:10:36.238Z", "hidden": false}, {"_id": "6940eedd65f1e24a11780679", "user": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "isPro": false, "fullname": "Adina Yakefu", "user": "AdinaY", "type": "user"}, "name": "Adina Yakefu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-16T09:47:16.061Z", "hidden": false}, {"_id": "6940eedd65f1e24a1178067a", "name": "Shuxin Zheng", "hidden": false}], "publishedAt": "2025-12-15T10:28:45.000Z", "submittedOnDailyAt": "2025-12-16T09:53:57.731Z", "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows", "submittedOnDailyBy": {"_id": "637b08057ce76c3b834da15d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PFmI1HNcZlHF0RoLjP3c2.png", "isPro": false, "fullname": "Haoyu Dong", "user": "HaoyuDong", "type": "user"}, "summary": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.", "upvotes": 46, "discussionId": "6940eedd65f1e24a1178067b", "projectPage": "https://huggingface.co/datasets/FinWorkBench/Finch", "ai_summary": "Finch, a benchmark for AI agents in enterprise finance and accounting, evaluates performance across complex, real-world workflows using authentic data from Enron and other institutions.", "ai_keywords": ["LLM-assisted discovery", "expert annotation", "composite workflows", "spreadsheets", "PDFs", "AI systems", "GPT 5.1", "Claude Sonnet 4.5", "Gemini 3 Pro", "Grok 4", "Qwen 3 Max"], "organization": {"_id": "690e8a61e75888a0f707c0ee", "name": "FinWorkBench", "fullname": "Finch: Finance and Accounting Workflow Benchmark", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/637b08057ce76c3b834da15d/0HrDjcjNQK-5-FCZfkLzp.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u7684\u8d22\u52a1\u4e0e\u4f1a\u8ba1\u57fa\u51c6\uff08Finch\uff09\uff0c\u8be5\u57fa\u51c6\u6db5\u76d6\u591a\u4e2a\u4e13\u4e1a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6bd4\u5982\u6570\u636e\u5f55\u5165\u3001\u683c\u5f0f\u5316\u3001\u7f51\u9875\u641c\u7d22\u7b49\u3002</li>\n    <li>Finch\u7684\u6570\u636e\u6765\u81ea\u4e8eEnron\u7b49\u4f01\u4e1a\u7684\u771f\u5b9e\u5de5\u4f5c\u73af\u5883\uff0c\u5305\u62ec15,000\u4e2a\u7535\u5b50\u8868\u683c\u548c500,000\u5c01\u7535\u5b50\u90ae\u4ef6\uff0c\u53cd\u6620\u4e86\u73b0\u5b9e\u5de5\u4f5c\u4e2d\u7684\u590d\u6742\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5de5\u4f5c\u6d41\u7a0b\u6784\u5efa\u8fc7\u7a0b\uff0c\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f85\u52a9\u53d1\u73b0\u548c\u4e13\u5bb6\u6ce8\u91ca\uff0c\u8017\u8d39\u4e86700\u591a\u5c0f\u65f6\u7684\u4e13\u5bb6\u65f6\u95f4\u3002</li>\n    <li>\u8be5\u8fc7\u7a0b\u751f\u6210\u4e86172\u4e2a\u590d\u5408\u5de5\u4f5c\u6d41\u7a0b\u548c384\u4e2a\u4efb\u52a1\uff0c\u6d89\u53ca1710\u4e2a\u7535\u5b50\u8868\u683c\u548c2700\u4e07\u4e2a\u5355\u5143\u683c\uff0c\u53cd\u6620\u4e86\u771f\u5b9e\u4f01\u4e1a\u5de5\u4f5c\u7684\u590d\u6742\u6027\u548c\u534f\u4f5c\u6027\u3002</li>\n    <li>\u5bf9\u524d\u6cbfAI\u7cfb\u7edf\u8fdb\u884c\u8bc4\u4f30\u53d1\u73b0\uff0cGPT 5.1 Pro\u572848\u5c0f\u65f6\u5185\u4ec5\u901a\u8fc738.4%\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u800cClaude Sonnet 4.5\u4ec5\u901a\u8fc725.0%\uff0c\u663e\u793a\u4e86AI\u5728\u5904\u7406\u771f\u5b9e\u5de5\u4f5c\u6d41\u65f6\u9762\u4e34\u7684\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Finch is a new benchmark for testing AI agents in real-world finance and accounting tasks, using data from Enron and other financial institutions.</li>\n    <li>The benchmark includes various tasks like data entry, calculations, and reporting, reflecting the complexity of actual enterprise workflows.</li>\n    <li>It was created through a process that combines AI assistance and expert reviews, resulting in 172 workflows with 384 tasks and a large amount of data.</li>\n    <li>Tests on advanced AI systems show low success rates, with GPT 5.1 passing 38.4% and Claude Sonnet 4.5 passing only 25% of workflows.</li>\n    <li>Case studies highlight the difficulties AI faces in handling real-world enterprise tasks effectively.</li>\n</ul>"}, "publishedAt": "2025-12-15T05:28:45.000Z", "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows", "summary": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13168.png", "numComments": 1, "submittedBy": {"_id": "637b08057ce76c3b834da15d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PFmI1HNcZlHF0RoLjP3c2.png", "fullname": "Haoyu Dong", "name": "HaoyuDong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "organization": {"_id": "690e8a61e75888a0f707c0ee", "name": "FinWorkBench", "fullname": "Finch: Finance and Accounting Workflow Benchmark", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/637b08057ce76c3b834da15d/0HrDjcjNQK-5-FCZfkLzp.png"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2511.18538", "authors": [{"_id": "692e667137312eaa83fd8832", "user": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "name": "Jian Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:34.886Z", "hidden": false}, {"_id": "692e667137312eaa83fd8833", "name": "Xianglong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8834", "name": "Weifeng Lv", "hidden": false}, {"_id": "692e667137312eaa83fd8835", "name": "Ken Deng", "hidden": false}, {"_id": "692e667137312eaa83fd8836", "name": "Shawn Guo", "hidden": false}, {"_id": "692e667137312eaa83fd8837", "name": "Lin Jing", "hidden": false}, {"_id": "692e667137312eaa83fd8838", "name": "Yizhi Li", "hidden": false}, {"_id": "692e667137312eaa83fd8839", "name": "Shark Liu", "hidden": false}, {"_id": "692e667137312eaa83fd883a", "name": "Xianzhen Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883b", "name": "Yuyu Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883c", "name": "Changzai Pan", "hidden": false}, {"_id": "692e667137312eaa83fd883d", "name": "Ensheng Shi", "hidden": false}, {"_id": "692e667137312eaa83fd883e", "name": "Yingshui Tan", "hidden": false}, {"_id": "692e667137312eaa83fd883f", "name": "Renshuai Tao", "hidden": false}, {"_id": "692e667137312eaa83fd8840", "user": {"_id": "66a8e2538407031e388c501f", "avatarUrl": "/avatars/d16d51f7b1e111efd6d0985995b614be.svg", "isPro": false, "fullname": "wjj", "user": "wuyuverse", "type": "user"}, "name": "Jiajun Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:36.195Z", "hidden": false}, {"_id": "692e667137312eaa83fd8841", "name": "Xianjie Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8842", "name": "Zhenhe Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8843", "name": "Daoguang Zan", "hidden": false}, {"_id": "692e667137312eaa83fd8844", "name": "Chenchen Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8845", "user": {"_id": "672c9ba69380700b602c46c1", "avatarUrl": "/avatars/3d0fd966df540d34095d2c84ce449180.svg", "isPro": false, "fullname": "wei zhang", "user": "zwpride", "type": "user"}, "name": "Wei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:37.970Z", "hidden": false}, {"_id": "692e667137312eaa83fd8846", "name": "He Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd8847", "user": {"_id": "62b7fb545233925f253531c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg", "isPro": false, "fullname": "Terry Yue Zhuo", "user": "terryyz", "type": "user"}, "name": "Terry Yue Zhuo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T16:50:22.285Z", "hidden": false}, {"_id": "692e667137312eaa83fd8848", "name": "Kerui Cao", "hidden": false}, {"_id": "692e667137312eaa83fd8849", "name": "Xianfu Cheng", "hidden": false}, {"_id": "692e667137312eaa83fd884a", "name": "Jun Dong", "hidden": false}, {"_id": "692e667137312eaa83fd884b", "name": "Shengjie Fang", "hidden": false}, {"_id": "692e667137312eaa83fd884c", "name": "Zhiwei Fei", "hidden": false}, {"_id": "692e667137312eaa83fd884d", "name": "Xiangyuan Guan", "hidden": false}, {"_id": "692e667137312eaa83fd884e", "name": "Qipeng Guo", "hidden": false}, {"_id": "692e667137312eaa83fd884f", "name": "Zhiguang Han", "hidden": false}, {"_id": "692e667137312eaa83fd8850", "name": "Joseph James", "hidden": false}, {"_id": "692e667137312eaa83fd8851", "name": "Tianqi Luo", "hidden": false}, {"_id": "692e667137312eaa83fd8852", "user": {"_id": "67f1037cd5f976f3d4777390", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0cXH40AcE-M-H21cSNBqZ.png", "isPro": false, "fullname": "RenyuanLi", "user": "RenyuanLi", "type": "user"}, "name": "Renyuan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:17:45.344Z", "hidden": false}, {"_id": "692e667137312eaa83fd8853", "name": "Yuhang Li", "hidden": false}, {"_id": "692e667137312eaa83fd8854", "name": "Yiming Liang", "hidden": false}, {"_id": "692e667137312eaa83fd8855", "name": "Congnan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8856", "name": "Jiaheng Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8857", "name": "Qian Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8858", "name": "Ruitong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8859", "name": "Tyler Loakman", "hidden": false}, {"_id": "692e667137312eaa83fd885a", "name": "Xiangxin Meng", "hidden": false}, {"_id": "692e667137312eaa83fd885b", "name": "Chuang Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885c", "name": "Tianhao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885d", "name": "Jiajun Shi", "hidden": false}, {"_id": "692e667137312eaa83fd885e", "name": "Mingjie Tang", "hidden": false}, {"_id": "692e667137312eaa83fd885f", "name": "Boyang Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8860", "name": "Haowen Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8861", "name": "Yunli Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8862", "user": {"_id": "668619ce7374cac565759731", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668619ce7374cac565759731/tUtiyIQRGsMdq3HB2yYIL.jpeg", "isPro": false, "fullname": "Fanglin Xu", "user": "Tswatery", "type": "user"}, "name": "Fanglin Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T15:15:03.333Z", "hidden": false}, {"_id": "692e667137312eaa83fd8863", "name": "Zihan Xu", "hidden": false}, {"_id": "692e667137312eaa83fd8864", "name": "Fei Yuan", "hidden": false}, {"_id": "692e667137312eaa83fd8865", "user": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "name": "Ge Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:34.025Z", "hidden": false}, {"_id": "692e667137312eaa83fd8866", "user": {"_id": "65f40e83653c231cbaf7defe", "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg", "isPro": false, "fullname": "Jiayi Zhang", "user": "didiforhugface", "type": "user"}, "name": "Jiayi Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:32.149Z", "hidden": false}, {"_id": "692e667137312eaa83fd8867", "name": "Xinhao Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8868", "name": "Wangchunshu Zhou", "hidden": false}, {"_id": "692e667137312eaa83fd8869", "name": "Hualei Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886a", "name": "King Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886b", "name": "Brown Dai", "hidden": false}, {"_id": "692e667137312eaa83fd886c", "name": "Aishan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd886d", "name": "Zhoujun Li", "hidden": false}, {"_id": "692e667137312eaa83fd886e", "name": "Chenghua Lin", "hidden": false}, {"_id": "692e667137312eaa83fd886f", "name": "Tianyu Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8870", "name": "Chao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd8871", "name": "Kai Shen", "hidden": false}, {"_id": "692e667137312eaa83fd8872", "name": "Libo Qin", "hidden": false}, {"_id": "692e667137312eaa83fd8873", "name": "Shuangyong Song", "hidden": false}, {"_id": "692e667137312eaa83fd8874", "name": "Zizheng Zhan", "hidden": false}, {"_id": "692e667137312eaa83fd8875", "name": "Jiajun Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8876", "name": "Jie Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8877", "name": "Zhaoxiang Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8878", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-11-23T17:09:34.000Z", "submittedOnDailyAt": "2025-12-02T02:55:07.234Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "submittedOnDailyBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "upvotes": 240, "discussionId": "692e667237312eaa83fd8879", "ai_summary": "A comprehensive guide to code LLMs, covering their lifecycle from data curation to deployment, including techniques, trade-offs, and research-practice gaps.", "ai_keywords": ["Transformer-based architectures", "HumanEval", "prompting paradigms", "code pre-training", "supervised fine-tuning", "reinforcement learning", "autonomous coding agents", "GPT-4", "Claude", "LLaMA", "StarCoder", "Code LLaMA", "DeepSeek-Coder", "QwenCoder", "code correctness", "security", "contextual awareness", "software-related code tasks", "scaling law", "framework selection", "hyperparameter sensitivity", "model architectures", "dataset comparisons"], "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u76f4\u63a5\u8f6c\u6362\u4e3a\u529f\u80fd\u4ee3\u7801\uff0c\u9769\u65b0\u4e86\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\u3002</li>\n    <li>\u7814\u7a76\u5206\u6790\u4e86\u901a\u7528 LLMs\uff08\u5982 GPT-4\u3001Claude\uff09\u548c\u4e13\u95e8\u7528\u4e8e\u4ee3\u7801\u7684 LLMs\uff08\u5982 StarCoder\u3001Code LLaMA\uff09\u7684\u80fd\u529b\u3002</li>\n    <li>\u63a2\u8ba8\u4e86\u4ece\u6570\u636e\u6574\u7406\u5230\u540e\u671f\u8bad\u7ec3\u7684\u6574\u4e2a\u6a21\u578b\u751f\u547d\u5468\u671f\uff0c\u5305\u62ec\u4ee3\u7801\u9884\u8bad\u7ec3\u548c\u76d1\u7763\u5fae\u8c03\u3002</li>\n    <li>\u5f3a\u8c03\u4e86\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u4ee3\u7801\u6b63\u786e\u6027\u3001\u5b89\u5168\u6027\u548c\u4e0a\u4e0b\u6587\u610f\u8bc6\u65b9\u9762\u3002</li>\n    <li>\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5206\u6790\u4e86\u4ee3\u7801\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u5404\u4e2a\u65b9\u9762\uff0c\u5305\u62ec\u6a21\u578b\u67b6\u6784\u548c\u6570\u636e\u96c6\u6bd4\u8f83\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) have changed how software is developed by turning natural language into working code, leading to tools like Github Copilot and others gaining popularity.</li>\n    <li>The technology has advanced from basic rule-based systems to complex Transformer models, improving performance significantly on coding tasks.</li>\n    <li>This work offers a detailed guide on how code LLMs work, exploring their development from data collection to training methods and deployment.</li>\n    <li>It compares general-purpose LLMs with those designed specifically for coding and discusses their capabilities, design choices, and challenges.</li>\n    <li>The study identifies gaps between academic research and real-world software tasks and suggests future research directions to meet practical needs in coding and software development.</li>\n</ul>"}, "publishedAt": "2025-11-23T12:09:34.000Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18538.png", "numComments": 11, "submittedBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "fullname": "Yang Jian", "name": "CSJianYang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 21}, "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.14993", "authors": [{"_id": "691e819a3c64d32b036458c0", "name": "Vladimir Arkhipkin", "hidden": false}, {"_id": "691e819a3c64d32b036458c1", "user": {"_id": "67bcb1012906865678a11f91", "avatarUrl": "/avatars/80fb0cc24f0d16c4740f9115b680df0f.svg", "isPro": false, "fullname": "Vladimir Korviakov", "user": "korviakov", "type": "user"}, "name": "Vladimir Korviakov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:03.925Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c2", "user": {"_id": "63cfa7ef3b7adfa99c0eb524", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674553277288-noauth.jpeg", "isPro": false, "fullname": "Nikolai Gerasimenko", "user": "nikgerasimenko", "type": "user"}, "name": "Nikolai Gerasimenko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:55.225Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c3", "name": "Denis Parkhomenko", "hidden": false}, {"_id": "691e819a3c64d32b036458c4", "user": {"_id": "64e4c7764af6c29a0697f57b", "avatarUrl": "/avatars/efc4e9f9b105586fd090b22a1bc7dbb7.svg", "isPro": false, "fullname": "Viacheslav Vasilev", "user": "vvasilev", "type": "user"}, "name": "Viacheslav Vasilev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:10.246Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c5", "user": {"_id": "68838d809080cc7010edf5e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68838d809080cc7010edf5e2/xBqg5ggt_PfLkiDLmsZxx.jpeg", "isPro": false, "fullname": "Alexey Letunovskiy", "user": "AlexeyLetunovskiy", "type": "user"}, "name": "Alexey Letunovskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:55.594Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c6", "user": {"_id": "678781c9e3c3c0163db4f99c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5Vi5J_XS9fbN2gDHfzHlh.png", "isPro": false, "fullname": "Kovaleva Maria", "user": "makovka2000", "type": "user"}, "name": "Maria Kovaleva", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T10:15:36.018Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c7", "user": {"_id": "67f38b14da604b256d393662", "avatarUrl": "/avatars/63445143f68995becc7702868387555b.svg", "isPro": false, "fullname": "Nikolay Vaulin", "user": "nvvaulin", "type": "user"}, "name": "Nikolai Vaulin", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:01.695Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c8", "user": {"_id": "62653f745f6f2e14d6ae128c", "avatarUrl": "/avatars/944b564ab810a5b31fa5e45f63bdf4ee.svg", "isPro": false, "fullname": "Ivan Kirillov", "user": "funnylittleman", "type": "user"}, "name": "Ivan Kirillov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T20:40:14.372Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c9", "user": {"_id": "60991602f7c9c7bf29603a88", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60991602f7c9c7bf29603a88/me8VFG_06ZOovTLldF-L7.jpeg", "isPro": false, "fullname": "Lev Novitskiy", "user": "leffff", "type": "user"}, "name": "Lev Novitskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:01:59.489Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ca", "name": "Denis Koposov", "hidden": false}, {"_id": "691e819a3c64d32b036458cb", "user": {"_id": "6628b73c35d27082500034f2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628b73c35d27082500034f2/CznOeIbjzJ9DmJaGzlWPD.jpeg", "isPro": false, "fullname": "Nikita Kiselev", "user": "kisnikser", "type": "user"}, "name": "Nikita Kiselev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:11.927Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cc", "user": {"_id": "654d4993938fbf1e695b589a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qY3MC94Uz3FGf_HQtHseK.png", "isPro": false, "fullname": "Varlamov Alexander", "user": "Alphonsce", "type": "user"}, "name": "Alexander Varlamov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:08.889Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cd", "user": {"_id": "6616719945336ca7746eaa38", "avatarUrl": "/avatars/ac77ebda8507d75376973144263beb83.svg", "isPro": false, "fullname": "Dmitrii Mikhailov", "user": "Botsman11", "type": "user"}, "name": "Dmitrii Mikhailov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:56.980Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ce", "name": "Vladimir Polovnikov", "hidden": false}, {"_id": "691e819a3c64d32b036458cf", "name": "Andrey Shutkin", "hidden": false}, {"_id": "691e819a3c64d32b036458d0", "name": "Ilya Vasiliev", "hidden": false}, {"_id": "691e819a3c64d32b036458d1", "name": "Julia Agafonova", "hidden": false}, {"_id": "691e819a3c64d32b036458d2", "name": "Anastasiia Kargapoltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d3", "user": {"_id": "65df46ac43bf08064bd8e656", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65df46ac43bf08064bd8e656/yR72X3fnBhdy_i34VqBxT.jpeg", "isPro": false, "fullname": "Anna Dmitrienko", "user": "dmitrienkoae", "type": "user"}, "name": "Anna Dmitrienko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:09.131Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d4", "name": "Anastasia Maltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d5", "user": {"_id": "66f1a9c87ce3d2d3938999ce", "avatarUrl": "/avatars/3016b15d4bae2591313537a4ea59b268.svg", "isPro": false, "fullname": "Anna Averchenkova", "user": "aaveraa", "type": "user"}, "name": "Anna Averchenkova", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:11.123Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d6", "name": "Olga Kim", "hidden": false}, {"_id": "691e819a3c64d32b036458d7", "name": "Tatiana Nikulina", "hidden": false}, {"_id": "691e819a3c64d32b036458d8", "user": {"_id": "6669a678465d1d802181e456", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6669a678465d1d802181e456/ZCthBBhDFQnh0bBkgUQUU.png", "isPro": false, "fullname": "Denis Dimitrov", "user": "dendimitrov", "type": "user"}, "name": "Denis Dimitrov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:08.661Z", "hidden": false}], "publishedAt": "2025-11-19T00:23:22.000Z", "submittedOnDailyAt": "2025-11-20T00:19:10.078Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "upvotes": 209, "discussionId": "691e819b3c64d32b036458d9", "projectPage": "https://kandinskylab.ai/", "githubRepo": "https://github.com/kandinskylab/kandinsky-5", "ai_summary": "Kandinsky 5.0 is a family of state-of-the-art generative models for high-resolution images and short videos, featuring model lineups with varying parameters and enhanced training techniques to achieve superior quality and performance.", "ai_keywords": ["foundation models", "high-resolution image synthesis", "10-second video synthesis", "image generation models", "text-to-video models", "image-to-video models", "multi-stage training pipeline", "self-supervised fine-tuning", "reinforcement learning", "pre-training", "quality-enhancement techniques", "architectural optimizations", "training optimizations", "inference optimizations", "human evaluation", "generative framework", "open-source code", "training checkpoints"], "githubStars": 477, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecdKandinsky 5.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c10\u79d2\u89c6\u9891\u5408\u6210\u7684\u5148\u8fdb\u57fa\u7840\u6a21\u578b\u7cfb\u5217\u3002</li>\n    <li>Kandinsky 5.0\u5305\u62ec\u4e09\u79cd\u6838\u5fc3\u6a21\u578b\uff1a6B\u53c2\u6570\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u30012B\u53c2\u6570\u7684\u5feb\u901f\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u548c19B\u53c2\u6570\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002</li>\n    <li>\u8be6\u7ec6\u8bf4\u660e\u4e86\u6570\u636e\u6574\u7406\u751f\u547d\u5468\u671f\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u5904\u7406\u3001\u8fc7\u6ee4\u548c\u805a\u7c7b\uff0c\u4ee5\u652f\u6301\u591a\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>\u5c55\u793a\u4e86\u65b0\u9896\u7684\u67b6\u6784\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u4f18\u5316\uff0c\u4f7fKandinsky 5.0\u5728\u751f\u6210\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002</li>\n    <li>\u5e0c\u671b\u901a\u8fc7\u53d1\u5e03\u5f00\u6e90\u4ee3\u7801\u548c\u8bad\u7ec3\u68c0\u67e5\u70b9\uff0c\u63a8\u52a8\u9ad8\u8d28\u91cf\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u548c\u4fbf\u5229\u6027\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Kandinsky 5.0 is a new set of advanced models for creating high-quality images and short videos.</li>\n    <li>It includes three main models: Kandinsky 5.0 Image Lite for image generation, Kandinsky 5.0 Video Lite for quick video creation, and Kandinsky 5.0 Video Pro for top-quality video generation.</li>\n    <li>The report details the process of gathering and preparing data for training these models, including various techniques to improve quality.</li>\n    <li>Kandinsky 5.0 features improvements in its design and training methods, allowing it to generate high-quality content quickly.</li>\n    <li>The models are open-source, making them accessible for researchers and developers to use in different creative projects.</li>\n</ul>"}, "publishedAt": "2025-11-18T19:23:22.000Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.14993.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "isAuthorParticipating": true}, {"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>DeepSeek-V3.2\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u517c\u5177\u8ba1\u7b97\u6548\u7387\u548c\u4f18\u79c0\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u201c\u7a00\u758f\u6ce8\u610f\u529b\u201d\uff08DSA\uff09\u673a\u5236\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u957f\u6587\u672c\u5904\u7406\u7684\u6027\u80fd\u3002</li>\n    <li>\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0cDeepSeek-V3.2\u7684\u8868\u73b0\u4e0eGPT-5\u76f8\u5f53\uff0c\u5176\u4e2d\u9ad8\u8ba1\u7b97\u53d8\u4f53DeepSeek-V3.2-Speciale\u8d85\u8d8a\u4e86GPT-5\u3002</li>\n    <li>\u5728\u6570\u5b66\u548c\u8ba1\u7b97\u673a\u7ade\u8d5b\u4e2d\uff0cDeepSeek-V3.2-Speciale\u7684\u63a8\u7406\u80fd\u529b\u4e0eGemini-3.0-Pro\u76f8\u5f53\uff0c\u5e76\u57282025\u5e74\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\u548c\u56fd\u9645\u4fe1\u606f\u5b66\u5965\u6797\u5339\u514b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e00\u79cd\u5927\u89c4\u6a21\u7684\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff0c\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that balances efficient computing with strong reasoning and performance.</li>\n    <li>It features DeepSeek Sparse Attention (DSA), which makes it easier to handle long contexts without losing effectiveness.</li>\n    <li>The model uses a scalable reinforcement learning system, performing as well as GPT-5 and even better in its high-compute version, DeepSeek-V3.2-Speciale.</li>\n    <li>DeepSeek-V3.2-Speciale has shown exceptional reasoning skills, winning top awards in major international math and coding competitions.</li>\n    <li>It includes a new method for creating training data that improves how well the model follows instructions and generalizes in complex tasks.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.22699", "authors": [{"_id": "692d06234397b1ec214f6788", "name": "Z-Image Team", "hidden": false}, {"_id": "692d06234397b1ec214f6789", "user": {"_id": "692d0e6bb14ceb758205d0dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/692d0e6bb14ceb758205d0dd/gGVq2KSJE11Sr3LkVn-n5.jpeg", "isPro": false, "fullname": "Huanqia Cai", "user": "Orion-Cai", "type": "user"}, "name": "Huanqia Cai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:26.669Z", "hidden": false}, {"_id": "692d06234397b1ec214f678a", "user": {"_id": "67777b7a8376dfe003afa951", "avatarUrl": "/avatars/2af9d3181306d4c53329d047eeadaf1e.svg", "isPro": false, "fullname": "Sihan Cao", "user": "Sihan-Cao", "type": "user"}, "name": "Sihan Cao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:33.191Z", "hidden": false}, {"_id": "692d06234397b1ec214f678b", "user": {"_id": "64a54586c0f13de8e7093314", "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg", "isPro": false, "fullname": "Ruoyi Du", "user": "RuoyiDu", "type": "user"}, "name": "Ruoyi Du", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:18:53.948Z", "hidden": false}, {"_id": "692d06234397b1ec214f678c", "name": "Peng Gao", "hidden": false}, {"_id": "692d06234397b1ec214f678d", "name": "Steven Hoi", "hidden": false}, {"_id": "692d06234397b1ec214f678e", "name": "Shijie Huang", "hidden": false}, {"_id": "692d06234397b1ec214f678f", "name": "Zhaohui Hou", "hidden": false}, {"_id": "692d06234397b1ec214f6790", "user": {"_id": "662a0f2d4bab737c1a279843", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662a0f2d4bab737c1a279843/fC2p3mjMHkVpDQdEqkuR4.png", "isPro": false, "fullname": "Dengyang Jiang", "user": "DyJiang", "type": "user"}, "name": "Dengyang Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:15.555Z", "hidden": false}, {"_id": "692d06234397b1ec214f6791", "user": {"_id": "6537e8eab01250d1d6efed3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gMx73gwdfEhcCFioStGCE.jpeg", "isPro": false, "fullname": "Xin", "user": "Srameo", "type": "user"}, "name": "Xin Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:15.288Z", "hidden": false}, {"_id": "692d06234397b1ec214f6792", "name": "Liangchen Li", "hidden": false}, {"_id": "692d06234397b1ec214f6793", "user": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "name": "Zhen Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:16.899Z", "hidden": false}, {"_id": "692d06234397b1ec214f6794", "user": {"_id": "6740a5730bb4a675446a80ad", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740a5730bb4a675446a80ad/dmruwMdQK3zluJm7YXUtN.jpeg", "isPro": false, "fullname": "Zhong-Yu Li", "user": "lzyhha", "type": "user"}, "name": "Zhong-Yu Li", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:08.972Z", "hidden": false}, {"_id": "692d06234397b1ec214f6795", "name": "David Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6796", "name": "Dongyang Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6797", "user": {"_id": "66332475351231c428653b6b", "avatarUrl": "/avatars/3997bcde54158f7ff9770c85a20875f1.svg", "isPro": false, "fullname": "Junhan Shi", "user": "jshmsjh", "type": "user"}, "name": "Junhan Shi", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:38.865Z", "hidden": false}, {"_id": "692d06234397b1ec214f6798", "user": {"_id": "64379d79fac5ea753f1c10f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png", "isPro": false, "fullname": "Jerry Wu", "user": "QJerry", "type": "user"}, "name": "Qilong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:18.709Z", "hidden": false}, {"_id": "692d06234397b1ec214f6799", "name": "Feng Yu", "hidden": false}, {"_id": "692d06234397b1ec214f679a", "name": "Chi Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679b", "name": "Shifeng Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679c", "user": {"_id": "641988978e0baaeed5a066c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641988978e0baaeed5a066c6/TdCjJ63gw5gdX1RqTvy9a.png", "isPro": false, "fullname": "Shilin", "user": "zsLin", "type": "user"}, "name": "Shilin Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T16:24:44.624Z", "hidden": false}], "publishedAt": "2025-11-27T18:52:07.000Z", "submittedOnDailyAt": "2025-12-01T00:38:17.269Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "submittedOnDailyBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "upvotes": 155, "discussionId": "692d06234397b1ec214f679d", "projectPage": "https://tongyi-mai.github.io/Z-Image-blog/", "githubRepo": "https://github.com/Tongyi-MAI/Z-Image", "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.", "ai_keywords": ["Scalable Single-Stream Diffusion Transformer", "S3-DiT", "diffusion transformer", "omni-pre-training", "instruction-following capabilities", "photorealistic image generation", "bilingual text rendering", "distillation scheme", "reward post-training", "H800 GPU", "VRAM"], "githubStars": 5595, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "summary_zh": "<ul>\n    <li>\u5f53\u524d\u9ad8\u6027\u80fd\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e3b\u8981\u7531\u4e13\u6709\u7cfb\u7edf\u4e3b\u5bfc\uff0c\u5982Nano Banana Pro\u548cSeedream 4.0\u3002</li>\n    <li>\u9886\u5148\u7684\u5f00\u6e90\u66ff\u4ee3\u54c1\u53c2\u6570\u91cf\u5de8\u5927\uff0820B\u523080B\uff09\uff0c\u5728\u6d88\u8d39\u8005\u7ea7\u786c\u4ef6\u4e0a\u4e0d\u5b9e\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51faZ-Image\uff0c\u4e00\u4e2a\u9ad8\u6548\u76846B\u53c2\u6570\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u5355\u6d41\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u3002</li>\n    <li>Z-Image\u7ecf\u8fc7\u4f18\u5316\u540e\uff0c\u4ec5\u9700314K H800 GPU\u5c0f\u65f6\u7684\u8bad\u7ec3\uff0c\u4e14\u80fd\u5728\u4f01\u4e1a\u7ea7\u548c\u6d88\u8d39\u8005\u7ea7\u786c\u4ef6\u4e0a\u8fd0\u884c\u3002</li>\n    <li>Z-Image\u5728\u751f\u6210\u903c\u771f\u7684\u56fe\u50cf\u548c\u53cc\u8bed\u6587\u672c\u6e32\u67d3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u4e0e\u9876\u7ea7\u5546\u4e1a\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current image generation models are mostly proprietary and require a lot of computational power.</li>\n    <li>Many open-source models have large parameter counts (20B to 80B), making them hard to use on regular hardware.</li>\n    <li>We introduce Z-Image, a 6B-parameter model that is efficient and can run on consumer-grade hardware.</li>\n    <li>Z-Image can generate high-quality images quickly and has good editing capabilities.</li>\n    <li>Our model achieves great results while being cost-effective, and we are sharing our resources for others to use.</li>\n</ul>"}, "publishedAt": "2025-11-27T13:52:07.000Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png", "numComments": 3, "submittedBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "fullname": "Zhen Li", "name": "Paper99", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 29}, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20626", "authors": [{"_id": "6927ab26243b2216fb75cd1b", "name": "Wei He", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1c", "user": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "name": "Kai Han", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:11.052Z", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1d", "name": "Hang Zhou", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1e", "name": "Hanting Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1f", "name": "Zhicheng Liu", "hidden": false}, {"_id": "6927ab26243b2216fb75cd20", "name": "Xinghao Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd21", "name": "Yunhe Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "publishedAt": "2025-11-25T18:48:05.000Z", "submittedOnDailyAt": "2025-11-26T23:08:13.066Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "submittedOnDailyBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "upvotes": 154, "discussionId": "6927ab27243b2216fb75cd22", "projectPage": "https://github.com/huawei-noah/noah-research/tree/master/ROOT", "githubRepo": "https://github.com/huawei-noah/noah-research", "ai_summary": "ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.", "ai_keywords": ["large language models", "LLMs", "momentum orthogonalization", "dimensional fragility", "outlier-induced noise", "adaptive Newton iterations", "proximal optimization", "Muon", "Adam-based optimizers", "robust optimizer"], "githubStars": 909, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u52a0\u5267\u4e86\u7b97\u6cd5\u4e0d\u7cbe\u786e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668ROOT\uff0c\u65e8\u5728\u901a\u8fc7\u53cc\u91cd\u7a33\u5065\u673a\u5236\u6765\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>ROOT\u91c7\u7528\u81ea\u9002\u5e94\u725b\u987f\u8fed\u4ee3\u7684\u7ef4\u5ea6\u7a33\u5065\u6b63\u4ea4\u5316\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4e0d\u540c\u67b6\u6784\u914d\u7f6e\u4e0b\u7684\u7cbe\u786e\u5ea6\u3002</li>\n    <li>ROOT\u8fd8\u901a\u8fc7\u90bb\u8fd1\u4f18\u5316\u6846\u67b6\u6291\u5236\u5f02\u5e38\u503c\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u610f\u4e49\u7684\u68af\u5ea6\u65b9\u5411\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cROOT\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e2d\u6bd4Muon\u548c\u57fa\u4e8eAdam\u7684\u4f18\u5316\u5668\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) face challenges with training stability and precision as they grow in size.</li>\n    <li>Recent optimizers have improved efficiency but struggle with robustness against noise and dimensional changes.</li>\n    <li>ROOT is a new optimizer that strengthens training stability using two innovative methods: dimension-robust orthogonalization and noise suppression.</li>\n    <li>Experiments show ROOT outperforms other optimizers like Muon and Adam in tough conditions, achieving faster training and better results.</li>\n    <li>The ROOT code will be available online for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:48:05.000Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20626.png", "numComments": 2, "submittedBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "fullname": "hankai", "name": "hankaixyz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20785", "authors": [{"_id": "692d430f4397b1ec214f696e", "user": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "name": "Zuhao Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:11.311Z", "hidden": false}, {"_id": "692d430f4397b1ec214f696f", "user": {"_id": "6690f58e2f9f6f9c88e91031", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6690f58e2f9f6f9c88e91031/QQ_VoEh7NlE6BUvii08zk.png", "isPro": false, "fullname": "Sudong Wang", "user": "xiao45791", "type": "user"}, "name": "Sudong Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:14.173Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6970", "user": {"_id": "64bb77e786e7fb5b8a317a43", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bb77e786e7fb5b8a317a43/J0jOrlZJ9gazdYaeSH2Bo.png", "isPro": false, "fullname": "kcz", "user": "kcz358", "type": "user"}, "name": "Kaichen Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T10:06:41.343Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6971", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:09.461Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6972", "name": "Sicong Leng", "hidden": false}, {"_id": "692d430f4397b1ec214f6973", "name": "Yifan Zhang", "hidden": false}, {"_id": "692d430f4397b1ec214f6974", "name": "Chengwei Qin", "hidden": false}, {"_id": "692d430f4397b1ec214f6975", "name": "Shijian Lu", "hidden": false}, {"_id": "692d430f4397b1ec214f6976", "name": "Xingxuan Li", "hidden": false}, {"_id": "692d430f4397b1ec214f6977", "user": {"_id": "6454685a548f22be598414c4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg", "isPro": false, "fullname": "Lidong Bing", "user": "LidongBing", "type": "user"}, "name": "Lidong Bing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:49:36.056Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "publishedAt": "2025-11-25T19:22:48.000Z", "submittedOnDailyAt": "2025-12-02T00:35:56.511Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "submittedOnDailyBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "upvotes": 148, "discussionId": "692d430f4397b1ec214f6978", "projectPage": "https://evolvinglmms-lab.github.io/LongVT/", "githubRepo": "https://github.com/EvolvingLMMs-Lab/LongVT", "ai_summary": "LongVT, an end-to-end framework, enhances long video reasoning by interleaving global and local analysis using multimodal tools, outperforming existing methods on challenging benchmarks.", "ai_keywords": ["multimodal models", "video reasoning", "textual Chain-of-Thought", "hallucinations", "long-form videos", "temporal grounding", "video cropping", "fine-grained question-answering", "VideoSIAH", "tool-integrated cold-start supervised fine-tuning", "agentic reinforcement learning", "agentic reinforcement fine-tuning"], "githubStars": 121, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u89c6\u9891\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u5bb9\u6613\u51fa\u73b0\u9519\u8bef\uff0c\u5c24\u5176\u662f\u8bc1\u636e\u7a00\u5c11\u548c\u5206\u6563\u65f6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86LongVT\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u66ff\u7684\u591a\u6a21\u6001\u601d\u7ef4\u65b9\u5f0f\u6765\u5e2e\u52a9\u7406\u89e3\u957f\u89c6\u9891\uff0c\u6a21\u62df\u4eba\u7c7b\u5148\u6d4f\u89c8\u518d\u7ec6\u770b\u76f8\u5173\u7247\u6bb5\u7684\u65b9\u5f0f\u3002</li>\n    <li>LongVT\u5229\u7528LMMs\u7684\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\u4f5c\u4e3a\u89c6\u9891\u526a\u8f91\u5de5\u5177\uff0c\u4ee5\u805a\u7126\u7279\u5b9a\u89c6\u9891\u7247\u6bb5\u5e76\u63d0\u53d6\u66f4\u7ec6\u81f4\u7684\u5e27\u3002</li>\n    <li>\u4e3a\u4e86\u652f\u6301\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aVideoSIAH\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b247.9K\u4e2a\u8bad\u7ec3\u6837\u672c\u548c1,280\u4e2a\u8bc4\u4f30\u95ee\u7b54\u5bf9\u3002</li>\n    <li>LongVT\u5728\u56db\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u5f3a\u57fa\u7ebf\uff0c\u5e76\u4e14\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u516c\u5f00\u53ef\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large multimodal models (LMMs) can struggle with understanding long videos, often creating false information.</li>\n    <li>The new framework, LongVT, helps improve video reasoning by mimicking how humans watch videos: first skimming, then focusing on details.</li>\n    <li>LongVT uses LMMs to zoom in on specific video clips and analyze them for better answers.</li>\n    <li>A new dataset called VideoSIAH will be released to help train and evaluate this framework, containing a large number of samples for fine-tuning and testing.</li>\n    <li>LongVT has shown better performance than existing models in understanding and reasoning about long videos.</li>\n</ul>"}, "publishedAt": "2025-11-25T14:22:48.000Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20785.png", "numComments": 3, "submittedBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "fullname": "Zuhao Yang", "name": "mwxely", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u5230\u8ba1\u7b97\u987a\u5e8f\u548c\u957f\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u9650\u5236\uff0c\u96be\u4ee5\u5b9e\u65f6\u5e94\u7528\u4e8e\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u5f62\u8c61\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u5b9e\u65f6\u865a\u62df\u5f62\u8c61\u201d\uff08Live Avatar\uff09\uff0c\u4f7f\u7528140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u548c\u65e0\u9650\u957f\u5ea6\u7684\u865a\u62df\u5f62\u8c61\u751f\u6210\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\uff08TPP\uff09\uff0c\u901a\u8fc7\u591a\u4e2aGPU\u5206\u5e03\u5f0f\u63a8\u7406\uff0c\u6253\u7834\u81ea\u56de\u5f52\u74f6\u9888\uff0c\u5b9e\u73b0\u7a33\u5b9a\u7684\u4f4e\u5ef6\u8fdf\u5b9e\u65f6\u6d41\u5a92\u4f53\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u6eda\u52a8\u6c89\u6d78\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5916\u89c2\uff0c\u4fdd\u6301\u5e8f\u5217\u4e00\u81f4\u6027\uff0c\u51cf\u8f7b\u8eab\u4efd\u6f02\u79fb\u548c\u989c\u8272\u4f2a\u5f71\u3002</li>\n    <li>\u6211\u4eec\u7684\u7cfb\u7edf\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e8620\u5e27\u6bcf\u79d2\u7684\u751f\u6210\u901f\u5ea6\uff0c\u9996\u6b21\u5728\u8be5\u89c4\u6a21\u4e0a\u5b9e\u73b0\u5b9e\u7528\u7684\u5b9e\u65f6\u9ad8\u4fdd\u771f\u865a\u62df\u5f62\u8c61\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Live Avatar is a new system for creating realistic avatars in real-time using a powerful diffusion model.</li>\n  <li>The method uses a technique called Timestep-forcing Pipeline Parallelism (TPP) to speed up processing by using multiple GPUs.</li>\n  <li>It includes a feature called Rolling Sink Frame Mechanism (RSFM) to improve visual consistency and prevent issues like color changes in avatars.</li>\n  <li>Live Avatar can generate videos at 20 frames per second on powerful hardware, making it suitable for real-time applications.</li>\n  <li>This system sets a new standard for using advanced diffusion models in creating long videos with high quality.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2511.18423", "authors": [{"_id": "692518ff16eb3a9f1310391c", "name": "B. Y. Yan", "hidden": false}, {"_id": "692518ff16eb3a9f1310391d", "name": "Chaofan Li", "hidden": false}, {"_id": "692518ff16eb3a9f1310391e", "name": "Hongjin Qian", "hidden": false}, {"_id": "692518ff16eb3a9f1310391f", "user": {"_id": "6145b3fd35135ec7e8d4ca45", "avatarUrl": "/avatars/5dc25d18d6a8418c9b1a29ece9a48f5a.svg", "isPro": false, "fullname": "Shuqi Lu", "user": "shuqi", "type": "user"}, "name": "Shuqi Lu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:18:11.163Z", "hidden": false}, {"_id": "692518ff16eb3a9f13103920", "user": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "name": "Zheng Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:17:59.618Z", "hidden": false}], "publishedAt": "2025-11-23T12:29:33.000Z", "submittedOnDailyAt": "2025-11-25T00:25:04.757Z", "title": "General Agentic Memory Via Deep Research", "submittedOnDailyBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "upvotes": 140, "discussionId": "692518ff16eb3a9f13103921", "projectPage": "https://github.com/VectorSpaceLab/general-agentic-memory", "githubRepo": "https://github.com/VectorSpaceLab/general-agentic-memory", "ai_summary": "GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.", "ai_keywords": ["general agentic memory", "GAM", "just-in time compilation", "JIT compilation", "memorizer", "researcher", "universal page-store", "large language models", "LLMs", "reinforcement learning"], "githubStars": 246, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u5185\u5b58\u5bf9\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u9759\u6001\u5185\u5b58\u5bb9\u6613\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u79f0\u4e3a\u4e00\u822c\u4ee3\u7406\u5185\u5b58\uff08GAM\uff09\uff0c\u91c7\u7528\u201c\u53ca\u65f6\u7f16\u8bd1\u201d\uff08JIT\uff09\u539f\u5219\u3002</li>\n    <li>GAM\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u8bb0\u5fc6\u5668\u548c\u7814\u7a76\u8005\uff0c\u5206\u522b\u7528\u4e8e\u7ba1\u7406\u5386\u53f2\u4fe1\u606f\u548c\u5728\u7ebf\u68c0\u7d22\u6709\u7528\u4fe1\u606f\u3002</li>\n    <li>\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u6709\u6548\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6574\u4f53\u6027\u80fd\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cGAM\u5728\u591a\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684\u4efb\u52a1\u5b8c\u6210\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5185\u5b58\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Memory is very important for AI agents, but traditional static memory can lead to significant information loss.</li>\n  <li>We introduce a new system called general agentic memory (GAM) that creates memory when needed, rather than in advance.</li>\n  <li>GAM consists of two main parts: a Memorizer that keeps important past information, and a Researcher that finds useful information when needed.</li>\n  <li>This system helps AI agents use large language models more effectively and improves their overall performance.</li>\n  <li>Tests show that GAM performs much better in memory-related tasks compared to existing memory systems.</li>\n</ul>"}, "publishedAt": "2025-11-23T07:29:33.000Z", "title": "General Agentic Memory Via Deep Research", "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18423.png", "numComments": 2, "submittedBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "fullname": "zhengliu", "name": "lz1001", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp \u662f\u4e00\u4e2a\u5305\u542b 210 \u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6a21\u62df\u771f\u5b9e\u4f01\u4e1a\u6570\u636e\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u9700\u8981\u5728\u5de5\u4e1a\u67b6\u6784\u4e0a\u8fdb\u884c\u590d\u6742\u7684\u5de5\u7a0b\u8bbe\u8ba1\u548c\u5f00\u53d1\uff0c\u5305\u62ec\u4ece\u5934\u6784\u5efa SQL \u7ba1\u9053\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\u4efb\u52a1\u8981\u6c42\u89e3\u51b3\u5f00\u653e\u5f0f\u5546\u4e1a\u95ee\u9898\uff0c\u9700\u8981\u6218\u7565\u89c4\u5212\u548c\u9010\u6b65\u5206\u6790\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7cfb\u7edf\u5728 DAComp \u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u7684\u6210\u529f\u7387\u4f4e\u4e8e 20%\u3002</li>\n    <li>DAComp \u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5e2e\u52a9\u5f00\u53d1\u771f\u6b63\u80fd\u529b\u5f3a\u5927\u7684\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DAComp is a new benchmark with 210 tasks that simulate real-world data workflows for businesses.</li>\n    <li>It includes data engineering tasks, which involve creating and improving complex SQL pipelines, and data analysis tasks, which require solving open-ended business problems.</li>\n    <li>Performance on data engineering tasks is very low, with success rates below 20%, indicating challenges in managing data workflows.</li>\n    <li>Scores for data analysis tasks are also below 40%, showing weaknesses in reasoning and problem-solving in these tasks.</li>\n    <li>DAComp helps identify these issues and aims to improve the development of autonomous data agents for enterprises.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.21631", "authors": [{"_id": "692ffb1a26742347f61daf38", "user": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "isPro": false, "fullname": "shuai bai", "user": "ShuaiBai623", "type": "user"}, "name": "Shuai Bai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:34:29.118Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf39", "name": "Yuxuan Cai", "hidden": false}, {"_id": "692ffb1a26742347f61daf3a", "name": "Ruizhe Chen", "hidden": false}, {"_id": "692ffb1a26742347f61daf3b", "name": "Keqin Chen", "hidden": false}, {"_id": "692ffb1a26742347f61daf3c", "user": {"_id": "63f30b870a16587ea970edfe", "avatarUrl": "/avatars/b58ab2d8a85a6d83462c297de2714ce4.svg", "isPro": false, "fullname": "Xiong-Hui Chen", "user": "xionghuichen", "type": "user"}, "name": "Xionghui Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:35:42.689Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf3d", "user": {"_id": "65b2529285b6c21448a10d65", "avatarUrl": "/avatars/1b09e2742aecce1bbdc57f0c4504cf38.svg", "isPro": false, "fullname": "Zesen Cheng", "user": "ClownRat", "type": "user"}, "name": "Zesen Cheng", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:35:51.365Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf3e", "name": "Lianghao Deng", "hidden": false}, {"_id": "692ffb1a26742347f61daf3f", "name": "Wei Ding", "hidden": false}, {"_id": "692ffb1a26742347f61daf40", "name": "Chang Gao", "hidden": false}, {"_id": "692ffb1a26742347f61daf41", "name": "Chunjiang Ge", "hidden": false}, {"_id": "692ffb1a26742347f61daf42", "name": "Wenbin Ge", "hidden": false}, {"_id": "692ffb1a26742347f61daf43", "name": "Zhifang Guo", "hidden": false}, {"_id": "692ffb1a26742347f61daf44", "user": {"_id": "656f1b21b075b63c90ba02ee", "avatarUrl": "/avatars/d6856815ef06261394178161e4d511b4.svg", "isPro": false, "fullname": "Huang Qidong", "user": "shikiw", "type": "user"}, "name": "Qidong Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:49.065Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf45", "name": "Jie Huang", "hidden": false}, {"_id": "692ffb1a26742347f61daf46", "name": "Fei Huang", "hidden": false}, {"_id": "692ffb1a26742347f61daf47", "name": "Binyuan Hui", "hidden": false}, {"_id": "692ffb1a26742347f61daf48", "name": "Shutong Jiang", "hidden": false}, {"_id": "692ffb1a26742347f61daf49", "name": "Zhaohai Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4a", "name": "Mingsheng Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4b", "name": "Mei Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4c", "user": {"_id": "6346be8f7fb9f11870c63998", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6346be8f7fb9f11870c63998/tFWawSkXL6bv1zgvzFWQd.png", "isPro": false, "fullname": "Kaixin Li", "user": "likaixin", "type": "user"}, "name": "Kaixin Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:53.648Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf4d", "user": {"_id": "67a31313cf9d856beb7f9afb", "avatarUrl": "/avatars/69395b134716f750545eab35a164e51f.svg", "isPro": false, "fullname": "Zicheng Lin", "user": "etonlin", "type": "user"}, "name": "Zicheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:36:50.803Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf4e", "name": "Junyang Lin", "hidden": false}, {"_id": "692ffb1a26742347f61daf4f", "name": "Xuejing Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf50", "name": "Jiawei Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf51", "name": "Chenglong Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf52", "name": "Yang Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf53", "name": "Dayiheng Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf54", "user": {"_id": "64e72776e9fc9d0475ef5188", "avatarUrl": "/avatars/d32b9d4e1da5486c3d5f9b04fa29d167.svg", "isPro": false, "fullname": "Shixuan Liu", "user": "liusx", "type": "user"}, "name": "Shixuan Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:58.470Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf55", "name": "Dunjie Lu", "hidden": false}, {"_id": "692ffb1a26742347f61daf56", "name": "Ruilin Luo", "hidden": false}, {"_id": "692ffb1a26742347f61daf57", "name": "Chenxu Lv", "hidden": false}, {"_id": "692ffb1a26742347f61daf58", "name": "Rui Men", "hidden": false}, {"_id": "692ffb1a26742347f61daf59", "name": "Lingchen Meng", "hidden": false}, {"_id": "692ffb1a26742347f61daf5a", "name": "Xuancheng Ren", "hidden": false}, {"_id": "692ffb1a26742347f61daf5b", "name": "Xingzhang Ren", "hidden": false}, {"_id": "692ffb1a26742347f61daf5c", "name": "Sibo Song", "hidden": false}, {"_id": "692ffb1a26742347f61daf5d", "name": "Yuchong Sun", "hidden": false}, {"_id": "692ffb1a26742347f61daf5e", "name": "Jun Tang", "hidden": false}, {"_id": "692ffb1a26742347f61daf5f", "name": "Jianhong Tu", "hidden": false}, {"_id": "692ffb1a26742347f61daf60", "name": "Jianqiang Wan", "hidden": false}, {"_id": "692ffb1a26742347f61daf61", "name": "Peng Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf62", "name": "Pengfei Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf63", "name": "Qiuyue Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf64", "name": "Yuxuan Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf65", "name": "Tianbao Xie", "hidden": false}, {"_id": "692ffb1a26742347f61daf66", "name": "Yiheng Xu", "hidden": false}, {"_id": "692ffb1a26742347f61daf67", "user": {"_id": "645b10e80c73ea27d13f7aca", "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg", "isPro": false, "fullname": "xuhaiyang", "user": "xhyandwyy", "type": "user"}, "name": "Haiyang Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:51.583Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf68", "name": "Jin Xu", "hidden": false}, {"_id": "692ffb1a26742347f61daf69", "name": "Zhibo Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6a", "name": "Mingkun Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6b", "name": "Jianxin Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6c", "name": "An Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6d", "name": "Bowen Yu", "hidden": false}, {"_id": "692ffb1a26742347f61daf6e", "name": "Fei Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6f", "name": "Hang Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf70", "name": "Xi Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf71", "name": "Bo Zheng", "hidden": false}, {"_id": "692ffb1a26742347f61daf72", "name": "Humen Zhong", "hidden": false}, {"_id": "692ffb1a26742347f61daf73", "name": "Jingren Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf74", "name": "Fan Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf75", "name": "Jing Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf76", "user": {"_id": "627d2723401f42c57b6b7c0c", "avatarUrl": "/avatars/6ff754e56aaee63d8572881a6a966171.svg", "isPro": false, "fullname": "Yuanzhi Zhu", "user": "Yuanzhi", "type": "user"}, "name": "Yuanzhi Zhu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:36:59.879Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf77", "name": "Ke Zhu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63451cf0a05b51f7ded25505/kVvzrCGrjwoK0CYh4DKif.jpeg"], "publishedAt": "2025-11-26T17:59:08.000Z", "submittedOnDailyAt": "2025-12-04T01:02:46.772Z", "title": "Qwen3-VL Technical Report", "submittedOnDailyBy": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "isPro": false, "fullname": "shuai bai", "user": "ShuaiBai623", "type": "user"}, "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.", "upvotes": 110, "discussionId": "692ffb1b26742347f61daf78", "ai_summary": "Qwen3-VL, a vision-language model, excels in text and multimodal understanding through advanced architectures and larger contexts, achieving superior performance across benchmarks.", "ai_keywords": ["vision-language model", "interleaved contexts", "multimodal benchmarks", "dense variants", "mixture-of-experts", "pure-text understanding", "long-context comprehension", "multimodal reasoning", "MMMU", "visual-math benchmarks", "interleaved-MRoPE", "DeepStack", "text-based time alignment", "T-RoPE", "explicit textual timestamp alignment", "vision-language alignment", "image-grounded reasoning", "agentic decision-making", "multimodal code intelligence"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>Qwen3-VL\u662fQwen\u7cfb\u5217\u4e2d\u6700\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u591a\u79cd\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u3002</li>\n    <li>\u8be5\u6a21\u578b\u53ef\u4ee5\u5904\u7406\u591a\u8fbe256K\u4e2a\u6807\u8bb0\u7684\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\uff0c\u652f\u6301\u590d\u6742\u7684\u4e0a\u4e0b\u6587\u6574\u5408\u3002</li>\n    <li>Qwen3-VL\u6709\u4e0d\u540c\u7248\u672c\uff0c\u5305\u62ec\u5bc6\u96c6\u6a21\u578b\u548c\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u5ef6\u8fdf\u548c\u8d28\u91cf\u9700\u6c42\u3002</li>\n    <li>\u6a21\u578b\u5728\u7eaf\u6587\u672c\u7406\u89e3\u3001\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5904\u7406\u957f\u6587\u6863\u548c\u89c6\u9891\u3002</li>\n    <li>\u5f15\u5165\u4e86\u4e09\u9879\u91cd\u8981\u67b6\u6784\u5347\u7ea7\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u56fe\u50cf\u548c\u89c6\u9891\u7684\u7a7a\u95f4\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\u548c\u591a\u6a21\u6001\u5bf9\u9f50\u7cbe\u5ea6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Qwen3-VL is a powerful vision-language model that excels in many tests involving text, images, and videos.</li>\n    <li>It can handle up to 256,000 tokens at once, integrating different types of media seamlessly.</li>\n    <li>The model comes in various sizes to balance performance and speed, including both dense and mixture-of-experts versions.</li>\n    <li>It has improved text understanding, long-context comprehension, and advanced reasoning skills for different types of media tasks.</li>\n    <li>Qwen3-VL includes several technical upgrades for better performance in real-world applications involving visual reasoning and decision-making.</li>\n</ul>"}, "publishedAt": "2025-11-26T12:59:08.000Z", "title": "Qwen3-VL Technical Report", "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63451cf0a05b51f7ded25505/kVvzrCGrjwoK0CYh4DKif.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21631.png", "numComments": 3, "submittedBy": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "fullname": "shuai bai", "name": "ShuaiBai623", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 42}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Dec 18, 2025";
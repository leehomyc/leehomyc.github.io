window.trendingPapers = {
    "today": [{"paper": {"id": "2512.09363", "authors": [{"_id": "693a379e74fced5bf9c32412", "user": {"_id": "6486ff6561053da6442fef1a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486ff6561053da6442fef1a/72sdWErAwWtWNJIV5VZsy.jpeg", "isPro": false, "fullname": "KeXing", "user": "KXingLab", "type": "user"}, "name": "Ke Xing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-11T10:13:26.656Z", "hidden": false}, {"_id": "693a379e74fced5bf9c32413", "name": "Longfei Li", "hidden": false}, {"_id": "693a379e74fced5bf9c32414", "user": {"_id": "64b7ab4c037d6452a31910eb", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7ab4c037d6452a31910eb/0UaBtwyQTysBMndFWZdKu.png", "isPro": false, "fullname": "yuyangyin", "user": "yuyangyin", "type": "user"}, "name": "Yuyang Yin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:43:30.256Z", "hidden": false}, {"_id": "693a379e74fced5bf9c32415", "name": "Hanwen Liang", "hidden": false}, {"_id": "693a379e74fced5bf9c32416", "name": "Guixun Luo", "hidden": false}, {"_id": "693a379e74fced5bf9c32417", "name": "Chen Fang", "hidden": false}, {"_id": "693a379e74fced5bf9c32418", "name": "Jue Wang", "hidden": false}, {"_id": "693a379e74fced5bf9c32419", "name": "Konstantinos N. Plataniotis", "hidden": false}, {"_id": "693a379e74fced5bf9c3241a", "name": "Xiaojie Jin", "hidden": false}, {"_id": "693a379e74fced5bf9c3241b", "name": "Yao Zhao", "hidden": false}, {"_id": "693a379e74fced5bf9c3241c", "name": "Yunchao Wei", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/dFNy8Tf5Ts5qNeWXCvkcB.mp4"], "publishedAt": "2025-12-10T06:50:16.000Z", "submittedOnDailyAt": "2025-12-11T00:46:52.612Z", "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.", "upvotes": 44, "discussionId": "693a379e74fced5bf9c3241d", "projectPage": "https://ke-xing.github.io/StereoWorld/", "ai_summary": "StereoWorld generates high-quality stereo video from monocular input using a pretrained video generator with geometry-aware regularization and spatio-temporal tiling.", "ai_keywords": ["stereo video", "monocular-to-stereo", "pretrained video generator", "geometry-aware regularization", "spatio-temporal tiling", "high-definition stereo video dataset", "natural human interpupillary distance (IPD)", "visual fidelity", "geometric consistency"], "summary_zh": "<ul>\n    <li>XR\u8bbe\u5907\u7684\u5e7f\u6cdb\u4f7f\u7528\u63d0\u9ad8\u4e86\u5bf9\u9ad8\u8d28\u91cf\u7acb\u4f53\u89c6\u9891\u7684\u9700\u6c42\uff0c\u4f46\u5236\u4f5c\u6210\u672c\u9ad8\u4e14\u5bb9\u6613\u51fa\u73b0\u7455\u75b5\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86StereoWorld\uff0c\u4e00\u4e2a\u5c06\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u5668\u7528\u4e8e\u5355\u76ee\u5230\u7acb\u4f53\u89c6\u9891\u751f\u6210\u7684\u6846\u67b6\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5229\u7528\u5355\u76ee\u89c6\u9891\u8f93\u5165\uff0c\u5e76\u91c7\u7528\u51e0\u4f55\u611f\u77e5\u7684\u76d1\u7763\u65b9\u5f0f\uff0c\u786e\u4fdd3D\u7ed3\u6784\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>\u8fd8\u96c6\u6210\u4e86\u4e00\u79cd\u65f6\u7a7a\u62fc\u63a5\u65b9\u6848\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u9ad8\u5206\u8fa8\u7387\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1100\u4e07\u5e27\u7684\u9ad8\u6e05\u7acb\u4f53\u89c6\u9891\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The use of XR devices is increasing, creating a need for high-quality stereo video, but making it is still expensive and can result in errors.</li>\n    <li>StereoWorld is a new system that turns regular videos into high-quality stereo videos using a pretrained video generator.</li>\n    <li>The system uses information from the original video and includes special techniques to make sure the 3D structure looks accurate.</li>\n    <li>It also uses a method to create high-resolution videos efficiently.</li>\n    <li>A large dataset of over 11 million frames was created to help train and test the system, and StereoWorld performs better than older methods in producing high-quality stereo videos.</li>\n</ul>"}, "publishedAt": "2025-12-10T01:50:16.000Z", "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation", "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/dFNy8Tf5Ts5qNeWXCvkcB.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09363.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "isAuthorParticipating": false}, {"paper": {"id": "2512.08560", "authors": [{"_id": "693a7def74fced5bf9c32537", "user": {"_id": "6492c419702103104f9450c4", "avatarUrl": "/avatars/d6700e4bb1b9f172096ea31ba15a83b2.svg", "isPro": false, "fullname": "navve wasserman", "user": "navvew", "type": "user"}, "name": "Navve Wasserman", "status": "claimed_verified", "statusLastChangedAt": "2025-12-11T10:07:40.033Z", "hidden": false}, {"_id": "693a7def74fced5bf9c32538", "user": {"_id": "6735088b22d14a01ae17501f", "avatarUrl": "/avatars/23d2eb2bb833dcf7a05434b499fedd5e.svg", "isPro": false, "fullname": "Matias Cosarinsky", "user": "mcosarinsky", "type": "user"}, "name": "Matias Cosarinsky", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:45:36.526Z", "hidden": false}, {"_id": "693a7def74fced5bf9c32539", "user": {"_id": "687f44e793eb81b0684b4eee", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/uDef9M8sQxTweYvwcVs07.png", "isPro": false, "fullname": "Yuval Golbari", "user": "yuvalgolbari", "type": "user"}, "name": "Yuval Golbari", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:45:41.956Z", "hidden": false}, {"_id": "693a7def74fced5bf9c3253a", "name": "Aude Oliva", "hidden": false}, {"_id": "693a7def74fced5bf9c3253b", "user": {"_id": "6744c6ec6ec99a37d4ba9235", "avatarUrl": "/avatars/a5384b63bb615192f6fa157c6ea89e92.svg", "isPro": false, "fullname": "Antonio", "user": "Antoniotorralbaborruel", "type": "user"}, "name": "Antonio Torralba", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:45:57.490Z", "hidden": false}, {"_id": "693a7def74fced5bf9c3253c", "user": {"_id": "63914afeb6b839bb6143a6be", "avatarUrl": "/avatars/da25b555d105f4755c8187479469ca77.svg", "isPro": false, "fullname": "Tamar Rott Shaham", "user": "tamarott", "type": "user"}, "name": "Tamar Rott Shaham", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:46:03.740Z", "hidden": false}, {"_id": "693a7def74fced5bf9c3253d", "name": "Michal Irani", "hidden": false}], "publishedAt": "2025-12-09T13:01:17.000Z", "submittedOnDailyAt": "2025-12-11T05:54:20.652Z", "title": "BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain", "submittedOnDailyBy": {"_id": "6492c419702103104f9450c4", "avatarUrl": "/avatars/d6700e4bb1b9f172096ea31ba15a83b2.svg", "isPro": false, "fullname": "navve wasserman", "user": "navvew", "type": "user"}, "summary": "Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.", "upvotes": 30, "discussionId": "693a7def74fced5bf9c3253e", "projectPage": "https://navvewas.github.io/BrainExplore/", "ai_summary": "An automated framework uses unsupervised decomposition and natural language descriptions to identify and explain visual representations in human brain fMRI data.", "ai_keywords": ["fMRI", "unsupervised decomposition", "natural images", "natural-language descriptions", "voxel patterns"], "summary_zh": "<ul>\n    <li>\u7814\u7a76\u4eba\u8111\u5982\u4f55\u8868\u793a\u89c6\u89c9\u6982\u5ff5\u53ca\u5176\u7f16\u7801\u533a\u57df\u662f\u4e00\u4e2a\u957f\u671f\u6311\u6218\u3002</li>\n    <li>\u5c3d\u7ba1\u5df2\u6709\u8bb8\u591a\u7814\u7a76\uff0c\u4f46\u8111\u4fe1\u53f7\u590d\u6742\uff0c\u89c6\u89c9\u6982\u5ff5\u7a7a\u95f4\u5e7f\u9614\uff0c\u5bfc\u81f4\u5927\u591a\u6570\u7814\u7a76\u89c4\u6a21\u8f83\u5c0f\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u9a8c\u8bc1\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u53d1\u73b0\u548c\u89e3\u91ca\u4eba\u8111\u76ae\u5c42\u4e2d\u7684\u89c6\u89c9\u8868\u5f81\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5206\u4e3a\u4e24\u4e2a\u4e3b\u8981\u9636\u6bb5\uff1a\u53d1\u73b0\u53ef\u89e3\u91ca\u7684fMRI\u6d3b\u52a8\u6a21\u5f0f\u548c\u751f\u6210\u81ea\u7136\u56fe\u50cf\u53ca\u5176\u89c6\u89c9\u610f\u4e49\u7684\u63cf\u8ff0\u3002</li>\n    <li>\u6211\u4eec\u7684\u6846\u67b6\u63ed\u793a\u4e86\u6570\u5343\u79cd\u53ef\u89e3\u91ca\u7684\u6a21\u5f0f\uff0c\u8986\u76d6\u8bb8\u591a\u4e0d\u540c\u7684\u89c6\u89c9\u6982\u5ff5\uff0c\u5305\u62ec\u4e4b\u524d\u672a\u62a5\u9053\u7684\u7ec6\u81f4\u8868\u5f81\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Understanding how the brain represents visual concepts is a complex challenge.</li>\n    <li>Most studies are small and focused, often lacking systematic validation.</li>\n    <li>This research presents a new large-scale automated method for analyzing brain activity related to visual concepts.</li>\n    <li>The method involves finding patterns in brain scans and explaining them with relevant images and descriptions.</li>\n    <li>The framework identifies thousands of distinct visual patterns, including many that were not previously reported.</li>\n</ul>"}, "publishedAt": "2025-12-09T08:01:17.000Z", "title": "BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain", "summary": "Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08560.png", "numComments": 2, "submittedBy": {"_id": "6492c419702103104f9450c4", "avatarUrl": "/avatars/d6700e4bb1b9f172096ea31ba15a83b2.svg", "fullname": "navve wasserman", "name": "navvew", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": true}, {"paper": {"id": "2512.09824", "authors": [{"_id": "693a2f6d74fced5bf9c323c2", "user": {"_id": "6428fd124fe87caede856311", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/5OrNPwZkxu3Dm1IInCxML.jpeg", "isPro": false, "fullname": "Xianghao Kong", "user": "refkxh", "type": "user"}, "name": "Xianghao Kong", "status": "claimed_verified", "statusLastChangedAt": "2025-12-11T10:13:32.571Z", "hidden": false}, {"_id": "693a2f6d74fced5bf9c323c3", "user": {"_id": "65b00730403a23a2fd765110", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b00730403a23a2fd765110/Uw-obs-VymyU9iMVKLURZ.jpeg", "isPro": false, "fullname": "Zeyu Zhang", "user": "ZeyuZhang", "type": "user"}, "name": "Zeyu Zhang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:45:03.580Z", "hidden": false}, {"_id": "693a2f6d74fced5bf9c323c4", "name": "Yuwei Guo", "hidden": false}, {"_id": "693a2f6d74fced5bf9c323c5", "user": {"_id": "67f87bc19d597ac661a75b68", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/CZiu75y2QIR53MZkEjgJd.png", "isPro": false, "fullname": "Zhuoran Zhao", "user": "Alicezrzhao", "type": "user"}, "name": "Zhuoran Zhao", "status": "claimed_verified", "statusLastChangedAt": "2025-12-11T10:13:29.933Z", "hidden": false}, {"_id": "693a2f6d74fced5bf9c323c6", "name": "Songchun Zhang", "hidden": false}, {"_id": "693a2f6d74fced5bf9c323c7", "user": {"_id": "63f8130749569335b679af62", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f8130749569335b679af62/vgTu23-y0UKocwAGqNMwT.jpeg", "isPro": false, "fullname": "Anyi Rao", "user": "anyirao", "type": "user"}, "name": "Anyi Rao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:45:24.045Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/67f87bc19d597ac661a75b68/zeD4CrSjVY6WpEKMkjlat.gif"], "publishedAt": "2025-12-10T16:57:31.000Z", "submittedOnDailyAt": "2025-12-11T04:44:05.510Z", "title": "Composing Concepts from Images and Videos via Concept-prompt Binding", "submittedOnDailyBy": {"_id": "67f87bc19d597ac661a75b68", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/CZiu75y2QIR53MZkEjgJd.png", "isPro": false, "fullname": "Zhuoran Zhao", "user": "Alicezrzhao", "type": "user"}, "summary": "Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.", "upvotes": 23, "discussionId": "693a2f6d74fced5bf9c323c8", "projectPage": "https://refkxh.github.io/BiCo_Webpage/", "githubRepo": "https://github.com/refkxh/bico", "githubRepoAddedBy": "user", "ai_summary": "Bind & Compose uses Diffusion Transformers with hierarchical binders and temporal strategies to accurately compose complex visual concepts from images and videos.", "ai_keywords": ["visual concept composition", "prompt tokens", "cross-attention conditioning", "Diffusion Transformers", "hierarchical binder structure", "Diversify-and-Absorb Mechanism", "absorbent token", "Temporal Disentanglement Strategy", "dual-branch binder structure"], "githubStars": 45, "organization": {"_id": "693a3d43dd71dc07fc3a7cfe", "name": "mmlab-hkust", "fullname": "MMLab@HKUST", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6428fd124fe87caede856311/u4kVky_MS1iH4OYTzMF0H.jpeg"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u6982\u5ff5\u7ec4\u5408\u6280\u672f\u5c1a\u672a\u80fd\u51c6\u786e\u63d0\u53d6\u590d\u6742\u7684\u89c6\u89c9\u6982\u5ff5\uff0c\u5e76\u7075\u6d3b\u5730\u7ed3\u5408\u56fe\u50cf\u548c\u89c6\u9891\u4e2d\u7684\u6982\u5ff5\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Bind & Compose \u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed1\u5b9a\u89c6\u89c9\u6982\u5ff5\u4e0e\u5bf9\u5e94\u7684\u63d0\u793a\u6807\u8bb0\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u89c6\u89c9\u6982\u5ff5\u7ec4\u5408\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u91c7\u7528\u5c42\u6b21\u7ed1\u5b9a\u7ed3\u6784\uff0c\u5229\u7528 Diffusion Transformers \u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u6765\u7cbe\u786e\u89e3\u6784\u590d\u6742\u7684\u89c6\u89c9\u6982\u5ff5\u3002</li>\n    <li>\u4e3a\u4e86\u63d0\u9ad8\u6982\u5ff5\u4e0e\u6807\u8bb0\u7684\u7ed1\u5b9a\u51c6\u786e\u6027\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u591a\u6837\u5316\u5438\u6536\u673a\u5236\uff0c\u4f7f\u7528\u989d\u5916\u7684\u5438\u6536\u6807\u8bb0\u6d88\u9664\u4e0e\u6982\u5ff5\u65e0\u5173\u7ec6\u8282\u7684\u5f71\u54cd\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u65f6\u95f4\u89e3\u8026\u7b56\u7565\uff0c\u5c06\u89c6\u9891\u6982\u5ff5\u7684\u8bad\u7ec3\u8fc7\u7a0b\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff0c\u4ee5\u589e\u5f3a\u56fe\u50cf\u548c\u89c6\u9891\u6982\u5ff5\u4e4b\u95f4\u7684\u517c\u5bb9\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Visual concept composition aims to combine elements from images and videos but struggles with complexity and flexibility.</li>\n    <li>Bind & Compose is a new method that links visual concepts to prompt tokens, allowing for better integration of different sources.</li>\n    <li>The method uses a hierarchical structure for accurate understanding of complex visual ideas.</li>\n    <li>It includes a special mechanism to improve the accuracy of concept binding by filtering out irrelevant details.</li>\n    <li>The approach also separates the training of video concepts to enhance compatibility between images and videos, showing better results than existing methods.</li>\n</ul>"}, "publishedAt": "2025-12-10T11:57:31.000Z", "title": "Composing Concepts from Images and Videos via Concept-prompt Binding", "summary": "Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/67f87bc19d597ac661a75b68/zeD4CrSjVY6WpEKMkjlat.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09824.png", "numComments": 1, "submittedBy": {"_id": "67f87bc19d597ac661a75b68", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/CZiu75y2QIR53MZkEjgJd.png", "fullname": "Zhuoran Zhao", "name": "Alicezrzhao", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "693a3d43dd71dc07fc3a7cfe", "name": "mmlab-hkust", "fullname": "MMLab@HKUST", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6428fd124fe87caede856311/u4kVky_MS1iH4OYTzMF0H.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.09247", "authors": [{"_id": "693a372474fced5bf9c323fd", "name": "Cheng Liu", "hidden": false}, {"_id": "693a372474fced5bf9c323fe", "user": {"_id": "64311a95034ecbefddd141ef", "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg", "isPro": false, "fullname": "Yiren Song", "user": "yiren98", "type": "user"}, "name": "Yiren Song", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:48:03.757Z", "hidden": false}, {"_id": "693a372474fced5bf9c323ff", "user": {"_id": "637745113a63a2983ffbde13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669187672174-637745113a63a2983ffbde13.jpeg", "isPro": false, "fullname": "Haofan Wang", "user": "wanghaofan", "type": "user"}, "name": "Haofan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:47:57.012Z", "hidden": false}, {"_id": "693a372474fced5bf9c32400", "user": {"_id": "661ab3da2b14565c7acccf5c", "avatarUrl": "/avatars/fa4fc03664803e02aede4d4c3d50b393.svg", "isPro": false, "fullname": "Mike Zheng Shou", "user": "AnalMom", "type": "user"}, "name": "Mike Zheng Shou", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:47:38.881Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GAFA0NvcooaUISwA3XiJq.mp4"], "publishedAt": "2025-12-10T02:09:59.000Z", "submittedOnDailyAt": "2025-12-11T00:45:04.130Z", "title": "OmniPSD: Layered PSD Generation with Diffusion Transformer", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent advances in diffusion models have greatly improved image generation and editing, yet generating or reconstructing layered PSD files with transparent alpha channels remains highly challenging. We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning. For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention, producing semantically coherent and hierarchically structured layers. For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image. An RGBA-VAE is employed as an auxiliary representation module to preserve transparency without affecting structure learning. Extensive experiments on our new RGBA-layered dataset demonstrate that OmniPSD achieves high-fidelity generation, structural consistency, and transparency awareness, offering a new paradigm for layered design generation and decomposition with diffusion transformers.", "upvotes": 20, "discussionId": "693a372474fced5bf9c32401", "projectPage": "https://showlab.github.io/OmniPSD/", "ai_summary": "OmniPSD, a diffusion framework within the Flux ecosystem, enables text-to-PSD generation and image-to-PSD decomposition, achieving high-fidelity results with transparency awareness.", "ai_keywords": ["diffusion models", "OmniPSD", "Flux ecosystem", "in-context learning", "spatial attention", "iterative in-context editing", "RGBA-VAE", "RGBA-layered dataset", "diffusion transformers"], "summary_zh": "<ul>\n    <li>OmniPSD \u662f\u4e00\u4e2a\u65b0\u7684\u6269\u6563\u6846\u67b6\uff0c\u53ef\u4ee5\u751f\u6210\u548c\u5206\u89e3 PSD \u6587\u4ef6\u3002</li>\n    <li>\u5b83\u652f\u6301\u4ece\u6587\u672c\u751f\u6210 PSD \u548c\u4ece\u56fe\u50cf\u5206\u89e3\u6210 PSD \u5c42\u3002</li>\n    <li>\u901a\u8fc7\u7a7a\u95f4\u6ce8\u610f\u529b\uff0cOmniPSD \u53ef\u4ee5\u521b\u5efa\u7ed3\u6784\u5316\u7684\u56fe\u5c42\uff0c\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u3002</li>\n    <li>\u5728\u56fe\u50cf\u5206\u89e3\u65f6\uff0c\u5b83\u9010\u6b65\u63d0\u53d6\u548c\u64e6\u9664\u5185\u5bb9\uff0c\u4ee5\u91cd\u5efa\u53ef\u7f16\u8f91\u7684 PSD \u5c42\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cOmniPSD \u5728\u751f\u6210\u8d28\u91cf\u3001\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u900f\u660e\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>OmniPSD is a new tool that helps create and edit layered PSD files with transparency using advanced diffusion models.</li>\n    <li>It can generate PSD files from text descriptions and break down flat images into editable layers.</li>\n    <li>The tool uses spatial attention to organize multiple layers on a canvas, ensuring they fit together nicely.</li>\n    <li>It employs a special method to keep transparency in the images while learning their structure.</li>\n    <li>Tests show that OmniPSD produces high-quality and consistent layers, making it a valuable tool for design work.</li>\n</ul>"}, "publishedAt": "2025-12-09T21:09:59.000Z", "title": "OmniPSD: Layered PSD Generation with Diffusion Transformer", "summary": "Recent advances in diffusion models have greatly improved image generation and editing, yet generating or reconstructing layered PSD files with transparent alpha channels remains highly challenging. We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning. For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention, producing semantically coherent and hierarchically structured layers. For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image. An RGBA-VAE is employed as an auxiliary representation module to preserve transparency without affecting structure learning. Extensive experiments on our new RGBA-layered dataset demonstrate that OmniPSD achieves high-fidelity generation, structural consistency, and transparency awareness, offering a new paradigm for layered design generation and decomposition with diffusion transformers.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GAFA0NvcooaUISwA3XiJq.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09247.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "isAuthorParticipating": false}, {"paper": {"id": "2512.08829", "authors": [{"_id": "693a1f0274fced5bf9c32399", "user": {"_id": "66a105bb456284adf458d656", "avatarUrl": "/avatars/b543a324f7e159d6e84bc68915e93d24.svg", "isPro": false, "fullname": "Tao Hongyuan", "user": "HongyuanTao", "type": "user"}, "name": "Hongyuan Tao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:46:24.642Z", "hidden": false}, {"_id": "693a1f0274fced5bf9c3239a", "user": {"_id": "6577073fc2bf55b1f6bafb49", "avatarUrl": "/avatars/58803398b1a918b7570db17893e65122.svg", "isPro": false, "fullname": "Bencheng liao", "user": "LegendBC", "type": "user"}, "name": "Bencheng Liao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:46:30.879Z", "hidden": false}, {"_id": "693a1f0274fced5bf9c3239b", "name": "Shaoyu Chen", "hidden": false}, {"_id": "693a1f0274fced5bf9c3239c", "name": "Haoran Yin", "hidden": false}, {"_id": "693a1f0274fced5bf9c3239d", "name": "Qian Zhang", "hidden": false}, {"_id": "693a1f0274fced5bf9c3239e", "user": {"_id": "66c2e7fc934e2f07753542ac", "avatarUrl": "/avatars/f6fa3f94435cf1c1d06daa6c925d07d0.svg", "isPro": false, "fullname": "LWY", "user": "wenyuliu", "type": "user"}, "name": "Wenyu Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:46:45.682Z", "hidden": false}, {"_id": "693a1f0274fced5bf9c3239f", "user": {"_id": "62600de6d47e3dbae32ce1ce", "avatarUrl": "/avatars/a536417cfec6e10ac415091bd1829426.svg", "isPro": false, "fullname": "Xinggang Wang", "user": "xinggangw", "type": "user"}, "name": "Xinggang Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:46:51.670Z", "hidden": false}], "publishedAt": "2025-12-09T17:18:32.000Z", "submittedOnDailyAt": "2025-12-11T03:25:34.652Z", "title": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models", "submittedOnDailyBy": {"_id": "66a105bb456284adf458d656", "avatarUrl": "/avatars/b543a324f7e159d6e84bc68915e93d24.svg", "isPro": false, "fullname": "Tao Hongyuan", "user": "HongyuanTao", "type": "user"}, "summary": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.", "upvotes": 12, "discussionId": "693a1f0274fced5bf9c323a0", "projectPage": "https://github.com/hustvl/InfiniteVL", "githubRepo": "https://github.com/hustvl/InfiniteVL", "githubRepoAddedBy": "user", "ai_summary": "InfiniteVL, a linear-complexity VLM architecture combining sliding window attention and Gated DeltaNet, achieves competitive performance with less data and faster inference than leading Transformer-based models.", "ai_keywords": ["window attention", "linear attention", "Vision-Language Models", "VLMs", "sequence length", "window size", "sliding window attention", "Gated DeltaNet", "distillation pretraining", "instruction tuning", "long-sequence SFT", "multimodal performance", "long-term memory retention", "FlashAttention-2", "inference speedup", "real-time prefill speed"], "githubStars": 30, "organization": {"_id": "62600e67ffe8827cb1d6180b", "name": "hustvl", "fullname": "HUST Vision Lab"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u67b6\u6784InfiniteVL\uff0c\u7ed3\u5408\u4e86\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u548cGated DeltaNet\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u6a21\u578b\u7684\u9650\u5236\u3002</li>\n    <li>\u4f7f\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u62ec\u84b8\u998f\u9884\u8bad\u7ec3\u3001\u6307\u4ee4\u8c03\u4f18\u548c\u957f\u5e8f\u5217\u5fae\u8c03\uff0c\u4ee5\u63d0\u9ad8\u591a\u6a21\u6001\u6027\u80fd\u3002</li>\n    <li>InfiniteVL\u5728\u8bad\u7ec3\u6570\u636e\u4f7f\u7528\u91cf\u4e0d\u5230\u9886\u5148\u6a21\u578b2%\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u7ebf\u6027\u590d\u6742\u5ea6\u6a21\u578b\uff0c\u5e76\u4e0e\u9886\u5148\u7684Transformer\u6a21\u578b\u76f8\u5f53\u3002</li>\n    <li>\u4e0e\u4f7f\u7528FlashAttention-2\u7684\u540c\u7b49\u5927\u5c0fTransformer\u6a21\u578b\u76f8\u6bd4\uff0cInfiniteVL\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u63d0\u9ad8\u4e863.6\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5ef6\u8fdf\u548c\u5185\u5b58\u5360\u7528\u4e0d\u53d8\u3002</li>\n    <li>\u5728\u6d41\u5a92\u4f53\u89c6\u9891\u7406\u89e3\u573a\u666f\u4e2d\uff0cInfiniteVL\u80fd\u591f\u7a33\u5b9a\u4ee524 FPS\u7684\u5b9e\u65f6\u901f\u5ea6\u5de5\u4f5c\uff0c\u5e76\u4fdd\u7559\u957f\u671f\u8bb0\u5fc6\u7f13\u5b58\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Window attention and linear attention are two methods used to improve the efficiency of Vision-Language Models (VLMs).</li>\n    <li>Window-based VLMs struggle when the sequence length is longer than the window size, and linear attention is not as effective for tasks that need a lot of information.</li>\n    <li>InfiniteVL is a new VLM design that combines sliding window attention with Gated DeltaNet to address these issues.</li>\n    <li>This model uses a three-stage training process and requires less than 2% of the data used by top VLMs, achieving high performance and good memory retention.</li>\n    <li>InfiniteVL is significantly faster than similar Transformer-based models and maintains a stable performance in real-time video understanding.</li>\n</ul>"}, "publishedAt": "2025-12-09T12:18:32.000Z", "title": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models", "summary": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08829.png", "numComments": 1, "submittedBy": {"_id": "66a105bb456284adf458d656", "avatarUrl": "/avatars/b543a324f7e159d6e84bc68915e93d24.svg", "fullname": "Tao Hongyuan", "name": "HongyuanTao", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "62600e67ffe8827cb1d6180b", "name": "hustvl", "fullname": "HUST Vision Lab"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.09928", "authors": [{"_id": "693a481d74fced5bf9c3247d", "name": "Minghui Lin", "hidden": false}, {"_id": "693a481d74fced5bf9c3247e", "name": "Pengxiang Ding", "hidden": false}, {"_id": "693a481d74fced5bf9c3247f", "name": "Shu Wang", "hidden": false}, {"_id": "693a481d74fced5bf9c32480", "name": "Zifeng Zhuang", "hidden": false}, {"_id": "693a481d74fced5bf9c32481", "name": "Yang Liu", "hidden": false}, {"_id": "693a481d74fced5bf9c32482", "name": "Xinyang Tong", "hidden": false}, {"_id": "693a481d74fced5bf9c32483", "name": "Wenxuan Song", "hidden": false}, {"_id": "693a481d74fced5bf9c32484", "name": "Shangke Lyu", "hidden": false}, {"_id": "693a481d74fced5bf9c32485", "user": {"_id": "65fd82762bf2cd20ddaa193f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png", "isPro": false, "fullname": "Siteng Huang", "user": "huangsiteng", "type": "user"}, "name": "Siteng Huang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:50:45.555Z", "hidden": false}, {"_id": "693a481d74fced5bf9c32486", "name": "Donglin Wang", "hidden": false}], "publishedAt": "2025-12-10T18:59:32.000Z", "submittedOnDailyAt": "2025-12-11T01:59:46.905Z", "title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models", "submittedOnDailyBy": {"_id": "65fd82762bf2cd20ddaa193f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png", "isPro": false, "fullname": "Siteng Huang", "user": "huangsiteng", "type": "user"}, "summary": "Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.", "upvotes": 10, "discussionId": "693a481e74fced5bf9c32487", "projectPage": "https://github.com/OpenHelix-Team/HiF-VLA", "githubRepo": "https://github.com/OpenHelix-Team/HiF-VLA", "githubRepoAddedBy": "user", "ai_summary": "HiF-VLA integrates motion for bidirectional temporal reasoning in VLA models, improving long-horizon manipulation performance with minimal additional latency.", "ai_keywords": ["Motion", "bidirectional temporal reasoning", "hindsight priors", "foresight reasoning", "hindsight-modulated joint expert", "think-while-acting", "LIBERO-Long", "CALVIN ABC-D benchmarks", "real-world long-horizon manipulation tasks"], "githubStars": 17, "organization": {"_id": "6757b280cd7b1c0076deca02", "name": "westlakerobotics", "fullname": "Westlake Robotics Technology (Hangzhou) Co., Ltd.", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64d0a450228324a28bef9816/TG6rVKYnq5xD0AtzBIlEz.jpeg"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5e2e\u52a9\u673a\u5668\u4eba\u901a\u8fc7\u89c6\u89c9\u548c\u8bed\u8a00\u63d0\u793a\u8fdb\u884c\u64cd\u4f5c\u3002</li>\n    <li>\u73b0\u6709\u7684VLA\u6a21\u578b\u4f9d\u8d56\u5f53\u524d\u89c2\u5bdf\uff0c\u5bb9\u6613\u5bfc\u81f4\u5728\u957f\u65f6\u95f4\u4efb\u52a1\u4e2d\u7684\u4e00\u81f4\u6027\u4e0b\u964d\u3002</li>\n    <li>HiF-VLA\u6846\u67b6\u901a\u8fc7\u8fd0\u52a8\u6765\u589e\u5f3a\u65f6\u95f4\u63a8\u7406\uff0c\u6355\u6349\u72b6\u6001\u53d8\u5316\u5e76\u8fc7\u6ee4\u9759\u6001\u566a\u97f3\u3002</li>\n    <li>HiF-VLA\u7ed3\u5408\u8fc7\u53bb\u548c\u672a\u6765\u7684\u52a8\u6001\u4fe1\u606f\uff0c\u652f\u6301\u201c\u8fb9\u60f3\u8fb9\u505a\u201d\u7684\u64cd\u4f5c\u65b9\u5f0f\u3002</li>\n    <li>\u5728\u5b9e\u9645\u7684\u957f\u65f6\u95f4\u64cd\u63a7\u4efb\u52a1\u4e2d\uff0cHiF-VLA\u8868\u73b0\u4f18\u8d8a\uff0c\u4e14\u989d\u5916\u7684\u63a8\u7406\u5ef6\u8fdf\u975e\u5e38\u5c0f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-Language-Action (VLA) models help robots perform tasks by connecting what they see and hear to actions.</li>\n    <li>Many VLA models struggle with understanding longer tasks because they only focus on current observations, which leads to short-sightedness.</li>\n    <li>The HiF-VLA framework improves this by using motion to better understand time and changes in the environment.</li>\n    <li>HiF-VLA looks at past and future movements to make better decisions while acting, allowing for more effective long-term manipulation.</li>\n    <li>This new approach outperforms existing models in tests and works well in real-world robot tasks without slowing down performance.</li>\n</ul>"}, "publishedAt": "2025-12-10T13:59:32.000Z", "title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models", "summary": "Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09928.png", "numComments": 1, "submittedBy": {"_id": "65fd82762bf2cd20ddaa193f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png", "fullname": "Siteng Huang", "name": "huangsiteng", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11}, "organization": {"_id": "6757b280cd7b1c0076deca02", "name": "westlakerobotics", "fullname": "Westlake Robotics Technology (Hangzhou) Co., Ltd.", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64d0a450228324a28bef9816/TG6rVKYnq5xD0AtzBIlEz.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.02892", "authors": [{"_id": "693a4c4d74fced5bf9c32493", "user": {"_id": "655efd24afee0e00788bb589", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg", "isPro": false, "fullname": "Amr Mohamed", "user": "amr-mohamed", "type": "user"}, "name": "Amr Mohamed", "status": "claimed_verified", "statusLastChangedAt": "2025-12-11T10:12:35.890Z", "hidden": false}, {"_id": "693a4c4d74fced5bf9c32494", "name": "Yang Zhang", "hidden": false}, {"_id": "693a4c4d74fced5bf9c32495", "user": {"_id": "6839c2d132331eaf76bea940", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U2daTtJiD-9JYYnbr4Xbu.png", "isPro": false, "fullname": "Michalis Vazirgiannis", "user": "mvazirg", "type": "user"}, "name": "Michalis Vazirgiannis", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:48:25.471Z", "hidden": false}, {"_id": "693a4c4d74fced5bf9c32496", "user": {"_id": "6087e598e2b7cc3a117b0dc5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6087e598e2b7cc3a117b0dc5/Ctz_W-uo1gOQRBHXalD1P.png", "isPro": false, "fullname": "Guokan Shang", "user": "guokan-shang", "type": "user"}, "name": "Guokan Shang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:48:31.176Z", "hidden": false}], "publishedAt": "2025-12-02T16:01:08.000Z", "submittedOnDailyAt": "2025-12-11T02:21:01.009Z", "title": "Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules", "submittedOnDailyBy": {"_id": "655efd24afee0e00788bb589", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg", "isPro": false, "fullname": "Amr Mohamed", "user": "amr-mohamed", "type": "user"}, "summary": "Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves 3.8-4.0times speedups while retaining 99.8-100% of the baseline score on average. On base models, SchED yields consistent speedup gains with 99.1-100% performance retention, with up to 2.34times under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, \u03b3{=}4), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.", "upvotes": 8, "discussionId": "693a4c4d74fced5bf9c32497", "ai_summary": "SchED, a training-free early-exit algorithm, accelerates diffusion large language model decoding with minimal performance loss across various tasks.", "ai_keywords": ["diffusion large language models", "dLLMs", "autoregressive models", "SchED", "early-exit algorithm", "full-span logit margins", "confidence threshold", "multiple-choice question answering", "math", "long-form QA/summarization", "translation", "instruction-tuned models", "base models", "QPS", "entropy analysis", "predictive entropy"], "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5SchED\uff0c\u53ef\u52a0\u901f\u6269\u6563\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u7684\u89e3\u7801\u8fc7\u7a0b\u3002</li>\n    <li>SchED\u901a\u8fc7\u805a\u5408\u5168\u8303\u56f4\u7684logit\u8fb9\u9645\u6765\u5b9e\u73b0\u63d0\u524d\u9000\u51fa\uff0c\u907f\u514d\u4e86\u6162\u901f\u7684\u8fed\u4ee3\u91c7\u6837\u3002</li>\n    <li>\u5728Dream\u548cLLaDA\u4e24\u79cddLLM\u6a21\u578b\u4e0a\u6d4b\u8bd5\uff0cSchED\u5728\u6307\u4ee4\u8c03\u6574\u6a21\u578b\u4e0a\u901f\u5ea6\u63d0\u5347\u8fbe\u52303.8-4.0\u500d\uff0c\u540c\u65f6\u4fdd\u630199.8-100%\u7684\u57fa\u51c6\u5206\u6570\u3002</li>\n    <li>\u5728\u57fa\u7840\u6a21\u578b\u4e2d\uff0cSchED\u4e5f\u5b9e\u73b0\u4e86\u901f\u5ea6\u63d0\u5347\uff0c\u6027\u80fd\u4fdd\u6301\u572899.1-100%\uff0c\u5728\u66f4\u6fc0\u8fdb\u7684\u8bbe\u7f6e\u4e0b\u6700\u9ad8\u53ef\u8fbe2.34\u500d\u3002</li>\n    <li>SchED\u5728\u957f\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u8fdc\u8d85\u4ee5\u5f80\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u63d0\u524d\u9000\u51fa\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u89e3\u7801\u6548\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>SchED is a new algorithm designed to speed up the decoding process of diffusion large language models (dLLMs) without needing additional training.</li>\n    <li>It allows the model to stop generating text once it reaches a certain confidence level, which can save time.</li>\n    <li>Tests on two dLLM families showed SchED can speed up decoding by 3.8-4.0 times while keeping nearly all performance intact (99.8-100% of baseline scores).</li>\n    <li>Even on base models, it achieves consistent speedups with performance retention between 99.1-100%, and up to 2.34 times faster under aggressive conditions.</li>\n    <li>SchED is more effective than previous methods, especially for long-form content, making the use of dLLMs more efficient overall.</li>\n</ul>"}, "publishedAt": "2025-12-02T11:01:08.000Z", "title": "Fast-Decoding Diffusion Language Models via Progress-Aware Confidence Schedules", "summary": "Diffusion large language models (dLLMs) offer a promising alternative to autoregressive models, but their practical utility is severely hampered by slow, iterative sampling. We present SchED, a training-free, model-agnostic early-exit algorithm that aggregates full-span logit margins and halts decoding once a smooth, progress-dependent confidence threshold is met. We evaluated SchED on two dLLM families (Dream and LLaDA), in base and instruction-tuned variants across ten benchmarks spanning downstream tasks including multiple-choice question answering (MCQ), math, long-form QA/summarization, and translation. SchED delivers large, stable accelerations: on instruction-tuned models, it achieves 3.8-4.0times speedups while retaining 99.8-100% of the baseline score on average. On base models, SchED yields consistent speedup gains with 99.1-100% performance retention, with up to 2.34times under more aggressive settings. Using a conservative speed metric that heavily penalizes quality loss (QPS, \u03b3{=}4), we show that SchED is robust and clearly outperforms prior confidence-based early-exit methods, which break down on long-form generation. An entropy analysis of the model's token predictions reveals that instruction tuning speeds up the decay of predictive entropy. By turning genuine confidence stabilization into computational savings, SchED makes dLLM decoding substantially more efficient.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02892.png", "numComments": 1, "submittedBy": {"_id": "655efd24afee0e00788bb589", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg", "fullname": "Amr Mohamed", "name": "amr-mohamed", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10}, "isAuthorParticipating": true}, {"paper": {"id": "2512.09616", "authors": [{"_id": "693ae233e1612ff4bc2a497e", "name": "Yiwu Zhong", "hidden": false}, {"_id": "693ae233e1612ff4bc2a497f", "name": "Zi-Yuan Hu", "hidden": false}, {"_id": "693ae233e1612ff4bc2a4980", "name": "Yin Li", "hidden": false}, {"_id": "693ae233e1612ff4bc2a4981", "name": "Liwei Wang", "hidden": false}], "publishedAt": "2025-12-10T13:05:55.000Z", "submittedOnDailyAt": "2025-12-11T12:55:11.736Z", "title": "Rethinking Chain-of-Thought Reasoning for Videos", "submittedOnDailyBy": {"_id": "62b4fe7ab25cb80fcf2ffd66", "avatarUrl": "/avatars/22f6a05cdbf4224d29ec9259c9fdd7a4.svg", "isPro": false, "fullname": "Yiwu Zhong", "user": "YiwuZhong", "type": "user"}, "summary": "Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.", "upvotes": 7, "discussionId": "693ae234e1612ff4bc2a4982", "githubRepo": "https://github.com/LaVi-Lab/Rethink_CoT_Video", "githubRepoAddedBy": "user", "ai_summary": "Efficient video reasoning can be achieved using concise chains of thought and reduced visual tokens without manual annotations or supervised fine-tuning.", "ai_keywords": ["chain-of-thought", "multimodal large language models", "video reasoning", "visual tokens", "reasoning traces", "post-training", "inference framework", "inference efficiency", "benchmarks", "human-like CoT reasoning"], "githubStars": 4, "summary_zh": "<ul>\n    <li>\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63a8\u7406\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u89c6\u9891\u63a8\u7406\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\u3002</li>\n    <li>\u6211\u4eec\u5047\u8bbe\u7b80\u6d01\u7684\u63a8\u7406\u548c\u51cf\u5c11\u7684\u89c6\u89c9\u8f93\u5165\u53ef\u4ee5\u6709\u6548\u8fdb\u884c\u89c6\u9891\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u540e\u8bad\u7ec3\u548c\u63a8\u7406\u6846\u67b6\uff0c\u63d0\u5347\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5141\u8bb8\u6a21\u578b\u5728\u538b\u7f29\u7684\u89c6\u89c9\u8f93\u5165\u4e0a\u5de5\u4f5c\uff0c\u5e76\u751f\u6210\u7b80\u77ed\u7684\u63a8\u7406\u8fc7\u7a0b\u3002</li>\n    <li>\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u957f\u7684\u7c7b\u4ebaCoT\u63a8\u7406\u5e76\u975e\u8fdb\u884c\u89c6\u9891\u63a8\u7406\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u7b80\u6d01\u63a8\u7406\u540c\u6837\u6709\u6548\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Chain-of-thought (CoT) reasoning helps in solving complex language tasks and is now being applied to video reasoning with multimodal large language models (MLLMs).</li>\n    <li>Long reasoning chains and many visual tokens are usually used, but our research suggests shorter reasoning and fewer visual tokens can also work well.</li>\n    <li>We created a new framework that allows MLLMs to use compressed visual tokens and generate shorter reasoning before answering questions.</li>\n    <li>This framework improves efficiency, performs well on various tests, and does not need manual annotations or extra training.</li>\n    <li>Our findings indicate that simpler reasoning can be just as effective for video reasoning as longer, more complex approaches.</li>\n</ul>"}, "publishedAt": "2025-12-10T08:05:55.000Z", "title": "Rethinking Chain-of-Thought Reasoning for Videos", "summary": "Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09616.png", "numComments": 1, "submittedBy": {"_id": "62b4fe7ab25cb80fcf2ffd66", "avatarUrl": "/avatars/22f6a05cdbf4224d29ec9259c9fdd7a4.svg", "fullname": "Yiwu Zhong", "name": "YiwuZhong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": false}, {"paper": {"id": "2512.04753", "authors": [{"_id": "693a66ff74fced5bf9c324db", "name": "Ruilin Li", "hidden": false}, {"_id": "693a66ff74fced5bf9c324dc", "user": {"_id": "654c6845bac6e6e49895a5b5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KXQaAxulqr8jNBSpEaYM4.png", "isPro": false, "fullname": "SII-Yibin Wang", "user": "CodeGoat24", "type": "user"}, "name": "Yibin Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:49:48.784Z", "hidden": false}, {"_id": "693a66ff74fced5bf9c324dd", "name": "Wenhong Zhu", "hidden": false}, {"_id": "693a66ff74fced5bf9c324de", "name": "Chenglin Li", "hidden": false}, {"_id": "693a66ff74fced5bf9c324df", "name": "Jinghao Zhang", "hidden": false}, {"_id": "693a66ff74fced5bf9c324e0", "name": "Chenliang Li", "hidden": false}, {"_id": "693a66ff74fced5bf9c324e1", "user": {"_id": "667289f903c802764985d8c6", "avatarUrl": "/avatars/916befcbf0e52ce56be49617f31c7bb2.svg", "isPro": false, "fullname": "Junchi Yan", "user": "Rethinker", "type": "user"}, "name": "Junchi Yan", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:50:22.116Z", "hidden": false}, {"_id": "693a66ff74fced5bf9c324e2", "name": "Jiaqi Wang", "hidden": false}], "publishedAt": "2025-12-04T12:43:50.000Z", "submittedOnDailyAt": "2025-12-11T04:11:11.838Z", "title": "EtCon: Edit-then-Consolidate for Reliable Knowledge Editing", "submittedOnDailyBy": {"_id": "654c6845bac6e6e49895a5b5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KXQaAxulqr8jNBSpEaYM4.png", "isPro": false, "fullname": "SII-Yibin Wang", "user": "CodeGoat24", "type": "user"}, "summary": "Knowledge editing aims to update specific facts in large language models (LLMs) without full retraining. Prior efforts sought to tune the knowledge layers of LLMs, proving effective for making selective edits. However, a significant gap exists between their performance in controlled, teacher-forcing evaluations and their real-world effectiveness in lifelong learning scenarios, which greatly limits their practical applicability. This work's empirical analysis reveals two recurring issues associated with this gap: (1) Most traditional methods lead the edited model to overfit to the new fact, thereby degrading pre-trained capabilities; (2) There is a critical absence of a knowledge consolidation stage, leaving new facts insufficiently integrated into LLMs' inference-time behavior under autoregressive generation, thereby leading to a mismatch between parametric knowledge and actual generation behavior. To this end, we propose Edit-then-Consolidate, a novel knowledge editing paradigm that aims to bridge the gap between theoretical knowledge editing methods and their real-world applicability. Specifically, (1) our framework mitigates overfitting via Targeted Proximal Supervised Fine-Tuning (TPSFT) that localizes the edit via a trust-region objective to limit policy drift; (2) Then, a consolidation stage using Group Relative Policy Optimization (GRPO) aligns the edited knowledge with CoT-based inference policy by optimizing trajectory-level behavior under comprehensive reward signals. Extensive experiments demonstrate our framework consistently improves editing reliability and generalization under real-world evaluations, while better preserving locality and pre-trained capabilities.", "upvotes": 7, "discussionId": "693a670074fced5bf9c324e3", "githubRepo": "https://github.com/RlinL/EtCon", "githubRepoAddedBy": "user", "ai_summary": "A novel knowledge editing framework, Edit-then-Consolidate, addresses overfitting and lack of knowledge integration in large language models through targeted fine-tuning and policy optimization, enhancing reliability and generalization.", "ai_keywords": ["knowledge editing", "large language models", "teacher-forcing evaluations", "lifelong learning", "overfitting", "knowledge consolidation", "Targeted Proximal Supervised Fine-Tuning", "trust-region objective", "policy drift", "Group Relative Policy Optimization", "trajectory-level behavior", "comprehensive reward signals", "CoT-based inference policy"], "githubStars": 5, "summary_zh": "<ul>\n    <li>\u77e5\u8bc6\u7f16\u8f91\u65e8\u5728\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u66f4\u65b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u7279\u5b9a\u4e8b\u5b9e\u3002</li>\n    <li>\u4f20\u7edf\u65b9\u6cd5\u5728\u63a7\u5236\u8bc4\u4f30\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u7528\u6027\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\u4e24\u5927\u95ee\u9898\uff1a\u4e00\u662f\u5927\u591a\u6570\u4f20\u7edf\u65b9\u6cd5\u5bfc\u81f4\u6a21\u578b\u8fc7\u5ea6\u62df\u5408\u65b0\u4e8b\u5b9e\uff0c\u635f\u5bb3\u9884\u8bad\u7ec3\u80fd\u529b\uff1b\u4e8c\u662f\u7f3a\u4e4f\u77e5\u8bc6\u6574\u5408\u9636\u6bb5\uff0c\u4f7f\u65b0\u4e8b\u5b9e\u5728\u63a8\u7406\u4e2d\u6574\u5408\u4e0d\u8db3\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u201c\u7f16\u8f91\u7136\u540e\u6574\u5408\u201d\uff0c\u901a\u8fc7\u7279\u5b9a\u7684\u8c03\u6574\u548c\u4f18\u5316\u6765\u6539\u5584\u77e5\u8bc6\u7f16\u8f91\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u73b0\u5b9e\u8bc4\u4f30\u4e2d\u63d0\u9ad8\u4e86\u7f16\u8f91\u7684\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u66f4\u597d\u5730\u4fdd\u7559\u4e86\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Knowledge editing aims to update specific facts in large language models (LLMs) without needing to retrain them completely.</li>\n    <li>Previous methods have shown effective selective edits but struggle in real-world situations, leading to a gap in performance.</li>\n    <li>Two main issues identified are overfitting to new facts and insufficient integration of new knowledge during model use.</li>\n    <li>The proposed solution, Edit-then-Consolidate, includes steps to prevent overfitting and better align new knowledge with the model's existing abilities.</li>\n    <li>Tests show that this new approach improves reliability and overall performance while maintaining the model's original strengths.</li>\n</ul>"}, "publishedAt": "2025-12-04T07:43:50.000Z", "title": "EtCon: Edit-then-Consolidate for Reliable Knowledge Editing", "summary": "Knowledge editing aims to update specific facts in large language models (LLMs) without full retraining. Prior efforts sought to tune the knowledge layers of LLMs, proving effective for making selective edits. However, a significant gap exists between their performance in controlled, teacher-forcing evaluations and their real-world effectiveness in lifelong learning scenarios, which greatly limits their practical applicability. This work's empirical analysis reveals two recurring issues associated with this gap: (1) Most traditional methods lead the edited model to overfit to the new fact, thereby degrading pre-trained capabilities; (2) There is a critical absence of a knowledge consolidation stage, leaving new facts insufficiently integrated into LLMs' inference-time behavior under autoregressive generation, thereby leading to a mismatch between parametric knowledge and actual generation behavior. To this end, we propose Edit-then-Consolidate, a novel knowledge editing paradigm that aims to bridge the gap between theoretical knowledge editing methods and their real-world applicability. Specifically, (1) our framework mitigates overfitting via Targeted Proximal Supervised Fine-Tuning (TPSFT) that localizes the edit via a trust-region objective to limit policy drift; (2) Then, a consolidation stage using Group Relative Policy Optimization (GRPO) aligns the edited knowledge with CoT-based inference policy by optimizing trajectory-level behavior under comprehensive reward signals. Extensive experiments demonstrate our framework consistently improves editing reliability and generalization under real-world evaluations, while better preserving locality and pre-trained capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04753.png", "numComments": 1, "submittedBy": {"_id": "654c6845bac6e6e49895a5b5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KXQaAxulqr8jNBSpEaYM4.png", "fullname": "SII-Yibin Wang", "name": "CodeGoat24", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 19}, "isAuthorParticipating": true}, {"paper": {"id": "2512.09864", "authors": [{"_id": "693a387b74fced5bf9c3241f", "name": "Hao Lu", "hidden": false}, {"_id": "693a387b74fced5bf9c32420", "name": "Ziyang Liu", "hidden": false}, {"_id": "693a387b74fced5bf9c32421", "user": {"_id": "660146b2bcf0790b8d73e447", "avatarUrl": "/avatars/726be122297a03585c107f44f7aa0ed8.svg", "isPro": false, "fullname": "jiang guangfeng", "user": "jiangxb24", "type": "user"}, "name": "Guangfeng Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:49:22.527Z", "hidden": false}, {"_id": "693a387b74fced5bf9c32422", "name": "Yuanfei Luo", "hidden": false}, {"_id": "693a387b74fced5bf9c32423", "name": "Sheng Chen", "hidden": false}, {"_id": "693a387b74fced5bf9c32424", "name": "Yangang Zhang", "hidden": false}, {"_id": "693a387b74fced5bf9c32425", "user": {"_id": "655cba1d87b67834000590e8", "avatarUrl": "/avatars/3bd43b7c9351f65b8f38f4c8237a0146.svg", "isPro": false, "fullname": "Yingcong Chen", "user": "yingcongchen", "type": "user"}, "name": "Ying-Cong Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:48:52.786Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/t3b2dEDsShvqIai2Iit1B.mp4"], "publishedAt": "2025-12-10T17:50:29.000Z", "submittedOnDailyAt": "2025-12-11T00:51:07.730Z", "title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.", "upvotes": 6, "discussionId": "693a387b74fced5bf9c32426", "ai_summary": "A unified framework combines vision-language models and video generation to improve autonomous driving in complex scenarios by enhancing reasoning, trajectory planning, and video generation.", "ai_keywords": ["vision-language-action", "world model", "Understanding-Generation-Planning", "VLMs", "video generation models", "chain-of-thought reasoning", "physically consistent trajectories", "coherent future videos"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u81ea\u4e3b\u9a7e\u9a76\u7cfb\u7edf\u5728\u7279\u6b8a\u60c5\u51b5\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u4e3a\u7f3a\u4e4f\u4e16\u754c\u77e5\u8bc6\u548c\u52a8\u6001\u89c6\u89c9\u5efa\u6a21\u80fd\u529b\u3002</li>\n    <li>\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\u65b9\u6cd5\u65e0\u6cd5\u5229\u7528\u672a\u6807\u8bb0\u7684\u89c6\u9891\u8fdb\u884c\u89c6\u89c9\u56e0\u679c\u5b66\u4e60\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86UniUGP\u6846\u67b6\uff0c\u7ed3\u5408\u573a\u666f\u63a8\u7406\u3001\u672a\u6765\u89c6\u9891\u751f\u6210\u548c\u8f68\u8ff9\u89c4\u5212\u3002</li>\n    <li>\u8be5\u6846\u67b6\u6574\u5408\u4e86\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u89c4\u5212\u6027\u80fd\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u611f\u77e5\u3001\u63a8\u7406\u548c\u51b3\u7b56\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u9002\u5e94\u590d\u6742\u7684\u957f\u5c3e\u573a\u666f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autonomous driving systems have difficulty in unusual situations because they lack knowledge and strong visual modeling.</li>\n    <li>Current methods can't use unlabeled videos for learning visual causes and lack reasoning from large language models.</li>\n    <li>The paper introduces a new framework called UniUGP that combines reasoning, video generation, and trajectory planning.</li>\n    <li>UniUGP uses pre-trained models to improve planning by understanding visual dynamics and semantics.</li>\n    <li>Experiments show that UniUGP performs better than other methods in understanding, reasoning, and decision-making, especially in complex scenarios.</li>\n</ul>"}, "publishedAt": "2025-12-10T12:50:29.000Z", "title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving", "summary": "Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/t3b2dEDsShvqIai2Iit1B.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09864.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2512.08765", "authors": [{"_id": "6938da63dfc35938ba129f3c", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:07.767Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3d", "name": "Yefei He", "hidden": false}, {"_id": "6938da63dfc35938ba129f3e", "user": {"_id": "62d812e143df7719860d05d1", "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg", "isPro": false, "fullname": "zhekai chen", "user": "Azily", "type": "user"}, "name": "Zhekai Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:00.513Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3f", "name": "Shiwei Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f40", "user": {"_id": "637ee45b2438d7485b8d8f6a", "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg", "isPro": false, "fullname": "Xiaogang Xu", "user": "xiaogang00", "type": "user"}, "name": "Xiaogang Xu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:51.241Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f41", "name": "Bin Xia", "hidden": false}, {"_id": "6938da63dfc35938ba129f42", "name": "Dingdong Wang", "hidden": false}, {"_id": "6938da63dfc35938ba129f43", "name": "Hongwei Yi", "hidden": false}, {"_id": "6938da63dfc35938ba129f44", "user": {"_id": "65d5ec74cd05bc1eaa125040", "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg", "isPro": false, "fullname": "Xihui Liu", "user": "XihuiLiu", "type": "user"}, "name": "Xihui Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:32.582Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f45", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:26.372Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f46", "name": "Yu Liu", "hidden": false}, {"_id": "6938da63dfc35938ba129f47", "name": "Yingya Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f48", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:10.566Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "publishedAt": "2025-12-09T16:13:55.000Z", "submittedOnDailyAt": "2025-12-10T00:20:18.797Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "upvotes": 94, "discussionId": "6938da64dfc35938ba129f49", "githubRepo": "https://github.com/ali-vilab/Wan-Move", "githubRepoAddedBy": "user", "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.", "ai_keywords": ["motion control", "video generative models", "dense point trajectories", "latent space", "spatiotemporal feature map", "motion guidance", "image-to-video model", "auxiliary motion encoders", "fine-tuning", "MoveBench", "motion annotations"], "githubStars": 197, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>Wan-Move\u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u73b0\u6709\u7684\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5\u5728\u63a7\u5236\u7cbe\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cWan-Move\u89e3\u51b3\u4e86\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u7269\u4f53\u8fd0\u52a8\u8868\u793a\u4e3a\u5bc6\u96c6\u7684\u70b9\u8f68\u8ff9\uff0c\u5141\u8bb8\u5bf9\u573a\u666f\u8fdb\u884c\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002</li>\n    <li>Wan-Move\u4e0e\u73b0\u6709\u7684\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\uff08\u5982Wan-I2V-14B\uff09\u7ed3\u5408\u4f7f\u7528\uff0c\u65e0\u9700\u6539\u53d8\u67b6\u6784\uff0c\u540c\u65f6\u53bb\u9664\u4e86\u5bf9\u8f85\u52a9\u8fd0\u52a8\u7f16\u7801\u5668\u7684\u9700\u6c42\u3002</li>\n    <li>\u901a\u8fc7\u5728MoveBench\u57fa\u51c6\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0cWan-Move\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u8fd0\u52a8\u8d28\u91cf\uff0c\u5e76\u4e14\u76f8\u5173\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6570\u636e\u5df2\u516c\u5f00\u4f9b\u4f7f\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Wan-Move is a new framework that improves motion control in video generation models.</li>\n    <li>It allows for precise and high-quality motion control, addressing issues found in previous methods.</li>\n    <li>The framework uses dense point trajectories to guide how objects move within a video.</li>\n    <li>It integrates easily with existing models without needing major changes, making it scalable.</li>\n    <li>Wan-Move produces high-quality videos and has been evaluated with a new benchmark called MoveBench.</li>\n</ul>"}, "publishedAt": "2025-12-09T11:13:55.000Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.08478", "authors": [{"_id": "6938e00fdfc35938ba129f4f", "name": "Yuning Gong", "hidden": false}, {"_id": "6938e00fdfc35938ba129f50", "name": "Yifei Liu", "hidden": false}, {"_id": "6938e00fdfc35938ba129f51", "name": "Yifan Zhan", "hidden": false}, {"_id": "6938e00fdfc35938ba129f52", "name": "Muyao Niu", "hidden": false}, {"_id": "6938e00fdfc35938ba129f53", "name": "Xueying Li", "hidden": false}, {"_id": "6938e00fdfc35938ba129f54", "name": "Yuanjun Liao", "hidden": false}, {"_id": "6938e00fdfc35938ba129f55", "name": "Jiaming Chen", "hidden": false}, {"_id": "6938e00fdfc35938ba129f56", "name": "Yuanyuan Gao", "hidden": false}, {"_id": "6938e00fdfc35938ba129f57", "name": "Jiaqi Chen", "hidden": false}, {"_id": "6938e00fdfc35938ba129f58", "name": "Minming Chen", "hidden": false}, {"_id": "6938e00fdfc35938ba129f59", "name": "Li Zhou", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5a", "name": "Yuning Zhang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5b", "name": "Wei Wang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5c", "name": "Xiaoqing Hou", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5d", "name": "Huaxi Huang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5e", "name": "Shixiang Tang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f5f", "name": "Le Ma", "hidden": false}, {"_id": "6938e00fdfc35938ba129f60", "name": "Dingwen Zhang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f61", "name": "Xue Yang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f62", "name": "Junchi Yan", "hidden": false}, {"_id": "6938e00fdfc35938ba129f63", "name": "Yanchi Zhang", "hidden": false}, {"_id": "6938e00fdfc35938ba129f64", "name": "Yinqiang Zheng", "hidden": false}, {"_id": "6938e00fdfc35938ba129f65", "name": "Xiao Sun", "hidden": false}, {"_id": "6938e00fdfc35938ba129f66", "user": {"_id": "6938f4de790b5cd0f6df6462", "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg", "isPro": false, "fullname": "Zhihang Zhong", "user": "Zuica96", "type": "user"}, "name": "Zhihang Zhong", "status": "claimed_verified", "statusLastChangedAt": "2025-12-10T08:56:28.162Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6938f4de790b5cd0f6df6462/OZHh1MEcn5fqR-GNW7m_m.mp4"], "publishedAt": "2025-12-09T10:54:58.000Z", "submittedOnDailyAt": "2025-12-10T07:43:37.566Z", "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform", "submittedOnDailyBy": {"_id": "6938f4de790b5cd0f6df6462", "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg", "isPro": false, "fullname": "Zhihang Zhong", "user": "Zuica96", "type": "user"}, "summary": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.", "upvotes": 64, "discussionId": "6938e00fdfc35938ba129f67", "projectPage": "https://visionary-laboratory.github.io/visionary/", "githubRepo": "https://github.com/Visionary-Laboratory/visionary", "githubRepoAddedBy": "user", "ai_summary": "Visionary is an open web-native platform enabling real-time rendering of 3D Gaussian Splatting and meshes with efficient GPU-based inference, supporting dynamic content and generative models.", "ai_keywords": ["Neural rendering", "3D Gaussian Splatting", "3DGS", "WebGPU", "ONNX inference", "Gaussian Generator contract", "three.js", "TypeScript API", "MLP-based 3DGS", "4DGS", "neural avatars", "style transformation", "GPU-based primitive sorting", "World Model Carrier"], "githubStars": 162, "summary_zh": "<ul>\n    <li>Visionary\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u3001\u57fa\u4e8e\u7f51\u7edc\u7684\u5e73\u53f0\uff0c\u53ef\u4ee5\u5b9e\u65f6\u6e32\u67d3\u5404\u79cd\u9ad8\u65af\u70b9\u548c\u7f51\u683c\u3002</li>\n    <li>\u5b83\u4f7f\u7528\u9ad8\u6548\u7684WebGPU\u6e32\u67d3\u5668\uff0c\u652f\u6301\u52a8\u6001\u795e\u7ecf\u5904\u7406\uff0c\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u7684\u6d4f\u89c8\u5668\u4f53\u9a8c\u3002</li>\n    <li>\u5e73\u53f0\u5f15\u5165\u4e86\u6807\u51c6\u5316\u7684\u9ad8\u65af\u751f\u6210\u5668\u5408\u7ea6\uff0c\u652f\u6301\u9010\u5e27\u751f\u6210\u6216\u66f4\u65b0\u9ad8\u65af\u3002</li>\n    <li>Visionary\u4e0e\u73b0\u6709\u7684Web\u5e94\u7528\u7a0b\u5e8f\u96c6\u6210\u987a\u7545\uff0c\u5e76\u63d0\u4f9b\u7b80\u5355\u7684TypeScript API\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cVisionary\u5728\u6e32\u67d3\u6548\u7387\u4e0a\u4f18\u4e8e\u5f53\u524d\u7684Web\u67e5\u770b\u5668\uff0c\u652f\u6301\u591a\u79cd3D\u9ad8\u65af\u6e32\u67d3\u53d8\u4f53\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Neural rendering, especially 3D Gaussian Splatting (3DGS), is important for building world models but current viewer solutions are complicated and limited.</li>\n    <li>Visionary is a new, open platform that allows for real-time rendering of Gaussian Splatting and meshes directly in web browsers.</li>\n    <li>The platform uses WebGPU for efficient rendering and supports dynamic content through a standardized Gaussian Generator contract.</li>\n    <li>Visionary includes a simple TypeScript API for easy integration into web applications and shows better rendering efficiency than existing viewers.</li>\n    <li>It supports various techniques, making it easier to use and deploy different 3DGS methods for both reconstruction and generative tasks.</li>\n</ul>"}, "publishedAt": "2025-12-09T05:54:58.000Z", "title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform", "summary": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6938f4de790b5cd0f6df6462/OZHh1MEcn5fqR-GNW7m_m.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08478.png", "numComments": 3, "submittedBy": {"_id": "6938f4de790b5cd0f6df6462", "avatarUrl": "/avatars/4f22f0499d96bb749af7e8dba2b0b533.svg", "fullname": "Zhihang Zhong", "name": "Zuica96", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07461", "authors": [{"_id": "6937b96219d912300c34a398", "user": {"_id": "626b889ff451470f861d8c78", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651214465695-noauth.jpeg", "isPro": false, "fullname": "victor wu", "user": "victor-wu", "type": "user"}, "name": "Tong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:22.731Z", "hidden": false}, {"_id": "6937b96219d912300c34a399", "user": {"_id": "6191cc9e6d34e827404cebab", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674119843175-6191cc9e6d34e827404cebab.jpeg", "isPro": false, "fullname": "Yang", "user": "jacklanda", "type": "user"}, "name": "Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:20.278Z", "hidden": false}, {"_id": "6937b96219d912300c34a39a", "user": {"_id": "624505fcd083d28d314de3dd", "avatarUrl": "/avatars/92cf6b6a1d81d7958dbbd21f0bf63f8f.svg", "isPro": false, "fullname": "bai jun", "user": "ba1jun", "type": "user"}, "name": "Jun Bai", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:17.404Z", "hidden": false}, {"_id": "6937b96219d912300c34a39b", "name": "Zixia Jia", "hidden": false}, {"_id": "6937b96219d912300c34a39c", "name": "Shuyi Zhang", "hidden": false}, {"_id": "6937b96219d912300c34a39d", "name": "Ziyong Lin", "hidden": false}, {"_id": "6937b96219d912300c34a39e", "user": {"_id": "64b119c4372d43407723136b", "avatarUrl": "/avatars/d523e181993eea06b7f6a71a592c995e.svg", "isPro": false, "fullname": "YANTING WANG", "user": "Noane", "type": "user"}, "name": "Yanting Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:22:14.418Z", "hidden": false}, {"_id": "6937b96219d912300c34a39f", "name": "Song-Chun Zhu", "hidden": false}, {"_id": "6937b96219d912300c34a3a0", "name": "Zilong Zheng", "hidden": false}], "publishedAt": "2025-12-08T11:39:43.000Z", "submittedOnDailyAt": "2025-12-09T04:12:55.960Z", "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning", "submittedOnDailyBy": {"_id": "63a95a6a7930fa8c7dd63d4e", "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg", "isPro": false, "fullname": "Zilong Zheng", "user": "zlzheng", "type": "user"}, "summary": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.", "upvotes": 49, "discussionId": "6937b96219d912300c34a3a1", "projectPage": "https://bigai-nlco.github.io/Native-Parallel-Reasoner/", "githubRepo": "https://github.com/bigai-nlco/Native-Parallel-Reasoner", "ai_summary": "NPR, a teacher-free framework, enhances Large Language Models with native parallel reasoning capabilities through self-distilled training, Parallel-Aware Policy Optimization, and a robust NPR Engine, achieving substantial performance and speed improvements.", "ai_keywords": ["Native Parallel Reasoner", "Large Language Models", "self-evolve", "parallel reasoning", "self-distilled progressive training", "cold-start format discovery", "topological constraints", "Parallel-Aware Policy Optimization", "branching policies", "execution graph", "adaptive decomposition", "trial and error", "NPR Engine", "memory management", "flow control", "parallel RL training", "reasoning benchmarks", "Qwen3-4B", "genuine parallel execution", "autoregressive decoding", "agentic reasoning"], "githubStars": 18, "organization": {"_id": "63a95ac93453852ef5399a77", "name": "bigai", "fullname": "Beijing Institute for General Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1672043197974-63a95a6a7930fa8c7dd63d4e.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aNative Parallel Reasoner (NPR) \u7684\u65e0\u6559\u5e08\u6846\u67b6\uff0c\u5e2e\u52a9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u6211\u53d1\u5c55\u5e76\u5177\u5907\u771f\u6b63\u7684\u5e76\u884c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>NPR \u901a\u8fc7\u4e09\u9879\u521b\u65b0\u5c06\u6a21\u578b\u4ece\u987a\u5e8f\u6a21\u62df\u8f6c\u53d8\u4e3a\u672c\u5730\u5e76\u884c\u8ba4\u77e5\u3002</li>\n    <li>\u91c7\u7528\u81ea\u6211\u84b8\u998f\u7684\u6e10\u8fdb\u8bad\u7ec3\u65b9\u6cd5\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\uff0c\u9010\u6b65\u8fc7\u6e21\u5230\u4e25\u683c\u7684\u62d3\u6251\u7ea6\u675f\u3002</li>\n    <li>\u5f15\u5165\u65b0\u7684\u5e76\u884c\u4f18\u5316\u7b97\u6cd5 (PAPO)\uff0c\u76f4\u63a5\u5728\u6267\u884c\u56fe\u4e2d\u4f18\u5316\u5206\u652f\u7b56\u7565\uff0c\u5141\u8bb8\u6a21\u578b\u901a\u8fc7\u8bd5\u9519\u5b66\u4e60\u9002\u5e94\u6027\u5206\u89e3\u3002</li>\n    <li>NPR \u5728\u516b\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6700\u9ad824.5%\u7684\u6027\u80fd\u63d0\u5347\u548c4.6\u500d\u7684\u63a8\u7406\u901f\u5ea6\u52a0\u5feb\uff0c\u5c55\u793a\u4e86100%\u7684\u771f\u6b63\u5e76\u884c\u6267\u884c\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>The Native Parallel Reasoner (NPR) is a new system that helps Large Language Models (LLMs) improve their reasoning skills without needing a teacher.</li>\n    <li>NPR uses three main innovations: a self-learning training method, a new algorithm for optimizing decision-making, and a special engine for managing memory and control for better performance.</li>\n    <li>By using NPR, a model called Qwen3-4B showed improvements up to 24.5% in reasoning tests and became 4.6 times faster in processing.</li>\n    <li>NPR can perform tasks in parallel without reverting to older methods, setting a new standard for efficient and scalable reasoning.</li>\n</ul>"}, "publishedAt": "2025-12-08T06:39:43.000Z", "title": "Native Parallel Reasoner: Reasoning in Parallelism via Self-Distilled Reinforcement Learning", "summary": "We introduce Native Parallel Reasoner (NPR), a teacher-free framework that enables Large Language Models (LLMs) to self-evolve genuine parallel reasoning capabilities. NPR transforms the model from sequential emulation to native parallel cognition through three key innovations: 1) a self-distilled progressive training paradigm that transitions from ``cold-start'' format discovery to strict topological constraints without external supervision; 2) a novel Parallel-Aware Policy Optimization (PAPO) algorithm that optimizes branching policies directly within the execution graph, allowing the model to learn adaptive decomposition via trial and error; and 3) a robust NPR Engine that refactors memory management and flow control of SGLang to enable stable, large-scale parallel RL training. Across eight reasoning benchmarks, NPR trained on Qwen3-4B achieves performance gains of up to 24.5% and inference speedups up to 4.6x. Unlike prior baselines that often fall back to autoregressive decoding, NPR demonstrates 100% genuine parallel execution, establishing a new standard for self-evolving, efficient, and scalable agentic reasoning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07461.png", "numComments": 1, "submittedBy": {"_id": "63a95a6a7930fa8c7dd63d4e", "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg", "fullname": "Zilong Zheng", "name": "zlzheng", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "63a95ac93453852ef5399a77", "name": "bigai", "fullname": "Beijing Institute for General Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1672043197974-63a95a6a7930fa8c7dd63d4e.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.09363", "authors": [{"_id": "693a379e74fced5bf9c32412", "user": {"_id": "6486ff6561053da6442fef1a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486ff6561053da6442fef1a/72sdWErAwWtWNJIV5VZsy.jpeg", "isPro": false, "fullname": "KeXing", "user": "KXingLab", "type": "user"}, "name": "Ke Xing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-11T10:13:26.656Z", "hidden": false}, {"_id": "693a379e74fced5bf9c32413", "name": "Longfei Li", "hidden": false}, {"_id": "693a379e74fced5bf9c32414", "user": {"_id": "64b7ab4c037d6452a31910eb", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b7ab4c037d6452a31910eb/0UaBtwyQTysBMndFWZdKu.png", "isPro": false, "fullname": "yuyangyin", "user": "yuyangyin", "type": "user"}, "name": "Yuyang Yin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:43:30.256Z", "hidden": false}, {"_id": "693a379e74fced5bf9c32415", "name": "Hanwen Liang", "hidden": false}, {"_id": "693a379e74fced5bf9c32416", "name": "Guixun Luo", "hidden": false}, {"_id": "693a379e74fced5bf9c32417", "name": "Chen Fang", "hidden": false}, {"_id": "693a379e74fced5bf9c32418", "name": "Jue Wang", "hidden": false}, {"_id": "693a379e74fced5bf9c32419", "name": "Konstantinos N. Plataniotis", "hidden": false}, {"_id": "693a379e74fced5bf9c3241a", "name": "Xiaojie Jin", "hidden": false}, {"_id": "693a379e74fced5bf9c3241b", "name": "Yao Zhao", "hidden": false}, {"_id": "693a379e74fced5bf9c3241c", "name": "Yunchao Wei", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/dFNy8Tf5Ts5qNeWXCvkcB.mp4"], "publishedAt": "2025-12-10T06:50:16.000Z", "submittedOnDailyAt": "2025-12-11T00:46:52.612Z", "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.", "upvotes": 44, "discussionId": "693a379e74fced5bf9c3241d", "projectPage": "https://ke-xing.github.io/StereoWorld/", "ai_summary": "StereoWorld generates high-quality stereo video from monocular input using a pretrained video generator with geometry-aware regularization and spatio-temporal tiling.", "ai_keywords": ["stereo video", "monocular-to-stereo", "pretrained video generator", "geometry-aware regularization", "spatio-temporal tiling", "high-definition stereo video dataset", "natural human interpupillary distance (IPD)", "visual fidelity", "geometric consistency"], "summary_zh": "<ul>\n    <li>XR\u8bbe\u5907\u7684\u5e7f\u6cdb\u4f7f\u7528\u63d0\u9ad8\u4e86\u5bf9\u9ad8\u8d28\u91cf\u7acb\u4f53\u89c6\u9891\u7684\u9700\u6c42\uff0c\u4f46\u5236\u4f5c\u6210\u672c\u9ad8\u4e14\u5bb9\u6613\u51fa\u73b0\u7455\u75b5\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86StereoWorld\uff0c\u4e00\u4e2a\u5c06\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u5668\u7528\u4e8e\u5355\u76ee\u5230\u7acb\u4f53\u89c6\u9891\u751f\u6210\u7684\u6846\u67b6\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5229\u7528\u5355\u76ee\u89c6\u9891\u8f93\u5165\uff0c\u5e76\u91c7\u7528\u51e0\u4f55\u611f\u77e5\u7684\u76d1\u7763\u65b9\u5f0f\uff0c\u786e\u4fdd3D\u7ed3\u6784\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>\u8fd8\u96c6\u6210\u4e86\u4e00\u79cd\u65f6\u7a7a\u62fc\u63a5\u65b9\u6848\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u9ad8\u5206\u8fa8\u7387\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b1100\u4e07\u5e27\u7684\u9ad8\u6e05\u7acb\u4f53\u89c6\u9891\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The use of XR devices is increasing, creating a need for high-quality stereo video, but making it is still expensive and can result in errors.</li>\n    <li>StereoWorld is a new system that turns regular videos into high-quality stereo videos using a pretrained video generator.</li>\n    <li>The system uses information from the original video and includes special techniques to make sure the 3D structure looks accurate.</li>\n    <li>It also uses a method to create high-resolution videos efficiently.</li>\n    <li>A large dataset of over 11 million frames was created to help train and test the system, and StereoWorld performs better than older methods in producing high-quality stereo videos.</li>\n</ul>"}, "publishedAt": "2025-12-10T01:50:16.000Z", "title": "StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation", "summary": "The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/dFNy8Tf5Ts5qNeWXCvkcB.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.09363.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "isAuthorParticipating": false}, {"paper": {"id": "2512.07525", "authors": [{"_id": "693794d319d912300c34a291", "user": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "isPro": false, "fullname": "Xiaoran Liu (SII)", "user": "SII-xrliu", "type": "user"}, "name": "Xiaoran Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:38:01.257Z", "hidden": false}, {"_id": "693794d319d912300c34a292", "name": "Yuerong Song", "hidden": false}, {"_id": "693794d319d912300c34a293", "name": "Zhigeng Liu", "hidden": false}, {"_id": "693794d319d912300c34a294", "user": {"_id": "64805e6dde559d48dbb00627", "avatarUrl": "/avatars/29ca34546411dcc28bbc934e3c26a2ba.svg", "isPro": false, "fullname": "Zengfeng", "user": "ZengfengHuang", "type": "user"}, "name": "Zengfeng Huang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T10:35:11.754Z", "hidden": false}, {"_id": "693794d319d912300c34a295", "user": {"_id": "6491cd52b1e5d3444528edb1", "avatarUrl": "/avatars/a85635d886c7f157b6723dec5c01c030.svg", "isPro": false, "fullname": "Qipeng Guo", "user": "QipengGuo", "type": "user"}, "name": "Qipeng Guo", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T10:35:18.117Z", "hidden": false}, {"_id": "693794d319d912300c34a296", "name": "Zhaoxiang Liu", "hidden": false}, {"_id": "693794d319d912300c34a297", "name": "Shiguo Lian", "hidden": false}, {"_id": "693794d319d912300c34a298", "user": {"_id": "64de18f41d826d7355c285e7", "avatarUrl": "/avatars/23e2c44e3a593415becc02463980f6e8.svg", "isPro": false, "fullname": "Ziwei He", "user": "ziweihe", "type": "user"}, "name": "Ziwei He", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T20:17:41.271Z", "hidden": false}, {"_id": "693794d319d912300c34a299", "user": {"_id": "61457b8deff2c9fdb4de4988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg", "isPro": false, "fullname": "Xipeng Qiu", "user": "xpqiu", "type": "user"}, "name": "Xipeng Qiu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T10:35:28.411Z", "hidden": false}], "publishedAt": "2025-12-08T12:59:54.000Z", "submittedOnDailyAt": "2025-12-09T00:49:29.234Z", "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs", "submittedOnDailyBy": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "isPro": false, "fullname": "Xiaoran Liu (SII)", "user": "SII-xrliu", "type": "user"}, "summary": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.", "upvotes": 41, "discussionId": "693794d419d912300c34a29a", "githubRepo": "https://github.com/OpenMOSS/rope_pp", "ai_summary": "The paper proposes a method to enhance Rotary Position Embeddings by utilizing both the real and imaginary components of the complex-valued dot product, improving long-context modeling in Large Language Models.", "ai_keywords": ["Rotary Position Embeddings", "Large Language Models", "complex-valued dot product", "attention score", "long-context dependencies", "positional information"], "githubStars": 12, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff08RoPE\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7528\u4e8e\u7f16\u7801\u5e8f\u5217\u987a\u5e8f\uff0c\u901a\u8fc7\u5bf9\u67e5\u8be2\u548c\u952e\u5411\u91cf\u8fdb\u884c\u65cb\u8f6c\u3002</li>\n    <li>\u6807\u51c6\u5b9e\u73b0\u53ea\u4f7f\u7528\u590d\u6570\u70b9\u79ef\u7684\u5b9e\u90e8\u6765\u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\uff0c\u5ffd\u7565\u4e86\u5305\u542b\u91cd\u8981\u76f8\u4f4d\u4fe1\u606f\u7684\u865a\u90e8\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u65b9\u6cd5\uff0c\u91cd\u65b0\u5f15\u5165\u88ab\u4e22\u5f03\u7684\u865a\u90e8\uff0c\u5229\u7528\u5b8c\u6574\u7684\u590d\u6570\u8868\u793a\u6765\u521b\u5efa\u53cc\u7ec4\u4ef6\u6ce8\u610f\u529b\u5206\u6570\u3002</li>\n    <li>\u7406\u8bba\u548c\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5efa\u6a21\u957f\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u65b9\u9762\u66f4\u6709\u6548\uff0c\u4fdd\u7559\u4e86\u66f4\u591a\u4f4d\u7f6e\u4fe1\u606f\u3002</li>\n    <li>\u5728\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u7684\u6027\u80fd\u4f18\u4e8e\u6807\u51c6RoPE\uff0c\u5c24\u5176\u5728\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u65f6\u6548\u679c\u66f4\u663e\u8457\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Rotary Position Embeddings (RoPE) help Large Language Models (LLMs) understand the order of sequences using rotations in the complex plane.</li>\n    <li>Current methods only use the real part of the complex dot product for attention scores, ignoring the valuable imaginary part.</li>\n    <li>This paper introduces a new method that includes the imaginary component to improve attention scores and preserve more positional information.</li>\n    <li>The new approach enhances the model's ability to handle long-context dependencies effectively.</li>\n    <li>Tests show that this method consistently performs better than standard RoPE, especially with longer context lengths.</li>\n</ul>"}, "publishedAt": "2025-12-08T07:59:54.000Z", "title": "Beyond Real: Imaginary Extension of Rotary Position Embeddings for Long-Context LLMs", "summary": "Rotary Position Embeddings (RoPE) have become a standard for encoding sequence order in Large Language Models (LLMs) by applying rotations to query and key vectors in the complex plane. Standard implementations, however, utilize only the real component of the complex-valued dot product for attention score calculation. This simplification discards the imaginary component, which contains valuable phase information, leading to a potential loss of relational details crucial for modeling long-context dependencies. In this paper, we propose an extension that re-incorporates this discarded imaginary component. Our method leverages the full complex-valued representation to create a dual-component attention score. We theoretically and empirically demonstrate that this approach enhances the modeling of long-context dependencies by preserving more positional information. Furthermore, evaluations on a suite of long-context language modeling benchmarks show that our method consistently improves performance over the standard RoPE, with the benefits becoming more significant as context length increases. The code is available at https://github.com/OpenMOSS/rope_pp.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07525.png", "numComments": 1, "submittedBy": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "fullname": "Xiaoran Liu (SII)", "name": "SII-xrliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07951", "authors": [{"_id": "6938e892dfc35938ba129ff5", "name": "Zekai Luo", "hidden": false}, {"_id": "6938e892dfc35938ba129ff6", "name": "Zongze Du", "hidden": false}, {"_id": "6938e892dfc35938ba129ff7", "name": "Zhouhang Zhu", "hidden": false}, {"_id": "6938e892dfc35938ba129ff8", "name": "Hao Zhong", "hidden": false}, {"_id": "6938e892dfc35938ba129ff9", "user": {"_id": "632179745fc60c44fd91fc33", "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg", "isPro": false, "fullname": "zhumuzhi", "user": "Z-MU-Z", "type": "user"}, "name": "Muzhi Zhu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T10:51:43.541Z", "hidden": false}, {"_id": "6938e892dfc35938ba129ffa", "name": "Wen Wang", "hidden": false}, {"_id": "6938e892dfc35938ba129ffb", "name": "Yuling Xi", "hidden": false}, {"_id": "6938e892dfc35938ba129ffc", "name": "Chenchen Jing", "hidden": false}, {"_id": "6938e892dfc35938ba129ffd", "name": "Hao Chen", "hidden": false}, {"_id": "6938e892dfc35938ba129ffe", "name": "Chunhua Shen", "hidden": false}], "publishedAt": "2025-12-08T19:00:04.000Z", "submittedOnDailyAt": "2025-12-10T00:59:39.366Z", "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality", "submittedOnDailyBy": {"_id": "632179745fc60c44fd91fc33", "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg", "isPro": false, "fullname": "zhumuzhi", "user": "Z-MU-Z", "type": "user"}, "summary": "Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap", "upvotes": 40, "discussionId": "6938e892dfc35938ba129fff", "projectPage": "https://aim-uofa.github.io/LivingSwap", "ai_summary": "LivingSwap enhances video face swapping by using keyframes and reference guidance to maintain identity and fidelity over long sequences, reducing manual effort and achieving state-of-the-art results.", "ai_keywords": ["keyframe conditioning", "video reference guidance", "temporal stitching", "identity preservation", "high-fidelity reconstruction", "Face2Face dataset"], "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u6362\u8138\u5728\u7535\u5f71\u548c\u5a31\u4e50\u5236\u4f5c\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5728\u957f\u89c6\u9891\u5e8f\u5217\u4e2d\u4fdd\u6301\u9ad8\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86LivingSwap\uff0c\u8fd9\u662f\u9996\u4e2a\u89c6\u9891\u53c2\u8003\u5f15\u5bfc\u7684\u6362\u8138\u6a21\u578b\uff0c\u5229\u7528\u6e90\u89c6\u9891\u7684\u89c6\u89c9\u5c5e\u6027\u6765\u63d0\u9ad8\u6362\u8138\u6548\u679c\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4f7f\u7528\u5173\u952e\u5e27\u4f5c\u4e3a\u4fe1\u53f7\uff0c\u7075\u6d3b\u5730\u6ce8\u5165\u76ee\u6807\u8eab\u4efd\uff0c\u5b9e\u73b0\u53ef\u63a7\u7684\u7f16\u8f91\u6548\u679c\u3002</li>\n    <li>\u901a\u8fc7\u5173\u952e\u5e27\u548c\u89c6\u9891\u53c2\u8003\u7684\u7ed3\u5408\uff0c\u6a21\u578b\u80fd\u591f\u5728\u957f\u89c6\u9891\u5e8f\u5217\u4e2d\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u9ad8\u4fdd\u771f\u91cd\u5efa\u3002</li>\n    <li>\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\u7684\u540c\u65f6\uff0c\u80fd\u591f\u65e0\u7f1d\u5730\u7ed3\u5408\u76ee\u6807\u8eab\u4efd\u4e0e\u6e90\u89c6\u9891\u7684\u8868\u60c5\u3001\u5149\u7ebf\u548c\u52a8\u4f5c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video face swapping is important for movies and entertainment, but it's challenging to keep the quality and consistency over long videos.</li>\n    <li>The new model, LivingSwap, uses keyframes from videos to improve how well it swaps faces while maintaining the original video's look and feel.</li>\n    <li>LivingSwap combines keyframe information with video references to keep the identity stable and enhance the quality across longer sequences.</li>\n    <li>To support this technology, a new dataset called Face2Face was created to provide reliable training data for the model.</li>\n    <li>Tests show that LivingSwap provides excellent results, making it easier to create high-quality face swaps with less manual work.</li>\n</ul>"}, "publishedAt": "2025-12-08T14:00:04.000Z", "title": "Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality", "summary": "Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07951.png", "numComments": 1, "submittedBy": {"_id": "632179745fc60c44fd91fc33", "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg", "fullname": "zhumuzhi", "name": "Z-MU-Z", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "61bac2af530e5c78d7b99667", "name": "zju", "fullname": "Zhejiang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5e1058e9fcf41d740b69966d/7G1xjlxwCdMEmKcxNR0n5.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07469", "authors": [{"_id": "69379e0319d912300c34a2fb", "user": {"_id": "6486df66373f79a52913e017", "avatarUrl": "/avatars/4741683fbbcec3a615d0a8df62bc6fec.svg", "isPro": false, "fullname": "Xiangpeng Yang", "user": "XiangpengYang", "type": "user"}, "name": "Xiangpeng Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T10:34:33.919Z", "hidden": false}, {"_id": "69379e0319d912300c34a2fc", "name": "Ji Xie", "hidden": false}, {"_id": "69379e0319d912300c34a2fd", "name": "Yiyuan Yang", "hidden": false}, {"_id": "69379e0319d912300c34a2fe", "name": "Yan Huang", "hidden": false}, {"_id": "69379e0319d912300c34a2ff", "name": "Min Xu", "hidden": false}, {"_id": "69379e0319d912300c34a300", "name": "Qiang Wu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6486df66373f79a52913e017/iKPp57sIku0K9HVBgHjuu.mp4"], "publishedAt": "2025-12-08T11:50:18.000Z", "submittedOnDailyAt": "2025-12-09T01:34:28.853Z", "title": "Unified Video Editing with Temporal Reasoner", "submittedOnDailyBy": {"_id": "6486df66373f79a52913e017", "avatarUrl": "/avatars/4741683fbbcec3a615d0a8df62bc6fec.svg", "isPro": false, "fullname": "Xiangpeng Yang", "user": "XiangpengYang", "type": "user"}, "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.", "upvotes": 31, "discussionId": "69379e0319d912300c34a301", "projectPage": "https://videocof.github.io/", "githubRepo": "https://github.com/knightyxp/VideoCoF", "ai_summary": "VideoCoF, a Chain-of-Frames approach, improves video editing precision and instruction-to-region mapping by using reasoning tokens without requiring user-provided masks.", "ai_keywords": ["chain-of-frames", "chain-of-thought reasoning", "video diffusion model", "reasoning tokens", "edit-region latents", "target video tokens", "instruction-to-region alignment", "fine-grained video editing", "RoPE alignment", "motion alignment", "length extrapolation"], "githubStars": 25, "organization": {"_id": "67c4a2574f5d0005fd418d85", "name": "staraj3", "fullname": "University of Technology Sydney", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67c4a1ab71e55dc41e273b5a/8l4iiw2XH5nbIlLURt7ep.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u5b58\u5728\u4e00\u4e2a\u91cd\u8981\u7684\u6743\u8861\uff1a\u4e13\u5bb6\u6a21\u578b\u7cbe\u786e\u4f46\u4f9d\u8d56\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u5148\u9a8c\uff08\u5982\u63a9\u7801\uff09\uff0c\u800c\u7edf\u4e00\u7684\u5b66\u4e60\u6a21\u578b\u65e0\u63a9\u7801\u4f46\u7f3a\u4e4f\u7a7a\u95f4\u7ebf\u7d22\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86VideoCoF\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u201c\u5e27\u94fe\u201d\u65b9\u6cd5\uff0c\u7075\u611f\u6765\u81ea\u201c\u601d\u7ef4\u94fe\u201d\u63a8\u7406\u3002</li>\n    <li>VideoCoF\u9075\u5faa\u201c\u5148\u770b\uff0c\u540e\u63a8\u7406\uff0c\u518d\u7f16\u8f91\u201d\u7684\u8fc7\u7a0b\uff0c\u8981\u6c42\u89c6\u9891\u6269\u6563\u6a21\u578b\u5148\u9884\u6d4b\u63a8\u7406\u6807\u8bb0\uff0c\u7136\u540e\u751f\u6210\u76ee\u6807\u89c6\u9891\u6807\u8bb0\u3002</li>\n    <li>\u8fd9\u79cd\u660e\u786e\u7684\u63a8\u7406\u6b65\u9aa4\u6d88\u9664\u4e86\u7528\u6237\u63d0\u4f9b\u63a9\u7801\u7684\u9700\u6c42\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u6307\u4ee4\u4e0e\u533a\u57df\u4e4b\u95f4\u7684\u7cbe\u786e\u5bf9\u9f50\u548c\u7ec6\u7c92\u5ea6\u89c6\u9891\u7f16\u8f91\u3002</li>\n    <li>\u6211\u4eec\u5c55\u793a\u4e86\u901a\u8fc7\u4ec5\u4f7f\u752850,000\u5bf9\u89c6\u9891\uff0cVideoCoF\u5728VideoCoF-Bench\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6548\u7387\u548c\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current video editing methods struggle between precision with expert models that need specific inputs and simpler models that lack accuracy.</li>\n    <li>VideoCoF is a new method that improves video editing by using a \"see, reason, then edit\" approach, allowing for better alignment of instructions to video regions.</li>\n    <li>This method eliminates the need for user-provided masks, making it easier to edit videos accurately.</li>\n    <li>VideoCoF includes a strategy for aligning motion and extending video length beyond what was used in training.</li>\n    <li>With only 50,000 video pairs for training, VideoCoF shows top performance on a benchmark test, proving it is both effective and efficient.</li>\n</ul>"}, "publishedAt": "2025-12-08T06:50:18.000Z", "title": "Unified Video Editing with Temporal Reasoner", "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6486df66373f79a52913e017/iKPp57sIku0K9HVBgHjuu.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07469.png", "numComments": 5, "submittedBy": {"_id": "6486df66373f79a52913e017", "avatarUrl": "/avatars/4741683fbbcec3a615d0a8df62bc6fec.svg", "fullname": "Xiangpeng Yang", "name": "XiangpengYang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "67c4a2574f5d0005fd418d85", "name": "staraj3", "fullname": "University of Technology Sydney", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67c4a1ab71e55dc41e273b5a/8l4iiw2XH5nbIlLURt7ep.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07802", "authors": [{"_id": "69392ceedfc35938ba12a187", "user": {"_id": "65e5eae6958b39864e8b683e", "avatarUrl": "/avatars/b6a857e7b725767197dd95bc876f8ad1.svg", "isPro": true, "fullname": "Zhaochong An", "user": "ZhaochongAn", "type": "user"}, "name": "Zhaochong An", "status": "claimed_verified", "statusLastChangedAt": "2025-12-10T13:07:50.839Z", "hidden": false}, {"_id": "69392ceedfc35938ba12a188", "name": "Menglin Jia", "hidden": false}, {"_id": "69392ceedfc35938ba12a189", "name": "Haonan Qiu", "hidden": false}, {"_id": "69392ceedfc35938ba12a18a", "name": "Zijian Zhou", "hidden": false}, {"_id": "69392ceedfc35938ba12a18b", "name": "Xiaoke Huang", "hidden": false}, {"_id": "69392ceedfc35938ba12a18c", "name": "Zhiheng Liu", "hidden": false}, {"_id": "69392ceedfc35938ba12a18d", "name": "Weiming Ren", "hidden": false}, {"_id": "69392ceedfc35938ba12a18e", "user": {"_id": "65a12ec4c9873890d15c4ab9", "avatarUrl": "/avatars/ce4d46f575ba757f78eabdb25b394171.svg", "isPro": false, "fullname": "Kumara Kahatapitiya", "user": "kumarak", "type": "user"}, "name": "Kumara Kahatapitiya", "status": "claimed_verified", "statusLastChangedAt": "2025-12-10T10:51:00.011Z", "hidden": false}, {"_id": "69392ceedfc35938ba12a18f", "name": "Ding Liu", "hidden": false}, {"_id": "69392ceedfc35938ba12a190", "name": "Sen He", "hidden": false}, {"_id": "69392ceedfc35938ba12a191", "name": "Chenyang Zhang", "hidden": false}, {"_id": "69392ceedfc35938ba12a192", "name": "Tao Xiang", "hidden": false}, {"_id": "69392ceedfc35938ba12a193", "name": "Fanny Yang", "hidden": false}, {"_id": "69392ceedfc35938ba12a194", "name": "Serge Belongie", "hidden": false}, {"_id": "69392ceedfc35938ba12a195", "name": "Tian Xie", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e5eae6958b39864e8b683e/iJ5L2BbZWW8XiE7ORZuFK.mp4"], "publishedAt": "2025-12-08T18:32:24.000Z", "submittedOnDailyAt": "2025-12-10T06:06:45.364Z", "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory", "submittedOnDailyBy": {"_id": "65e5eae6958b39864e8b683e", "avatarUrl": "/avatars/b6a857e7b725767197dd95bc876f8ad1.svg", "isPro": true, "fullname": "Zhaochong An", "user": "ZhaochongAn", "type": "user"}, "summary": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.", "upvotes": 31, "discussionId": "69392ceedfc35938ba12a196", "projectPage": "https://zhaochongan.github.io/projects/OneStory/", "ai_summary": "OneStory generates coherent multi-shot videos by modeling global cross-shot context through a Frame Selection module and an Adaptive Conditioner, leveraging pretrained image-to-video models and a curated dataset.", "ai_keywords": ["multi-shot video generation", "next-shot generation", "autoregressive shot synthesis", "pretrained image-to-video models", "Frame Selection module", "Adaptive Conditioner", "semantically-relevant global memory", "importance-guided patchification", "referential captions", "text-conditioned", "image-conditioned"], "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "summary_zh": "<ul>\n    <li>OneStory \u662f\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u5904\u7406\u591a\u4e2a\u955c\u5934\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u4ece\u800c\u8bb2\u8ff0\u8fde\u8d2f\u7684\u6545\u4e8b\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e0b\u4e00\u955c\u5934\u751f\u6210\u4efb\u52a1\uff0c\u63d0\u5347\u4e86\u89c6\u9891\u5408\u6210\u7684\u8fde\u8d2f\u6027\u548c\u8d28\u91cf\u3002</li>\n    <li>OneStory \u5f15\u5165\u4e86\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u5e27\u9009\u62e9\u6a21\u5757\u548c\u81ea\u9002\u5e94\u8c03\u8282\u5668\uff0c\u5e2e\u52a9\u521b\u5efa\u8bed\u4e49\u76f8\u5173\u7684\u5168\u7403\u8bb0\u5fc6\u548c\u7d27\u51d1\u7684\u4e0a\u4e0b\u6587\u3002</li>\n    <li>\u7814\u7a76\u56e2\u961f\u8fd8\u5efa\u7acb\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u591a\u955c\u5934\u6570\u636e\u96c6\uff0c\u4ee5\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u7684\u53d9\u4e8b\u6a21\u5f0f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\u3002</li>\n    <li>\u7ecf\u8fc7\u5fae\u8c03\u7684 OneStory \u5728\u591a\u79cd\u590d\u6742\u573a\u666f\u4e2d\uff0c\u5728\u6587\u672c\u548c\u56fe\u50cf\u6761\u4ef6\u4e0b\uff0c\u8fbe\u5230\u4e86\u53d9\u4e8b\u8fde\u8d2f\u6027\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Storytelling in videos often uses multiple shots that connect to tell a complete story.</li>\n    <li>Current methods for generating multi-shot videos struggle with understanding the context between shots, leading to poor results in complex narratives.</li>\n    <li>OneStory is a new approach that improves how videos are generated by focusing on the next shot and using previous shots for better context.</li>\n    <li>It includes two main features: a Frame Selection module that picks important frames from earlier shots, and an Adaptive Conditioner that creates a compact context for better video generation.</li>\n    <li>OneStory has been trained on a large dataset and performs well in creating coherent narratives for both text and image-based inputs.</li>\n</ul>"}, "publishedAt": "2025-12-08T13:32:24.000Z", "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory", "summary": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e5eae6958b39864e8b683e/iJ5L2BbZWW8XiE7ORZuFK.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07802.png", "numComments": 1, "submittedBy": {"_id": "65e5eae6958b39864e8b683e", "avatarUrl": "/avatars/b6a857e7b725767197dd95bc876f8ad1.svg", "fullname": "Zhaochong An", "name": "ZhaochongAn", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.08560", "authors": [{"_id": "693a7def74fced5bf9c32537", "user": {"_id": "6492c419702103104f9450c4", "avatarUrl": "/avatars/d6700e4bb1b9f172096ea31ba15a83b2.svg", "isPro": false, "fullname": "navve wasserman", "user": "navvew", "type": "user"}, "name": "Navve Wasserman", "status": "claimed_verified", "statusLastChangedAt": "2025-12-11T10:07:40.033Z", "hidden": false}, {"_id": "693a7def74fced5bf9c32538", "user": {"_id": "6735088b22d14a01ae17501f", "avatarUrl": "/avatars/23d2eb2bb833dcf7a05434b499fedd5e.svg", "isPro": false, "fullname": "Matias Cosarinsky", "user": "mcosarinsky", "type": "user"}, "name": "Matias Cosarinsky", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:45:36.526Z", "hidden": false}, {"_id": "693a7def74fced5bf9c32539", "user": {"_id": "687f44e793eb81b0684b4eee", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/uDef9M8sQxTweYvwcVs07.png", "isPro": false, "fullname": "Yuval Golbari", "user": "yuvalgolbari", "type": "user"}, "name": "Yuval Golbari", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:45:41.956Z", "hidden": false}, {"_id": "693a7def74fced5bf9c3253a", "name": "Aude Oliva", "hidden": false}, {"_id": "693a7def74fced5bf9c3253b", "user": {"_id": "6744c6ec6ec99a37d4ba9235", "avatarUrl": "/avatars/a5384b63bb615192f6fa157c6ea89e92.svg", "isPro": false, "fullname": "Antonio", "user": "Antoniotorralbaborruel", "type": "user"}, "name": "Antonio Torralba", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:45:57.490Z", "hidden": false}, {"_id": "693a7def74fced5bf9c3253c", "user": {"_id": "63914afeb6b839bb6143a6be", "avatarUrl": "/avatars/da25b555d105f4755c8187479469ca77.svg", "isPro": false, "fullname": "Tamar Rott Shaham", "user": "tamarott", "type": "user"}, "name": "Tamar Rott Shaham", "status": "admin_assigned", "statusLastChangedAt": "2025-12-11T10:46:03.740Z", "hidden": false}, {"_id": "693a7def74fced5bf9c3253d", "name": "Michal Irani", "hidden": false}], "publishedAt": "2025-12-09T13:01:17.000Z", "submittedOnDailyAt": "2025-12-11T05:54:20.652Z", "title": "BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain", "submittedOnDailyBy": {"_id": "6492c419702103104f9450c4", "avatarUrl": "/avatars/d6700e4bb1b9f172096ea31ba15a83b2.svg", "isPro": false, "fullname": "navve wasserman", "user": "navvew", "type": "user"}, "summary": "Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.", "upvotes": 30, "discussionId": "693a7def74fced5bf9c3253e", "projectPage": "https://navvewas.github.io/BrainExplore/", "ai_summary": "An automated framework uses unsupervised decomposition and natural language descriptions to identify and explain visual representations in human brain fMRI data.", "ai_keywords": ["fMRI", "unsupervised decomposition", "natural images", "natural-language descriptions", "voxel patterns"], "summary_zh": "<ul>\n    <li>\u7814\u7a76\u4eba\u8111\u5982\u4f55\u8868\u793a\u89c6\u89c9\u6982\u5ff5\u53ca\u5176\u7f16\u7801\u533a\u57df\u662f\u4e00\u4e2a\u957f\u671f\u6311\u6218\u3002</li>\n    <li>\u5c3d\u7ba1\u5df2\u6709\u8bb8\u591a\u7814\u7a76\uff0c\u4f46\u8111\u4fe1\u53f7\u590d\u6742\uff0c\u89c6\u89c9\u6982\u5ff5\u7a7a\u95f4\u5e7f\u9614\uff0c\u5bfc\u81f4\u5927\u591a\u6570\u7814\u7a76\u89c4\u6a21\u8f83\u5c0f\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u9a8c\u8bc1\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u53d1\u73b0\u548c\u89e3\u91ca\u4eba\u8111\u76ae\u5c42\u4e2d\u7684\u89c6\u89c9\u8868\u5f81\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5206\u4e3a\u4e24\u4e2a\u4e3b\u8981\u9636\u6bb5\uff1a\u53d1\u73b0\u53ef\u89e3\u91ca\u7684fMRI\u6d3b\u52a8\u6a21\u5f0f\u548c\u751f\u6210\u81ea\u7136\u56fe\u50cf\u53ca\u5176\u89c6\u89c9\u610f\u4e49\u7684\u63cf\u8ff0\u3002</li>\n    <li>\u6211\u4eec\u7684\u6846\u67b6\u63ed\u793a\u4e86\u6570\u5343\u79cd\u53ef\u89e3\u91ca\u7684\u6a21\u5f0f\uff0c\u8986\u76d6\u8bb8\u591a\u4e0d\u540c\u7684\u89c6\u89c9\u6982\u5ff5\uff0c\u5305\u62ec\u4e4b\u524d\u672a\u62a5\u9053\u7684\u7ec6\u81f4\u8868\u5f81\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Understanding how the brain represents visual concepts is a complex challenge.</li>\n    <li>Most studies are small and focused, often lacking systematic validation.</li>\n    <li>This research presents a new large-scale automated method for analyzing brain activity related to visual concepts.</li>\n    <li>The method involves finding patterns in brain scans and explaining them with relevant images and descriptions.</li>\n    <li>The framework identifies thousands of distinct visual patterns, including many that were not previously reported.</li>\n</ul>"}, "publishedAt": "2025-12-09T08:01:17.000Z", "title": "BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain", "summary": "Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08560.png", "numComments": 2, "submittedBy": {"_id": "6492c419702103104f9450c4", "avatarUrl": "/avatars/d6700e4bb1b9f172096ea31ba15a83b2.svg", "fullname": "navve wasserman", "name": "navvew", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": true}, {"paper": {"id": "2512.07834", "authors": [{"_id": "69379f8019d912300c34a30d", "user": {"_id": "67178582bc4492cad19a1f14", "avatarUrl": "/avatars/f2481c0c70a857a862d887beb05c428e.svg", "isPro": false, "fullname": "Yi-Chuan Huang", "user": "YiChuanH", "type": "user"}, "name": "Yi-Chuan Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-09T09:37:53.207Z", "hidden": false}, {"_id": "69379f8019d912300c34a30e", "user": {"_id": "6630a67d6b3957f5983658b1", "avatarUrl": "/avatars/4cc0c264e0d8cc0ebc346bb9b1abe8de.svg", "isPro": false, "fullname": "Chan Jiewen", "user": "JiewenChan", "type": "user"}, "name": "Jiewen Chan", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T11:13:18.274Z", "hidden": false}, {"_id": "69379f8019d912300c34a30f", "user": {"_id": "6784959669a180a9949ec5d9", "avatarUrl": "/avatars/b1fcc9a00a8e6e2baaa338c4b04ea9e2.svg", "isPro": false, "fullname": "Hao-Jen Chien", "user": "chien90190", "type": "user"}, "name": "Hao-Jen Chien", "status": "admin_assigned", "statusLastChangedAt": "2025-12-09T11:13:24.865Z", "hidden": false}, {"_id": "69379f8019d912300c34a310", "name": "Yu-Lun Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xmWIqFounUj-kosdkGMIN.mp4"], "publishedAt": "2025-12-08T18:59:58.000Z", "submittedOnDailyAt": "2025-12-09T01:33:21.818Z", "title": "Voxify3D: Pixel Art Meets Volumetric Rendering", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/", "upvotes": 28, "discussionId": "69379f8019d912300c34a311", "projectPage": "https://yichuanh.github.io/Voxify-3D/", "ai_summary": "Voxify3D is a two-stage framework that combines 3D mesh optimization with 2D pixel art supervision to generate high-quality voxel art with semantic preservation, pixel-art aesthetics, and discrete color coherence.", "ai_keywords": ["orthographic pixel art supervision", "patch-based CLIP alignment", "Gumbel-Softmax quantization", "volumetric rendering", "CLIP-IQA", "user preference"], "summary_zh": "<ul>\n    <li>Voxel\u827a\u672f\u5728\u6e38\u620f\u548c\u6570\u5b57\u5a92\u4f53\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u4ece3D\u7f51\u683c\u81ea\u52a8\u751f\u6210\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8fc7\u4e8e\u7b80\u5316\u51e0\u4f55\u5f62\u72b6\uff0c\u8981\u4e48\u65e0\u6cd5\u5b9e\u73b0\u50cf\u7d20\u7cbe\u786e\u7684\u827a\u672f\u98ce\u683c\u3002</li>\n    <li>Voxify3D\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u5c063D\u7f51\u683c\u4f18\u5316\u4e0e2D\u50cf\u7d20\u827a\u672f\u76d1\u7763\u7ed3\u5408\u8d77\u6765\u3002</li>\n    <li>\u8be5\u6846\u67b6\u7684\u521b\u65b0\u70b9\u5305\u62ec\u6d88\u9664\u900f\u89c6\u5931\u771f\u7684\u6b63\u4ea4\u50cf\u7d20\u827a\u672f\u76d1\u7763\u3001\u4fdd\u6301\u8bed\u4e49\u7684\u4e00\u81f4\u6027\u3001\u4ee5\u53ca\u53ef\u63a7\u8c03\u8272\u677f\u7684\u91cf\u5316\u65b9\u6cd5\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u89d2\u8272\u548c\u53ef\u63a7\u62bd\u8c61\u4e0a\u8868\u73b0\u4f18\u79c0\uff0c\u7528\u6237\u504f\u597d\u5ea6\u8fbe\u523077.90%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Voxel art is popular in games but creating it automatically from 3D models is difficult because of conflicting needs.</li>\n    <li>Existing methods either simplify the geometry too much or don\u2019t match the detailed look of voxel art.</li>\n    <li>Voxify3D is a new framework that combines 3D mesh optimization with 2D pixel art guidance.</li>\n    <li>It includes unique features like pixel art supervision for better alignment, semantic preservation techniques, and a special color optimization method.</li>\n    <li>Tests show it outperforms other methods in creating voxel art with a high level of user preference and quality.</li>\n</ul>"}, "publishedAt": "2025-12-08T13:59:58.000Z", "title": "Voxify3D: Pixel Art Meets Volumetric Rendering", "summary": "Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/xmWIqFounUj-kosdkGMIN.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.07834.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2511.18538", "authors": [{"_id": "692e667137312eaa83fd8832", "user": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "name": "Jian Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:34.886Z", "hidden": false}, {"_id": "692e667137312eaa83fd8833", "name": "Xianglong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8834", "name": "Weifeng Lv", "hidden": false}, {"_id": "692e667137312eaa83fd8835", "name": "Ken Deng", "hidden": false}, {"_id": "692e667137312eaa83fd8836", "name": "Shawn Guo", "hidden": false}, {"_id": "692e667137312eaa83fd8837", "name": "Lin Jing", "hidden": false}, {"_id": "692e667137312eaa83fd8838", "name": "Yizhi Li", "hidden": false}, {"_id": "692e667137312eaa83fd8839", "name": "Shark Liu", "hidden": false}, {"_id": "692e667137312eaa83fd883a", "name": "Xianzhen Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883b", "name": "Yuyu Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883c", "name": "Changzai Pan", "hidden": false}, {"_id": "692e667137312eaa83fd883d", "name": "Ensheng Shi", "hidden": false}, {"_id": "692e667137312eaa83fd883e", "name": "Yingshui Tan", "hidden": false}, {"_id": "692e667137312eaa83fd883f", "name": "Renshuai Tao", "hidden": false}, {"_id": "692e667137312eaa83fd8840", "user": {"_id": "66a8e2538407031e388c501f", "avatarUrl": "/avatars/d16d51f7b1e111efd6d0985995b614be.svg", "isPro": false, "fullname": "wjj", "user": "wuyuverse", "type": "user"}, "name": "Jiajun Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:36.195Z", "hidden": false}, {"_id": "692e667137312eaa83fd8841", "name": "Xianjie Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8842", "name": "Zhenhe Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8843", "name": "Daoguang Zan", "hidden": false}, {"_id": "692e667137312eaa83fd8844", "name": "Chenchen Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8845", "user": {"_id": "672c9ba69380700b602c46c1", "avatarUrl": "/avatars/3d0fd966df540d34095d2c84ce449180.svg", "isPro": false, "fullname": "wei zhang", "user": "zwpride", "type": "user"}, "name": "Wei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:37.970Z", "hidden": false}, {"_id": "692e667137312eaa83fd8846", "name": "He Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd8847", "user": {"_id": "62b7fb545233925f253531c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg", "isPro": false, "fullname": "Terry Yue Zhuo", "user": "terryyz", "type": "user"}, "name": "Terry Yue Zhuo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T16:50:22.285Z", "hidden": false}, {"_id": "692e667137312eaa83fd8848", "name": "Kerui Cao", "hidden": false}, {"_id": "692e667137312eaa83fd8849", "name": "Xianfu Cheng", "hidden": false}, {"_id": "692e667137312eaa83fd884a", "name": "Jun Dong", "hidden": false}, {"_id": "692e667137312eaa83fd884b", "name": "Shengjie Fang", "hidden": false}, {"_id": "692e667137312eaa83fd884c", "name": "Zhiwei Fei", "hidden": false}, {"_id": "692e667137312eaa83fd884d", "name": "Xiangyuan Guan", "hidden": false}, {"_id": "692e667137312eaa83fd884e", "name": "Qipeng Guo", "hidden": false}, {"_id": "692e667137312eaa83fd884f", "name": "Zhiguang Han", "hidden": false}, {"_id": "692e667137312eaa83fd8850", "name": "Joseph James", "hidden": false}, {"_id": "692e667137312eaa83fd8851", "name": "Tianqi Luo", "hidden": false}, {"_id": "692e667137312eaa83fd8852", "user": {"_id": "67f1037cd5f976f3d4777390", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0cXH40AcE-M-H21cSNBqZ.png", "isPro": false, "fullname": "RenyuanLi", "user": "RenyuanLi", "type": "user"}, "name": "Renyuan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:17:45.344Z", "hidden": false}, {"_id": "692e667137312eaa83fd8853", "name": "Yuhang Li", "hidden": false}, {"_id": "692e667137312eaa83fd8854", "name": "Yiming Liang", "hidden": false}, {"_id": "692e667137312eaa83fd8855", "name": "Congnan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8856", "name": "Jiaheng Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8857", "name": "Qian Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8858", "name": "Ruitong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8859", "name": "Tyler Loakman", "hidden": false}, {"_id": "692e667137312eaa83fd885a", "name": "Xiangxin Meng", "hidden": false}, {"_id": "692e667137312eaa83fd885b", "name": "Chuang Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885c", "name": "Tianhao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885d", "name": "Jiajun Shi", "hidden": false}, {"_id": "692e667137312eaa83fd885e", "name": "Mingjie Tang", "hidden": false}, {"_id": "692e667137312eaa83fd885f", "name": "Boyang Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8860", "name": "Haowen Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8861", "name": "Yunli Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8862", "user": {"_id": "668619ce7374cac565759731", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668619ce7374cac565759731/tUtiyIQRGsMdq3HB2yYIL.jpeg", "isPro": false, "fullname": "Fanglin Xu", "user": "Tswatery", "type": "user"}, "name": "Fanglin Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T15:15:03.333Z", "hidden": false}, {"_id": "692e667137312eaa83fd8863", "name": "Zihan Xu", "hidden": false}, {"_id": "692e667137312eaa83fd8864", "name": "Fei Yuan", "hidden": false}, {"_id": "692e667137312eaa83fd8865", "user": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "name": "Ge Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:34.025Z", "hidden": false}, {"_id": "692e667137312eaa83fd8866", "user": {"_id": "65f40e83653c231cbaf7defe", "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg", "isPro": false, "fullname": "Jiayi Zhang", "user": "didiforhugface", "type": "user"}, "name": "Jiayi Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:32.149Z", "hidden": false}, {"_id": "692e667137312eaa83fd8867", "name": "Xinhao Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8868", "name": "Wangchunshu Zhou", "hidden": false}, {"_id": "692e667137312eaa83fd8869", "name": "Hualei Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886a", "name": "King Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886b", "name": "Brown Dai", "hidden": false}, {"_id": "692e667137312eaa83fd886c", "name": "Aishan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd886d", "name": "Zhoujun Li", "hidden": false}, {"_id": "692e667137312eaa83fd886e", "name": "Chenghua Lin", "hidden": false}, {"_id": "692e667137312eaa83fd886f", "name": "Tianyu Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8870", "name": "Chao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd8871", "name": "Kai Shen", "hidden": false}, {"_id": "692e667137312eaa83fd8872", "name": "Libo Qin", "hidden": false}, {"_id": "692e667137312eaa83fd8873", "name": "Shuangyong Song", "hidden": false}, {"_id": "692e667137312eaa83fd8874", "name": "Zizheng Zhan", "hidden": false}, {"_id": "692e667137312eaa83fd8875", "name": "Jiajun Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8876", "name": "Jie Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8877", "name": "Zhaoxiang Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8878", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-11-23T17:09:34.000Z", "submittedOnDailyAt": "2025-12-02T02:55:07.234Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "submittedOnDailyBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "upvotes": 240, "discussionId": "692e667237312eaa83fd8879", "ai_summary": "A comprehensive guide to code LLMs, covering their lifecycle from data curation to deployment, including techniques, trade-offs, and research-practice gaps.", "ai_keywords": ["Transformer-based architectures", "HumanEval", "prompting paradigms", "code pre-training", "supervised fine-tuning", "reinforcement learning", "autonomous coding agents", "GPT-4", "Claude", "LLaMA", "StarCoder", "Code LLaMA", "DeepSeek-Coder", "QwenCoder", "code correctness", "security", "contextual awareness", "software-related code tasks", "scaling law", "framework selection", "hyperparameter sensitivity", "model architectures", "dataset comparisons"], "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6539\u53d8\u4e86\u81ea\u52a8\u5316\u8f6f\u4ef6\u5f00\u53d1\uff0c\u80fd\u591f\u5c06\u81ea\u7136\u8bed\u8a00\u76f4\u63a5\u7ffb\u8bd1\u4e3a\u529f\u80fd\u4ee3\u7801\u3002</li>\n    <li>\u968f\u7740\u6280\u672f\u7684\u8fdb\u6b65\uff0cLLMs\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6210\u529f\u7387\u4ece\u4e2a\u4f4d\u6570\u63d0\u5347\u5230\u8d85\u8fc795%\u3002</li>\n    <li>\u672c\u6587\u63d0\u4f9b\u4e86\u5173\u4e8e\u4ee3\u7801LLMs\u7684\u5168\u9762\u6307\u5357\uff0c\u6db5\u76d6\u4ece\u6570\u636e\u6574\u7406\u5230\u8bad\u7ec3\u540e\u7684\u6a21\u578b\u751f\u547d\u5468\u671f\u3002</li>\n    <li>\u5206\u6790\u4e86\u901a\u7528LLMs\uff08\u5982GPT-4\uff09\u4e0e\u4e13\u95e8\u7528\u4e8e\u4ee3\u7801\u7684LLMs\uff08\u5982StarCoder\uff09\u7684\u80fd\u529b\u548c\u8bbe\u8ba1\u51b3\u7b56\u3002</li>\n    <li>\u63a2\u8ba8\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u8fdb\u884c\u4e86\u4e00\u7cfb\u5217\u5b9e\u9a8c\u5206\u6790\u4ee3\u7801\u9884\u8bad\u7ec3\u548c\u5f3a\u5316\u5b66\u4e60\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) help turn natural language descriptions into working code, improving software development tools like Github Copilot and others.</li>\n    <li>The models have greatly improved, with success rates on coding tasks rising from single digits to over 95% on benchmarks.</li>\n    <li>This work offers a detailed guide on how code LLMs work, covering everything from data gathering to training and testing methods.</li>\n    <li>It compares general LLMs (like GPT-4) to specialized code LLMs (like StarCoder) and discusses the choices made in their design and performance.</li>\n    <li>It also highlights the gap between academic research and practical applications in software development, addressing issues like code accuracy and integration into workflows.</li>\n</ul>"}, "publishedAt": "2025-11-23T12:09:34.000Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18538.png", "numComments": 11, "submittedBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "fullname": "Yang Jian", "name": "CSJianYang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 21}, "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.14993", "authors": [{"_id": "691e819a3c64d32b036458c0", "name": "Vladimir Arkhipkin", "hidden": false}, {"_id": "691e819a3c64d32b036458c1", "user": {"_id": "67bcb1012906865678a11f91", "avatarUrl": "/avatars/80fb0cc24f0d16c4740f9115b680df0f.svg", "isPro": false, "fullname": "Vladimir Korviakov", "user": "korviakov", "type": "user"}, "name": "Vladimir Korviakov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:03.925Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c2", "user": {"_id": "63cfa7ef3b7adfa99c0eb524", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674553277288-noauth.jpeg", "isPro": false, "fullname": "Nikolai Gerasimenko", "user": "nikgerasimenko", "type": "user"}, "name": "Nikolai Gerasimenko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:55.225Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c3", "name": "Denis Parkhomenko", "hidden": false}, {"_id": "691e819a3c64d32b036458c4", "user": {"_id": "64e4c7764af6c29a0697f57b", "avatarUrl": "/avatars/efc4e9f9b105586fd090b22a1bc7dbb7.svg", "isPro": false, "fullname": "Viacheslav Vasilev", "user": "vvasilev", "type": "user"}, "name": "Viacheslav Vasilev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:10.246Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c5", "user": {"_id": "68838d809080cc7010edf5e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68838d809080cc7010edf5e2/xBqg5ggt_PfLkiDLmsZxx.jpeg", "isPro": false, "fullname": "Alexey Letunovskiy", "user": "AlexeyLetunovskiy", "type": "user"}, "name": "Alexey Letunovskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:55.594Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c6", "user": {"_id": "678781c9e3c3c0163db4f99c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5Vi5J_XS9fbN2gDHfzHlh.png", "isPro": false, "fullname": "Kovaleva Maria", "user": "makovka2000", "type": "user"}, "name": "Maria Kovaleva", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T10:15:36.018Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c7", "user": {"_id": "67f38b14da604b256d393662", "avatarUrl": "/avatars/63445143f68995becc7702868387555b.svg", "isPro": false, "fullname": "Nikolay Vaulin", "user": "nvvaulin", "type": "user"}, "name": "Nikolai Vaulin", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:01.695Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c8", "user": {"_id": "62653f745f6f2e14d6ae128c", "avatarUrl": "/avatars/944b564ab810a5b31fa5e45f63bdf4ee.svg", "isPro": false, "fullname": "Ivan Kirillov", "user": "funnylittleman", "type": "user"}, "name": "Ivan Kirillov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T20:40:14.372Z", "hidden": false}, {"_id": "691e819a3c64d32b036458c9", "user": {"_id": "60991602f7c9c7bf29603a88", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60991602f7c9c7bf29603a88/me8VFG_06ZOovTLldF-L7.jpeg", "isPro": false, "fullname": "Lev Novitskiy", "user": "leffff", "type": "user"}, "name": "Lev Novitskiy", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:01:59.489Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ca", "name": "Denis Koposov", "hidden": false}, {"_id": "691e819a3c64d32b036458cb", "user": {"_id": "6628b73c35d27082500034f2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628b73c35d27082500034f2/CznOeIbjzJ9DmJaGzlWPD.jpeg", "isPro": false, "fullname": "Nikita Kiselev", "user": "kisnikser", "type": "user"}, "name": "Nikita Kiselev", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:11.927Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cc", "user": {"_id": "654d4993938fbf1e695b589a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qY3MC94Uz3FGf_HQtHseK.png", "isPro": false, "fullname": "Varlamov Alexander", "user": "Alphonsce", "type": "user"}, "name": "Alexander Varlamov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T09:02:08.889Z", "hidden": false}, {"_id": "691e819a3c64d32b036458cd", "user": {"_id": "6616719945336ca7746eaa38", "avatarUrl": "/avatars/ac77ebda8507d75376973144263beb83.svg", "isPro": false, "fullname": "Dmitrii Mikhailov", "user": "Botsman11", "type": "user"}, "name": "Dmitrii Mikhailov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-24T07:58:56.980Z", "hidden": false}, {"_id": "691e819a3c64d32b036458ce", "name": "Vladimir Polovnikov", "hidden": false}, {"_id": "691e819a3c64d32b036458cf", "name": "Andrey Shutkin", "hidden": false}, {"_id": "691e819a3c64d32b036458d0", "name": "Ilya Vasiliev", "hidden": false}, {"_id": "691e819a3c64d32b036458d1", "name": "Julia Agafonova", "hidden": false}, {"_id": "691e819a3c64d32b036458d2", "name": "Anastasiia Kargapoltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d3", "user": {"_id": "65df46ac43bf08064bd8e656", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65df46ac43bf08064bd8e656/yR72X3fnBhdy_i34VqBxT.jpeg", "isPro": false, "fullname": "Anna Dmitrienko", "user": "dmitrienkoae", "type": "user"}, "name": "Anna Dmitrienko", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:09.131Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d4", "name": "Anastasia Maltseva", "hidden": false}, {"_id": "691e819a3c64d32b036458d5", "user": {"_id": "66f1a9c87ce3d2d3938999ce", "avatarUrl": "/avatars/3016b15d4bae2591313537a4ea59b268.svg", "isPro": false, "fullname": "Anna Averchenkova", "user": "aaveraa", "type": "user"}, "name": "Anna Averchenkova", "status": "claimed_verified", "statusLastChangedAt": "2025-11-21T16:49:11.123Z", "hidden": false}, {"_id": "691e819a3c64d32b036458d6", "name": "Olga Kim", "hidden": false}, {"_id": "691e819a3c64d32b036458d7", "name": "Tatiana Nikulina", "hidden": false}, {"_id": "691e819a3c64d32b036458d8", "user": {"_id": "6669a678465d1d802181e456", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6669a678465d1d802181e456/ZCthBBhDFQnh0bBkgUQUU.png", "isPro": false, "fullname": "Denis Dimitrov", "user": "dendimitrov", "type": "user"}, "name": "Denis Dimitrov", "status": "claimed_verified", "statusLastChangedAt": "2025-11-20T08:49:08.661Z", "hidden": false}], "publishedAt": "2025-11-19T00:23:22.000Z", "submittedOnDailyAt": "2025-11-20T00:19:10.078Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "upvotes": 209, "discussionId": "691e819b3c64d32b036458d9", "projectPage": "https://kandinskylab.ai/", "githubRepo": "https://github.com/kandinskylab/kandinsky-5", "ai_summary": "Kandinsky 5.0 is a family of state-of-the-art generative models for high-resolution images and short videos, featuring model lineups with varying parameters and enhanced training techniques to achieve superior quality and performance.", "ai_keywords": ["foundation models", "high-resolution image synthesis", "10-second video synthesis", "image generation models", "text-to-video models", "image-to-video models", "multi-stage training pipeline", "self-supervised fine-tuning", "reinforcement learning", "pre-training", "quality-enhancement techniques", "architectural optimizations", "training optimizations", "inference optimizations", "human evaluation", "generative framework", "open-source code", "training checkpoints"], "githubStars": 477, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecdKandinsky 5.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c10\u79d2\u89c6\u9891\u5408\u6210\u7684\u5148\u8fdb\u57fa\u7840\u6a21\u578b\u7cfb\u5217\u3002</li>\n    <li>Kandinsky 5.0\u5305\u62ec\u4e09\u79cd\u6838\u5fc3\u6a21\u578b\uff1a6B\u53c2\u6570\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\u30012B\u53c2\u6570\u7684\u5feb\u901f\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u548c19B\u53c2\u6570\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002</li>\n    <li>\u8be6\u7ec6\u8bf4\u660e\u4e86\u6570\u636e\u6574\u7406\u751f\u547d\u5468\u671f\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u5904\u7406\u3001\u8fc7\u6ee4\u548c\u805a\u7c7b\uff0c\u4ee5\u652f\u6301\u591a\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>\u5c55\u793a\u4e86\u65b0\u9896\u7684\u67b6\u6784\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u4f18\u5316\uff0c\u4f7fKandinsky 5.0\u5728\u751f\u6210\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002</li>\n    <li>\u5e0c\u671b\u901a\u8fc7\u53d1\u5e03\u5f00\u6e90\u4ee3\u7801\u548c\u8bad\u7ec3\u68c0\u67e5\u70b9\uff0c\u63a8\u52a8\u9ad8\u8d28\u91cf\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u548c\u4fbf\u5229\u6027\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>Kandinsky 5.0 is a new set of advanced models for creating high-quality images and short videos.</li>\n    <li>It includes three main models: Kandinsky 5.0 Image Lite for image generation, Kandinsky 5.0 Video Lite for quick video creation, and Kandinsky 5.0 Video Pro for top-quality video generation.</li>\n    <li>The report details the process of gathering and preparing data for training these models, including various techniques to improve quality.</li>\n    <li>Kandinsky 5.0 features improvements in its design and training methods, allowing it to generate high-quality content quickly.</li>\n    <li>The models are open-source, making them accessible for researchers and developers to use in different creative projects.</li>\n</ul>"}, "publishedAt": "2025-11-18T19:23:22.000Z", "title": "Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation", "summary": "This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.14993.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "isAuthorParticipating": true}, {"paper": {"id": "2511.08892", "authors": [{"_id": "69154dffa1b06ca3cc81351e", "name": "Weihao Tan", "hidden": false}, {"_id": "69154dffa1b06ca3cc81351f", "name": "Xiangyang Li", "hidden": false}, {"_id": "69154dffa1b06ca3cc813520", "name": "Yunhao Fang", "hidden": false}, {"_id": "69154dffa1b06ca3cc813521", "name": "Heyuan Yao", "hidden": false}, {"_id": "69154dffa1b06ca3cc813522", "name": "Shi Yan", "hidden": false}, {"_id": "69154dffa1b06ca3cc813523", "name": "Hao Luo", "hidden": false}, {"_id": "69154dffa1b06ca3cc813524", "name": "Tenglong Ao", "hidden": false}, {"_id": "69154dffa1b06ca3cc813525", "name": "Huihui Li", "hidden": false}, {"_id": "69154dffa1b06ca3cc813526", "name": "Hongbin Ren", "hidden": false}, {"_id": "69154dffa1b06ca3cc813527", "user": {"_id": "6369d92f64aad59d4d44d362", "avatarUrl": "/avatars/73956400cfbfd53116aefc17b3c9f0fd.svg", "isPro": false, "fullname": "Yi", "user": "Bairen", "type": "user"}, "name": "Bairen Yi", "status": "claimed_verified", "statusLastChangedAt": "2025-11-17T10:31:49.043Z", "hidden": false}, {"_id": "69154dffa1b06ca3cc813528", "name": "Yujia Qin", "hidden": false}, {"_id": "69154dffa1b06ca3cc813529", "name": "Bo An", "hidden": false}, {"_id": "69154dffa1b06ca3cc81352a", "name": "Libin Liu", "hidden": false}, {"_id": "69154dffa1b06ca3cc81352b", "name": "Guang Shi", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/FVxpP05KrXQ1HkQ1G1uNl.mp4"], "publishedAt": "2025-11-12T02:01:26.000Z", "submittedOnDailyAt": "2025-11-13T00:49:21.639Z", "title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.", "upvotes": 185, "discussionId": "69154dffa1b06ca3cc81352c", "projectPage": "https://www.lumine-ai.org/", "ai_summary": "Lumine, a vision-language model-based agent, completes complex missions in real-time across different 3D open-world environments with human-like efficiency and zero-shot cross-game generalization.", "ai_keywords": ["vision-language model", "end-to-end", "3D open-world environments", "human-like interaction", "real-time", "zero-shot cross-game generalization"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>Lumine\u662f\u7b2c\u4e00\u4e2a\u5f00\u653e\u7684\u901a\u7528\u667a\u80fd\u4f53\u5f00\u53d1\u914d\u65b9\uff0c\u80fd\u591f\u5728\u590d\u6742\u76843D\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u5b9e\u65f6\u5b8c\u6210\u957f\u8fbe\u6570\u5c0f\u65f6\u7684\u4efb\u52a1\u3002</li>\n    <li>\u5b83\u91c7\u7528\u7c7b\u4f3c\u4eba\u7c7b\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u5c06\u611f\u77e5\u3001\u63a8\u7406\u548c\u884c\u52a8\u7edf\u4e00\u5728\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u7cfb\u7edf\u4e2d\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u3002</li>\n    <li>Lumine\u5728\u300a\u539f\u795e\u300b\u4e2d\u8bad\u7ec3\uff0c\u80fd\u591f\u4e0e\u4eba\u7c7b\u6548\u7387\u76f8\u5f53\u5730\u5b8c\u6210\u4e94\u5c0f\u65f6\u7684\u4e3b\u7ebf\u4efb\u52a1\uff0c\u5e76\u6839\u636e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6267\u884c\u5404\u79cd\u4efb\u52a1\u3002</li>\n    <li>\u5b83\u4e0d\u4ec5\u5728\u7279\u5b9a\u6e38\u620f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8fd8\u80fd\u591f\u5728\u6ca1\u6709\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5176\u4ed6\u6e38\u620f\u4e2d\u5b8c\u6210\u957f\u8fbe100\u5206\u949f\u7684\u4efb\u52a1\u3002</li>\n    <li>Lumine\u7684\u6210\u529f\u5c55\u793a\u4e86\u5728\u5f00\u653e\u73af\u5883\u4e2d\u5f00\u53d1\u901a\u7528\u667a\u80fd\u4f53\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Lumine is a new open recipe for creating generalist agents that can handle long, complex tasks in 3D open-world games.</li>\n    <li>It uses a human-like approach to combine sensing, thinking, and acting, using a vision-language model.</li>\n    <li>Lumine processes visual information quickly and efficiently to perform actions based on natural language commands.</li>\n    <li>It can complete the five-hour main storyline of Genshin Impact at a level similar to human players and follow various instructions across different tasks.</li>\n    <li>Lumine shows strong performance in other games without needing extra training, successfully completing missions in Wuthering Waves and Honkai: Star Rail.</li>\n</ul>"}, "publishedAt": "2025-11-11T21:01:26.000Z", "title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds", "summary": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/FVxpP05KrXQ1HkQ1G1uNl.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.08892.png", "numComments": 12, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51faDeepSeek-V3.2\u6a21\u578b\uff0c\u517c\u5177\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u5353\u8d8a\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5f15\u5165DeepSeek\u7a00\u758f\u6ce8\u610f\u673a\u5236\uff08DSA\uff09\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u60c5\u51b5\u4e0b\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u901a\u8fc7\u5f3a\u5927\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u548c\u540e\u671f\u8ba1\u7b97\u6269\u5c55\uff0cDeepSeek-V3.2\u7684\u8868\u73b0\u4e0eGPT-5\u76f8\u5f53\uff0c\u4e14\u9ad8\u8ba1\u7b97\u7248\u672c\u8d85\u8d8aGPT-5\uff0c\u63a8\u7406\u80fd\u529b\u4e0eGemini-3.0-Pro\u76f8\u5f53\u3002</li>\n    <li>\u5f00\u53d1\u5927\u89c4\u6a21\u667a\u80fd\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff0c\u7cfb\u7edf\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u5728\u590d\u6742\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9075\u5faa\u6307\u4ee4\u7684\u9c81\u68d2\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that combines high efficiency with great reasoning abilities and performance.</li>\n    <li>It features DeepSeek Sparse Attention (DSA), which makes it easier to manage long contexts without losing performance.</li>\n    <li>The model uses a strong reinforcement learning system, allowing it to perform similarly to GPT-5, and even outperform it in some cases.</li>\n    <li>DeepSeek-V3.2 has achieved top results in prestigious competitions like the International Mathematical Olympiad and the International Olympiad in Informatics.</li>\n    <li>It includes a new pipeline for creating large amounts of training data, improving its ability to follow instructions and handle complex tasks.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.22699", "authors": [{"_id": "692d06234397b1ec214f6788", "name": "Z-Image Team", "hidden": false}, {"_id": "692d06234397b1ec214f6789", "user": {"_id": "692d0e6bb14ceb758205d0dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/692d0e6bb14ceb758205d0dd/gGVq2KSJE11Sr3LkVn-n5.jpeg", "isPro": false, "fullname": "Huanqia Cai", "user": "Orion-Cai", "type": "user"}, "name": "Huanqia Cai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:26.669Z", "hidden": false}, {"_id": "692d06234397b1ec214f678a", "user": {"_id": "67777b7a8376dfe003afa951", "avatarUrl": "/avatars/2af9d3181306d4c53329d047eeadaf1e.svg", "isPro": false, "fullname": "Sihan Cao", "user": "Sihan-Cao", "type": "user"}, "name": "Sihan Cao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:33.191Z", "hidden": false}, {"_id": "692d06234397b1ec214f678b", "user": {"_id": "64a54586c0f13de8e7093314", "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg", "isPro": false, "fullname": "Ruoyi Du", "user": "RuoyiDu", "type": "user"}, "name": "Ruoyi Du", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:18:53.948Z", "hidden": false}, {"_id": "692d06234397b1ec214f678c", "name": "Peng Gao", "hidden": false}, {"_id": "692d06234397b1ec214f678d", "name": "Steven Hoi", "hidden": false}, {"_id": "692d06234397b1ec214f678e", "name": "Shijie Huang", "hidden": false}, {"_id": "692d06234397b1ec214f678f", "name": "Zhaohui Hou", "hidden": false}, {"_id": "692d06234397b1ec214f6790", "user": {"_id": "662a0f2d4bab737c1a279843", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662a0f2d4bab737c1a279843/fC2p3mjMHkVpDQdEqkuR4.png", "isPro": false, "fullname": "Dengyang Jiang", "user": "DyJiang", "type": "user"}, "name": "Dengyang Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:15.555Z", "hidden": false}, {"_id": "692d06234397b1ec214f6791", "user": {"_id": "6537e8eab01250d1d6efed3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gMx73gwdfEhcCFioStGCE.jpeg", "isPro": false, "fullname": "Xin", "user": "Srameo", "type": "user"}, "name": "Xin Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:15.288Z", "hidden": false}, {"_id": "692d06234397b1ec214f6792", "name": "Liangchen Li", "hidden": false}, {"_id": "692d06234397b1ec214f6793", "user": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "name": "Zhen Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:16.899Z", "hidden": false}, {"_id": "692d06234397b1ec214f6794", "user": {"_id": "6740a5730bb4a675446a80ad", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740a5730bb4a675446a80ad/dmruwMdQK3zluJm7YXUtN.jpeg", "isPro": false, "fullname": "Zhong-Yu Li", "user": "lzyhha", "type": "user"}, "name": "Zhong-Yu Li", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:08.972Z", "hidden": false}, {"_id": "692d06234397b1ec214f6795", "name": "David Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6796", "name": "Dongyang Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6797", "user": {"_id": "66332475351231c428653b6b", "avatarUrl": "/avatars/3997bcde54158f7ff9770c85a20875f1.svg", "isPro": false, "fullname": "Junhan Shi", "user": "jshmsjh", "type": "user"}, "name": "Junhan Shi", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:38.865Z", "hidden": false}, {"_id": "692d06234397b1ec214f6798", "user": {"_id": "64379d79fac5ea753f1c10f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png", "isPro": false, "fullname": "Jerry Wu", "user": "QJerry", "type": "user"}, "name": "Qilong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:18.709Z", "hidden": false}, {"_id": "692d06234397b1ec214f6799", "name": "Feng Yu", "hidden": false}, {"_id": "692d06234397b1ec214f679a", "name": "Chi Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679b", "name": "Shifeng Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679c", "user": {"_id": "641988978e0baaeed5a066c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641988978e0baaeed5a066c6/TdCjJ63gw5gdX1RqTvy9a.png", "isPro": false, "fullname": "Shilin", "user": "zsLin", "type": "user"}, "name": "Shilin Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T16:24:44.624Z", "hidden": false}], "publishedAt": "2025-11-27T18:52:07.000Z", "submittedOnDailyAt": "2025-12-01T00:38:17.269Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "submittedOnDailyBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "upvotes": 155, "discussionId": "692d06234397b1ec214f679d", "projectPage": "https://tongyi-mai.github.io/Z-Image-blog/", "githubRepo": "https://github.com/Tongyi-MAI/Z-Image", "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.", "ai_keywords": ["Scalable Single-Stream Diffusion Transformer", "S3-DiT", "diffusion transformer", "omni-pre-training", "instruction-following capabilities", "photorealistic image generation", "bilingual text rendering", "distillation scheme", "reward post-training", "H800 GPU", "VRAM"], "githubStars": 5595, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "summary_zh": "<ul>\n    <li>\u76ee\u524d\u9ad8\u6027\u80fd\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e3b\u8981\u7531\u4e13\u6709\u7cfb\u7edf\u4e3b\u5bfc\uff0c\u6bd4\u5982Nano Banana Pro\u548cSeedream 4.0\u3002</li>\n    <li>\u5f00\u6e90\u66ff\u4ee3\u54c1\u5982Qwen-Image\u548cHunyuan-Image-3.0\u53c2\u6570\u91cf\u5de8\u5927\uff0820B\u523080B\uff09\uff0c\u4e0d\u9002\u5408\u666e\u901a\u786c\u4ef6\u8fdb\u884c\u63a8\u7406\u548c\u5fae\u8c03\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Z-Image\uff0c\u8fd9\u662f\u4e00\u4e2a\u9ad8\u6548\u76846B\u53c2\u6570\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5355\u6d41\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\uff08S3-DiT\uff09\u3002</li>\n    <li>Z-Image\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f18\u5316\u4e86\u6570\u636e\u57fa\u7840\u8bbe\u65bd\u548c\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u5b8c\u6210\u6574\u4e2a\u8bad\u7ec3\u6d41\u7a0b\u53ea\u9700314K H800 GPU\u5c0f\u65f6\uff08\u7ea6630K\u7f8e\u5143\uff09\u3002</li>\n    <li>Z-Image\u5728\u7167\u7247\u7ea7\u771f\u5b9e\u56fe\u50cf\u751f\u6210\u548c\u53cc\u8bed\u6587\u672c\u6e32\u67d3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u4e0e\u9876\u7ea7\u5546\u4e1a\u6a21\u578b\u5ab2\u7f8e\uff0c\u6211\u4eec\u516c\u5f00\u53d1\u5e03\u4e86\u4ee3\u7801\u548c\u5728\u7ebf\u6f14\u793a\uff0c\u4fc3\u8fdb\u66f4\u5177\u53ef\u8bbf\u95ee\u6027\u548c\u9884\u7b97\u53cb\u597d\u7684\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current image generation models are mostly proprietary, with leading open-source options being large and hard to use on regular hardware.</li>\n    <li>Z-Image is a new 6B-parameter model designed to be efficient and easy to run on consumer-grade hardware.</li>\n    <li>It was trained quickly and cost-effectively, taking only 314K GPU hours and approximately $630K.</li>\n    <li>Z-Image offers fast performance, with less than one-second response times, and can run on systems with less than 16GB of VRAM.</li>\n    <li>The model performs well in photorealistic image generation and bilingual text rendering, competing with high-end commercial models.</li>\n</ul>"}, "publishedAt": "2025-11-27T13:52:07.000Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png", "numComments": 3, "submittedBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "fullname": "Zhen Li", "name": "Paper99", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 29}, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20626", "authors": [{"_id": "6927ab26243b2216fb75cd1b", "name": "Wei He", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1c", "user": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "name": "Kai Han", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:11.052Z", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1d", "name": "Hang Zhou", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1e", "name": "Hanting Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1f", "name": "Zhicheng Liu", "hidden": false}, {"_id": "6927ab26243b2216fb75cd20", "name": "Xinghao Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd21", "name": "Yunhe Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "publishedAt": "2025-11-25T18:48:05.000Z", "submittedOnDailyAt": "2025-11-26T23:08:13.066Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "submittedOnDailyBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "upvotes": 154, "discussionId": "6927ab27243b2216fb75cd22", "projectPage": "https://github.com/huawei-noah/noah-research/tree/master/ROOT", "githubRepo": "https://github.com/huawei-noah/noah-research", "ai_summary": "ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.", "ai_keywords": ["large language models", "LLMs", "momentum orthogonalization", "dimensional fragility", "outlier-induced noise", "adaptive Newton iterations", "proximal optimization", "Muon", "Adam-based optimizers", "robust optimizer"], "githubStars": 909, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u52a0\u5267\u4e86\u7b97\u6cd5\u4e0d\u7cbe\u786e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668ROOT\uff0c\u65e8\u5728\u901a\u8fc7\u53cc\u91cd\u7a33\u5065\u673a\u5236\u6765\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>ROOT\u91c7\u7528\u81ea\u9002\u5e94\u725b\u987f\u8fed\u4ee3\u7684\u7ef4\u5ea6\u7a33\u5065\u6b63\u4ea4\u5316\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4e0d\u540c\u67b6\u6784\u914d\u7f6e\u4e0b\u7684\u7cbe\u786e\u5ea6\u3002</li>\n    <li>ROOT\u8fd8\u901a\u8fc7\u90bb\u8fd1\u4f18\u5316\u6846\u67b6\u6291\u5236\u5f02\u5e38\u503c\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u610f\u4e49\u7684\u68af\u5ea6\u65b9\u5411\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cROOT\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e2d\u6bd4Muon\u548c\u57fa\u4e8eAdam\u7684\u4f18\u5316\u5668\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) face challenges with training stability and precision as they grow in size.</li>\n    <li>Recent optimizers have improved efficiency but struggle with robustness against noise and dimensional changes.</li>\n    <li>ROOT is a new optimizer that strengthens training stability using two innovative methods: dimension-robust orthogonalization and noise suppression.</li>\n    <li>Experiments show ROOT outperforms other optimizers like Muon and Adam in tough conditions, achieving faster training and better results.</li>\n    <li>The ROOT code will be available online for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:48:05.000Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20626.png", "numComments": 2, "submittedBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "fullname": "hankai", "name": "hankaixyz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.11793", "authors": [{"_id": "691be81b6bfd5965c0fd37e2", "name": "MiroMind Team", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e3", "name": "Song Bai", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e4", "name": "Lidong Bing", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e5", "name": "Carson Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e6", "name": "Guanzheng Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e7", "user": {"_id": "632dab84fdb35759ea6646a0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632dab84fdb35759ea6646a0/IxO5mbtzHJsr0YHW-YtVk.jpeg", "isPro": false, "fullname": "Yuntao Chen", "user": "YuntaoChen", "type": "user"}, "name": "Yuntao Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:45.403Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e8", "name": "Zhe Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37e9", "name": "Ziyi Chen", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ea", "name": "Jifeng Dai", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37eb", "name": "Xuan Dong", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ec", "name": "Yue Deng", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ed", "name": "Yunjie Fu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ee", "name": "Junqi Ge", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ef", "name": "Chenxia Han", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f0", "name": "Tammy Huang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f1", "name": "Zhenhang Huang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f2", "name": "Jerry Jiao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f3", "name": "Shilei Jiang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f4", "name": "Tianyu Jiao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f5", "user": {"_id": "64be2455b567ae97c34bb948", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64be2455b567ae97c34bb948/QuCdStDDGaXjDmp4V-dBj.jpeg", "isPro": false, "fullname": "Xiaoqi Jian", "user": "mx1024", "type": "user"}, "name": "Xiaoqi Jian", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:52.417Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f6", "name": "Lei Lei", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f7", "user": {"_id": "6466e7be1343dce20e59191b", "avatarUrl": "/avatars/6779560b203c3773dc76372c0b8cbe4e.svg", "isPro": false, "fullname": "Li Ruilin", "user": "Eric-LRL-130", "type": "user"}, "name": "Ruilin Li", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:43.774Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f8", "name": "Ryan Luo", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37f9", "name": "Tiantong Li", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fa", "name": "Xiang Lin", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fb", "name": "Ziyuan Liu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fc", "name": "Zhiqi Li", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fd", "name": "Jie Ni", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37fe", "name": "Qiang Ren", "hidden": false}, {"_id": "691be81b6bfd5965c0fd37ff", "name": "Pax Sun", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3800", "name": "Shiqian Su", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3801", "name": "Chenxin Tao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3802", "name": "Bin Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3803", "name": "Hellen Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3804", "name": "Haonan Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3805", "name": "James Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3806", "name": "Jin Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3807", "name": "Jojo Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3808", "name": "Letian Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3809", "name": "Shizun Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380a", "user": {"_id": "63d34004b734eaa4d4faeccf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d34004b734eaa4d4faeccf/zf6d1p0GN8gsagi8N6y4V.jpeg", "isPro": false, "fullname": "Weizhi Wang", "user": "weizhiwang", "type": "user"}, "name": "Weizhi Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:47.000Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380b", "name": "Zixuan Wang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380c", "name": "Jinfan Xu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380d", "name": "Sen Xing", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380e", "user": {"_id": "637f347a52229c639211bee8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f347a52229c639211bee8/I9_PET-_6SJQJ6hXrACV4.jpeg", "isPro": false, "fullname": "Chenyu Yang", "user": "cyyang822", "type": "user"}, "name": "Chenyu Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:48.746Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd380f", "user": {"_id": "6239888e7fef05b7bdd5fcff", "avatarUrl": "/avatars/54fcc756b8c0936b6bb410c6e0e02d75.svg", "isPro": false, "fullname": "Hai Ye", "user": "oceanpty", "type": "user"}, "name": "Hai Ye", "status": "claimed_verified", "statusLastChangedAt": "2025-11-19T08:40:50.623Z", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3810", "name": "Jiaheng Yu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3811", "name": "Yue Yu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3812", "name": "Muyan Zhong", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3813", "name": "Tianchen Zhao", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3814", "name": "Xizhou Zhu", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3815", "name": "Yanpeng Zhou", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3816", "name": "Yifan Zhang", "hidden": false}, {"_id": "691be81b6bfd5965c0fd3817", "name": "Zhi Zhu", "hidden": false}], "publishedAt": "2025-11-14T18:52:07.000Z", "submittedOnDailyAt": "2025-11-18T02:00:07.077Z", "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.", "upvotes": 153, "discussionId": "691be81b6bfd5965c0fd3818", "projectPage": "https://dr.miromind.ai/", "githubRepo": "https://github.com/MiroMindAI/MiroThinker", "githubStars": 1133, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86MiroThinker v1.0\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u7814\u7a76\u4ee3\u7406\uff0c\u65e8\u5728\u589e\u5f3a\u5de5\u5177\u8f85\u52a9\u63a8\u7406\u548c\u4fe1\u606f\u83b7\u53d6\u80fd\u529b\u3002</li>\n    <li>MiroThinker\u901a\u8fc7\u5728\u6a21\u578b\u5c42\u9762\u63a2\u7d22\u4ea4\u4e92\u6269\u5c55\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u4ee3\u7406\u4e0e\u73af\u5883\u4e4b\u95f4\u7684\u4e92\u52a8\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u7684\u4ea4\u4e92\u6269\u5c55\uff0c\u652f\u6301\u591a\u8fbe600\u6b21\u5de5\u5177\u8c03\u7528\uff0c\u9002\u5e94\u590d\u6742\u7684\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>MiroThinker\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u8d85\u8fc7\u4e86\u4ee5\u5f80\u7684\u5f00\u6e90\u4ee3\u7406\uff0c\u63a5\u8fd1\u5546\u4e1a\u6a21\u578b\u7684\u6c34\u5e73\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u4ea4\u4e92\u6df1\u5ea6\u7684\u6269\u5c55\u4e0e\u6a21\u578b\u7684\u89c4\u6a21\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u5177\u6709\u76f8\u4f3c\u7684\u6269\u5c55\u7279\u6027\uff0c\u662f\u6784\u5efa\u4e0b\u4e00\u4ee3\u5f00\u653e\u7814\u7a76\u4ee3\u7406\u7684\u91cd\u8981\u7ef4\u5ea6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MiroThinker v1.0 is an open-source research agent aimed at improving reasoning and information-seeking skills.</li>\n    <li>It focuses on enhancing how the model interacts with its environment, rather than just increasing model size or context length.</li>\n    <li>The model uses reinforcement learning to effectively manage many tool calls (up to 600) during tasks, supporting complex research activities.</li>\n    <li>MiroThinker outperforms previous open-source agents in various benchmarks, showing accuracy rates that approach those of advanced commercial models.</li>\n    <li>The research highlights that better performance comes from deeper and more frequent interactions with the environment, suggesting that this interaction is crucial for future research agent development.</li>\n</ul>"}, "publishedAt": "2025-11-14T13:52:07.000Z", "title": "MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling", "summary": "We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.11793.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 171}, "isAuthorParticipating": false}, {"paper": {"id": "2511.20785", "authors": [{"_id": "692d430f4397b1ec214f696e", "user": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "name": "Zuhao Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:11.311Z", "hidden": false}, {"_id": "692d430f4397b1ec214f696f", "user": {"_id": "6690f58e2f9f6f9c88e91031", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6690f58e2f9f6f9c88e91031/QQ_VoEh7NlE6BUvii08zk.png", "isPro": false, "fullname": "Sudong Wang", "user": "xiao45791", "type": "user"}, "name": "Sudong Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:14.173Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6970", "user": {"_id": "64bb77e786e7fb5b8a317a43", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bb77e786e7fb5b8a317a43/J0jOrlZJ9gazdYaeSH2Bo.png", "isPro": false, "fullname": "kcz", "user": "kcz358", "type": "user"}, "name": "Kaichen Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T10:06:41.343Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6971", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:09.461Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6972", "name": "Sicong Leng", "hidden": false}, {"_id": "692d430f4397b1ec214f6973", "name": "Yifan Zhang", "hidden": false}, {"_id": "692d430f4397b1ec214f6974", "name": "Chengwei Qin", "hidden": false}, {"_id": "692d430f4397b1ec214f6975", "name": "Shijian Lu", "hidden": false}, {"_id": "692d430f4397b1ec214f6976", "name": "Xingxuan Li", "hidden": false}, {"_id": "692d430f4397b1ec214f6977", "user": {"_id": "6454685a548f22be598414c4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg", "isPro": false, "fullname": "Lidong Bing", "user": "LidongBing", "type": "user"}, "name": "Lidong Bing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:49:36.056Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "publishedAt": "2025-11-25T19:22:48.000Z", "submittedOnDailyAt": "2025-12-02T00:35:56.511Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "submittedOnDailyBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "upvotes": 148, "discussionId": "692d430f4397b1ec214f6978", "projectPage": "https://evolvinglmms-lab.github.io/LongVT/", "githubRepo": "https://github.com/EvolvingLMMs-Lab/LongVT", "ai_summary": "LongVT, an end-to-end framework, enhances long video reasoning by interleaving global and local analysis using multimodal tools, outperforming existing methods on challenging benchmarks.", "ai_keywords": ["multimodal models", "video reasoning", "textual Chain-of-Thought", "hallucinations", "long-form videos", "temporal grounding", "video cropping", "fine-grained question-answering", "VideoSIAH", "tool-integrated cold-start supervised fine-tuning", "agentic reinforcement learning", "agentic reinforcement fine-tuning"], "githubStars": 121, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86LongVT\u6846\u67b6\uff0c\u65e8\u5728\u6539\u5584\u957f\u89c6\u9891\u63a8\u7406\u80fd\u529b\uff0c\u501f\u9274\u4eba\u7c7b\u89c2\u770b\u89c6\u9891\u7684\u65b9\u5f0f\u3002</li>\n    <li>\u6846\u67b6\u5229\u7528\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\u7684\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\uff0c\u805a\u7126\u4e8e\u7279\u5b9a\u89c6\u9891\u7247\u6bb5\u8fdb\u884c\u7ec6\u8282\u5206\u6790\u3002</li>\n    <li>\u521b\u5efa\u5e76\u5c06\u53d1\u5e03\u540d\u4e3aVideoSIAH\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u957f\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002</li>\n    <li>\u8bad\u7ec3\u6570\u636e\u96c6\u5305\u542b\u5927\u91cf\u6837\u672c\uff0c\u7528\u4e8e\u51b7\u542f\u52a8\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u3002</li>\n    <li>LongVT\u5728\u56db\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u5f3a\u57fa\u7ebf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongVT is a new framework designed to improve video reasoning using large multimodal models (LMMs).</li>\n    <li>The framework mimics human video understanding by first skimming the whole video, then focusing on specific clips for details.</li>\n    <li>LongVT uses LMMs' ability to identify important video segments and refine video frames for better analysis.</li>\n    <li>A new dataset called VideoSIAH will be released to support training and evaluating the model, containing a large number of samples for various tasks.</li>\n    <li>LongVT has been tested and shown to perform better than existing methods on several challenging benchmarks for long video understanding.</li>\n</ul>"}, "publishedAt": "2025-11-25T14:22:48.000Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20785.png", "numComments": 3, "submittedBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "fullname": "Zuhao Yang", "name": "mwxely", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n  <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u5230\u987a\u5e8f\u8ba1\u7b97\u548c\u957f\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u9650\u5236\uff0c\u96be\u4ee5\u7528\u4e8e\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u5f62\u8c61\u5408\u6210\u3002</li>\n  <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u5b9e\u65f6\u865a\u62df\u5f62\u8c61\u201d\u7cfb\u7edf\uff0c\u5229\u7528\u4e00\u4e2a140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u6e05\u548c\u65e0\u9650\u957f\u5ea6\u7684\u865a\u62df\u5f62\u8c61\u751f\u6210\u3002</li>\n  <li>\u91c7\u7528\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u5e76\u884c\uff08TPP\uff09\u6280\u672f\uff0c\u80fd\u591f\u5728\u591a\u4e2aGPU\u4e0a\u5e76\u884c\u5904\u7406\u53bb\u566a\u6b65\u9aa4\uff0c\u63d0\u9ad8\u5b9e\u65f6\u6d41\u5a92\u4f53\u7684\u7a33\u5b9a\u6027\u548c\u4f4e\u5ef6\u8fdf\u3002</li>\n  <li>\u5f15\u5165\u4e86\u6eda\u52a8\u6c89\u6d78\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5916\u89c2\u6765\u7ef4\u62a4\u5e8f\u5217\u7684\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u8eab\u4efd\u6f02\u79fb\u548c\u989c\u8272\u4f2a\u5f71\u3002</li>\n  <li>\u6211\u4eec\u7684\u7cfb\u7edf\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e86\u6bcf\u79d220\u5e27\u7684\u751f\u6210\u901f\u5ea6\uff0c\u9996\u6b21\u5728\u6b64\u89c4\u6a21\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u5b9e\u65f6\u865a\u62df\u5f62\u8c61\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new system for creating high-quality, real-time avatars using a powerful diffusion model with 14 billion parameters.</li>\n    <li>The system uses a method called Timestep-forcing Pipeline Parallelism to speed up processing by sharing the workload across multiple GPUs.</li>\n    <li>To improve consistency in the generated avatars, it incorporates a technique called Rolling Sink Frame Mechanism that adjusts the appearance based on a reference image.</li>\n    <li>It also uses Self-Forcing Distribution Matching Distillation to adapt large models for streaming without losing visual quality.</li>\n    <li>Live Avatar achieves impressive performance, generating 20 frames per second on five GPUs, setting a new standard for real-time avatar generation.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2511.18423", "authors": [{"_id": "692518ff16eb3a9f1310391c", "name": "B. Y. Yan", "hidden": false}, {"_id": "692518ff16eb3a9f1310391d", "name": "Chaofan Li", "hidden": false}, {"_id": "692518ff16eb3a9f1310391e", "name": "Hongjin Qian", "hidden": false}, {"_id": "692518ff16eb3a9f1310391f", "user": {"_id": "6145b3fd35135ec7e8d4ca45", "avatarUrl": "/avatars/5dc25d18d6a8418c9b1a29ece9a48f5a.svg", "isPro": false, "fullname": "Shuqi Lu", "user": "shuqi", "type": "user"}, "name": "Shuqi Lu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:18:11.163Z", "hidden": false}, {"_id": "692518ff16eb3a9f13103920", "user": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "name": "Zheng Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:17:59.618Z", "hidden": false}], "publishedAt": "2025-11-23T12:29:33.000Z", "submittedOnDailyAt": "2025-11-25T00:25:04.757Z", "title": "General Agentic Memory Via Deep Research", "submittedOnDailyBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "upvotes": 140, "discussionId": "692518ff16eb3a9f13103921", "projectPage": "https://github.com/VectorSpaceLab/general-agentic-memory", "githubRepo": "https://github.com/VectorSpaceLab/general-agentic-memory", "ai_summary": "GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.", "ai_keywords": ["general agentic memory", "GAM", "just-in time compilation", "JIT compilation", "memorizer", "researcher", "universal page-store", "large language models", "LLMs", "reinforcement learning"], "githubStars": 246, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u5185\u5b58\u5bf9\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u9759\u6001\u5185\u5b58\u5bb9\u6613\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u79f0\u4e3a\u4e00\u822c\u4ee3\u7406\u5185\u5b58\uff08GAM\uff09\uff0c\u91c7\u7528\u201c\u53ca\u65f6\u7f16\u8bd1\u201d\uff08JIT\uff09\u539f\u5219\u3002</li>\n    <li>GAM\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u8bb0\u5fc6\u5668\u548c\u7814\u7a76\u8005\uff0c\u5206\u522b\u7528\u4e8e\u7ba1\u7406\u5386\u53f2\u4fe1\u606f\u548c\u5728\u7ebf\u68c0\u7d22\u6709\u7528\u4fe1\u606f\u3002</li>\n    <li>\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u6709\u6548\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6574\u4f53\u6027\u80fd\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cGAM\u5728\u591a\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684\u4efb\u52a1\u5b8c\u6210\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5185\u5b58\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Memory is very important for AI agents, but traditional static memory can lead to significant information loss.</li>\n  <li>We introduce a new system called general agentic memory (GAM) that creates memory when needed, rather than in advance.</li>\n  <li>GAM consists of two main parts: a Memorizer that keeps important past information, and a Researcher that finds useful information when needed.</li>\n  <li>This system helps AI agents use large language models more effectively and improves their overall performance.</li>\n  <li>Tests show that GAM performs much better in memory-related tasks compared to existing memory systems.</li>\n</ul>"}, "publishedAt": "2025-11-23T07:29:33.000Z", "title": "General Agentic Memory Via Deep Research", "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18423.png", "numComments": 2, "submittedBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "fullname": "zhengliu", "name": "lz1001", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Dec 12, 2025";
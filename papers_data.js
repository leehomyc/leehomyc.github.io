window.trendingPapers = {
    "today": [{"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u6570\u636e\u9700\u6c42\u5feb\u901f\u589e\u957f\uff0c\u63a8\u52a8\u9ad8\u8d28\u91cf\u6570\u636e\u51c6\u5907\u7ba1\u9053\u7684\u53d1\u5c55\u3002</li>\n    <li>\u76ee\u524d\u7684\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u4e3b\u8981\u4f9d\u8d56\u4e34\u65f6\u811a\u672c\uff0c\u7f3a\u4e4f\u89c4\u8303\uff0c\u96be\u4ee5\u91cd\u590d\u548c\u4f18\u5316\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DataFlow\uff0c\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684LLM\u9a71\u52a8\u6570\u636e\u51c6\u5907\u6846\u67b6\u3002</li>\n    <li>DataFlow\u63d0\u4f9b\u8fd1200\u4e2a\u53ef\u91cd\u7528\u64cd\u4f5c\u7b26\u548c\u591a\u79cd\u9886\u57df\u901a\u7528\u7ba1\u9053\uff0c\u4ee5\u652f\u6301\u9ad8\u6548\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>DataFlow\u5728\u516d\u4e2a\u6848\u4f8b\u4e2d\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u4eba\u7c7b\u6570\u636e\u96c6\u548c\u4e13\u7528\u5408\u6210\u57fa\u7ebf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is a growing need for high-quality data in Large Language Models (LLMs), but current methods are often inconsistent and hard to reproduce.</li>\n    <li>DataFlow is a new framework that helps create better data preparation processes for LLMs, using modular and reusable components.</li>\n    <li>The framework includes almost 200 reusable tools and six general pipelines for various tasks like text processing and code generation.</li>\n    <li>DataFlow-Agent can convert simple language instructions into working data pipelines automatically.</li>\n    <li>In tests, DataFlow improved performance in multiple use cases, outperforming both human-curated datasets and other synthetic datasets.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19693", "authors": [{"_id": "694a0ffa335742716e93227d", "name": "Weichen Fan", "hidden": false}, {"_id": "694a0ffa335742716e93227e", "name": "Haiwen Diao", "hidden": false}, {"_id": "694a0ffa335742716e93227f", "name": "Quan Wang", "hidden": false}, {"_id": "694a0ffa335742716e932280", "name": "Dahua Lin", "hidden": false}, {"_id": "694a0ffa335742716e932281", "name": "Ziwei Liu", "hidden": false}], "publishedAt": "2025-12-22T18:59:57.000Z", "submittedOnDailyAt": "2025-12-23T01:15:14.379Z", "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.", "upvotes": 51, "discussionId": "694a0ffa335742716e932282", "githubRepo": "https://github.com/WeichenFan/UAE", "githubRepoAddedBy": "user", "ai_summary": "Unified Autoencoding combines semantic and pixel-level information through a frequency-band modulator, resulting in a latent space with state-of-the-art performance on image benchmarks.", "ai_keywords": ["spectral characteristics", "semantic encoders", "pixel encoders", "feature spectrum", "low-frequency components", "high-frequency information", "Prism Hypothesis", "Unified Autoencoding", "frequency-band modulator", "ImageNet", "MS-COCO", "latent space"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u5206\u6790\u4e86\u4e0d\u540c\u8bed\u4e49\u7f16\u7801\u5668\u548c\u50cf\u7d20\u7f16\u7801\u5668\u7684\u9891\u8c31\u7279\u6027\u3002</li>\n    <li>\u53d1\u73b0\u8bed\u4e49\u7f16\u7801\u5668\u4e3b\u8981\u6355\u6349\u4f4e\u9891\u6210\u5206\uff0c\u800c\u50cf\u7d20\u7f16\u7801\u5668\u8fd8\u4fdd\u7559\u4e86\u9ad8\u9891\u4fe1\u606f\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u201c\u68f1\u955c\u5047\u8bbe\u201d\uff0c\u8ba4\u4e3a\u4e0d\u540c\u6570\u636e\u6a21\u6001\u53ef\u4ee5\u89c6\u4e3a\u81ea\u7136\u4e16\u754c\u5728\u5171\u4eab\u7279\u5f81\u9891\u8c31\u4e0a\u7684\u6295\u5f71\u3002</li>\n    <li>\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u81ea\u7f16\u7801\u6a21\u578b\uff08UAE\uff09\uff0c\u7ed3\u5408\u4e86\u8bed\u4e49\u7ed3\u6784\u548c\u50cf\u7d20\u7ec6\u8282\u3002</li>\n    <li>\u5728ImageNet\u548cMS-COCO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUAE\u5728\u8bed\u4e49\u62bd\u8c61\u548c\u50cf\u7d20\u4fdd\u771f\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study looks at how different types of encoders (for meaning and images) behave in terms of their frequency characteristics.</li>\n    <li>It finds that semantic encoders focus on low-frequency components, which capture abstract meanings, while pixel encoders also include high-frequency details for finer image quality.</li>\n    <li>This relationship is described by the Prism Hypothesis, suggesting that different data types project onto a common frequency spectrum.</li>\n    <li>The authors introduce Unified Autoencoding (UAE), a model that combines abstract meaning and detailed image data using a frequency modulator.</li>\n    <li>Tests on ImageNet and MS-COCO show that UAE performs excellently by merging semantic and pixel information effectively.</li>\n</ul>"}, "publishedAt": "2025-12-22T13:59:57.000Z", "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding", "summary": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19693.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 189}, "isAuthorParticipating": false}, {"paper": {"id": "2512.17650", "authors": [{"_id": "6948c8f334f46eaf46cbb325", "name": "Zhongwei Zhang", "hidden": false}, {"_id": "6948c8f334f46eaf46cbb326", "name": "Fuchen Long", "hidden": false}, {"_id": "6948c8f334f46eaf46cbb327", "name": "Wei Li", "hidden": false}, {"_id": "6948c8f334f46eaf46cbb328", "name": "Zhaofan Qiu", "hidden": false}, {"_id": "6948c8f334f46eaf46cbb329", "name": "Wu Liu", "hidden": false}, {"_id": "6948c8f334f46eaf46cbb32a", "name": "Ting Yao", "hidden": false}, {"_id": "6948c8f334f46eaf46cbb32b", "name": "Tao Mei", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6496f5754a3c31df8e3139f6/3S3unvdbINRHQFW85psrn.mp4"], "publishedAt": "2025-12-19T14:49:30.000Z", "submittedOnDailyAt": "2025-12-23T01:38:37.820Z", "title": "Region-Constraint In-Context Generation for Instructional Video Editing", "submittedOnDailyBy": {"_id": "6496f5754a3c31df8e3139f6", "avatarUrl": "/avatars/cf789d1986f976373c82b2976df4542a.svg", "isPro": false, "fullname": "Zhongwei Zhang", "user": "zzwustc", "type": "user"}, "summary": "The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.", "upvotes": 38, "discussionId": "6948c8f334f46eaf46cbb32c", "projectPage": "https://zhw-zhang.github.io/ReCo-page/", "githubRepo": "https://github.com/HiDream-ai/ReCo", "githubRepoAddedBy": "user", "ai_summary": "ReCo is a novel instructional video editing paradigm that enhances accuracy and reduces token interference by incorporating constraint modeling and regularization techniques during in-context generation.", "ai_keywords": ["in-context generation", "instructional video editing", "denoising", "ReCo", "constraint modeling", "latent regularization", "attention regularization", "backward denoised latents", "attention maps", "ReCo-Data", "video diffusion learning", "instruction-video pairs", "video editing tasks"], "githubStars": 32, "organization": {"_id": "61d8000084231b832e5bbd99", "name": "ustc", "fullname": "university of science and technology  of china", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1641545773772-61d7fdeb22a383817a543b68.png"}, "summary_zh": "<ul>\n    <li>ReCo\u662f\u4e00\u79cd\u65b0\u7684\u6307\u4ee4\u6027\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u7f16\u8f91\u4e2d\u7f16\u8f91\u533a\u57df\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u6e90\u89c6\u9891\u548c\u76ee\u6807\u89c6\u9891\u5bbd\u5ea6\u62fc\u63a5\u6765\u8fdb\u884c\u8054\u5408\u53bb\u566a\u3002</li>\n    <li>ReCo\u4f7f\u7528\u4e24\u79cd\u6b63\u5219\u5316\u6280\u672f\uff0c\u589e\u5f3a\u7f16\u8f91\u533a\u57df\u7684\u5dee\u5f02\u6027\uff0c\u51cf\u5c11\u975e\u7f16\u8f91\u533a\u57df\u7684\u5e72\u6270\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u8fd8\u6291\u5236\u7f16\u8f91\u533a\u57df\u7684\u6ce8\u610f\u529b\uff0c\u4ee5\u964d\u4f4e\u5728\u76ee\u6807\u89c6\u9891\u4e2d\u751f\u6210\u65b0\u5bf9\u8c61\u65f6\u7684\u5e72\u6270\u3002</li>\n    <li>ReCo-Data\u662f\u4e00\u4e2a\u5305\u542b50\u4e07\u4e2a\u6307\u4ee4-\u89c6\u9891\u5bf9\u7684\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u89c6\u9891\u7f16\u8f91\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>ReCo is a new approach for instructional video editing that improves how editing and non-editing areas are handled.</li>\n    <li>It combines source and target videos to improve the quality of edits and reduce errors in editing regions.</li>\n    <li>ReCo uses two techniques, latent regularization and attention regularization, to enhance the editing process and minimize unwanted changes.</li>\n    <li>A new dataset called ReCo-Data has been created with 500,000 instruction-video pairs to help train models for better video editing.</li>\n    <li>Tests show that ReCo performs better than existing methods in various video editing tasks.</li>\n</ul>"}, "publishedAt": "2025-12-19T09:49:30.000Z", "title": "Region-Constraint In-Context Generation for Instructional Video Editing", "summary": "The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6496f5754a3c31df8e3139f6/3S3unvdbINRHQFW85psrn.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17650.png", "numComments": 2, "submittedBy": {"_id": "6496f5754a3c31df8e3139f6", "avatarUrl": "/avatars/cf789d1986f976373c82b2976df4542a.svg", "fullname": "Zhongwei Zhang", "name": "zzwustc", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61d8000084231b832e5bbd99", "name": "ustc", "fullname": "university of science and technology  of china", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1641545773772-61d7fdeb22a383817a543b68.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.17040", "authors": [{"_id": "6948b33d34f46eaf46cbb293", "user": {"_id": "6449db44df4e6cb7eaef912a", "avatarUrl": "/avatars/777cea252e06933863bda10dc3543f59.svg", "isPro": false, "fullname": "Min-Jung Kim", "user": "emjay73", "type": "user"}, "name": "Min-Jung Kim", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:39.498Z", "hidden": false}, {"_id": "6948b33d34f46eaf46cbb294", "name": "Jeongho Kim", "hidden": false}, {"_id": "6948b33d34f46eaf46cbb295", "name": "Hoiyeong Jin", "hidden": false}, {"_id": "6948b33d34f46eaf46cbb296", "name": "Junha Hyung", "hidden": false}, {"_id": "6948b33d34f46eaf46cbb297", "name": "Jaegul Choo", "hidden": false}], "publishedAt": "2025-12-18T20:03:05.000Z", "submittedOnDailyAt": "2025-12-23T03:45:12.735Z", "title": "Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation", "submittedOnDailyBy": {"_id": "6449db44df4e6cb7eaef912a", "avatarUrl": "/avatars/777cea252e06933863bda10dc3543f59.svg", "isPro": false, "fullname": "Min-Jung Kim", "user": "emjay73", "type": "user"}, "summary": "Recent progress in video diffusion models has spurred growing interest in camera-controlled novel-view video generation for dynamic scenes, aiming to provide creators with cinematic camera control capabilities in post-production. A key challenge in camera-controlled video generation is ensuring fidelity to the specified camera pose, while maintaining view consistency and reasoning about occluded geometry from limited observations. To address this, existing methods either train trajectory-conditioned video generation model on trajectory-video pair dataset, or estimate depth from the input video to reproject it along a target trajectory and generate the unprojected regions. Nevertheless, existing methods struggle to generate camera-pose-faithful, high-quality videos for two main reasons: (1) reprojection-based approaches are highly susceptible to errors caused by inaccurate depth estimation; and (2) the limited diversity of camera trajectories in existing datasets restricts learned models. To address these limitations, we present InfCam, a depth-free, camera-controlled video-to-video generation framework with high pose fidelity. The framework integrates two key components: (1) infinite homography warping, which encodes 3D camera rotations directly within the 2D latent space of a video diffusion model. Conditioning on this noise-free rotational information, the residual parallax term is predicted through end-to-end training to achieve high camera-pose fidelity; and (2) a data augmentation pipeline that transforms existing synthetic multiview datasets into sequences with diverse trajectories and focal lengths. Experimental results demonstrate that InfCam outperforms baseline methods in camera-pose accuracy and visual fidelity, generalizing well from synthetic to real-world data. Link to our project page:https://emjay73.github.io/InfCam/", "upvotes": 26, "discussionId": "6948b33d34f46eaf46cbb298", "projectPage": "https://emjay73.github.io/InfCam/", "githubRepo": "https://github.com/emjay73/InfCam", "githubRepoAddedBy": "user", "ai_summary": "InfCam generates high-fidelity videos with accurate camera poses by using infinite homography warping and augmenting synthetic datasets with diverse trajectories.", "ai_keywords": ["video diffusion models", "camera-controlled video generation", "trajectory-conditioned video generation", "depth-free", "infinite homography warping", "3D camera rotations", "2D latent space", "residual parallax term", "data augmentation pipeline", "multiview datasets", "focal lengths"], "githubStars": 21, "organization": {"_id": "6475760c33192631bad2bb38", "name": "kaist-ai", "fullname": "KAIST AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u8fdb\u5c55\u5f15\u53d1\u4e86\u5bf9\u52a8\u6001\u573a\u666f\u4e2d\u76f8\u673a\u63a7\u5236\u7684\u65b0\u89c6\u89d2\u89c6\u9891\u751f\u6210\u7684\u5174\u8da3\u3002</li>\n    <li>\u76f8\u673a\u63a7\u5236\u89c6\u9891\u751f\u6210\u7684\u4e3b\u8981\u6311\u6218\u662f\u786e\u4fdd\u4e0e\u6307\u5b9a\u76f8\u673a\u59ff\u6001\u7684\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u5904\u7406\u88ab\u906e\u6321\u7684\u51e0\u4f55\u4fe1\u606f\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6df1\u5ea6\u4f30\u8ba1\u548c\u91cd\u6295\u5f71\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u6df1\u5ea6\u4f30\u8ba1\u4e0d\u51c6\u786e\u7684\u5f71\u54cd\uff0c\u4e14\u6570\u636e\u96c6\u4e2d\u7684\u76f8\u673a\u8f68\u8ff9\u591a\u6837\u6027\u6709\u9650\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86InfCam\u6846\u67b6\uff0c\u5b83\u4e0d\u4f9d\u8d56\u6df1\u5ea6\u4f30\u8ba1\uff0c\u80fd\u9ad8\u4fdd\u771f\u5730\u751f\u6210\u89c6\u9891\u3002</li>\n    <li>InfCam\u901a\u8fc7\u65e0\u9650\u5355\u5e94\u6027\u53d8\u6362\u548c\u6570\u636e\u589e\u5f3a\u7ba1\u9053\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76f8\u673a\u59ff\u6001\u51c6\u786e\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent advancements in video diffusion models are improving how videos can be generated with camera control for dynamic scenes.</li>\n    <li>A major challenge is ensuring the generated videos accurately match the specified camera angles while keeping a consistent view.</li>\n    <li>Current methods face problems with inaccurate depth estimation and limited camera trajectory diversity in training data.</li>\n    <li>The new framework, InfCam, does not rely on depth estimation and uses advanced techniques to improve camera-pose accuracy.</li>\n    <li>InfCam shows better performance in video quality and camera accuracy compared to existing methods and works well with both synthetic and real-world data.</li>\n</ul>"}, "publishedAt": "2025-12-18T15:03:05.000Z", "title": "Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation", "summary": "Recent progress in video diffusion models has spurred growing interest in camera-controlled novel-view video generation for dynamic scenes, aiming to provide creators with cinematic camera control capabilities in post-production. A key challenge in camera-controlled video generation is ensuring fidelity to the specified camera pose, while maintaining view consistency and reasoning about occluded geometry from limited observations. To address this, existing methods either train trajectory-conditioned video generation model on trajectory-video pair dataset, or estimate depth from the input video to reproject it along a target trajectory and generate the unprojected regions. Nevertheless, existing methods struggle to generate camera-pose-faithful, high-quality videos for two main reasons: (1) reprojection-based approaches are highly susceptible to errors caused by inaccurate depth estimation; and (2) the limited diversity of camera trajectories in existing datasets restricts learned models. To address these limitations, we present InfCam, a depth-free, camera-controlled video-to-video generation framework with high pose fidelity. The framework integrates two key components: (1) infinite homography warping, which encodes 3D camera rotations directly within the 2D latent space of a video diffusion model. Conditioning on this noise-free rotational information, the residual parallax term is predicted through end-to-end training to achieve high camera-pose fidelity; and (2) a data augmentation pipeline that transforms existing synthetic multiview datasets into sequences with diverse trajectories and focal lengths. Experimental results demonstrate that InfCam outperforms baseline methods in camera-pose accuracy and visual fidelity, generalizing well from synthetic to real-world data. Link to our project page:https://emjay73.github.io/InfCam/", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17040.png", "numComments": 5, "submittedBy": {"_id": "6449db44df4e6cb7eaef912a", "avatarUrl": "/avatars/777cea252e06933863bda10dc3543f59.svg", "fullname": "Min-Jung Kim", "name": "emjay73", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "6475760c33192631bad2bb38", "name": "kaist-ai", "fullname": "KAIST AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.19134", "authors": [{"_id": "694a1765335742716e9322b7", "name": "Dehai Min", "hidden": false}, {"_id": "694a1765335742716e9322b8", "name": "Kailin Zhang", "hidden": false}, {"_id": "694a1765335742716e9322b9", "name": "Tongtong Wu", "hidden": false}, {"_id": "694a1765335742716e9322ba", "name": "Lu Cheng", "hidden": false}], "publishedAt": "2025-12-22T08:28:05.000Z", "submittedOnDailyAt": "2025-12-23T01:46:57.477Z", "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation", "submittedOnDailyBy": {"_id": "629c6ee73a3221bb210afc2d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629c6ee73a3221bb210afc2d/Mg-VymVvHQn_pDrTgks0s.jpeg", "isPro": false, "fullname": "Dehai Min", "user": "ZhishanQ", "type": "user"}, "summary": "Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.", "upvotes": 25, "discussionId": "694a1765335742716e9322bb", "githubRepo": "https://github.com/ZhishanQ/QuCo-RAG", "githubRepoAddedBy": "user", "ai_summary": "QuCo-RAG uses objective corpus statistics to mitigate hallucinations in large language models during generation, improving accuracy across various benchmarks.", "ai_keywords": ["dynamic retrieval-augmented generation", "large language models", "hallucinations", "model-internal signals", "logits", "entropy", "pre-training data", "uncertainty quantification", "low-frequency entities", "entity co-occurrence", "Infini-gram", "multi-hop QA", "EM gains", "OLMo-2", "Llama", "Qwen", "GPT", "biomedical QA", "domain generalization", "corpus-grounded verification"], "githubStars": 6, "summary_zh": "<ul>\n    <li>\u52a8\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08Dynamic RAG\uff09\u901a\u8fc7\u5728\u751f\u6210\u65f6\u9002\u5e94\u6027\u5730\u51b3\u5b9a\u4f55\u65f6\u68c0\u7d22\uff0c\u6765\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u73b0\u8c61\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6a21\u578b\u5185\u90e8\u4fe1\u53f7\uff0c\u8fd9\u4e9b\u4fe1\u53f7\u4e0d\u53ef\u9760\uff0c\u56e0\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86QuCo-RAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u5ba2\u89c2\u7edf\u8ba1\u6570\u636e\u4e2d\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a\u751f\u6210\u524d\u8bc6\u522b\u4f4e\u9891\u5b9e\u4f53\u548c\u751f\u6210\u65f6\u9a8c\u8bc1\u5b9e\u4f53\u5171\u73b0\uff0c\u4ee5\u5224\u65ad\u5e7b\u89c9\u98ce\u9669\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cQuCo-RAG\u5728\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u51c6\u63d0\u9ad8\u4e865-12\u70b9\uff0c\u5e76\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>QuCo-RAG is a new method that helps large language models (LLMs) make better decisions about when to retrieve information to reduce mistakes.</li>\n    <li>Unlike existing methods that rely on unreliable internal signals from the model, QuCo-RAG uses objective statistics from pre-training data to assess uncertainty.</li>\n    <li>The method identifies gaps in knowledge before generation and checks for entity co-occurrence during generation to reduce the risk of errors.</li>\n    <li>Experiments show that QuCo-RAG significantly improves performance on question-answering tasks compared to current top methods.</li>\n    <li>The approach is effective across different models and areas, including biomedical questions, and it is available for public use.</li>\n</ul>"}, "publishedAt": "2025-12-22T03:28:05.000Z", "title": "QuCo-RAG: Quantifying Uncertainty from the Pre-training Corpus for Dynamic Retrieval-Augmented Generation", "summary": "Dynamic Retrieval-Augmented Generation adaptively determines when to retrieve during generation to mitigate hallucinations in large language models (LLMs). However, existing methods rely on model-internal signals (e.g., logits, entropy), which are fundamentally unreliable because LLMs are typically ill-calibrated and often exhibit high confidence in erroneous outputs. We propose QuCo-RAG, which shifts from subjective confidence to objective statistics computed from pre-training data. Our method quantifies uncertainty through two stages: (1) before generation, we identify low-frequency entities indicating long-tail knowledge gaps; (2) during generation, we verify entity co-occurrence in the pre-training corpus, where zero co-occurrence often signals hallucination risk. Both stages leverage Infini-gram for millisecond-latency queries over 4 trillion tokens, triggering retrieval when uncertainty is high. Experiments on multi-hop QA benchmarks show QuCo-RAG achieves EM gains of 5--12 points over state-of-the-art baselines with OLMo-2 models, and transfers effectively to models with undisclosed pre-training data (Llama, Qwen, GPT), improving EM by up to 14 points. Domain generalization on biomedical QA further validates the robustness of our paradigm. These results establish corpus-grounded verification as a principled, practically model-agnostic paradigm for dynamic RAG. Our code is publicly available at https://github.com/ZhishanQ/QuCo-RAG.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19134.png", "numComments": 2, "submittedBy": {"_id": "629c6ee73a3221bb210afc2d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629c6ee73a3221bb210afc2d/Mg-VymVvHQn_pDrTgks0s.jpeg", "fullname": "Dehai Min", "name": "ZhishanQ", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": false}, {"paper": {"id": "2512.18880", "authors": [{"_id": "694a0eb6335742716e932275", "name": "Ming Li", "hidden": false}, {"_id": "694a0eb6335742716e932276", "name": "Han Chen", "hidden": false}, {"_id": "694a0eb6335742716e932277", "name": "Yunze Xiao", "hidden": false}, {"_id": "694a0eb6335742716e932278", "name": "Jian Chen", "hidden": false}, {"_id": "694a0eb6335742716e932279", "name": "Hong Jiao", "hidden": false}, {"_id": "694a0eb6335742716e93227a", "name": "Tianyi Zhou", "hidden": false}], "publishedAt": "2025-12-21T20:41:36.000Z", "submittedOnDailyAt": "2025-12-23T01:11:10.771Z", "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction", "submittedOnDailyBy": {"_id": "647f5af5b0e96764589f3b2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg", "isPro": false, "fullname": "Tianyi Zhou", "user": "zhoutianyi", "type": "user"}, "summary": "Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of human learners. In this work, we present a large-scale empirical analysis of Human-AI Difficulty Alignment for over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a shared machine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specific proficiency levels. Furthermore, we identify a critical lack of introspection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of human cognitive struggles, highlighting the challenge of using current models for automated difficulty prediction.", "upvotes": 20, "discussionId": "694a0eb7335742716e93227b", "githubRepo": "https://github.com/MingLiiii/Difficulty_Alignment", "githubRepoAddedBy": "user", "ai_summary": "Large Language Models struggle to accurately estimate human cognitive difficulty due to a misalignment with human perceptions and a lack of introspection regarding their own limitations.", "ai_keywords": ["Human-AI Difficulty Alignment", "Large Language Models", "cognitive struggles", "machine consensus", "proficiency levels", "introspection"], "githubStars": 0, "summary_zh": "<ul>\n    <li>\u51c6\u786e\u4f30\u8ba1\u9898\u76ee\u96be\u5ea6\u5bf9\u6559\u80b2\u8bc4\u4f30\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u51b7\u542f\u52a8\u95ee\u9898\u3002</li>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c55\u793a\u4e86\u8d85\u4eba\u7c7b\u7684\u89e3\u9898\u80fd\u529b\uff0c\u4f46\u4e0d\u786e\u5b9a\u5b83\u4eec\u662f\u5426\u80fd\u7406\u89e3\u4eba\u7c7b\u5b66\u4e60\u8005\u7684\u8ba4\u77e5\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u5bf920\u591a\u79cd\u6a21\u578b\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u53d1\u73b0\u6a21\u578b\u4e0e\u4eba\u7c7b\u5728\u96be\u5ea6\u4f30\u8ba1\u4e0a\u5b58\u5728\u7cfb\u7edf\u6027\u4e0d\u4e00\u81f4\u3002</li>\n    <li>\u6a21\u578b\u89c4\u6a21\u589e\u5927\u5e76\u6ca1\u6709\u53ef\u9760\u5730\u6539\u5584\u96be\u5ea6\u4f30\u8ba1\uff0c\u76f8\u53cd\uff0c\u6a21\u578b\u8d8b\u5411\u4e8e\u4e0e\u673a\u5668\u8fbe\u6210\u5171\u8bc6\u3002</li>\n    <li>\u6a21\u578b\u5728\u9ad8\u6027\u80fd\u65f6\u96be\u4ee5\u51c6\u786e\u4f30\u8ba1\u5b66\u751f\u7684\u80fd\u529b\u9650\u5236\uff0c\u5e76\u4e14\u7f3a\u4e4f\u81ea\u6211\u53cd\u601d\u80fd\u529b\uff0c\u65e0\u6cd5\u9884\u6d4b\u81ea\u8eab\u7684\u5c40\u9650\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Estimating how hard items (like questions) are is important for education but faces challenges.</li>\n    <li>Large Language Models (LLMs) are good at solving problems, but it's unclear if they understand how hard tasks are for human learners.</li>\n    <li>A study analyzed over 20 models in areas like medical knowledge and math to see how well they align with human perceptions of difficulty.</li>\n    <li>The results showed that bigger models don't necessarily align with human difficulty ratings and often agree with each other instead.</li>\n    <li>High-performing models struggle to estimate task difficulty accurately and often can't recognize their own limitations, making them less useful for predicting difficulty in education.</li>\n</ul>"}, "publishedAt": "2025-12-21T15:41:36.000Z", "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction", "summary": "Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of human learners. In this work, we present a large-scale empirical analysis of Human-AI Difficulty Alignment for over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a shared machine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specific proficiency levels. Furthermore, we identify a critical lack of introspection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of human cognitive struggles, highlighting the challenge of using current models for automated difficulty prediction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.18880.png", "numComments": 2, "submittedBy": {"_id": "647f5af5b0e96764589f3b2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg", "fullname": "Tianyi Zhou", "name": "zhoutianyi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 19}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19678", "authors": [{"_id": "694a087f335742716e9321e9", "name": "Hanyang Kong", "hidden": false}, {"_id": "694a087f335742716e9321ea", "name": "Xingyi Yang", "hidden": false}, {"_id": "694a087f335742716e9321eb", "name": "Xiaoxu Zheng", "hidden": false}, {"_id": "694a087f335742716e9321ec", "name": "Xinchao Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6442882f8443bce4c98a88aa/mSmlhlqlIsBQJ1S0aEeSo.mp4"], "publishedAt": "2025-12-22T18:53:50.000Z", "submittedOnDailyAt": "2025-12-23T01:31:35.792Z", "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion", "submittedOnDailyBy": {"_id": "6442882f8443bce4c98a88aa", "avatarUrl": "/avatars/70d5aa651b07b43629554096d76efd4c.svg", "isPro": false, "fullname": "Kong", "user": "imsuperkong", "type": "user"}, "summary": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: https://hyokong.github.io/worldwarp-page/{https://hyokong.github.io/worldwarp-page/}.", "upvotes": 18, "discussionId": "694a087f335742716e9321ed", "projectPage": "https://hyokong.github.io/worldwarp-page/", "githubRepo": "https://github.com/HyoKong/WorldWarp", "githubRepoAddedBy": "user", "ai_summary": "WorldWarp addresses the challenge of generating consistent long-range videos by integrating a 3D geometric cache with a spatio-temporal diffusion model, ensuring structural consistency and textural refinement.", "ai_keywords": ["3D structural anchor", "2D generative refiner", "Gaussian Splatting", "Spatio-Temporal Diffusion", "spatio-temporal varying noise schedule", "3D cache", "structural scaffold", "fill-and-revise objective"], "githubStars": 31, "organization": {"_id": "6508ab2b349930913196378b", "name": "NationalUniversityofSingapore", "fullname": "National University of Singapore", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"}, "summary_zh": "<ul>\n    <li>\u751f\u6210\u4e00\u81f4\u7684\u957f\u89c6\u9891\u9762\u4e34\u6311\u6218\uff0c\u8981\u6c42\u5728\u50cf\u7d20\u7a7a\u95f4\u4e25\u683c\u9075\u5faa3D\u51e0\u4f55\u3002</li>\n    <li>\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u76f8\u673a\u6761\u4ef6\u4e0b\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6548\u679c\u6700\u597d\uff0c\u4f46\u96be\u4ee5\u5904\u7406\u906e\u6321\u548c\u590d\u6742\u7684\u76f8\u673a\u8f68\u8ff9\u3002</li>\n    <li>\u63d0\u51faWorldWarp\u6846\u67b6\uff0c\u5c063D\u7ed3\u6784\u951a\u5b9a\u4e0e2D\u751f\u6210\u7ec6\u5316\u7ed3\u5408\u3002</li>\n    <li>WorldWarp\u901a\u8fc7\u9ad8\u65af\u55b7\u5c04\u6280\u672f\u7ef4\u62a4\u5728\u7ebf3D\u51e0\u4f55\u7f13\u5b58\uff0c\u786e\u4fdd\u65b0\u5e27\u9075\u5faa\u5148\u524d\u51e0\u4f55\u7ed3\u6784\u3002</li>\n    <li>\u4f7f\u7528\u65f6\u7a7a\u6269\u6563\u6a21\u578b\u586b\u8865\u548c\u4fee\u6b63\u7a7a\u767d\u533a\u57df\uff0c\u5b9e\u73b0\u89c6\u9891\u4e00\u81f4\u6027\u548c\u9ad8\u4fdd\u771f\u5ea6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>A new framework called WorldWarp helps create videos that look good and follow 3D geometry rules.</li>\n    <li>It uses a 3D cache built through a method called Gaussian Splatting to keep video frames consistent with past frames.</li>\n    <li>WorldWarp solves problems with blank areas and artifacts by using a special noise model to fill and improve these regions.</li>\n    <li>The system updates the 3D cache continuously to keep the video consistent while enhancing the visual quality.</li>\n    <li>Overall, WorldWarp combines 3D structure and texture refinement to achieve high-quality video generation.</li>\n</ul>"}, "publishedAt": "2025-12-22T13:53:50.000Z", "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion", "summary": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: https://hyokong.github.io/worldwarp-page/{https://hyokong.github.io/worldwarp-page/}.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6442882f8443bce4c98a88aa/mSmlhlqlIsBQJ1S0aEeSo.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19678.png", "numComments": 2, "submittedBy": {"_id": "6442882f8443bce4c98a88aa", "avatarUrl": "/avatars/70d5aa651b07b43629554096d76efd4c.svg", "fullname": "Kong", "name": "imsuperkong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "6508ab2b349930913196378b", "name": "NationalUniversityofSingapore", "fullname": "National University of Singapore", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/630ca0817dacb93b33506ce7/ZYUmpSMsa5Whihw3me2Bw.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19629", "authors": [{"_id": "694a13d0335742716e9322a0", "name": "Jiaqi Peng", "hidden": false}, {"_id": "694a13d0335742716e9322a1", "name": "Wenzhe Cai", "hidden": false}, {"_id": "694a13d0335742716e9322a2", "name": "Yuqiang Yang", "hidden": false}, {"_id": "694a13d0335742716e9322a3", "name": "Tai Wang", "hidden": false}, {"_id": "694a13d0335742716e9322a4", "name": "Yuan Shen", "hidden": false}, {"_id": "694a13d0335742716e9322a5", "name": "Jiangmiao Pang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64e6d9d229a548f66aff6e5b/E5JZHmhqVQdd32fECzT8R.mp4"], "publishedAt": "2025-12-22T18:03:08.000Z", "submittedOnDailyAt": "2025-12-23T03:05:41.880Z", "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry", "submittedOnDailyBy": {"_id": "64e6d9d229a548f66aff6e5b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6d9d229a548f66aff6e5b/yQ9E2TyzM4CfSjMPigcey.jpeg", "isPro": false, "fullname": "Tai Wang", "user": "taiwang", "type": "user"}, "summary": "Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the https://steinate.github.io/logoplanner.github.io/{project page}.", "upvotes": 16, "discussionId": "694a13d1335742716e9322a6", "ai_summary": "LoGoPlanner is an end-to-end navigation framework that improves trajectory planning in unstructured environments by integrating localization, scene geometry reconstruction, and policy conditioning.", "ai_keywords": ["trajectory planning", "unstructured environments", "end-to-end learning", "localization", "scene geometry reconstruction", "policy conditioning", "long-horizon visual-geometry backbone", "implicit state estimation", "obstacle avoidance", "metric-aware geometry memory"], "summary_zh": "<ul>\n    <li>LoGoPlanner\u662f\u4e00\u4e2a\u57fa\u4e8e\u5b9a\u4f4d\u7684\u7aef\u5230\u7aef\u5bfc\u822a\u6846\u67b6\uff0c\u65e8\u5728\u6539\u5584\u79fb\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u3002</li>\n    <li>\u8be5\u6846\u67b6\u901a\u8fc7\u7cbe\u7ec6\u8c03\u6574\u89c6\u89c9\u51e0\u4f55\u6a21\u578b\uff0c\u63d0\u4f9b\u51c6\u786e\u7684\u5b9a\u4f4d\uff0c\u4ece\u800c\u51cf\u5c11\u9519\u8bef\u4f20\u64ad\u3002</li>\n    <li>LoGoPlanner\u5229\u7528\u5386\u53f2\u89c2\u5bdf\u91cd\u5efa\u73af\u5883\u51e0\u4f55\uff0c\u589e\u5f3a\u969c\u788d\u7269\u89c4\u907f\u80fd\u529b\u3002</li>\n    <li>\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u6d4b\u8bd5\u663e\u793a\uff0cLoGoPlanner\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u9ad8\u4e8627.3%\u7684\u6027\u80fd\u3002</li>\n    <li>\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5728\u9879\u76ee\u9875\u9762\u516c\u5f00\u53d1\u5e03\uff0c\u4f9b\u516c\u4f17\u4f7f\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LoGoPlanner is a new framework for mobile robots that improves navigation in complex environments.</li>\n    <li>It combines visual observations with accurate localization to help robots understand their surroundings better.</li>\n    <li>The framework uses past observations to build a detailed map of the environment, which helps in avoiding obstacles.</li>\n    <li>LoGoPlanner has been tested in both simulations and real life, showing over a 27.3% improvement in navigation accuracy compared to previous methods.</li>\n    <li>The project\u2019s code and models are publicly available for others to use and explore.</li>\n</ul>"}, "publishedAt": "2025-12-22T13:03:08.000Z", "title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry", "summary": "Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the https://steinate.github.io/logoplanner.github.io/{project page}.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64e6d9d229a548f66aff6e5b/E5JZHmhqVQdd32fECzT8R.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19629.png", "numComments": 2, "submittedBy": {"_id": "64e6d9d229a548f66aff6e5b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6d9d229a548f66aff6e5b/yQ9E2TyzM4CfSjMPigcey.jpeg", "fullname": "Tai Wang", "name": "taiwang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "isAuthorParticipating": false}, {"paper": {"id": "2512.17385", "authors": [{"_id": "6948fd1234f46eaf46cbb3be", "name": "Jiajun Wu", "hidden": false}, {"_id": "6948fd1234f46eaf46cbb3bf", "name": "Jian Yang", "hidden": false}, {"_id": "6948fd1234f46eaf46cbb3c0", "name": "Wei Zhang", "hidden": false}, {"_id": "6948fd1234f46eaf46cbb3c1", "name": "Lin Jing", "hidden": false}, {"_id": "6948fd1234f46eaf46cbb3c2", "name": "Yuqing Ma", "hidden": false}, {"_id": "6948fd1234f46eaf46cbb3c3", "name": "Ensheng Shi", "hidden": false}, {"_id": "6948fd1234f46eaf46cbb3c4", "name": "Yuchi Ma", "hidden": false}, {"_id": "6948fd1234f46eaf46cbb3c5", "name": "Zhoujun Li", "hidden": false}, {"_id": "6948fd1234f46eaf46cbb3c6", "name": "Xianglong Liu", "hidden": false}], "publishedAt": "2025-12-19T09:42:04.000Z", "submittedOnDailyAt": "2025-12-23T03:58:44.703Z", "title": "UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models", "submittedOnDailyBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios.", "upvotes": 15, "discussionId": "6948fd1334f46eaf46cbb3c7", "ai_summary": "IPC is an unsupervised framework that uses internal probing of large language models to generate code without labeled datasets, achieving competitive performance with reduced resource dependency.", "ai_keywords": ["large language models", "unsupervised framework", "internal probing", "problem space probing", "test understanding probing", "solution space probing", "knowledge consolidation", "reinforcement", "self-consistency mechanisms", "representation-based quality estimation", "UCoder", "code benchmarks", "internal model states", "code quality", "correctness", "resource-constrained scenarios"], "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6216\u672a\u6807\u6ce8\u7684\u6570\u636e\uff0c\u83b7\u53d6\u8fd9\u4e9b\u6570\u636e\u6210\u672c\u9ad8\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u6846\u67b6IPC\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u63a2\u6d4b\u8fdb\u884c\u4ee3\u7801\u751f\u6210\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\u96c6\u3002</li>\n    <li>IPC\u901a\u8fc7\u95ee\u9898\u7a7a\u95f4\u63a2\u6d4b\u3001\u7406\u89e3\u63a2\u6d4b\u3001\u89e3\u51b3\u65b9\u6848\u63a2\u6d4b\u7b49\u65b9\u6cd5\uff0c\u6316\u6398\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u77e5\u8bc6\u548c\u4fe1\u5fc3\u6a21\u5f0f\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u6211\u4e00\u81f4\u6027\u673a\u5236\u548c\u57fa\u4e8e\u8868\u793a\u7684\u8d28\u91cf\u8bc4\u4f30\u6765\u8bc6\u522b\u53ef\u9760\u7684\u4ee3\u7801\u5019\u9009\uff0c\u5e76\u8bad\u7ec3\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u7f16\u7801\u5668UCoder\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u65e0\u76d1\u7763\u65b9\u6cd5\u5728\u591a\u4e2a\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0e\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5bf9\u6807\u6ce8\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u4f9d\u8d56\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) are good at generating code but need a lot of labeled or unlabeled data, which can be hard to get.</li>\n    <li>This paper presents a new method called IPC that allows code generation without needing external datasets.</li>\n    <li>IPC uses techniques to explore the internal knowledge and confidence of LLMs to improve code generation.</li>\n    <li>The method identifies good code candidates through self-consistency and quality checks, training a model called UCoder.</li>\n    <li>Results show that IPC can perform well on code tasks without relying heavily on labeled data, making it useful in situations with limited resources.</li>\n</ul>"}, "publishedAt": "2025-12-19T04:42:04.000Z", "title": "UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17385.png", "numComments": 2, "submittedBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "fullname": "Yang Jian", "name": "CSJianYang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 22}, "isAuthorParticipating": false}, {"paper": {"id": "2512.19682", "authors": [{"_id": "694a0dc4335742716e932249", "name": "Jiacheng Guo", "hidden": false}, {"_id": "694a0dc4335742716e93224a", "name": "Ling Yang", "hidden": false}, {"_id": "694a0dc4335742716e93224b", "name": "Peter Chen", "hidden": false}, {"_id": "694a0dc4335742716e93224c", "name": "Qixin Xiao", "hidden": false}, {"_id": "694a0dc4335742716e93224d", "name": "Yinjie Wang", "hidden": false}, {"_id": "694a0dc4335742716e93224e", "name": "Xinzhe Juan", "hidden": false}, {"_id": "694a0dc4335742716e93224f", "name": "Jiahao Qiu", "hidden": false}, {"_id": "694a0dc4335742716e932250", "name": "Ke Shen", "hidden": false}, {"_id": "694a0dc4335742716e932251", "name": "Mengdi Wang", "hidden": false}], "publishedAt": "2025-12-22T18:57:13.000Z", "submittedOnDailyAt": "2025-12-23T01:04:41.574Z", "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective \u03b1-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to +40.3\\% over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3times less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.", "upvotes": 11, "discussionId": "694a0dc5335742716e932252", "githubRepo": "https://github.com/Gen-Verse/GenEnv", "githubRepoAddedBy": "user", "ai_summary": "GenEnv, a framework using a co-evolutionary game with a generative environment simulator, enhances LLM agent performance by 40.3% over 7B baselines and uses less data than offline augmentation.", "ai_keywords": ["Large Language Model", "LLM", "GenEnv", "co-evolutionary game", "generative environment simulator", "dynamic curriculum policy", "zone of proximal development", "\u03b1-Curriculum Reward", "API-Bank", "ALFWorld", "BFCL", "Bamboogle", "TravelPlanner", "data-efficient", "adaptive simulation"], "githubStars": 8, "organization": {"_id": "64374111a701a7e744c02b0e", "name": "princetonu", "fullname": "Princeton University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/b3xXusq8Zz3ej8Z6fRTSZ.png"}, "summary_zh": "<ul>\n    <li>GenEnv\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u4e0e\u53ef\u6269\u5c55\u7684\u751f\u6210\u73af\u5883\u6a21\u62df\u5668\u8fdb\u884c\u5408\u4f5c\uff0c\u89e3\u51b3\u4e86\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u7684\u9ad8\u6210\u672c\u548c\u9759\u6001\u6570\u636e\u95ee\u9898\u3002</li>\n    <li>\u4e0e\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\uff0cGenEnv\u53ef\u4ee5\u52a8\u6001\u5730\u751f\u6210\u4efb\u52a1\uff0c\u5e2e\u52a9\u4ee3\u7406\u5728\u5176\u80fd\u529b\u8303\u56f4\u5185\u4e0d\u65ad\u63d0\u5347\u3002</li>\n    <li>\u901a\u8fc7\u7b80\u5355\u6709\u6548\u7684\u03b1-\u8bfe\u7a0b\u5956\u52b1\uff0cGenEnv\u5c06\u4efb\u52a1\u96be\u5ea6\u4e0e\u4ee3\u7406\u5f53\u524d\u80fd\u529b\u76f8\u5339\u914d\u3002</li>\n    <li>\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGenEnv\u63d0\u9ad8\u4e86\u4ee3\u7406\u7684\u8868\u73b0\uff0c\u6700\u9ad8\u63d0\u5347\u8fbe40.3%\u3002</li>\n    <li>\u4e0eGemini 2.5 Pro\u7684\u79bb\u7ebf\u6570\u636e\u589e\u5f3a\u76f8\u6bd4\uff0cGenEnv\u5728\u4f7f\u75283.3\u500d\u66f4\u5c11\u7684\u6570\u636e\u65f6\uff0c\u8868\u73b0\u66f4\u597d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>GenEnv is a new framework that helps train Large Language Model (LLM) agents more effectively by using a dynamic environment simulator.</li>\n    <li>It creates a game where the agent and simulator evolve together, generating tasks that match the agent's skill level.</li>\n    <li>GenEnv uses an \u03b1-Curriculum Reward to ensure tasks are appropriately challenging for the agent.</li>\n    <li>Tests showed that GenEnv improved agent performance by up to 40.3% compared to previous baseline models.</li>\n    <li>It achieves better results than other methods while needing significantly less data for training.</li>\n</ul>"}, "publishedAt": "2025-12-22T13:57:13.000Z", "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators", "summary": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective \u03b1-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to +40.3\\% over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3times less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19682.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 189}, "organization": {"_id": "64374111a701a7e744c02b0e", "name": "princetonu", "fullname": "Princeton University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/b3xXusq8Zz3ej8Z6fRTSZ.png"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u6570\u636e\u9700\u6c42\u5feb\u901f\u589e\u957f\uff0c\u63a8\u52a8\u9ad8\u8d28\u91cf\u6570\u636e\u51c6\u5907\u7ba1\u9053\u7684\u53d1\u5c55\u3002</li>\n    <li>\u76ee\u524d\u7684\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u4e3b\u8981\u4f9d\u8d56\u4e34\u65f6\u811a\u672c\uff0c\u7f3a\u4e4f\u89c4\u8303\uff0c\u96be\u4ee5\u91cd\u590d\u548c\u4f18\u5316\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DataFlow\uff0c\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684LLM\u9a71\u52a8\u6570\u636e\u51c6\u5907\u6846\u67b6\u3002</li>\n    <li>DataFlow\u63d0\u4f9b\u8fd1200\u4e2a\u53ef\u91cd\u7528\u64cd\u4f5c\u7b26\u548c\u591a\u79cd\u9886\u57df\u901a\u7528\u7ba1\u9053\uff0c\u4ee5\u652f\u6301\u9ad8\u6548\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>DataFlow\u5728\u516d\u4e2a\u6848\u4f8b\u4e2d\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u4eba\u7c7b\u6570\u636e\u96c6\u548c\u4e13\u7528\u5408\u6210\u57fa\u7ebf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is a growing need for high-quality data in Large Language Models (LLMs), but current methods are often inconsistent and hard to reproduce.</li>\n    <li>DataFlow is a new framework that helps create better data preparation processes for LLMs, using modular and reusable components.</li>\n    <li>The framework includes almost 200 reusable tools and six general pipelines for various tasks like text processing and code generation.</li>\n    <li>DataFlow-Agent can convert simple language instructions into working data pipelines automatically.</li>\n    <li>In tests, DataFlow improved performance in multiple use cases, outperforming both human-curated datasets and other synthetic datasets.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni\u662f\u4e00\u4e2a\u901a\u7528\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u8be5\u7cfb\u7edf\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\uff0c\u63d0\u4f9b\u66f4\u6d41\u7545\u7684\u7528\u6237\u4f53\u9a8c\u3002</li>\n    <li>Kling-Omni\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5982\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\uff0c\u80fd\u591f\u521b\u5efa\u7535\u5f71\u8d28\u91cf\u7684\u89c6\u9891\u5185\u5bb9\u3002</li>\n    <li>\u5b83\u5efa\u7acb\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u6765\u589e\u5f3a\u529f\u80fd\u3002</li>\n    <li>Kling-Omni\u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from various input types like text, images, and videos.</li>\n    <li>It combines video generation, editing, and reasoning tasks into one seamless framework.</li>\n    <li>The system processes different inputs into a single format to produce smart and cinematic videos.</li>\n    <li>Kling-Omni is built on a strong data foundation and uses advanced training methods for better performance.</li>\n    <li>It shows great results in generating content, editing based on reasoning, and following multimodal instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u79d1\u5b66\u4e00\u822c\u667a\u80fd\uff08SGI\uff09\u5c1a\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u6307\u7684\u662f\u5728\u79d1\u5b66\u9886\u57df\u81ea\u4e3b\u6784\u601d\u3001\u7814\u7a76\u548c\u63a8\u7406\u7684\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u5b9e\u7528\u63a2\u7a76\u6a21\u578b\uff08PIM\uff09\u7684SGI\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u4e0e\u79d1\u5b66\u5bb6\u76f8\u5173\u7684\u4efb\u52a1\u8fdb\u884c\u64cd\u4f5c\uff1a\u6df1\u5165\u7814\u7a76\u3001\u521b\u610f\u751f\u6210\u3001\u5b9e\u9a8c\u548c\u5b9e\u9a8c\u63a8\u7406\u3002</li>\n    <li>SGI-Bench\u5305\u542b1000\u591a\u4e2a\u8de8\u5b66\u79d1\u6837\u672c\uff0c\u5e2e\u52a9\u7cfb\u7edf\u8bc4\u4f30\u6700\u65b0\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff1a\u6df1\u5165\u7814\u7a76\u7684\u51c6\u786e\u5339\u914d\u7387\u4f4e\uff0810-20%\uff09\uff0c\u521b\u610f\u7f3a\u4e4f\u53ef\u884c\u6027\u548c\u7ec6\u8282\uff0c\u5e72\u5b9e\u9a8c\u7684\u4ee3\u7801\u53ef\u6267\u884c\u6027\u9ad8\u4f46\u7ed3\u679c\u51c6\u786e\u6027\u4f4e\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\uff0c\u4f18\u5316\u63a8\u7406\u4e2d\u7684\u65b0\u9896\u6027\u5956\u52b1\uff0c\u589e\u5f3a\u5047\u8bbe\u7684\u65b0\u9896\u6027\uff0c\u4e3aAI\u7cfb\u7edf\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u63d0\u4f9b\u4e86\u57fa\u7840\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is currently no clear framework for Scientific General Intelligence (SGI), which is the ability of AI to independently explore and reason in science.</li>\n    <li>The authors propose a new definition of SGI based on the Practical Inquiry Model, focusing on four key tasks: deep research, idea generation, experiments, and reasoning.</li>\n    <li>They created SGI-Bench, a benchmark with over 1,000 examples from various scientific fields to test AI models on their scientific abilities.</li>\n    <li>Results show that AI struggles with deep research accuracy, idea feasibility, and executing experiments correctly, particularly in comparing different types of data.</li>\n    <li>The authors also introduce a method called Test-Time Reinforcement Learning to improve the novelty of AI hypotheses without needing a reference answer.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.15431", "authors": [{"_id": "69437417542d62d58a7bf6c4", "name": "Haolong Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6c5", "name": "Jia Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c6", "name": "Xin Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c7", "name": "Yeqing Shen", "hidden": false}, {"_id": "69437417542d62d58a7bf6c8", "user": {"_id": "653614073f4248157d60ccdc", "avatarUrl": "/avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg", "isPro": false, "fullname": "mengziyang", "user": "zylate", "type": "user"}, "name": "Ziyang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:53.033Z", "hidden": false}, {"_id": "69437417542d62d58a7bf6c9", "name": "Zhimin Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6ca", "name": "Kaijun Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6cb", "name": "Jin Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf6cc", "name": "Lieyu Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6cd", "name": "Mi Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6ce", "name": "Shiliang Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6cf", "name": "Zhirui Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d0", "name": "Brian Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d1", "name": "Kang An", "hidden": false}, {"_id": "69437417542d62d58a7bf6d2", "name": "Chenyang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d3", "name": "Lei Lei", "hidden": false}, {"_id": "69437417542d62d58a7bf6d4", "name": "Mengmeng Duan", "hidden": false}, {"_id": "69437417542d62d58a7bf6d5", "name": "Danxun Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d6", "name": "Guodong Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d7", "name": "Hang Cheng", "hidden": false}, {"_id": "69437417542d62d58a7bf6d8", "name": "Hao Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d9", "name": "Jie Dong", "hidden": false}, {"_id": "69437417542d62d58a7bf6da", "name": "Junhao Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6db", "name": "Mei Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6dc", "name": "Renjie Yu", "hidden": false}, {"_id": "69437417542d62d58a7bf6dd", "name": "Shunshan Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6de", "name": "Xu Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf6df", "name": "Yiting Dai", "hidden": false}, {"_id": "69437417542d62d58a7bf6e0", "name": "Yineng Deng", "hidden": false}, {"_id": "69437417542d62d58a7bf6e1", "name": "Yingdan Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6e2", "name": "Zelin Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6e3", "name": "Wen Sun", "hidden": false}, {"_id": "69437417542d62d58a7bf6e4", "name": "Chengxu Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e5", "name": "Chunqin Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6e6", "name": "Dong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6e7", "name": "Fengqiong Xiao", "hidden": false}, {"_id": "69437417542d62d58a7bf6e8", "name": "Guanghao Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e9", "name": "Guopeng Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ea", "name": "Guozhen Peng", "hidden": false}, {"_id": "69437417542d62d58a7bf6eb", "name": "Hongbing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ec", "name": "Hang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ed", "name": "Hongming Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6ee", "name": "Jingjing Xie", "hidden": false}, {"_id": "69437417542d62d58a7bf6ef", "name": "Jianyong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6f0", "name": "Jingyang Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf6f1", "name": "Jiaju Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f2", "name": "Jiayu Yuan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f3", "name": "Jianpeng Yin", "hidden": false}, {"_id": "69437417542d62d58a7bf6f4", "name": "Kai Cao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f5", "name": "Liang Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f6", "name": "Liguo Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f7", "name": "Liying Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6f8", "name": "Mengqiang Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f9", "name": "Min Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fa", "name": "Manjiao Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fb", "name": "Mao Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf6fc", "name": "Mingxin Wan", "hidden": false}, {"_id": "69437417542d62d58a7bf6fd", "name": "Na Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6fe", "name": "Nan Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6ff", "name": "Ning Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf700", "name": "Peiyao Ma", "hidden": false}, {"_id": "69437417542d62d58a7bf701", "name": "Qingzhou Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf702", "name": "Qiao Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf703", "name": "Qinlin Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf704", "name": "Qiong Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf705", "name": "Qiongyao Li", "hidden": false}, {"_id": "69437417542d62d58a7bf706", "name": "Shangwu Zhong", "hidden": false}, {"_id": "69437417542d62d58a7bf707", "name": "Shuli Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf708", "name": "Shaofan Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf709", "name": "Shisi Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf70a", "name": "Shuang Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf70b", "name": "Xingbin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70c", "name": "Xiaojia Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70d", "name": "Xiaojie Hou", "hidden": false}, {"_id": "69437417542d62d58a7bf70e", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70f", "name": "Xuanti Feng", "hidden": false}, {"_id": "69437417542d62d58a7bf710", "name": "Xuedan Cai", "hidden": false}, {"_id": "69437417542d62d58a7bf711", "name": "Xuan Wen", "hidden": false}, {"_id": "69437417542d62d58a7bf712", "name": "Xianwei Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf713", "name": "Xin Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf714", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf715", "name": "Xin Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf716", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf717", "name": "Yukang Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf718", "name": "Yunfang Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf719", "name": "Yuqing Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf71a", "name": "Yixun Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf71b", "name": "Zejia Weng", "hidden": false}, {"_id": "69437417542d62d58a7bf71c", "name": "Zhonghao Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf71d", "name": "Zhiguo Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf71e", "name": "Zhuoyu Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf71f", "name": "Zheng Ge", "hidden": false}, {"_id": "69437417542d62d58a7bf720", "name": "Jing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf721", "name": "Yibo Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf722", "name": "Binxing Jiao", "hidden": false}, {"_id": "69437417542d62d58a7bf723", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf724", "name": "Daxin Jiang", "hidden": false}], "publishedAt": "2025-12-17T13:26:30.000Z", "submittedOnDailyAt": "2025-12-18T00:55:26.804Z", "title": "Step-GUI Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "upvotes": 77, "discussionId": "69437418542d62d58a7bf725", "projectPage": "https://opengelab.github.io/", "githubRepo": "https://github.com/stepfun-ai/gelab-zero", "githubRepoAddedBy": "user", "ai_summary": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.", "ai_keywords": ["multimodal large language models", "GUI automation", "self-evolving training pipeline", "Calibrated Step Reward System", "trajectory-level calibration", "Step-GUI", "GUI performance", "GUI-MCP", "Model Context Protocol", "AndroidWorld", "OSWorld", "ScreenShot-Pro", "AndroidDaily", "real-world mobile usage patterns", "hierarchical architecture", "low-level atomic operations", "high-level task delegation", "local specialist models", "high-privacy execution"], "githubStars": 1417, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u5c55\u4e3aGUI\u81ea\u52a8\u5316\u5e26\u6765\u4e86\u65b0\u7684\u673a\u4f1a\uff0c\u4f46\u83b7\u53d6\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6211\u6f14\u5316\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5229\u7528\u6821\u51c6\u6b65\u5956\u52b1\u7cfb\u7edf\uff0c\u5c06\u6a21\u578b\u751f\u6210\u7684\u8f68\u8ff9\u8f6c\u6362\u4e3a\u53ef\u9760\u7684\u8bad\u7ec3\u4fe1\u53f7\uff0c\u8282\u7701\u4e86\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u6ce8\u91ca\u51c6\u786e\u7387\u3002</li>\n    <li>\u4ecb\u7ecd\u4e86Step-GUI\u6a21\u578b\uff0c\u5177\u6709\u51fa\u8272\u7684GUI\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u901a\u7528\u80fd\u529b\u3002</li>\n    <li>\u4e3a\u4e86\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86GUI-MCP\uff0c\u9996\u6b21\u7528\u4e8eGUI\u81ea\u52a8\u5316\u7684\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff0c\u7ed3\u5408\u4e86\u4f4e\u7ea7\u539f\u5b50\u64cd\u4f5c\u548c\u9ad8\u7ea7\u4efb\u52a1\u59d4\u6d3e\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u63a8\u51fa\u4e86AndroidDaily\u57fa\u51c6\uff0c\u57fa\u4e8e\u771f\u5b9e\u7684\u79fb\u52a8\u4f7f\u7528\u6a21\u5f0f\u8bc4\u4f30\u4ee3\u7406\u7684\u8868\u73b0\uff0c\u63a8\u52a8\u4e86\u5b9e\u9645GUI\u4ee3\u7406\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>New multimodal language models can improve GUI automation but need reliable training data.</li>\n  <li>A self-evolving training system is introduced, achieving over 90% accuracy at a much lower cost.</li>\n  <li>Step-GUI models show top performance in GUI tasks while keeping strong general capabilities.</li>\n  <li>GUI-MCP is proposed to ensure user privacy and standardize interfaces across different devices.</li>\n  <li>AndroidDaily is a new benchmark based on real mobile usage, testing agents with common daily tasks.</li>\n</ul>"}, "publishedAt": "2025-12-17T08:26:30.000Z", "title": "Step-GUI Technical Report", "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15431.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16793", "authors": [{"_id": "6944ba3efbf17e708e185f60", "user": {"_id": "6944c28d3cd5eeb7a7838663", "avatarUrl": "/avatars/63f9f8c4e8462cbc9726bdee249fbcde.svg", "isPro": false, "fullname": "lin", "user": "birdxp", "type": "user"}, "name": "Xiaopeng Lin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:14.947Z", "hidden": false}, {"_id": "6944ba3efbf17e708e185f61", "user": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "isPro": false, "fullname": "Shijie Lian", "user": "LiamLian0727", "type": "user"}, "name": "Shijie Lian", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:18.093Z", "hidden": false}, {"_id": "6944ba3efbf17e708e185f62", "user": {"_id": "63d3b5f1640bb0f77173baea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674819020331-noauth.jpeg", "isPro": false, "fullname": "yubin", "user": "VLyb", "type": "user"}, "name": "Bin Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T11:00:33.691Z", "hidden": false}, {"_id": "6944ba3efbf17e708e185f63", "user": {"_id": "684a7f750fff63d29d18444c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kXTIoUPOM7LDztBcRL6YI.png", "isPro": false, "fullname": "Ruoqi Yang", "user": "lyceen", "type": "user"}, "name": "Ruoqi Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T11:00:35.617Z", "hidden": false}, {"_id": "6944ba3efbf17e708e185f64", "name": "Changti Wu", "hidden": false}, {"_id": "6944ba3efbf17e708e185f65", "name": "Yuzhuo Miao", "hidden": false}, {"_id": "6944ba3efbf17e708e185f66", "name": "Yurun Jin", "hidden": false}, {"_id": "6944ba3efbf17e708e185f67", "name": "Yukun Shi", "hidden": false}, {"_id": "6944ba3efbf17e708e185f68", "name": "Cong Huang", "hidden": false}, {"_id": "6944ba3efbf17e708e185f69", "name": "Bojun Cheng", "hidden": false}, {"_id": "6944ba3efbf17e708e185f6a", "name": "Kai Chen", "hidden": false}], "publishedAt": "2025-12-18T17:27:03.000Z", "submittedOnDailyAt": "2025-12-22T00:59:36.650Z", "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence", "submittedOnDailyBy": {"_id": "6944c28d3cd5eeb7a7838663", "avatarUrl": "/avatars/63f9f8c4e8462cbc9726bdee249fbcde.svg", "isPro": false, "fullname": "lin", "user": "birdxp", "type": "user"}, "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.", "upvotes": 63, "discussionId": "6944ba3efbf17e708e185f6b", "projectPage": "https://zgc-embodyai.github.io/PhysBrain/", "ai_summary": "Proposed Egocentric2Embodiment pipeline translates human egocentric videos into structured training data for robots, enhancing their egocentric understanding and task performance.", "ai_keywords": ["Egocentric2Embodiment", "VQA supervision", "evidence grounding", "temporal consistency", "Egocentric2Embodiment dataset", "E2E-3M", "PhysBrain", "egocentric-aware", "VLA fine-tuning", "SimplerEnv success rates"], "organization": {"_id": "6948d884070dda0c2ae35a78", "name": "DeepCybo", "fullname": "DeepCybo", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65ec01fd770aa0e25d9374dc/QOsz6P_7AxyqGrjsRHTGk.png"}, "summary_zh": "<ul>\n    <li>\u673a\u5668\u4eba\u6cdb\u5316\u4f9d\u8d56\u4e8e\u7269\u7406\u667a\u80fd\uff0c\u5305\u62ec\u5bf9\u72b6\u6001\u53d8\u5316\u3001\u63a5\u89e6\u4e92\u52a8\u548c\u957f\u671f\u89c4\u5212\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5927\u591a\u6570\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e3b\u8981\u5728\u7b2c\u4e09\u4eba\u79f0\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u8fd9\u5bfc\u81f4\u4eba\u5f62\u673a\u5668\u4eba\u5b58\u5728\u89c6\u89d2\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002</li>\n    <li>\u6536\u96c6\u673a\u5668\u4eba\u81ea\u6211\u4e2d\u5fc3\u6570\u636e\u7684\u6210\u672c\u9ad8\u4e14\u591a\u6837\u6027\u6709\u9650\uff0c\u800c\u5927\u91cf\u4eba\u7c7b\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u8bad\u7ec3\u76d1\u7763\u7684\u7ba1\u9053\uff0c\u79f0\u4e3aEgocentric2Embodiment\uff08E2E\uff09\u7ffb\u8bd1\u7ba1\u9053\u3002</li>\n    <li>\u901a\u8fc7\u5728E2E-3M\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0cPhysBrain\u663e\u8457\u63d0\u9ad8\u4e86\u81ea\u6211\u4e2d\u5fc3\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u6709\u6548\u8f6c\u79fb\u4e86\u4eba\u7c7b\u76d1\u7763\u7684\u4f18\u52bf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Robotic generalization needs physical intelligence to understand state changes and interactions.</li>\n    <li>Most current models are trained on third-person data, which doesn't match how humanoid robots perceive their environment.</li>\n    <li>Collecting robot-specific data is expensive and limited, but human egocentric videos can provide a more diverse and scalable solution.</li>\n    <li>The Egocentric2Embodiment pipeline converts first-person videos into structured training data for robots.</li>\n    <li>Training on the new dataset, E2E-3M, improves a robot's understanding and performance in tasks that require planning and interaction.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:27:03.000Z", "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence", "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16793.png", "numComments": 2, "submittedBy": {"_id": "6944c28d3cd5eeb7a7838663", "avatarUrl": "/avatars/63f9f8c4e8462cbc9726bdee249fbcde.svg", "fullname": "lin", "name": "birdxp", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "6948d884070dda0c2ae35a78", "name": "DeepCybo", "fullname": "DeepCybo", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65ec01fd770aa0e25d9374dc/QOsz6P_7AxyqGrjsRHTGk.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.19693", "authors": [{"_id": "694a0ffa335742716e93227d", "name": "Weichen Fan", "hidden": false}, {"_id": "694a0ffa335742716e93227e", "name": "Haiwen Diao", "hidden": false}, {"_id": "694a0ffa335742716e93227f", "name": "Quan Wang", "hidden": false}, {"_id": "694a0ffa335742716e932280", "name": "Dahua Lin", "hidden": false}, {"_id": "694a0ffa335742716e932281", "name": "Ziwei Liu", "hidden": false}], "publishedAt": "2025-12-22T18:59:57.000Z", "submittedOnDailyAt": "2025-12-23T01:15:14.379Z", "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.", "upvotes": 51, "discussionId": "694a0ffa335742716e932282", "githubRepo": "https://github.com/WeichenFan/UAE", "githubRepoAddedBy": "user", "ai_summary": "Unified Autoencoding combines semantic and pixel-level information through a frequency-band modulator, resulting in a latent space with state-of-the-art performance on image benchmarks.", "ai_keywords": ["spectral characteristics", "semantic encoders", "pixel encoders", "feature spectrum", "low-frequency components", "high-frequency information", "Prism Hypothesis", "Unified Autoencoding", "frequency-band modulator", "ImageNet", "MS-COCO", "latent space"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u5206\u6790\u4e86\u4e0d\u540c\u8bed\u4e49\u7f16\u7801\u5668\u548c\u50cf\u7d20\u7f16\u7801\u5668\u7684\u9891\u8c31\u7279\u6027\u3002</li>\n    <li>\u53d1\u73b0\u8bed\u4e49\u7f16\u7801\u5668\u4e3b\u8981\u6355\u6349\u4f4e\u9891\u6210\u5206\uff0c\u800c\u50cf\u7d20\u7f16\u7801\u5668\u8fd8\u4fdd\u7559\u4e86\u9ad8\u9891\u4fe1\u606f\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u201c\u68f1\u955c\u5047\u8bbe\u201d\uff0c\u8ba4\u4e3a\u4e0d\u540c\u6570\u636e\u6a21\u6001\u53ef\u4ee5\u89c6\u4e3a\u81ea\u7136\u4e16\u754c\u5728\u5171\u4eab\u7279\u5f81\u9891\u8c31\u4e0a\u7684\u6295\u5f71\u3002</li>\n    <li>\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u81ea\u7f16\u7801\u6a21\u578b\uff08UAE\uff09\uff0c\u7ed3\u5408\u4e86\u8bed\u4e49\u7ed3\u6784\u548c\u50cf\u7d20\u7ec6\u8282\u3002</li>\n    <li>\u5728ImageNet\u548cMS-COCO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUAE\u5728\u8bed\u4e49\u62bd\u8c61\u548c\u50cf\u7d20\u4fdd\u771f\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study looks at how different types of encoders (for meaning and images) behave in terms of their frequency characteristics.</li>\n    <li>It finds that semantic encoders focus on low-frequency components, which capture abstract meanings, while pixel encoders also include high-frequency details for finer image quality.</li>\n    <li>This relationship is described by the Prism Hypothesis, suggesting that different data types project onto a common frequency spectrum.</li>\n    <li>The authors introduce Unified Autoencoding (UAE), a model that combines abstract meaning and detailed image data using a frequency modulator.</li>\n    <li>Tests on ImageNet and MS-COCO show that UAE performs excellently by merging semantic and pixel information effectively.</li>\n</ul>"}, "publishedAt": "2025-12-22T13:59:57.000Z", "title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding", "summary": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.19693.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 189}, "isAuthorParticipating": false}, {"paper": {"id": "2512.17901", "authors": [{"_id": "6948af7534f46eaf46cbb1da", "user": {"_id": "6719bfd07c6e6c83a388aeae", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719bfd07c6e6c83a388aeae/jHxryk04dzHo23TX5F5sz.png", "isPro": false, "fullname": "Junyu Zhang", "user": "jyzhang1208", "type": "user"}, "name": "Junyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:53.009Z", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1db", "name": "Yifan Sun", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1dc", "name": "Tianang Leng", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1dd", "name": "Jingyan Shen", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1de", "name": "Liu Ziyin", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1df", "name": "Paul Pu Liang", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1e0", "name": "Huan Zhang", "hidden": false}], "publishedAt": "2025-12-19T18:59:11.000Z", "submittedOnDailyAt": "2025-12-22T00:10:07.525Z", "title": "When Reasoning Meets Its Laws", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/", "upvotes": 48, "discussionId": "6948af7534f46eaf46cbb1e1", "projectPage": "https://lore-project.github.io/", "githubRepo": "https://github.com/ASTRAL-Group/LoRe", "githubRepoAddedBy": "user", "ai_summary": "A framework called Laws of Reasoning (LoRe) is introduced to theoretically define desired reasoning behaviors in Large Reasoning Models, with a focus on compute and accuracy laws, and a benchmark (LoRe-Bench) to measure these properties.", "ai_keywords": ["Laws of Reasoning", "LoRe", "compute law", "accuracy law", "question complexity", "monotonicity", "compositionality", "LoRe-Bench", "finetuning approach", "compute-law compositionality"], "githubStars": 15, "summary_zh": "<ul>\n    <li>\u5c3d\u7ba1\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u63a8\u7406\u884c\u4e3a\u5e38\u5e38\u4e0d\u76f4\u89c2\uff0c\u5bfc\u81f4\u63a8\u7406\u80fd\u529b\u4e0d\u7406\u60f3\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u63a8\u7406\u5b9a\u5f8b\uff08LoRe\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u63cf\u8ff0LRMs\u4e2d\u7684\u5185\u5728\u63a8\u7406\u6a21\u5f0f\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u8ba1\u7b97\u6cd5\u5219\uff0c\u5047\u8bbe\u63a8\u7406\u8ba1\u7b97\u5e94\u4e0e\u95ee\u9898\u590d\u6742\u6027\u5448\u7ebf\u6027\u5173\u7cfb\u3002</li>\n    <li>\u901a\u8fc7\u5f15\u5165LoRe-Bench\u57fa\u51c6\uff0c\u7cfb\u7edf\u6027\u5730\u6d4b\u91cf\u63a8\u7406\u6a21\u578b\u7684\u5355\u8c03\u6027\u548c\u7ec4\u5408\u6027\u8fd9\u4e24\u4e2a\u5c5e\u6027\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u5927\u591a\u6570\u63a8\u7406\u6a21\u578b\u5177\u6709\u5408\u7406\u7684\u5355\u8c03\u6027\uff0c\u4f46\u7f3a\u4e4f\u7ec4\u5408\u6027\uff1b\u901a\u8fc7\u6709\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\u53ef\u4ee5\u6539\u5584\u8fd9\u4e00\u70b9\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Reasoning Models (LRMs) perform well but often reason in confusing ways.</li>\n    <li>This paper introduces the Laws of Reasoning (LoRe) to define better reasoning patterns in LRMs.</li>\n    <li>LoRe includes a compute law, suggesting reasoning should increase with question complexity, and an accuracy law.</li>\n    <li>The researchers created LoRe-Bench to measure two key traits of reasoning models: monotonicity and compositionality.</li>\n    <li>Most models show good monotonicity but struggle with compositionality, leading to a new finetuning method that improves reasoning performance.</li>\n</ul>"}, "publishedAt": "2025-12-19T13:59:11.000Z", "title": "When Reasoning Meets Its Laws", "summary": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17901.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16922", "authors": [{"_id": "6944c39ffbf17e708e18605d", "user": {"_id": "63f233820a16587ea967adc2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f233820a16587ea967adc2/1nSoZofPV7UseXzjI2qAH.png", "isPro": false, "fullname": "Sihan XU", "user": "sihanxu", "type": "user"}, "name": "Sihan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:57:39.350Z", "hidden": false}, {"_id": "6944c39ffbf17e708e18605e", "name": "Ziqiao Ma", "hidden": false}, {"_id": "6944c39ffbf17e708e18605f", "name": "Wenhao Chai", "hidden": false}, {"_id": "6944c39ffbf17e708e186060", "name": "Xuweiyi Chen", "hidden": false}, {"_id": "6944c39ffbf17e708e186061", "user": {"_id": "66608add236f958513d21d2e", "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg", "isPro": false, "fullname": "Weiyang Jin", "user": "Wayne-King", "type": "user"}, "name": "Weiyang Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T09:56:57.800Z", "hidden": false}, {"_id": "6944c39ffbf17e708e186062", "name": "Joyce Chai", "hidden": false}, {"_id": "6944c39ffbf17e708e186063", "name": "Saining Xie", "hidden": false}, {"_id": "6944c39ffbf17e708e186064", "name": "Stella X. Yu", "hidden": false}], "publishedAt": "2025-12-18T18:59:58.000Z", "submittedOnDailyAt": "2025-12-19T01:03:18.428Z", "title": "Next-Embedding Prediction Makes Strong Vision Learners", "submittedOnDailyBy": {"_id": "66608add236f958513d21d2e", "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg", "isPro": false, "fullname": "Weiyang Jin", "user": "Wayne-King", "type": "user"}, "summary": "Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.", "upvotes": 47, "discussionId": "6944c3a0fbf17e708e186065", "projectPage": "https://sihanxu.me/nepa", "githubRepo": "https://github.com/SihanXU/nepa", "githubRepoAddedBy": "user", "ai_summary": "Generative pretraining using next embedding prediction outperforms traditional self-supervised methods in visual learning tasks, achieving high accuracy on ImageNet and effective transfer to semantic segmentation.", "ai_keywords": ["generative pretraining", "predictive tasks", "Next-Embedding Predictive Autoregression (NEPA)", "causal masking", "stop gradient", "Transformer", "ImageNet-1k", "top-1 accuracy", "ViT-B", "ViT-L", "semantic segmentation", "ADE20K"], "githubStars": 43, "organization": {"_id": "66df3cb0cf19a8918414cbfe", "name": "SixAILab", "fullname": "SixAILab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f233820a16587ea967adc2/FSRWuJTgSvG0HFKm9_K4Z.jpeg"}, "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u662f\u5426\u53ef\u4ee5\u5229\u7528\u751f\u6210\u9884\u8bad\u7ec3\u7684\u539f\u7406\u6765\u63d0\u9ad8\u89c6\u89c9\u5b66\u4e60\u6a21\u578b\u7684\u81ea\u6211\u76d1\u7763\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3a\u4e0b\u4e00\u5d4c\u5165\u9884\u6d4b\u81ea\u56de\u5f52\uff08NEPA\uff09\uff0c\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u7684\u5d4c\u5165\u6765\u8fdb\u884c\u5b66\u4e60\u3002</li>\n    <li>\u6a21\u578b\u57fa\u4e8e\u8fc7\u53bb\u7684\u5d4c\u5165\u8fdb\u884c\u9884\u6d4b\uff0c\u4f7f\u7528\u56e0\u679c\u63a9\u7801\u548c\u505c\u6b62\u68af\u5ea6\u7684\u65b9\u6cd5\u3002</li>\n    <li>\u5728ImageNet-1k\u6570\u636e\u96c6\u4e0a\uff0c\u7b80\u5355\u7684Transformer\u6a21\u578b\u5728\u4ec5\u4f7f\u7528\u4e0b\u4e00\u5d4c\u5165\u9884\u6d4b\u4f5c\u4e3a\u5b66\u4e60\u76ee\u6807\u65f6\uff0c\u8fbe\u5230\u4e86\u826f\u597d\u7684\u6548\u679c\u3002</li>\n    <li>NEPA\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u80fd\u591f\u6709\u6548\u8f6c\u79fb\u5230\u5176\u4ed6\u4efb\u52a1\uff0c\u6bd4\u5982\u8bed\u4e49\u5206\u5272\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>The study investigates if techniques that work in natural language processing can also be applied to visual learning.</li>\n    <li>Instead of focusing on creating features for other tasks, the models are trained to predict future visual data based on past data.</li>\n    <li>This method, called Next-Embedding Predictive Autoregression (NEPA), uses a simple Transformer and does not require complicated processes like pixel reconstruction or contrastive loss.</li>\n    <li>NEPA shows strong performance, achieving high accuracy on ImageNet-1K and effectively transferring to other tasks like semantic segmentation.</li>\n    <li>The researchers suggest that this approach could be a simpler and more scalable way to handle visual self-supervised learning.</li>\n</ul>"}, "publishedAt": "2025-12-18T13:59:58.000Z", "title": "Next-Embedding Prediction Makes Strong Vision Learners", "summary": "Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16922.png", "numComments": 1, "submittedBy": {"_id": "66608add236f958513d21d2e", "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg", "fullname": "Weiyang Jin", "name": "Wayne-King", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "66df3cb0cf19a8918414cbfe", "name": "SixAILab", "fullname": "SixAILab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f233820a16587ea967adc2/FSRWuJTgSvG0HFKm9_K4Z.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16301", "authors": [{"_id": "6944be0ffbf17e708e185fde", "user": {"_id": "63724cfada3183d9d53f2009", "avatarUrl": "/avatars/17838fcf244ecf8d139343bb6c6d8562.svg", "isPro": false, "fullname": "Patrick Jiang", "user": "pat-jj", "type": "user"}, "name": "Pengcheng Jiang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:04.988Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fdf", "user": {"_id": "650488e454b989666d042a49", "avatarUrl": "/avatars/3dc79c6f1a9dce872636dddd38a04670.svg", "isPro": false, "fullname": "Jiacheng Lin", "user": "linjc16", "type": "user"}, "name": "Jiacheng Lin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:02.806Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe0", "user": {"_id": "66d4af28033492801d82b890", "avatarUrl": "/avatars/5e8a2dc1b932a679341976d11b22f6c8.svg", "isPro": false, "fullname": "shi", "user": "Gabshi", "type": "user"}, "name": "Zhiyi Shi", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T16:25:52.445Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe1", "name": "Zifeng Wang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe2", "name": "Luxi He", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe3", "name": "Yichen Wu", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe4", "name": "Ming Zhong", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe5", "user": {"_id": "649c5cf5c1ae48cf4d7dda34", "avatarUrl": "/avatars/a2264945f9f876b690017a93f225f937.svg", "isPro": false, "fullname": "Peiyang Song", "user": "p-song1", "type": "user"}, "name": "Peiyang Song", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:06.854Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe6", "name": "Qizheng Zhang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe7", "name": "Heng Wang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe8", "user": {"_id": "66a3f1c4c38ce500371fd8d4", "avatarUrl": "/avatars/381de938091f1a5c179eef72aa247bbf.svg", "isPro": false, "fullname": "Xueqiang Xu", "user": "XueqiangXu", "type": "user"}, "name": "Xueqiang Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:00.653Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe9", "name": "Hanwen Xu", "hidden": false}, {"_id": "6944be0ffbf17e708e185fea", "name": "Pengrui Han", "hidden": false}, {"_id": "6944be0ffbf17e708e185feb", "name": "Dylan Zhang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fec", "name": "Jiashuo Sun", "hidden": false}, {"_id": "6944be0ffbf17e708e185fed", "name": "Chaoqi Yang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fee", "name": "Kun Qian", "hidden": false}, {"_id": "6944be0ffbf17e708e185fef", "name": "Tian Wang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff0", "name": "Changran Hu", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff1", "name": "Manling Li", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff2", "name": "Quanzheng Li", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff3", "name": "Hao Peng", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff4", "name": "Sheng Wang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff5", "name": "Jingbo Shang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff6", "name": "Chao Zhang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff7", "name": "Jiaxuan You", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff8", "name": "Liyuan Liu", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff9", "name": "Pan Lu", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffa", "name": "Yu Zhang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffb", "name": "Heng Ji", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffc", "name": "Yejin Choi", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffd", "name": "Dawn Song", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffe", "name": "Jimeng Sun", "hidden": false}, {"_id": "6944be0ffbf17e708e185fff", "name": "Jiawei Han", "hidden": false}], "publishedAt": "2025-12-18T08:38:51.000Z", "submittedOnDailyAt": "2025-12-19T00:23:16.990Z", "title": "Adaptation of Agentic AI", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.", "upvotes": 43, "discussionId": "6944be10fbf17e708e186000", "githubRepo": "https://github.com/pat-jj/Awesome-Adaptation-of-Agentic-AI", "githubRepoAddedBy": "user", "ai_summary": "This paper presents a framework for agent and tool adaptation in agentic AI systems, clarifying design strategies and identifying open challenges for improving AI capabilities.", "ai_keywords": ["agentic AI systems", "foundation models", "agent adaptations", "tool adaptations", "tool-execution-signaled", "agent-output-signaled", "agent-agnostic", "agent-supervised"], "githubStars": 262, "summary_zh": "<ul>\n    <li>\u524d\u6cbf\u7684\u81ea\u4e3bAI\u7cfb\u7edf\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u53ef\u4ee5\u8fdb\u884c\u89c4\u5212\u3001\u63a8\u7406\u548c\u4e0e\u5916\u90e8\u5de5\u5177\u4e92\u52a8\u3002</li>\n    <li>\u9002\u5e94\u6027\u662f\u63d0\u9ad8\u6027\u80fd\u548c\u53ef\u9760\u6027\u7684\u5173\u952e\u673a\u5236\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u81ea\u4e3b\u4f53\u9002\u5e94\u548c\u5de5\u5177\u9002\u5e94\u7684\u7814\u7a76\u3002</li>\n    <li>\u6846\u67b6\u660e\u786e\u4e86\u4e0d\u540c\u9002\u5e94\u7b56\u7565\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5e2e\u52a9\u9009\u62e9\u548c\u5207\u6362\u7b56\u7565\u3002</li>\n    <li>\u4f5c\u8005\u56de\u987e\u4e86\u5404\u7c7b\u4ee3\u8868\u6027\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7684\u6311\u6218\u4e0e\u673a\u4f1a\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>New AI systems are based on models that can learn to plan, reason, and use tools for complex tasks.</li>\n  <li>Improving these systems relies heavily on adapting them for better performance and reliability.</li>\n  <li>This paper presents a framework that organizes different ways to adapt both agents and tools in AI.</li>\n  <li>The framework clarifies how to choose adaptation strategies and their advantages and disadvantages.</li>\n  <li>It also reviews various approaches to adaptation and identifies future challenges and opportunities in AI development.</li>\n</ul>"}, "publishedAt": "2025-12-18T03:38:51.000Z", "title": "Adaptation of Agentic AI", "summary": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16301.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "isAuthorParticipating": true}, {"paper": {"id": "2512.17650", "authors": [{"_id": "6948c8f334f46eaf46cbb325", "name": "Zhongwei Zhang", "hidden": false}, {"_id": "6948c8f334f46eaf46cbb326", "name": "Fuchen Long", "hidden": false}, {"_id": "6948c8f334f46eaf46cbb327", "name": "Wei Li", "hidden": false}, {"_id": "6948c8f334f46eaf46cbb328", "name": "Zhaofan Qiu", "hidden": false}, {"_id": "6948c8f334f46eaf46cbb329", "name": "Wu Liu", "hidden": false}, {"_id": "6948c8f334f46eaf46cbb32a", "name": "Ting Yao", "hidden": false}, {"_id": "6948c8f334f46eaf46cbb32b", "name": "Tao Mei", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6496f5754a3c31df8e3139f6/3S3unvdbINRHQFW85psrn.mp4"], "publishedAt": "2025-12-19T14:49:30.000Z", "submittedOnDailyAt": "2025-12-23T01:38:37.820Z", "title": "Region-Constraint In-Context Generation for Instructional Video Editing", "submittedOnDailyBy": {"_id": "6496f5754a3c31df8e3139f6", "avatarUrl": "/avatars/cf789d1986f976373c82b2976df4542a.svg", "isPro": false, "fullname": "Zhongwei Zhang", "user": "zzwustc", "type": "user"}, "summary": "The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.", "upvotes": 38, "discussionId": "6948c8f334f46eaf46cbb32c", "projectPage": "https://zhw-zhang.github.io/ReCo-page/", "githubRepo": "https://github.com/HiDream-ai/ReCo", "githubRepoAddedBy": "user", "ai_summary": "ReCo is a novel instructional video editing paradigm that enhances accuracy and reduces token interference by incorporating constraint modeling and regularization techniques during in-context generation.", "ai_keywords": ["in-context generation", "instructional video editing", "denoising", "ReCo", "constraint modeling", "latent regularization", "attention regularization", "backward denoised latents", "attention maps", "ReCo-Data", "video diffusion learning", "instruction-video pairs", "video editing tasks"], "githubStars": 32, "organization": {"_id": "61d8000084231b832e5bbd99", "name": "ustc", "fullname": "university of science and technology  of china", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1641545773772-61d7fdeb22a383817a543b68.png"}, "summary_zh": "<ul>\n    <li>ReCo\u662f\u4e00\u79cd\u65b0\u7684\u6307\u4ee4\u6027\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u7f16\u8f91\u4e2d\u7f16\u8f91\u533a\u57df\u4e0d\u51c6\u786e\u7684\u95ee\u9898\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u6e90\u89c6\u9891\u548c\u76ee\u6807\u89c6\u9891\u5bbd\u5ea6\u62fc\u63a5\u6765\u8fdb\u884c\u8054\u5408\u53bb\u566a\u3002</li>\n    <li>ReCo\u4f7f\u7528\u4e24\u79cd\u6b63\u5219\u5316\u6280\u672f\uff0c\u589e\u5f3a\u7f16\u8f91\u533a\u57df\u7684\u5dee\u5f02\u6027\uff0c\u51cf\u5c11\u975e\u7f16\u8f91\u533a\u57df\u7684\u5e72\u6270\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u8fd8\u6291\u5236\u7f16\u8f91\u533a\u57df\u7684\u6ce8\u610f\u529b\uff0c\u4ee5\u964d\u4f4e\u5728\u76ee\u6807\u89c6\u9891\u4e2d\u751f\u6210\u65b0\u5bf9\u8c61\u65f6\u7684\u5e72\u6270\u3002</li>\n    <li>ReCo-Data\u662f\u4e00\u4e2a\u5305\u542b50\u4e07\u4e2a\u6307\u4ee4-\u89c6\u9891\u5bf9\u7684\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u89c6\u9891\u7f16\u8f91\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>ReCo is a new approach for instructional video editing that improves how editing and non-editing areas are handled.</li>\n    <li>It combines source and target videos to improve the quality of edits and reduce errors in editing regions.</li>\n    <li>ReCo uses two techniques, latent regularization and attention regularization, to enhance the editing process and minimize unwanted changes.</li>\n    <li>A new dataset called ReCo-Data has been created with 500,000 instruction-video pairs to help train models for better video editing.</li>\n    <li>Tests show that ReCo performs better than existing methods in various video editing tasks.</li>\n</ul>"}, "publishedAt": "2025-12-19T09:49:30.000Z", "title": "Region-Constraint In-Context Generation for Instructional Video Editing", "summary": "The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6496f5754a3c31df8e3139f6/3S3unvdbINRHQFW85psrn.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17650.png", "numComments": 2, "submittedBy": {"_id": "6496f5754a3c31df8e3139f6", "avatarUrl": "/avatars/cf789d1986f976373c82b2976df4542a.svg", "fullname": "Zhongwei Zhang", "name": "zzwustc", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61d8000084231b832e5bbd99", "name": "ustc", "fullname": "university of science and technology  of china", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1641545773772-61d7fdeb22a383817a543b68.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>DeepSeek-V3.2\u662f\u4e00\u79cd\u9ad8\u6548\u80fd\u4e0e\u5353\u8d8a\u63a8\u7406\u7684\u6a21\u578b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff08DSA\uff09\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u957f\u6587\u672c\u5904\u7406\u7684\u6027\u80fd\u3002</li>\n    <li>\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7fDeepSeek-V3.2\u7684\u8868\u73b0\u63a5\u8fd1GPT-5\uff0c\u4e14\u9ad8\u8ba1\u7b97\u7248\u672c\u8d85\u8fc7GPT-5\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u5927\u89c4\u6a21\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff0c\u80fd\u7cfb\u7edf\u5730\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u589e\u5f3a\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6307\u4ee4\u6267\u884c\u80fd\u529b\u3002</li>\n    <li>\u57282025\u5e74\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\u548c\u56fd\u9645\u4fe1\u606f\u5b66\u5965\u6797\u5339\u514b\u4e2d\uff0cDeepSeek-V3.2\u5c55\u73b0\u4e86\u91d1\u724c\u6c34\u5e73\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that balances efficiency with strong reasoning and performance.</li>\n    <li>It features DeepSeek Sparse Attention (DSA), which makes attention processing faster while maintaining performance in long contexts.</li>\n    <li>The model uses an advanced reinforcement learning system that allows it to perform as well as GPT-5, and in some cases, even better.</li>\n    <li>DeepSeek-V3.2-Speciale outperforms GPT-5 and shows excellent reasoning skills, winning top awards in prestigious mathematical and informatics competitions.</li>\n    <li>It includes a new pipeline for creating training data, improving its ability to handle complex tasks and follow instructions effectively.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.22699", "authors": [{"_id": "692d06234397b1ec214f6788", "name": "Z-Image Team", "hidden": false}, {"_id": "692d06234397b1ec214f6789", "user": {"_id": "692d0e6bb14ceb758205d0dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/692d0e6bb14ceb758205d0dd/gGVq2KSJE11Sr3LkVn-n5.jpeg", "isPro": false, "fullname": "Huanqia Cai", "user": "Orion-Cai", "type": "user"}, "name": "Huanqia Cai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:26.669Z", "hidden": false}, {"_id": "692d06234397b1ec214f678a", "user": {"_id": "67777b7a8376dfe003afa951", "avatarUrl": "/avatars/2af9d3181306d4c53329d047eeadaf1e.svg", "isPro": false, "fullname": "Sihan Cao", "user": "Sihan-Cao", "type": "user"}, "name": "Sihan Cao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:33.191Z", "hidden": false}, {"_id": "692d06234397b1ec214f678b", "user": {"_id": "64a54586c0f13de8e7093314", "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg", "isPro": false, "fullname": "Ruoyi Du", "user": "RuoyiDu", "type": "user"}, "name": "Ruoyi Du", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:18:53.948Z", "hidden": false}, {"_id": "692d06234397b1ec214f678c", "name": "Peng Gao", "hidden": false}, {"_id": "692d06234397b1ec214f678d", "name": "Steven Hoi", "hidden": false}, {"_id": "692d06234397b1ec214f678e", "name": "Shijie Huang", "hidden": false}, {"_id": "692d06234397b1ec214f678f", "name": "Zhaohui Hou", "hidden": false}, {"_id": "692d06234397b1ec214f6790", "user": {"_id": "662a0f2d4bab737c1a279843", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662a0f2d4bab737c1a279843/fC2p3mjMHkVpDQdEqkuR4.png", "isPro": false, "fullname": "Dengyang Jiang", "user": "DyJiang", "type": "user"}, "name": "Dengyang Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:15.555Z", "hidden": false}, {"_id": "692d06234397b1ec214f6791", "user": {"_id": "6537e8eab01250d1d6efed3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gMx73gwdfEhcCFioStGCE.jpeg", "isPro": false, "fullname": "Xin", "user": "Srameo", "type": "user"}, "name": "Xin Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:15.288Z", "hidden": false}, {"_id": "692d06234397b1ec214f6792", "name": "Liangchen Li", "hidden": false}, {"_id": "692d06234397b1ec214f6793", "user": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "name": "Zhen Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:16.899Z", "hidden": false}, {"_id": "692d06234397b1ec214f6794", "user": {"_id": "6740a5730bb4a675446a80ad", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740a5730bb4a675446a80ad/dmruwMdQK3zluJm7YXUtN.jpeg", "isPro": false, "fullname": "Zhong-Yu Li", "user": "lzyhha", "type": "user"}, "name": "Zhong-Yu Li", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:08.972Z", "hidden": false}, {"_id": "692d06234397b1ec214f6795", "name": "David Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6796", "name": "Dongyang Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6797", "user": {"_id": "66332475351231c428653b6b", "avatarUrl": "/avatars/3997bcde54158f7ff9770c85a20875f1.svg", "isPro": false, "fullname": "Junhan Shi", "user": "jshmsjh", "type": "user"}, "name": "Junhan Shi", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:38.865Z", "hidden": false}, {"_id": "692d06234397b1ec214f6798", "user": {"_id": "64379d79fac5ea753f1c10f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png", "isPro": false, "fullname": "Jerry Wu", "user": "QJerry", "type": "user"}, "name": "Qilong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:18.709Z", "hidden": false}, {"_id": "692d06234397b1ec214f6799", "name": "Feng Yu", "hidden": false}, {"_id": "692d06234397b1ec214f679a", "name": "Chi Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679b", "name": "Shifeng Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679c", "user": {"_id": "641988978e0baaeed5a066c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641988978e0baaeed5a066c6/TdCjJ63gw5gdX1RqTvy9a.png", "isPro": false, "fullname": "Shilin", "user": "zsLin", "type": "user"}, "name": "Shilin Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T16:24:44.624Z", "hidden": false}], "publishedAt": "2025-11-27T18:52:07.000Z", "submittedOnDailyAt": "2025-12-01T00:38:17.269Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "submittedOnDailyBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "upvotes": 155, "discussionId": "692d06234397b1ec214f679d", "projectPage": "https://tongyi-mai.github.io/Z-Image-blog/", "githubRepo": "https://github.com/Tongyi-MAI/Z-Image", "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.", "ai_keywords": ["Scalable Single-Stream Diffusion Transformer", "S3-DiT", "diffusion transformer", "omni-pre-training", "instruction-following capabilities", "photorealistic image generation", "bilingual text rendering", "distillation scheme", "reward post-training", "H800 GPU", "VRAM"], "githubStars": 5595, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "summary_zh": "<ul>\n    <li>\u76ee\u524d\u9ad8\u6027\u80fd\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e3b\u8981\u7531\u5546\u4e1a\u7cfb\u7edf\u4e3b\u5bfc\uff0c\u5982Nano Banana Pro\u548cSeedream 4.0\u3002</li>\n    <li>\u4e00\u4e9b\u9886\u5148\u7684\u5f00\u6e90\u66ff\u4ee3\u54c1\u53c2\u6570\u91cf\u5de8\u5927\uff0820B\u523080B\uff09\uff0c\u4e0d\u9002\u5408\u5728\u666e\u901a\u786c\u4ef6\u4e0a\u4f7f\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Z-Image\uff0c\u4e00\u4e2a\u9ad8\u6548\u76846B\u53c2\u6570\u751f\u6210\u6a21\u578b\uff0c\u4f7f\u7528\u53ef\u6269\u5c55\u7684\u5355\u6d41\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u3002</li>\n    <li>Z-Image\u5728\u8bad\u7ec3\u65f6\u4f18\u5316\u4e86\u6574\u4e2a\u6a21\u578b\u751f\u547d\u5468\u671f\uff0c\u5b8c\u6210\u8bad\u7ec3\u4ec5\u9700314K H800 GPU\u5c0f\u65f6\u3002</li>\n    <li>\u6211\u4eec\u7684\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u53cc\u8bed\u6587\u672c\u6e32\u67d3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6548\u679c\u4e0e\u9876\u5c16\u5546\u4e1a\u6a21\u578b\u76f8\u5f53\uff0c\u5e76\u516c\u5f00\u53d1\u5e03\u4e86\u4ee3\u7801\u548c\u5728\u7ebf\u6f14\u793a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Many current high-performance image generation models are proprietary and require a lot of resources.</li>\n    <li>Leading open-source models are large (20B to 80B parameters), making them hard to use on regular hardware.</li>\n    <li>We introduce Z-Image, a more efficient model with only 6B parameters, designed for better accessibility.</li>\n    <li>Z-Image can run quickly on both high-end and consumer-grade hardware, thanks to its optimized training process.</li>\n    <li>Our model performs comparably or better than top competitors in generating realistic images and handling bilingual text, and we are sharing our work publicly to support accessible AI development.</li>\n</ul>"}, "publishedAt": "2025-11-27T13:52:07.000Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png", "numComments": 3, "submittedBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "fullname": "Zhen Li", "name": "Paper99", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 29}, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u6570\u636e\u9700\u6c42\u5feb\u901f\u589e\u957f\uff0c\u63a8\u52a8\u9ad8\u8d28\u91cf\u6570\u636e\u51c6\u5907\u7ba1\u9053\u7684\u53d1\u5c55\u3002</li>\n    <li>\u76ee\u524d\u7684\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u4e3b\u8981\u4f9d\u8d56\u4e34\u65f6\u811a\u672c\uff0c\u7f3a\u4e4f\u89c4\u8303\uff0c\u96be\u4ee5\u91cd\u590d\u548c\u4f18\u5316\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DataFlow\uff0c\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684LLM\u9a71\u52a8\u6570\u636e\u51c6\u5907\u6846\u67b6\u3002</li>\n    <li>DataFlow\u63d0\u4f9b\u8fd1200\u4e2a\u53ef\u91cd\u7528\u64cd\u4f5c\u7b26\u548c\u591a\u79cd\u9886\u57df\u901a\u7528\u7ba1\u9053\uff0c\u4ee5\u652f\u6301\u9ad8\u6548\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>DataFlow\u5728\u516d\u4e2a\u6848\u4f8b\u4e2d\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u4eba\u7c7b\u6570\u636e\u96c6\u548c\u4e13\u7528\u5408\u6210\u57fa\u7ebf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is a growing need for high-quality data in Large Language Models (LLMs), but current methods are often inconsistent and hard to reproduce.</li>\n    <li>DataFlow is a new framework that helps create better data preparation processes for LLMs, using modular and reusable components.</li>\n    <li>The framework includes almost 200 reusable tools and six general pipelines for various tasks like text processing and code generation.</li>\n    <li>DataFlow-Agent can convert simple language instructions into working data pipelines automatically.</li>\n    <li>In tests, DataFlow improved performance in multiple use cases, outperforming both human-curated datasets and other synthetic datasets.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.20626", "authors": [{"_id": "6927ab26243b2216fb75cd1b", "name": "Wei He", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1c", "user": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "name": "Kai Han", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:11.052Z", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1d", "name": "Hang Zhou", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1e", "name": "Hanting Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1f", "name": "Zhicheng Liu", "hidden": false}, {"_id": "6927ab26243b2216fb75cd20", "name": "Xinghao Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd21", "name": "Yunhe Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "publishedAt": "2025-11-25T18:48:05.000Z", "submittedOnDailyAt": "2025-11-26T23:08:13.066Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "submittedOnDailyBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "upvotes": 154, "discussionId": "6927ab27243b2216fb75cd22", "projectPage": "https://github.com/huawei-noah/noah-research/tree/master/ROOT", "githubRepo": "https://github.com/huawei-noah/noah-research", "ai_summary": "ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.", "ai_keywords": ["large language models", "LLMs", "momentum orthogonalization", "dimensional fragility", "outlier-induced noise", "adaptive Newton iterations", "proximal optimization", "Muon", "Adam-based optimizers", "robust optimizer"], "githubStars": 909, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u52a0\u5267\u4e86\u7b97\u6cd5\u4e0d\u7cbe\u786e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668ROOT\uff0c\u65e8\u5728\u901a\u8fc7\u53cc\u91cd\u7a33\u5065\u673a\u5236\u6765\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>ROOT\u91c7\u7528\u81ea\u9002\u5e94\u725b\u987f\u8fed\u4ee3\u7684\u7ef4\u5ea6\u7a33\u5065\u6b63\u4ea4\u5316\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4e0d\u540c\u67b6\u6784\u914d\u7f6e\u4e0b\u7684\u7cbe\u786e\u5ea6\u3002</li>\n    <li>ROOT\u8fd8\u901a\u8fc7\u90bb\u8fd1\u4f18\u5316\u6846\u67b6\u6291\u5236\u5f02\u5e38\u503c\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u610f\u4e49\u7684\u68af\u5ea6\u65b9\u5411\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cROOT\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e2d\u6bd4Muon\u548c\u57fa\u4e8eAdam\u7684\u4f18\u5316\u5668\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) face challenges with training stability and precision as they grow in size.</li>\n    <li>Recent optimizers have improved efficiency but struggle with robustness against noise and dimensional changes.</li>\n    <li>ROOT is a new optimizer that strengthens training stability using two innovative methods: dimension-robust orthogonalization and noise suppression.</li>\n    <li>Experiments show ROOT outperforms other optimizers like Muon and Adam in tough conditions, achieving faster training and better results.</li>\n    <li>The ROOT code will be available online for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:48:05.000Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20626.png", "numComments": 2, "submittedBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "fullname": "hankai", "name": "hankaixyz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20785", "authors": [{"_id": "692d430f4397b1ec214f696e", "user": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "name": "Zuhao Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:11.311Z", "hidden": false}, {"_id": "692d430f4397b1ec214f696f", "user": {"_id": "6690f58e2f9f6f9c88e91031", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6690f58e2f9f6f9c88e91031/QQ_VoEh7NlE6BUvii08zk.png", "isPro": false, "fullname": "Sudong Wang", "user": "xiao45791", "type": "user"}, "name": "Sudong Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:14.173Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6970", "user": {"_id": "64bb77e786e7fb5b8a317a43", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bb77e786e7fb5b8a317a43/J0jOrlZJ9gazdYaeSH2Bo.png", "isPro": false, "fullname": "kcz", "user": "kcz358", "type": "user"}, "name": "Kaichen Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T10:06:41.343Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6971", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:09.461Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6972", "name": "Sicong Leng", "hidden": false}, {"_id": "692d430f4397b1ec214f6973", "name": "Yifan Zhang", "hidden": false}, {"_id": "692d430f4397b1ec214f6974", "name": "Chengwei Qin", "hidden": false}, {"_id": "692d430f4397b1ec214f6975", "name": "Shijian Lu", "hidden": false}, {"_id": "692d430f4397b1ec214f6976", "name": "Xingxuan Li", "hidden": false}, {"_id": "692d430f4397b1ec214f6977", "user": {"_id": "6454685a548f22be598414c4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg", "isPro": false, "fullname": "Lidong Bing", "user": "LidongBing", "type": "user"}, "name": "Lidong Bing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:49:36.056Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "publishedAt": "2025-11-25T19:22:48.000Z", "submittedOnDailyAt": "2025-12-02T00:35:56.511Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "submittedOnDailyBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "upvotes": 148, "discussionId": "692d430f4397b1ec214f6978", "projectPage": "https://evolvinglmms-lab.github.io/LongVT/", "githubRepo": "https://github.com/EvolvingLMMs-Lab/LongVT", "ai_summary": "LongVT, an end-to-end framework, enhances long video reasoning by interleaving global and local analysis using multimodal tools, outperforming existing methods on challenging benchmarks.", "ai_keywords": ["multimodal models", "video reasoning", "textual Chain-of-Thought", "hallucinations", "long-form videos", "temporal grounding", "video cropping", "fine-grained question-answering", "VideoSIAH", "tool-integrated cold-start supervised fine-tuning", "agentic reinforcement learning", "agentic reinforcement fine-tuning"], "githubStars": 121, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u89c6\u9891\u63a8\u7406\u65b9\u9762\u5177\u6709\u5f88\u5927\u6f5c\u529b\uff0c\u4f46\u5bb9\u6613\u51fa\u73b0\u9519\u8bef\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6LongVT\uff0c\u6a21\u4eff\u4eba\u7c7b\u5728\u7406\u89e3\u957f\u89c6\u9891\u65f6\u7684\u601d\u7ef4\u65b9\u5f0f\uff0c\u901a\u8fc7\u89c6\u9891\u526a\u8f91\u9010\u6b65\u6df1\u5165\u3002</li>\n    <li>LongVT\u5229\u7528LMMs\u7684\u65f6\u95f4\u57fa\u7840\u80fd\u529b\uff0c\u805a\u7126\u4e8e\u7279\u5b9a\u89c6\u9891\u7247\u6bb5\uff0c\u5e76\u63d0\u53d6\u66f4\u7cbe\u7ec6\u7684\u89c6\u9891\u5e27\u3002</li>\n    <li>\u6211\u4eec\u5c06\u53d1\u5e03\u4e00\u4e2a\u540d\u4e3aVideoSIAH\u7684\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u957f\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002</li>\n    <li>LongVT\u5728\u56db\u4e2a\u6311\u6218\u6027\u7684\u957f\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u5f3a\u57fa\u7ebf\uff0c\u4ee3\u7801\u548c\u6570\u636e\u53ef\u516c\u5f00\u83b7\u53d6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large multimodal models (LMMs) can analyze videos with text reasoning, but they often make mistakes, especially with long videos.</li>\n    <li>LongVT is a new framework that helps these models understand long videos better by first looking at the whole video and then focusing on specific parts.</li>\n    <li>It uses LMMs' ability to identify time in videos to zoom in on relevant clips for more detailed analysis.</li>\n    <li>A new dataset called VideoSIAH will be released to help train and evaluate models for this long video reasoning task, containing over 247,000 training samples and 1,280 evaluation question-answer pairs.</li>\n    <li>LongVT has been tested and shown to perform better than existing models on various long-video understanding tasks, and all resources are available online.</li>\n</ul>"}, "publishedAt": "2025-11-25T14:22:48.000Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20785.png", "numComments": 3, "submittedBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "fullname": "Zuhao Yang", "name": "mwxely", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u5230\u8ba1\u7b97\u987a\u5e8f\u548c\u957f\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u9650\u5236\uff0c\u5f71\u54cd\u4e86\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u89d2\u8272\u5408\u6210\u7684\u5e94\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Live Avatar\uff0c\u8fd9\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u9ad8\u6e05\u6670\u5ea6\u7684\u89d2\u8272\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u7528\u4e86\u4e00\u4e2a140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c(TPP)\uff0c\u901a\u8fc7\u591a\u4e2aGPU\u6d41\u6c34\u7ebf\u5904\u7406\u53bb\u566a\u6b65\u9aa4\uff0c\u964d\u4f4e\u5ef6\u8fdf\u5e76\u786e\u4fdd\u5b9e\u65f6\u6d41\u7545\u3002</li>\n    <li>\u91c7\u7528\u6eda\u52a8\u6c89\u6d78\u6846\u67b6\u673a\u5236(RSFM)\uff0c\u4fdd\u6301\u5e8f\u5217\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u8eab\u4efd\u6f02\u79fb\u548c\u989c\u8272\u4f2a\u5f71\u3002</li>\n    <li>Live Avatar\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e8620 FPS\u7684\u7aef\u5230\u7aef\u751f\u6210\uff0c\u9996\u6b21\u5728\u6b64\u89c4\u6a21\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5b9e\u65f6\u89d2\u8272\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new system for creating high-quality avatars in real-time using a powerful 14-billion-parameter model.</li>\n    <li>It uses a technique called Timestep-forcing Pipeline Parallelism (TPP) to speed up the video generation process by using multiple GPUs.</li>\n    <li>The system includes a method called Rolling Sink Frame Mechanism (RSFM) to improve the consistency and quality of the avatars over time.</li>\n    <li>Live Avatar can generate videos at 20 frames per second using 5 GPUs, making it one of the fastest systems for real-time avatar creation.</li>\n    <li>This work sets a new standard for using advanced models in long-form video production, suitable for streaming applications.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp\u662f\u4e00\u4e2a\u5305\u542b210\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6a21\u62df\u771f\u5b9e\u4f01\u4e1a\u6570\u636e\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u9700\u8981\u5728\u5de5\u4e1a\u6a21\u5f0f\u4e0b\u8fdb\u884c\u591a\u7ea7SQL\u7ba1\u9053\u7684\u8bbe\u8ba1\u548c\u6784\u5efa\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\u4efb\u52a1\u6d89\u53ca\u5f00\u653e\u5f0f\u5546\u4e1a\u95ee\u9898\uff0c\u9700\u8981\u6218\u7565\u89c4\u5212\u548c\u63a2\u7d22\u6027\u5206\u6790\u3002</li>\n    <li>\u5b9e\u9a8c\u53d1\u73b0\uff0c\u73b0\u6709\u7684\u5148\u8fdb\u667a\u80fd\u4f53\u5728DE\u548cDA\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u90fd\u5f88\u5dee\uff0c\u6210\u529f\u7387\u4f4e\u4e8e20%\u548c40%\u3002</li>\n    <li>DAComp\u4e3a\u5f00\u53d1\u771f\u6b63\u9ad8\u6548\u7684\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DAComp is a benchmark with 210 tasks designed to simulate real-world data workflows in enterprises.</li>\n    <li>It includes data engineering tasks that involve building and improving SQL pipelines and managing data schemas.</li>\n    <li>Data analysis tasks require solving open-ended business problems through strategic planning and coding.</li>\n    <li>Current AI agents struggle with these tasks, especially in data engineering where success rates are below 20%.</li>\n    <li>DAComp helps identify weaknesses in AI performance, aiming to improve autonomous data agents for businesses.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni\u662f\u4e00\u4e2a\u901a\u7528\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u8be5\u7cfb\u7edf\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\uff0c\u63d0\u4f9b\u66f4\u6d41\u7545\u7684\u7528\u6237\u4f53\u9a8c\u3002</li>\n    <li>Kling-Omni\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5982\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\uff0c\u80fd\u591f\u521b\u5efa\u7535\u5f71\u8d28\u91cf\u7684\u89c6\u9891\u5185\u5bb9\u3002</li>\n    <li>\u5b83\u5efa\u7acb\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u6765\u589e\u5f3a\u529f\u80fd\u3002</li>\n    <li>Kling-Omni\u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from various input types like text, images, and videos.</li>\n    <li>It combines video generation, editing, and reasoning tasks into one seamless framework.</li>\n    <li>The system processes different inputs into a single format to produce smart and cinematic videos.</li>\n    <li>Kling-Omni is built on a strong data foundation and uses advanced training methods for better performance.</li>\n    <li>It shows great results in generating content, editing based on reasoning, and following multimodal instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.21631", "authors": [{"_id": "692ffb1a26742347f61daf38", "user": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "isPro": false, "fullname": "shuai bai", "user": "ShuaiBai623", "type": "user"}, "name": "Shuai Bai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:34:29.118Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf39", "name": "Yuxuan Cai", "hidden": false}, {"_id": "692ffb1a26742347f61daf3a", "name": "Ruizhe Chen", "hidden": false}, {"_id": "692ffb1a26742347f61daf3b", "name": "Keqin Chen", "hidden": false}, {"_id": "692ffb1a26742347f61daf3c", "user": {"_id": "63f30b870a16587ea970edfe", "avatarUrl": "/avatars/b58ab2d8a85a6d83462c297de2714ce4.svg", "isPro": false, "fullname": "Xiong-Hui Chen", "user": "xionghuichen", "type": "user"}, "name": "Xionghui Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:35:42.689Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf3d", "user": {"_id": "65b2529285b6c21448a10d65", "avatarUrl": "/avatars/1b09e2742aecce1bbdc57f0c4504cf38.svg", "isPro": false, "fullname": "Zesen Cheng", "user": "ClownRat", "type": "user"}, "name": "Zesen Cheng", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:35:51.365Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf3e", "name": "Lianghao Deng", "hidden": false}, {"_id": "692ffb1a26742347f61daf3f", "name": "Wei Ding", "hidden": false}, {"_id": "692ffb1a26742347f61daf40", "name": "Chang Gao", "hidden": false}, {"_id": "692ffb1a26742347f61daf41", "name": "Chunjiang Ge", "hidden": false}, {"_id": "692ffb1a26742347f61daf42", "name": "Wenbin Ge", "hidden": false}, {"_id": "692ffb1a26742347f61daf43", "name": "Zhifang Guo", "hidden": false}, {"_id": "692ffb1a26742347f61daf44", "user": {"_id": "656f1b21b075b63c90ba02ee", "avatarUrl": "/avatars/d6856815ef06261394178161e4d511b4.svg", "isPro": false, "fullname": "Huang Qidong", "user": "shikiw", "type": "user"}, "name": "Qidong Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:49.065Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf45", "name": "Jie Huang", "hidden": false}, {"_id": "692ffb1a26742347f61daf46", "name": "Fei Huang", "hidden": false}, {"_id": "692ffb1a26742347f61daf47", "name": "Binyuan Hui", "hidden": false}, {"_id": "692ffb1a26742347f61daf48", "name": "Shutong Jiang", "hidden": false}, {"_id": "692ffb1a26742347f61daf49", "name": "Zhaohai Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4a", "name": "Mingsheng Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4b", "name": "Mei Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4c", "user": {"_id": "6346be8f7fb9f11870c63998", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6346be8f7fb9f11870c63998/tFWawSkXL6bv1zgvzFWQd.png", "isPro": false, "fullname": "Kaixin Li", "user": "likaixin", "type": "user"}, "name": "Kaixin Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:53.648Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf4d", "user": {"_id": "67a31313cf9d856beb7f9afb", "avatarUrl": "/avatars/69395b134716f750545eab35a164e51f.svg", "isPro": false, "fullname": "Zicheng Lin", "user": "etonlin", "type": "user"}, "name": "Zicheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:36:50.803Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf4e", "name": "Junyang Lin", "hidden": false}, {"_id": "692ffb1a26742347f61daf4f", "name": "Xuejing Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf50", "name": "Jiawei Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf51", "name": "Chenglong Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf52", "name": "Yang Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf53", "name": "Dayiheng Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf54", "user": {"_id": "64e72776e9fc9d0475ef5188", "avatarUrl": "/avatars/d32b9d4e1da5486c3d5f9b04fa29d167.svg", "isPro": false, "fullname": "Shixuan Liu", "user": "liusx", "type": "user"}, "name": "Shixuan Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:58.470Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf55", "name": "Dunjie Lu", "hidden": false}, {"_id": "692ffb1a26742347f61daf56", "name": "Ruilin Luo", "hidden": false}, {"_id": "692ffb1a26742347f61daf57", "name": "Chenxu Lv", "hidden": false}, {"_id": "692ffb1a26742347f61daf58", "name": "Rui Men", "hidden": false}, {"_id": "692ffb1a26742347f61daf59", "name": "Lingchen Meng", "hidden": false}, {"_id": "692ffb1a26742347f61daf5a", "name": "Xuancheng Ren", "hidden": false}, {"_id": "692ffb1a26742347f61daf5b", "name": "Xingzhang Ren", "hidden": false}, {"_id": "692ffb1a26742347f61daf5c", "name": "Sibo Song", "hidden": false}, {"_id": "692ffb1a26742347f61daf5d", "name": "Yuchong Sun", "hidden": false}, {"_id": "692ffb1a26742347f61daf5e", "name": "Jun Tang", "hidden": false}, {"_id": "692ffb1a26742347f61daf5f", "name": "Jianhong Tu", "hidden": false}, {"_id": "692ffb1a26742347f61daf60", "name": "Jianqiang Wan", "hidden": false}, {"_id": "692ffb1a26742347f61daf61", "name": "Peng Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf62", "name": "Pengfei Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf63", "name": "Qiuyue Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf64", "name": "Yuxuan Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf65", "name": "Tianbao Xie", "hidden": false}, {"_id": "692ffb1a26742347f61daf66", "name": "Yiheng Xu", "hidden": false}, {"_id": "692ffb1a26742347f61daf67", "user": {"_id": "645b10e80c73ea27d13f7aca", "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg", "isPro": false, "fullname": "xuhaiyang", "user": "xhyandwyy", "type": "user"}, "name": "Haiyang Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:51.583Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf68", "name": "Jin Xu", "hidden": false}, {"_id": "692ffb1a26742347f61daf69", "name": "Zhibo Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6a", "name": "Mingkun Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6b", "name": "Jianxin Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6c", "name": "An Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6d", "name": "Bowen Yu", "hidden": false}, {"_id": "692ffb1a26742347f61daf6e", "name": "Fei Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6f", "name": "Hang Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf70", "name": "Xi Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf71", "name": "Bo Zheng", "hidden": false}, {"_id": "692ffb1a26742347f61daf72", "name": "Humen Zhong", "hidden": false}, {"_id": "692ffb1a26742347f61daf73", "name": "Jingren Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf74", "name": "Fan Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf75", "name": "Jing Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf76", "user": {"_id": "627d2723401f42c57b6b7c0c", "avatarUrl": "/avatars/6ff754e56aaee63d8572881a6a966171.svg", "isPro": false, "fullname": "Yuanzhi Zhu", "user": "Yuanzhi", "type": "user"}, "name": "Yuanzhi Zhu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:36:59.879Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf77", "name": "Ke Zhu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63451cf0a05b51f7ded25505/kVvzrCGrjwoK0CYh4DKif.jpeg"], "publishedAt": "2025-11-26T17:59:08.000Z", "submittedOnDailyAt": "2025-12-04T01:02:46.772Z", "title": "Qwen3-VL Technical Report", "submittedOnDailyBy": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "isPro": false, "fullname": "shuai bai", "user": "ShuaiBai623", "type": "user"}, "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.", "upvotes": 110, "discussionId": "692ffb1b26742347f61daf78", "ai_summary": "Qwen3-VL, a vision-language model, excels in text and multimodal understanding through advanced architectures and larger contexts, achieving superior performance across benchmarks.", "ai_keywords": ["vision-language model", "interleaved contexts", "multimodal benchmarks", "dense variants", "mixture-of-experts", "pure-text understanding", "long-context comprehension", "multimodal reasoning", "MMMU", "visual-math benchmarks", "interleaved-MRoPE", "DeepStack", "text-based time alignment", "T-RoPE", "explicit textual timestamp alignment", "vision-language alignment", "image-grounded reasoning", "agentic decision-making", "multimodal code intelligence"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>Qwen3-VL \u662f Qwen \u7cfb\u5217\u4e2d\u6700\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u8868\u73b0\u4f18\u8d8a\uff0c\u652f\u6301\u591a\u79cd\u6a21\u5f0f\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002</li>\n    <li>\u8be5\u6a21\u578b\u652f\u6301\u6700\u591a 256K \u5b57\u7b26\u7684\u4ea4\u9519\u4e0a\u4e0b\u6587\uff0c\u53ef\u4ee5\u65e0\u7f1d\u6574\u5408\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u3002</li>\n    <li>Qwen3-VL \u63d0\u4f9b\u66f4\u5f3a\u7684\u6587\u672c\u7406\u89e3\u80fd\u529b\uff0c\u80fd\u591f\u5904\u7406\u957f\u6587\u672c\u548c\u591a\u6a21\u6001\u8f93\u5165\uff0c\u9002\u7528\u4e8e\u957f\u6587\u6863\u548c\u89c6\u9891\u7684\u4fdd\u7559\u4e0e\u68c0\u7d22\u3002</li>\n    <li>\u6a21\u578b\u5728\u5355\u56fe\u50cf\u3001\u591a\u56fe\u50cf\u548c\u89c6\u9891\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5148\u8fdb\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u8868\u73b0\u9886\u5148\u3002</li>\n    <li>\u5f15\u5165\u4e86\u4e09\u9879\u5173\u952e\u5347\u7ea7\uff0c\u63d0\u9ad8\u4e86\u56fe\u50cf\u548c\u89c6\u9891\u7684\u7a7a\u95f4\u65f6\u95f4\u5efa\u6a21\u80fd\u529b\uff0c\u589e\u5f3a\u4e86\u89c6\u89c9\u8bed\u8a00\u7684\u5bf9\u9f50\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Qwen3-VL is a new advanced vision-language model that performs well on many tests involving text, images, and video.</li>\n    <li>It can handle up to 256,000 tokens at once, making it great for understanding long documents and videos.</li>\n    <li>The model comes in different sizes to balance speed and quality, including versions with up to 235 billion parameters.</li>\n    <li>Qwen3-VL excels at understanding text, managing long contexts, and reasoning with multiple types of media, often outperforming similar models.</li>\n    <li>It includes new architectural features for better modeling of images and videos, and is designed to support various practical applications in real-world tasks.</li>\n</ul>"}, "publishedAt": "2025-11-26T12:59:08.000Z", "title": "Qwen3-VL Technical Report", "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63451cf0a05b51f7ded25505/kVvzrCGrjwoK0CYh4DKif.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21631.png", "numComments": 3, "submittedBy": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "fullname": "shuai bai", "name": "ShuaiBai623", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 42}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.21689", "authors": [{"_id": "692f11ffbfc6eea1b6fb6900", "name": "Hongjin Su", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6901", "name": "Shizhe Diao", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6902", "name": "Ximing Lu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6903", "name": "Mingjie Liu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6904", "name": "Jiacheng Xu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6905", "name": "Xin Dong", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6906", "name": "Yonggan Fu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6907", "name": "Peter Belcak", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6908", "name": "Hanrong Ye", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb6909", "name": "Hongxu Yin", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690a", "name": "Yi Dong", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690b", "name": "Evelina Bakhturina", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690c", "name": "Tao Yu", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690d", "name": "Yejin Choi", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690e", "name": "Jan Kautz", "hidden": false}, {"_id": "692f11ffbfc6eea1b6fb690f", "name": "Pavlo Molchanov", "hidden": false}], "publishedAt": "2025-11-26T18:59:46.000Z", "submittedOnDailyAt": "2025-12-03T13:46:43.945Z", "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration", "submittedOnDailyBy": {"_id": "633bd54b00732349209a18fe", "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg", "isPro": false, "fullname": "Shizhe Diao", "user": "shizhediao", "type": "user"}, "summary": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.", "upvotes": 96, "discussionId": "692f11ffbfc6eea1b6fb6910", "projectPage": "https://research.nvidia.com/labs/lpr/ToolOrchestra/", "githubRepo": "https://github.com/NVlabs/ToolOrchestra/", "ai_summary": "A small orchestrator using ToolOrchestra method coordinates various intelligent tools with reinforcement learning, achieving higher accuracy and efficiency in solving complex tasks like Humanity's Last Exam compared to larger models.", "ai_keywords": ["large language models", "ToolOrchestra", "reinforcement learning", "outcome-aware rewards", "efficiency-aware rewards", "user-preference-aware rewards", "Orchestrator", "tool-use agents", "humanity's last exam", "tau2-bench", "FRAMES", "tool-augmented reasoning systems"], "githubStars": 297, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5f3a\u5927\uff0c\u4f46\u89e3\u51b3\u590d\u6742\u95ee\u9898\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86ToolOrchestra\uff0c\u4e00\u79cd\u8bad\u7ec3\u5c0f\u578b\u534f\u8c03\u8005\u7684\u65b9\u6cd5\uff0c\u4ee5\u7ba1\u7406\u667a\u80fd\u5de5\u5177\u5e76\u63d0\u9ad8\u89e3\u51b3\u56f0\u96be\u4efb\u52a1\u7684\u6548\u7387\u3002</li>\n    <li>\u901a\u8fc7ToolOrchestra\uff0c\u6211\u4eec\u5f00\u53d1\u4e86Orchestrator\uff0c\u4e00\u4e2a8B\u6a21\u578b\uff0c\u5728\u6210\u672c\u66f4\u4f4e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>Orchestrator\u5728\u201c\u4eba\u7c7b\u6700\u540e\u8003\u8bd5\u201d(HLE)\u4e0a\u5f97\u520637.1%\uff0c\u8d85\u8d8a\u4e86GPT-5\u768435.1%\uff0c\u4e14\u6548\u7387\u662f\u51762.5\u500d\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0cOrchestrator\u5728\u6027\u80fd\u4e0e\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u5e76\u80fd\u6709\u6548\u9002\u5e94\u65b0\u7684\u5de5\u5177\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models are good at many tasks but struggle with very complex problems.</li>\n    <li>ToolOrchestra is a new method that helps small models manage and coordinate various intelligent tools efficiently.</li>\n    <li>Using ToolOrchestra, a model called Orchestrator was created, which is more accurate and cheaper than previous models.</li>\n    <li>Orchestrator scored 37.1% on a tough exam, beating GPT-5's score of 35.1% while being 2.5 times more efficient.</li>\n    <li>The results show that using a small orchestrator with different tools is a better approach for solving difficult tasks.</li>\n</ul>"}, "publishedAt": "2025-11-26T13:59:46.000Z", "title": "ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration", "summary": "Large language models are powerful generalists, yet solving deep and complex problems such as those of the Humanity's Last Exam (HLE) remains both conceptually challenging and computationally expensive. We show that small orchestrators managing other models and a variety of tools can both push the upper bound of intelligence and improve efficiency in solving difficult agentic tasks. We introduce ToolOrchestra, a method for training small orchestrators that coordinate intelligent tools. ToolOrchestra explicitly uses reinforcement learning with outcome-, efficiency-, and user-preference-aware rewards. Using ToolOrchestra, we produce Orchestrator, an 8B model that achieves higher accuracy at lower cost than previous tool-use agents while aligning with user preferences on which tools are to be used for a given query. On HLE, Orchestrator achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being 2.5x more efficient. On tau2-Bench and FRAMES, Orchestrator surpasses GPT-5 by a wide margin while using only about 30% of the cost. Extensive analysis shows that Orchestrator achieves the best trade-off between performance and cost under multiple metrics, and generalizes robustly to unseen tools. These results demonstrate that composing diverse tools with a lightweight orchestration model is both more efficient and more effective than existing methods, paving the way for practical and scalable tool-augmented reasoning systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21689.png", "numComments": 3, "submittedBy": {"_id": "633bd54b00732349209a18fe", "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg", "fullname": "Shizhe Diao", "name": "shizhediao", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Dec 24, 2025";
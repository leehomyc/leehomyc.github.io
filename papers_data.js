window.trendingPapers = {
    "today": [{"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u5c3d\u7ba1\u79d1\u5b66\u4eba\u5de5\u667a\u80fd\u6709\u6240\u8fdb\u6b65\uff0c\u4f46\u8fd8\u6ca1\u6709\u4e00\u4e2a\u7edf\u4e00\u7684\u79d1\u5b66\u901a\u7528\u667a\u80fd\uff08SGI\uff09\u6846\u67b6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b9e\u7528\u63a2\u7a76\u6a21\u578b\uff08PIM\uff09\u7684SGI\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u79d1\u5b66\u5bb6\u76f8\u5173\u7684\u4efb\u52a1\u8fdb\u884c\u64cd\u4f5c\u5316\u3002</li>\n    <li>SGI-Bench\u5305\u542b1000\u591a\u4e2a\u8de8\u5b66\u79d1\u7684\u6837\u672c\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u6700\u65b0\u7684\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u663e\u793a\u5728\u6df1\u5ea6\u7814\u7a76\u548c\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u9762\u5b58\u5728\u591a\u4e2a\u4e0d\u8db3\u4e4b\u5904\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\uff0c\u4ee5\u4f18\u5316\u5047\u8bbe\u7684\u65b0\u9896\u6027\uff0c\u652f\u6301AI\u7cfb\u7edf\u53c2\u4e0e\u79d1\u5b66\u53d1\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is no clear framework for Scientific General Intelligence (SGI), which is the ability of AI to think and work across different scientific fields.</li>\n    <li>We define SGI using the Practical Inquiry Model and create tasks that align with how scientists work, including research and experiments.</li>\n    <li>SGI-Bench includes over 1,000 examples from various scientific fields to test AI performance on important scientific questions.</li>\n    <li>Results show significant gaps in AI performance, like low accuracy in deep research and issues with executing experiments properly.</li>\n    <li>We propose Test-Time Reinforcement Learning (TTRL) to improve AI's ability to generate new ideas during testing without relying on pre-set answers.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16793", "authors": [{"_id": "6944ba3efbf17e708e185f60", "user": {"_id": "6944c28d3cd5eeb7a7838663", "avatarUrl": "/avatars/63f9f8c4e8462cbc9726bdee249fbcde.svg", "isPro": false, "fullname": "lin", "user": "birdxp", "type": "user"}, "name": "Xiaopeng Lin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:14.947Z", "hidden": false}, {"_id": "6944ba3efbf17e708e185f61", "user": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "isPro": false, "fullname": "Shijie Lian", "user": "LiamLian0727", "type": "user"}, "name": "Shijie Lian", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:18.093Z", "hidden": false}, {"_id": "6944ba3efbf17e708e185f62", "user": {"_id": "63d3b5f1640bb0f77173baea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674819020331-noauth.jpeg", "isPro": false, "fullname": "yubin", "user": "VLyb", "type": "user"}, "name": "Bin Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T11:00:33.691Z", "hidden": false}, {"_id": "6944ba3efbf17e708e185f63", "user": {"_id": "684a7f750fff63d29d18444c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kXTIoUPOM7LDztBcRL6YI.png", "isPro": false, "fullname": "Ruoqi Yang", "user": "lyceen", "type": "user"}, "name": "Ruoqi Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T11:00:35.617Z", "hidden": false}, {"_id": "6944ba3efbf17e708e185f64", "name": "Changti Wu", "hidden": false}, {"_id": "6944ba3efbf17e708e185f65", "name": "Yuzhuo Miao", "hidden": false}, {"_id": "6944ba3efbf17e708e185f66", "name": "Yurun Jin", "hidden": false}, {"_id": "6944ba3efbf17e708e185f67", "name": "Yukun Shi", "hidden": false}, {"_id": "6944ba3efbf17e708e185f68", "name": "Cong Huang", "hidden": false}, {"_id": "6944ba3efbf17e708e185f69", "name": "Bojun Cheng", "hidden": false}, {"_id": "6944ba3efbf17e708e185f6a", "name": "Kai Chen", "hidden": false}], "publishedAt": "2025-12-18T17:27:03.000Z", "submittedOnDailyAt": "2025-12-22T00:59:36.650Z", "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence", "submittedOnDailyBy": {"_id": "6944c28d3cd5eeb7a7838663", "avatarUrl": "/avatars/63f9f8c4e8462cbc9726bdee249fbcde.svg", "isPro": false, "fullname": "lin", "user": "birdxp", "type": "user"}, "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.", "upvotes": 63, "discussionId": "6944ba3efbf17e708e185f6b", "projectPage": "https://zgc-embodyai.github.io/PhysBrain/", "ai_summary": "Proposed Egocentric2Embodiment pipeline translates human egocentric videos into structured training data for robots, enhancing their egocentric understanding and task performance.", "ai_keywords": ["Egocentric2Embodiment", "VQA supervision", "evidence grounding", "temporal consistency", "Egocentric2Embodiment dataset", "E2E-3M", "PhysBrain", "egocentric-aware", "VLA fine-tuning", "SimplerEnv success rates"], "organization": {"_id": "6948d884070dda0c2ae35a78", "name": "DeepCybo", "fullname": "DeepCybo", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65ec01fd770aa0e25d9374dc/QOsz6P_7AxyqGrjsRHTGk.png"}, "summary_zh": "<ul>\n    <li>\u673a\u5668\u4eba\u7684\u4e00\u822c\u5316\u4f9d\u8d56\u4e8e\u7269\u7406\u667a\u80fd\uff0c\u5305\u62ec\u5bf9\u72b6\u6001\u53d8\u5316\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u4ea4\u4e92\u548c\u957f\u671f\u89c4\u5212\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5927\u591a\u6570\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e3b\u8981\u57fa\u4e8e\u7b2c\u4e09\u4eba\u79f0\u6570\u636e\u8bad\u7ec3\uff0c\u8fd9\u5bfc\u81f4\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u89c6\u89d2\u7684\u4e0d\u5339\u914d\u3002</li>\n    <li>\u6536\u96c6\u673a\u5668\u4eba\u7b2c\u4e00\u4eba\u79f0\u6570\u636e\u6210\u672c\u9ad8\u4e14\u591a\u6837\u6027\u6709\u9650\uff0c\u800c\u5927\u89c4\u6a21\u7684\u4eba\u7c7b\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u8f6c\u6362\u4e3a\u591a\u5c42\u6b21\u3001\u7ed3\u6784\u5316\u8bad\u7ec3\u76d1\u7763\u7684Egocentric2Embodiment\u7ffb\u8bd1\u7ba1\u9053\uff0c\u5e76\u6784\u5efa\u4e86E2E-3M\u6570\u636e\u96c6\u3002</li>\n    <li>PhysBrain\u662f\u57fa\u4e8eE2E-3M\u6570\u636e\u96c6\u8bad\u7ec3\u7684\uff0c\u5c55\u73b0\u4e86\u663e\u8457\u7684\u7b2c\u4e00\u4eba\u79f0\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u63a7\u5236\u7684\u6548\u7387\u548c\u6210\u529f\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Robots need to understand their own actions and interactions in their environment to generalize effectively.</li>\n    <li>Most existing models are trained on third-person data, which doesn't match how humanoid robots perceive their surroundings.</li>\n    <li>Collecting robot-generated egocentric data is costly and has limited variety; using human egocentric videos is a better, scalable option.</li>\n    <li>We developed a system called Egocentric2Embodiment to convert first-person videos into structured training data for robots.</li>\n    <li>The resulting PhysBrain model shows improved understanding and performance in robot tasks, benefiting from human-like perspectives in training.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:27:03.000Z", "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence", "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16793.png", "numComments": 2, "submittedBy": {"_id": "6944c28d3cd5eeb7a7838663", "avatarUrl": "/avatars/63f9f8c4e8462cbc9726bdee249fbcde.svg", "fullname": "lin", "name": "birdxp", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "6948d884070dda0c2ae35a78", "name": "DeepCybo", "fullname": "DeepCybo", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65ec01fd770aa0e25d9374dc/QOsz6P_7AxyqGrjsRHTGk.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.17901", "authors": [{"_id": "6948af7534f46eaf46cbb1da", "user": {"_id": "6719bfd07c6e6c83a388aeae", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719bfd07c6e6c83a388aeae/jHxryk04dzHo23TX5F5sz.png", "isPro": false, "fullname": "Junyu Zhang", "user": "jyzhang1208", "type": "user"}, "name": "Junyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:53.009Z", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1db", "name": "Yifan Sun", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1dc", "name": "Tianang Leng", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1dd", "name": "Jingyan Shen", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1de", "name": "Liu Ziyin", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1df", "name": "Paul Pu Liang", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1e0", "name": "Huan Zhang", "hidden": false}], "publishedAt": "2025-12-19T18:59:11.000Z", "submittedOnDailyAt": "2025-12-22T00:10:07.525Z", "title": "When Reasoning Meets Its Laws", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/", "upvotes": 48, "discussionId": "6948af7534f46eaf46cbb1e1", "projectPage": "https://lore-project.github.io/", "githubRepo": "https://github.com/ASTRAL-Group/LoRe", "githubRepoAddedBy": "user", "ai_summary": "A framework called Laws of Reasoning (LoRe) is introduced to theoretically define desired reasoning behaviors in Large Reasoning Models, with a focus on compute and accuracy laws, and a benchmark (LoRe-Bench) to measure these properties.", "ai_keywords": ["Laws of Reasoning", "LoRe", "compute law", "accuracy law", "question complexity", "monotonicity", "compositionality", "LoRe-Bench", "finetuning approach", "compute-law compositionality"], "githubStars": 15, "summary_zh": "<ul>\n    <li>\u5c3d\u7ba1\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u7684\u63a8\u7406\u884c\u4e3a\u5e38\u5e38\u4e0d\u7b26\u5408\u76f4\u89c9\uff0c\u5bfc\u81f4\u63a8\u7406\u80fd\u529b\u4e0d\u7406\u60f3\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u63a8\u7406\u6cd5\u5219\uff08LoRe\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u63cf\u8ff0LRMs\u4e2d\u7684\u5185\u5728\u63a8\u7406\u6a21\u5f0f\u3002</li>\n    <li>\u6211\u4eec\u9996\u5148\u63d0\u51fa\u8ba1\u7b97\u6cd5\u5219\uff0c\u5047\u8bbe\u63a8\u7406\u8ba1\u7b97\u5e94\u4e0e\u95ee\u9898\u590d\u6742\u6027\u7ebf\u6027\u589e\u957f\uff0c\u5e76\u6269\u5c55\u4e86\u51c6\u786e\u6027\u6cd5\u5219\u3002</li>\n    <li>\u7531\u4e8e\u95ee\u9898\u590d\u6742\u6027\u96be\u4ee5\u91cf\u5316\uff0c\u6211\u4eec\u901a\u8fc7\u5355\u8c03\u6027\u548c\u7ec4\u5408\u6027\u8fd9\u4e24\u4e2a\u7279\u6027\u6765\u68c0\u9a8c\u8fd9\u4e9b\u5047\u8bbe\u3002</li>\n    <li>\u8bc4\u4f30\u8868\u660e\uff0c\u5927\u591a\u6570\u63a8\u7406\u6a21\u578b\u8868\u73b0\u51fa\u5408\u7406\u7684\u5355\u8c03\u6027\uff0c\u4f46\u7f3a\u4e4f\u7ec4\u5408\u6027\uff0c\u56e0\u6b64\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\u6765\u589e\u5f3a\u7ec4\u5408\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Reasoning Models (LRMs) often have confusing reasoning patterns that can lead to poor performance.</li>\n    <li>The study introduces the Laws of Reasoning (LoRe), a framework to understand and improve reasoning in LRMs.</li>\n    <li>LoRe includes a compute law that suggests reasoning should increase steadily with question complexity.</li>\n    <li>Researchers created LoRe-Bench to test two key properties of reasoning: monotonicity and compositionality.</li>\n    <li>Findings show most models are monotonic but struggle with compositionality; a new finetuning method improves reasoning performance significantly.</li>\n</ul>"}, "publishedAt": "2025-12-19T13:59:11.000Z", "title": "When Reasoning Meets Its Laws", "summary": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17901.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.17260", "authors": [{"_id": "6948afc434f46eaf46cbb1f1", "name": "Jiangjie Chen", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1f2", "name": "Wenxiang Chen", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1f3", "name": "Jiacheng Du", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1f4", "user": {"_id": "637a06580a77f602dc4ac922", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637a06580a77f602dc4ac922/qOJAhHOEE2N-HzRcZOu1L.jpeg", "isPro": false, "fullname": "Jinyi Hu", "user": "JamesHujy", "type": "user"}, "name": "Jinyi Hu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:50.739Z", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1f5", "name": "Zhicheng Jiang", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1f6", "user": {"_id": "5f729da2f1e7ef6e919a1c27", "avatarUrl": "/avatars/bdf48f0d3290fab54d7e9636a3d14158.svg", "isPro": false, "fullname": "Zhanming (Allan) Jie", "user": "allanjie", "type": "user"}, "name": "Allan Jie", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:49.005Z", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1f7", "name": "Xiaoran Jin", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1f8", "name": "Xing Jin", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1f9", "name": "Chenggang Li", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1fa", "name": "Wenlei Shi", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1fb", "name": "Zhihong Wang", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1fc", "name": "Mingxuan Wang", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1fd", "name": "Chenrui Wei", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1fe", "name": "Shufa Wei", "hidden": false}, {"_id": "6948afc434f46eaf46cbb1ff", "name": "Huajian Xin", "hidden": false}, {"_id": "6948afc434f46eaf46cbb200", "name": "Fan Yang", "hidden": false}, {"_id": "6948afc434f46eaf46cbb201", "name": "Weihao Gao", "hidden": false}, {"_id": "6948afc434f46eaf46cbb202", "name": "Zheng Yuan", "hidden": false}, {"_id": "6948afc434f46eaf46cbb203", "name": "Tianyang Zhan", "hidden": false}, {"_id": "6948afc434f46eaf46cbb204", "name": "Zeyu Zheng", "hidden": false}, {"_id": "6948afc434f46eaf46cbb205", "name": "Tianxi Zhou", "hidden": false}, {"_id": "6948afc434f46eaf46cbb206", "name": "Thomas Hanwen Zhu", "hidden": false}], "publishedAt": "2025-12-19T06:19:55.000Z", "submittedOnDailyAt": "2025-12-22T00:11:16.085Z", "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present Seed-Prover 1.5, a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves 88\\% of PutnamBench (undergraduate-level), 80\\% of Fate-H (graduate-level), and 33\\% of Fate-X (PhD-level) problems. Notably, using our system, we solved 11 out of 12 problems from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.", "upvotes": 37, "discussionId": "6948afc534f46eaf46cbb207", "ai_summary": "Seed-Prover 1.5, a formal theorem-proving model using large-scale agentic reinforcement learning and an efficient test-time scaling workflow, demonstrates superior performance in solving mathematical problems across various levels with reduced computational resources.", "ai_keywords": ["large language models", "theorem proving", "formal languages", "Lean", "reinforcement learning", "test-time scaling", "PutnamBench", "Fate-H", "Fate-X", "formal mathematical reasoning"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u4e25\u8c28\u7684\u6570\u5b66\u8bc1\u660e\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002</li>\n    <li>Seed-Prover 1.5 \u662f\u4e00\u79cd\u901a\u8fc7\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u5f62\u5f0f\u5b9a\u7406\u8bc1\u660e\u6a21\u578b\uff0c\u5177\u6709\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u4e0e Lean \u7b49\u5de5\u5177\u7684\u4e92\u52a8\u4e2d\u4e0d\u65ad\u79ef\u7d2f\u7ecf\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f62\u5f0f\u5b9a\u7406\u8bc1\u660e\u7684\u80fd\u529b\u548c\u6548\u7387\u3002</li>\n    <li>Seed-Prover 1.5 \u5728\u8ba1\u7b97\u8d44\u6e90\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u89e3\u51b3\u4e86 88% \u7684\u672c\u79d1\u6c34\u5e73\u95ee\u9898\uff0c80% \u7684\u7814\u7a76\u751f\u6c34\u5e73\u95ee\u9898\uff0c\u4ee5\u53ca 33% \u7684\u535a\u58eb\u751f\u6c34\u5e73\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u7684\u7cfb\u7edf\u5728 9 \u5c0f\u65f6\u5185\u89e3\u51b3\u4e86 Putnam 2025 \u4e2d\u7684 11 \u4e2a\u95ee\u9898\uff0c\u663e\u793a\u4e86\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u7684\u91cd\u8981\u6027\u548c\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models have improved in generating mathematical proofs, but using them for formal theorem proving is still tough and costly.</li>\n    <li>Seed-Prover 1.5 is a new model developed to enhance formal theorem proving using reinforcement learning and a new efficient workflow.</li>\n    <li>The model learns from its interactions with formal tools like Lean, improving its performance over time.</li>\n    <li>Seed-Prover 1.5 outperforms other methods while using less computing power, solving many undergraduate and graduate-level problems effectively.</li>\n    <li>The model achieved significant success, solving most problems from a recent competition within a limited time, indicating a promising future for formal mathematical reasoning.</li>\n</ul>"}, "publishedAt": "2025-12-19T01:19:55.000Z", "title": "Seed-Prover 1.5: Mastering Undergraduate-Level Theorem Proving via Learning from Experience", "summary": "Large language models have recently made significant progress to generate rigorous mathematical proofs. In contrast, utilizing LLMs for theorem proving in formal languages (such as Lean) remains challenging and computationally expensive, particularly when addressing problems at the undergraduate level and beyond. In this work, we present Seed-Prover 1.5, a formal theorem-proving model trained via large-scale agentic reinforcement learning, alongside an efficient test-time scaling (TTS) workflow. Through extensive interactions with Lean and other tools, the model continuously accumulates experience during the RL process, substantially enhancing the capability and efficiency of formal theorem proving. Furthermore, leveraging recent advancements in natural language proving, our TTS workflow efficiently bridges the gap between natural and formal languages. Compared to state-of-the-art methods, Seed-Prover 1.5 achieves superior performance with a smaller compute budget. It solves 88\\% of PutnamBench (undergraduate-level), 80\\% of Fate-H (graduate-level), and 33\\% of Fate-X (PhD-level) problems. Notably, using our system, we solved 11 out of 12 problems from Putnam 2025 within 9 hours. Our findings suggest that scaling learning from experience, driven by high-quality formal feedback, holds immense potential for the future of formal mathematical reasoning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17260.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.17909", "authors": [{"_id": "6948d11034f46eaf46cbb338", "name": "Shilong Zhang", "hidden": false}, {"_id": "6948d11034f46eaf46cbb339", "name": "He Zhang", "hidden": false}, {"_id": "6948d11034f46eaf46cbb33a", "name": "Zhifei Zhang", "hidden": false}, {"_id": "6948d11034f46eaf46cbb33b", "name": "Chongjian Ge", "hidden": false}, {"_id": "6948d11034f46eaf46cbb33c", "name": "Shuchen Xue", "hidden": false}, {"_id": "6948d11034f46eaf46cbb33d", "name": "Shaoteng Liu", "hidden": false}, {"_id": "6948d11034f46eaf46cbb33e", "name": "Mengwei Ren", "hidden": false}, {"_id": "6948d11034f46eaf46cbb33f", "name": "Soo Ye Kim", "hidden": false}, {"_id": "6948d11034f46eaf46cbb340", "name": "Yuqian Zhou", "hidden": false}, {"_id": "6948d11034f46eaf46cbb341", "name": "Qing Liu", "hidden": false}, {"_id": "6948d11034f46eaf46cbb342", "name": "Daniil Pakhomov", "hidden": false}, {"_id": "6948d11034f46eaf46cbb343", "name": "Kai Zhang", "hidden": false}, {"_id": "6948d11034f46eaf46cbb344", "name": "Zhe Lin", "hidden": false}, {"_id": "6948d11034f46eaf46cbb345", "name": "Ping Luo", "hidden": false}], "publishedAt": "2025-12-19T18:59:57.000Z", "submittedOnDailyAt": "2025-12-22T03:13:08.364Z", "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing", "submittedOnDailyBy": {"_id": "6424ffce46d202ad3d918a67", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6424ffce46d202ad3d918a67/gmYmOA072fP_5cJLc9Qs4.jpeg", "isPro": false, "fullname": "Shilong Zhang", "user": "shilongz", "type": "user"}, "summary": "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.", "upvotes": 29, "discussionId": "6948d11034f46eaf46cbb346", "projectPage": "https://jshilong.github.io/PS-VAE-PAGE/", "ai_summary": "A systematic framework adapts understanding-oriented encoder features for generative tasks by introducing a semantic-pixel reconstruction objective, enabling state-of-the-art image reconstruction and generation.", "ai_keywords": ["Latent Diffusion Models", "VAE", "latent space", "generative latents", "discriminative feature space", "off-manifold latents", "pixel-level reconstruction", "semantic-pixel reconstruction objective", "Text-to-Image", "image editing", "feature spaces"], "organization": {"_id": "61e5d14f77496de0a6d95c6b", "name": "adobe", "fullname": "Adobe", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u4ee3\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDMs\uff09\u901a\u5e38\u5728\u4f4e\u7ea7\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5de5\u4f5c\uff0c\u4e3b\u8981\u4f18\u5316\u50cf\u7d20\u7ea7\u91cd\u5efa\u3002</li>\n    <li>\u91c7\u7528\u9ad8\u7ef4\u7279\u5f81\u4f5c\u4e3a\u751f\u6210\u6f5c\u5728\u7a7a\u95f4\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u7279\u5f81\u7a7a\u95f4\u7f3a\u4e4f\u7d27\u51d1\u7684\u6b63\u5219\u5316\uff0c\u5bfc\u81f4\u751f\u6210\u6a21\u578b\u751f\u6210\u4e0d\u51c6\u786e\u7684\u5bf9\u8c61\u7ed3\u6784\uff1b\u7f16\u7801\u5668\u7684\u50cf\u7d20\u7ea7\u91cd\u5efa\u80fd\u529b\u5f31\uff0c\u5f71\u54cd\u751f\u6210\u5668\u5b66\u4e60\u7ec6\u81f4\u7684\u51e0\u4f55\u5f62\u72b6\u548c\u7eb9\u7406\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u7cfb\u7edf\u6846\u67b6\uff0c\u5c06\u7406\u89e3\u5bfc\u5411\u7684\u7f16\u7801\u5668\u7279\u5f81\u9002\u5e94\u4e8e\u751f\u6210\u4efb\u52a1\uff0c\u4f7f\u7528\u8bed\u4e49\u50cf\u7d20\u91cd\u5efa\u76ee\u6807\u6765\u6b63\u5219\u5316\u6f5c\u5728\u7a7a\u95f4\u3002</li>\n    <li>\u8fd9\u79cd\u8bbe\u8ba1\u786e\u4fdd\u6f5c\u5728\u7a7a\u95f4\u8bed\u4e49\u4e30\u5bcc\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u91cd\u5efa\uff0c\u540c\u65f6\u4fdd\u6301\u8db3\u591f\u7d27\u51d1\u4ee5\u4fbf\u4e8e\u51c6\u786e\u751f\u6210\u3002</li>\n    <li>\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u548c\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u8fd9\u79cd\u65b9\u6cd5\u5728\u91cd\u5efa\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u7684\u663e\u8457\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Modern Latent Diffusion Models often use low-level features that focus on pixel-level detail.</li>\n    <li>New trends are using high-dimensional features from representation encoders for better vision tasks.</li>\n    <li>Two main challenges are identified: weak reconstruction of images and inaccuracies in object structure.</li>\n    <li>The paper proposes a new framework that improves the use of encoder features for generating images.</li>\n    <li>This approach leads to better image quality and performance in text-to-image and editing tasks.</li>\n</ul>"}, "publishedAt": "2025-12-19T13:59:57.000Z", "title": "Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing", "summary": "Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17909.png", "numComments": 2, "submittedBy": {"_id": "6424ffce46d202ad3d918a67", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6424ffce46d202ad3d918a67/gmYmOA072fP_5cJLc9Qs4.jpeg", "fullname": "Shilong Zhang", "name": "shilongz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "61e5d14f77496de0a6d95c6b", "name": "adobe", "fullname": "Adobe", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1645217431826-61e35e517ac6b6d06cfa8081.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.17012", "authors": [{"_id": "6948aa3d34f46eaf46cbb1cb", "name": "Chiao-An Yang", "hidden": false}, {"_id": "6948aa3d34f46eaf46cbb1cc", "name": "Ryo Hachiuma", "hidden": false}, {"_id": "6948aa3d34f46eaf46cbb1cd", "name": "Sifei Liu", "hidden": false}, {"_id": "6948aa3d34f46eaf46cbb1ce", "name": "Subhashree Radhakrishnan", "hidden": false}, {"_id": "6948aa3d34f46eaf46cbb1cf", "name": "Raymond A. Yeh", "hidden": false}, {"_id": "6948aa3d34f46eaf46cbb1d0", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "6948aa3d34f46eaf46cbb1d1", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:54.795Z", "hidden": false}], "publishedAt": "2025-12-18T19:13:44.000Z", "submittedOnDailyAt": "2025-12-22T00:08:35.679Z", "title": "4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation", "submittedOnDailyBy": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "summary": "Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.", "upvotes": 28, "discussionId": "6948aa3d34f46eaf46cbb1d2", "projectPage": "https://ca-joe-yang.github.io/resource/projects/4D_RGPT", "ai_summary": "4D-RGPT, a specialized multimodal LLM, enhances 4D perception in video inputs through Perceptual 4D Distillation and is evaluated on R4D-Bench, a new benchmark for depth-aware dynamic scenes.", "ai_keywords": ["4D-RGPT", "Multimodal LLMs", "4D Video Question Answering", "4D representations", "Perceptual 4D Distillation", "R4D-Bench", "depth-aware", "dynamic scenes", "region-level prompting"], "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5728\u5904\u74063D\u7ed3\u6784\u548c\u65f6\u95f4\u52a8\u6001\u65b9\u9762\u4ecd\u7136\u6709\u9650\u3002</li>\n    <li>\u73b0\u6709\u76843D\u548c4D\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u573a\u666f\uff0c\u7f3a\u5c11\u533a\u57df\u7ea7\u63d0\u793a\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e864D-RGPT\uff0c\u4e00\u4e2a\u4e13\u95e8\u8bbe\u8ba1\u7684MLLM\uff0c\u53ef\u4ee5\u4ece\u89c6\u9891\u8f93\u5165\u4e2d\u6355\u63494D\u8868\u793a\uff0c\u5e76\u589e\u5f3a\u65f6\u95f4\u611f\u77e5\u80fd\u529b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u611f\u77e54D\u84b8\u998f\uff08P4D\uff09\u8bad\u7ec3\u6846\u67b6\uff0c\u5c064D\u8868\u793a\u4ece\u4e00\u4e2a\u56fa\u5b9a\u7684\u4e13\u5bb6\u6a21\u578b\u8f6c\u79fb\u52304D-RGPT\u4e2d\u3002</li>\n    <li>\u6211\u4eec\u76844D-RGPT\u5728\u73b0\u6709\u76844D\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\u548c\u65b0\u63d0\u51fa\u7684R4D-Bench\u57fa\u51c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multimodal LLMs struggle with understanding 3D structures and time-based changes.</li>\n    <li>Current video question answering benchmarks mostly focus on still images and do not allow detailed region prompts.</li>\n    <li>We introduce 4D-RGPT, a new model that better understands 4D video data.</li>\n    <li>Perceptual 4D Distillation (P4D) is a new training method that helps 4D-RGPT learn from an expert model.</li>\n    <li>R4D-Bench is a new benchmark for evaluating depth-aware dynamic scenes with more detailed prompts.</li>\n</ul>"}, "publishedAt": "2025-12-18T14:13:44.000Z", "title": "4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation", "summary": "Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17012.png", "numComments": 1, "submittedBy": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "fullname": "Min-Hung Chen", "name": "cmhungsteve", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16041", "authors": [{"_id": "6944c0a4fbf17e708e186021", "name": "Yuanning Feng", "hidden": false}, {"_id": "6944c0a4fbf17e708e186022", "name": "Sinan Wang", "hidden": false}, {"_id": "6944c0a4fbf17e708e186023", "name": "Zhengxiang Cheng", "hidden": false}, {"_id": "6944c0a4fbf17e708e186024", "name": "Yao Wan", "hidden": false}, {"_id": "6944c0a4fbf17e708e186025", "name": "Dongping Chen", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/643be8879f5d314db2d9ed23/HvOcmtVvwfWr0mRw_E8Uz.png"], "publishedAt": "2025-12-17T23:49:55.000Z", "submittedOnDailyAt": "2025-12-22T01:17:34.013Z", "title": "Are We on the Right Way to Assessing LLM-as-a-Judge?", "submittedOnDailyBy": {"_id": "643be8879f5d314db2d9ed23", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643be8879f5d314db2d9ed23/VrW2UtJ7ppOnGIYjTWd7b.png", "isPro": false, "fullname": "Chen Dongping", "user": "shuaishuaicdp", "type": "user"}, "summary": "LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.", "upvotes": 22, "discussionId": "6944c0a4fbf17e708e186026", "ai_summary": "Sage is a human-free evaluation suite for LLM-as-a-Judge, using rational choice theory to assess local and global consistency, revealing significant reliability issues with current LLM judges.", "ai_keywords": ["LLM-as-a-Judge", "human-annotated ground truth", "human bias", "scalability constraints", "rational choice theory", "local self-consistency", "global logical consistency", "pairwise preference stability", "transitivity", "LLMBar", "RewardBench2", "reliability problems", "scoring", "pairwise settings", "situational preference", "finetuned LLM-as-a-Judge", "panel-based judge", "deep reasoning", "human judgments"], "organization": {"_id": "6822dcdbce3e59dd136b24fa", "name": "ONE-Lab", "fullname": "ONE Lab"}, "summary_zh": "<ul>\n    <li>LLM\u4f5c\u4e3a\u8bc4\u5224\u5de5\u5177\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u6807\u51c6\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\uff0c\u5bb9\u6613\u5f15\u5165\u504f\u89c1\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86Sage\uff0c\u4e00\u4e2a\u65e0\u9700\u4eba\u7c7b\u6807\u6ce8\u7684\u65b0\u8bc4\u4f30\u5de5\u5177\u3002</li>\n    <li>Sage\u901a\u8fc7\u5c40\u90e8\u81ea\u6d3d\u6027\u548c\u5168\u5c40\u903b\u8f91\u4e00\u81f4\u6027\u6765\u8861\u91cfLLM\u7684\u8bc4\u5224\u8d28\u91cf\u3002</li>\n    <li>\u5b9e\u9a8c\u663e\u793a\uff0cSage\u7684\u8bc4\u4f30\u4e0e\u73b0\u6709\u7684\u76d1\u7763\u57fa\u51c6\u6709\u5f88\u9ad8\u7684\u76f8\u5173\u6027\uff0c\u8bc1\u660e\u5176\u53ef\u9760\u6027\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684LLM\u5728\u8bc4\u5206\u65f6\u5b58\u5728\u663e\u8457\u4e0d\u4e00\u81f4\u6027\uff0c\u8bf4\u660e\u4eba\u7c7b\u6807\u6ce8\u4e5f\u53ef\u80fd\u4e0d\u591f\u53ef\u9760\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current methods for evaluating LLMs (Large Language Models) as judges rely on human input, which can introduce bias and limit scalability.</li>\n    <li>Sage is a new evaluation system that assesses LLM judges without needing human annotations, using principles from rational choice theory.</li>\n    <li>Sage measures two key aspects: local self-consistency (stability in preferences) and global logical consistency (overall preference order). </li>\n    <li>Tests with Sage show that leading LLMs, like Gemini-2.5-Pro and GPT-5, struggle with reliability in judging, especially in tough situations.</li>\n    <li>There is also inconsistency in human judgments, suggesting that human evaluations may not always be a dependable standard for comparison.</li>\n</ul>"}, "publishedAt": "2025-12-17T18:49:55.000Z", "title": "Are We on the Right Way to Assessing LLM-as-a-Judge?", "summary": "LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/643be8879f5d314db2d9ed23/HvOcmtVvwfWr0mRw_E8Uz.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16041.png", "numComments": 1, "submittedBy": {"_id": "643be8879f5d314db2d9ed23", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643be8879f5d314db2d9ed23/VrW2UtJ7ppOnGIYjTWd7b.png", "fullname": "Chen Dongping", "name": "shuaishuaicdp", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "6822dcdbce3e59dd136b24fa", "name": "ONE-Lab", "fullname": "ONE Lab"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.17897", "authors": [{"_id": "6948c3bc34f46eaf46cbb316", "user": {"_id": "676d699f6f93172ba941a382", "avatarUrl": "/avatars/15dddb7b577bb202646dbab3f249d9b2.svg", "isPro": false, "fullname": "Tomer Borreda", "user": "TomerBo", "type": "user"}, "name": "Tomer Borreda", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:26.699Z", "hidden": false}, {"_id": "6948c3bc34f46eaf46cbb317", "name": "Fangqiang Ding", "hidden": false}, {"_id": "6948c3bc34f46eaf46cbb318", "name": "Sanja Fidler", "hidden": false}, {"_id": "6948c3bc34f46eaf46cbb319", "name": "Shengyu Huang", "hidden": false}, {"_id": "6948c3bc34f46eaf46cbb31a", "name": "Or Litany", "hidden": false}], "publishedAt": "2025-12-19T18:57:33.000Z", "submittedOnDailyAt": "2025-12-22T06:10:48.469Z", "title": "RadarGen: Automotive Radar Point Cloud Generation from Cameras", "submittedOnDailyBy": {"_id": "672b303918685a38713dd8f1", "avatarUrl": "/avatars/800802127af17be6370bf7f65b4a0cc9.svg", "isPro": false, "fullname": "Daniel Gilo", "user": "danielgilo", "type": "user"}, "summary": "We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.", "upvotes": 16, "discussionId": "6948c3bc34f46eaf46cbb31b", "projectPage": "https://radargen.github.io/", "githubRepo": "https://github.com/tomerborreda/RadarGen", "githubRepoAddedBy": "user", "ai_summary": "RadarGen uses diffusion models to synthesize realistic radar point clouds from multi-view camera images, incorporating depth, semantic, and motion cues from pretrained models to enhance physical plausibility.", "ai_keywords": ["diffusion model", "radar point clouds", "multi-view camera imagery", "efficient image-latent diffusion", "bird's-eye-view", "radar measurements", "spatial structure", "radar cross section", "Doppler attributes", "point clouds", "BEV-aligned", "pretrained foundation models", "stochastic generation process", "multimodal generative simulation", "large-scale driving data", "perception models", "unified generative simulation"], "githubStars": 6, "summary_zh": "<ul>\n    <li>RadarGen\u662f\u4e00\u79cd\u6269\u6563\u6a21\u578b\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u89c6\u89d2\u76f8\u673a\u56fe\u50cf\u5408\u6210\u771f\u5b9e\u7684\u6c7d\u8f66\u96f7\u8fbe\u70b9\u4e91\u3002</li>\n    <li>\u5b83\u901a\u8fc7\u9e1f\u77b0\u56fe\u5f62\u5f0f\u8868\u793a\u96f7\u8fbe\u6d4b\u91cf\uff0c\u7ed3\u5408\u4e86\u7a7a\u95f4\u7ed3\u6784\u548c\u96f7\u8fbe\u7279\u5f81\u4fe1\u606f\u3002</li>\n    <li>\u6a21\u578b\u5229\u7528\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u6df1\u5ea6\u3001\u8bed\u4e49\u548c\u8fd0\u52a8\u4fe1\u606f\uff0c\u5e2e\u52a9\u751f\u6210\u7b26\u5408\u7269\u7406\u89c4\u5f8b\u7684\u96f7\u8fbe\u6a21\u5f0f\u3002</li>\n    <li>RadarGen\u4e0e\u73b0\u6709\u89c6\u89c9\u6570\u636e\u96c6\u548c\u4eff\u771f\u6846\u67b6\u517c\u5bb9\uff0c\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u751f\u6210\u4eff\u771f\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cRadarGen\u80fd\u591f\u6355\u6349\u96f7\u8fbe\u6d4b\u91cf\u7684\u7279\u5f81\u5206\u5e03\uff0c\u7f29\u5c0f\u4e0e\u771f\u5b9e\u6570\u636e\u611f\u77e5\u6a21\u578b\u7684\u5dee\u8ddd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>RadarGen is a new model that creates realistic radar data from images taken by cameras.</li>\n    <li>It uses a special method to represent radar information in a bird's-eye view, combining spatial details with radar features.</li>\n    <li>A simple process is used to turn these representations back into point clouds, which are 3D data structures.</li>\n    <li>RadarGen improves the accuracy of radar generation by using information from advanced models about depth, object types, and motion.</li>\n    <li>Tests show that RadarGen effectively mimics real radar data, helping bridge the gap between simulated and real-world data for better analysis.</li>\n</ul>"}, "publishedAt": "2025-12-19T13:57:33.000Z", "title": "RadarGen: Automotive Radar Point Cloud Generation from Cameras", "summary": "We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17897.png", "numComments": 1, "submittedBy": {"_id": "672b303918685a38713dd8f1", "avatarUrl": "/avatars/800802127af17be6370bf7f65b4a0cc9.svg", "fullname": "Daniel Gilo", "name": "danielgilo", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "isAuthorParticipating": false}, {"paper": {"id": "2512.17495", "authors": [{"_id": "6948dc2634f46eaf46cbb34d", "name": "Rang Li", "hidden": false}, {"_id": "6948dc2634f46eaf46cbb34e", "user": {"_id": "6038d6d0612f5eef3cc05ea9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg", "isPro": false, "fullname": "Lei Li", "user": "tobiaslee", "type": "user"}, "name": "Lei Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:23.401Z", "hidden": false}, {"_id": "6948dc2634f46eaf46cbb34f", "name": "Shuhuai Ren", "hidden": false}, {"_id": "6948dc2634f46eaf46cbb350", "name": "Hao Tian", "hidden": false}, {"_id": "6948dc2634f46eaf46cbb351", "name": "Shuhao Gu", "hidden": false}, {"_id": "6948dc2634f46eaf46cbb352", "name": "Shicheng Li", "hidden": false}, {"_id": "6948dc2634f46eaf46cbb353", "name": "Zihao Yue", "hidden": false}, {"_id": "6948dc2634f46eaf46cbb354", "name": "Yudong Wang", "hidden": false}, {"_id": "6948dc2634f46eaf46cbb355", "name": "Wenhan Ma", "hidden": false}, {"_id": "6948dc2634f46eaf46cbb356", "name": "Zhe Yang", "hidden": false}, {"_id": "6948dc2634f46eaf46cbb357", "name": "Jingyuan Ma", "hidden": false}, {"_id": "6948dc2634f46eaf46cbb358", "name": "Zhifang Sui", "hidden": false}, {"_id": "6948dc2634f46eaf46cbb359", "name": "Fuli Luo", "hidden": false}], "publishedAt": "2025-12-19T12:06:25.000Z", "submittedOnDailyAt": "2025-12-22T03:21:37.716Z", "title": "GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation", "submittedOnDailyBy": {"_id": "6038d6d0612f5eef3cc05ea9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg", "isPro": false, "fullname": "Lei Li", "user": "tobiaslee", "type": "user"}, "summary": "Visual grounding, localizing objects from natural language descriptions, represents a critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, a fundamental question remains: can MLLMs truly ground language in vision with human-like sophistication, or are they merely pattern-matching on simplified datasets? Current benchmarks fail to capture real-world complexity where humans effortlessly navigate ambiguous references and recognize when grounding is impossible. To rigorously assess MLLMs' true capabilities, we introduce GroundingME, a benchmark that systematically challenges models across four critical dimensions: (1) Discriminative, distinguishing highly similar objects, (2) Spatial, understanding complex relational descriptions, (3) Limited, handling occlusions or tiny objects, and (4) Rejection, recognizing ungroundable queries. Through careful curation combining automated generation with human verification, we create 1,005 challenging examples mirroring real-world complexity. Evaluating 25 state-of-the-art MLLMs reveals a profound capability gap: the best model achieves only 45.1% accuracy, while most score 0% on rejection tasks, reflexively hallucinating objects rather than acknowledging their absence, raising critical safety concerns for deployment. We explore two strategies for improvements: (1) test-time scaling selects optimal response by thinking trajectory to improve complex grounding by up to 2.9%, and (2) data-mixture training teaches models to recognize ungroundable queries, boosting rejection accuracy from 0% to 27.9%. GroundingME thus serves as both a diagnostic tool revealing current limitations in MLLMs and a roadmap toward human-level visual grounding.", "upvotes": 14, "discussionId": "6948dc2734f46eaf46cbb35a", "projectPage": "https://groundingme.github.io/", "ai_summary": "GroundingME benchmark evaluates multimodal large language models' visual grounding capabilities across complexity dimensions, revealing significant gaps and proposing strategies for improvement.", "ai_keywords": ["multimodal large language models", "visual grounding", "GroundingME", "discriminative", "spatial", "limited", "rejection", "thinking trajectory", "data-mixture training"], "organization": {"_id": "680cb4c37f289defb2210940", "name": "XiaomiMiMo", "fullname": "Xiaomi MiMo", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/680cb7d1233834890a64acee/5w_4aLfF-7MAyaIPOV498.jpeg"}, "summary_zh": "<ul>\n    <li>\u89c6\u89c9\u5b9a\u4f4d\u662f\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e2d\u7684\u7269\u4f53\u8fdb\u884c\u5b9a\u4f4d\u7684\u91cd\u8981\u4efb\u52a1\uff0c\u6d89\u53ca\u8bed\u8a00\u548c\u89c6\u89c9\u7684\u7406\u89e3\u3002</li>\n    <li>\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u9700\u786e\u8ba4\u5b83\u4eec\u662f\u5426\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u7406\u89e3\u8bed\u8a00\u4e0e\u89c6\u89c9\u7684\u5173\u7cfb\u3002</li>\n    <li>GroundingME\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30MLLMs\u5728\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\u7684\u80fd\u529b\uff0c\u5305\u62ec\u533a\u5206\u76f8\u4f3c\u7269\u4f53\u3001\u7406\u89e3\u7a7a\u95f4\u5173\u7cfb\u3001\u5904\u7406\u906e\u6321\u548c\u5c0f\u7269\u4f53\uff0c\u4ee5\u53ca\u8bc6\u522b\u65e0\u6cd5\u5b9a\u4f4d\u7684\u67e5\u8be2\u3002</li>\n    <li>\u7ecf\u8fc7\u8bc4\u4f30\uff0c\u6700\u597d\u7684\u6a21\u578b\u5728\u51c6\u786e\u7387\u4e0a\u4ec5\u8fbe\u523045.1%\uff0c\u800c\u5927\u591a\u6570\u6a21\u578b\u5728\u62d2\u7edd\u4efb\u52a1\u4e0a\u5f97\u5206\u4e3a0%\uff0c\u663e\u793a\u51fa\u663e\u8457\u7684\u80fd\u529b\u5dee\u8ddd\u3002</li>\n    <li>\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e24\u79cd\u6539\u8fdb\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\uff0c\u5305\u62ec\u4f18\u5316\u54cd\u5e94\u9009\u62e9\u548c\u8bad\u7ec3\u6a21\u578b\u8bc6\u522b\u65e0\u6cd5\u5b9a\u4f4d\u7684\u67e5\u8be2\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Visual grounding helps connect language and vision by locating objects based on descriptions.</li>\n  <li>Current benchmarks do not fully test the real-world abilities of multimodal large language models (MLLMs).</li>\n  <li>GroundingME is a new benchmark that challenges MLLMs in four key areas: distinguishing similar objects, understanding spatial relationships, handling occlusions, and recognizing when queries can't be grounded.</li>\n  <li>Evaluation shows a significant gap in MLLM performance, with the best model only achieving 45.1% accuracy and most failing to identify ungroundable queries.</li>\n  <li>The study suggests two strategies for improvement: optimizing responses during testing and training models to better recognize ungroundable queries.</li>\n</ul>"}, "publishedAt": "2025-12-19T07:06:25.000Z", "title": "GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation", "summary": "Visual grounding, localizing objects from natural language descriptions, represents a critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, a fundamental question remains: can MLLMs truly ground language in vision with human-like sophistication, or are they merely pattern-matching on simplified datasets? Current benchmarks fail to capture real-world complexity where humans effortlessly navigate ambiguous references and recognize when grounding is impossible. To rigorously assess MLLMs' true capabilities, we introduce GroundingME, a benchmark that systematically challenges models across four critical dimensions: (1) Discriminative, distinguishing highly similar objects, (2) Spatial, understanding complex relational descriptions, (3) Limited, handling occlusions or tiny objects, and (4) Rejection, recognizing ungroundable queries. Through careful curation combining automated generation with human verification, we create 1,005 challenging examples mirroring real-world complexity. Evaluating 25 state-of-the-art MLLMs reveals a profound capability gap: the best model achieves only 45.1% accuracy, while most score 0% on rejection tasks, reflexively hallucinating objects rather than acknowledging their absence, raising critical safety concerns for deployment. We explore two strategies for improvements: (1) test-time scaling selects optimal response by thinking trajectory to improve complex grounding by up to 2.9%, and (2) data-mixture training teaches models to recognize ungroundable queries, boosting rejection accuracy from 0% to 27.9%. GroundingME thus serves as both a diagnostic tool revealing current limitations in MLLMs and a roadmap toward human-level visual grounding.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17495.png", "numComments": 2, "submittedBy": {"_id": "6038d6d0612f5eef3cc05ea9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg", "fullname": "Lei Li", "name": "tobiaslee", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 19}, "organization": {"_id": "680cb4c37f289defb2210940", "name": "XiaomiMiMo", "fullname": "Xiaomi MiMo", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/680cb7d1233834890a64acee/5w_4aLfF-7MAyaIPOV498.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.17351", "authors": [{"_id": "6948bb2734f46eaf46cbb2ec", "user": {"_id": "6316d14c0db6c679cf77fe1f", "avatarUrl": "/avatars/b33dcadc0ff8941da6aa850284be3207.svg", "isPro": false, "fullname": "Zeyuan Allen-Zhu", "user": "zhuzeyuan", "type": "user"}, "name": "Zeyuan Allen-Zhu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:30.273Z", "hidden": false}], "publishedAt": "2025-12-19T08:47:28.000Z", "submittedOnDailyAt": "2025-12-22T01:04:44.279Z", "title": "Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers", "submittedOnDailyBy": {"_id": "6316d14c0db6c679cf77fe1f", "avatarUrl": "/avatars/b33dcadc0ff8941da6aa850284be3207.svg", "isPro": false, "fullname": "Zeyuan Allen-Zhu", "user": "zhuzeyuan", "type": "user"}, "summary": "Understanding architectural differences in language models is challenging, especially at academic-scale pretraining (e.g., 1.3B parameters, 100B tokens), where results are often dominated by noise and randomness. To overcome this, we introduce controlled synthetic pretraining tasks that isolate and evaluate core model capabilities. Within this framework, we discover CANON LAYERS: lightweight architectural components -- named after the musical term \"canon\" -- that promote horizontal information flow across neighboring tokens. Canon layers compute weighted sums of nearby token representations and integrate seamlessly into Transformers, linear attention, state-space models, or any sequence architecture.\n  We present 12 key results. This includes how Canon layers enhance reasoning depth (e.g., by 2times), reasoning breadth, knowledge manipulation, etc. They lift weak architectures like NoPE to match RoPE, and linear attention to rival SOTA linear models like Mamba2/GDN -- validated both through synthetic tasks and real-world academic-scale pretraining. This synthetic playground offers an economical, principled path to isolate core model capabilities often obscured at academic scales. Equipped with infinite high-quality data, it may even PREDICT how future architectures will behave as training pipelines improve -- e.g., through better data curation or RL-based post-training -- unlocking deeper reasoning and hierarchical inference.", "upvotes": 13, "discussionId": "6948bb2834f46eaf46cbb2ed", "projectPage": "https://physics.allen-zhu.com/part-4-architecture-design/part-4-1", "githubRepo": "https://github.com/facebookresearch/PhysicsLM4", "githubRepoAddedBy": "user", "ai_summary": "Canon layers, lightweight architectural components, enhance reasoning depth and breadth in language models, improving performance in synthetic and real-world pretraining tasks.", "ai_keywords": ["CANON LAYERS", "horizontal information flow", "weighted sums", "Transformer", "linear attention", "state-space models", "sequence architecture", "reasoning depth", "reasoning breadth", "knowledge manipulation", "NoPE", "RoPE", "Mamba2", "GDN", "synthetic tasks", "academic-scale pretraining", "future architectures", "data curation", "RL-based post-training", "hierarchical inference"], "githubStars": 278, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "summary_zh": "<ul>\n    <li>\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u7684\u67b6\u6784\u5dee\u5f02\u5f88\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u65f6\uff0c\u7ed3\u679c\u5e38\u5e38\u53d7\u5230\u566a\u97f3\u548c\u968f\u673a\u6027\u7684\u5f71\u54cd\u3002</li>\n    <li>\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u5f15\u5165\u4e86\u63a7\u5236\u7684\u5408\u6210\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u4ee5\u8bc4\u4f30\u6a21\u578b\u7684\u6838\u5fc3\u80fd\u529b\u3002</li>\n    <li>\u53d1\u73b0\u4e86\u201cCANON LAYERS\u201d\uff0c\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u67b6\u6784\u7ec4\u4ef6\uff0c\u4fc3\u8fdb\u76f8\u90bb\u6807\u8bb0\u4e4b\u95f4\u7684\u4fe1\u606f\u6d41\u52a8\u3002</li>\n    <li>Canon\u5c42\u80fd\u663e\u8457\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u80fd\u591f\u63d0\u5347\u8f83\u5f31\u67b6\u6784\u7684\u8868\u73b0\uff0c\u4f7f\u5176\u4e0e\u5148\u8fdb\u6a21\u578b\u76f8\u5f53\u3002</li>\n    <li>\u8fd9\u79cd\u5408\u6210\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u6761\u7ecf\u6d4e\u4e14\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u9694\u79bb\u6a21\u578b\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u5e76\u53ef\u80fd\u9884\u6d4b\u672a\u6765\u67b6\u6784\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Understanding differences in language models is hard, especially when they are large (like 1.3 billion parameters).</li>\n    <li>The authors created controlled synthetic tasks to better evaluate model capabilities without noise affecting results.</li>\n    <li>They introduced CANON LAYERS, which help information flow between neighboring tokens in models like Transformers.</li>\n    <li>Canon layers significantly improve reasoning and match the performance of stronger architectures.</li>\n    <li>This research could help predict how future models will perform as training methods improve.</li>\n</ul>"}, "publishedAt": "2025-12-19T03:47:28.000Z", "title": "Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers", "summary": "Understanding architectural differences in language models is challenging, especially at academic-scale pretraining (e.g., 1.3B parameters, 100B tokens), where results are often dominated by noise and randomness. To overcome this, we introduce controlled synthetic pretraining tasks that isolate and evaluate core model capabilities. Within this framework, we discover CANON LAYERS: lightweight architectural components -- named after the musical term \"canon\" -- that promote horizontal information flow across neighboring tokens. Canon layers compute weighted sums of nearby token representations and integrate seamlessly into Transformers, linear attention, state-space models, or any sequence architecture.\n  We present 12 key results. This includes how Canon layers enhance reasoning depth (e.g., by 2times), reasoning breadth, knowledge manipulation, etc. They lift weak architectures like NoPE to match RoPE, and linear attention to rival SOTA linear models like Mamba2/GDN -- validated both through synthetic tasks and real-world academic-scale pretraining. This synthetic playground offers an economical, principled path to isolate core model capabilities often obscured at academic scales. Equipped with infinite high-quality data, it may even PREDICT how future architectures will behave as training pipelines improve -- e.g., through better data curation or RL-based post-training -- unlocking deeper reasoning and hierarchical inference.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17351.png", "numComments": 1, "submittedBy": {"_id": "6316d14c0db6c679cf77fe1f", "avatarUrl": "/avatars/b33dcadc0ff8941da6aa850284be3207.svg", "fullname": "Zeyuan Allen-Zhu", "name": "zhuzeyuan", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 28}, "organization": {"_id": "5e63d8713071d5be688861b8", "name": "facebook", "fullname": "AI at Meta", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1592839207516-noauth.png"}, "isAuthorParticipating": true}],
    "week": [{"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni\u662f\u4e00\u4e2a\u901a\u7528\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u800c\u4e0d\u662f\u5206\u5f00\u7684\u6d41\u7a0b\u3002</li>\n    <li>Kling-Omni\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5305\u62ec\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u5185\u5bb9\uff0c\u5e76\u5c06\u5176\u5904\u7406\u4e3a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\u3002</li>\n    <li>\u8be5\u6846\u67b6\u57fa\u4e8e\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u5229\u7528\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u6765\u63d0\u5347\u6027\u80fd\u3002</li>\n    <li>Kling-Omni\u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u6267\u884c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u662f\u8fc8\u5411\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u91cd\u8981\u8fdb\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from various types of inputs like text, images, and videos.</li>\n    <li>It combines different video tasks (like generation and editing) into one system, rather than using separate processes.</li>\n    <li>The framework turns mixed inputs into a single format to produce impressive and smart video content.</li>\n    <li>Kling-Omni is built on a strong data system and uses advanced training methods for better performance.</li>\n    <li>It shows great results in generating content, editing videos based on reasoning, and following instructions from different media sources.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u89c6\u9891\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u89c6\u89c9\u771f\u5b9e\u548c\u65f6\u95f4\u4e00\u81f4\u7684\u5185\u5bb9\uff0c\u4f46\u5b83\u4eec\u7684\u53ef\u9760\u6027\u53d6\u51b3\u4e8e\u662f\u5426\u6355\u6349\u5230\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7ea6\u675f\u3002</li>\n    <li>\u73b0\u6709\u6307\u6807\u5982\u5f17\u96f7\u6b47\u89c6\u9891\u8ddd\u79bb(FVD)\u53ea\u5173\u6ce8\u611f\u77e5\u8d28\u91cf\uff0c\u5ffd\u89c6\u4e86\u56e0\u679c\u6027\u3001\u7269\u7406\u548c\u5168\u5c40\u4e00\u81f4\u6027\u7b49\u63a8\u7406\u9519\u8bef\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86MMGR\uff08\u591a\u6a21\u6001\u751f\u6210\u63a8\u7406\u8bc4\u4f30\u4e0e\u57fa\u51c6\uff09\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8bc4\u4f30\uff1a\u7269\u7406\u3001\u903b\u8f91\u30013D\u7a7a\u95f4\u30012D\u7a7a\u95f4\u548c\u65f6\u95f4\u3002</li>\n    <li>MMGR\u5728\u4e09\u4e2a\u9886\u57df\u8fdb\u884c\u8bc4\u4f30\uff1a\u62bd\u8c61\u63a8\u7406\u3001\u4f53\u611f\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\uff0c\u4f7f\u7528\u7ec6\u81f4\u7684\u6307\u6807\u68c0\u67e5\u89c6\u9891\u548c\u56fe\u50cf\u751f\u6210\u7684\u6574\u4f53\u6b63\u786e\u6027\u3002</li>\n    <li>\u6a21\u578b\u5728\u7269\u7406\u5e38\u8bc6\u4efb\u52a1\u4e0a\u8868\u73b0\u4e2d\u7b49\uff0c\u4f46\u5728\u62bd\u8c61\u63a8\u7406\uff08ARC-AGI\u51c6\u786e\u7387\u4f4e\u4e8e10%\uff09\u548c\u957f\u65f6\u95f4\u7a7a\u95f4\u89c4\u5212\u65b9\u9762\u8868\u73b0\u8f83\u5dee\uff0c\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u4e3b\u8981\u9650\u5236\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video foundation models can create realistic videos, but they may not always follow real-world rules and logic.</li>\n    <li>Current evaluation methods focus on visual quality but miss issues related to reasoning, like causality and physics.</li>\n    <li>MMGR is a new evaluation framework that tests models on five reasoning skills: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>\n    <li>MMGR assesses models in three areas: Abstract Reasoning (like Sudoku), Embodied Navigation (3D navigation), and Physical Commonsense (sports interactions).</li>\n    <li>Tests show that while some models do okay in Physical Commonsense, they struggle significantly with Abstract Reasoning and long-term planning.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u5c3d\u7ba1\u79d1\u5b66\u4eba\u5de5\u667a\u80fd\u6709\u6240\u8fdb\u6b65\uff0c\u4f46\u8fd8\u6ca1\u6709\u4e00\u4e2a\u7edf\u4e00\u7684\u79d1\u5b66\u901a\u7528\u667a\u80fd\uff08SGI\uff09\u6846\u67b6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b9e\u7528\u63a2\u7a76\u6a21\u578b\uff08PIM\uff09\u7684SGI\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u79d1\u5b66\u5bb6\u76f8\u5173\u7684\u4efb\u52a1\u8fdb\u884c\u64cd\u4f5c\u5316\u3002</li>\n    <li>SGI-Bench\u5305\u542b1000\u591a\u4e2a\u8de8\u5b66\u79d1\u7684\u6837\u672c\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u6700\u65b0\u7684\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u663e\u793a\u5728\u6df1\u5ea6\u7814\u7a76\u548c\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u9762\u5b58\u5728\u591a\u4e2a\u4e0d\u8db3\u4e4b\u5904\u3002</li>\n    <li>\u6211\u4eec\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\uff0c\u4ee5\u4f18\u5316\u5047\u8bbe\u7684\u65b0\u9896\u6027\uff0c\u652f\u6301AI\u7cfb\u7edf\u53c2\u4e0e\u79d1\u5b66\u53d1\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is no clear framework for Scientific General Intelligence (SGI), which is the ability of AI to think and work across different scientific fields.</li>\n    <li>We define SGI using the Practical Inquiry Model and create tasks that align with how scientists work, including research and experiments.</li>\n    <li>SGI-Bench includes over 1,000 examples from various scientific fields to test AI performance on important scientific questions.</li>\n    <li>Results show significant gaps in AI performance, like low accuracy in deep research and issues with executing experiments properly.</li>\n    <li>We propose Test-Time Reinforcement Learning (TTRL) to improve AI's ability to generate new ideas during testing without relying on pre-set answers.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.15431", "authors": [{"_id": "69437417542d62d58a7bf6c4", "name": "Haolong Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6c5", "name": "Jia Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c6", "name": "Xin Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c7", "name": "Yeqing Shen", "hidden": false}, {"_id": "69437417542d62d58a7bf6c8", "user": {"_id": "653614073f4248157d60ccdc", "avatarUrl": "/avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg", "isPro": false, "fullname": "mengziyang", "user": "zylate", "type": "user"}, "name": "Ziyang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:53.033Z", "hidden": false}, {"_id": "69437417542d62d58a7bf6c9", "name": "Zhimin Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6ca", "name": "Kaijun Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6cb", "name": "Jin Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf6cc", "name": "Lieyu Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6cd", "name": "Mi Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6ce", "name": "Shiliang Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6cf", "name": "Zhirui Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d0", "name": "Brian Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d1", "name": "Kang An", "hidden": false}, {"_id": "69437417542d62d58a7bf6d2", "name": "Chenyang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d3", "name": "Lei Lei", "hidden": false}, {"_id": "69437417542d62d58a7bf6d4", "name": "Mengmeng Duan", "hidden": false}, {"_id": "69437417542d62d58a7bf6d5", "name": "Danxun Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d6", "name": "Guodong Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d7", "name": "Hang Cheng", "hidden": false}, {"_id": "69437417542d62d58a7bf6d8", "name": "Hao Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d9", "name": "Jie Dong", "hidden": false}, {"_id": "69437417542d62d58a7bf6da", "name": "Junhao Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6db", "name": "Mei Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6dc", "name": "Renjie Yu", "hidden": false}, {"_id": "69437417542d62d58a7bf6dd", "name": "Shunshan Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6de", "name": "Xu Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf6df", "name": "Yiting Dai", "hidden": false}, {"_id": "69437417542d62d58a7bf6e0", "name": "Yineng Deng", "hidden": false}, {"_id": "69437417542d62d58a7bf6e1", "name": "Yingdan Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6e2", "name": "Zelin Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6e3", "name": "Wen Sun", "hidden": false}, {"_id": "69437417542d62d58a7bf6e4", "name": "Chengxu Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e5", "name": "Chunqin Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6e6", "name": "Dong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6e7", "name": "Fengqiong Xiao", "hidden": false}, {"_id": "69437417542d62d58a7bf6e8", "name": "Guanghao Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e9", "name": "Guopeng Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ea", "name": "Guozhen Peng", "hidden": false}, {"_id": "69437417542d62d58a7bf6eb", "name": "Hongbing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ec", "name": "Hang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ed", "name": "Hongming Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6ee", "name": "Jingjing Xie", "hidden": false}, {"_id": "69437417542d62d58a7bf6ef", "name": "Jianyong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6f0", "name": "Jingyang Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf6f1", "name": "Jiaju Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f2", "name": "Jiayu Yuan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f3", "name": "Jianpeng Yin", "hidden": false}, {"_id": "69437417542d62d58a7bf6f4", "name": "Kai Cao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f5", "name": "Liang Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f6", "name": "Liguo Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f7", "name": "Liying Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6f8", "name": "Mengqiang Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f9", "name": "Min Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fa", "name": "Manjiao Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fb", "name": "Mao Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf6fc", "name": "Mingxin Wan", "hidden": false}, {"_id": "69437417542d62d58a7bf6fd", "name": "Na Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6fe", "name": "Nan Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6ff", "name": "Ning Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf700", "name": "Peiyao Ma", "hidden": false}, {"_id": "69437417542d62d58a7bf701", "name": "Qingzhou Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf702", "name": "Qiao Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf703", "name": "Qinlin Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf704", "name": "Qiong Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf705", "name": "Qiongyao Li", "hidden": false}, {"_id": "69437417542d62d58a7bf706", "name": "Shangwu Zhong", "hidden": false}, {"_id": "69437417542d62d58a7bf707", "name": "Shuli Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf708", "name": "Shaofan Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf709", "name": "Shisi Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf70a", "name": "Shuang Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf70b", "name": "Xingbin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70c", "name": "Xiaojia Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70d", "name": "Xiaojie Hou", "hidden": false}, {"_id": "69437417542d62d58a7bf70e", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70f", "name": "Xuanti Feng", "hidden": false}, {"_id": "69437417542d62d58a7bf710", "name": "Xuedan Cai", "hidden": false}, {"_id": "69437417542d62d58a7bf711", "name": "Xuan Wen", "hidden": false}, {"_id": "69437417542d62d58a7bf712", "name": "Xianwei Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf713", "name": "Xin Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf714", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf715", "name": "Xin Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf716", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf717", "name": "Yukang Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf718", "name": "Yunfang Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf719", "name": "Yuqing Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf71a", "name": "Yixun Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf71b", "name": "Zejia Weng", "hidden": false}, {"_id": "69437417542d62d58a7bf71c", "name": "Zhonghao Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf71d", "name": "Zhiguo Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf71e", "name": "Zhuoyu Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf71f", "name": "Zheng Ge", "hidden": false}, {"_id": "69437417542d62d58a7bf720", "name": "Jing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf721", "name": "Yibo Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf722", "name": "Binxing Jiao", "hidden": false}, {"_id": "69437417542d62d58a7bf723", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf724", "name": "Daxin Jiang", "hidden": false}], "publishedAt": "2025-12-17T13:26:30.000Z", "submittedOnDailyAt": "2025-12-18T00:55:26.804Z", "title": "Step-GUI Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "upvotes": 77, "discussionId": "69437418542d62d58a7bf725", "projectPage": "https://opengelab.github.io/", "githubRepo": "https://github.com/stepfun-ai/gelab-zero", "githubRepoAddedBy": "user", "ai_summary": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.", "ai_keywords": ["multimodal large language models", "GUI automation", "self-evolving training pipeline", "Calibrated Step Reward System", "trajectory-level calibration", "Step-GUI", "GUI performance", "GUI-MCP", "Model Context Protocol", "AndroidWorld", "OSWorld", "ScreenShot-Pro", "AndroidDaily", "real-world mobile usage patterns", "hierarchical architecture", "low-level atomic operations", "high-level task delegation", "local specialist models", "high-privacy execution"], "githubStars": 1417, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u578b\u81ea\u6211\u8fdb\u5316\u7684\u8bad\u7ec3\u7ba1\u9053\uff0c\u4f7f\u7528\u6821\u51c6\u6b65\u5956\u52b1\u7cfb\u7edf\u63d0\u9ad8\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u3002</li>\n    <li>\u63a8\u51fa\u4e86Step-GUI\u6a21\u578b\u7cfb\u5217\uff0c\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u6700\u65b0\u7684GUI\u6027\u80fd\u6307\u6807\u3002</li>\n    <li>\u63d0\u51faGUI-MCP\u534f\u8bae\uff0c\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u5e76\u5b9e\u73b0\u8de8\u8bbe\u5907\u7684\u6807\u51c6\u5316\u63a5\u53e3\u3002</li>\n    <li>\u5f15\u5165AndroidDaily\u57fa\u51c6\uff0c\u57fa\u4e8e\u771f\u5b9e\u7684\u79fb\u52a8\u4f7f\u7528\u6a21\u5f0f\u8bc4\u4f30\u4ee3\u7406\u7684\u65e5\u5e38\u4f7f\u7528\u80fd\u529b\u3002</li>\n    <li>\u7814\u7a76\u6210\u679c\u63a8\u52a8\u4e86\u5b9e\u7528GUI\u4ee3\u7406\u7684\u53d1\u5c55\uff0c\u5c55\u793a\u4e86\u5728\u65e5\u5e38\u6570\u5b57\u4ea4\u4e92\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New techniques in large language models help automate graphical user interfaces (GUIs) effectively.</li>\n    <li>We developed a training system that efficiently produces reliable data at a lower cost, achieving over 90% accuracy.</li>\n    <li>Our models, named Step-GUI, perform exceptionally well in GUI tasks while maintaining strong general skills.</li>\n    <li>We introduced GUI-MCP, a protocol that allows safe and private execution of tasks on various devices.</li>\n    <li>We created AndroidDaily, a benchmark that tests how well our models can handle real-life mobile tasks and actions.</li>\n</ul>"}, "publishedAt": "2025-12-17T08:26:30.000Z", "title": "Step-GUI Technical Report", "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15431.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16793", "authors": [{"_id": "6944ba3efbf17e708e185f60", "user": {"_id": "6944c28d3cd5eeb7a7838663", "avatarUrl": "/avatars/63f9f8c4e8462cbc9726bdee249fbcde.svg", "isPro": false, "fullname": "lin", "user": "birdxp", "type": "user"}, "name": "Xiaopeng Lin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:14.947Z", "hidden": false}, {"_id": "6944ba3efbf17e708e185f61", "user": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "isPro": false, "fullname": "Shijie Lian", "user": "LiamLian0727", "type": "user"}, "name": "Shijie Lian", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:18.093Z", "hidden": false}, {"_id": "6944ba3efbf17e708e185f62", "user": {"_id": "63d3b5f1640bb0f77173baea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674819020331-noauth.jpeg", "isPro": false, "fullname": "yubin", "user": "VLyb", "type": "user"}, "name": "Bin Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T11:00:33.691Z", "hidden": false}, {"_id": "6944ba3efbf17e708e185f63", "user": {"_id": "684a7f750fff63d29d18444c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kXTIoUPOM7LDztBcRL6YI.png", "isPro": false, "fullname": "Ruoqi Yang", "user": "lyceen", "type": "user"}, "name": "Ruoqi Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T11:00:35.617Z", "hidden": false}, {"_id": "6944ba3efbf17e708e185f64", "name": "Changti Wu", "hidden": false}, {"_id": "6944ba3efbf17e708e185f65", "name": "Yuzhuo Miao", "hidden": false}, {"_id": "6944ba3efbf17e708e185f66", "name": "Yurun Jin", "hidden": false}, {"_id": "6944ba3efbf17e708e185f67", "name": "Yukun Shi", "hidden": false}, {"_id": "6944ba3efbf17e708e185f68", "name": "Cong Huang", "hidden": false}, {"_id": "6944ba3efbf17e708e185f69", "name": "Bojun Cheng", "hidden": false}, {"_id": "6944ba3efbf17e708e185f6a", "name": "Kai Chen", "hidden": false}], "publishedAt": "2025-12-18T17:27:03.000Z", "submittedOnDailyAt": "2025-12-22T00:59:36.650Z", "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence", "submittedOnDailyBy": {"_id": "6944c28d3cd5eeb7a7838663", "avatarUrl": "/avatars/63f9f8c4e8462cbc9726bdee249fbcde.svg", "isPro": false, "fullname": "lin", "user": "birdxp", "type": "user"}, "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.", "upvotes": 63, "discussionId": "6944ba3efbf17e708e185f6b", "projectPage": "https://zgc-embodyai.github.io/PhysBrain/", "ai_summary": "Proposed Egocentric2Embodiment pipeline translates human egocentric videos into structured training data for robots, enhancing their egocentric understanding and task performance.", "ai_keywords": ["Egocentric2Embodiment", "VQA supervision", "evidence grounding", "temporal consistency", "Egocentric2Embodiment dataset", "E2E-3M", "PhysBrain", "egocentric-aware", "VLA fine-tuning", "SimplerEnv success rates"], "organization": {"_id": "6948d884070dda0c2ae35a78", "name": "DeepCybo", "fullname": "DeepCybo", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65ec01fd770aa0e25d9374dc/QOsz6P_7AxyqGrjsRHTGk.png"}, "summary_zh": "<ul>\n    <li>\u673a\u5668\u4eba\u7684\u4e00\u822c\u5316\u4f9d\u8d56\u4e8e\u7269\u7406\u667a\u80fd\uff0c\u5305\u62ec\u5bf9\u72b6\u6001\u53d8\u5316\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u4ea4\u4e92\u548c\u957f\u671f\u89c4\u5212\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5927\u591a\u6570\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e3b\u8981\u57fa\u4e8e\u7b2c\u4e09\u4eba\u79f0\u6570\u636e\u8bad\u7ec3\uff0c\u8fd9\u5bfc\u81f4\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u89c6\u89d2\u7684\u4e0d\u5339\u914d\u3002</li>\n    <li>\u6536\u96c6\u673a\u5668\u4eba\u7b2c\u4e00\u4eba\u79f0\u6570\u636e\u6210\u672c\u9ad8\u4e14\u591a\u6837\u6027\u6709\u9650\uff0c\u800c\u5927\u89c4\u6a21\u7684\u4eba\u7c7b\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u8f6c\u6362\u4e3a\u591a\u5c42\u6b21\u3001\u7ed3\u6784\u5316\u8bad\u7ec3\u76d1\u7763\u7684Egocentric2Embodiment\u7ffb\u8bd1\u7ba1\u9053\uff0c\u5e76\u6784\u5efa\u4e86E2E-3M\u6570\u636e\u96c6\u3002</li>\n    <li>PhysBrain\u662f\u57fa\u4e8eE2E-3M\u6570\u636e\u96c6\u8bad\u7ec3\u7684\uff0c\u5c55\u73b0\u4e86\u663e\u8457\u7684\u7b2c\u4e00\u4eba\u79f0\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u63a7\u5236\u7684\u6548\u7387\u548c\u6210\u529f\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Robots need to understand their own actions and interactions in their environment to generalize effectively.</li>\n    <li>Most existing models are trained on third-person data, which doesn't match how humanoid robots perceive their surroundings.</li>\n    <li>Collecting robot-generated egocentric data is costly and has limited variety; using human egocentric videos is a better, scalable option.</li>\n    <li>We developed a system called Egocentric2Embodiment to convert first-person videos into structured training data for robots.</li>\n    <li>The resulting PhysBrain model shows improved understanding and performance in robot tasks, benefiting from human-like perspectives in training.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:27:03.000Z", "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence", "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16793.png", "numComments": 2, "submittedBy": {"_id": "6944c28d3cd5eeb7a7838663", "avatarUrl": "/avatars/63f9f8c4e8462cbc9726bdee249fbcde.svg", "fullname": "lin", "name": "birdxp", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "6948d884070dda0c2ae35a78", "name": "DeepCybo", "fullname": "DeepCybo", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65ec01fd770aa0e25d9374dc/QOsz6P_7AxyqGrjsRHTGk.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.14614", "authors": [{"_id": "694219f25d5b2dc1052747ff", "user": {"_id": "64897b1f0ec897cfe579a399", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64897b1f0ec897cfe579a399/ICR_75b877BaSE94gjBuj.jpeg", "isPro": false, "fullname": "wenq", "user": "wenqsun", "type": "user"}, "name": "Wenqiang Sun", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:38.348Z", "hidden": false}, {"_id": "694219f25d5b2dc105274800", "name": "Haiyu Zhang", "hidden": false}, {"_id": "694219f25d5b2dc105274801", "name": "Haoyuan Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274802", "name": "Junta Wu", "hidden": false}, {"_id": "694219f25d5b2dc105274803", "name": "Zehan Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274804", "name": "Zhenwei Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274805", "name": "Yunhong Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274806", "name": "Jun Zhang", "hidden": false}, {"_id": "694219f25d5b2dc105274807", "name": "Tengfei Wang", "hidden": false}, {"_id": "694219f25d5b2dc105274808", "name": "Chunchao Guo", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/z23kgc1LgY1qvs-Tfp4zj.mp4"], "publishedAt": "2025-12-16T17:22:46.000Z", "submittedOnDailyAt": "2025-12-17T00:24:30.301Z", "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.", "upvotes": 48, "discussionId": "694219f35d5b2dc105274809", "projectPage": "https://3d-models.hunyuan.tencent.com/world/", "githubRepo": "https://github.com/Tencent-Hunyuan/HY-WorldPlay", "githubRepoAddedBy": "user", "ai_summary": "WorldPlay is a streaming video diffusion model that achieves real-time, interactive world modeling with long-term geometric consistency by using a Dual Action Representation, Reconstituted Context Memory, and Context Forcing.", "ai_keywords": ["Dual Action Representation", "Reconstituted Context Memory", "temporal reframing", "Context Forcing", "memory-aware model", "long-horizon streaming video"], "githubStars": 302, "summary_zh": "<ul>\n    <li>\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86WorldPlay\uff0c\u4e00\u4e2a\u5b9e\u65f6\u4e92\u52a8\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u901f\u5ea6\u548c\u5185\u5b58\u4e4b\u95f4\u7684\u77db\u76fe\u3002</li>\n    <li>WorldPlay\u7684\u4e09\u4e2a\u5173\u952e\u521b\u65b0\u5305\u62ec\uff1a\u53cc\u91cd\u52a8\u4f5c\u8868\u793a\uff0c\u80fd\u6709\u6548\u54cd\u5e94\u7528\u6237\u7684\u952e\u76d8\u548c\u9f20\u6807\u8f93\u5165\u3002</li>\n    <li>\u91cd\u6784\u7684\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u52a8\u6001\u91cd\u5efa\u8fc7\u53bb\u5e27\u7684\u4e0a\u4e0b\u6587\uff0c\u4fdd\u6301\u51e0\u4f55\u91cd\u8981\u7684\u957f\u65f6\u95f4\u5e27\u53ef\u8bbf\u95ee\uff0c\u4ece\u800c\u589e\u5f3a\u957f\u671f\u4e00\u81f4\u6027\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u5f3a\u8feb\uff08Context Forcing\uff09\u65b9\u6cd5\uff0c\u5e2e\u52a9\u5b66\u751f\u6a21\u578b\u4fdd\u6301\u4f7f\u7528\u957f\u8ddd\u79bb\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u5b9e\u73b0\u5b9e\u65f6\u901f\u5ea6\u5e76\u9632\u6b62\u9519\u8bef\u6f02\u79fb\u3002</li>\n    <li>WorldPlay\u80fd\u591f\u4ee524\u5e27\u6bcf\u79d2\u751f\u6210720p\u89c6\u9891\uff0c\u5177\u6709\u66f4\u597d\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u5404\u79cd\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>WorldPlay is a new streaming video model that creates interactive 3D worlds quickly and consistently.</li>\n    <li>It uses a special control method that responds well to user inputs from keyboard and mouse.</li>\n    <li>The model keeps important information from past frames to maintain consistency over time, reducing memory loss.</li>\n    <li>It includes a unique method that helps the model remember long-term information while working fast.</li>\n    <li>WorldPlay produces high-quality video at 24 frames per second and performs well across different scenes.</li>\n</ul>"}, "publishedAt": "2025-12-16T12:22:46.000Z", "title": "WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling", "summary": "This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/z23kgc1LgY1qvs-Tfp4zj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14614.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.17901", "authors": [{"_id": "6948af7534f46eaf46cbb1da", "user": {"_id": "6719bfd07c6e6c83a388aeae", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719bfd07c6e6c83a388aeae/jHxryk04dzHo23TX5F5sz.png", "isPro": false, "fullname": "Junyu Zhang", "user": "jyzhang1208", "type": "user"}, "name": "Junyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:53.009Z", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1db", "name": "Yifan Sun", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1dc", "name": "Tianang Leng", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1dd", "name": "Jingyan Shen", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1de", "name": "Liu Ziyin", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1df", "name": "Paul Pu Liang", "hidden": false}, {"_id": "6948af7534f46eaf46cbb1e0", "name": "Huan Zhang", "hidden": false}], "publishedAt": "2025-12-19T18:59:11.000Z", "submittedOnDailyAt": "2025-12-22T00:10:07.525Z", "title": "When Reasoning Meets Its Laws", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/", "upvotes": 48, "discussionId": "6948af7534f46eaf46cbb1e1", "projectPage": "https://lore-project.github.io/", "githubRepo": "https://github.com/ASTRAL-Group/LoRe", "githubRepoAddedBy": "user", "ai_summary": "A framework called Laws of Reasoning (LoRe) is introduced to theoretically define desired reasoning behaviors in Large Reasoning Models, with a focus on compute and accuracy laws, and a benchmark (LoRe-Bench) to measure these properties.", "ai_keywords": ["Laws of Reasoning", "LoRe", "compute law", "accuracy law", "question complexity", "monotonicity", "compositionality", "LoRe-Bench", "finetuning approach", "compute-law compositionality"], "githubStars": 15, "summary_zh": "<ul>\n    <li>\u5c3d\u7ba1\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u7684\u63a8\u7406\u884c\u4e3a\u5e38\u5e38\u4e0d\u7b26\u5408\u76f4\u89c9\uff0c\u5bfc\u81f4\u63a8\u7406\u80fd\u529b\u4e0d\u7406\u60f3\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u63a8\u7406\u6cd5\u5219\uff08LoRe\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u63cf\u8ff0LRMs\u4e2d\u7684\u5185\u5728\u63a8\u7406\u6a21\u5f0f\u3002</li>\n    <li>\u6211\u4eec\u9996\u5148\u63d0\u51fa\u8ba1\u7b97\u6cd5\u5219\uff0c\u5047\u8bbe\u63a8\u7406\u8ba1\u7b97\u5e94\u4e0e\u95ee\u9898\u590d\u6742\u6027\u7ebf\u6027\u589e\u957f\uff0c\u5e76\u6269\u5c55\u4e86\u51c6\u786e\u6027\u6cd5\u5219\u3002</li>\n    <li>\u7531\u4e8e\u95ee\u9898\u590d\u6742\u6027\u96be\u4ee5\u91cf\u5316\uff0c\u6211\u4eec\u901a\u8fc7\u5355\u8c03\u6027\u548c\u7ec4\u5408\u6027\u8fd9\u4e24\u4e2a\u7279\u6027\u6765\u68c0\u9a8c\u8fd9\u4e9b\u5047\u8bbe\u3002</li>\n    <li>\u8bc4\u4f30\u8868\u660e\uff0c\u5927\u591a\u6570\u63a8\u7406\u6a21\u578b\u8868\u73b0\u51fa\u5408\u7406\u7684\u5355\u8c03\u6027\uff0c\u4f46\u7f3a\u4e4f\u7ec4\u5408\u6027\uff0c\u56e0\u6b64\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\u6765\u589e\u5f3a\u7ec4\u5408\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Reasoning Models (LRMs) often have confusing reasoning patterns that can lead to poor performance.</li>\n    <li>The study introduces the Laws of Reasoning (LoRe), a framework to understand and improve reasoning in LRMs.</li>\n    <li>LoRe includes a compute law that suggests reasoning should increase steadily with question complexity.</li>\n    <li>Researchers created LoRe-Bench to test two key properties of reasoning: monotonicity and compositionality.</li>\n    <li>Findings show most models are monotonic but struggle with compositionality; a new finetuning method improves reasoning performance significantly.</li>\n</ul>"}, "publishedAt": "2025-12-19T13:59:11.000Z", "title": "When Reasoning Meets Its Laws", "summary": "Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.17901.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16922", "authors": [{"_id": "6944c39ffbf17e708e18605d", "user": {"_id": "63f233820a16587ea967adc2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f233820a16587ea967adc2/1nSoZofPV7UseXzjI2qAH.png", "isPro": false, "fullname": "Sihan XU", "user": "sihanxu", "type": "user"}, "name": "Sihan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:57:39.350Z", "hidden": false}, {"_id": "6944c39ffbf17e708e18605e", "name": "Ziqiao Ma", "hidden": false}, {"_id": "6944c39ffbf17e708e18605f", "name": "Wenhao Chai", "hidden": false}, {"_id": "6944c39ffbf17e708e186060", "name": "Xuweiyi Chen", "hidden": false}, {"_id": "6944c39ffbf17e708e186061", "user": {"_id": "66608add236f958513d21d2e", "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg", "isPro": false, "fullname": "Weiyang Jin", "user": "Wayne-King", "type": "user"}, "name": "Weiyang Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T09:56:57.800Z", "hidden": false}, {"_id": "6944c39ffbf17e708e186062", "name": "Joyce Chai", "hidden": false}, {"_id": "6944c39ffbf17e708e186063", "name": "Saining Xie", "hidden": false}, {"_id": "6944c39ffbf17e708e186064", "name": "Stella X. Yu", "hidden": false}], "publishedAt": "2025-12-18T18:59:58.000Z", "submittedOnDailyAt": "2025-12-19T01:03:18.428Z", "title": "Next-Embedding Prediction Makes Strong Vision Learners", "submittedOnDailyBy": {"_id": "66608add236f958513d21d2e", "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg", "isPro": false, "fullname": "Weiyang Jin", "user": "Wayne-King", "type": "user"}, "summary": "Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.", "upvotes": 47, "discussionId": "6944c3a0fbf17e708e186065", "projectPage": "https://sihanxu.me/nepa", "githubRepo": "https://github.com/SihanXU/nepa", "githubRepoAddedBy": "user", "ai_summary": "Generative pretraining using next embedding prediction outperforms traditional self-supervised methods in visual learning tasks, achieving high accuracy on ImageNet and effective transfer to semantic segmentation.", "ai_keywords": ["generative pretraining", "predictive tasks", "Next-Embedding Predictive Autoregression (NEPA)", "causal masking", "stop gradient", "Transformer", "ImageNet-1k", "top-1 accuracy", "ViT-B", "ViT-L", "semantic segmentation", "ADE20K"], "githubStars": 43, "organization": {"_id": "66df3cb0cf19a8918414cbfe", "name": "SixAILab", "fullname": "SixAILab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f233820a16587ea967adc2/FSRWuJTgSvG0HFKm9_K4Z.jpeg"}, "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u63a2\u8ba8\u5982\u4f55\u5c06\u751f\u6210\u9884\u8bad\u7ec3\u7684\u65b9\u6cd5\u5e94\u7528\u4e8e\u89c6\u89c9\u5b66\u4e60\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u65b9\u5f0f\u3002</li>\n    <li>\u6211\u4eec\u7684\u65b9\u6cd5\u79f0\u4e3a\u4e0b\u4e00\u5d4c\u5165\u9884\u6d4b\u81ea\u56de\u5f52\uff08NEPA\uff09\uff0c\u6a21\u578b\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u7684\u5d4c\u5165\u6765\u8fdb\u884c\u5b66\u4e60\u3002</li>\n    <li>\u4f7f\u7528\u7b80\u5355\u7684Transformer\u6a21\u578b\u5728ImageNet-1k\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u53ea\u9700\u8fdb\u884c\u4e0b\u4e00\u5d4c\u5165\u9884\u6d4b\u3002</li>\n    <li>NEPA\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0cViT-B\u548cViT-L\u6a21\u578b\u5728ImageNet-1K\u4e0a\u5206\u522b\u8fbe\u5230\u4e8683.8%\u548c85.3%\u7684\u51c6\u786e\u7387\u3002</li>\n    <li>\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u3001\u53ef\u6269\u5c55\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u89c6\u89c9\u81ea\u76d1\u7763\u5b66\u4e60\u66ff\u4ee3\u65b9\u6848\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study explores using generative pretraining techniques from language to improve visual learning.</li>\n    <li>It introduces a method called Next-Embedding Predictive Autoregression (NEPA) that predicts future visual features based on past ones.</li>\n    <li>NEPA uses a simple Transformer model trained only on future embedding prediction without complex processes like reconstruction or contrastive loss.</li>\n    <li>The method achieved impressive accuracy on the ImageNet-1K dataset and worked well for semantic segmentation tasks.</li>\n    <li>The authors suggest that this approach could be a simple and effective alternative for visual self-supervised learning.</li>\n</ul>"}, "publishedAt": "2025-12-18T13:59:58.000Z", "title": "Next-Embedding Prediction Makes Strong Vision Learners", "summary": "Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16922.png", "numComments": 1, "submittedBy": {"_id": "66608add236f958513d21d2e", "avatarUrl": "/avatars/53eca0891c98cbb93be899885160a983.svg", "fullname": "Weiyang Jin", "name": "Wayne-King", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "66df3cb0cf19a8918414cbfe", "name": "SixAILab", "fullname": "SixAILab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f233820a16587ea967adc2/FSRWuJTgSvG0HFKm9_K4Z.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16301", "authors": [{"_id": "6944be0ffbf17e708e185fde", "user": {"_id": "63724cfada3183d9d53f2009", "avatarUrl": "/avatars/17838fcf244ecf8d139343bb6c6d8562.svg", "isPro": false, "fullname": "Patrick Jiang", "user": "pat-jj", "type": "user"}, "name": "Pengcheng Jiang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:04.988Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fdf", "user": {"_id": "650488e454b989666d042a49", "avatarUrl": "/avatars/3dc79c6f1a9dce872636dddd38a04670.svg", "isPro": false, "fullname": "Jiacheng Lin", "user": "linjc16", "type": "user"}, "name": "Jiacheng Lin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:02.806Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe0", "user": {"_id": "66d4af28033492801d82b890", "avatarUrl": "/avatars/5e8a2dc1b932a679341976d11b22f6c8.svg", "isPro": false, "fullname": "shi", "user": "Gabshi", "type": "user"}, "name": "Zhiyi Shi", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T16:25:52.445Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe1", "name": "Zifeng Wang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe2", "name": "Luxi He", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe3", "name": "Yichen Wu", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe4", "name": "Ming Zhong", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe5", "user": {"_id": "649c5cf5c1ae48cf4d7dda34", "avatarUrl": "/avatars/a2264945f9f876b690017a93f225f937.svg", "isPro": false, "fullname": "Peiyang Song", "user": "p-song1", "type": "user"}, "name": "Peiyang Song", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:06.854Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe6", "name": "Qizheng Zhang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe7", "name": "Heng Wang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe8", "user": {"_id": "66a3f1c4c38ce500371fd8d4", "avatarUrl": "/avatars/381de938091f1a5c179eef72aa247bbf.svg", "isPro": false, "fullname": "Xueqiang Xu", "user": "XueqiangXu", "type": "user"}, "name": "Xueqiang Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-19T08:58:00.653Z", "hidden": false}, {"_id": "6944be0ffbf17e708e185fe9", "name": "Hanwen Xu", "hidden": false}, {"_id": "6944be0ffbf17e708e185fea", "name": "Pengrui Han", "hidden": false}, {"_id": "6944be0ffbf17e708e185feb", "name": "Dylan Zhang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fec", "name": "Jiashuo Sun", "hidden": false}, {"_id": "6944be0ffbf17e708e185fed", "name": "Chaoqi Yang", "hidden": false}, {"_id": "6944be0ffbf17e708e185fee", "name": "Kun Qian", "hidden": false}, {"_id": "6944be0ffbf17e708e185fef", "name": "Tian Wang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff0", "name": "Changran Hu", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff1", "name": "Manling Li", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff2", "name": "Quanzheng Li", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff3", "name": "Hao Peng", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff4", "name": "Sheng Wang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff5", "name": "Jingbo Shang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff6", "name": "Chao Zhang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff7", "name": "Jiaxuan You", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff8", "name": "Liyuan Liu", "hidden": false}, {"_id": "6944be0ffbf17e708e185ff9", "name": "Pan Lu", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffa", "name": "Yu Zhang", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffb", "name": "Heng Ji", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffc", "name": "Yejin Choi", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffd", "name": "Dawn Song", "hidden": false}, {"_id": "6944be0ffbf17e708e185ffe", "name": "Jimeng Sun", "hidden": false}, {"_id": "6944be0ffbf17e708e185fff", "name": "Jiawei Han", "hidden": false}], "publishedAt": "2025-12-18T08:38:51.000Z", "submittedOnDailyAt": "2025-12-19T00:23:16.990Z", "title": "Adaptation of Agentic AI", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.", "upvotes": 43, "discussionId": "6944be10fbf17e708e186000", "githubRepo": "https://github.com/pat-jj/Awesome-Adaptation-of-Agentic-AI", "githubRepoAddedBy": "user", "ai_summary": "This paper presents a framework for agent and tool adaptation in agentic AI systems, clarifying design strategies and identifying open challenges for improving AI capabilities.", "ai_keywords": ["agentic AI systems", "foundation models", "agent adaptations", "tool adaptations", "tool-execution-signaled", "agent-output-signaled", "agent-agnostic", "agent-supervised"], "githubStars": 262, "summary_zh": "<ul>\n    <li>\u524d\u6cbf\u7684\u4ee3\u7406 AI \u7cfb\u7edf\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u9002\u5e94\u89c4\u5212\u3001\u63a8\u7406\u548c\u4e0e\u5916\u90e8\u5de5\u5177\u4e92\u52a8\u3002</li>\n    <li>\u9002\u5e94\u6027\u662f\u63d0\u9ad8\u6027\u80fd\u3001\u53ef\u9760\u6027\u548c\u901a\u7528\u6027\u7684\u5173\u952e\u673a\u5236\u3002</li>\n    <li>\u672c\u6587\u5c06\u7814\u7a76\u9886\u57df\u7edf\u4e00\u4e3a\u4e00\u4e2a\u7cfb\u7edf\u6846\u67b6\uff0c\u6db5\u76d6\u4ee3\u7406\u9002\u5e94\u548c\u5de5\u5177\u9002\u5e94\u3002</li>\n    <li>\u6846\u67b6\u5e2e\u52a9\u6f84\u6e05\u9002\u5e94\u7b56\u7565\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5e76\u63d0\u4f9b\u5b9e\u9645\u7684\u9009\u62e9\u548c\u5207\u6362\u7b56\u7565\u7684\u6307\u5bfc\u3002</li>\n    <li>\u672c\u6587\u8fd8\u56de\u987e\u4e86\u5404\u7c7b\u4ee3\u8868\u6027\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u4f18\u7f3a\u70b9\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7684\u6311\u6218\u548c\u673a\u4f1a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Advanced AI systems use foundational models that can be customized to perform complex tasks.</li>\n    <li>The paper proposes a structured framework to understand different ways AI systems can adapt.</li>\n    <li>Adaptation types include how tools are used and how the AI interacts with them.</li>\n    <li>It discusses the advantages and disadvantages of various adaptation strategies.</li>\n    <li>The goal is to help researchers and developers create better and more reliable AI systems.</li>\n</ul>"}, "publishedAt": "2025-12-18T03:38:51.000Z", "title": "Adaptation of Agentic AI", "summary": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16301.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "isAuthorParticipating": true}, {"paper": {"id": "2512.15176", "authors": [{"_id": "694370d7542d62d58a7bf698", "name": "Zicong Cheng", "hidden": false}, {"_id": "694370d7542d62d58a7bf699", "name": "Guo-Wei Yang", "hidden": false}, {"_id": "694370d7542d62d58a7bf69a", "name": "Jia Li", "hidden": false}, {"_id": "694370d7542d62d58a7bf69b", "name": "Zhijie Deng", "hidden": false}, {"_id": "694370d7542d62d58a7bf69c", "user": {"_id": "6571b51fd5c6a6d3b0ba68ad", "avatarUrl": "/avatars/0ccd8fe8de857753b534356a90eb10f0.svg", "isPro": false, "fullname": "gmh", "user": "menghao22", "type": "user"}, "name": "Meng-Hao Guo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:54.790Z", "hidden": false}, {"_id": "694370d7542d62d58a7bf69d", "name": "Shi-Min Hu", "hidden": false}], "publishedAt": "2025-12-17T08:19:04.000Z", "submittedOnDailyAt": "2025-12-18T00:44:13.153Z", "title": "DEER: Draft with Diffusion, Verify with Autoregressive Models", "submittedOnDailyBy": {"_id": "6571b51fd5c6a6d3b0ba68ad", "avatarUrl": "/avatars/0ccd8fe8de857753b534356a90eb10f0.svg", "isPro": false, "fullname": "gmh", "user": "menghao22", "type": "user"}, "summary": "Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/", "upvotes": 37, "discussionId": "694370d8542d62d58a7bf69e", "projectPage": "https://czc726.github.io/DEER/", "ai_summary": "DEER framework uses diffusion large language models for efficient speculative decoding, overcoming the limitations of autoregressive drafters with better speed and draft quality.", "ai_keywords": ["autoregressive decoding", "speculative decoding", "diffusion large language model", "dLLM", "draft-verify scheme", "step-wise uncertainty accumulation", "parallel decoding", "two-stage training pipeline", "single-step decoding", "HumanEval", "Qwen3-30B-A3B"], "summary_zh": "<ul>\n    <li>\u6548\u7387\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7cfb\u7edf\u9762\u4e34\u7684\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u5f53\u524d\u7684\u81ea\u56de\u5f52\u89e3\u7801\u65b9\u6cd5\u5b58\u5728\u5ef6\u8fdf\u95ee\u9898\u3002</li>\n    <li>\u73b0\u6709\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u81ea\u56de\u5f52\u8349\u7a3f\u6a21\u578b\uff0c\u5bfc\u81f4\u4fe1\u4efb\u9010\u6e10\u4e0b\u964d\u548c\u89e3\u7801\u901f\u5ea6\u53d7\u9650\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u6269\u6563\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08dLLM\uff09\u4f5c\u4e3a\u8349\u7a3f\u751f\u6210\u5668\uff0c\u80fd\u591f\u901a\u8fc7\u4e0d\u540c\u7684\u6982\u7387\u5efa\u6a21\u548c\u9ad8\u6548\u7684\u5e76\u884c\u89e3\u7801\u7b56\u7565\u6765\u514b\u670d\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86DEER\u6846\u67b6\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u8fdb\u884c\u8349\u7a3f\u751f\u6210\uff0c\u5e76\u7528\u81ea\u56de\u5f52\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u6548\u7387\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cDEER\u5728\u8349\u7a3f\u63a5\u53d7\u957f\u5ea6\u548c\u901f\u5ea6\u4e0a\u5747\u8d85\u8fc7\u73b0\u6709\u6a21\u578b\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Efficiency is a big challenge for systems that use large language models (LLMs) due to slow autoregressive (AR) decoding.</li>\n    <li>Speculative decoding helps improve speed but has issues with trust and speed due to reliance on AR models.</li>\n    <li>This paper introduces DEER, a new framework that uses diffusion models for drafting and AR models for verification, solving the previous issues.</li>\n    <li>DEER can draft longer segments (up to 32 tokens) compared to existing methods (10 tokens with EAGLE-3).</li>\n    <li>DEER also achieves a significant speed increase (5.54x) in performance over EAGLE-3 (2.41x). Code and resources for DEER will be available online.</li>\n</ul>"}, "publishedAt": "2025-12-17T03:19:04.000Z", "title": "DEER: Draft with Diffusion, Verify with Autoregressive Models", "summary": "Efficiency, as a critical practical challenge for LLM-driven agentic and reasoning systems, is increasingly constrained by the inherent latency of autoregressive (AR) decoding. Speculative decoding mitigates this cost through a draft-verify scheme, yet existing approaches rely on AR draft models (a.k.a., drafters), which introduce two fundamental issues: (1) step-wise uncertainty accumulation leads to a progressive collapse of trust between the target model and the drafter, and (2) inherently sequential decoding of AR drafters. Together, these factors cause limited speedups. In this paper, we show that a diffusion large language model (dLLM) drafters can naturally overcome these issues through its fundamentally different probabilistic modeling and efficient parallel decoding strategy. Building on this insight, we introduce DEER, an efficient speculative decoding framework that drafts with diffusion and verifies with AR models. To enable high-quality drafting, DEER employs a two-stage training pipeline to align the dLLM-based drafters with the target AR model, and further adopts single-step decoding to generate long draft segments. Experiments show DEER reaches draft acceptance lengths of up to 32 tokens, far surpassing the 10 tokens achieved by EAGLE-3. Moreover, on HumanEval with Qwen3-30B-A3B, DEER attains a 5.54x speedup, while EAGLE-3 achieves only 2.41x. Code, model, demo, etc, will be available at https://czc726.github.io/DEER/", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15176.png", "numComments": 2, "submittedBy": {"_id": "6571b51fd5c6a6d3b0ba68ad", "avatarUrl": "/avatars/0ccd8fe8de857753b534356a90eb10f0.svg", "fullname": "gmh", "name": "menghao22", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2511.18538", "authors": [{"_id": "692e667137312eaa83fd8832", "user": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "name": "Jian Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:34.886Z", "hidden": false}, {"_id": "692e667137312eaa83fd8833", "name": "Xianglong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8834", "name": "Weifeng Lv", "hidden": false}, {"_id": "692e667137312eaa83fd8835", "name": "Ken Deng", "hidden": false}, {"_id": "692e667137312eaa83fd8836", "name": "Shawn Guo", "hidden": false}, {"_id": "692e667137312eaa83fd8837", "name": "Lin Jing", "hidden": false}, {"_id": "692e667137312eaa83fd8838", "name": "Yizhi Li", "hidden": false}, {"_id": "692e667137312eaa83fd8839", "name": "Shark Liu", "hidden": false}, {"_id": "692e667137312eaa83fd883a", "name": "Xianzhen Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883b", "name": "Yuyu Luo", "hidden": false}, {"_id": "692e667137312eaa83fd883c", "name": "Changzai Pan", "hidden": false}, {"_id": "692e667137312eaa83fd883d", "name": "Ensheng Shi", "hidden": false}, {"_id": "692e667137312eaa83fd883e", "name": "Yingshui Tan", "hidden": false}, {"_id": "692e667137312eaa83fd883f", "name": "Renshuai Tao", "hidden": false}, {"_id": "692e667137312eaa83fd8840", "user": {"_id": "66a8e2538407031e388c501f", "avatarUrl": "/avatars/d16d51f7b1e111efd6d0985995b614be.svg", "isPro": false, "fullname": "wjj", "user": "wuyuverse", "type": "user"}, "name": "Jiajun Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:36.195Z", "hidden": false}, {"_id": "692e667137312eaa83fd8841", "name": "Xianjie Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8842", "name": "Zhenhe Wu", "hidden": false}, {"_id": "692e667137312eaa83fd8843", "name": "Daoguang Zan", "hidden": false}, {"_id": "692e667137312eaa83fd8844", "name": "Chenchen Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8845", "user": {"_id": "672c9ba69380700b602c46c1", "avatarUrl": "/avatars/3d0fd966df540d34095d2c84ce449180.svg", "isPro": false, "fullname": "wei zhang", "user": "zwpride", "type": "user"}, "name": "Wei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:37.970Z", "hidden": false}, {"_id": "692e667137312eaa83fd8846", "name": "He Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd8847", "user": {"_id": "62b7fb545233925f253531c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg", "isPro": false, "fullname": "Terry Yue Zhuo", "user": "terryyz", "type": "user"}, "name": "Terry Yue Zhuo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T16:50:22.285Z", "hidden": false}, {"_id": "692e667137312eaa83fd8848", "name": "Kerui Cao", "hidden": false}, {"_id": "692e667137312eaa83fd8849", "name": "Xianfu Cheng", "hidden": false}, {"_id": "692e667137312eaa83fd884a", "name": "Jun Dong", "hidden": false}, {"_id": "692e667137312eaa83fd884b", "name": "Shengjie Fang", "hidden": false}, {"_id": "692e667137312eaa83fd884c", "name": "Zhiwei Fei", "hidden": false}, {"_id": "692e667137312eaa83fd884d", "name": "Xiangyuan Guan", "hidden": false}, {"_id": "692e667137312eaa83fd884e", "name": "Qipeng Guo", "hidden": false}, {"_id": "692e667137312eaa83fd884f", "name": "Zhiguang Han", "hidden": false}, {"_id": "692e667137312eaa83fd8850", "name": "Joseph James", "hidden": false}, {"_id": "692e667137312eaa83fd8851", "name": "Tianqi Luo", "hidden": false}, {"_id": "692e667137312eaa83fd8852", "user": {"_id": "67f1037cd5f976f3d4777390", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0cXH40AcE-M-H21cSNBqZ.png", "isPro": false, "fullname": "RenyuanLi", "user": "RenyuanLi", "type": "user"}, "name": "Renyuan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:17:45.344Z", "hidden": false}, {"_id": "692e667137312eaa83fd8853", "name": "Yuhang Li", "hidden": false}, {"_id": "692e667137312eaa83fd8854", "name": "Yiming Liang", "hidden": false}, {"_id": "692e667137312eaa83fd8855", "name": "Congnan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8856", "name": "Jiaheng Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8857", "name": "Qian Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8858", "name": "Ruitong Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8859", "name": "Tyler Loakman", "hidden": false}, {"_id": "692e667137312eaa83fd885a", "name": "Xiangxin Meng", "hidden": false}, {"_id": "692e667137312eaa83fd885b", "name": "Chuang Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885c", "name": "Tianhao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd885d", "name": "Jiajun Shi", "hidden": false}, {"_id": "692e667137312eaa83fd885e", "name": "Mingjie Tang", "hidden": false}, {"_id": "692e667137312eaa83fd885f", "name": "Boyang Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8860", "name": "Haowen Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8861", "name": "Yunli Wang", "hidden": false}, {"_id": "692e667137312eaa83fd8862", "user": {"_id": "668619ce7374cac565759731", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668619ce7374cac565759731/tUtiyIQRGsMdq3HB2yYIL.jpeg", "isPro": false, "fullname": "Fanglin Xu", "user": "Tswatery", "type": "user"}, "name": "Fanglin Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T15:15:03.333Z", "hidden": false}, {"_id": "692e667137312eaa83fd8863", "name": "Zihan Xu", "hidden": false}, {"_id": "692e667137312eaa83fd8864", "name": "Fei Yuan", "hidden": false}, {"_id": "692e667137312eaa83fd8865", "user": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "name": "Ge Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:34.025Z", "hidden": false}, {"_id": "692e667137312eaa83fd8866", "user": {"_id": "65f40e83653c231cbaf7defe", "avatarUrl": "/avatars/afa5ce72324112739e539865c9aee26b.svg", "isPro": false, "fullname": "Jiayi Zhang", "user": "didiforhugface", "type": "user"}, "name": "Jiayi Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:39:32.149Z", "hidden": false}, {"_id": "692e667137312eaa83fd8867", "name": "Xinhao Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8868", "name": "Wangchunshu Zhou", "hidden": false}, {"_id": "692e667137312eaa83fd8869", "name": "Hualei Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886a", "name": "King Zhu", "hidden": false}, {"_id": "692e667137312eaa83fd886b", "name": "Brown Dai", "hidden": false}, {"_id": "692e667137312eaa83fd886c", "name": "Aishan Liu", "hidden": false}, {"_id": "692e667137312eaa83fd886d", "name": "Zhoujun Li", "hidden": false}, {"_id": "692e667137312eaa83fd886e", "name": "Chenghua Lin", "hidden": false}, {"_id": "692e667137312eaa83fd886f", "name": "Tianyu Liu", "hidden": false}, {"_id": "692e667137312eaa83fd8870", "name": "Chao Peng", "hidden": false}, {"_id": "692e667137312eaa83fd8871", "name": "Kai Shen", "hidden": false}, {"_id": "692e667137312eaa83fd8872", "name": "Libo Qin", "hidden": false}, {"_id": "692e667137312eaa83fd8873", "name": "Shuangyong Song", "hidden": false}, {"_id": "692e667137312eaa83fd8874", "name": "Zizheng Zhan", "hidden": false}, {"_id": "692e667137312eaa83fd8875", "name": "Jiajun Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8876", "name": "Jie Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8877", "name": "Zhaoxiang Zhang", "hidden": false}, {"_id": "692e667137312eaa83fd8878", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-11-23T17:09:34.000Z", "submittedOnDailyAt": "2025-12-02T02:55:07.234Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "submittedOnDailyBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "isPro": false, "fullname": "Yang Jian", "user": "CSJianYang", "type": "user"}, "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "upvotes": 240, "discussionId": "692e667237312eaa83fd8879", "ai_summary": "A comprehensive guide to code LLMs, covering their lifecycle from data curation to deployment, including techniques, trade-offs, and research-practice gaps.", "ai_keywords": ["Transformer-based architectures", "HumanEval", "prompting paradigms", "code pre-training", "supervised fine-tuning", "reinforcement learning", "autonomous coding agents", "GPT-4", "Claude", "LLaMA", "StarCoder", "Code LLaMA", "DeepSeek-Coder", "QwenCoder", "code correctness", "security", "contextual awareness", "software-related code tasks", "scaling law", "framework selection", "hyperparameter sensitivity", "model architectures", "dataset comparisons"], "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6539\u53d8\u4e86\u81ea\u52a8\u8f6f\u4ef6\u5f00\u53d1\uff0c\u53ef\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u76f4\u63a5\u7ffb\u8bd1\u6210\u529f\u80fd\u4ee3\u7801\u3002</li>\n    <li>\u8bb8\u591a\u5de5\u5177\uff08\u5982GitHub Copilot\u3001Cursor\u7b49\uff09\u63a8\u52a8\u4e86\u8fd9\u79cd\u6280\u672f\u7684\u5546\u4e1a\u5e94\u7528\uff0c\u6027\u80fd\u5927\u5e45\u63d0\u5347\u3002</li>\n    <li>\u672c\u6587\u63d0\u4f9b\u4e86\u5173\u4e8e\u4ee3\u7801LLMs\u7684\u5168\u9762\u6307\u5357\uff0c\u6db5\u76d6\u6a21\u578b\u751f\u547d\u5468\u671f\u7684\u5404\u4e2a\u9636\u6bb5\u3002</li>\n    <li>\u5206\u6790\u4e86\u901a\u7528LLMs\u548c\u4e13\u7528\u4ee3\u7801LLMs\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u5b83\u4eec\u7684\u8bbe\u8ba1\u548c\u6280\u672f\u9009\u62e9\u3002</li>\n    <li>\u63a2\u8ba8\u4e86\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u8fdb\u884c\u4e86\u591a\u9879\u5b9e\u9a8c\u4ee5\u5206\u6790\u4ee3\u7801\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7b49\u65b9\u9762\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) are changing software development by turning natural language into code, with tools like GitHub Copilot leading the way.</li>\n    <li>The performance of these models has greatly improved from basic systems to advanced architectures, achieving over 95% success on coding benchmarks.</li>\n    <li>This work offers a detailed guide on code LLMs, covering their entire lifecycle from data collection to training methods and how they work in practice.</li>\n    <li>It examines both general LLMs and those specialized for coding, discussing their strengths and weaknesses in real-world applications.</li>\n    <li>The study addresses gaps between academic research and practical software development needs, and includes experiments on training methods and model performance.</li>\n</ul>"}, "publishedAt": "2025-11-23T12:09:34.000Z", "title": "From Code Foundation Models to Agents and Applications: A Practical Guide to Code Intelligence", "summary": "Large language models (LLMs) have fundamentally transformed automated software development by enabling direct translation of natural language descriptions into functional code, driving commercial adoption through tools like Github Copilot (Microsoft), Cursor (Anysphere), Trae (ByteDance), and Claude Code (Anthropic). While the field has evolved dramatically from rule-based systems to Transformer-based architectures, achieving performance improvements from single-digit to over 95\\% success rates on benchmarks like HumanEval. In this work, we provide a comprehensive synthesis and practical guide (a series of analytic and probing experiments) about code LLMs, systematically examining the complete model life cycle from data curation to post-training through advanced prompting paradigms, code pre-training, supervised fine-tuning, reinforcement learning, and autonomous coding agents. We analyze the code capability of the general LLMs (GPT-4, Claude, LLaMA) and code-specialized LLMs (StarCoder, Code LLaMA, DeepSeek-Coder, and QwenCoder), critically examining the techniques, design decisions, and trade-offs. Further, we articulate the research-practice gap between academic research (e.g., benchmarks and tasks) and real-world deployment (e.g., software-related code tasks), including code correctness, security, contextual awareness of large codebases, and integration with development workflows, and map promising research directions to practical needs. Last, we conduct a series of experiments to provide a comprehensive analysis of code pre-training, supervised fine-tuning, and reinforcement learning, covering scaling law, framework selection, hyperparameter sensitivity, model architectures, and dataset comparisons.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18538.png", "numComments": 11, "submittedBy": {"_id": "64ccb9bfead94891d12aef42", "avatarUrl": "/avatars/c54809d43d93d3f0766bd2555cecc4e3.svg", "fullname": "Yang Jian", "name": "CSJianYang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 21}, "organization": {"_id": "63ba7720fc454697637969f1", "name": "Beihang", "fullname": "Beihang University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63ba7666c138c8f2b7844b58/n98lZU9VWxYgWIkzE_6o4.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.02556", "authors": [{"_id": "692fa6da26742347f61dab24", "name": "DeepSeek-AI", "hidden": false}, {"_id": "692fa6da26742347f61dab25", "name": "Aixin Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab26", "name": "Aoxue Mei", "hidden": false}, {"_id": "692fa6da26742347f61dab27", "name": "Bangcai Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab28", "name": "Bing Xue", "hidden": false}, {"_id": "692fa6da26742347f61dab29", "user": {"_id": "6523d81d56fe05f216a559f6", "avatarUrl": "/avatars/07fcf56b5b8a0b64c31bdfe8fbf41cc6.svg", "isPro": false, "fullname": "Bingxuan Wang", "user": "YellowDoge", "type": "user"}, "name": "Bingxuan Wang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:23.047Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2a", "name": "Bingzheng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab2b", "name": "Bochao Wu", "hidden": false}, {"_id": "692fa6da26742347f61dab2c", "name": "Bowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab2d", "user": {"_id": "644200d95d600fb09520de53", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/prs0wIjQx7PE4-IYkXDvw.jpeg", "isPro": false, "fullname": "Chaofan Lin", "user": "siriusneo", "type": "user"}, "name": "Chaofan Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-03T09:26:56.864Z", "hidden": false}, {"_id": "692fa6da26742347f61dab2e", "name": "Chen Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab2f", "name": "Chengda Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab30", "name": "Chenggang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab31", "name": "Chengqi Deng", "hidden": false}, {"_id": "692fa6da26742347f61dab32", "name": "Chenhao Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab33", "name": "Chong Ruan", "hidden": false}, {"_id": "692fa6da26742347f61dab34", "name": "Damai Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab35", "name": "Daya Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab36", "name": "Dejian Yang", "hidden": false}, {"_id": "692fa6da26742347f61dab37", "name": "Deli Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab38", "name": "Erhang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab39", "name": "Fangqi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab3a", "name": "Fangyun Lin", "hidden": false}, {"_id": "692fa6da26742347f61dab3b", "name": "Fucong Dai", "hidden": false}, {"_id": "692fa6da26742347f61dab3c", "name": "Guangbo Hao", "hidden": false}, {"_id": "692fa6da26742347f61dab3d", "name": "Guanting Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab3e", "name": "Guowei Li", "hidden": false}, {"_id": "692fa6da26742347f61dab3f", "name": "H. Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab40", "name": "Hanwei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab41", "name": "Hao Li", "hidden": false}, {"_id": "692fa6da26742347f61dab42", "name": "Haofen Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab43", "name": "Haoran Wei", "hidden": false}, {"_id": "692fa6da26742347f61dab44", "name": "Haowei Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab45", "name": "Haowen Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab46", "name": "Haozhe Ji", "hidden": false}, {"_id": "692fa6da26742347f61dab47", "name": "Honghui Ding", "hidden": false}, {"_id": "692fa6da26742347f61dab48", "name": "Hongxuan Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab49", "name": "Huanqi Cao", "hidden": false}, {"_id": "692fa6da26742347f61dab4a", "name": "Huazuo Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab4b", "name": "Hui Qu", "hidden": false}, {"_id": "692fa6da26742347f61dab4c", "name": "Hui Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab4d", "name": "Jialiang Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab4e", "name": "Jiashi Li", "hidden": false}, {"_id": "692fa6da26742347f61dab4f", "name": "Jiaxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab50", "name": "Jiewen Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab51", "name": "Jingchang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab52", "name": "Jingting Xiang", "hidden": false}, {"_id": "692fa6da26742347f61dab53", "name": "Jingyang Yuan", "hidden": false}, {"_id": "692fa6da26742347f61dab54", "name": "Jingyuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dab55", "name": "Jinhua Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab56", "name": "Jun Ran", "hidden": false}, {"_id": "692fa6da26742347f61dab57", "name": "Junguang Jiang", "hidden": false}, {"_id": "692fa6da26742347f61dab58", "name": "Junjie Qiu", "hidden": false}, {"_id": "692fa6da26742347f61dab59", "name": "Junlong Li", "hidden": false}, {"_id": "692fa6da26742347f61dab5a", "name": "Junxiao Song", "hidden": false}, {"_id": "692fa6da26742347f61dab5b", "name": "Kai Dong", "hidden": false}, {"_id": "692fa6da26742347f61dab5c", "name": "Kaige Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab5d", "name": "Kang Guan", "hidden": false}, {"_id": "692fa6da26742347f61dab5e", "name": "Kexin Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab5f", "name": "Kexing Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab60", "name": "Kezhao Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab61", "name": "Kuai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab62", "name": "Lean Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab63", "name": "Lecong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab64", "name": "Lei Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab65", "name": "Liang Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dab66", "name": "Liangsheng Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab67", "name": "Lihua Guo", "hidden": false}, {"_id": "692fa6da26742347f61dab68", "name": "Lingxiao Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab69", "name": "Linwang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab6a", "name": "Litong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab6b", "name": "Liyue Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6c", "name": "M. S. Di", "hidden": false}, {"_id": "692fa6da26742347f61dab6d", "name": "M. Y Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab6e", "name": "Mingchuan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab6f", "name": "Minghua Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab70", "name": "Minghui Tang", "hidden": false}, {"_id": "692fa6da26742347f61dab71", "name": "Mingxu Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab72", "name": "Panpan Huang", "hidden": false}, {"_id": "692fa6da26742347f61dab73", "name": "Peixin Cong", "hidden": false}, {"_id": "692fa6da26742347f61dab74", "name": "Peiyi Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab75", "name": "Qiancheng Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab76", "name": "Qihao Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dab77", "name": "Qingyang Li", "hidden": false}, {"_id": "692fa6da26742347f61dab78", "name": "Qinyu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab79", "name": "Qiushi Du", "hidden": false}, {"_id": "692fa6da26742347f61dab7a", "name": "Ruiling Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab7b", "name": "Ruiqi Ge", "hidden": false}, {"_id": "692fa6da26742347f61dab7c", "name": "Ruisong Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab7d", "name": "Ruizhe Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab7e", "name": "Runji Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab7f", "name": "Runqiu Yin", "hidden": false}, {"_id": "692fa6da26742347f61dab80", "name": "Runxin Xu", "hidden": false}, {"_id": "692fa6da26742347f61dab81", "name": "Ruomeng Shen", "hidden": false}, {"_id": "692fa6da26742347f61dab82", "name": "Ruoyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab83", "name": "S. H. Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab84", "name": "Shanghao Lu", "hidden": false}, {"_id": "692fa6da26742347f61dab85", "name": "Shangyan Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab86", "name": "Shanhuang Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab87", "name": "Shaofei Cai", "hidden": false}, {"_id": "692fa6da26742347f61dab88", "name": "Shaoyuan Chen", "hidden": false}, {"_id": "692fa6da26742347f61dab89", "name": "Shengding Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8a", "name": "Shengyu Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab8b", "name": "Shiqiang Hu", "hidden": false}, {"_id": "692fa6da26742347f61dab8c", "name": "Shirong Ma", "hidden": false}, {"_id": "692fa6da26742347f61dab8d", "name": "Shiyu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dab8e", "name": "Shuiping Yu", "hidden": false}, {"_id": "692fa6da26742347f61dab8f", "name": "Shunfeng Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab90", "name": "Shuting Pan", "hidden": false}, {"_id": "692fa6da26742347f61dab91", "name": "Songyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dab92", "name": "Tao Ni", "hidden": false}, {"_id": "692fa6da26742347f61dab93", "name": "Tao Yun", "hidden": false}, {"_id": "692fa6da26742347f61dab94", "name": "Tian Pei", "hidden": false}, {"_id": "692fa6da26742347f61dab95", "name": "Tian Ye", "hidden": false}, {"_id": "692fa6da26742347f61dab96", "name": "Tianyuan Yue", "hidden": false}, {"_id": "692fa6da26742347f61dab97", "name": "Wangding Zeng", "hidden": false}, {"_id": "692fa6da26742347f61dab98", "name": "Wen Liu", "hidden": false}, {"_id": "692fa6da26742347f61dab99", "name": "Wenfeng Liang", "hidden": false}, {"_id": "692fa6da26742347f61dab9a", "name": "Wenjie Pang", "hidden": false}, {"_id": "692fa6da26742347f61dab9b", "name": "Wenjing Luo", "hidden": false}, {"_id": "692fa6da26742347f61dab9c", "name": "Wenjun Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9d", "name": "Wentao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dab9e", "name": "Xi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dab9f", "name": "Xiangwen Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba0", "name": "Xiao Bi", "hidden": false}, {"_id": "692fa6da26742347f61daba1", "name": "Xiaodong Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba2", "name": "Xiaohan Wang", "hidden": false}, {"_id": "692fa6da26742347f61daba3", "name": "Xiaokang Chen", "hidden": false}, {"_id": "692fa6da26742347f61daba4", "name": "Xiaokang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61daba5", "name": "Xiaotao Nie", "hidden": false}, {"_id": "692fa6da26742347f61daba6", "name": "Xin Cheng", "hidden": false}, {"_id": "692fa6da26742347f61daba7", "name": "Xin Liu", "hidden": false}, {"_id": "692fa6da26742347f61daba8", "name": "Xin Xie", "hidden": false}, {"_id": "692fa6da26742347f61daba9", "name": "Xingchao Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabaa", "name": "Xingkai Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabab", "name": "Xingyou Li", "hidden": false}, {"_id": "692fa6da26742347f61dabac", "name": "Xinyu Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabad", "name": "Xinyuan Li", "hidden": false}, {"_id": "692fa6da26742347f61dabae", "name": "Xu Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabaf", "name": "Xuecheng Su", "hidden": false}, {"_id": "692fa6da26742347f61dabb0", "user": {"_id": "64364e87fae2870051496e13", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/t67EsNoRvRYXKwi0G59oa.jpeg", "isPro": false, "fullname": "Xuehai Pan", "user": "XuehaiPan", "type": "user"}, "name": "Xuehai Pan", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:49:11.632Z", "hidden": false}, {"_id": "692fa6da26742347f61dabb1", "name": "Xuheng Lin", "hidden": false}, {"_id": "692fa6da26742347f61dabb2", "name": "Xuwei Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabb3", "name": "Y. Q. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabb4", "name": "Yang Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabb5", "name": "Yanhong Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabb6", "name": "Yanru Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabb7", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb8", "name": "Yao Li", "hidden": false}, {"_id": "692fa6da26742347f61dabb9", "name": "Yao Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabba", "name": "Yaofeng Sun", "hidden": false}, {"_id": "692fa6da26742347f61dabbb", "name": "Yaohui Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabbc", "name": "Yi Qian", "hidden": false}, {"_id": "692fa6da26742347f61dabbd", "name": "Yi Yu", "hidden": false}, {"_id": "692fa6da26742347f61dabbe", "name": "Yichao Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabbf", "name": "Yifan Ding", "hidden": false}, {"_id": "692fa6da26742347f61dabc0", "name": "Yifan Shi", "hidden": false}, {"_id": "692fa6da26742347f61dabc1", "name": "Yiliang Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabc2", "name": "Ying He", "hidden": false}, {"_id": "692fa6da26742347f61dabc3", "name": "Ying Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabc4", "name": "Yinmin Zhong", "hidden": false}, {"_id": "692fa6da26742347f61dabc5", "name": "Yishi Piao", "hidden": false}, {"_id": "692fa6da26742347f61dabc6", "name": "Yisong Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabc7", "name": "Yixiao Chen", "hidden": false}, {"_id": "692fa6da26742347f61dabc8", "name": "Yixuan Tan", "hidden": false}, {"_id": "692fa6da26742347f61dabc9", "name": "Yixuan Wei", "hidden": false}, {"_id": "692fa6da26742347f61dabca", "name": "Yiyang Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabcb", "name": "Yiyuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabcc", "name": "Yonglun Yang", "hidden": false}, {"_id": "692fa6da26742347f61dabcd", "name": "Yongqiang Guo", "hidden": false}, {"_id": "692fa6da26742347f61dabce", "name": "Yongtong Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabcf", "name": "Yu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd0", "name": "Yuan Cheng", "hidden": false}, {"_id": "692fa6da26742347f61dabd1", "name": "Yuan Ou", "hidden": false}, {"_id": "692fa6da26742347f61dabd2", "name": "Yuanfan Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabd3", "name": "Yuduan Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabd4", "name": "Yue Gong", "hidden": false}, {"_id": "692fa6da26742347f61dabd5", "name": "Yuhan Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabd6", "name": "Yuheng Zou", "hidden": false}, {"_id": "692fa6da26742347f61dabd7", "name": "Yukun Li", "hidden": false}, {"_id": "692fa6da26742347f61dabd8", "name": "Yunfan Xiong", "hidden": false}, {"_id": "692fa6da26742347f61dabd9", "name": "Yuxiang Luo", "hidden": false}, {"_id": "692fa6da26742347f61dabda", "name": "Yuxiang You", "hidden": false}, {"_id": "692fa6da26742347f61dabdb", "name": "Yuxuan Liu", "hidden": false}, {"_id": "692fa6da26742347f61dabdc", "name": "Yuyang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dabdd", "name": "Z. F. Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabde", "name": "Z. Z. Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabdf", "name": "Zehua Zhao", "hidden": false}, {"_id": "692fa6da26742347f61dabe0", "name": "Zehui Ren", "hidden": false}, {"_id": "692fa6da26742347f61dabe1", "name": "Zhangli Sha", "hidden": false}, {"_id": "692fa6da26742347f61dabe2", "name": "Zhe Fu", "hidden": false}, {"_id": "692fa6da26742347f61dabe3", "name": "Zhean Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabe4", "name": "Zhenda Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabe5", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabe6", "name": "Zhewen Hao", "hidden": false}, {"_id": "692fa6da26742347f61dabe7", "name": "Zhibin Gou", "hidden": false}, {"_id": "692fa6da26742347f61dabe8", "name": "Zhicheng Ma", "hidden": false}, {"_id": "692fa6da26742347f61dabe9", "name": "Zhigang Yan", "hidden": false}, {"_id": "692fa6da26742347f61dabea", "name": "Zhihong Shao", "hidden": false}, {"_id": "692fa6da26742347f61dabeb", "name": "Zhixian Huang", "hidden": false}, {"_id": "692fa6da26742347f61dabec", "name": "Zhiyu Wu", "hidden": false}, {"_id": "692fa6da26742347f61dabed", "name": "Zhuoshu Li", "hidden": false}, {"_id": "692fa6da26742347f61dabee", "name": "Zhuping Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabef", "name": "Zian Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabf0", "name": "Zihao Wang", "hidden": false}, {"_id": "692fa6da26742347f61dabf1", "name": "Zihui Gu", "hidden": false}, {"_id": "692fa6da26742347f61dabf2", "name": "Zijia Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dabf3", "name": "Zilin Li", "hidden": false}, {"_id": "692fa6da26742347f61dabf4", "name": "Zipeng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dabf5", "name": "Ziwei Xie", "hidden": false}, {"_id": "692fa6da26742347f61dabf6", "name": "Ziyi Gao", "hidden": false}, {"_id": "692fa6da26742347f61dabf7", "name": "Zizheng Pan", "hidden": false}, {"_id": "692fa6da26742347f61dabf8", "name": "Zongqing Yao", "hidden": false}, {"_id": "692fa6da26742347f61dabf9", "name": "Bei Feng", "hidden": false}, {"_id": "692fa6da26742347f61dabfa", "name": "Hui Li", "hidden": false}, {"_id": "692fa6da26742347f61dabfb", "name": "J. L. Cai", "hidden": false}, {"_id": "692fa6da26742347f61dabfc", "name": "Jiaqi Ni", "hidden": false}, {"_id": "692fa6da26742347f61dabfd", "name": "Lei Xu", "hidden": false}, {"_id": "692fa6da26742347f61dabfe", "name": "Meng Li", "hidden": false}, {"_id": "692fa6da26742347f61dabff", "name": "Ning Tian", "hidden": false}, {"_id": "692fa6da26742347f61dac00", "name": "R. J. Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac01", "name": "R. L. Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac02", "name": "S. S. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac03", "name": "Shuang Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac04", "name": "Tianyu Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac05", "name": "X. Q. Li", "hidden": false}, {"_id": "692fa6da26742347f61dac06", "name": "Xiangyue Jin", "hidden": false}, {"_id": "692fa6da26742347f61dac07", "name": "Xiaojin Shen", "hidden": false}, {"_id": "692fa6da26742347f61dac08", "name": "Xiaosha Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac09", "name": "Xinnan Song", "hidden": false}, {"_id": "692fa6da26742347f61dac0a", "name": "Xinyi Zhou", "hidden": false}, {"_id": "692fa6da26742347f61dac0b", "name": "Y. X. Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac0c", "name": "Yanping Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac0d", "name": "Yaohui Li", "hidden": false}, {"_id": "692fa6da26742347f61dac0e", "name": "Yi Zheng", "hidden": false}, {"_id": "692fa6da26742347f61dac0f", "name": "Yuchen Zhu", "hidden": false}, {"_id": "692fa6da26742347f61dac10", "name": "Yunxian Ma", "hidden": false}, {"_id": "692fa6da26742347f61dac11", "name": "Zhen Huang", "hidden": false}, {"_id": "692fa6da26742347f61dac12", "name": "Zhipeng Xu", "hidden": false}, {"_id": "692fa6da26742347f61dac13", "name": "Zhongyu Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac14", "name": "Dongjie Ji", "hidden": false}, {"_id": "692fa6da26742347f61dac15", "name": "Jian Liang", "hidden": false}, {"_id": "692fa6da26742347f61dac16", "name": "Jianzhong Guo", "hidden": false}, {"_id": "692fa6da26742347f61dac17", "name": "Jin Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac18", "name": "Leyi Xia", "hidden": false}, {"_id": "692fa6da26742347f61dac19", "name": "Miaojun Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac1a", "name": "Mingming Li", "hidden": false}, {"_id": "692fa6da26742347f61dac1b", "name": "Peng Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac1c", "name": "Ruyi Chen", "hidden": false}, {"_id": "692fa6da26742347f61dac1d", "name": "Shangmian Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac1e", "name": "Shaoqing Wu", "hidden": false}, {"_id": "692fa6da26742347f61dac1f", "name": "Shengfeng Ye", "hidden": false}, {"_id": "692fa6da26742347f61dac20", "name": "T. Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac21", "name": "W. L. Xiao", "hidden": false}, {"_id": "692fa6da26742347f61dac22", "name": "Wei An", "hidden": false}, {"_id": "692fa6da26742347f61dac23", "name": "Xianzu Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac24", "name": "Xiaowen Sun", "hidden": false}, {"_id": "692fa6da26742347f61dac25", "name": "Xiaoxiang Wang", "hidden": false}, {"_id": "692fa6da26742347f61dac26", "name": "Ying Tang", "hidden": false}, {"_id": "692fa6da26742347f61dac27", "name": "Yukun Zha", "hidden": false}, {"_id": "692fa6da26742347f61dac28", "name": "Zekai Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac29", "name": "Zhe Ju", "hidden": false}, {"_id": "692fa6da26742347f61dac2a", "name": "Zhen Zhang", "hidden": false}, {"_id": "692fa6da26742347f61dac2b", "name": "Zihua Qu", "hidden": false}], "publishedAt": "2025-12-02T09:25:14.000Z", "submittedOnDailyAt": "2025-12-03T00:26:37.248Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "upvotes": 175, "discussionId": "692fa6da26742347f61dac2c", "ai_summary": "DeepSeek-V3.2 introduces DeepSeek Sparse Attention and a scalable reinforcement learning framework, achieving superior reasoning and performance compared to GPT-5 and Gemini-3.0-Pro in complex reasoning tasks.", "ai_keywords": ["DeepSeek Sparse Attention", "DSA", "reinforcement learning framework", "agentic task synthesis pipeline", "computational efficiency", "long-context scenarios", "gold-medal performance", "International Mathematical Olympiad", "International Olympiad in Informatics", "reasoning proficiency"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>\u63a8\u51fa\u4e86DeepSeek-V3.2\u6a21\u578b\uff0c\u517c\u5177\u9ad8\u6548\u80fd\u548c\u51fa\u8272\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff08DSA\uff09\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u7684\u6a21\u578b\u8868\u73b0\u3002</li>\n    <li>\u901a\u8fc7\u5b9e\u65bd\u5f3a\u5927\u7684\u5f3a\u5316\u5b66\u4e60\u534f\u8bae\uff0cDeepSeek-V3.2\u7684\u8868\u73b0\u4e0eGPT-5\u76f8\u5f53\uff0c\u4e14\u9ad8\u8ba1\u7b97\u7248\u672c\u8d85\u8d8aGPT-5\u3002</li>\n    <li>DeepSeek-V3.2\u5728\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\u548c\u56fd\u9645\u4fe1\u606f\u5b66\u5965\u6797\u5339\u514b\u4e2d\u53d6\u5f97\u4e86\u91d1\u724c\u8868\u73b0\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u5927\u89c4\u6a21\u667a\u80fd\u4efb\u52a1\u5408\u6210\u7ba1\u9053\uff0c\u7cfb\u7edf\u6027\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u4e86\u590d\u6742\u4e92\u52a8\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6307\u4ee4\u9075\u5faa\u7684\u7a33\u5065\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DeepSeek-V3.2 is a new model that balances efficiency with strong performance in reasoning and tasks.</li>\n    <li>It features DeepSeek Sparse Attention (DSA), which makes it faster and more efficient for processing long contexts without losing performance.</li>\n    <li>The model uses a scalable reinforcement learning framework that allows it to perform as well as GPT-5, with a special variant even outperforming it.</li>\n    <li>DeepSeek-V3.2 has shown top-tier reasoning skills, winning gold medals in prestigious competitions like the International Mathematical Olympiad and the International Olympiad in Informatics.</li>\n    <li>It includes a new system for creating training data that helps improve its ability to follow instructions and generalize in complex situations.</li>\n</ul>"}, "publishedAt": "2025-12-02T04:25:14.000Z", "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models", "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.02556.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.22699", "authors": [{"_id": "692d06234397b1ec214f6788", "name": "Z-Image Team", "hidden": false}, {"_id": "692d06234397b1ec214f6789", "user": {"_id": "692d0e6bb14ceb758205d0dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/692d0e6bb14ceb758205d0dd/gGVq2KSJE11Sr3LkVn-n5.jpeg", "isPro": false, "fullname": "Huanqia Cai", "user": "Orion-Cai", "type": "user"}, "name": "Huanqia Cai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:26.669Z", "hidden": false}, {"_id": "692d06234397b1ec214f678a", "user": {"_id": "67777b7a8376dfe003afa951", "avatarUrl": "/avatars/2af9d3181306d4c53329d047eeadaf1e.svg", "isPro": false, "fullname": "Sihan Cao", "user": "Sihan-Cao", "type": "user"}, "name": "Sihan Cao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T09:13:33.191Z", "hidden": false}, {"_id": "692d06234397b1ec214f678b", "user": {"_id": "64a54586c0f13de8e7093314", "avatarUrl": "/avatars/389e43e9a32cf2fc95f8f3a23b8f0508.svg", "isPro": false, "fullname": "Ruoyi Du", "user": "RuoyiDu", "type": "user"}, "name": "Ruoyi Du", "status": "claimed_verified", "statusLastChangedAt": "2025-12-03T09:18:53.948Z", "hidden": false}, {"_id": "692d06234397b1ec214f678c", "name": "Peng Gao", "hidden": false}, {"_id": "692d06234397b1ec214f678d", "name": "Steven Hoi", "hidden": false}, {"_id": "692d06234397b1ec214f678e", "name": "Shijie Huang", "hidden": false}, {"_id": "692d06234397b1ec214f678f", "name": "Zhaohui Hou", "hidden": false}, {"_id": "692d06234397b1ec214f6790", "user": {"_id": "662a0f2d4bab737c1a279843", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662a0f2d4bab737c1a279843/fC2p3mjMHkVpDQdEqkuR4.png", "isPro": false, "fullname": "Dengyang Jiang", "user": "DyJiang", "type": "user"}, "name": "Dengyang Jiang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:15.555Z", "hidden": false}, {"_id": "692d06234397b1ec214f6791", "user": {"_id": "6537e8eab01250d1d6efed3a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gMx73gwdfEhcCFioStGCE.jpeg", "isPro": false, "fullname": "Xin", "user": "Srameo", "type": "user"}, "name": "Xin Jin", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:15.288Z", "hidden": false}, {"_id": "692d06234397b1ec214f6792", "name": "Liangchen Li", "hidden": false}, {"_id": "692d06234397b1ec214f6793", "user": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "name": "Zhen Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:16.899Z", "hidden": false}, {"_id": "692d06234397b1ec214f6794", "user": {"_id": "6740a5730bb4a675446a80ad", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740a5730bb4a675446a80ad/dmruwMdQK3zluJm7YXUtN.jpeg", "isPro": false, "fullname": "Zhong-Yu Li", "user": "lzyhha", "type": "user"}, "name": "Zhong-Yu Li", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:08.972Z", "hidden": false}, {"_id": "692d06234397b1ec214f6795", "name": "David Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6796", "name": "Dongyang Liu", "hidden": false}, {"_id": "692d06234397b1ec214f6797", "user": {"_id": "66332475351231c428653b6b", "avatarUrl": "/avatars/3997bcde54158f7ff9770c85a20875f1.svg", "isPro": false, "fullname": "Junhan Shi", "user": "jshmsjh", "type": "user"}, "name": "Junhan Shi", "status": "admin_assigned", "statusLastChangedAt": "2025-12-01T10:07:38.865Z", "hidden": false}, {"_id": "692d06234397b1ec214f6798", "user": {"_id": "64379d79fac5ea753f1c10f3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png", "isPro": false, "fullname": "Jerry Wu", "user": "QJerry", "type": "user"}, "name": "Qilong Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:11:18.709Z", "hidden": false}, {"_id": "692d06234397b1ec214f6799", "name": "Feng Yu", "hidden": false}, {"_id": "692d06234397b1ec214f679a", "name": "Chi Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679b", "name": "Shifeng Zhang", "hidden": false}, {"_id": "692d06234397b1ec214f679c", "user": {"_id": "641988978e0baaeed5a066c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641988978e0baaeed5a066c6/TdCjJ63gw5gdX1RqTvy9a.png", "isPro": false, "fullname": "Shilin", "user": "zsLin", "type": "user"}, "name": "Shilin Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T16:24:44.624Z", "hidden": false}], "publishedAt": "2025-11-27T18:52:07.000Z", "submittedOnDailyAt": "2025-12-01T00:38:17.269Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "submittedOnDailyBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "isPro": false, "fullname": "Zhen Li", "user": "Paper99", "type": "user"}, "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "upvotes": 155, "discussionId": "692d06234397b1ec214f679d", "projectPage": "https://tongyi-mai.github.io/Z-Image-blog/", "githubRepo": "https://github.com/Tongyi-MAI/Z-Image", "ai_summary": "Z-Image, a 6B-parameter Scalable Single-Stream Diffusion Transformer (S3-DiT) model, achieves high-performance image generation with reduced computational cost, offering sub-second inference and compatibility with consumer hardware.", "ai_keywords": ["Scalable Single-Stream Diffusion Transformer", "S3-DiT", "diffusion transformer", "omni-pre-training", "instruction-following capabilities", "photorealistic image generation", "bilingual text rendering", "distillation scheme", "reward post-training", "H800 GPU", "VRAM"], "githubStars": 5595, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "summary_zh": "<ul>\n    <li>\u5f53\u524d\u9ad8\u6027\u80fd\u56fe\u50cf\u751f\u6210\u6a21\u578b\u591a\u7531\u4e13\u6709\u7cfb\u7edf\u4e3b\u5bfc\uff0c\u5982Nano Banana Pro\u548cSeedream 4.0\u3002</li>\n    <li>\u4e00\u4e9b\u9886\u5148\u7684\u5f00\u6e90\u66ff\u4ee3\u54c1\u53c2\u6570\u91cf\u5de8\u5927\uff0820B\u523080B\uff09\uff0c\u4e0d\u9002\u5408\u666e\u901a\u786c\u4ef6\u4f7f\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Z-Image\uff0c\u4e00\u4e2a\u9ad8\u6548\u76846B\u53c2\u6570\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5355\u6d41\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u3002</li>\n    <li>Z-Image\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f18\u5316\u4e86\u6574\u4e2a\u6a21\u578b\u751f\u547d\u5468\u671f\uff0c\u4f7f\u7528314K H800 GPU\u5c0f\u65f6\u5b8c\u6210\u8bad\u7ec3\u3002</li>\n    <li>\u6211\u4eec\u7684\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u53cc\u8bed\u6587\u672c\u5448\u73b0\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u4e14\u6211\u4eec\u516c\u5f00\u4e86\u4ee3\u7801\u3001\u6743\u91cd\u548c\u5728\u7ebf\u6f14\u793a\uff0c\u4ee5\u652f\u6301\u66f4\u6613\u83b7\u53d6\u7684\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Many high-performance image generation models are proprietary, like Nano Banana Pro and Seedream 4.0, while popular open-source models are too large for ordinary hardware.</li>\n    <li>Z-Image is a new 6B-parameter model designed to be efficient and easy to use, built on a new architecture called S3-DiT.</li>\n    <li>The training of Z-Image was completed quickly and cost-effectively, using 314K GPU hours, around $630K in total.</li>\n    <li>Z-Image can run very fast on enterprise hardware and is compatible with consumer-grade systems with less than 16GB VRAM.</li>\n    <li>The model performs very well in generating realistic images and handling bilingual text, and the developers have made the code and demo publicly available to encourage wider use.</li>\n</ul>"}, "publishedAt": "2025-11-27T13:52:07.000Z", "title": "Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer", "summary": "The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the \"scale-at-all-costs\" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.22699.png", "numComments": 3, "submittedBy": {"_id": "6285a9133ab6642179158944", "avatarUrl": "/avatars/6e10fa07c94141fcdbe0cab02bb731ca.svg", "fullname": "Zhen Li", "name": "Paper99", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 29}, "organization": {"_id": "6925b20fed452d1567c012d3", "name": "Tongyi-MAI", "fullname": "Tongyi-MAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64379d79fac5ea753f1c10f3/fxHO6QoYjdv9_LTyiUD3g.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20626", "authors": [{"_id": "6927ab26243b2216fb75cd1b", "name": "Wei He", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1c", "user": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "name": "Kai Han", "status": "claimed_verified", "statusLastChangedAt": "2025-11-27T09:59:11.052Z", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1d", "name": "Hang Zhou", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1e", "name": "Hanting Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd1f", "name": "Zhicheng Liu", "hidden": false}, {"_id": "6927ab26243b2216fb75cd20", "name": "Xinghao Chen", "hidden": false}, {"_id": "6927ab26243b2216fb75cd21", "name": "Yunhe Wang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "publishedAt": "2025-11-25T18:48:05.000Z", "submittedOnDailyAt": "2025-11-26T23:08:13.066Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "submittedOnDailyBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "isPro": false, "fullname": "hankai", "user": "hankaixyz", "type": "user"}, "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "upvotes": 154, "discussionId": "6927ab27243b2216fb75cd22", "projectPage": "https://github.com/huawei-noah/noah-research/tree/master/ROOT", "githubRepo": "https://github.com/huawei-noah/noah-research", "ai_summary": "ROOT, a robust optimizer, enhances training stability and convergence for large language models by addressing dimensional fragility and outlier noise through adaptive Newton iterations and proximal optimization.", "ai_keywords": ["large language models", "LLMs", "momentum orthogonalization", "dimensional fragility", "outlier-induced noise", "adaptive Newton iterations", "proximal optimization", "Muon", "Adam-based optimizers", "robust optimizer"], "githubStars": 909, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u5316\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u52a0\u52a0\u5267\u4e86\u7b97\u6cd5\u4e0d\u7cbe\u786e\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u5668ROOT\uff0c\u65e8\u5728\u901a\u8fc7\u53cc\u91cd\u7a33\u5065\u673a\u5236\u6765\u589e\u5f3a\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002</li>\n    <li>ROOT\u91c7\u7528\u81ea\u9002\u5e94\u725b\u987f\u8fed\u4ee3\u7684\u7ef4\u5ea6\u7a33\u5065\u6b63\u4ea4\u5316\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u4e0d\u540c\u67b6\u6784\u914d\u7f6e\u4e0b\u7684\u7cbe\u786e\u5ea6\u3002</li>\n    <li>ROOT\u8fd8\u901a\u8fc7\u90bb\u8fd1\u4f18\u5316\u6846\u67b6\u6291\u5236\u5f02\u5e38\u503c\u566a\u58f0\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u610f\u4e49\u7684\u68af\u5ea6\u65b9\u5411\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cROOT\u5728\u566a\u58f0\u548c\u975e\u51f8\u573a\u666f\u4e2d\u6bd4Muon\u548c\u57fa\u4e8eAdam\u7684\u4f18\u5316\u5668\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) face challenges with training stability and precision as they grow in size.</li>\n    <li>Recent optimizers have improved efficiency but struggle with robustness against noise and dimensional changes.</li>\n    <li>ROOT is a new optimizer that strengthens training stability using two innovative methods: dimension-robust orthogonalization and noise suppression.</li>\n    <li>Experiments show ROOT outperforms other optimizers like Muon and Adam in tough conditions, achieving faster training and better results.</li>\n    <li>The ROOT code will be available online for others to use and build upon.</li>\n</ul>"}, "publishedAt": "2025-11-25T13:48:05.000Z", "title": "ROOT: Robust Orthogonalized Optimizer for Neural Network Training", "summary": "The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65e52e7d27dc8aa470a640e3/r9QpXyUHL3SWym780xlxD.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20626.png", "numComments": 2, "submittedBy": {"_id": "65e52e7d27dc8aa470a640e3", "avatarUrl": "/avatars/022a179d14de29b9ab9d96fcc85aa264.svg", "fullname": "hankai", "name": "hankaixyz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "organization": {"_id": "5f83c275f0801648bf88454a", "name": "huawei-noah", "fullname": "HUAWEI Noah's Ark Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1602470452594-5f83c19ff0801648bf884549.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2511.20785", "authors": [{"_id": "692d430f4397b1ec214f696e", "user": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "name": "Zuhao Yang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:11.311Z", "hidden": false}, {"_id": "692d430f4397b1ec214f696f", "user": {"_id": "6690f58e2f9f6f9c88e91031", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6690f58e2f9f6f9c88e91031/QQ_VoEh7NlE6BUvii08zk.png", "isPro": false, "fullname": "Sudong Wang", "user": "xiao45791", "type": "user"}, "name": "Sudong Wang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:14.173Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6970", "user": {"_id": "64bb77e786e7fb5b8a317a43", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bb77e786e7fb5b8a317a43/J0jOrlZJ9gazdYaeSH2Bo.png", "isPro": false, "fullname": "kcz", "user": "kcz358", "type": "user"}, "name": "Kaichen Zhang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T10:06:41.343Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6971", "user": {"_id": "66bf00ca5b4e241fe266059d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66bf00ca5b4e241fe266059d/VoWPC_C4zoeT6dS699t7L.png", "isPro": false, "fullname": "Keming Wu", "user": "wukeming11", "type": "user"}, "name": "Keming Wu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-01T09:10:09.461Z", "hidden": false}, {"_id": "692d430f4397b1ec214f6972", "name": "Sicong Leng", "hidden": false}, {"_id": "692d430f4397b1ec214f6973", "name": "Yifan Zhang", "hidden": false}, {"_id": "692d430f4397b1ec214f6974", "name": "Chengwei Qin", "hidden": false}, {"_id": "692d430f4397b1ec214f6975", "name": "Shijian Lu", "hidden": false}, {"_id": "692d430f4397b1ec214f6976", "name": "Xingxuan Li", "hidden": false}, {"_id": "692d430f4397b1ec214f6977", "user": {"_id": "6454685a548f22be598414c4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eMjMWKJ-AouF7eY1-RzGF.jpeg", "isPro": false, "fullname": "Lidong Bing", "user": "LidongBing", "type": "user"}, "name": "Lidong Bing", "status": "claimed_verified", "statusLastChangedAt": "2025-12-02T08:49:36.056Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "publishedAt": "2025-11-25T19:22:48.000Z", "submittedOnDailyAt": "2025-12-02T00:35:56.511Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "submittedOnDailyBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "isPro": false, "fullname": "Zuhao Yang", "user": "mwxely", "type": "user"}, "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "upvotes": 148, "discussionId": "692d430f4397b1ec214f6978", "projectPage": "https://evolvinglmms-lab.github.io/LongVT/", "githubRepo": "https://github.com/EvolvingLMMs-Lab/LongVT", "ai_summary": "LongVT, an end-to-end framework, enhances long video reasoning by interleaving global and local analysis using multimodal tools, outperforming existing methods on challenging benchmarks.", "ai_keywords": ["multimodal models", "video reasoning", "textual Chain-of-Thought", "hallucinations", "long-form videos", "temporal grounding", "video cropping", "fine-grained question-answering", "VideoSIAH", "tool-integrated cold-start supervised fine-tuning", "agentic reinforcement learning", "agentic reinforcement fine-tuning"], "githubStars": 121, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u89c6\u9891\u63a8\u7406\u65b9\u9762\u5177\u6709\u5f88\u5927\u6f5c\u529b\uff0c\u4f46\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u5bb9\u6613\u51fa\u73b0\u9519\u8bef\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86LongVT\u6846\u67b6\uff0c\u6a21\u4eff\u4eba\u7c7b\u7406\u89e3\u957f\u89c6\u9891\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5148\u5168\u5c40\u6d4f\u89c8\u518d\u7ec6\u770b\u76f8\u5173\u7247\u6bb5\u6765\u8fdb\u884c\u63a8\u7406\u3002</li>\n    <li>LongVT\u5229\u7528LMMs\u7684\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\uff0c\u9009\u62e9\u7279\u5b9a\u89c6\u9891\u7247\u6bb5\u5e76\u91cd\u91c7\u6837\u66f4\u7ec6\u81f4\u7684\u89c6\u9891\u5e27\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u540d\u4e3aVideoSIAH\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b247.9K\u4e2a\u6837\u672c\u7528\u4e8e\u8bad\u7ec3\uff0c1.6K\u4e2a\u6837\u672c\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\uff0c15.4K\u4e2a\u6837\u672c\u7528\u4e8e\u5fae\u8c03\u3002</li>\n    <li>\u7ecf\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bad\u7ec3\u7b56\u7565\uff0cLongVT\u5728\u56db\u4e2a\u957f\u89c6\u9891\u7406\u89e3\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u5f3a\u57fa\u7ebf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large multimodal models (LMMs) can struggle with video reasoning, especially with long videos that have sparse information.</li>\n    <li>LongVT is a new framework that mimics how humans watch videos by first skimming and then focusing on specific clips.</li>\n    <li>The framework uses LMMs to zoom in on video clips and gather detailed information to improve reasoning.</li>\n    <li>To support this task, a new dataset called VideoSIAH will be released, which includes 247.9K training samples and 1,280 evaluation question-answer pairs.</li>\n    <li>LongVT has shown better performance than existing methods on four tough long-video understanding tasks, and all related resources are publicly available online.</li>\n</ul>"}, "publishedAt": "2025-11-25T14:22:48.000Z", "title": "LongVT: Incentivizing \"Thinking with Long Videos\" via Native Tool Calling", "summary": "Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables \"Thinking with Long Videos\" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6524d665ab1416594149e07e/SZBdiFzskclJytgjtsLNu.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20785.png", "numComments": 3, "submittedBy": {"_id": "6524d665ab1416594149e07e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6524d665ab1416594149e07e/KMsCaAtV0DLC4tqN8f2a7.png", "fullname": "Zuhao Yang", "name": "mwxely", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "organization": {"_id": "6583eb89bed3689928f5d845", "name": "lmms-lab", "fullname": "LMMs-Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d3f7d84b0933c48f3cdd9c/RpVWux8y4hHjNO6guD6sp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u8ba1\u7b97\u901f\u5ea6\u548c\u957f\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u96be\u4ee5\u7528\u4e8e\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u5f62\u8c61\u5408\u6210\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Live Avatar\uff0c\u4e00\u4e2a\u9ad8\u6548\u7684\u865a\u62df\u5f62\u8c61\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u7528\u4e00\u4e2a\u5177\u6709140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\uff08TPP\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u591aGPU\u5e76\u884c\u5904\u7406\u6765\u63d0\u9ad8\u751f\u6210\u901f\u5ea6\uff0c\u786e\u4fdd\u4f4e\u5ef6\u8fdf\u5b9e\u65f6\u6d41\u5a92\u4f53\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u52a8\u6001\u91cd\u6821\u51c6\u5916\u89c2\u7684\u6eda\u52a8\u63a5\u6536\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u4ee5\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u8eab\u4efd\u6f02\u79fb\u548c\u989c\u8272\u4f2a\u5f71\u3002</li>\n    <li>Live Avatar\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e86\u6bcf\u79d220\u5e27\u7684\u751f\u6210\uff0c\u6807\u5fd7\u7740\u5b9e\u65f6\u9ad8\u4fdd\u771f\u865a\u62df\u5f62\u8c61\u751f\u6210\u7684\u65b0\u6807\u51c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new system for creating realistic avatars in real-time, designed to work with audio input for streaming.</li>\n    <li>It uses a powerful 14-billion-parameter diffusion model and a new method called Timestep-forcing Pipeline Parallelism (TPP) to speed up processing across multiple GPUs.</li>\n    <li>The system addresses issues like consistency and quality through the Rolling Sink Frame Mechanism (RSFM), which helps maintain appearance and color accuracy.</li>\n    <li>It allows for quick adaptation of large models without losing visual quality, achieving 20 frames per second on 5 GPUs.</li>\n    <li>This represents a significant advancement in the field, making high-quality avatar generation practical for long-form video applications.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2511.18423", "authors": [{"_id": "692518ff16eb3a9f1310391c", "name": "B. Y. Yan", "hidden": false}, {"_id": "692518ff16eb3a9f1310391d", "name": "Chaofan Li", "hidden": false}, {"_id": "692518ff16eb3a9f1310391e", "name": "Hongjin Qian", "hidden": false}, {"_id": "692518ff16eb3a9f1310391f", "user": {"_id": "6145b3fd35135ec7e8d4ca45", "avatarUrl": "/avatars/5dc25d18d6a8418c9b1a29ece9a48f5a.svg", "isPro": false, "fullname": "Shuqi Lu", "user": "shuqi", "type": "user"}, "name": "Shuqi Lu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:18:11.163Z", "hidden": false}, {"_id": "692518ff16eb3a9f13103920", "user": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "name": "Zheng Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-11-25T12:17:59.618Z", "hidden": false}], "publishedAt": "2025-11-23T12:29:33.000Z", "submittedOnDailyAt": "2025-11-25T00:25:04.757Z", "title": "General Agentic Memory Via Deep Research", "submittedOnDailyBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "isPro": false, "fullname": "zhengliu", "user": "lz1001", "type": "user"}, "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "upvotes": 140, "discussionId": "692518ff16eb3a9f13103921", "projectPage": "https://github.com/VectorSpaceLab/general-agentic-memory", "githubRepo": "https://github.com/VectorSpaceLab/general-agentic-memory", "ai_summary": "GAM, a novel framework that employs JIT compilation principles, improves memory efficiency and task completion by leveraging a lightweight memorizer and researcher in conjunction with reinforcement learning.", "ai_keywords": ["general agentic memory", "GAM", "just-in time compilation", "JIT compilation", "memorizer", "researcher", "universal page-store", "large language models", "LLMs", "reinforcement learning"], "githubStars": 246, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u5185\u5b58\u5bf9\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u9759\u6001\u5185\u5b58\u5bb9\u6613\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u79f0\u4e3a\u4e00\u822c\u4ee3\u7406\u5185\u5b58\uff08GAM\uff09\uff0c\u91c7\u7528\u201c\u53ca\u65f6\u7f16\u8bd1\u201d\uff08JIT\uff09\u539f\u5219\u3002</li>\n    <li>GAM\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1a\u8bb0\u5fc6\u5668\u548c\u7814\u7a76\u8005\uff0c\u5206\u522b\u7528\u4e8e\u7ba1\u7406\u5386\u53f2\u4fe1\u606f\u548c\u5728\u7ebf\u68c0\u7d22\u6709\u7528\u4fe1\u606f\u3002</li>\n    <li>\u8fd9\u79cd\u8bbe\u8ba1\u80fd\u6709\u6548\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6574\u4f53\u6027\u80fd\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cGAM\u5728\u591a\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684\u4efb\u52a1\u5b8c\u6210\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5185\u5b58\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Memory is very important for AI agents, but traditional static memory can lead to significant information loss.</li>\n  <li>We introduce a new system called general agentic memory (GAM) that creates memory when needed, rather than in advance.</li>\n  <li>GAM consists of two main parts: a Memorizer that keeps important past information, and a Researcher that finds useful information when needed.</li>\n  <li>This system helps AI agents use large language models more effectively and improves their overall performance.</li>\n  <li>Tests show that GAM performs much better in memory-related tasks compared to existing memory systems.</li>\n</ul>"}, "publishedAt": "2025-11-23T07:29:33.000Z", "title": "General Agentic Memory Via Deep Research", "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called general agentic memory (GAM). GAM follows the principle of \"just-in time (JIT) compilation\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) Memorizer, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) Researcher, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18423.png", "numComments": 2, "submittedBy": {"_id": "64a38c590111d5ff6c3d5f2b", "avatarUrl": "/avatars/ef13dc7ce243819bc0da9b04e778b432.svg", "fullname": "zhengliu", "name": "lz1001", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp \u662f\u4e00\u4e2a\u5305\u542b 210 \u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6a21\u62df\u771f\u5b9e\u4f01\u4e1a\u7684\u6570\u636e\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u5305\u62ec\u8bbe\u8ba1\u548c\u6784\u5efa\u591a\u9636\u6bb5 SQL \u7ba1\u9053\uff0c\u8981\u6c42\u5bf9\u5de5\u4e1a\u6a21\u5f0f\u8fdb\u884c\u9ad8\u7ea7\u522b\u7684\u5de5\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\u4efb\u52a1\u6d89\u53ca\u5f00\u653e\u6027\u5546\u4e1a\u95ee\u9898\uff0c\u9700\u8981\u6218\u7565\u89c4\u5212\u548c\u63a2\u7d22\u6027\u5206\u6790\u3002</li>\n    <li>\u5b9e\u9a8c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u5728 DAComp \u4e0a\u7684\u8868\u73b0\u4e5f\u4e0d\u4f73\uff0c\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u7684\u6210\u529f\u7387\u4f4e\u4e8e 20%\u3002</li>\n    <li>DAComp \u901a\u8fc7\u8bca\u65ad\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u53d1\u771f\u6b63\u6709\u80fd\u529b\u7684\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DAComp is a new benchmark with 210 tasks that reflects complex data workflows in businesses.</li>\n    <li>It includes data engineering tasks that require building and improving SQL pipelines, and data analysis tasks that involve solving open-ended business problems.</li>\n    <li>Performance on data engineering tasks is very low, with success rates under 20%, indicating issues in managing complete data processes.</li>\n    <li>Data analysis tasks also show poor results, averaging below 40%, which points to weaknesses in reasoning and problem-solving.</li>\n    <li>DAComp helps identify these challenges, aiming to improve the development of autonomous data agents for businesses.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni\u662f\u4e00\u4e2a\u901a\u7528\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u4ece\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u800c\u4e0d\u662f\u5206\u5f00\u7684\u6d41\u7a0b\u3002</li>\n    <li>Kling-Omni\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5305\u62ec\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u5185\u5bb9\uff0c\u5e76\u5c06\u5176\u5904\u7406\u4e3a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\u3002</li>\n    <li>\u8be5\u6846\u67b6\u57fa\u4e8e\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u5229\u7528\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u6765\u63d0\u5347\u6027\u80fd\u3002</li>\n    <li>Kling-Omni\u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u6267\u884c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u662f\u8fc8\u5411\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u91cd\u8981\u8fdb\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from various types of inputs like text, images, and videos.</li>\n    <li>It combines different video tasks (like generation and editing) into one system, rather than using separate processes.</li>\n    <li>The framework turns mixed inputs into a single format to produce impressive and smart video content.</li>\n    <li>Kling-Omni is built on a strong data system and uses advanced training methods for better performance.</li>\n    <li>It shows great results in generating content, editing videos based on reasoning, and following instructions from different media sources.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.21631", "authors": [{"_id": "692ffb1a26742347f61daf38", "user": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "isPro": false, "fullname": "shuai bai", "user": "ShuaiBai623", "type": "user"}, "name": "Shuai Bai", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:34:29.118Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf39", "name": "Yuxuan Cai", "hidden": false}, {"_id": "692ffb1a26742347f61daf3a", "name": "Ruizhe Chen", "hidden": false}, {"_id": "692ffb1a26742347f61daf3b", "name": "Keqin Chen", "hidden": false}, {"_id": "692ffb1a26742347f61daf3c", "user": {"_id": "63f30b870a16587ea970edfe", "avatarUrl": "/avatars/b58ab2d8a85a6d83462c297de2714ce4.svg", "isPro": false, "fullname": "Xiong-Hui Chen", "user": "xionghuichen", "type": "user"}, "name": "Xionghui Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:35:42.689Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf3d", "user": {"_id": "65b2529285b6c21448a10d65", "avatarUrl": "/avatars/1b09e2742aecce1bbdc57f0c4504cf38.svg", "isPro": false, "fullname": "Zesen Cheng", "user": "ClownRat", "type": "user"}, "name": "Zesen Cheng", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:35:51.365Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf3e", "name": "Lianghao Deng", "hidden": false}, {"_id": "692ffb1a26742347f61daf3f", "name": "Wei Ding", "hidden": false}, {"_id": "692ffb1a26742347f61daf40", "name": "Chang Gao", "hidden": false}, {"_id": "692ffb1a26742347f61daf41", "name": "Chunjiang Ge", "hidden": false}, {"_id": "692ffb1a26742347f61daf42", "name": "Wenbin Ge", "hidden": false}, {"_id": "692ffb1a26742347f61daf43", "name": "Zhifang Guo", "hidden": false}, {"_id": "692ffb1a26742347f61daf44", "user": {"_id": "656f1b21b075b63c90ba02ee", "avatarUrl": "/avatars/d6856815ef06261394178161e4d511b4.svg", "isPro": false, "fullname": "Huang Qidong", "user": "shikiw", "type": "user"}, "name": "Qidong Huang", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:49.065Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf45", "name": "Jie Huang", "hidden": false}, {"_id": "692ffb1a26742347f61daf46", "name": "Fei Huang", "hidden": false}, {"_id": "692ffb1a26742347f61daf47", "name": "Binyuan Hui", "hidden": false}, {"_id": "692ffb1a26742347f61daf48", "name": "Shutong Jiang", "hidden": false}, {"_id": "692ffb1a26742347f61daf49", "name": "Zhaohai Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4a", "name": "Mingsheng Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4b", "name": "Mei Li", "hidden": false}, {"_id": "692ffb1a26742347f61daf4c", "user": {"_id": "6346be8f7fb9f11870c63998", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6346be8f7fb9f11870c63998/tFWawSkXL6bv1zgvzFWQd.png", "isPro": false, "fullname": "Kaixin Li", "user": "likaixin", "type": "user"}, "name": "Kaixin Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:53.648Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf4d", "user": {"_id": "67a31313cf9d856beb7f9afb", "avatarUrl": "/avatars/69395b134716f750545eab35a164e51f.svg", "isPro": false, "fullname": "Zicheng Lin", "user": "etonlin", "type": "user"}, "name": "Zicheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:36:50.803Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf4e", "name": "Junyang Lin", "hidden": false}, {"_id": "692ffb1a26742347f61daf4f", "name": "Xuejing Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf50", "name": "Jiawei Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf51", "name": "Chenglong Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf52", "name": "Yang Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf53", "name": "Dayiheng Liu", "hidden": false}, {"_id": "692ffb1a26742347f61daf54", "user": {"_id": "64e72776e9fc9d0475ef5188", "avatarUrl": "/avatars/d32b9d4e1da5486c3d5f9b04fa29d167.svg", "isPro": false, "fullname": "Shixuan Liu", "user": "liusx", "type": "user"}, "name": "Shixuan Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:58.470Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf55", "name": "Dunjie Lu", "hidden": false}, {"_id": "692ffb1a26742347f61daf56", "name": "Ruilin Luo", "hidden": false}, {"_id": "692ffb1a26742347f61daf57", "name": "Chenxu Lv", "hidden": false}, {"_id": "692ffb1a26742347f61daf58", "name": "Rui Men", "hidden": false}, {"_id": "692ffb1a26742347f61daf59", "name": "Lingchen Meng", "hidden": false}, {"_id": "692ffb1a26742347f61daf5a", "name": "Xuancheng Ren", "hidden": false}, {"_id": "692ffb1a26742347f61daf5b", "name": "Xingzhang Ren", "hidden": false}, {"_id": "692ffb1a26742347f61daf5c", "name": "Sibo Song", "hidden": false}, {"_id": "692ffb1a26742347f61daf5d", "name": "Yuchong Sun", "hidden": false}, {"_id": "692ffb1a26742347f61daf5e", "name": "Jun Tang", "hidden": false}, {"_id": "692ffb1a26742347f61daf5f", "name": "Jianhong Tu", "hidden": false}, {"_id": "692ffb1a26742347f61daf60", "name": "Jianqiang Wan", "hidden": false}, {"_id": "692ffb1a26742347f61daf61", "name": "Peng Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf62", "name": "Pengfei Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf63", "name": "Qiuyue Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf64", "name": "Yuxuan Wang", "hidden": false}, {"_id": "692ffb1a26742347f61daf65", "name": "Tianbao Xie", "hidden": false}, {"_id": "692ffb1a26742347f61daf66", "name": "Yiheng Xu", "hidden": false}, {"_id": "692ffb1a26742347f61daf67", "user": {"_id": "645b10e80c73ea27d13f7aca", "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg", "isPro": false, "fullname": "xuhaiyang", "user": "xhyandwyy", "type": "user"}, "name": "Haiyang Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-04T08:48:51.583Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf68", "name": "Jin Xu", "hidden": false}, {"_id": "692ffb1a26742347f61daf69", "name": "Zhibo Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6a", "name": "Mingkun Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6b", "name": "Jianxin Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6c", "name": "An Yang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6d", "name": "Bowen Yu", "hidden": false}, {"_id": "692ffb1a26742347f61daf6e", "name": "Fei Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf6f", "name": "Hang Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf70", "name": "Xi Zhang", "hidden": false}, {"_id": "692ffb1a26742347f61daf71", "name": "Bo Zheng", "hidden": false}, {"_id": "692ffb1a26742347f61daf72", "name": "Humen Zhong", "hidden": false}, {"_id": "692ffb1a26742347f61daf73", "name": "Jingren Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf74", "name": "Fan Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf75", "name": "Jing Zhou", "hidden": false}, {"_id": "692ffb1a26742347f61daf76", "user": {"_id": "627d2723401f42c57b6b7c0c", "avatarUrl": "/avatars/6ff754e56aaee63d8572881a6a966171.svg", "isPro": false, "fullname": "Yuanzhi Zhu", "user": "Yuanzhi", "type": "user"}, "name": "Yuanzhi Zhu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-04T10:36:59.879Z", "hidden": false}, {"_id": "692ffb1a26742347f61daf77", "name": "Ke Zhu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63451cf0a05b51f7ded25505/kVvzrCGrjwoK0CYh4DKif.jpeg"], "publishedAt": "2025-11-26T17:59:08.000Z", "submittedOnDailyAt": "2025-12-04T01:02:46.772Z", "title": "Qwen3-VL Technical Report", "submittedOnDailyBy": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "isPro": false, "fullname": "shuai bai", "user": "ShuaiBai623", "type": "user"}, "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.", "upvotes": 110, "discussionId": "692ffb1b26742347f61daf78", "ai_summary": "Qwen3-VL, a vision-language model, excels in text and multimodal understanding through advanced architectures and larger contexts, achieving superior performance across benchmarks.", "ai_keywords": ["vision-language model", "interleaved contexts", "multimodal benchmarks", "dense variants", "mixture-of-experts", "pure-text understanding", "long-context comprehension", "multimodal reasoning", "MMMU", "visual-math benchmarks", "interleaved-MRoPE", "DeepStack", "text-based time alignment", "T-RoPE", "explicit textual timestamp alignment", "vision-language alignment", "image-grounded reasoning", "agentic decision-making", "multimodal code intelligence"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>Qwen3-VL\u662fQwen\u7cfb\u5217\u4e2d\u6700\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u3002</li>\n    <li>\u8be5\u6a21\u578b\u652f\u6301\u6700\u591a256K\u4e2a\u6807\u8bb0\u7684\u4ea4\u9519\u4e0a\u4e0b\u6587\uff0c\u53ef\u4ee5\u65e0\u7f1d\u6574\u5408\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u3002</li>\n    <li>Qwen3-VL\u63d0\u4f9b\u4e09\u5927\u6838\u5fc3\u529f\u80fd\uff1a\u66f4\u5f3a\u7684\u7eaf\u6587\u672c\u7406\u89e3\u3001\u5f3a\u5927\u7684\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u4ee5\u53ca\u5148\u8fdb\u7684\u591a\u6a21\u6001\u63a8\u7406\u3002</li>\n    <li>\u6a21\u578b\u67b6\u6784\u8fdb\u884c\u4e86\u4e09\u9879\u5173\u952e\u5347\u7ea7\uff0c\u5305\u62ec\u66f4\u5f3a\u7684\u65f6\u7a7a\u5efa\u6a21\u3001\u6df1\u5ea6\u5806\u53e0\u6574\u5408\u548c\u57fa\u4e8e\u6587\u672c\u7684\u89c6\u9891\u65f6\u95f4\u5bf9\u9f50\u3002</li>\n    <li>Qwen3-VL\u65e8\u5728\u4e3a\u56fe\u50cf\u57fa\u7840\u63a8\u7406\u3001\u667a\u80fd\u51b3\u7b56\u548c\u591a\u6a21\u6001\u4ee3\u7801\u667a\u80fd\u63d0\u4f9b\u57fa\u7840\u652f\u6301\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Qwen3-VL is a powerful vision-language model that performs well on various multimodal tasks.</li>\n    <li>It can handle up to 256,000 tokens, allowing it to work with text, images, and videos together.</li>\n    <li>The model comes in different sizes to balance speed and quality, including dense and mixture-of-experts options.</li>\n    <li>It excels in understanding text, managing long contexts, and reasoning about images and videos.</li>\n    <li>Qwen3-VL includes improvements like better spatial-temporal modeling and precise video time alignment to enhance performance.</li>\n</ul>"}, "publishedAt": "2025-11-26T12:59:08.000Z", "title": "Qwen3-VL Technical Report", "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63451cf0a05b51f7ded25505/kVvzrCGrjwoK0CYh4DKif.jpeg"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.21631.png", "numComments": 3, "submittedBy": {"_id": "63451cf0a05b51f7ded25505", "avatarUrl": "/avatars/dec4bbee4a82b773fc58dfc2dce9dbeb.svg", "fullname": "shuai bai", "name": "ShuaiBai623", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 42}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Dec 23, 2025";
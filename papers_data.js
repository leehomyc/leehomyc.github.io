window.trendingPapers = {
    "today": [{"paper": {"id": "2512.24880", "authors": [{"_id": "69561fbf832867f253525726", "name": "Zhenda Xie", "hidden": false}, {"_id": "69561fbf832867f253525727", "name": "Yixuan Wei", "hidden": false}, {"_id": "69561fbf832867f253525728", "name": "Huanqi Cao", "hidden": false}, {"_id": "69561fbf832867f253525729", "name": "Chenggang Zhao", "hidden": false}, {"_id": "69561fbf832867f25352572a", "name": "Chengqi Deng", "hidden": false}, {"_id": "69561fbf832867f25352572b", "name": "Jiashi Li", "hidden": false}, {"_id": "69561fbf832867f25352572c", "name": "Damai Dai", "hidden": false}, {"_id": "69561fbf832867f25352572d", "name": "Huazuo Gao", "hidden": false}, {"_id": "69561fbf832867f25352572e", "name": "Jiang Chang", "hidden": false}, {"_id": "69561fbf832867f25352572f", "name": "Liang Zhao", "hidden": false}, {"_id": "69561fbf832867f253525730", "name": "Shangyan Zhou", "hidden": false}, {"_id": "69561fbf832867f253525731", "name": "Zhean Xu", "hidden": false}, {"_id": "69561fbf832867f253525732", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "69561fbf832867f253525733", "name": "Wangding Zeng", "hidden": false}, {"_id": "69561fbf832867f253525734", "name": "Shengding Hu", "hidden": false}, {"_id": "69561fbf832867f253525735", "name": "Yuqing Wang", "hidden": false}, {"_id": "69561fbf832867f253525736", "name": "Jingyang Yuan", "hidden": false}, {"_id": "69561fbf832867f253525737", "name": "Lean Wang", "hidden": false}, {"_id": "69561fbf832867f253525738", "name": "Wenfeng Liang", "hidden": false}], "publishedAt": "2025-12-31T14:16:26.000Z", "submittedOnDailyAt": "2026-01-01T07:30:32.169Z", "title": "mHC: Manifold-Constrained Hyper-Connections", "submittedOnDailyBy": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "isPro": false, "fullname": "Adina Yakefu", "user": "AdinaY", "type": "user"}, "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.", "upvotes": 59, "discussionId": "69561fc0832867f253525739", "ai_summary": "Manifold-Constrained Hyper-Connections (mHC) stabilize and scale residual connection architectures by restoring identity mapping properties through manifold projection and infrastructure optimization.", "ai_keywords": ["Hyper-Connections (HC)", "Manifold-Constrained Hyper-Connections (mHC)", "residual connections", "residual stream width", "connectivity patterns", "identity mapping property", "training instability", "memory access overhead", "manifold projection", "infrastructure optimization", "scalability"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u7814\u7a76\u63d0\u51fa\u4e86\u201c\u8d85\u8fde\u63a5\u201d\uff08Hyper-Connections, HC\uff09\uff0c\u6269\u5c55\u4e86\u6b8b\u5dee\u8fde\u63a5\u7684\u6982\u5ff5\u3002</li>\n    <li>\u867d\u7136HC\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f46\u4e5f\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u53ef\u6269\u5c55\u6027\u5dee\u3002</li>\n    <li>\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u201c\u6d41\u5f62\u7ea6\u675f\u8d85\u8fde\u63a5\u201d\uff08mHC\uff09\u7684\u6846\u67b6\uff0c\u6062\u590d\u4e86\u6b8b\u5dee\u8fde\u63a5\u7684\u7279\u6027\u3002</li>\n    <li>mHC\u5728\u8bad\u7ec3\u6548\u7387\u4e0a\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u8272\u3002</li>\n    <li>mHC\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u62d3\u6251\u67b6\u6784\u8bbe\u8ba1\uff0c\u5e76\u4e3a\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent studies have explored Hyper-Connections (HC) that improve residual connections in neural networks.</li>\n    <li>These improvements can lead to better performance but also cause training problems and increased memory use.</li>\n    <li>The proposed solution, Manifold-Constrained Hyper-Connections (mHC), aims to fix these issues by maintaining the identity mapping property.</li>\n    <li>mHC includes optimizations for better efficiency and scalability during training.</li>\n    <li>Experiments show that mHC provides significant performance benefits and could influence future designs of neural network architectures.</li>\n</ul>"}, "publishedAt": "2025-12-31T09:16:26.000Z", "title": "mHC: Manifold-Constrained Hyper-Connections", "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24880.png", "numComments": 1, "submittedBy": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "fullname": "Adina Yakefu", "name": "AdinaY", "type": "user", "isPro": false, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 1140}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24618", "authors": [{"_id": "6955d543832867f25352555d", "name": "Junru Lu", "hidden": false}, {"_id": "6955d543832867f25352555e", "name": "Jiarui Qin", "hidden": false}, {"_id": "6955d543832867f25352555f", "name": "Lingfeng Qiao", "hidden": false}, {"_id": "6955d543832867f253525560", "name": "Yinghui Li", "hidden": false}, {"_id": "6955d543832867f253525561", "name": "Xinyi Dai", "hidden": false}, {"_id": "6955d543832867f253525562", "name": "Bo Ke", "hidden": false}, {"_id": "6955d543832867f253525563", "name": "Jianfeng He", "hidden": false}, {"_id": "6955d543832867f253525564", "name": "Ruizhi Qiao", "hidden": false}, {"_id": "6955d543832867f253525565", "name": "Di Yin", "hidden": false}, {"_id": "6955d543832867f253525566", "name": "Xing Sun", "hidden": false}, {"_id": "6955d543832867f253525567", "name": "Yunsheng Wu", "hidden": false}, {"_id": "6955d543832867f253525568", "name": "Yinsong Liu", "hidden": false}, {"_id": "6955d543832867f253525569", "name": "Shuangyin Liu", "hidden": false}, {"_id": "6955d543832867f25352556a", "name": "Mingkong Tang", "hidden": false}, {"_id": "6955d543832867f25352556b", "name": "Haodong Lin", "hidden": false}, {"_id": "6955d543832867f25352556c", "name": "Jiayi Kuang", "hidden": false}, {"_id": "6955d543832867f25352556d", "name": "Fanxu Meng", "hidden": false}, {"_id": "6955d543832867f25352556e", "name": "Xiaojuan Tang", "hidden": false}, {"_id": "6955d543832867f25352556f", "name": "Yunjia Xi", "hidden": false}, {"_id": "6955d543832867f253525570", "name": "Junjie Huang", "hidden": false}, {"_id": "6955d543832867f253525571", "name": "Haotong Yang", "hidden": false}, {"_id": "6955d543832867f253525572", "name": "Zhenyi Shen", "hidden": false}, {"_id": "6955d543832867f253525573", "name": "Yangning Li", "hidden": false}, {"_id": "6955d543832867f253525574", "name": "Qianwen Zhang", "hidden": false}, {"_id": "6955d543832867f253525575", "name": "Yifei Yu", "hidden": false}, {"_id": "6955d543832867f253525576", "name": "Siyu An", "hidden": false}, {"_id": "6955d543832867f253525577", "name": "Junnan Dong", "hidden": false}, {"_id": "6955d543832867f253525578", "name": "Qiufeng Wang", "hidden": false}, {"_id": "6955d543832867f253525579", "name": "Jie Wang", "hidden": false}, {"_id": "6955d543832867f25352557a", "name": "Keyu Chen", "hidden": false}, {"_id": "6955d543832867f25352557b", "name": "Wei Wen", "hidden": false}, {"_id": "6955d543832867f25352557c", "name": "Taian Guo", "hidden": false}, {"_id": "6955d543832867f25352557d", "name": "Zhifeng Shen", "hidden": false}, {"_id": "6955d543832867f25352557e", "name": "Daohai Yu", "hidden": false}, {"_id": "6955d543832867f25352557f", "name": "Jiahao Li", "hidden": false}, {"_id": "6955d543832867f253525580", "name": "Ke Li", "hidden": false}, {"_id": "6955d543832867f253525581", "name": "Zongyi Li", "hidden": false}, {"_id": "6955d543832867f253525582", "name": "Xiaoyu Tan", "hidden": false}], "publishedAt": "2025-12-31T04:25:11.000Z", "submittedOnDailyAt": "2026-01-01T00:33:09.720Z", "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "upvotes": 43, "discussionId": "6955d543832867f253525583", "ai_summary": "Youtu-LLM is a lightweight language model optimized for computational efficiency and agentic intelligence through a compact architecture, STEM-focused training curriculum, and scalable mid-training strategies for planning and reasoning tasks.", "ai_keywords": ["Multi-Latent Attention (MLA) architecture", "STEM-oriented vocabulary", "128k context window", "Commonsense-STEM-Agent Curriculum", "multi-stage training strategy", "agentic mid-training", "data construction schemes", "planning and reflection behaviors", "long-context reasoning", "state tracking", "agentic capabilities"], "summary_zh": "<ul>\n    <li>Youtu-LLM\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4f46\u5f3a\u5927\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u6709\u9ad8\u6548\u7387\u548c\u667a\u80fd\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4ece\u96f6\u5f00\u59cb\u9884\u8bad\u7ec3\uff0c\u4e13\u6ce8\u4e8e\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u7684\u57f9\u517b\u3002</li>\n    <li>Youtu-LLM\u652f\u6301128k\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u9002\u5408\u957f\u65f6\u95f4\u7684\u63a8\u7406\u548c\u72b6\u6001\u8ddf\u8e2a\u3002</li>\n    <li>\u91c7\u7528\u4e86\u9010\u6b65\u8bad\u7ec3\u7b56\u7565\uff0c\u4ece\u4e00\u822c\u5e38\u8bc6\u5230\u590d\u6742\u7684STEM\u548c\u667a\u80fd\u4efb\u52a1\uff0c\u786e\u4fdd\u6a21\u578b\u5177\u5907\u6df1\u539a\u7684\u8ba4\u77e5\u80fd\u529b\u3002</li>\n    <li>\u5728\u8bc4\u4f30\u4e2d\uff0cYoutu-LLM\u5728\u5c0f\u4e8e2B\u53c2\u6570\u7684\u6a21\u578b\u4e2d\u8bbe\u7f6e\u4e86\u65b0\u7684\u6027\u80fd\u6807\u51c6\uff0c\u5c24\u5176\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Youtu-LLM is a new lightweight language model designed for efficient computation and smart decision-making.</li>\n    <li>It features a unique architecture that can handle long contexts up to 128,000 tokens, making it great for complex reasoning tasks.</li>\n    <li>The model is trained on a large dataset of 11 trillion tokens and follows a structured learning approach to develop strong cognitive skills.</li>\n    <li>It uses diverse training methods to teach the model planning and reflection skills across various subjects like math and coding.</li>\n    <li>Youtu-LLM outperforms other small models and competes well with larger ones in general tasks, especially excelling in specific agent-related tasks.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:25:11.000Z", "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24618.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 197}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24873", "authors": [{"_id": "6955e3f8832867f2535255cd", "name": "Weixun Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255ce", "name": "XiaoXiao Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255cf", "name": "Wanhe An", "hidden": false}, {"_id": "6955e3f8832867f2535255d0", "name": "Fangwen Dai", "hidden": false}, {"_id": "6955e3f8832867f2535255d1", "name": "Wei Gao", "hidden": false}, {"_id": "6955e3f8832867f2535255d2", "name": "Yancheng He", "hidden": false}, {"_id": "6955e3f8832867f2535255d3", "name": "Ju Huang", "hidden": false}, {"_id": "6955e3f8832867f2535255d4", "name": "Qiang Ji", "hidden": false}, {"_id": "6955e3f8832867f2535255d5", "name": "Hanqi Jin", "hidden": false}, {"_id": "6955e3f8832867f2535255d6", "name": "Xiaoyang Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d7", "name": "Yang Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d8", "name": "Zhongwen Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d9", "name": "Shirong Lin", "hidden": false}, {"_id": "6955e3f8832867f2535255da", "name": "Jiashun Liu", "hidden": false}, {"_id": "6955e3f8832867f2535255db", "name": "Zenan Liu", "hidden": false}, {"_id": "6955e3f8832867f2535255dc", "name": "Tao Luo", "hidden": false}, {"_id": "6955e3f8832867f2535255dd", "name": "Dilxat Muhtar", "hidden": false}, {"_id": "6955e3f8832867f2535255de", "name": "Yuanbin Qu", "hidden": false}, {"_id": "6955e3f8832867f2535255df", "name": "Jiaqiang Shi", "hidden": false}, {"_id": "6955e3f8832867f2535255e0", "name": "Qinghui Sun", "hidden": false}, {"_id": "6955e3f8832867f2535255e1", "name": "Yingshui Tan", "hidden": false}, {"_id": "6955e3f8832867f2535255e2", "name": "Hao Tang", "hidden": false}, {"_id": "6955e3f8832867f2535255e3", "name": "Runze Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e4", "name": "Yi Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e5", "name": "Zhaoguo Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e6", "name": "Yanan Wu", "hidden": false}, {"_id": "6955e3f8832867f2535255e7", "name": "Shaopan Xiong", "hidden": false}, {"_id": "6955e3f8832867f2535255e8", "name": "Binchen Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255e9", "name": "Xander Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255ea", "name": "Yuchi Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255eb", "name": "Qipeng Zhang", "hidden": false}, {"_id": "6955e3f8832867f2535255ec", "name": "Xixia Zhang", "hidden": false}, {"_id": "6955e3f8832867f2535255ed", "name": "Haizhou Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255ee", "name": "Jie Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255ef", "name": "Shuaibing Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255f0", "name": "Baihui Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f1", "name": "Jianhui Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f2", "name": "Suhang Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f3", "name": "Yanni Zhu", "hidden": false}, {"_id": "6955e3f8832867f2535255f4", "name": "Mengze Cai", "hidden": false}, {"_id": "6955e3f8832867f2535255f5", "name": "Kerui Cao", "hidden": false}, {"_id": "6955e3f8832867f2535255f6", "name": "Xitong Chen", "hidden": false}, {"_id": "6955e3f8832867f2535255f7", "name": "Yue Dai", "hidden": false}, {"_id": "6955e3f8832867f2535255f8", "name": "Lifan Du", "hidden": false}, {"_id": "6955e3f8832867f2535255f9", "name": "Tao Feng", "hidden": false}, {"_id": "6955e3f8832867f2535255fa", "name": "Tao He", "hidden": false}, {"_id": "6955e3f8832867f2535255fb", "name": "Jin Hu", "hidden": false}, {"_id": "6955e3f8832867f2535255fc", "name": "Yijie Hu", "hidden": false}, {"_id": "6955e3f8832867f2535255fd", "name": "Ziyu Jiang", "hidden": false}, {"_id": "6955e3f8832867f2535255fe", "name": "Cheng Li", "hidden": false}, {"_id": "6955e3f8832867f2535255ff", "name": "Xiang Li", "hidden": false}, {"_id": "6955e3f8832867f253525600", "name": "Jing Liang", "hidden": false}, {"_id": "6955e3f8832867f253525601", "name": "Chonghuan Liu", "hidden": false}, {"_id": "6955e3f8832867f253525602", "name": "ZhenDong Liu", "hidden": false}, {"_id": "6955e3f8832867f253525603", "name": "Haodong Mi", "hidden": false}, {"_id": "6955e3f8832867f253525604", "name": "Yanhu Mo", "hidden": false}, {"_id": "6955e3f8832867f253525605", "name": "Junjia Ni", "hidden": false}, {"_id": "6955e3f8832867f253525606", "name": "Shixin Pei", "hidden": false}, {"_id": "6955e3f8832867f253525607", "name": "Jingyu Shen", "hidden": false}, {"_id": "6955e3f8832867f253525608", "name": "XiaoShuai Song", "hidden": false}, {"_id": "6955e3f8832867f253525609", "name": "Cecilia Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560a", "name": "Chaofan Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560b", "name": "Kangyu Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560c", "name": "Pei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560d", "name": "Tao Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560e", "name": "Wei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560f", "name": "Ke Xiao", "hidden": false}, {"_id": "6955e3f8832867f253525610", "name": "Mingyu Xu", "hidden": false}, {"_id": "6955e3f8832867f253525611", "name": "Tiange Xu", "hidden": false}, {"_id": "6955e3f8832867f253525612", "name": "Nan Ya", "hidden": false}, {"_id": "6955e3f8832867f253525613", "name": "Siran Yang", "hidden": false}, {"_id": "6955e3f8832867f253525614", "name": "Jianan Ye", "hidden": false}, {"_id": "6955e3f8832867f253525615", "name": "Yaxing Zang", "hidden": false}, {"_id": "6955e3f8832867f253525616", "name": "Duo Zhang", "hidden": false}, {"_id": "6955e3f8832867f253525617", "name": "Junbo Zhang", "hidden": false}, {"_id": "6955e3f8832867f253525618", "name": "Boren Zheng", "hidden": false}, {"_id": "6955e3f8832867f253525619", "name": "Wanxi Deng", "hidden": false}, {"_id": "6955e3f8832867f25352561a", "name": "Ling Pan", "hidden": false}, {"_id": "6955e3f8832867f25352561b", "name": "Lin Qu", "hidden": false}, {"_id": "6955e3f8832867f25352561c", "name": "Wenbo Su", "hidden": false}, {"_id": "6955e3f8832867f25352561d", "name": "Jiamang Wang", "hidden": false}, {"_id": "6955e3f8832867f25352561e", "name": "Wei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352561f", "name": "Hu Wei", "hidden": false}, {"_id": "6955e3f8832867f253525620", "name": "Minggang Wu", "hidden": false}, {"_id": "6955e3f8832867f253525621", "name": "Cheng Yu", "hidden": false}, {"_id": "6955e3f8832867f253525622", "name": "Bing Zhao", "hidden": false}, {"_id": "6955e3f8832867f253525623", "name": "Zhicheng Zheng", "hidden": false}, {"_id": "6955e3f8832867f253525624", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-12-31T14:03:39.000Z", "submittedOnDailyAt": "2026-01-01T00:33:23.374Z", "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.", "upvotes": 33, "discussionId": "6955e3f8832867f253525625", "ai_summary": "The Agentic Learning Ecosystem (ALE) introduces a principled infrastructure for agent development, combining post-training optimization, sandbox environments, and policy alignment to enhance long-horizon training stability and performance in real-world tasks.", "ai_keywords": ["ROLL (post-training framework)", "ROCK (sandbox environment manager)", "iFlow CLI (agent framework)", "ROME (agentic model)", "data composition protocols", "Interaction-based Policy Alignment (IPA)", "semantic interaction chunks", "Terminal Bench Pro", "SWE-bench Verified"], "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u201cAgentic Learning Ecosystem (ALE)\u201d\uff0c\u65e8\u5728\u4f18\u5316\u4ee3\u7406\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5f00\u53d1\u6d41\u7a0b\u3002</li>\n    <li>ALE\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1aROLL\uff08\u6743\u91cd\u4f18\u5316\u6846\u67b6\uff09\u3001ROCK\uff08\u8f68\u8ff9\u751f\u6210\u7ba1\u7406\u5de5\u5177\uff09\u548ciFlow CLI\uff08\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u6846\u67b6\uff09\u3002</li>\n    <li>\u63a8\u51fa\u4e86\u5f00\u6e90\u4ee3\u7406\u6a21\u578bROME\uff0c\u57fa\u4e8eALE\u8bad\u7ec3\uff0c\u4f7f\u7528\u4e86\u8d85\u8fc7\u4e00\u767e\u4e07\u4e2a\u8f68\u8ff9\u3002</li>\n    <li>\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u201c\u57fa\u4e8e\u4ea4\u4e92\u7684\u7b56\u7565\u5bf9\u9f50\uff08IPA\uff09\u201d\uff0c\u4ee5\u63d0\u9ad8\u957f\u671f\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u901a\u8fc7\u7ed3\u6784\u5316\u8bc4\u4f30\u548c\u201cTerminal Bench Pro\u201d\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86ROME\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u4f18\u5f02\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Agentic crafting allows LLMs to work in real-life situations by learning from their actions over time.</li>\n    <li>The open-source community lacks a complete system for developing these agent LLMs, which is why we created the Agentic Learning Ecosystem (ALE).</li>\n    <li>ALE has three main parts: ROLL for optimizing model weights, ROCK for creating test environments, and iFlow CLI for managing agent context.</li>\n    <li>We launched ROME, an open-source agent model trained on a large amount of data, using ALE to improve its functionality.</li>\n    <li>ROME shows strong results in various tests, confirming that ALE is effective for developing agent-based models.</li>\n</ul>"}, "publishedAt": "2025-12-31T09:03:39.000Z", "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem", "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24873.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 197}, "isAuthorParticipating": false}, {"paper": {"id": "2512.25073", "authors": [{"_id": "69561b54832867f25352571f", "name": "Yi-Chuan Huang", "hidden": false}, {"_id": "69561b54832867f253525720", "name": "Hao-Jen Chien", "hidden": false}, {"_id": "69561b54832867f253525721", "name": "Chin-Yang Lin", "hidden": false}, {"_id": "69561b54832867f253525722", "name": "Ying-Huan Chen", "hidden": false}, {"_id": "69561b54832867f253525723", "name": "Yu-Lun Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/V45K0HdzwrxQ-2ZiZGMti.mp4"], "publishedAt": "2025-12-31T18:59:55.000Z", "submittedOnDailyAt": "2026-01-01T05:31:31.113Z", "title": "GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction", "submittedOnDailyBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "isPro": false, "fullname": "Yu-Lun Liu", "user": "yulunliu", "type": "user"}, "summary": "Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a 25times speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/", "upvotes": 21, "discussionId": "69561b55832867f253525724", "projectPage": "https://yichuanh.github.io/GaMO/", "ai_summary": "GaMO enhances sparse-view 3D reconstruction by using geometry-aware multi-view outpainting to improve scene coverage and consistency, achieving state-of-the-art performance with reduced computational cost.", "ai_keywords": ["diffusion-based methods", "multi-view outpainting", "geometry-aware denoising", "zero-shot", "PSNR", "LPIPS", "Replica", "ScanNet++", "GaMO"], "summary_zh": "<ul>\n    <li>3D\u91cd\u5efa\u6280\u672f\u5728\u591a\u89c6\u56fe\u56fe\u50cf\u573a\u666f\u6355\u6349\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u8f93\u5165\u89c6\u89d2\u6709\u9650\u65f6\u4ecd\u9762\u4e34\u6311\u6218\u3002</li>\n    <li>\u4f20\u7edf\u65b9\u6cd5\u5305\u62ec\u6b63\u5219\u5316\u6280\u672f\u3001\u8bed\u4e49\u5148\u9a8c\u548c\u51e0\u4f55\u7ea6\u675f\uff0c\u4f46\u6700\u65b0\u7684\u6269\u6563\u65b9\u6cd5\u901a\u8fc7\u751f\u6210\u65b0\u89c6\u89d2\u663e\u8457\u63d0\u5347\u4e86\u6548\u679c\u3002</li>\n    <li>\u76ee\u524d\u5148\u8fdb\u7684\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u89c6\u89d2\u8986\u76d6\u4e0d\u8db3\u3001\u751f\u6210\u89c6\u56fe\u95f4\u51e0\u4f55\u4e0d\u4e00\u81f4\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86GaMO\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u591a\u89c6\u56fe\u5916\u753b\u6280\u672f\u6539\u5584\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\uff0c\u62d3\u5c55\u73b0\u6709\u89c6\u89d2\u7684\u89c6\u91ce\u3002</li>\n    <li>GaMO\u5728Replica\u548cScanNet++\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5176\u57283\u30016\u30019\u4e2a\u8f93\u5165\u89c6\u89d2\u4e0b\u7684\u91cd\u5efa\u8d28\u91cf\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5904\u7406\u65f6\u95f4\u51cf\u5c11\u523010\u5206\u949f\u4ee5\u4e0b\uff0c\u901f\u5ea6\u63d0\u534725\u500d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>3D reconstruction has improved with dense multi-view imagery but struggles with limited views.</li>\n    <li>New diffusion-based methods create new views to improve training data, but have some limitations.</li>\n    <li>GaMO (Geometry-aware Multi-view Outpainter) is introduced to enhance sparse-view reconstruction by expanding the field of view from existing images.</li>\n    <li>GaMO maintains geometric consistency and provides better scene coverage without needing additional training.</li>\n    <li>It shows better reconstruction quality and is much faster than previous methods, processing in under 10 minutes.</li>\n</ul>"}, "publishedAt": "2025-12-31T13:59:55.000Z", "title": "GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction", "summary": "Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a 25times speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/V45K0HdzwrxQ-2ZiZGMti.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.25073.png", "numComments": 1, "submittedBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "fullname": "Yu-Lun Liu", "name": "yulunliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23380", "authors": [{"_id": "6953b35489916ff627aa414c", "user": {"_id": "658b21225c6fb5d5e312ad59", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658b21225c6fb5d5e312ad59/784jR4mqGh2e4CzXMBDEf.jpeg", "isPro": false, "fullname": "Mohammad Nasirzadeh", "user": "NasirzadehMoh", "type": "user"}, "name": "Mohammad Nasirzadeh", "status": "claimed_verified", "statusLastChangedAt": "2025-12-31T20:55:39.375Z", "hidden": false}, {"_id": "6953b35489916ff627aa414d", "name": "Jafar Tahmoresnezhad", "hidden": false}, {"_id": "6953b35489916ff627aa414e", "name": "Parviz Rashidi-Khazaee", "hidden": false}], "publishedAt": "2025-12-29T11:18:34.000Z", "submittedOnDailyAt": "2026-01-01T13:20:25.016Z", "title": "A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers", "submittedOnDailyBy": {"_id": "658b21225c6fb5d5e312ad59", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658b21225c6fb5d5e312ad59/784jR4mqGh2e4CzXMBDEf.jpeg", "isPro": false, "fullname": "Mohammad Nasirzadeh", "user": "NasirzadehMoh", "type": "user"}, "summary": "Log anomaly detection is crucial for preserving the security of operating systems. Depending on the source of log data collection, various information is recorded in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the different modalities of log data. Meanwhile, multimodal methods fail to handle the interactions between these modalities. Applying multimodal sentiment analysis to log anomaly detection, we propose CoLog, a framework that collaboratively encodes logs utilizing various modalities. CoLog utilizes collaborative transformers and multi-head impressed attention to learn interactions among several modalities, ensuring comprehensive anomaly detection. To handle the heterogeneity caused by these interactions, CoLog incorporates a modality adaptation layer, which adapts the representations from different log modalities. This methodology enables CoLog to learn nuanced patterns and dependencies within the data, enhancing its anomaly detection capabilities. Extensive experiments demonstrate CoLog's superiority over existing state-of-the-art methods. Furthermore, in detecting both point and collective anomalies, CoLog achieves a mean precision of 99.63%, a mean recall of 99.59%, and a mean F1 score of 99.61% across seven benchmark datasets for log-based anomaly detection. The comprehensive detection capabilities of CoLog make it highly suitable for cybersecurity, system monitoring, and operational efficiency. CoLog represents a significant advancement in log anomaly detection, providing a sophisticated and effective solution to point and collective anomaly detection through a unified framework and a solution to the complex challenges automatic log data analysis poses. We also provide the implementation of CoLog at https://github.com/NasirzadehMoh/CoLog.", "upvotes": 17, "discussionId": "6953b35489916ff627aa414f", "projectPage": "https://www.alarmif.com", "githubRepo": "https://github.com/NasirzadehMoh/CoLog", "githubRepoAddedBy": "user", "ai_summary": "CoLog, a log anomaly detection framework, employs collaborative transformers and multi-head impressed attention with a modality adaptation layer to achieve high-precision detection of both point and collective anomalies across diverse log modalities.", "ai_keywords": ["collaborative transformers", "multi-head impressed attention", "modality adaptation layer", "CoLog"], "githubStars": 1, "organization": {"_id": "6956bd05454ba27bc5d83432", "name": "alarmif", "fullname": "Alarmif", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/658b21225c6fb5d5e312ad59/SDDdjTaykSShBqSnAnfdg.png"}, "summary_zh": "<ul>\n    <li>\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u5bf9\u64cd\u4f5c\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>\u4f20\u7edf\u7684\u5355\u6a21\u6001\u65b9\u6cd5\u5ffd\u7565\u4e86\u65e5\u5fd7\u6570\u636e\u7684\u4e0d\u540c\u6a21\u6001\uff0c\u800c\u591a\u6a21\u6001\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u6a21\u6001\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002</li>\n    <li>CoLog\u6846\u67b6\u901a\u8fc7\u534f\u4f5c\u7f16\u7801\u4e0d\u540c\u6a21\u6001\u7684\u65e5\u5fd7\uff0c\u63d0\u9ad8\u5f02\u5e38\u68c0\u6d4b\u80fd\u529b\u3002</li>\n    <li>CoLog\u7ed3\u5408\u4e86\u534f\u4f5c\u53d8\u6362\u5668\u548c\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff0c\u540c\u65f6\u52a0\u5165\u6a21\u6001\u9002\u5e94\u5c42\uff0c\u4ee5\u5904\u7406\u6a21\u6001\u95f4\u7684\u5f02\u8d28\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cCoLog\u5728\u4e03\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7cbe\u786e\u7387\u8fbe\u523099.63%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Log anomaly detection is important for keeping operating systems secure.</li>\n    <li>Current methods struggle with different types of log data, called modalities, leading to ineffective detection.</li>\n    <li>CoLog is a new framework that uses collaborative transformers to improve the detection of anomalies in logs by understanding interactions between different modalities.</li>\n    <li>CoLog adapts data from various log types to learn complex patterns, resulting in high detection accuracy (99.63% precision, 99.59% recall, and 99.61% F1 score).</li>\n    <li>This framework is beneficial for cybersecurity and system monitoring and is available for implementation on GitHub.</li>\n</ul>"}, "publishedAt": "2025-12-29T06:18:34.000Z", "title": "A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers", "summary": "Log anomaly detection is crucial for preserving the security of operating systems. Depending on the source of log data collection, various information is recorded in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the different modalities of log data. Meanwhile, multimodal methods fail to handle the interactions between these modalities. Applying multimodal sentiment analysis to log anomaly detection, we propose CoLog, a framework that collaboratively encodes logs utilizing various modalities. CoLog utilizes collaborative transformers and multi-head impressed attention to learn interactions among several modalities, ensuring comprehensive anomaly detection. To handle the heterogeneity caused by these interactions, CoLog incorporates a modality adaptation layer, which adapts the representations from different log modalities. This methodology enables CoLog to learn nuanced patterns and dependencies within the data, enhancing its anomaly detection capabilities. Extensive experiments demonstrate CoLog's superiority over existing state-of-the-art methods. Furthermore, in detecting both point and collective anomalies, CoLog achieves a mean precision of 99.63%, a mean recall of 99.59%, and a mean F1 score of 99.61% across seven benchmark datasets for log-based anomaly detection. The comprehensive detection capabilities of CoLog make it highly suitable for cybersecurity, system monitoring, and operational efficiency. CoLog represents a significant advancement in log anomaly detection, providing a sophisticated and effective solution to point and collective anomaly detection through a unified framework and a solution to the complex challenges automatic log data analysis poses. We also provide the implementation of CoLog at https://github.com/NasirzadehMoh/CoLog.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23380.png", "numComments": 1, "submittedBy": {"_id": "658b21225c6fb5d5e312ad59", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658b21225c6fb5d5e312ad59/784jR4mqGh2e4CzXMBDEf.jpeg", "fullname": "Mohammad Nasirzadeh", "name": "NasirzadehMoh", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "6956bd05454ba27bc5d83432", "name": "alarmif", "fullname": "Alarmif", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/658b21225c6fb5d5e312ad59/SDDdjTaykSShBqSnAnfdg.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.25070", "authors": [{"_id": "6955e691832867f25352563b", "name": "Nikhil Chandak", "hidden": false}, {"_id": "6955e691832867f25352563c", "name": "Shashwat Goel", "hidden": false}, {"_id": "6955e691832867f25352563d", "name": "Ameya Prabhu", "hidden": false}, {"_id": "6955e691832867f25352563e", "name": "Moritz Hardt", "hidden": false}, {"_id": "6955e691832867f25352563f", "name": "Jonas Geiping", "hidden": false}], "publishedAt": "2025-12-31T18:59:51.000Z", "submittedOnDailyAt": "2026-01-01T00:59:02.810Z", "title": "Scaling Open-Ended Reasoning to Predict the Future", "submittedOnDailyBy": {"_id": "6506832221ac448013f94995", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6506832221ac448013f94995/sVUI1JV4Dxan5l-MqNze4.jpeg", "isPro": false, "fullname": "Shashwat Goel", "user": "shash42", "type": "user"}, "summary": "High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.", "upvotes": 13, "discussionId": "6955e691832867f253525640", "projectPage": "https://www.openforecaster.github.io", "githubRepo": "https://github.com/OpenForecaster/scaling-forecasting-training", "githubRepoAddedBy": "user", "githubStars": 6, "organization": {"_id": "638df552a11654155baca408", "name": "Intelligent-Systems", "fullname": "Max Planck Institute for Intelligent Systems", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670247618868-6183d0b249ef1d984699e4a3.jpeg"}, "summary_zh": "<ul>\n  <li>\u672c\u7814\u7a76\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u9884\u6d4b\u672a\u6765\u4e0d\u786e\u5b9a\u6027\u7684\u5f00\u653e\u5f0f\u95ee\u9898\u3002</li>\n  <li>\u6211\u4eec\u4ece\u6bcf\u65e5\u65b0\u95fb\u4e2d\u81ea\u52a8\u751f\u6210\u65b0\u7684\u9884\u6d4b\u95ee\u9898\uff0c\u4ee5\u6269\u5927\u8bad\u7ec3\u6570\u636e\u3002</li>\n  <li>\u4f7f\u7528\u79bb\u7ebf\u65b0\u95fb\u8bed\u6599\u5e93\uff0c\u907f\u514d\u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e2d\u6cc4\u9732\u672a\u6765\u4fe1\u606f\u3002</li>\n  <li>\u6211\u4eec\u7684\u6a21\u578bOpenForecaster 8B\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u8bb8\u591a\u5927\u578b\u4e13\u6709\u6a21\u578b\u3002</li>\n  <li>\u6240\u6709\u6a21\u578b\u3001\u4ee3\u7801\u548c\u6570\u636e\u90fd\u5df2\u5f00\u6e90\uff0c\u65b9\u4fbf\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>This research focuses on helping language models make predictions about future events using uncertain information.</li>\n    <li>New forecasting questions are created from daily news to train the models, using a fully automated process.</li>\n    <li>The model, called OpenForecaster 8B, shows strong performance, rivaling larger proprietary models in accuracy and consistency.</li>\n    <li>To ensure reliable training, the researchers use past news to avoid including any future information.</li>\n    <li>All models, code, and data are made publicly available to support further research in language model forecasting.</li>\n</ul>"}, "publishedAt": "2025-12-31T13:59:51.000Z", "title": "Scaling Open-Ended Reasoning to Predict the Future", "summary": "High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.25070.png", "numComments": 1, "submittedBy": {"_id": "6506832221ac448013f94995", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6506832221ac448013f94995/sVUI1JV4Dxan5l-MqNze4.jpeg", "fullname": "Shashwat Goel", "name": "shash42", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "638df552a11654155baca408", "name": "Intelligent-Systems", "fullname": "Max Planck Institute for Intelligent Systems", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670247618868-6183d0b249ef1d984699e4a3.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24551", "authors": [{"_id": "6955e985832867f253525678", "name": "Yuanhao Cai", "hidden": false}, {"_id": "6955e985832867f253525679", "name": "Kunpeng Li", "hidden": false}, {"_id": "6955e985832867f25352567a", "name": "Menglin Jia", "hidden": false}, {"_id": "6955e985832867f25352567b", "name": "Jialiang Wang", "hidden": false}, {"_id": "6955e985832867f25352567c", "name": "Junzhe Sun", "hidden": false}, {"_id": "6955e985832867f25352567d", "name": "Feng Liang", "hidden": false}, {"_id": "6955e985832867f25352567e", "name": "Weifeng Chen", "hidden": false}, {"_id": "6955e985832867f25352567f", "name": "Felix Juefei-Xu", "hidden": false}, {"_id": "6955e985832867f253525680", "name": "Chu Wang", "hidden": false}, {"_id": "6955e985832867f253525681", "name": "Ali Thabet", "hidden": false}, {"_id": "6955e985832867f253525682", "name": "Xiaoliang Dai", "hidden": false}, {"_id": "6955e985832867f253525683", "name": "Xuan Ju", "hidden": false}, {"_id": "6955e985832867f253525684", "name": "Alan Yuille", "hidden": false}, {"_id": "6955e985832867f253525685", "name": "Ji Hou", "hidden": false}], "publishedAt": "2025-12-31T01:19:14.000Z", "submittedOnDailyAt": "2026-01-01T00:58:49.694Z", "title": "PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation", "submittedOnDailyBy": {"_id": "673969726c12c4b98b6ab29f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C2elfn7L68jAt4dtHzDAW.png", "isPro": false, "fullname": "Yuanhao Cai", "user": "CaiYuanhao", "type": "user"}, "summary": "Recent advances in text-to-video (T2V) generation have achieved good visual quality, yet synthesizing videos that faithfully follow physical laws remains an open challenge. Existing methods mainly based on graphics or prompt extension struggle to generalize beyond simple simulated environments or learn implicit physical reasoning. The scarcity of training data with rich physics interactions and phenomena is also a problem. In this paper, we first introduce a Physics-Augmented video data construction Pipeline, PhyAugPipe, that leverages a vision-language model (VLM) with chain-of-thought reasoning to collect a large-scale training dataset, PhyVidGen-135K. Then we formulate a principled Physics-aware Groupwise Direct Preference Optimization, PhyGDPO, framework that builds upon the groupwise Plackett-Luce probabilistic model to capture holistic preferences beyond pairwise comparisons. In PhyGDPO, we design a Physics-Guided Rewarding (PGR) scheme that embeds VLM-based physics rewards to steer optimization toward physical consistency. We also propose a LoRA-Switch Reference (LoRA-SR) scheme that eliminates memory-heavy reference duplication for efficient training. Experiments show that our method significantly outperforms state-of-the-art open-source methods on PhyGenBench and VideoPhy2. Please check our project page at https://caiyuanhao1998.github.io/project/PhyGDPO for more video results. Our code, models, and data will be released at https://github.com/caiyuanhao1998/Open-PhyGDPO", "upvotes": 12, "discussionId": "6955e985832867f253525686", "projectPage": "https://caiyuanhao1998.github.io/project/PhyGDPO/", "githubRepo": "https://github.com/caiyuanhao1998/Open-PhyGDPO", "githubRepoAddedBy": "user", "githubStars": 4, "organization": {"_id": "6552117889e482b9ce8244d2", "name": "meta-ai-for-media-research", "fullname": "Meta AI for Media Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/PbL9kB-lO4M1oD2LvJugk.png"}, "summary_zh": "<ul>\n    <li>\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u6570\u636e\u6784\u5efa\u7ba1\u9053\uff0c\u79f0\u4e3aPhyAugPipe\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6536\u96c6\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u96c6PhyVidGen-135K\u3002</li>\n    <li>\u5efa\u7acb\u4e86\u4e00\u4e2a\u7269\u7406\u611f\u77e5\u7684\u4f18\u5316\u6846\u67b6PhyGDPO\uff0c\u53ef\u4ee5\u6355\u6349\u6574\u4f53\u504f\u597d\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6210\u5bf9\u6bd4\u8f83\u3002</li>\n    <li>\u5728PhyGDPO\u4e2d\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7269\u7406\u5f15\u5bfc\u5956\u52b1\u673a\u5236\uff0c\u5e2e\u52a9\u4f18\u5316\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cdLoRA-Switch Reference\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u8bad\u7ec3\u4e2d\u7684\u5185\u5b58\u5360\u7528\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6d4b\u8bd5\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5f00\u6e90\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Recent advancements in text-to-video generation are good in visual quality but struggle with obeying physical laws.</li>\n  <li>Current methods have issues with generalizing beyond simple environments and lack sufficient training data with complex physics interactions.</li>\n  <li>This paper introduces PhyAugPipe, a method to create a large dataset called PhyVidGen-135K using advanced reasoning techniques.</li>\n  <li>It also presents a new optimization framework called PhyGDPO that focuses on capturing overall preferences and ensuring physical consistency in videos.</li>\n  <li>Experiments show that this method performs significantly better than existing methods on specific benchmarks.</li>\n</ul>"}, "publishedAt": "2025-12-30T20:19:14.000Z", "title": "PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation", "summary": "Recent advances in text-to-video (T2V) generation have achieved good visual quality, yet synthesizing videos that faithfully follow physical laws remains an open challenge. Existing methods mainly based on graphics or prompt extension struggle to generalize beyond simple simulated environments or learn implicit physical reasoning. The scarcity of training data with rich physics interactions and phenomena is also a problem. In this paper, we first introduce a Physics-Augmented video data construction Pipeline, PhyAugPipe, that leverages a vision-language model (VLM) with chain-of-thought reasoning to collect a large-scale training dataset, PhyVidGen-135K. Then we formulate a principled Physics-aware Groupwise Direct Preference Optimization, PhyGDPO, framework that builds upon the groupwise Plackett-Luce probabilistic model to capture holistic preferences beyond pairwise comparisons. In PhyGDPO, we design a Physics-Guided Rewarding (PGR) scheme that embeds VLM-based physics rewards to steer optimization toward physical consistency. We also propose a LoRA-Switch Reference (LoRA-SR) scheme that eliminates memory-heavy reference duplication for efficient training. Experiments show that our method significantly outperforms state-of-the-art open-source methods on PhyGenBench and VideoPhy2. Please check our project page at https://caiyuanhao1998.github.io/project/PhyGDPO for more video results. Our code, models, and data will be released at https://github.com/caiyuanhao1998/Open-PhyGDPO", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24551.png", "numComments": 2, "submittedBy": {"_id": "673969726c12c4b98b6ab29f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C2elfn7L68jAt4dtHzDAW.png", "fullname": "Yuanhao Cai", "name": "CaiYuanhao", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "organization": {"_id": "6552117889e482b9ce8244d2", "name": "meta-ai-for-media-research", "fullname": "Meta AI for Media Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/PbL9kB-lO4M1oD2LvJugk.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23343", "authors": [{"_id": "69549fd9869a8627b452c945", "name": "Jiafeng Liang", "hidden": false}, {"_id": "69549fd9869a8627b452c946", "name": "Hao Li", "hidden": false}, {"_id": "69549fd9869a8627b452c947", "name": "Chang Li", "hidden": false}, {"_id": "69549fd9869a8627b452c948", "name": "Jiaqi Zhou", "hidden": false}, {"_id": "69549fd9869a8627b452c949", "name": "Shixin Jiang", "hidden": false}, {"_id": "69549fd9869a8627b452c94a", "name": "Zekun Wang", "hidden": false}, {"_id": "69549fd9869a8627b452c94b", "name": "Changkai Ji", "hidden": false}, {"_id": "69549fd9869a8627b452c94c", "name": "Zhihao Zhu", "hidden": false}, {"_id": "69549fd9869a8627b452c94d", "name": "Runxuan Liu", "hidden": false}, {"_id": "69549fd9869a8627b452c94e", "name": "Tao Ren", "hidden": false}, {"_id": "69549fd9869a8627b452c94f", "name": "Jinlan Fu", "hidden": false}, {"_id": "69549fd9869a8627b452c950", "name": "See-Kiong Ng", "hidden": false}, {"_id": "69549fd9869a8627b452c951", "name": "Xia Liang", "hidden": false}, {"_id": "69549fd9869a8627b452c952", "name": "Ming Liu", "hidden": false}, {"_id": "69549fd9869a8627b452c953", "name": "Bing Qin", "hidden": false}], "publishedAt": "2025-12-29T10:01:32.000Z", "submittedOnDailyAt": "2026-01-01T05:49:15.279Z", "title": "AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents", "submittedOnDailyBy": {"_id": "656832dfbd65fd41ee7aa8cd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656832dfbd65fd41ee7aa8cd/HHkyetTqNq1wIBPipzjQA.jpeg", "isPro": false, "fullname": "Zekun Wang", "user": "kugwzk", "type": "user"}, "summary": "Memory serves as the pivotal nexus bridging past and future, providing both humans and AI systems with invaluable concepts and experience to navigate complex tasks. Recent research on autonomous agents has increasingly focused on designing efficient memory workflows by drawing on cognitive neuroscience. However, constrained by interdisciplinary barriers, existing works struggle to assimilate the essence of human memory mechanisms. To bridge this gap, we systematically synthesizes interdisciplinary knowledge of memory, connecting insights from cognitive neuroscience with LLM-driven agents. Specifically, we first elucidate the definition and function of memory along a progressive trajectory from cognitive neuroscience through LLMs to agents. We then provide a comparative analysis of memory taxonomy, storage mechanisms, and the complete management lifecycle from both biological and artificial perspectives. Subsequently, we review the mainstream benchmarks for evaluating agent memory. Additionally, we explore memory security from dual perspectives of attack and defense. Finally, we envision future research directions, with a focus on multimodal memory systems and skill acquisition.", "upvotes": 12, "discussionId": "69549fd9869a8627b452c954", "githubRepo": "https://github.com/AgentMemory/Huaman-Agent-Memory", "githubRepoAddedBy": "auto", "githubStars": 24, "summary_zh": "<ul>\n    <li>\u8bb0\u5fc6\u662f\u8fde\u63a5\u8fc7\u53bb\u548c\u672a\u6765\u7684\u91cd\u8981\u7ebd\u5e26\uff0c\u5bf9\u4eba\u7c7b\u548c\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u90fd\u5f88\u91cd\u8981\u3002</li>\n    <li>\u6700\u8fd1\u7684\u7814\u7a76\u96c6\u4e2d\u5728\u8bbe\u8ba1\u9ad8\u6548\u7684\u8bb0\u5fc6\u5de5\u4f5c\u6d41\u7a0b\uff0c\u4f46\u5b58\u5728\u8de8\u5b66\u79d1\u7684\u969c\u788d\u3002</li>\n    <li>\u672c\u6587\u7cfb\u7edf\u5730\u6574\u5408\u4e86\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u7684\u8bb0\u5fc6\u77e5\u8bc6\u3002</li>\n    <li>\u6211\u4eec\u5206\u6790\u4e86\u8bb0\u5fc6\u7684\u5b9a\u4e49\u3001\u529f\u80fd\u548c\u5206\u7c7b\uff0c\u4ee5\u53ca\u751f\u7269\u548c\u4eba\u5de5\u7cfb\u7edf\u7684\u7ba1\u7406\u751f\u547d\u5468\u671f\u3002</li>\n    <li>\u63a2\u8ba8\u4e86\u8bb0\u5fc6\u5b89\u5168\u6027\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5982\u591a\u6a21\u6001\u8bb0\u5fc6\u7cfb\u7edf\u548c\u6280\u80fd\u83b7\u53d6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Memory connects our past experiences to future actions for both humans and AI.</li>\n    <li>Research is looking into how AI can use memory like humans do, based on insights from brain science.</li>\n    <li>The study compares different types of memory and how they are stored and managed in both humans and AI agents.</li>\n    <li>It reviews current ways to evaluate how well AI agents use memory.</li>\n    <li>The paper suggests future research on improving memory systems in AI and how they can learn new skills.</li>\n</ul>"}, "publishedAt": "2025-12-29T05:01:32.000Z", "title": "AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents", "summary": "Memory serves as the pivotal nexus bridging past and future, providing both humans and AI systems with invaluable concepts and experience to navigate complex tasks. Recent research on autonomous agents has increasingly focused on designing efficient memory workflows by drawing on cognitive neuroscience. However, constrained by interdisciplinary barriers, existing works struggle to assimilate the essence of human memory mechanisms. To bridge this gap, we systematically synthesizes interdisciplinary knowledge of memory, connecting insights from cognitive neuroscience with LLM-driven agents. Specifically, we first elucidate the definition and function of memory along a progressive trajectory from cognitive neuroscience through LLMs to agents. We then provide a comparative analysis of memory taxonomy, storage mechanisms, and the complete management lifecycle from both biological and artificial perspectives. Subsequently, we review the mainstream benchmarks for evaluating agent memory. Additionally, we explore memory security from dual perspectives of attack and defense. Finally, we envision future research directions, with a focus on multimodal memory systems and skill acquisition.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23343.png", "numComments": 1, "submittedBy": {"_id": "656832dfbd65fd41ee7aa8cd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656832dfbd65fd41ee7aa8cd/HHkyetTqNq1wIBPipzjQA.jpeg", "fullname": "Zekun Wang", "name": "kugwzk", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24210", "authors": [{"_id": "6955e1ac832867f25352559b", "name": "Ruoshi Wen", "hidden": false}, {"_id": "6955e1ac832867f25352559c", "name": "Guangzeng Chen", "hidden": false}, {"_id": "6955e1ac832867f25352559d", "name": "Zhongren Cui", "hidden": false}, {"_id": "6955e1ac832867f25352559e", "name": "Min Du", "hidden": false}, {"_id": "6955e1ac832867f25352559f", "name": "Yang Gou", "hidden": false}, {"_id": "6955e1ac832867f2535255a0", "name": "Zhigang Han", "hidden": false}, {"_id": "6955e1ac832867f2535255a1", "name": "Liqun Huang", "hidden": false}, {"_id": "6955e1ac832867f2535255a2", "name": "Mingyu Lei", "hidden": false}, {"_id": "6955e1ac832867f2535255a3", "name": "Yunfei Li", "hidden": false}, {"_id": "6955e1ac832867f2535255a4", "name": "Zhuohang Li", "hidden": false}, {"_id": "6955e1ac832867f2535255a5", "name": "Wenlei Liu", "hidden": false}, {"_id": "6955e1ac832867f2535255a6", "name": "Yuxiao Liu", "hidden": false}, {"_id": "6955e1ac832867f2535255a7", "name": "Xiao Ma", "hidden": false}, {"_id": "6955e1ac832867f2535255a8", "name": "Hao Niu", "hidden": false}, {"_id": "6955e1ac832867f2535255a9", "name": "Yutao Ouyang", "hidden": false}, {"_id": "6955e1ac832867f2535255aa", "name": "Zeyu Ren", "hidden": false}, {"_id": "6955e1ac832867f2535255ab", "name": "Haixin Shi", "hidden": false}, {"_id": "6955e1ac832867f2535255ac", "name": "Wei Xu", "hidden": false}, {"_id": "6955e1ac832867f2535255ad", "name": "Haoxiang Zhang", "hidden": false}, {"_id": "6955e1ac832867f2535255ae", "name": "Jiajun Zhang", "hidden": false}, {"_id": "6955e1ac832867f2535255af", "name": "Xiao Zhang", "hidden": false}, {"_id": "6955e1ac832867f2535255b0", "name": "Liwei Zheng", "hidden": false}, {"_id": "6955e1ac832867f2535255b1", "name": "Weiheng Zhong", "hidden": false}, {"_id": "6955e1ac832867f2535255b2", "name": "Yifei Zhou", "hidden": false}, {"_id": "6955e1ac832867f2535255b3", "name": "Zhengming Zhu", "hidden": false}, {"_id": "6955e1ac832867f2535255b4", "name": "Hang Li", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6478597d91398856110a6738/xYPPHmiwlBKZ8t6hU2WKm.mp4"], "publishedAt": "2025-12-30T13:22:16.000Z", "submittedOnDailyAt": "2026-01-01T00:44:36.055Z", "title": "GR-Dexter Technical Report", "submittedOnDailyBy": {"_id": "6478597d91398856110a6738", "avatarUrl": "/avatars/c3bc61eb7554ac21946b424e1314f1a7.svg", "isPro": false, "fullname": "Xiao Ma", "user": "yusufma555", "type": "user"}, "summary": "Vision-language-action (VLA) models have enabled language-conditioned, long-horizon robot manipulation, but most existing systems are limited to grippers. Scaling VLA policies to bimanual robots with high degree-of-freedom (DoF) dexterous hands remains challenging due to the expanded action space, frequent hand-object occlusions, and the cost of collecting real-robot data. We present GR-Dexter, a holistic hardware-model-data framework for VLA-based generalist manipulation on a bimanual dexterous-hand robot. Our approach combines the design of a compact 21-DoF robotic hand, an intuitive bimanual teleoperation system for real-robot data collection, and a training recipe that leverages teleoperated robot trajectories together with large-scale vision-language and carefully curated cross-embodiment datasets. Across real-world evaluations spanning long-horizon everyday manipulation and generalizable pick-and-place, GR-Dexter achieves strong in-domain performance and improved robustness to unseen objects and unseen instructions. We hope GR-Dexter serves as a practical step toward generalist dexterous-hand robotic manipulation.", "upvotes": 11, "discussionId": "6955e1ad832867f2535255b5", "projectPage": "https://byte-dexter.github.io/gr-dexter/", "ai_summary": "GR-Dexter introduces a hardware-model-data framework for bimanual dexterous-hand robot manipulation using vision-language-action models, combining teleoperation data and multimodal datasets to achieve robust generalization.", "ai_keywords": ["Vision-language-action (VLA) models", "training recipe", "large-scale vision-language datasets", "cross-embodiment datasets"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>GR-Dexter\u662f\u4e00\u4e2a\u7528\u4e8e\u53cc\u624b\u7075\u5de7\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6846\u67b6\u3002</li>\n    <li>\u5b83\u7ed3\u5408\u4e86\u4e00\u4e2a\u7d27\u51d1\u768421\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u624b\u548c\u4e00\u4e2a\u76f4\u89c2\u7684\u53cc\u624b\u9065\u64cd\u4f5c\u7cfb\u7edf\u3002</li>\n    <li>\u8be5\u7cfb\u7edf\u80fd\u591f\u901a\u8fc7\u9065\u64cd\u4f5c\u6536\u96c6\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\uff0c\u5e76\u4f7f\u7528\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002</li>\n    <li>GR-Dexter\u5728\u65e5\u5e38\u64cd\u4f5c\u548c\u901a\u7528\u7684\u53d6\u653e\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002</li>\n    <li>\u8be5\u6846\u67b6\u4e3a\u7075\u5de7\u624b\u673a\u5668\u4eba\u7684\u901a\u7528\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Vision-language-action (VLA) models help robots manipulate objects using language but mostly work with simple grippers.</li>\n  <li>Scaling these models to bimanual robots with complex hands is difficult due to more actions, hand-object hiding, and collecting data.</li>\n  <li>GR-Dexter is a new framework that combines a compact 21-DoF robotic hand, an easy-to-use system for collecting data, and effective training methods.</li>\n  <li>It uses data from teleoperated robot movements and large datasets to improve robot performance in real-world tasks.</li>\n  <li>GR-Dexter shows good results in everyday tasks and is better at handling new objects and instructions, making it a step forward for advanced robot manipulation.</li>\n</ul>"}, "publishedAt": "2025-12-30T08:22:16.000Z", "title": "GR-Dexter Technical Report", "summary": "Vision-language-action (VLA) models have enabled language-conditioned, long-horizon robot manipulation, but most existing systems are limited to grippers. Scaling VLA policies to bimanual robots with high degree-of-freedom (DoF) dexterous hands remains challenging due to the expanded action space, frequent hand-object occlusions, and the cost of collecting real-robot data. We present GR-Dexter, a holistic hardware-model-data framework for VLA-based generalist manipulation on a bimanual dexterous-hand robot. Our approach combines the design of a compact 21-DoF robotic hand, an intuitive bimanual teleoperation system for real-robot data collection, and a training recipe that leverages teleoperated robot trajectories together with large-scale vision-language and carefully curated cross-embodiment datasets. Across real-world evaluations spanning long-horizon everyday manipulation and generalizable pick-and-place, GR-Dexter achieves strong in-domain performance and improved robustness to unseen objects and unseen instructions. We hope GR-Dexter serves as a practical step toward generalist dexterous-hand robotic manipulation.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6478597d91398856110a6738/xYPPHmiwlBKZ8t6hU2WKm.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24210.png", "numComments": 2, "submittedBy": {"_id": "6478597d91398856110a6738", "avatarUrl": "/avatars/c3bc61eb7554ac21946b424e1314f1a7.svg", "fullname": "Xiao Ma", "name": "yusufma555", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23988", "authors": [{"_id": "6955e629832867f253525630", "name": "Zhenyu Zhang", "hidden": false}, {"_id": "6955e629832867f253525631", "name": "Shujian Zhang", "hidden": false}, {"_id": "6955e629832867f253525632", "name": "John Lambert", "hidden": false}, {"_id": "6955e629832867f253525633", "name": "Wenxuan Zhou", "hidden": false}, {"_id": "6955e629832867f253525634", "name": "Zhangyang Wang", "hidden": false}, {"_id": "6955e629832867f253525635", "name": "Mingqing Chen", "hidden": false}, {"_id": "6955e629832867f253525636", "name": "Andrew Hard", "hidden": false}, {"_id": "6955e629832867f253525637", "name": "Rajiv Mathews", "hidden": false}, {"_id": "6955e629832867f253525638", "name": "Lun Wang", "hidden": false}], "publishedAt": "2025-12-30T05:09:11.000Z", "submittedOnDailyAt": "2026-01-01T00:42:58.807Z", "title": "Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we propose an unsupervised framework (namely, RISE: Reasoning behavior Interpretability via Sparse auto-Encoder) for discovering reasoning vectors, which we define as directions in the activation space that encode distinct reasoning behaviors. By segmenting chain-of-thought traces into sentence-level 'steps' and training sparse auto-encoders (SAEs) on step-level activations, we uncover disentangled features corresponding to interpretable behaviors such as reflection and backtracking. Visualization and clustering analyses show that these behaviors occupy separable regions in the decoder column space. Moreover, targeted interventions on SAE-derived vectors can controllably amplify or suppress specific reasoning behaviors, altering inference trajectories without retraining. Beyond behavior-specific disentanglement, SAEs capture structural properties such as response length, revealing clusters of long versus short reasoning traces. More interestingly, SAEs enable the discovery of novel behaviors beyond human supervision. We demonstrate the ability to control response confidence by identifying confidence-related vectors in the SAE decoder space. These findings underscore the potential of unsupervised latent discovery for both interpreting and controllably steering reasoning in LLMs.", "upvotes": 6, "discussionId": "6955e62a832867f253525639", "ai_summary": "An unsupervised framework using sparse auto-encoders identifies and controls interpretable reasoning behaviors in large language models through disentangled latent vectors.", "ai_keywords": ["sparse auto-encoders (SAEs)", "chain-of-thought traces", "activation space", "decoder column space", "disentangled features", "reasoning vectors", "SAE-derived vectors", "confidence-related vectors", "unsupervised latent discovery"], "organization": {"_id": "60f6cbb2852126bac698c89e", "name": "deepmind", "fullname": "Deepmind", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"}, "summary_zh": "<ul>\n    <li>\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u4e0d\u65ad\u63d0\u9ad8\uff0c\u4f46\u5b83\u4eec\u7684\u5185\u90e8\u63a8\u7406\u673a\u5236\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002</li>\n    <li>\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u5b9a\u4e49\u7684\u6982\u5ff5\u6765\u5206\u6790\u63a8\u7406\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u6240\u6709\u63a8\u7406\u884c\u4e3a\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u6846\u67b6\uff08RISE\uff09\uff0c\u7528\u4e8e\u53d1\u73b0\u63a8\u7406\u5411\u91cf\uff0c\u8fd9\u4e9b\u5411\u91cf\u8868\u793a\u4e0d\u540c\u7684\u63a8\u7406\u884c\u4e3a\u3002</li>\n    <li>\u901a\u8fc7\u5c06\u63a8\u7406\u8fc7\u7a0b\u5206\u89e3\u4e3a\u53e5\u5b50\u7ea7\u522b\u7684\u201c\u6b65\u9aa4\u201d\uff0c\u5e76\u8bad\u7ec3\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff0c\u63ed\u793a\u51fa\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u884c\u4e3a\u7279\u5f81\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u8fd8\u53ef\u4ee5\u63a7\u5236\u548c\u8c03\u6574\u63a8\u7406\u884c\u4e3a\uff0c\u5e76\u53d1\u73b0\u65b0\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u663e\u793a\u51fa\u65e0\u76d1\u7763\u53d1\u73b0\u7684\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) are improving in reasoning, but how they reason is not well understood.</li>\n    <li>Current methods to analyze reasoning rely on human-defined concepts, which can't capture all reasoning behaviors.</li>\n    <li>This study introduces an unsupervised framework called RISE to find reasoning behaviors as distinct vectors in the model's activation space.</li>\n    <li>By analyzing reasoning steps, the framework reveals different behaviors like reflection and backtracking, allowing for better interpretation.</li>\n    <li>The framework also helps discover new behaviors and control aspects like response confidence without needing to retrain the model.</li>\n</ul>"}, "publishedAt": "2025-12-30T00:09:11.000Z", "title": "Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process", "summary": "Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we propose an unsupervised framework (namely, RISE: Reasoning behavior Interpretability via Sparse auto-Encoder) for discovering reasoning vectors, which we define as directions in the activation space that encode distinct reasoning behaviors. By segmenting chain-of-thought traces into sentence-level 'steps' and training sparse auto-encoders (SAEs) on step-level activations, we uncover disentangled features corresponding to interpretable behaviors such as reflection and backtracking. Visualization and clustering analyses show that these behaviors occupy separable regions in the decoder column space. Moreover, targeted interventions on SAE-derived vectors can controllably amplify or suppress specific reasoning behaviors, altering inference trajectories without retraining. Beyond behavior-specific disentanglement, SAEs capture structural properties such as response length, revealing clusters of long versus short reasoning traces. More interestingly, SAEs enable the discovery of novel behaviors beyond human supervision. We demonstrate the ability to control response confidence by identifying confidence-related vectors in the SAE decoder space. These findings underscore the potential of unsupervised latent discovery for both interpreting and controllably steering reasoning in LLMs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23988.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 197}, "organization": {"_id": "60f6cbb2852126bac698c89e", "name": "deepmind", "fullname": "Deepmind", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1638956859875-5f1158120c833276f61f1a84.jpeg"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2512.23447", "authors": [{"_id": "69534a9589916ff627aa3f5c", "name": "Ang Lv", "hidden": false}, {"_id": "69534a9589916ff627aa3f5d", "name": "Jin Ma", "hidden": false}, {"_id": "69534a9589916ff627aa3f5e", "name": "Yiyuan Ma", "hidden": false}, {"_id": "69534a9589916ff627aa3f5f", "name": "Siyuan Qiao", "hidden": false}], "publishedAt": "2025-12-29T13:03:18.000Z", "submittedOnDailyAt": "2025-12-30T01:18:40.635Z", "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss", "submittedOnDailyBy": {"_id": "64b8ca3c5067873176d4b436", "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg", "isPro": false, "fullname": "AngLv", "user": "AngLv", "type": "user"}, "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.", "upvotes": 71, "discussionId": "69534a9589916ff627aa3f60", "ai_summary": "An expert-router coupling (ERC) loss aligns router decisions with expert capabilities in Mixture-of-Experts (MoE) models by enforcing constraints on internal activations, improving performance and computational efficiency.", "ai_keywords": ["Mixture-of-Experts (MoE)", "expert-router coupling (ERC) loss", "router embeddings", "proxy tokens", "internal activations", "MoE-LLMs", "expert specialization levels", "n\u00b2 activations"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>Mixture-of-Experts (MoE) \u6a21\u578b\u7f3a\u4e4f\u6709\u6548\u7684\u7ea6\u675f\uff0c\u5bfc\u81f4\u8def\u7531\u5668\u7684\u51b3\u7b56\u4e0e\u4e13\u5bb6\u80fd\u529b\u4e0d\u5339\u914d\uff0c\u4ece\u800c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u8f85\u52a9\u635f\u5931\u51fd\u6570\uff0c\u79f0\u4e3a\u4e13\u5bb6-\u8def\u7531\u5668\u8026\u5408\u635f\u5931\uff08ERC loss\uff09\uff0c\u4f7f\u8def\u7531\u5668\u7684\u51b3\u7b56\u4e0e\u4e13\u5bb6\u80fd\u529b\u7d27\u5bc6\u7ed3\u5408\u3002</li>\n    <li>ERC\u635f\u5931\u5f3a\u5236\u6267\u884c\u4e24\u4e2a\u7ea6\u675f\uff1a\u6bcf\u4e2a\u4e13\u5bb6\u5bf9\u5176\u4ee3\u7406\u6807\u8bb0\u7684\u6fc0\u6d3b\u5fc5\u987b\u9ad8\u4e8e\u5bf9\u5176\u4ed6\u4e13\u5bb6\u7684\u4ee3\u7406\u6807\u8bb0\uff1b\u6bcf\u4e2a\u4ee3\u7406\u6807\u8bb0\u5fc5\u987b\u5f15\u53d1\u5176\u5bf9\u5e94\u4e13\u5bb6\u7684\u66f4\u5f3a\u6fc0\u6d3b\u3002</li>\n    <li>ERC\u635f\u5931\u8ba1\u7b97\u9ad8\u6548\uff0c\u4ec5\u5728n^2\u7684\u6fc0\u6d3b\u4e0a\u64cd\u4f5c\uff0c\u5176\u4e2dn\u662f\u4e13\u5bb6\u6570\u91cf\uff0c\u4e0e\u6279\u91cf\u5927\u5c0f\u65e0\u5173\u3002</li>\n    <li>\u901a\u8fc7\u5bf93B\u523015B\u53c2\u6570\u7684MoE-LLMs\u8fdb\u884c\u9884\u8bad\u7ec3\u548c\u5bf9\u6570\u4e07\u4ebf\u4e2a\u6807\u8bb0\u7684\u5206\u6790\uff0c\u6211\u4eec\u8bc1\u660e\u4e86ERC\u635f\u5931\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e13\u5bb6\u4e13\u95e8\u5316\u6c34\u5e73\u7684\u7075\u6d3b\u63a7\u5236\u548c\u8ddf\u8e2a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Mixture-of-Experts (MoE) models have issues because the router's choices don't always match the experts' abilities, which can hurt performance.</li>\n    <li>We introduce a new method called expert-router coupling (ERC) loss to improve this by linking the router's decisions more closely with what the experts can do.</li>\n    <li>The ERC loss enforces two key rules: experts should respond better to their own tokens than to others, and each token should activate its expert more than any other expert.</li>\n    <li>This method is efficient, working with a fixed cost related to the number of experts, rather than the number of tokens, making it suitable for large models.</li>\n    <li>Our tests on large models show that the ERC loss helps track and control how well each expert is specializing during training.</li>\n</ul>"}, "publishedAt": "2025-12-29T08:03:18.000Z", "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss", "summary": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23447.png", "numComments": 1, "submittedBy": {"_id": "64b8ca3c5067873176d4b436", "avatarUrl": "/avatars/b659d147b2454b47c9a7e89bbed525fc.svg", "fullname": "AngLv", "name": "AngLv", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24880", "authors": [{"_id": "69561fbf832867f253525726", "name": "Zhenda Xie", "hidden": false}, {"_id": "69561fbf832867f253525727", "name": "Yixuan Wei", "hidden": false}, {"_id": "69561fbf832867f253525728", "name": "Huanqi Cao", "hidden": false}, {"_id": "69561fbf832867f253525729", "name": "Chenggang Zhao", "hidden": false}, {"_id": "69561fbf832867f25352572a", "name": "Chengqi Deng", "hidden": false}, {"_id": "69561fbf832867f25352572b", "name": "Jiashi Li", "hidden": false}, {"_id": "69561fbf832867f25352572c", "name": "Damai Dai", "hidden": false}, {"_id": "69561fbf832867f25352572d", "name": "Huazuo Gao", "hidden": false}, {"_id": "69561fbf832867f25352572e", "name": "Jiang Chang", "hidden": false}, {"_id": "69561fbf832867f25352572f", "name": "Liang Zhao", "hidden": false}, {"_id": "69561fbf832867f253525730", "name": "Shangyan Zhou", "hidden": false}, {"_id": "69561fbf832867f253525731", "name": "Zhean Xu", "hidden": false}, {"_id": "69561fbf832867f253525732", "name": "Zhengyan Zhang", "hidden": false}, {"_id": "69561fbf832867f253525733", "name": "Wangding Zeng", "hidden": false}, {"_id": "69561fbf832867f253525734", "name": "Shengding Hu", "hidden": false}, {"_id": "69561fbf832867f253525735", "name": "Yuqing Wang", "hidden": false}, {"_id": "69561fbf832867f253525736", "name": "Jingyang Yuan", "hidden": false}, {"_id": "69561fbf832867f253525737", "name": "Lean Wang", "hidden": false}, {"_id": "69561fbf832867f253525738", "name": "Wenfeng Liang", "hidden": false}], "publishedAt": "2025-12-31T14:16:26.000Z", "submittedOnDailyAt": "2026-01-01T07:30:32.169Z", "title": "mHC: Manifold-Constrained Hyper-Connections", "submittedOnDailyBy": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "isPro": false, "fullname": "Adina Yakefu", "user": "AdinaY", "type": "user"}, "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.", "upvotes": 59, "discussionId": "69561fc0832867f253525739", "ai_summary": "Manifold-Constrained Hyper-Connections (mHC) stabilize and scale residual connection architectures by restoring identity mapping properties through manifold projection and infrastructure optimization.", "ai_keywords": ["Hyper-Connections (HC)", "Manifold-Constrained Hyper-Connections (mHC)", "residual connections", "residual stream width", "connectivity patterns", "identity mapping property", "training instability", "memory access overhead", "manifold projection", "infrastructure optimization", "scalability"], "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u7814\u7a76\u63d0\u51fa\u4e86\u201c\u8d85\u8fde\u63a5\u201d\uff08Hyper-Connections, HC\uff09\uff0c\u6269\u5c55\u4e86\u6b8b\u5dee\u8fde\u63a5\u7684\u6982\u5ff5\u3002</li>\n    <li>\u867d\u7136HC\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f46\u4e5f\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u53ef\u6269\u5c55\u6027\u5dee\u3002</li>\n    <li>\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u201c\u6d41\u5f62\u7ea6\u675f\u8d85\u8fde\u63a5\u201d\uff08mHC\uff09\u7684\u6846\u67b6\uff0c\u6062\u590d\u4e86\u6b8b\u5dee\u8fde\u63a5\u7684\u7279\u6027\u3002</li>\n    <li>mHC\u5728\u8bad\u7ec3\u6548\u7387\u4e0a\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u8272\u3002</li>\n    <li>mHC\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u62d3\u6251\u67b6\u6784\u8bbe\u8ba1\uff0c\u5e76\u4e3a\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent studies have explored Hyper-Connections (HC) that improve residual connections in neural networks.</li>\n    <li>These improvements can lead to better performance but also cause training problems and increased memory use.</li>\n    <li>The proposed solution, Manifold-Constrained Hyper-Connections (mHC), aims to fix these issues by maintaining the identity mapping property.</li>\n    <li>mHC includes optimizations for better efficiency and scalability during training.</li>\n    <li>Experiments show that mHC provides significant performance benefits and could influence future designs of neural network architectures.</li>\n</ul>"}, "publishedAt": "2025-12-31T09:16:26.000Z", "title": "mHC: Manifold-Constrained Hyper-Connections", "summary": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24880.png", "numComments": 1, "submittedBy": {"_id": "63a369d98c0c89dcae3b8329", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg", "fullname": "Adina Yakefu", "name": "AdinaY", "type": "user", "isPro": false, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 1140}, "organization": {"_id": "652faff917096ceb6bf53f3f", "name": "deepseek-ai", "fullname": "DeepSeek", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6538815d1bdb3c40db94fbfa/xMBly9PUMphrFVMxLX4kq.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23576", "authors": [{"_id": "69534f1e89916ff627aa3fe3", "name": "Ethan Chern", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe4", "name": "Zhulin Hu", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe5", "name": "Bohao Tang", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe6", "name": "Jiadi Su", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe7", "name": "Steffi Chern", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe8", "name": "Zhijie Deng", "hidden": false}, {"_id": "69534f1e89916ff627aa3fe9", "name": "Pengfei Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64bb5f9d8e051085bace4d1e/skNa_3Ly0Bg7F6aL0mk92.mp4"], "publishedAt": "2025-12-29T16:17:36.000Z", "submittedOnDailyAt": "2025-12-30T02:36:23.479Z", "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation", "submittedOnDailyBy": {"_id": "64bb5f9d8e051085bace4d1e", "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg", "isPro": false, "fullname": "Ethan Chern", "user": "ethanchern", "type": "user"}, "summary": "Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.", "upvotes": 51, "discussionId": "69534f1e89916ff627aa3fea", "githubRepo": "https://github.com/GAIR-NLP/LiveTalk", "githubRepoAddedBy": "user", "ai_summary": "Real-time multimodal video generation via diffusion is enabled by an improved distillation approach with multimodal conditioning and optimized scheduling, reducing inference latency while maintaining quality for interactive systems.", "ai_keywords": ["diffusion models", "bidirectional attention", "distillation methods", "on-policy distillation", "Self Forcing", "audio language models", "Anchor-Heavy Identity Sinks", "multimodal conditioning", "autoregressive", "on-policy optimization"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u5b9e\u65f6\u89c6\u9891\u751f\u6210\u5bf9\u6784\u5efa\u591a\u6a21\u6001\u4ea4\u4e92AI\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u89c6\u9891\u5e27\u65f6\u5b58\u5728\u5ef6\u8fdf\u95ee\u9898\u3002</li>\n    <li>\u73b0\u6709\u7684\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u6587\u672c\u8f6c\u89c6\u9891\u751f\u6210\uff0c\u5bfc\u81f4\u4eba\u673a\u4ea4\u4e92\u4e0d\u591f\u81ea\u7136\u548c\u9ad8\u6548\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u4f18\u5316\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\uff08\u6587\u672c\u3001\u56fe\u50cf\u548c\u97f3\u9891\uff09\u7684\u5b9e\u65f6\u89c6\u9891\u6269\u6563\u3002</li>\n    <li>\u6211\u4eec\u7684\u6a21\u578b\u5728\u591a\u6a21\u6001\u6761\u4ef6\u4e0b\u751f\u6210\u7684\u5934\u50cf\u89c6\u9891\u8d28\u91cf\u4e0e\u5168\u6b65\u9aa4\u57fa\u7ebf\u76f8\u5f53\uff0c\u4f46\u63a8\u7406\u6210\u672c\u548c\u5ef6\u8fdf\u964d\u4f4e\u4e8620\u500d\u3002</li>\n    <li>\u901a\u8fc7\u4e0e\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u548c\u957f\u89c6\u9891\u63a8\u7406\u6280\u672f\u7ed3\u5408\uff0c\u6784\u5efa\u4e86\u5b9e\u65f6\u4ea4\u4e92\u7684LiveTalk\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u56de\u5408\u4ea4\u4e92\u7684\u4e00\u81f4\u6027\u548c\u5185\u5bb9\u8d28\u91cf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>This paper focuses on improving real-time video generation for interactive AI systems using a method called diffusion.</li>\n    <li>Current models struggle with real-time interaction due to complicated processes that affect performance, especially in multimodal contexts like text, images, and audio.</li>\n    <li>The authors propose an enhanced distillation method to improve video quality and reduce issues like flickering and black frames during video generation.</li>\n    <li>The resulting model performs comparably to more complex models but is significantly faster and more efficient, making it suitable for real-time use.</li>\n    <li>They developed a system called LiveTalk that integrates this model for seamless human-AI interactions, outperforming existing models in video coherence and reducing response times dramatically.</li>\n</ul>"}, "publishedAt": "2025-12-29T11:17:36.000Z", "title": "LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation", "summary": "Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64bb5f9d8e051085bace4d1e/skNa_3Ly0Bg7F6aL0mk92.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23576.png", "numComments": 1, "submittedBy": {"_id": "64bb5f9d8e051085bace4d1e", "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg", "fullname": "Ethan Chern", "name": "ethanchern", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22096", "authors": [{"_id": "695206a8746a34b55dd548dd", "name": "Xiaofeng Mao", "hidden": false}, {"_id": "695206a8746a34b55dd548de", "name": "Zhen Li", "hidden": false}, {"_id": "695206a8746a34b55dd548df", "name": "Chuanhao Li", "hidden": false}, {"_id": "695206a8746a34b55dd548e0", "name": "Xiaojie Xu", "hidden": false}, {"_id": "695206a8746a34b55dd548e1", "name": "Kaining Ying", "hidden": false}, {"_id": "695206a8746a34b55dd548e2", "name": "Tong He", "hidden": false}, {"_id": "695206a8746a34b55dd548e3", "name": "Jiangmiao Pang", "hidden": false}, {"_id": "695206a8746a34b55dd548e4", "name": "Yu Qiao", "hidden": false}, {"_id": "695206a8746a34b55dd548e5", "name": "Kaipeng Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/NMttpBTOZdYJorkqyoD67.mp4"], "publishedAt": "2025-12-26T17:52:49.000Z", "submittedOnDailyAt": "2025-12-30T01:50:23.447Z", "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model", "submittedOnDailyBy": {"_id": "65f1713552c38a91e0a445e8", "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg", "isPro": false, "fullname": "kaipeng", "user": "kpzhang996", "type": "user"}, "summary": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.", "upvotes": 50, "discussionId": "695206a8746a34b55dd548e6", "projectPage": "https://stdstu12.github.io/YUME-Project/", "githubRepo": "https://github.com/stdstu12/YUME", "githubRepoAddedBy": "user", "githubStars": 426, "summary_zh": "<ul>\n  <li>\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\uff0c\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u7528\u4e8e\u751f\u6210\u4e92\u52a8\u548c\u53ef\u63a2\u7d22\u7684\u865a\u62df\u4e16\u754c\u3002</li>\n  <li>\u5f53\u524d\u65b9\u6cd5\u5b58\u5728\u53c2\u6570\u8fc7\u5927\u3001\u63a8\u7406\u65f6\u95f4\u957f\u548c\u5386\u53f2\u4e0a\u4e0b\u6587\u8fc5\u901f\u589e\u52a0\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u6027\u80fd\u548c\u6587\u672c\u63a7\u5236\u751f\u6210\u80fd\u529b\u3002</li>\n  <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6 \\method\uff0c\u53ef\u4ee5\u4ece\u5355\u5f20\u56fe\u7247\u6216\u6587\u672c\u63d0\u793a\u751f\u6210\u771f\u5b9e\u7684\u4e92\u52a8\u548c\u8fde\u7eed\u7684\u4e16\u754c\u3002</li>\n  <li>\\method \u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u957f\u89c6\u9891\u751f\u6210\u6846\u67b6\u3001\u5b9e\u65f6\u6d41\u52a0\u901f\u7b56\u7565\u548c\u6587\u672c\u63a7\u5236\u7684\u4e16\u754c\u4e8b\u4ef6\u751f\u6210\u65b9\u6cd5\u3002</li>\n  <li>\u4ee3\u7801\u5e93\u5df2\u5305\u542b\u5728\u8865\u5145\u6750\u6599\u4e2d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New methods using diffusion models can create interactive worlds, but they have problems like being too large and slow.</li>\n    <li>We introduce a new framework called \\method to generate realistic worlds from just one image or text prompt.</li>\n    <li>\\method allows users to explore these worlds using keyboard controls.</li>\n    <li>It includes three main parts: a video generation system, a fast streaming strategy, and a way to control events with text.</li>\n    <li>The code for this framework is available in the supplementary material.</li>\n</ul>"}, "publishedAt": "2025-12-26T12:52:49.000Z", "title": "Yume-1.5: A Text-Controlled Interactive World Generation Model", "summary": "Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \\method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \\method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/NMttpBTOZdYJorkqyoD67.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22096.png", "numComments": 1, "submittedBy": {"_id": "65f1713552c38a91e0a445e8", "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg", "fullname": "kaipeng", "name": "kpzhang996", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24618", "authors": [{"_id": "6955d543832867f25352555d", "name": "Junru Lu", "hidden": false}, {"_id": "6955d543832867f25352555e", "name": "Jiarui Qin", "hidden": false}, {"_id": "6955d543832867f25352555f", "name": "Lingfeng Qiao", "hidden": false}, {"_id": "6955d543832867f253525560", "name": "Yinghui Li", "hidden": false}, {"_id": "6955d543832867f253525561", "name": "Xinyi Dai", "hidden": false}, {"_id": "6955d543832867f253525562", "name": "Bo Ke", "hidden": false}, {"_id": "6955d543832867f253525563", "name": "Jianfeng He", "hidden": false}, {"_id": "6955d543832867f253525564", "name": "Ruizhi Qiao", "hidden": false}, {"_id": "6955d543832867f253525565", "name": "Di Yin", "hidden": false}, {"_id": "6955d543832867f253525566", "name": "Xing Sun", "hidden": false}, {"_id": "6955d543832867f253525567", "name": "Yunsheng Wu", "hidden": false}, {"_id": "6955d543832867f253525568", "name": "Yinsong Liu", "hidden": false}, {"_id": "6955d543832867f253525569", "name": "Shuangyin Liu", "hidden": false}, {"_id": "6955d543832867f25352556a", "name": "Mingkong Tang", "hidden": false}, {"_id": "6955d543832867f25352556b", "name": "Haodong Lin", "hidden": false}, {"_id": "6955d543832867f25352556c", "name": "Jiayi Kuang", "hidden": false}, {"_id": "6955d543832867f25352556d", "name": "Fanxu Meng", "hidden": false}, {"_id": "6955d543832867f25352556e", "name": "Xiaojuan Tang", "hidden": false}, {"_id": "6955d543832867f25352556f", "name": "Yunjia Xi", "hidden": false}, {"_id": "6955d543832867f253525570", "name": "Junjie Huang", "hidden": false}, {"_id": "6955d543832867f253525571", "name": "Haotong Yang", "hidden": false}, {"_id": "6955d543832867f253525572", "name": "Zhenyi Shen", "hidden": false}, {"_id": "6955d543832867f253525573", "name": "Yangning Li", "hidden": false}, {"_id": "6955d543832867f253525574", "name": "Qianwen Zhang", "hidden": false}, {"_id": "6955d543832867f253525575", "name": "Yifei Yu", "hidden": false}, {"_id": "6955d543832867f253525576", "name": "Siyu An", "hidden": false}, {"_id": "6955d543832867f253525577", "name": "Junnan Dong", "hidden": false}, {"_id": "6955d543832867f253525578", "name": "Qiufeng Wang", "hidden": false}, {"_id": "6955d543832867f253525579", "name": "Jie Wang", "hidden": false}, {"_id": "6955d543832867f25352557a", "name": "Keyu Chen", "hidden": false}, {"_id": "6955d543832867f25352557b", "name": "Wei Wen", "hidden": false}, {"_id": "6955d543832867f25352557c", "name": "Taian Guo", "hidden": false}, {"_id": "6955d543832867f25352557d", "name": "Zhifeng Shen", "hidden": false}, {"_id": "6955d543832867f25352557e", "name": "Daohai Yu", "hidden": false}, {"_id": "6955d543832867f25352557f", "name": "Jiahao Li", "hidden": false}, {"_id": "6955d543832867f253525580", "name": "Ke Li", "hidden": false}, {"_id": "6955d543832867f253525581", "name": "Zongyi Li", "hidden": false}, {"_id": "6955d543832867f253525582", "name": "Xiaoyu Tan", "hidden": false}], "publishedAt": "2025-12-31T04:25:11.000Z", "submittedOnDailyAt": "2026-01-01T00:33:09.720Z", "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "upvotes": 43, "discussionId": "6955d543832867f253525583", "ai_summary": "Youtu-LLM is a lightweight language model optimized for computational efficiency and agentic intelligence through a compact architecture, STEM-focused training curriculum, and scalable mid-training strategies for planning and reasoning tasks.", "ai_keywords": ["Multi-Latent Attention (MLA) architecture", "STEM-oriented vocabulary", "128k context window", "Commonsense-STEM-Agent Curriculum", "multi-stage training strategy", "agentic mid-training", "data construction schemes", "planning and reflection behaviors", "long-context reasoning", "state tracking", "agentic capabilities"], "summary_zh": "<ul>\n    <li>Youtu-LLM\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4f46\u5f3a\u5927\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u6709\u9ad8\u6548\u7387\u548c\u667a\u80fd\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4ece\u96f6\u5f00\u59cb\u9884\u8bad\u7ec3\uff0c\u4e13\u6ce8\u4e8e\u63a8\u7406\u548c\u89c4\u5212\u80fd\u529b\u7684\u57f9\u517b\u3002</li>\n    <li>Youtu-LLM\u652f\u6301128k\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u9002\u5408\u957f\u65f6\u95f4\u7684\u63a8\u7406\u548c\u72b6\u6001\u8ddf\u8e2a\u3002</li>\n    <li>\u91c7\u7528\u4e86\u9010\u6b65\u8bad\u7ec3\u7b56\u7565\uff0c\u4ece\u4e00\u822c\u5e38\u8bc6\u5230\u590d\u6742\u7684STEM\u548c\u667a\u80fd\u4efb\u52a1\uff0c\u786e\u4fdd\u6a21\u578b\u5177\u5907\u6df1\u539a\u7684\u8ba4\u77e5\u80fd\u529b\u3002</li>\n    <li>\u5728\u8bc4\u4f30\u4e2d\uff0cYoutu-LLM\u5728\u5c0f\u4e8e2B\u53c2\u6570\u7684\u6a21\u578b\u4e2d\u8bbe\u7f6e\u4e86\u65b0\u7684\u6027\u80fd\u6807\u51c6\uff0c\u5c24\u5176\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Youtu-LLM is a new lightweight language model designed for efficient computation and smart decision-making.</li>\n    <li>It features a unique architecture that can handle long contexts up to 128,000 tokens, making it great for complex reasoning tasks.</li>\n    <li>The model is trained on a large dataset of 11 trillion tokens and follows a structured learning approach to develop strong cognitive skills.</li>\n    <li>It uses diverse training methods to teach the model planning and reflection skills across various subjects like math and coding.</li>\n    <li>Youtu-LLM outperforms other small models and competes well with larger ones in general tasks, especially excelling in specific agent-related tasks.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:25:11.000Z", "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models", "summary": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24618.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 197}, "isAuthorParticipating": false}, {"paper": {"id": "2512.24873", "authors": [{"_id": "6955e3f8832867f2535255cd", "name": "Weixun Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255ce", "name": "XiaoXiao Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255cf", "name": "Wanhe An", "hidden": false}, {"_id": "6955e3f8832867f2535255d0", "name": "Fangwen Dai", "hidden": false}, {"_id": "6955e3f8832867f2535255d1", "name": "Wei Gao", "hidden": false}, {"_id": "6955e3f8832867f2535255d2", "name": "Yancheng He", "hidden": false}, {"_id": "6955e3f8832867f2535255d3", "name": "Ju Huang", "hidden": false}, {"_id": "6955e3f8832867f2535255d4", "name": "Qiang Ji", "hidden": false}, {"_id": "6955e3f8832867f2535255d5", "name": "Hanqi Jin", "hidden": false}, {"_id": "6955e3f8832867f2535255d6", "name": "Xiaoyang Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d7", "name": "Yang Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d8", "name": "Zhongwen Li", "hidden": false}, {"_id": "6955e3f8832867f2535255d9", "name": "Shirong Lin", "hidden": false}, {"_id": "6955e3f8832867f2535255da", "name": "Jiashun Liu", "hidden": false}, {"_id": "6955e3f8832867f2535255db", "name": "Zenan Liu", "hidden": false}, {"_id": "6955e3f8832867f2535255dc", "name": "Tao Luo", "hidden": false}, {"_id": "6955e3f8832867f2535255dd", "name": "Dilxat Muhtar", "hidden": false}, {"_id": "6955e3f8832867f2535255de", "name": "Yuanbin Qu", "hidden": false}, {"_id": "6955e3f8832867f2535255df", "name": "Jiaqiang Shi", "hidden": false}, {"_id": "6955e3f8832867f2535255e0", "name": "Qinghui Sun", "hidden": false}, {"_id": "6955e3f8832867f2535255e1", "name": "Yingshui Tan", "hidden": false}, {"_id": "6955e3f8832867f2535255e2", "name": "Hao Tang", "hidden": false}, {"_id": "6955e3f8832867f2535255e3", "name": "Runze Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e4", "name": "Yi Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e5", "name": "Zhaoguo Wang", "hidden": false}, {"_id": "6955e3f8832867f2535255e6", "name": "Yanan Wu", "hidden": false}, {"_id": "6955e3f8832867f2535255e7", "name": "Shaopan Xiong", "hidden": false}, {"_id": "6955e3f8832867f2535255e8", "name": "Binchen Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255e9", "name": "Xander Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255ea", "name": "Yuchi Xu", "hidden": false}, {"_id": "6955e3f8832867f2535255eb", "name": "Qipeng Zhang", "hidden": false}, {"_id": "6955e3f8832867f2535255ec", "name": "Xixia Zhang", "hidden": false}, {"_id": "6955e3f8832867f2535255ed", "name": "Haizhou Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255ee", "name": "Jie Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255ef", "name": "Shuaibing Zhao", "hidden": false}, {"_id": "6955e3f8832867f2535255f0", "name": "Baihui Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f1", "name": "Jianhui Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f2", "name": "Suhang Zheng", "hidden": false}, {"_id": "6955e3f8832867f2535255f3", "name": "Yanni Zhu", "hidden": false}, {"_id": "6955e3f8832867f2535255f4", "name": "Mengze Cai", "hidden": false}, {"_id": "6955e3f8832867f2535255f5", "name": "Kerui Cao", "hidden": false}, {"_id": "6955e3f8832867f2535255f6", "name": "Xitong Chen", "hidden": false}, {"_id": "6955e3f8832867f2535255f7", "name": "Yue Dai", "hidden": false}, {"_id": "6955e3f8832867f2535255f8", "name": "Lifan Du", "hidden": false}, {"_id": "6955e3f8832867f2535255f9", "name": "Tao Feng", "hidden": false}, {"_id": "6955e3f8832867f2535255fa", "name": "Tao He", "hidden": false}, {"_id": "6955e3f8832867f2535255fb", "name": "Jin Hu", "hidden": false}, {"_id": "6955e3f8832867f2535255fc", "name": "Yijie Hu", "hidden": false}, {"_id": "6955e3f8832867f2535255fd", "name": "Ziyu Jiang", "hidden": false}, {"_id": "6955e3f8832867f2535255fe", "name": "Cheng Li", "hidden": false}, {"_id": "6955e3f8832867f2535255ff", "name": "Xiang Li", "hidden": false}, {"_id": "6955e3f8832867f253525600", "name": "Jing Liang", "hidden": false}, {"_id": "6955e3f8832867f253525601", "name": "Chonghuan Liu", "hidden": false}, {"_id": "6955e3f8832867f253525602", "name": "ZhenDong Liu", "hidden": false}, {"_id": "6955e3f8832867f253525603", "name": "Haodong Mi", "hidden": false}, {"_id": "6955e3f8832867f253525604", "name": "Yanhu Mo", "hidden": false}, {"_id": "6955e3f8832867f253525605", "name": "Junjia Ni", "hidden": false}, {"_id": "6955e3f8832867f253525606", "name": "Shixin Pei", "hidden": false}, {"_id": "6955e3f8832867f253525607", "name": "Jingyu Shen", "hidden": false}, {"_id": "6955e3f8832867f253525608", "name": "XiaoShuai Song", "hidden": false}, {"_id": "6955e3f8832867f253525609", "name": "Cecilia Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560a", "name": "Chaofan Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560b", "name": "Kangyu Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560c", "name": "Pei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560d", "name": "Tao Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560e", "name": "Wei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352560f", "name": "Ke Xiao", "hidden": false}, {"_id": "6955e3f8832867f253525610", "name": "Mingyu Xu", "hidden": false}, {"_id": "6955e3f8832867f253525611", "name": "Tiange Xu", "hidden": false}, {"_id": "6955e3f8832867f253525612", "name": "Nan Ya", "hidden": false}, {"_id": "6955e3f8832867f253525613", "name": "Siran Yang", "hidden": false}, {"_id": "6955e3f8832867f253525614", "name": "Jianan Ye", "hidden": false}, {"_id": "6955e3f8832867f253525615", "name": "Yaxing Zang", "hidden": false}, {"_id": "6955e3f8832867f253525616", "name": "Duo Zhang", "hidden": false}, {"_id": "6955e3f8832867f253525617", "name": "Junbo Zhang", "hidden": false}, {"_id": "6955e3f8832867f253525618", "name": "Boren Zheng", "hidden": false}, {"_id": "6955e3f8832867f253525619", "name": "Wanxi Deng", "hidden": false}, {"_id": "6955e3f8832867f25352561a", "name": "Ling Pan", "hidden": false}, {"_id": "6955e3f8832867f25352561b", "name": "Lin Qu", "hidden": false}, {"_id": "6955e3f8832867f25352561c", "name": "Wenbo Su", "hidden": false}, {"_id": "6955e3f8832867f25352561d", "name": "Jiamang Wang", "hidden": false}, {"_id": "6955e3f8832867f25352561e", "name": "Wei Wang", "hidden": false}, {"_id": "6955e3f8832867f25352561f", "name": "Hu Wei", "hidden": false}, {"_id": "6955e3f8832867f253525620", "name": "Minggang Wu", "hidden": false}, {"_id": "6955e3f8832867f253525621", "name": "Cheng Yu", "hidden": false}, {"_id": "6955e3f8832867f253525622", "name": "Bing Zhao", "hidden": false}, {"_id": "6955e3f8832867f253525623", "name": "Zhicheng Zheng", "hidden": false}, {"_id": "6955e3f8832867f253525624", "name": "Bo Zheng", "hidden": false}], "publishedAt": "2025-12-31T14:03:39.000Z", "submittedOnDailyAt": "2026-01-01T00:33:23.374Z", "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.", "upvotes": 33, "discussionId": "6955e3f8832867f253525625", "ai_summary": "The Agentic Learning Ecosystem (ALE) introduces a principled infrastructure for agent development, combining post-training optimization, sandbox environments, and policy alignment to enhance long-horizon training stability and performance in real-world tasks.", "ai_keywords": ["ROLL (post-training framework)", "ROCK (sandbox environment manager)", "iFlow CLI (agent framework)", "ROME (agentic model)", "data composition protocols", "Interaction-based Policy Alignment (IPA)", "semantic interaction chunks", "Terminal Bench Pro", "SWE-bench Verified"], "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u201cAgentic Learning Ecosystem (ALE)\u201d\uff0c\u65e8\u5728\u4f18\u5316\u4ee3\u7406\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5f00\u53d1\u6d41\u7a0b\u3002</li>\n    <li>ALE\u5305\u542b\u4e09\u4e2a\u4e3b\u8981\u7ec4\u4ef6\uff1aROLL\uff08\u6743\u91cd\u4f18\u5316\u6846\u67b6\uff09\u3001ROCK\uff08\u8f68\u8ff9\u751f\u6210\u7ba1\u7406\u5de5\u5177\uff09\u548ciFlow CLI\uff08\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u6846\u67b6\uff09\u3002</li>\n    <li>\u63a8\u51fa\u4e86\u5f00\u6e90\u4ee3\u7406\u6a21\u578bROME\uff0c\u57fa\u4e8eALE\u8bad\u7ec3\uff0c\u4f7f\u7528\u4e86\u8d85\u8fc7\u4e00\u767e\u4e07\u4e2a\u8f68\u8ff9\u3002</li>\n    <li>\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u201c\u57fa\u4e8e\u4ea4\u4e92\u7684\u7b56\u7565\u5bf9\u9f50\uff08IPA\uff09\u201d\uff0c\u4ee5\u63d0\u9ad8\u957f\u671f\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u901a\u8fc7\u7ed3\u6784\u5316\u8bc4\u4f30\u548c\u201cTerminal Bench Pro\u201d\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86ROME\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u4f18\u5f02\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Agentic crafting allows LLMs to work in real-life situations by learning from their actions over time.</li>\n    <li>The open-source community lacks a complete system for developing these agent LLMs, which is why we created the Agentic Learning Ecosystem (ALE).</li>\n    <li>ALE has three main parts: ROLL for optimizing model weights, ROCK for creating test environments, and iFlow CLI for managing agent context.</li>\n    <li>We launched ROME, an open-source agent model trained on a large amount of data, using ALE to improve its functionality.</li>\n    <li>ROME shows strong results in various tests, confirming that ALE is effective for developing agent-based models.</li>\n</ul>"}, "publishedAt": "2025-12-31T09:03:39.000Z", "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem", "summary": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24873.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 197}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22322", "authors": [{"_id": "69533fb889916ff627aa3ecb", "name": "Shaofei Cai", "hidden": false}, {"_id": "69533fb889916ff627aa3ecc", "name": "Yulei Qin", "hidden": false}, {"_id": "69533fb889916ff627aa3ecd", "name": "Haojia Lin", "hidden": false}, {"_id": "69533fb889916ff627aa3ece", "name": "Zihan Xu", "hidden": false}, {"_id": "69533fb889916ff627aa3ecf", "name": "Gang Li", "hidden": false}, {"_id": "69533fb889916ff627aa3ed0", "name": "Yuchen Shi", "hidden": false}, {"_id": "69533fb889916ff627aa3ed1", "name": "Zongyi Li", "hidden": false}, {"_id": "69533fb889916ff627aa3ed2", "name": "Yong Mao", "hidden": false}, {"_id": "69533fb889916ff627aa3ed3", "name": "Siqi Cai", "hidden": false}, {"_id": "69533fb889916ff627aa3ed4", "name": "Xiaoyu Tan", "hidden": false}, {"_id": "69533fb889916ff627aa3ed5", "name": "Yitao Liang", "hidden": false}, {"_id": "69533fb889916ff627aa3ed6", "name": "Ke Li", "hidden": false}, {"_id": "69533fb889916ff627aa3ed7", "name": "Xing Sun", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6390525c00fb8ec4a424e0ff/h0k49_chVHOUTywBgnJR6.gif"], "publishedAt": "2025-12-26T14:51:39.000Z", "submittedOnDailyAt": "2025-12-30T01:07:21.942Z", "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents", "submittedOnDailyBy": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "isPro": false, "fullname": "Yulei Qin", "user": "yolay", "type": "user"}, "summary": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.", "upvotes": 33, "discussionId": "69533fb889916ff627aa3ed8", "projectPage": "https://huggingface.co/collections/yolay/smartsnap", "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u56fe\u5f62\u7528\u6237\u754c\u9762\u4efb\u52a1\u4e2d\u6709\u5f88\u5927\u524d\u666f\uff0c\u4f46\u4efb\u52a1\u5b8c\u6210\u7684\u9a8c\u8bc1\u5b58\u5728\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002</li>\n    <li>\u73b0\u6709\u7684\u4efb\u52a1\u9a8c\u8bc1\u901a\u5e38\u662f\u4e8b\u540e\u5904\u7406\uff0c\u4f9d\u8d56\u4e8e\u5206\u6790\u4ee3\u7406\u7684\u6574\u4e2a\u4ea4\u4e92\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u6210\u672c\u9ad8\u548c\u53ef\u9760\u6027\u4f4e\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86SmartSnap\uff0c\u8fd9\u662f\u4e00\u79cd\u4e3b\u52a8\u7684\u81ea\u6211\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u4ee3\u7406\u53ef\u4ee5\u5728\u6267\u884c\u4efb\u52a1\u65f6\u5373\u65f6\u9a8c\u8bc1\u81ea\u5df1\u7684\u5b8c\u6210\u60c5\u51b5\u3002</li>\n    <li>\u81ea\u6211\u9a8c\u8bc1\u4ee3\u7406\u4e0d\u4ec5\u8981\u5b8c\u6210\u4efb\u52a1\uff0c\u8fd8\u8981\u63d0\u4f9b\u7ecf\u8fc7\u7cbe\u5fc3\u51c6\u5907\u7684\u5feb\u7167\u8bc1\u636e\u6765\u8bc1\u660e\u5176\u5b8c\u6210\u60c5\u51b5\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSmartSnap\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f7f\u5f97\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u4e2d\u8bad\u7ec3\u53d8\u5f97\u66f4\u52a0\u53ef\u6269\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Agentic reinforcement learning (RL) is promising for creating autonomous agents for complex GUI tasks, but checking if tasks are completed is a big challenge.</li>\n    <li>Current verification methods analyze the entire interaction after it happens, which can be inefficient and unreliable due to irrelevant information.</li>\n    <li>The proposed SmartSnap changes this by allowing agents to verify their own task completion in real-time using selected evidence snapshots.</li>\n    <li>The Self-Verifying Agent uses the 3C Principles (Completeness, Conciseness, and Creativity) to collect important snapshots for verification.</li>\n    <li>Tests show that this new approach improves the performance of agents significantly, making them more efficient and competitive with existing models.</li>\n</ul>"}, "publishedAt": "2025-12-26T09:51:39.000Z", "title": "SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents", "summary": "Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6390525c00fb8ec4a424e0ff/h0k49_chVHOUTywBgnJR6.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22322.png", "numComments": 2, "submittedBy": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "fullname": "Yulei Qin", "name": "yolay", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23705", "authors": [{"_id": "6953546989916ff627aa4002", "name": "Shaocong Xu", "hidden": false}, {"_id": "6953546989916ff627aa4003", "name": "Songlin Wei", "hidden": false}, {"_id": "6953546989916ff627aa4004", "name": "Qizhe Wei", "hidden": false}, {"_id": "6953546989916ff627aa4005", "name": "Zheng Geng", "hidden": false}, {"_id": "6953546989916ff627aa4006", "name": "Hong Li", "hidden": false}, {"_id": "6953546989916ff627aa4007", "name": "Licheng Shen", "hidden": false}, {"_id": "6953546989916ff627aa4008", "name": "Qianpu Sun", "hidden": false}, {"_id": "6953546989916ff627aa4009", "name": "Shu Han", "hidden": false}, {"_id": "6953546989916ff627aa400a", "name": "Bin Ma", "hidden": false}, {"_id": "6953546989916ff627aa400b", "name": "Bohan Li", "hidden": false}, {"_id": "6953546989916ff627aa400c", "name": "Chongjie Ye", "hidden": false}, {"_id": "6953546989916ff627aa400d", "name": "Yuhang Zheng", "hidden": false}, {"_id": "6953546989916ff627aa400e", "name": "Nan Wang", "hidden": false}, {"_id": "6953546989916ff627aa400f", "name": "Saining Zhang", "hidden": false}, {"_id": "6953546989916ff627aa4010", "name": "Hao Zhao", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/652bd2493a416e1f21beb01a/6NM5vLS_B3DlYtmX1N4A_.gif"], "publishedAt": "2025-12-29T18:59:24.000Z", "submittedOnDailyAt": "2025-12-30T01:56:18.708Z", "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation", "submittedOnDailyBy": {"_id": "652bd2493a416e1f21beb01a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652bd2493a416e1f21beb01a/tKijq1pbjmBZuRm82dNEV.jpeg", "isPro": true, "fullname": "Shaocong.Xu", "user": "Daniellesry", "type": "user"}, "summary": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.", "upvotes": 32, "discussionId": "6953546a89916ff627aa4011", "projectPage": "https://daniellli.github.io/projects/DKT/", "githubRepo": "https://github.com/Daniellli/DKT", "githubRepoAddedBy": "user", "githubStars": 94, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "summary_zh": "<ul>\n    <li>\u900f\u660e\u7269\u4f53\u5bf9\u611f\u77e5\u7cfb\u7edf\u6765\u8bf4\u5f88\u96be\u5904\u7406\uff0c\u56e0\u4e3a\u6298\u5c04\u3001\u53cd\u5c04\u548c\u900f\u5c04\u4f1a\u7834\u574f\u6df1\u5ea6\u4f30\u8ba1\u7684\u5047\u8bbe\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86TransPhy3D\uff0c\u4e00\u4e2a\u5305\u542b\u900f\u660e\u548c\u53cd\u5c04\u573a\u666f\u7684\u5408\u6210\u89c6\u9891\u6570\u636e\u96c6\uff0c\u5171\u670911000\u4e2a\u5e8f\u5217\u3002</li>\n    <li>\u4f7f\u7528Blender/Cycles\u6e32\u67d3\uff0c\u751f\u6210RGB\u3001\u6df1\u5ea6\u548c\u6cd5\u7ebf\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u7269\u7406\u57fa\u7840\u7684\u5149\u7ebf\u8ffd\u8e2a\u548cOptiX\u53bb\u566a\u5904\u7406\u3002</li>\n    <li>\u6211\u4eec\u8bad\u7ec3\u7684\u6a21\u578bDKT\u5728\u900f\u660e\u5ea6\u76f8\u5173\u7684\u771f\u5b9e\u548c\u5408\u6210\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u51fa\u8272\u7684\u96f6-shot\u6548\u679c\uff0c\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002</li>\n    <li>DKT\u7684\u6df1\u5ea6\u4f30\u8ba1\u5728\u5904\u7406\u534a\u900f\u660e\u3001\u53cd\u5c04\u548c\u6563\u5c04\u8868\u9762\u65f6\uff0c\u6210\u529f\u7387\u9ad8\u4e8e\u4e4b\u524d\u7684\u4f30\u8ba1\u5668\uff0c\u5c55\u793a\u4e86\u751f\u6210\u89c6\u9891\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Transparent objects are difficult for perception systems because of issues with refraction, reflection, and transmission.</li>\n    <li>The authors created TransPhy3D, a video collection of 11,000 scenes featuring transparent and reflective materials using Blender/Cycles.</li>\n    <li>They developed a model called DKT that can predict depth and normals from videos, showing better accuracy and stability than previous methods.</li>\n    <li>DKT achieved state-of-the-art results on several benchmarks for videos involving transparency and improved success rates in robotic grasping tasks.</li>\n    <li>The findings suggest that generative video models can effectively handle complex perceptions in real-world settings without needing extensive labels.</li>\n</ul>"}, "publishedAt": "2025-12-29T13:59:24.000Z", "title": "Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation", "summary": "Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: \"Diffusion knows transparency.\" Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/652bd2493a416e1f21beb01a/6NM5vLS_B3DlYtmX1N4A_.gif"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23705.png", "numComments": 1, "submittedBy": {"_id": "652bd2493a416e1f21beb01a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652bd2493a416e1f21beb01a/tKijq1pbjmBZuRm82dNEV.jpeg", "fullname": "Shaocong.Xu", "name": "Daniellesry", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "61be9739d2f9358e24ca0a4f", "name": "BAAI", "fullname": "Beijing Academy of Artificial Intelligence", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1664511063789-632c234f42c386ebd2710434.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23709", "authors": [{"_id": "69537f4189916ff627aa40c0", "name": "Hau-Shiang Shiu", "hidden": false}, {"_id": "69537f4189916ff627aa40c1", "name": "Chin-Yang Lin", "hidden": false}, {"_id": "69537f4189916ff627aa40c2", "name": "Zhixiang Wang", "hidden": false}, {"_id": "69537f4189916ff627aa40c3", "name": "Chi-Wei Hsiao", "hidden": false}, {"_id": "69537f4189916ff627aa40c4", "name": "Po-Fan Yu", "hidden": false}, {"_id": "69537f4189916ff627aa40c5", "name": "Yu-Chih Chen", "hidden": false}, {"_id": "69537f4189916ff627aa40c6", "name": "Yu-Lun Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/4Urho_F4h3YB3NjJxTgCc.mp4"], "publishedAt": "2025-12-29T18:59:57.000Z", "submittedOnDailyAt": "2025-12-30T05:04:09.292Z", "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion", "submittedOnDailyBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "isPro": false, "fullname": "Yu-Lun Liu", "user": "yulunliu", "type": "user"}, "summary": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/", "upvotes": 29, "discussionId": "69537f4289916ff627aa40c7", "projectPage": "https://jamichss.github.io/stream-diffvsr-project-page/", "summary_zh": "<ul>\n    <li>Stream-DiffVSR \u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u4e13\u4e3a\u9ad8\u6548\u7684\u89c6\u9891\u8d85\u5206\u8fa8\u7387\uff08VSR\uff09\u8bbe\u8ba1\uff0c\u9002\u7528\u4e8e\u4f4e\u5ef6\u8fdf\u573a\u666f\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u53ea\u4f7f\u7528\u8fc7\u53bb\u7684\u5e27\uff0c\u7ed3\u5408\u5feb\u901f\u63a8\u7406\u7684\u56db\u6b65\u53bb\u566a\u5668\u548c\u8fd0\u52a8\u5bf9\u9f50\u7684\u63d0\u793a\u6a21\u5757\uff0c\u63d0\u9ad8\u4e86\u5904\u7406\u901f\u5ea6\u3002</li>\n    <li>Stream-DiffVSR \u5728 RTX4090 GPU \u4e0a\u5904\u7406 720p \u5e27\u7684\u65f6\u95f4\u4e3a 0.328 \u79d2\uff0c\u663e\u8457\u4f18\u4e8e\u4ee5\u524d\u7684\u6269\u6563\u65b9\u6cd5\u3002</li>\n    <li>\u4e0e\u73b0\u6709\u7684\u5728\u7ebf\u9876\u5c16\u6280\u672f\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u611f\u77e5\u8d28\u91cf\uff0c\u5e76\u5c06\u5ef6\u8fdf\u51cf\u5c11\u4e86\u8d85\u8fc7 130 \u500d\u3002</li>\n    <li>Stream-DiffVSR \u5b9e\u73b0\u4e86\u6269\u6563\u65b9\u6cd5\u4e2d\u6700\u4f4e\u7684\u5ef6\u8fdf\uff0c\u4f7f\u5176\u6210\u4e3a\u9002\u5408\u5728\u7ebf\u5e94\u7528\u7684\u9996\u4e2a\u6269\u6563 VSR \u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Stream-DiffVSR is a new video super-resolution method that works quickly and efficiently.</li>\n    <li>It only uses past video frames, making it suitable for real-time applications.</li>\n    <li>The method includes a fast four-step denoiser and a module that helps with motion alignment.</li>\n    <li>It can process 720p video frames in just 0.328 seconds on a powerful GPU, outperforming previous methods.</li>\n    <li>This approach significantly reduces latency, making it the first diffusion-based video super-resolution method usable for low-latency online tasks.</li>\n</ul>"}, "publishedAt": "2025-12-29T13:59:57.000Z", "title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion", "summary": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/4Urho_F4h3YB3NjJxTgCc.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23709.png", "numComments": 1, "submittedBy": {"_id": "6459d5da3b6fafd9664807ab", "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg", "fullname": "Yu-Lun Liu", "name": "yulunliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6}, "isAuthorParticipating": false}, {"paper": {"id": "2512.22615", "authors": [{"_id": "6953489889916ff627aa3f25", "name": "Jiacheng Ye", "hidden": false}, {"_id": "6953489889916ff627aa3f26", "name": "Shansan Gong", "hidden": false}, {"_id": "6953489889916ff627aa3f27", "name": "Jiahui Gao", "hidden": false}, {"_id": "6953489889916ff627aa3f28", "name": "Junming Fan", "hidden": false}, {"_id": "6953489889916ff627aa3f29", "name": "Shuang Wu", "hidden": false}, {"_id": "6953489889916ff627aa3f2a", "name": "Wei Bi", "hidden": false}, {"_id": "6953489889916ff627aa3f2b", "name": "Haoli Bai", "hidden": false}, {"_id": "6953489889916ff627aa3f2c", "name": "Lifeng Shang", "hidden": false}, {"_id": "6953489889916ff627aa3f2d", "name": "Lingpeng Kong", "hidden": false}], "publishedAt": "2025-12-27T14:46:24.000Z", "submittedOnDailyAt": "2025-12-30T03:42:33.237Z", "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone", "submittedOnDailyBy": {"_id": "628c83d186fc004b14e1ed48", "avatarUrl": "/avatars/05ff943a9b89b5f67c5bc254bf45b8f5.svg", "isPro": false, "fullname": "Shansan Gong", "user": "Sansa", "type": "user"}, "summary": "While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as \u03c0_0 and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.", "upvotes": 27, "discussionId": "6953489889916ff627aa3f2e", "projectPage": "https://hkunlp.github.io/blog/2025/dream-vlx/", "githubRepo": "https://github.com/DreamLM/Dream-VLX", "githubRepoAddedBy": "user", "ai_summary": "Diffusion-based vision-language models and action frameworks demonstrate superior performance in visual planning and robotic control tasks compared to autoregressive baselines.", "ai_keywords": ["diffusion-based large language models (dLLMs)", "Vision-Language Models (VLMs)", "Dream-VL", "Vision-Language-Action model (dVLA)", "Dream-VLA", "action chunking", "parallel generation", "LIBERO", "SimplerEnv-Bridge", "SimplerEnv-Fractal", "continuous pre-training"], "githubStars": 40, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "summary_zh": "<ul>\n    <li>\u81ea\u56de\u5f52\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u7684\u89c6\u89c9\u89c4\u5212\u548c\u52a8\u6001\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u3002</li>\n    <li>\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578bDream-VL\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u6a21\u578b\uff0c\u7279\u522b\u5728\u89c6\u89c9\u89c4\u5212\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002</li>\n    <li>Dream-VLA\u662f\u57fa\u4e8eDream-VL\u53d1\u5c55\u7684\u4e00\u4e2a\u6a21\u578b\uff0c\u7ecf\u8fc7\u5728\u5f00\u653e\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e0a\u7684\u8fde\u7eed\u9884\u8bad\u7ec3\uff0c\u9002\u5408\u4e8e\u52a8\u4f5c\u5206\u5757\u548c\u5e76\u884c\u751f\u6210\u3002</li>\n    <li>Dream-VLA\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0cLIBERO\u7684\u5e73\u5747\u6210\u529f\u7387\u8fbe\u523097.2%\uff0c\u8d85\u8d8a\u4e86\u5176\u4ed6\u9886\u5148\u6a21\u578b\u3002</li>\n    <li>\u6211\u4eec\u53d1\u5e03\u4e86Dream-VL\u548cDream-VLA\uff0c\u4ee5\u4fc3\u8fdb\u793e\u533a\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoregressive Vision-Language Models (VLMs) have limitations in complex visual planning and robotic control.</li>\n    <li>The authors introduce Dream-VL, a new diffusion-based VLM that outperforms previous models in benchmarks and is effective for visual planning.</li>\n    <li>Dream-VLA, an advanced model built on Dream-VL, is designed for Vision-Language-Action tasks and shows improved performance due to its unique training approach.</li>\n    <li>Dream-VLA achieves impressive success rates on various tasks, outperforming leading models in the field.</li>\n    <li>The authors have made Dream-VL and Dream-VLA available for further research by the community.</li>\n</ul>"}, "publishedAt": "2025-12-27T09:46:24.000Z", "title": "Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone", "summary": "While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as \u03c0_0 and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.22615.png", "numComments": 1, "submittedBy": {"_id": "628c83d186fc004b14e1ed48", "avatarUrl": "/avatars/05ff943a9b89b5f67c5bc254bf45b8f5.svg", "fullname": "Shansan Gong", "name": "Sansa", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "organization": {"_id": "67ea9ecfc234715db8dbf339", "name": "hkuhk", "fullname": "The University of Hong Kong", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67ea9e8d2d95c10a0da11b0c/FNnR4M7YqKRuG43N5771B.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\u589e\u52a0\uff0c\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u9700\u8981\u66f4\u9ad8\u6548\u548c\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u5f53\u524d\u7684\u6570\u636e\u51c6\u5907\u4e3b\u8981\u4f9d\u8d56\u4e34\u65f6\u811a\u672c\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u62bd\u8c61\u548c\u53ef\u91cd\u590d\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DataFlow\uff0c\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u51c6\u5907\u6846\u67b6\uff0c\u652f\u6301\u6a21\u5757\u5316\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>DataFlow\u5305\u62ec\u8fd1200\u4e2a\u53ef\u91cd\u7528\u7684\u64cd\u4f5c\u7b26\u548c\u516d\u4e2a\u901a\u7528\u7ba1\u9053\uff0c\u8986\u76d6\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u9886\u57df\u3002</li>\n    <li>\u901a\u8fc7DataFlow-Agent\uff0c\u7528\u6237\u53ef\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u81ea\u52a8\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u7ba1\u9053\uff0c\u63d0\u5347\u4e86LLM\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DataFlow is a new framework for preparing high-quality data for Large Language Models (LLMs), addressing issues with current practices that are inconsistent and hard to reproduce.</li>\n    <li>It offers a modular and reusable system for data transformations with a user-friendly API, making it easier to build and optimize data flows.</li>\n    <li>The framework includes nearly 200 reusable operators and covers various domains like text, code, and mathematical reasoning.</li>\n    <li>DataFlow-Agent helps users create executable data pipelines from natural language instructions, making it more accessible.</li>\n    <li>Tests show that DataFlow significantly improves the performance of LLMs compared to existing datasets, demonstrating its effectiveness for scalable data preparation.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.04677", "authors": [{"_id": "693251d36d1060ca587a2746", "name": "Yubo Huang", "hidden": false}, {"_id": "693251d36d1060ca587a2747", "name": "Hailong Guo", "hidden": false}, {"_id": "693251d36d1060ca587a2748", "name": "Fangtai Wu", "hidden": false}, {"_id": "693251d36d1060ca587a2749", "name": "Shifeng Zhang", "hidden": false}, {"_id": "693251d36d1060ca587a274a", "name": "Shijie Huang", "hidden": false}, {"_id": "693251d36d1060ca587a274b", "name": "Qijun Gan", "hidden": false}, {"_id": "693251d36d1060ca587a274c", "name": "Lin Liu", "hidden": false}, {"_id": "693251d36d1060ca587a274d", "name": "Sirui Zhao", "hidden": false}, {"_id": "693251d36d1060ca587a274e", "name": "Enhong Chen", "hidden": false}, {"_id": "693251d36d1060ca587a274f", "user": {"_id": "637c941588699fba70e29f70", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c941588699fba70e29f70/b6G_QZkT-MhE47dx87i0d.png", "isPro": true, "fullname": "LIU JIAMING", "user": "jamesliu1217", "type": "user"}, "name": "Jiaming Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:19.340Z", "hidden": false}, {"_id": "693251d36d1060ca587a2750", "name": "Steven Hoi", "hidden": false}], "publishedAt": "2025-12-04T11:11:24.000Z", "submittedOnDailyAt": "2025-12-05T01:00:58.773Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "submittedOnDailyBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "isPro": true, "fullname": "AK", "user": "akhaliq", "type": "user"}, "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "upvotes": 142, "discussionId": "693251d46d1060ca587a2751", "projectPage": "https://liveavatar.github.io/", "githubRepo": "https://github.com/Alibaba-Quark/LiveAvatar", "ai_summary": "Live Avatar uses a 14-billion-parameter diffusion model with Timestep-forcing Pipeline Parallelism and Rolling Sink Frame Mechanism to achieve real-time, high-fidelity avatar generation.", "ai_keywords": ["diffusion-based video generation", "sequential computation", "long-horizon inconsistency", "real-time", "streaming audio-driven avatar synthesis", "Timestep-forcing Pipeline Parallelism", "distributed inference paradigm", "pipeline parallelism", "denoising steps", "autoregressive bottleneck", "low-latency real-time streaming", "Rolling Sink Frame Mechanism", "sequence fidelity", "Self-Forcing Distribution Matching Distillation", "causal", "streamable adaptation", "visual quality", "end-to-end generation", "H800 GPUs"], "githubStars": 348, "summary_zh": "<ul>\n  <li>\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u53d7\u5230\u987a\u5e8f\u8ba1\u7b97\u548c\u957f\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u7684\u9650\u5236\uff0c\u5f71\u54cd\u5b9e\u65f6\u97f3\u9891\u9a71\u52a8\u7684\u865a\u62df\u5f62\u8c61\u5408\u6210\u3002</li>\n  <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u5b9e\u65f6\u865a\u62df\u5f62\u8c61\u201d\uff08Live Avatar\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u4e14\u652f\u6301\u65e0\u9650\u957f\u5ea6\u865a\u62df\u5f62\u8c61\u751f\u6210\u7684\u6846\u67b6\u3002</li>\n  <li>\u8be5\u6846\u67b6\u4f7f\u7528\u4e86\u4e00\u4e2a140\u4ebf\u53c2\u6570\u7684\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e86\u65f6\u95f4\u6b65\u5f3a\u5236\u7ba1\u9053\u5e76\u884c\uff08TPP\uff09\u6280\u672f\uff0c\u80fd\u591f\u5728\u591a\u4e2aGPU\u4e0a\u9ad8\u6548\u5904\u7406\u53bb\u566a\u6b65\u9aa4\u3002</li>\n  <li>\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u6eda\u52a8\u6c89\u6d78\u5e27\u673a\u5236\uff08RSFM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5916\u89c2\u6765\u7ef4\u62a4\u5e8f\u5217\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u8eab\u4efd\u6f02\u79fb\u548c\u989c\u8272\u4f2a\u5f71\u3002</li>\n  <li>\u5b9e\u65f6\u865a\u62df\u5f62\u8c61\u57285\u4e2aH800 GPU\u4e0a\u5b9e\u73b0\u4e86\u6bcf\u79d220\u5e27\u7684\u751f\u6210\u901f\u5ea6\uff0c\u6807\u5fd7\u7740\u9ad8\u4fdd\u771f\u5b9e\u65f6\u865a\u62df\u5f62\u8c61\u751f\u6210\u7684\u65b0\u6807\u51c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Live Avatar is a new system for generating realistic, animated avatars in real-time using a powerful 14-billion-parameter model.</li>\n    <li>It uses a method called Timestep-forcing Pipeline Parallelism (TPP) to speed up the processing by using multiple GPUs, allowing for low-latency streaming.</li>\n    <li>The system includes a technique called Rolling Sink Frame Mechanism (RSFM) to keep the avatar's appearance consistent and reduce visual errors.</li>\n    <li>Live Avatar can generate high-quality video at 20 frames per second using 5 GPUs, marking a significant advancement in real-time avatar creation.</li>\n    <li>This work sets a new standard for using advanced models in long videos for practical applications.</li>\n</ul>"}, "publishedAt": "2025-12-04T06:11:24.000Z", "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length", "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04677.png", "numComments": 4, "submittedBy": {"_id": "60f1abe7544c2adfd699860c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg", "fullname": "AK", "name": "akhaliq", "type": "user", "isPro": true, "isHf": true, "isHfAdmin": false, "isMod": false, "followerCount": 8858}, "isAuthorParticipating": true}, {"paper": {"id": "2512.04324", "authors": [{"_id": "693245c66d1060ca587a265c", "name": "Fangyu Lei", "hidden": false}, {"_id": "693245c66d1060ca587a265d", "user": {"_id": "67f231b5ac0b61b184e84482", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qJZfkOZEn5Zx_VP2MR7ab.png", "isPro": false, "fullname": "mengjinxiang", "user": "Mjx0221", "type": "user"}, "name": "Jinxiang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:39:10.222Z", "hidden": false}, {"_id": "693245c66d1060ca587a265e", "name": "Yiming Huang", "hidden": false}, {"_id": "693245c66d1060ca587a265f", "name": "Junjie Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a2660", "name": "Yitong Zhang", "hidden": false}, {"_id": "693245c66d1060ca587a2661", "user": {"_id": "66adf5cc0c6056d9f4dc308f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66adf5cc0c6056d9f4dc308f/mVKo06P7M1qf6RYNG-c2i.jpeg", "isPro": false, "fullname": "Jane Luo", "user": "Luo2003", "type": "user"}, "name": "Jianwen Luo", "status": "claimed_verified", "statusLastChangedAt": "2025-12-05T08:29:34.047Z", "hidden": false}, {"_id": "693245c66d1060ca587a2662", "name": "Xin Zou", "hidden": false}, {"_id": "693245c66d1060ca587a2663", "name": "Ruiyi Yang", "hidden": false}, {"_id": "693245c66d1060ca587a2664", "name": "Wenbo Shi", "hidden": false}, {"_id": "693245c66d1060ca587a2665", "name": "Yan Gao", "hidden": false}, {"_id": "693245c66d1060ca587a2666", "name": "Shizhu He", "hidden": false}, {"_id": "693245c66d1060ca587a2667", "name": "Zuo Wang", "hidden": false}, {"_id": "693245c66d1060ca587a2668", "name": "Qian Liu", "hidden": false}, {"_id": "693245c66d1060ca587a2669", "name": "Yang Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266a", "name": "Ke Wang", "hidden": false}, {"_id": "693245c66d1060ca587a266b", "name": "Jun Zhao", "hidden": false}, {"_id": "693245c66d1060ca587a266c", "name": "Kang Liu", "hidden": false}], "publishedAt": "2025-12-03T23:21:28.000Z", "submittedOnDailyAt": "2025-12-05T00:09:12.656Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "upvotes": 133, "discussionId": "693245c66d1060ca587a266d", "projectPage": "https://da-comp.github.io/", "ai_summary": "DAComp is a benchmark of 210 tasks that evaluates the capabilities of agents in real-world data engineering and data analysis workflows, revealing significant deficiencies in both areas.", "ai_keywords": ["data engineering", "data analysis", "DE tasks", "DA tasks", "SQL pipelines", "multi-metric evaluation", "LLM-judge", "hierarchical rubrics", "autonomous data agents"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>DAComp\u662f\u4e00\u4e2a\u5305\u542b210\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u6a21\u62df\u4f01\u4e1a\u6570\u636e\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u9700\u8981\u5bf9\u5de5\u4e1a\u67b6\u6784\u8fdb\u884c\u6df1\u5165\u8bbe\u8ba1\uff0c\u5e76\u6784\u5efa\u590d\u6742\u7684SQL\u7ba1\u9053\u3002</li>\n    <li>\u6570\u636e\u5206\u6790\u4efb\u52a1\u6d89\u53ca\u5f00\u653e\u6027\u5546\u4e1a\u95ee\u9898\uff0c\u9700\u8981\u6218\u7565\u89c4\u5212\u548c\u6df1\u5165\u5206\u6790\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u73b0\u6709\u7684\u667a\u80fd\u4ee3\u7406\u5728DAComp\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u5de5\u7a0b\u4efb\u52a1\u4e2d\uff0c\u6210\u529f\u7387\u4f4e\u4e8e20%\u3002</li>\n    <li>DAComp\u5e2e\u52a9\u8bc6\u522b\u6570\u636e\u5904\u7406\u4e2d\u7684\u4e0d\u8db3\uff0c\u63a8\u52a8\u66f4\u5f3a\u5927\u7684\u81ea\u4e3b\u6570\u636e\u4ee3\u7406\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DAComp is a benchmark with 210 tasks that simulate real-world data workflows in businesses.</li>\n    <li>It includes data engineering tasks like designing SQL pipelines and adapting systems to new requirements.</li>\n    <li>Data analysis tasks focus on solving business problems using strategic planning and exploration.</li>\n    <li>Results show that current AI agents struggle with these tasks, especially in data engineering with success rates below 20%.</li>\n    <li>DAComp helps identify weaknesses in AI systems, aiming to improve their ability to work autonomously with data.</li>\n</ul>"}, "publishedAt": "2025-12-03T18:21:28.000Z", "title": "DAComp: Benchmarking Data Agents across the Full Data Intelligence Lifecycle", "summary": "Real-world enterprise data intelligence workflows encompass data engineering that turns raw sources into analytical-ready tables and data analysis that convert those tables into decision-oriented insights. We introduce DAComp, a benchmark of 210 tasks that mirrors these complex workflows. Data engineering (DE) tasks require repository-level engineering on industrial schemas, including designing and building multi-stage SQL pipelines from scratch and evolving existing systems under evolving requirements. Data analysis (DA) tasks pose open-ended business problems that demand strategic planning, exploratory analysis through iterative coding, interpretation of intermediate results, and the synthesis of actionable recommendations. Engineering tasks are scored through execution-based, multi-metric evaluation. Open-ended tasks are assessed by a reliable, experimentally validated LLM-judge, which is guided by hierarchical, meticulously crafted rubrics. Our experiments reveal that even state-of-the-art agents falter on DAComp. Performance on DE tasks is particularly low, with success rates under 20%, exposing a critical bottleneck in holistic pipeline orchestration, not merely code generation. Scores on DA tasks also average below 40%, highlighting profound deficiencies in open-ended reasoning and demonstrating that engineering and analysis are distinct capabilities. By clearly diagnosing these limitations, DAComp provides a rigorous and realistic testbed to drive the development of truly capable autonomous data agents for enterprise settings. Our data and code are available at https://da-comp.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.04324.png", "numComments": 5, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 178}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni \u662f\u4e00\u4e2a\u901a\u7528\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u76f4\u63a5\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u7b49\u4efb\u52a1\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u7684\u65b9\u6cd5\u3002</li>\n    <li>Kling-Omni \u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u5982\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\uff0c\u5e76\u5c06\u5176\u5904\u7406\u4e3a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5efa\u7acb\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6570\u636e\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u63a8\u7406\u4f18\u5316\u6765\u589e\u5f3a\u5176\u80fd\u529b\u3002</li>\n    <li>Kling-Omni \u5728\u4e0a\u4e0b\u6587\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u6267\u884c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from various types of visual and language inputs.</li>\n    <li>It combines video generation, editing, and reasoning tasks into one comprehensive framework.</li>\n    <li>The system can process text instructions, reference images, and video contexts to produce cinematic-quality videos.</li>\n    <li>Kling-Omni uses a robust data system and efficient training methods to enhance video creation capabilities.</li>\n    <li>It shows strong performance in generating content, editing based on reasoning, and following different instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.08765", "authors": [{"_id": "6938da63dfc35938ba129f3c", "user": {"_id": "642e3bcb958faf258a40e89c", "avatarUrl": "/avatars/dad142df2217f8eed1f45c9e7287d3ea.svg", "isPro": false, "fullname": "Ruihang Chu", "user": "Ruihang", "type": "user"}, "name": "Ruihang Chu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:07.767Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3d", "name": "Yefei He", "hidden": false}, {"_id": "6938da63dfc35938ba129f3e", "user": {"_id": "62d812e143df7719860d05d1", "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg", "isPro": false, "fullname": "zhekai chen", "user": "Azily", "type": "user"}, "name": "Zhekai Chen", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:42:00.513Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f3f", "name": "Shiwei Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f40", "user": {"_id": "637ee45b2438d7485b8d8f6a", "avatarUrl": "/avatars/11b7d29b6fa6c1b392641e0cd4002863.svg", "isPro": false, "fullname": "Xiaogang Xu", "user": "xiaogang00", "type": "user"}, "name": "Xiaogang Xu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:51.241Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f41", "name": "Bin Xia", "hidden": false}, {"_id": "6938da63dfc35938ba129f42", "name": "Dingdong Wang", "hidden": false}, {"_id": "6938da63dfc35938ba129f43", "name": "Hongwei Yi", "hidden": false}, {"_id": "6938da63dfc35938ba129f44", "user": {"_id": "65d5ec74cd05bc1eaa125040", "avatarUrl": "/avatars/2de1b1539a86452c2c89570eeb02f5ab.svg", "isPro": false, "fullname": "Xihui Liu", "user": "XihuiLiu", "type": "user"}, "name": "Xihui Liu", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:32.582Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f45", "user": {"_id": "690090cca41c454e4786c0e5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/690090cca41c454e4786c0e5/ykyy4gV7EV_xfv4glxC1m.png", "isPro": false, "fullname": "Hengshuang Zhao", "user": "Hengshuang", "type": "user"}, "name": "Hengshuang Zhao", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:26.372Z", "hidden": false}, {"_id": "6938da63dfc35938ba129f46", "name": "Yu Liu", "hidden": false}, {"_id": "6938da63dfc35938ba129f47", "name": "Yingya Zhang", "hidden": false}, {"_id": "6938da63dfc35938ba129f48", "user": {"_id": "64ca1fe838837b12d5e529b7", "avatarUrl": "/avatars/44a3ad9e59318784ac531993b5f69f6b.svg", "isPro": false, "fullname": "Yujiu Yang", "user": "Thu-redrobot", "type": "user"}, "name": "Yujiu Yang", "status": "admin_assigned", "statusLastChangedAt": "2025-12-10T09:41:10.566Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "publishedAt": "2025-12-09T16:13:55.000Z", "submittedOnDailyAt": "2025-12-10T00:20:18.797Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "upvotes": 94, "discussionId": "6938da64dfc35938ba129f49", "githubRepo": "https://github.com/ali-vilab/Wan-Move", "githubRepoAddedBy": "user", "ai_summary": "Wan-Move enhances motion control in video generative models by integrating motion-aware features into latent space, enabling high-quality and scalable video synthesis.", "ai_keywords": ["motion control", "video generative models", "dense point trajectories", "latent space", "spatiotemporal feature map", "motion guidance", "image-to-video model", "auxiliary motion encoders", "fine-tuning", "MoveBench", "motion annotations"], "githubStars": 197, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "summary_zh": "<ul>\n    <li>Wan-Move\u662f\u4e00\u4e2a\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u8fd0\u52a8\u63a7\u5236\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7cbe\u786e\u4e14\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u63a7\u5236\u7c97\u7cd9\u548c\u6269\u5c55\u6027\u6709\u9650\u7684\u95ee\u9898\u3002</li>\n    <li>\u4f7f\u7528\u5bc6\u96c6\u7684\u70b9\u8f68\u8ff9\u8868\u793a\u7269\u4f53\u8fd0\u52a8\uff0c\u4f7f\u5f97\u573a\u666f\u7684\u63a7\u5236\u66f4\u52a0\u7ec6\u81f4\u3002</li>\n    <li>Wan-Move\u80fd\u751f\u62105\u79d2\u3001480p\u7684\u89c6\u9891\uff0c\u5176\u8fd0\u52a8\u63a7\u5236\u80fd\u529b\u4e0e\u5546\u4e1a\u8f6f\u4ef6Kling 1.5 Pro\u7684Motion Brush\u76f8\u5f53\u3002</li>\n    <li>\u8bbe\u8ba1\u4e86MoveBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u591a\u79cd\u5185\u5bb9\u7c7b\u522b\u548c\u9ad8\u8d28\u91cf\u7684\u8fd0\u52a8\u6ce8\u91ca\uff0c\u4ee5\u652f\u6301\u7efc\u5408\u8bc4\u4f30\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Wan-Move is a new framework that improves how we control motion in video generation models.</li>\n    <li>It allows for precise and high-quality motion control, overcoming limitations of existing methods.</li>\n    <li>The framework uses dense point trajectories to represent object movements, enhancing scene control.</li>\n    <li>Wan-Move integrates easily with existing video models without needing to change their architecture.</li>\n    <li>It outperforms competitors in generating controllable motion in videos, supported by a new evaluation benchmark called MoveBench.</li>\n</ul>"}, "publishedAt": "2025-12-09T11:13:55.000Z", "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "summary": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/GRHR-g0jymQza9KMw0iLn.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/K8nyGDyLuL_hxp15wJdMk.png", "https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/4b8dLB_xvGpTjJlWP8oeh.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.08765.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 181}, "organization": {"_id": "67d15cca6e2cf0e062dbfb54", "name": "AlibabaTongyiLab", "fullname": "TongyiLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d1502bfabfe9974d1f77bb/XdUSVf6HqBzE7zFBfSDQP.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.14691", "authors": [{"_id": "69421eb65d5b2dc105274811", "name": "Zefan Cai", "hidden": false}, {"_id": "69421eb65d5b2dc105274812", "name": "Haoyi Qiu", "hidden": false}, {"_id": "69421eb65d5b2dc105274813", "user": {"_id": "643ebfac1a12dcf01c6b5263", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ebfac1a12dcf01c6b5263/thkBlRvwgf83GULvOveM6.png", "isPro": false, "fullname": "Tianyi Ma", "user": "SueMintony", "type": "user"}, "name": "Tianyi Ma", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:32.897Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274814", "name": "Haozhe Zhao", "hidden": false}, {"_id": "69421eb65d5b2dc105274815", "user": {"_id": "6450bcd3673b2bcfaf8681af", "avatarUrl": "/avatars/f5f93d780562d0772ec5dc1728945fcf.svg", "isPro": false, "fullname": "Gengze Zhou", "user": "ZGZzz", "type": "user"}, "name": "Gengze Zhou", "status": "claimed_verified", "statusLastChangedAt": "2025-12-17T10:08:34.841Z", "hidden": false}, {"_id": "69421eb65d5b2dc105274816", "name": "Kung-Hsiang Huang", "hidden": false}, {"_id": "69421eb65d5b2dc105274817", "name": "Parisa Kordjamshidi", "hidden": false}, {"_id": "69421eb65d5b2dc105274818", "name": "Minjia Zhang", "hidden": false}, {"_id": "69421eb65d5b2dc105274819", "name": "Xiao Wen", "hidden": false}, {"_id": "69421eb65d5b2dc10527481a", "name": "Jiuxiang Gu", "hidden": false}, {"_id": "69421eb65d5b2dc10527481b", "name": "Nanyun Peng", "hidden": false}, {"_id": "69421eb65d5b2dc10527481c", "name": "Junjie Hu", "hidden": false}], "publishedAt": "2025-12-16T18:58:04.000Z", "submittedOnDailyAt": "2025-12-17T00:38:46.609Z", "title": "MMGR: Multi-Modal Generative Reasoning", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "upvotes": 78, "discussionId": "69421eb65d5b2dc10527481d", "ai_summary": "MMGR evaluates video and image models across reasoning abilities in multiple domains, revealing performance gaps and highlighting limitations in perceptual data and causal correctness.", "ai_keywords": ["Frechet Video Distance (FVD)", "MMGR", "Multi-Modal Generative Reasoning Evaluation and Benchmark", "Physical", "Logical", "3D Spatial", "2D Spatial", "Temporal", "Abstract Reasoning", "ARC-AGI", "Sudoku", "Embodied Navigation", "Physical Commonsense", "Veo-3", "Sora-2", "Wan-2.2", "Nano-banana", "Nano-banana Pro", "GPT-4o-image", "Qwen-image", "perceptual quality", "reasoning failures", "causality", "physics", "global consistency", "holistic correctness", "generative reasoning", "world simulators"], "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u57fa\u7840\u6a21\u578b\u5728\u751f\u6210\u89c6\u89c9\u771f\u5b9e\u548c\u65f6\u95f4\u4e00\u81f4\u7684\u5185\u5bb9\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u53ef\u9760\u6027\u53d7\u5230\u7269\u7406\u3001\u903b\u8f91\u548c\u7a7a\u95f4\u7ea6\u675f\u7684\u5f71\u54cd\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u6307\u6807\uff08\u5982FVD\uff09\u4fa7\u91cd\u4e8e\u611f\u77e5\u8d28\u91cf\uff0c\u5ffd\u89c6\u4e86\u56e0\u679c\u5173\u7cfb\u3001\u7269\u7406\u89c4\u5f8b\u548c\u5168\u7403\u4e00\u81f4\u6027\u7684\u63a8\u7406\u5931\u8d25\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51faMMGR\uff08\u591a\u6a21\u6001\u751f\u6210\u63a8\u7406\u8bc4\u4f30\u548c\u57fa\u51c6\uff09\uff0c\u57fa\u4e8e\u4e94\u79cd\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u8bc4\u4f30\uff1a\u7269\u7406\u3001\u903b\u8f91\u30013D\u7a7a\u95f4\u30012D\u7a7a\u95f4\u548c\u65f6\u95f4\u3002</li>\n    <li>MMGR\u5728\u62bd\u8c61\u63a8\u7406\u3001\u5177\u4f53\u5bfc\u822a\u548c\u7269\u7406\u5e38\u8bc6\u4e09\u4e2a\u9886\u57df\u8fdb\u884c\u8bc4\u4f30\uff0c\u663e\u793a\u51fa\u76ee\u524d\u6a21\u578b\u5728\u4e0d\u540c\u9886\u57df\u7684\u8868\u73b0\u5dee\u5f02\u3002</li>\n    <li>\u867d\u7136\u6a21\u578b\u5728\u7269\u7406\u5e38\u8bc6\u4efb\u52a1\u4e0a\u8868\u73b0\u4e2d\u7b49\uff0c\u4f46\u5728\u62bd\u8c61\u63a8\u7406\u4e0a\u51c6\u786e\u7387\u4f4e\u4e8e10%\uff0c\u5e76\u4e14\u5728\u590d\u6742\u7a7a\u95f4\u89c4\u5212\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video models can create realistic content, but their accuracy in simulating the world depends on understanding physics and logic.</li>\n    <li>Current evaluation methods focus too much on visual quality and miss important reasoning errors like causality and consistency.</li>\n    <li>MMGR is a new evaluation framework that tests five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal.</li>\n    <li>It assesses video and image models across various tasks, revealing significant performance gaps, especially in abstract reasoning.</li>\n    <li>MMGR aims to improve generative models by highlighting their weaknesses and promoting better reasoning in world simulations.</li>\n</ul>"}, "publishedAt": "2025-12-16T13:58:04.000Z", "title": "MMGR: Multi-Modal Generative Reasoning", "summary": "Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.14691.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "isAuthorParticipating": false}, {"paper": {"id": "2512.16969", "authors": [{"_id": "6948b09934f46eaf46cbb214", "user": {"_id": "65f3f43fc9940817ca9a427b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg", "isPro": false, "fullname": "Wanghan Xu", "user": "CoCoOne", "type": "user"}, "name": "Wanghan Xu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:47.069Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb215", "name": "Yuhao Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb216", "name": "Yifan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb217", "name": "Qinglong Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb218", "name": "Shuo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb219", "name": "Jia Bu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21a", "user": {"_id": "61e6dd8a82b19b93e1a51fa6", "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg", "isPro": false, "fullname": "Kelvin Liu", "user": "BoKelvin", "type": "user"}, "name": "Bo Liu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:41.476Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21b", "name": "Yixin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21c", "name": "Xuming He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21d", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21e", "name": "Xiang Zhuang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb21f", "name": "Fengxiang Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb220", "name": "Zhiwang Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb221", "name": "Qiantai Feng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb222", "name": "Wenxuan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb223", "user": {"_id": "6539bc7756c9b35961021fa8", "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg", "isPro": false, "fullname": "Jiaqi Wei", "user": "VitaCoco", "type": "user"}, "name": "Jiaqi Wei", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:43.408Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb224", "name": "Hao Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb225", "name": "Yuejin Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb226", "name": "Guangshuai Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb227", "name": "Sheng Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb228", "name": "Ziyan Huang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb229", "name": "Xinyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22a", "name": "Jiyao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22b", "name": "Cheng Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22c", "name": "Wei Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22d", "name": "Ying Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22e", "name": "Junzhi Ning", "hidden": false}, {"_id": "6948b09934f46eaf46cbb22f", "name": "Pengfei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb230", "name": "Chenglong Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb231", "name": "Ye Du", "hidden": false}, {"_id": "6948b09934f46eaf46cbb232", "name": "Changkai Ji", "hidden": false}, {"_id": "6948b09934f46eaf46cbb233", "name": "Huihui Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb234", "name": "Ming Hu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb235", "name": "Jiangbin Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb236", "name": "Xin Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb237", "name": "Yucheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb238", "name": "Feifei Jiang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb239", "name": "Xi Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23a", "name": "Xiangru Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23b", "name": "Yuchen Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23c", "name": "Yingzhou Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23d", "name": "Yuanyuan Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23e", "name": "Lihao Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb23f", "name": "Chengbo Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb240", "name": "Jinzhe Ma", "hidden": false}, {"_id": "6948b09934f46eaf46cbb241", "name": "Wanhao Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb242", "name": "Yating Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb243", "name": "Kuo-Cheng Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb244", "name": "Shengdu Chai", "hidden": false}, {"_id": "6948b09934f46eaf46cbb245", "name": "Yizhou Wang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb246", "name": "Ouwen Zhangjin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb247", "name": "Chen Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb248", "name": "Shufei Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb249", "name": "Wenbo Cao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24a", "name": "Junjie Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24b", "name": "Taoyong Cui", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24c", "name": "Zhouheng Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24d", "name": "Juntao Deng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24e", "name": "Yijie Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb24f", "name": "Feng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb250", "name": "Wangxu Wei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb251", "name": "Jingyi Xu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb252", "name": "Zhangrui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb253", "name": "Junchao Gong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb254", "name": "Zijie Guo", "hidden": false}, {"_id": "6948b09934f46eaf46cbb255", "name": "Zhiyu Yao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb256", "name": "Zaoyu Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb257", "name": "Tianhao Peng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb258", "user": {"_id": "68ad9cb3bcaa8d84217a8bdf", "avatarUrl": "/avatars/dbb3199cf5bfc2acdbd38069c823c027.svg", "isPro": false, "fullname": "Fangchen Yu", "user": "SciYu", "type": "user"}, "name": "Fangchen Yu", "status": "claimed_verified", "statusLastChangedAt": "2025-12-22T10:58:45.323Z", "hidden": false}, {"_id": "6948b09934f46eaf46cbb259", "name": "Bo Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25a", "name": "Dongzhan Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25b", "name": "Shixiang Tang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25c", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25d", "name": "Fenghua Ling", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25e", "name": "Yan Lu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb25f", "name": "Yuchen Ren", "hidden": false}, {"_id": "6948b09934f46eaf46cbb260", "name": "Ben Fei", "hidden": false}, {"_id": "6948b09934f46eaf46cbb261", "name": "Zhen Zhao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb262", "name": "Xinyu Gu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb263", "name": "Rui Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb264", "name": "Xiao-Ming Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb265", "name": "Weikang Si", "hidden": false}, {"_id": "6948b09934f46eaf46cbb266", "name": "Yang Liu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb267", "name": "Hao Chen", "hidden": false}, {"_id": "6948b09934f46eaf46cbb268", "name": "Xiangchao Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb269", "name": "Xue Yang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26a", "name": "Junchi Yan", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26b", "name": "Jiamin Wu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26c", "name": "Qihao Zheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26d", "name": "Chenhui Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26e", "name": "Zhiqiang Gao", "hidden": false}, {"_id": "6948b09934f46eaf46cbb26f", "name": "Hao Kong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb270", "name": "Junjun He", "hidden": false}, {"_id": "6948b09934f46eaf46cbb271", "name": "Mao Su", "hidden": false}, {"_id": "6948b09934f46eaf46cbb272", "name": "Tianfan Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb273", "name": "Peng Ye", "hidden": false}, {"_id": "6948b09934f46eaf46cbb274", "name": "Chunfeng Song", "hidden": false}, {"_id": "6948b09934f46eaf46cbb275", "name": "Nanqing Dong", "hidden": false}, {"_id": "6948b09934f46eaf46cbb276", "name": "Yuqiang Li", "hidden": false}, {"_id": "6948b09934f46eaf46cbb277", "name": "Huazhu Fu", "hidden": false}, {"_id": "6948b09934f46eaf46cbb278", "name": "Siqi Sun", "hidden": false}, {"_id": "6948b09934f46eaf46cbb279", "name": "Lijing Cheng", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27a", "name": "Jintai Lin", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27b", "name": "Wanli Ouyang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27c", "name": "Bowen Zhou", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27d", "name": "Wenlong Zhang", "hidden": false}, {"_id": "6948b09934f46eaf46cbb27e", "name": "Lei Bai", "hidden": false}], "publishedAt": "2025-12-18T12:44:36.000Z", "submittedOnDailyAt": "2025-12-22T00:14:52.424Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "upvotes": 78, "discussionId": "6948b09934f46eaf46cbb27f", "projectPage": "https://internscience.github.io/SGI-Page/", "githubRepo": "https://github.com/InternScience/SGI-Bench", "githubRepoAddedBy": "user", "ai_summary": "A framework for Scientific General Intelligence (SGI) is presented, evaluated using SGI-Bench, and improved with Test-Time Reinforcement Learning, highlighting gaps in existing models' scientific capabilities.", "ai_keywords": ["Scientific General Intelligence", "SGI", "Practical Inquiry Model", "PIM", "deep research", "idea generation", "dry experiments", "wet experiments", "experimental reasoning", "SGI-Bench", "Big Questions", "Low exact match", "feasibility", "detail", "code executability", "execution result accuracy", "sequence fidelity", "multimodal comparative-reasoning", "Test-Time Reinforcement Learning", "TTRL", "retrieval-augmented novelty rewards", "hypothesis novelty"], "githubStars": 56, "summary_zh": "<ul>\n    <li>\u79d1\u5b66\u901a\u7528\u667a\u80fd\uff08SGI\uff09\u5c1a\u7f3a\u4e4f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u6307\u7684\u662f\u5728\u79d1\u5b66\u9886\u57df\u4e2d\u81ea\u4e3b\u6784\u601d\u3001\u7814\u7a76\u548c\u63a8\u7406\u7684\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u57fa\u4e8e\u5b9e\u9645\u63a2\u7a76\u6a21\u578b\uff08PIM\uff09\u7684SGI\u5b9a\u4e49\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u4e0e\u79d1\u5b66\u5bb6\u76f8\u5173\u7684\u4efb\u52a1\u8fdb\u884c\u64cd\u4f5c\u5316\uff0c\u5206\u522b\u662f\u6df1\u5165\u7814\u7a76\u3001\u521b\u610f\u751f\u6210\u3001\u5e72\u5b9e\u9a8c\u548c\u6e7f\u5b9e\u9a8c\u3002</li>\n    <li>SGI-Bench\u5305\u542b\u8d85\u8fc71000\u4e2a\u4e13\u5bb6\u7b56\u5212\u7684\u8de8\u5b66\u79d1\u6837\u672c\uff0c\u80fd\u591f\u7cfb\u7edf\u8bc4\u4f30\u6700\u65b0\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002</li>\n    <li>\u7814\u7a76\u7ed3\u679c\u663e\u793a\u5728\u6df1\u5165\u7814\u7a76\u4e2d\uff0c\u51c6\u786e\u5339\u914d\u7387\u4f4e\uff0810-20%\uff09\uff0c\u521b\u610f\u7f3a\u4e4f\u53ef\u884c\u6027\u548c\u7ec6\u8282\uff0c\u5e72\u5b9e\u9a8c\u7684\u4ee3\u7801\u53ef\u6267\u884c\u6027\u9ad8\u4f46\u7ed3\u679c\u51c6\u786e\u7387\u4f4e\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u6d4b\u8bd5\u65f6\u95f4\u5f3a\u5316\u5b66\u4e60\uff08TTRL\uff09\uff0c\u5728\u63a8\u7406\u65f6\u4f18\u5316\u68c0\u7d22\u589e\u5f3a\u7684\u65b0\u9896\u6027\u5956\u52b1\uff0c\u63d0\u5347\u5047\u8bbe\u7684\u65b0\u9896\u6027\uff0c\u52a9\u529bAI\u7cfb\u7edf\u66f4\u597d\u5730\u53c2\u4e0e\u79d1\u5b66\u53d1\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is no clear framework for Scientific General Intelligence (SGI), which is the ability of AI to think and work like scientists.</li>\n    <li>This study defines SGI based on a model that includes thinking, creating, acting, and observing, and tests it with tasks similar to those scientists do.</li>\n    <li>SGI-Bench, a benchmark created for evaluation, includes over 1,000 examples related to big scientific questions to test AI performance.</li>\n    <li>Results show that AI struggles in areas like deep research, generating detailed ideas, and executing experiments accurately.</li>\n    <li>The study also introduces a method called Test-Time Reinforcement Learning (TTRL) to improve AI's ability to generate new ideas during testing.</li>\n</ul>"}, "publishedAt": "2025-12-18T07:44:36.000Z", "title": "Probing Scientific General Intelligence of LLMs with Scientist-Aligned Workflows", "summary": "Despite advances in scientific AI, a coherent framework for Scientific General Intelligence (SGI)-the ability to autonomously conceive, investigate, and reason across scientific domains-remains lacking. We present an operational SGI definition grounded in the Practical Inquiry Model (PIM: Deliberation, Conception, Action, Perception) and operationalize it via four scientist-aligned tasks: deep research, idea generation, dry/wet experiments, and experimental reasoning. SGI-Bench comprises over 1,000 expert-curated, cross-disciplinary samples inspired by Science's 125 Big Questions, enabling systematic evaluation of state-of-the-art LLMs. Results reveal gaps: low exact match (10--20%) in deep research despite step-level alignment; ideas lacking feasibility and detail; high code executability but low execution result accuracy in dry experiments; low sequence fidelity in wet protocols; and persistent multimodal comparative-reasoning challenges. We further introduce Test-Time Reinforcement Learning (TTRL), which optimizes retrieval-augmented novelty rewards at inference, enhancing hypothesis novelty without reference answer. Together, our PIM-grounded definition, workflow-centric benchmark, and empirical insights establish a foundation for AI systems that genuinely participate in scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16969.png", "numComments": 6, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 188}, "isAuthorParticipating": true}, {"paper": {"id": "2512.20619", "authors": [{"_id": "694b614d746a34b55dd53d1a", "name": "Jianhong Bai", "hidden": false}, {"_id": "694b614d746a34b55dd53d1b", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "694b614d746a34b55dd53d1c", "name": "Xintao Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1d", "name": "Fu Xiao", "hidden": false}, {"_id": "694b614d746a34b55dd53d1e", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "694b614d746a34b55dd53d1f", "name": "Qinghe Wang", "hidden": false}, {"_id": "694b614d746a34b55dd53d20", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "694b614d746a34b55dd53d21", "name": "Menghan Xia", "hidden": false}, {"_id": "694b614d746a34b55dd53d22", "name": "Zuozhu Liu", "hidden": false}, {"_id": "694b614d746a34b55dd53d23", "name": "Haoji Hu", "hidden": false}, {"_id": "694b614d746a34b55dd53d24", "name": "Pengfei Wan", "hidden": false}, {"_id": "694b614d746a34b55dd53d25", "name": "Kun Gai", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "publishedAt": "2025-12-23T18:59:56.000Z", "submittedOnDailyAt": "2025-12-24T01:20:51.117Z", "title": "SemanticGen: Video Generation in Semantic Space", "submittedOnDailyBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "isPro": false, "fullname": "Jianhong Bai", "user": "jianhongbai", "type": "user"}, "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "upvotes": 77, "discussionId": "694b614d746a34b55dd53d26", "projectPage": "https://jianhongbai.github.io/SemanticGen/", "ai_summary": "SemanticGen addresses slow convergence and computational costs in video generation by using a two-stage diffusion model approach that first generates semantic features and then VAE latents, leading to faster convergence and high-quality results.", "ai_keywords": ["VAE space", "VAE decoder", "semantic space", "diffusion model", "semantic video features", "bi-directional attention"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>\u4f20\u7edf\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u4f7f\u7528VAE\u7a7a\u95f4\u5b66\u4e60\u89c6\u9891\u7684\u5206\u5e03\uff0c\u4f46\u751f\u6210\u957f\u89c6\u9891\u65f6\u901f\u5ea6\u6162\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86SemanticGen\uff0c\u901a\u8fc7\u5728\u8bed\u4e49\u7a7a\u95f4\u751f\u6210\u89c6\u9891\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002</li>\n    <li>SemanticGen\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u8fc7\u7a0b\uff0c\u9996\u5148\u751f\u6210\u8bed\u4e49\u89c6\u9891\u7279\u5f81\uff0c\u518d\u57fa\u4e8e\u8fd9\u4e9b\u7279\u5f81\u751f\u6210\u6700\u7ec8\u89c6\u9891\u3002</li>\n    <li>\u5728\u8bed\u4e49\u7a7a\u95f4\u751f\u6210\u89c6\u9891\u6bd4\u5728VAE\u6f5c\u5728\u7a7a\u95f4\u751f\u6210\u66f4\u5feb\uff0c\u5e76\u4e14\u5728\u957f\u89c6\u9891\u751f\u6210\u65f6\u4e5f\u5f88\u6709\u6548\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSemanticGen\u751f\u6210\u7684\u89c6\u9891\u8d28\u91cf\u9ad8\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u5148\u8fdb\u65b9\u6cd5\u548c\u57fa\u51c6\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>SemanticGen is a new method for generating videos that aims to improve speed and efficiency.</li>\n    <li>It starts by creating a simple overview of the video in a semantic space before adding detailed features.</li>\n    <li>This two-stage process involves generating basic video concepts first and then refining them with specific details.</li>\n    <li>Using the semantic space helps the model learn faster and work better for longer videos.</li>\n    <li>Tests show that SemanticGen produces high-quality videos and performs better than existing models.</li>\n</ul>"}, "publishedAt": "2025-12-23T13:59:56.000Z", "title": "SemanticGen: Video Generation in Semantic Space", "summary": "State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6530bf50f145530101ec03a2/amGfUgsGwtKhlryqSDYnU.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.20619.png", "numComments": 2, "submittedBy": {"_id": "6530bf50f145530101ec03a2", "avatarUrl": "/avatars/c61c00c314cf202b64968e51e855694d.svg", "fullname": "Jianhong Bai", "name": "jianhongbai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 14}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.15431", "authors": [{"_id": "69437417542d62d58a7bf6c4", "name": "Haolong Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6c5", "name": "Jia Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c6", "name": "Xin Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6c7", "name": "Yeqing Shen", "hidden": false}, {"_id": "69437417542d62d58a7bf6c8", "user": {"_id": "653614073f4248157d60ccdc", "avatarUrl": "/avatars/c9298bab1cdc1d0b6ffe4c7c5ef18bd5.svg", "isPro": false, "fullname": "mengziyang", "user": "zylate", "type": "user"}, "name": "Ziyang Meng", "status": "claimed_verified", "statusLastChangedAt": "2025-12-18T07:59:53.033Z", "hidden": false}, {"_id": "69437417542d62d58a7bf6c9", "name": "Zhimin Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6ca", "name": "Kaijun Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6cb", "name": "Jin Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf6cc", "name": "Lieyu Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6cd", "name": "Mi Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6ce", "name": "Shiliang Yang", "hidden": false}, {"_id": "69437417542d62d58a7bf6cf", "name": "Zhirui Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d0", "name": "Brian Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d1", "name": "Kang An", "hidden": false}, {"_id": "69437417542d62d58a7bf6d2", "name": "Chenyang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6d3", "name": "Lei Lei", "hidden": false}, {"_id": "69437417542d62d58a7bf6d4", "name": "Mengmeng Duan", "hidden": false}, {"_id": "69437417542d62d58a7bf6d5", "name": "Danxun Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6d6", "name": "Guodong Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d7", "name": "Hang Cheng", "hidden": false}, {"_id": "69437417542d62d58a7bf6d8", "name": "Hao Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6d9", "name": "Jie Dong", "hidden": false}, {"_id": "69437417542d62d58a7bf6da", "name": "Junhao Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf6db", "name": "Mei Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6dc", "name": "Renjie Yu", "hidden": false}, {"_id": "69437417542d62d58a7bf6dd", "name": "Shunshan Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6de", "name": "Xu Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf6df", "name": "Yiting Dai", "hidden": false}, {"_id": "69437417542d62d58a7bf6e0", "name": "Yineng Deng", "hidden": false}, {"_id": "69437417542d62d58a7bf6e1", "name": "Yingdan Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf6e2", "name": "Zelin Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6e3", "name": "Wen Sun", "hidden": false}, {"_id": "69437417542d62d58a7bf6e4", "name": "Chengxu Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e5", "name": "Chunqin Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6e6", "name": "Dong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6e7", "name": "Fengqiong Xiao", "hidden": false}, {"_id": "69437417542d62d58a7bf6e8", "name": "Guanghao Fan", "hidden": false}, {"_id": "69437417542d62d58a7bf6e9", "name": "Guopeng Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ea", "name": "Guozhen Peng", "hidden": false}, {"_id": "69437417542d62d58a7bf6eb", "name": "Hongbing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ec", "name": "Hang Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6ed", "name": "Hongming Chen", "hidden": false}, {"_id": "69437417542d62d58a7bf6ee", "name": "Jingjing Xie", "hidden": false}, {"_id": "69437417542d62d58a7bf6ef", "name": "Jianyong Li", "hidden": false}, {"_id": "69437417542d62d58a7bf6f0", "name": "Jingyang Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf6f1", "name": "Jiaju Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f2", "name": "Jiayu Yuan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f3", "name": "Jianpeng Yin", "hidden": false}, {"_id": "69437417542d62d58a7bf6f4", "name": "Kai Cao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f5", "name": "Liang Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf6f6", "name": "Liguo Tan", "hidden": false}, {"_id": "69437417542d62d58a7bf6f7", "name": "Liying Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf6f8", "name": "Mengqiang Ren", "hidden": false}, {"_id": "69437417542d62d58a7bf6f9", "name": "Min Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fa", "name": "Manjiao Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf6fb", "name": "Mao Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf6fc", "name": "Mingxin Wan", "hidden": false}, {"_id": "69437417542d62d58a7bf6fd", "name": "Na Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf6fe", "name": "Nan Wu", "hidden": false}, {"_id": "69437417542d62d58a7bf6ff", "name": "Ning Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf700", "name": "Peiyao Ma", "hidden": false}, {"_id": "69437417542d62d58a7bf701", "name": "Qingzhou Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf702", "name": "Qiao Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf703", "name": "Qinlin Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf704", "name": "Qiong Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf705", "name": "Qiongyao Li", "hidden": false}, {"_id": "69437417542d62d58a7bf706", "name": "Shangwu Zhong", "hidden": false}, {"_id": "69437417542d62d58a7bf707", "name": "Shuli Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf708", "name": "Shaofan Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf709", "name": "Shisi Gao", "hidden": false}, {"_id": "69437417542d62d58a7bf70a", "name": "Shuang Luo", "hidden": false}, {"_id": "69437417542d62d58a7bf70b", "name": "Xingbin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70c", "name": "Xiaojia Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70d", "name": "Xiaojie Hou", "hidden": false}, {"_id": "69437417542d62d58a7bf70e", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf70f", "name": "Xuanti Feng", "hidden": false}, {"_id": "69437417542d62d58a7bf710", "name": "Xuedan Cai", "hidden": false}, {"_id": "69437417542d62d58a7bf711", "name": "Xuan Wen", "hidden": false}, {"_id": "69437417542d62d58a7bf712", "name": "Xianwei Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf713", "name": "Xin Liang", "hidden": false}, {"_id": "69437417542d62d58a7bf714", "name": "Xin Liu", "hidden": false}, {"_id": "69437417542d62d58a7bf715", "name": "Xin Zhou", "hidden": false}, {"_id": "69437417542d62d58a7bf716", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "69437417542d62d58a7bf717", "name": "Yukang Shi", "hidden": false}, {"_id": "69437417542d62d58a7bf718", "name": "Yunfang Xu", "hidden": false}, {"_id": "69437417542d62d58a7bf719", "name": "Yuqing Zeng", "hidden": false}, {"_id": "69437417542d62d58a7bf71a", "name": "Yixun Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf71b", "name": "Zejia Weng", "hidden": false}, {"_id": "69437417542d62d58a7bf71c", "name": "Zhonghao Yan", "hidden": false}, {"_id": "69437417542d62d58a7bf71d", "name": "Zhiguo Huang", "hidden": false}, {"_id": "69437417542d62d58a7bf71e", "name": "Zhuoyu Wang", "hidden": false}, {"_id": "69437417542d62d58a7bf71f", "name": "Zheng Ge", "hidden": false}, {"_id": "69437417542d62d58a7bf720", "name": "Jing Li", "hidden": false}, {"_id": "69437417542d62d58a7bf721", "name": "Yibo Zhu", "hidden": false}, {"_id": "69437417542d62d58a7bf722", "name": "Binxing Jiao", "hidden": false}, {"_id": "69437417542d62d58a7bf723", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "69437417542d62d58a7bf724", "name": "Daxin Jiang", "hidden": false}], "publishedAt": "2025-12-17T13:26:30.000Z", "submittedOnDailyAt": "2025-12-18T00:55:26.804Z", "title": "Step-GUI Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "upvotes": 77, "discussionId": "69437418542d62d58a7bf725", "projectPage": "https://opengelab.github.io/", "githubRepo": "https://github.com/stepfun-ai/gelab-zero", "githubRepoAddedBy": "user", "ai_summary": "A self-evolving training pipeline with the Calibrated Step Reward System and GUI-MCP protocol improve GUI automation efficiency, accuracy, and privacy in real-world scenarios.", "ai_keywords": ["multimodal large language models", "GUI automation", "self-evolving training pipeline", "Calibrated Step Reward System", "trajectory-level calibration", "Step-GUI", "GUI performance", "GUI-MCP", "Model Context Protocol", "AndroidWorld", "OSWorld", "ScreenShot-Pro", "AndroidDaily", "real-world mobile usage patterns", "hierarchical architecture", "low-level atomic operations", "high-level task delegation", "local specialist models", "high-privacy execution"], "githubStars": 1417, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u5c55\u4e3a\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u81ea\u52a8\u5316\u5e26\u6765\u4e86\u65b0\u7684\u673a\u4f1a\uff0c\u4f46\u83b7\u53d6\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u4ecd\u7136\u662f\u4e2a\u6311\u6218\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u6211\u6f14\u5316\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4f7f\u7528\u6821\u51c6\u6b65\u5956\u52b1\u7cfb\u7edf\uff0c\u80fd\u4ee5\u4f4e\u6210\u672c\u751f\u6210\u9ad8\u51c6\u786e\u5ea6\u7684\u8bad\u7ec3\u4fe1\u53f7\u3002</li>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86Step-GUI\u6a21\u578b\u7cfb\u5217\uff0c\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u6700\u4f73\u7684GUI\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u901a\u7528\u80fd\u529b\u3002</li>\n    <li>\u4e3a\u4e86\u5b9e\u73b0\u9ad8\u9690\u79c1\u6267\u884c\uff0c\u6211\u4eec\u63d0\u51fa\u4e86GUI-MCP\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8eGUI\u81ea\u52a8\u5316\u7684\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff0c\u80fd\u5728\u8bbe\u5907\u4e0a\u4fdd\u62a4\u7528\u6237\u6570\u636e\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5f15\u5165\u4e86AndroidDaily\u57fa\u51c6\uff0c\u57fa\u4e8e\u771f\u5b9e\u624b\u673a\u4f7f\u7528\u6a21\u5f0f\u8bc4\u4f30\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u63a8\u52a8\u4e86\u5b9e\u9645GUI\u4ee3\u7406\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New techniques are helping to automate graphical user interfaces (GUIs) more effectively.</li>\n    <li>A self-evolving training system improves the quality of training data while reducing costs significantly.</li>\n    <li>Step-GUI models achieve top performance in GUI tasks while remaining generally capable across different tasks.</li>\n    <li>GUI-MCP is introduced as a new protocol that balances task execution with user privacy by keeping sensitive data on devices.</li>\n    <li>AndroidDaily is a benchmark that tests agents using real mobile usage patterns, highlighting their effectiveness in daily scenarios.</li>\n</ul>"}, "publishedAt": "2025-12-17T08:26:30.000Z", "title": "Step-GUI Technical Report", "summary": "Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.15431.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.13586", "authors": [{"_id": "6940d86d65f1e24a117805cd", "user": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "isPro": false, "fullname": "Jia-Nan Li", "user": "JinaLeejnl", "type": "user"}, "name": "Jia-Nan Li", "status": "claimed_verified", "statusLastChangedAt": "2025-12-16T09:38:49.797Z", "hidden": false}, {"_id": "6940d86d65f1e24a117805ce", "name": "Jian Guan", "hidden": false}, {"_id": "6940d86d65f1e24a117805cf", "name": "Wei Wu", "hidden": false}, {"_id": "6940d86d65f1e24a117805d0", "name": "Chongxuan Li", "hidden": false}], "publishedAt": "2025-12-15T17:41:19.000Z", "submittedOnDailyAt": "2025-12-16T01:39:12.101Z", "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "submittedOnDailyBy": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "isPro": false, "fullname": "Jia-Nan Li", "user": "JinaLeejnl", "type": "user"}, "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.", "upvotes": 74, "discussionId": "6940d86d65f1e24a117805d1", "githubRepo": "https://github.com/ML-GSAI/ReFusion", "githubRepoAddedBy": "user", "ai_summary": "ReFusion, a novel masked diffusion model, improves performance and efficiency by using slot-based parallel decoding, achieving superior results compared to autoregressive models and traditional masked diffusion models.", "ai_keywords": ["autoregressive models", "masked diffusion models", "Key-Value caching", "parallel decoding", "token level", "slot level", "diffusion-based planning", "autoregressive infilling", "slot-based design", "slot-level permutation space"], "githubStars": 18, "summary_zh": "<ul>\n    <li>\u81ea\u56de\u5f52\u6a21\u578b\uff08ARMs\uff09\u5b58\u5728\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\u3002</li>\n    <li>\u63a9\u853d\u6269\u6563\u6a21\u578b\uff08MDMs\uff09\u63d0\u4f9b\u4e86\u5e76\u884c\u63a8\u7406\u7684\u65b9\u6848\uff0c\u4f46\u9762\u4e34\u9ad8\u8ba1\u7b97\u5f00\u9500\u548c\u751f\u6210\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002</li>\n    <li>ReFusion \u662f\u4e00\u79cd\u65b0\u9896\u7684\u63a9\u853d\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u5e76\u884c\u89e3\u7801\u63d0\u5347\u5230\u66f4\u9ad8\u7684\u69fd\u4f4d\u7ea7\u522b\u6765\u63d0\u9ad8\u6027\u80fd\u548c\u6548\u7387\u3002</li>\n    <li>ReFusion \u91c7\u7528\u201c\u8ba1\u5212\u4e0e\u586b\u5145\u201d\u7684\u8fed\u4ee3\u89e3\u7801\u8fc7\u7a0b\uff0c\u9996\u5148\u89c4\u5212\u5f31\u4f9d\u8d56\u69fd\u4f4d\uff0c\u7136\u540e\u5e76\u884c\u89e3\u7801\u8fd9\u4e9b\u69fd\u4f4d\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cReFusion \u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684 MDMs\uff0c\u63d0\u5347\u4e86 34%\uff0c\u5e76\u4e14\u5728\u901f\u5ea6\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e86 18 \u500d\uff0c\u540c\u65f6\u4e5f\u7f29\u5c0f\u4e86\u4e0e\u5f3a\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoregressive models (ARMs) are slow at making predictions one by one.</li>\n    <li>Masked diffusion models (MDMs) can work faster but have issues like high computing costs and poor generation quality.</li>\n    <li>ReFusion is a new model that improves speed and performance by using a slot-based approach for decoding.</li>\n    <li>This method allows for better reuse of resources and simplifies the learning process.</li>\n    <li>Tests show ReFusion outperforms previous models by 34% and is over 18 times faster on average, while also being 2.33 times faster than traditional ARMs.</li>\n</ul>"}, "publishedAt": "2025-12-15T12:41:19.000Z", "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18times speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33times average speedup.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.13586.png", "numComments": 2, "submittedBy": {"_id": "67be0888267276f5a2f9ce71", "avatarUrl": "/avatars/c27dc0003351d2e59d7361064d572e55.svg", "fullname": "Jia-Nan Li", "name": "JinaLeejnl", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Jan 02, 2026";
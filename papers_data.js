window.trendingPapers = {
    "today": [{"paper": {"id": "2602.12675", "authors": [{"_id": "69967cd61268a6b79e0d02d9", "user": {"_id": "66c0a08bac74db25de8427ec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg", "isPro": false, "fullname": "Jintao Zhang", "user": "jt-zhang", "type": "user"}, "name": "Jintao Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T09:51:51.054Z", "hidden": false}, {"_id": "69967cd61268a6b79e0d02da", "user": {"_id": "658c1802a1105f8157ad1db9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658c1802a1105f8157ad1db9/WzjY29SkngxkKfiTYcssh.jpeg", "isPro": false, "fullname": "whx1003", "user": "whx1003", "type": "user"}, "name": "Haoxu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T09:51:48.596Z", "hidden": false}, {"_id": "69967cd61268a6b79e0d02db", "name": "Kai Jiang", "hidden": false}, {"_id": "69967cd61268a6b79e0d02dc", "name": "Kaiwen Zheng", "hidden": false}, {"_id": "69967cd61268a6b79e0d02dd", "name": "Youhe Jiang", "hidden": false}, {"_id": "69967cd61268a6b79e0d02de", "name": "Ion Stoica", "hidden": false}, {"_id": "69967cd61268a6b79e0d02df", "name": "Jianfei Chen", "hidden": false}, {"_id": "69967cd61268a6b79e0d02e0", "name": "Jun Zhu", "hidden": false}, {"_id": "69967cd61268a6b79e0d02e1", "name": "Joseph E. Gonzalez", "hidden": false}], "publishedAt": "2026-02-13T07:16:02.000Z", "submittedOnDailyAt": "2026-02-19T00:32:11.371Z", "title": "SLA2: Sparse-Linear Attention with Learnable Routing and QAT", "submittedOnDailyBy": {"_id": "66c0a08bac74db25de8427ec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg", "isPro": false, "fullname": "Jintao Zhang", "user": "jt-zhang", "type": "user"}, "summary": "Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality.", "upvotes": 43, "discussionId": "69967cd61268a6b79e0d02e2", "projectPage": "https://github.com/thu-ml/SLA", "ai_summary": "SLA2 improves sparse-linear attention in diffusion models by introducing a learnable router, direct attention formulation, and quantization-aware fine-tuning for enhanced efficiency and quality.", "ai_keywords": ["sparse-linear attention", "diffusion models", "attention sparsity", "learnable router", "quantization-aware fine-tuning", "attention error", "direct decomposition"], "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "summary_zh": "<ul>\n    <li>\u7a00\u758f\u7ebf\u6027\u6ce8\u610f\u529b\uff08SLA\uff09\u7ed3\u5408\u4e86\u7a00\u758f\u548c\u7ebf\u6027\u6ce8\u610f\u529b\uff0c\u80fd\u591f\u52a0\u901f\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5728\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\u3002</li>\n    <li>SLA\u4f9d\u8d56\u4e8e\u4e00\u79cd\u542f\u53d1\u5f0f\u5206\u5272\u65b9\u6cd5\uff0c\u6839\u636e\u6ce8\u610f\u529b\u6743\u91cd\u7684\u5927\u5c0f\u5c06\u8ba1\u7b97\u5206\u914d\u7ed9\u7a00\u758f\u6216\u7ebf\u6027\u5206\u652f\uff0c\u8fd9\u79cd\u65b9\u6cd5\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u7684\u3002</li>\n    <li>\u901a\u8fc7\u5bf9SLA\u7684\u6ce8\u610f\u529b\u8bef\u5dee\u8fdb\u884c\u5206\u6790\uff0c\u6211\u4eec\u53d1\u73b0SLA\u4e0e\u7a00\u758f\u548c\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u76f4\u63a5\u5206\u89e3\u4e4b\u95f4\u5b58\u5728\u4e0d\u5339\u914d\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86SLA2\uff0c\u5f15\u5165\u4e86\u53ef\u5b66\u4e60\u7684\u8def\u7531\u5668\uff0c\u52a8\u6001\u9009\u62e9\u6bcf\u4e2a\u6ce8\u610f\u529b\u8ba1\u7b97\u4f7f\u7528\u7a00\u758f\u6216\u7ebf\u6027\u6ce8\u610f\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSLA2\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u53ef\u4ee5\u5b9e\u73b097%\u7684\u6ce8\u610f\u529b\u7a00\u758f\u6027\uff0c\u5e76\u63d0\u4f9b18.6\u500d\u7684\u6ce8\u610f\u529b\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Sparse-Linear Attention (SLA) helps speed up video generation but has some limitations in its current method.</li>\n    <li>It uses a fixed rule to decide how to split attention calculations, which may not be the best approach.</li>\n    <li>The researchers found that SLA does not perfectly match a direct split of attention into sparse and linear parts.</li>\n    <li>They developed a new method called SLA2 that features a smart system to choose between sparse and linear attention dynamically.</li>\n    <li>SLA2 achieves 97% attention sparsity and speeds up attention calculations by 18.6 times while maintaining high-quality video generation.</li>\n</ul>"}, "publishedAt": "2026-02-13T02:16:02.000Z", "title": "SLA2: Sparse-Linear Attention with Learnable Routing and QAT", "summary": "Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12675.png", "numComments": 3, "submittedBy": {"_id": "66c0a08bac74db25de8427ec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg", "fullname": "Jintao Zhang", "name": "jt-zhang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 46, "isUserFollowing": false}, "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.14979", "authors": [{"_id": "69968e1e1268a6b79e0d02f1", "user": {"_id": "64731a68a7f23affe7736d3d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8wESKLFcS2ltPzL-wpG4Z.jpeg", "isPro": false, "fullname": "Ronghao Dang", "user": "RH-Dang", "type": "user"}, "name": "Ronghao Dang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-19T10:00:05.563Z", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02f2", "user": {"_id": "66224557c61c7fbd98099079", "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg", "isPro": false, "fullname": "Jiayan Guo", "user": "SpaceProduct", "type": "user"}, "name": "Jiayan Guo", "status": "admin_assigned", "statusLastChangedAt": "2026-02-19T10:00:12.888Z", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02f3", "name": "Bohan Hou", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02f4", "user": {"_id": "609115c79a8bcaa437b234a9", "avatarUrl": "/avatars/1631a91030703d8397133363cf82c863.svg", "isPro": false, "fullname": "Leng Sicong", "user": "Sicong", "type": "user"}, "name": "Sicong Leng", "status": "admin_assigned", "statusLastChangedAt": "2026-02-19T10:00:23.420Z", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02f5", "user": {"_id": "6388af095a3d2a335622cb7c", "avatarUrl": "/avatars/f548ce6a902cee8bdc74179bcd45534c.svg", "isPro": false, "fullname": "Kehan Li", "user": "lkhl", "type": "user"}, "name": "Kehan Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T09:51:43.746Z", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02f6", "name": "Xin Li", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02f7", "user": {"_id": "67a6082b2f32323bfb5e6641", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NZvjm4Kapc7AgReexgRgi.png", "isPro": false, "fullname": "jiangpin", "user": "jiangpinliu", "type": "user"}, "name": "Jiangpin Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-02-19T10:00:30.219Z", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02f8", "user": {"_id": "67fcc97cede5c434e0cc37e3", "avatarUrl": "/avatars/b07e0a4744c1045828a621146ee6d3c2.svg", "isPro": false, "fullname": "yunxuan mao", "user": "maoyunxuan", "type": "user"}, "name": "Yunxuan Mao", "status": "admin_assigned", "statusLastChangedAt": "2026-02-19T10:00:35.800Z", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02f9", "name": "Zhikai Wang", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02fa", "name": "Yuqian Yuan", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02fb", "name": "Minghao Zhu", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02fc", "user": {"_id": "667e2e4ebfbdb7d21df57084", "avatarUrl": "/avatars/dfc6e9e2805c6f22773495c4f02399b8.svg", "isPro": false, "fullname": "Xiao Lin", "user": "Chrislin21", "type": "user"}, "name": "Xiao Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T11:48:50.334Z", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02fd", "name": "Yang Bai", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02fe", "name": "Qian Jiang", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02ff", "name": "Yaxi Zhao", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0300", "name": "Minghua Zeng", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0301", "user": {"_id": "64bbe6904d2052b1aaf4f2d7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bbe6904d2052b1aaf4f2d7/g0A43kSoy2Wba5JZ2Hcry.jpeg", "isPro": false, "fullname": "junlong gao", "user": "jlgao23", "type": "user"}, "name": "Junlong Gao", "status": "admin_assigned", "statusLastChangedAt": "2026-02-19T10:01:11.266Z", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0302", "name": "Yuming Jiang", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0303", "name": "Jun Cen", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0304", "user": {"_id": "65fd82762bf2cd20ddaa193f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png", "isPro": false, "fullname": "Siteng Huang", "user": "huangsiteng", "type": "user"}, "name": "Siteng Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T09:51:46.265Z", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0305", "name": "Liuyi Wang", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0306", "name": "Wenqiao Zhang", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0307", "name": "Chengju Liu", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0308", "name": "Jianfei Yang", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0309", "name": "Shijian Lu", "hidden": false}, {"_id": "69968e1e1268a6b79e0d030a", "name": "Deli Zhao", "hidden": false}], "publishedAt": "2026-02-13T18:59:56.000Z", "submittedOnDailyAt": "2026-02-19T02:00:43.226Z", "title": "RynnBrain: Open Embodied Foundation Models", "submittedOnDailyBy": {"_id": "609115c79a8bcaa437b234a9", "avatarUrl": "/avatars/1631a91030703d8397133363cf82c863.svg", "isPro": false, "fullname": "Leng Sicong", "user": "Sicong", "type": "user"}, "summary": "Despite rapid progress in multimodal foundation models, embodied intelligence community still lacks a unified, physically grounded foundation model that integrates perception, reasoning, and planning within real-world spatial-temporal dynamics. We introduce RynnBrain, an open-source spatiotemporal foundation model for embodied intelligence. RynnBrain strengthens four core capabilities in a unified framework: comprehensive egocentric understanding, diverse spatiotemporal localization, physically grounded reasoning, and physics-aware planning. The RynnBrain family comprises three foundation model scales (2B, 8B, and 30B-A3B MoE) and four post-trained variants tailored for downstream embodied tasks (i.e., RynnBrain-Nav, RynnBrain-Plan, and RynnBrain-VLA) or complex spatial reasoning tasks (i.e., RynnBrain-CoP). In terms of extensive evaluations on 20 embodied benchmarks and 8 general vision understanding benchmarks, our RynnBrain foundation models largely outperform existing embodied foundation models by a significant margin. The post-trained model suite further substantiates two key potentials of the RynnBrain foundation model: (i) enabling physically grounded reasoning and planning, and (ii) serving as a strong pretrained backbone that can be efficiently adapted to diverse embodied tasks.", "upvotes": 26, "discussionId": "69968e1e1268a6b79e0d030b", "projectPage": "https://alibaba-damo-academy.github.io/RynnBrain.github.io/", "githubRepo": "https://github.com/alibaba-damo-academy/RynnBrain", "githubRepoAddedBy": "user", "ai_summary": "RynnBrain is an open-source spatiotemporal foundation model for embodied intelligence that unifies perception, reasoning, and planning capabilities across multiple scales and task-specific variants.", "ai_keywords": ["multimodal foundation models", "embodied intelligence", "spatiotemporal foundation model", "egocentric understanding", "spatiotemporal localization", "physically grounded reasoning", "physics-aware planning", "MoE", "post-trained variants", "embodied foundation models", "vision understanding benchmarks"], "githubStars": 402, "organization": {"_id": "6808e7522a4d69d5111da55f", "name": "Alibaba-DAMO-Academy", "fullname": "DAMO Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"}, "summary_zh": "<ul>\n    <li>RynnBrain\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u589e\u5f3a\u5177\u8eab\u667a\u80fd\u7684\u80fd\u529b\u3002</li>\n    <li>\u5b83\u6574\u5408\u4e86\u611f\u77e5\u3001\u63a8\u7406\u548c\u89c4\u5212\uff0c\u5177\u6709\u5168\u9762\u7684\u81ea\u6211\u4e2d\u5fc3\u7406\u89e3\u548c\u591a\u6837\u7684\u65f6\u7a7a\u5b9a\u4f4d\u80fd\u529b\u3002</li>\n    <li>RynnBrain\u6709\u4e09\u79cd\u57fa\u7840\u6a21\u578b\u89c4\u6a21\u548c\u56db\u79cd\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u7684\u540e\u8bad\u7ec3\u53d8\u4f53\u3002</li>\n    <li>\u572820\u4e2a\u5177\u8eab\u57fa\u51c6\u548c8\u4e2a\u89c6\u89c9\u7406\u89e3\u57fa\u51c6\u7684\u8bc4\u4f30\u4e2d\uff0cRynnBrain\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u652f\u6301\u7269\u7406\u57fa\u7840\u7684\u63a8\u7406\u548c\u89c4\u5212\uff0c\u5e76\u53ef\u9ad8\u6548\u9002\u5e94\u591a\u79cd\u5177\u8eab\u4efb\u52a1\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>RynnBrain is a new open-source model designed to improve embodied intelligence by combining perception, reasoning, and planning.</li>\n    <li>It focuses on four main abilities: understanding one's own perspective, locating objects in space and time, reasoning based on physical laws, and planning using physics.</li>\n    <li>The model comes in three sizes (2B, 8B, and 30B-A3B MoE) and includes four specialized versions for different tasks.</li>\n    <li>RynnBrain outperforms existing models in 20 tests for embodied tasks and 8 tests for general vision understanding.</li>\n    <li>It shows promise for enabling realistic reasoning and planning, and can be adapted for various tasks in embodied intelligence.</li>\n</ul>"}, "publishedAt": "2026-02-13T13:59:56.000Z", "title": "RynnBrain: Open Embodied Foundation Models", "summary": "Despite rapid progress in multimodal foundation models, embodied intelligence community still lacks a unified, physically grounded foundation model that integrates perception, reasoning, and planning within real-world spatial-temporal dynamics. We introduce RynnBrain, an open-source spatiotemporal foundation model for embodied intelligence. RynnBrain strengthens four core capabilities in a unified framework: comprehensive egocentric understanding, diverse spatiotemporal localization, physically grounded reasoning, and physics-aware planning. The RynnBrain family comprises three foundation model scales (2B, 8B, and 30B-A3B MoE) and four post-trained variants tailored for downstream embodied tasks (i.e., RynnBrain-Nav, RynnBrain-Plan, and RynnBrain-VLA) or complex spatial reasoning tasks (i.e., RynnBrain-CoP). In terms of extensive evaluations on 20 embodied benchmarks and 8 general vision understanding benchmarks, our RynnBrain foundation models largely outperform existing embodied foundation models by a significant margin. The post-trained model suite further substantiates two key potentials of the RynnBrain foundation model: (i) enabling physically grounded reasoning and planning, and (ii) serving as a strong pretrained backbone that can be efficiently adapted to diverse embodied tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14979.png", "numComments": 3, "submittedBy": {"_id": "609115c79a8bcaa437b234a9", "avatarUrl": "/avatars/1631a91030703d8397133363cf82c863.svg", "fullname": "Leng Sicong", "name": "Sicong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "6808e7522a4d69d5111da55f", "name": "Alibaba-DAMO-Academy", "fullname": "DAMO Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.16705", "authors": [{"_id": "69967a291268a6b79e0d02bb", "user": {"_id": "6201fc5d91d53938a6432fbf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg", "isPro": false, "fullname": "Runpei Dong", "user": "RunpeiDong", "type": "user"}, "name": "Runpei Dong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T09:51:52.994Z", "hidden": false}, {"_id": "69967a291268a6b79e0d02bc", "name": "Ziyan Li", "hidden": false}, {"_id": "69967a291268a6b79e0d02bd", "name": "Xialin He", "hidden": false}, {"_id": "69967a291268a6b79e0d02be", "name": "Saurabh Gupta", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/g69sakZn_StFwG0neUtqz.mp4"], "publishedAt": "2026-02-18T18:55:02.000Z", "submittedOnDailyAt": "2026-02-19T00:21:31.776Z", "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation", "submittedOnDailyBy": {"_id": "6201fc5d91d53938a6432fbf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg", "isPro": false, "fullname": "Runpei Dong", "user": "RunpeiDong", "type": "user"}, "summary": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.", "upvotes": 25, "discussionId": "69967a2a1268a6b79e0d02bf", "projectPage": "https://hero-humanoid.github.io/", "ai_summary": "HERO enables humanoid robots to perform object manipulation in diverse real-world environments by combining accurate end-effector control with open-vocabulary vision models for generalizable scene understanding.", "ai_keywords": ["end-effector tracking policy", "inverse kinematics", "neural forward model", "goal adjustment", "replanning", "open-vocabulary large vision models", "loco-manipulation", "visual generalization"], "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5HERO\uff0c\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u64cd\u63a7\u5404\u79cd\u7269\u4f53\u3002</li>\n    <li>\u7ed3\u5408\u4e86\u5927\u578b\u89c6\u89c9\u6a21\u578b\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u6a21\u62df\u8bad\u7ec3\u7684\u5f3a\u63a7\u5236\u6027\u80fd\u3002</li>\n    <li>\u8bbe\u8ba1\u4e86\u4e00\u79cd\u51c6\u786e\u7684\u672b\u7aef\u6267\u884c\u5668\u8ddf\u8e2a\u7b56\u7565\uff0c\u51cf\u5c11\u4e86\u8ddf\u8e2a\u8bef\u5dee3.2\u500d\u3002</li>\n    <li>\u7cfb\u7edf\u80fd\u591f\u5728\u4e0d\u540c\u7684\u73b0\u5b9e\u73af\u5883\u4e2d\u5de5\u4f5c\uff0c\u6bd4\u5982\u529e\u516c\u5ba4\u548c\u5496\u5561\u5e97\uff0c\u53ef\u9760\u5730\u64cd\u63a7\u65e5\u5e38\u7269\u54c1\u3002</li>\n    <li>\u901a\u8fc7\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u8be5\u8bbe\u8ba1\u7684\u6709\u6548\u6027\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u4e0e\u65e5\u5e38\u7269\u4f53\u7684\u4e92\u52a8\u5f00\u8f9f\u4e86\u65b0\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper introduces HERO, a new method for humanoid robots to manipulate objects accurately in various environments.</li>\n    <li>HERO combines advanced visual understanding from large vision models with effective control techniques from simulations.</li>\n    <li>It features a special end-effector tracking policy that significantly reduces tracking errors by 3.2 times.</li>\n    <li>The system can work in different real-world settings, effectively handling common objects like mugs and toys.</li>\n    <li>Tests show that HERO is effective both in simulations and in real-world applications, potentially improving how robots interact with everyday items.</li>\n</ul>"}, "publishedAt": "2026-02-18T13:55:02.000Z", "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation", "summary": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/g69sakZn_StFwG0neUtqz.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16705.png", "numComments": 2, "submittedBy": {"_id": "6201fc5d91d53938a6432fbf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg", "fullname": "Runpei Dong", "name": "RunpeiDong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.16317", "authors": [{"_id": "6996e36c2b6c6794ff97a40f", "name": "Maksim Elistratov", "hidden": false}, {"_id": "6996e36c2b6c6794ff97a410", "name": "Marina Barannikov", "hidden": false}, {"_id": "6996e36c2b6c6794ff97a411", "name": "Gregory Ivanov", "hidden": false}, {"_id": "6996e36c2b6c6794ff97a412", "name": "Valentin Khrulkov", "hidden": false}, {"_id": "6996e36c2b6c6794ff97a413", "name": "Anton Konushin", "hidden": false}, {"_id": "6996e36c2b6c6794ff97a414", "name": "Andrey Kuznetsov", "hidden": false}, {"_id": "6996e36c2b6c6794ff97a415", "user": {"_id": "67d5a331eab66ce9cb01bae4", "avatarUrl": "/avatars/3ed437c874889e8a6db66c3ef88a60c0.svg", "isPro": false, "fullname": "DMITRII ZHEMCHUZHNIKOV", "user": "zhemchuzhnikov", "type": "user"}, "name": "Dmitrii Zhemchuzhnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T11:48:52.062Z", "hidden": false}], "publishedAt": "2026-02-18T09:54:57.000Z", "submittedOnDailyAt": "2026-02-19T11:41:42.311Z", "title": "CADEvolve: Creating Realistic CAD via Program Evolution", "submittedOnDailyBy": {"_id": "67d5a331eab66ce9cb01bae4", "avatarUrl": "/avatars/3ed437c874889e8a6db66c3ef88a60c0.svg", "isPro": false, "fullname": "DMITRII ZHEMCHUZHNIKOV", "user": "zhemchuzhnikov", "type": "user"}, "summary": "Computer-Aided Design (CAD) delivers rapid, editable modeling for engineering and manufacturing. Recent AI progress now makes full automation feasible for various CAD tasks. However, progress is bottlenecked by data: public corpora mostly contain sketch-extrude sequences, lack complex operations, multi-operation composition and design intent, and thus hinder effective fine-tuning. Attempts to bypass this with frozen VLMs often yield simple or invalid programs due to limited 3D grounding in current foundation models. We present CADEvolve, an evolution-based pipeline and dataset that starts from simple primitives and, via VLM-guided edits and validations, incrementally grows CAD programs toward industrial-grade complexity. The result is 8k complex parts expressed as executable CadQuery parametric generators. After multi-stage post-processing and augmentation, we obtain a unified dataset of 1.3m scripts paired with rendered geometry and exercising the full CadQuery operation set. A VLM fine-tuned on CADEvolve achieves state-of-the-art results on the Image2CAD task across the DeepCAD, Fusion 360, and MCB benchmarks.", "upvotes": 19, "discussionId": "6996e36d2b6c6794ff97a416", "githubRepo": "https://github.com/zhemdi/CADEvolve", "githubRepoAddedBy": "user", "ai_summary": "CADEvolve presents an evolution-based approach using VLM-guided edits to generate complex CAD programs from simple primitives, creating a large dataset for improved Image2CAD performance.", "ai_keywords": ["CAD", "VLM", "evolution-based pipeline", "parametric generators", "CadQuery", "Image2CAD", "DeepCAD", "Fusion 360", "MCB"], "githubStars": 7, "summary_zh": "<ul>\n    <li>\u8ba1\u7b97\u673a\u8f85\u52a9\u8bbe\u8ba1\uff08CAD\uff09\u53ef\u4ee5\u5feb\u901f\u3001\u53ef\u7f16\u8f91\u5730\u8fdb\u884c\u5de5\u7a0b\u548c\u5236\u9020\u5efa\u6a21\u3002</li>\n    <li>\u6700\u8fd1\u7684\u4eba\u5de5\u667a\u80fd\u8fdb\u5c55\u4f7f\u5f97CAD\u4efb\u52a1\u7684\u5168\u81ea\u52a8\u5316\u6210\u4e3a\u53ef\u80fd\uff0c\u4f46\u6570\u636e\u4e0d\u8db3\u6210\u4e3a\u74f6\u9888\u3002</li>\n    <li>\u73b0\u6709\u516c\u5171\u6570\u636e\u96c6\u4e3b\u8981\u5305\u542b\u7b80\u5355\u7684\u8349\u56fe\u6324\u51fa\u5e8f\u5217\uff0c\u7f3a\u4e4f\u590d\u6742\u64cd\u4f5c\u548c\u8bbe\u8ba1\u610f\u56fe\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u6709\u6548\u5fae\u8c03\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u7684CADEvolve\u662f\u4e00\u4e2a\u57fa\u4e8e\u8fdb\u5316\u7684\u7ba1\u9053\u548c\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u9010\u6b65\u5f15\u5bfc\u7f16\u8f91\u548c\u9a8c\u8bc1\uff0c\u9010\u6e10\u6784\u5efa\u590d\u6742\u7684CAD\u7a0b\u5e8f\u3002</li>\n    <li>\u7ecf\u8fc7\u540e\u5904\u7406\u548c\u589e\u5f3a\uff0c\u6700\u7ec8\u5f97\u5230\u4e00\u4e2a\u5305\u542b130\u4e07\u811a\u672c\u548c\u51e0\u4f55\u56fe\u5f62\u7684\u7edf\u4e00\u6570\u636e\u96c6\uff0c\u4f18\u5316\u4e86Image2CAD\u4efb\u52a1\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Computer-Aided Design (CAD) is becoming more automated thanks to advancements in AI.</li>\n    <li>Current data available for training AI in CAD is limited and lacks complexity, making it hard to improve AI models.</li>\n    <li>CADEvolve is a new system that builds CAD programs gradually, starting with simple designs and improving them through guided edits.</li>\n    <li>This project created a large dataset of 1.3 million scripts with various CAD operations and rendered designs.</li>\n    <li>A model trained on this dataset has achieved top performance on several CAD-related tasks and benchmarks.</li>\n</ul>"}, "publishedAt": "2026-02-18T04:54:57.000Z", "title": "CADEvolve: Creating Realistic CAD via Program Evolution", "summary": "Computer-Aided Design (CAD) delivers rapid, editable modeling for engineering and manufacturing. Recent AI progress now makes full automation feasible for various CAD tasks. However, progress is bottlenecked by data: public corpora mostly contain sketch-extrude sequences, lack complex operations, multi-operation composition and design intent, and thus hinder effective fine-tuning. Attempts to bypass this with frozen VLMs often yield simple or invalid programs due to limited 3D grounding in current foundation models. We present CADEvolve, an evolution-based pipeline and dataset that starts from simple primitives and, via VLM-guided edits and validations, incrementally grows CAD programs toward industrial-grade complexity. The result is 8k complex parts expressed as executable CadQuery parametric generators. After multi-stage post-processing and augmentation, we obtain a unified dataset of 1.3m scripts paired with rendered geometry and exercising the full CadQuery operation set. A VLM fine-tuned on CADEvolve achieves state-of-the-art results on the Image2CAD task across the DeepCAD, Fusion 360, and MCB benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16317.png", "numComments": 2, "submittedBy": {"_id": "67d5a331eab66ce9cb01bae4", "avatarUrl": "/avatars/3ed437c874889e8a6db66c3ef88a60c0.svg", "fullname": "DMITRII ZHEMCHUZHNIKOV", "name": "zhemchuzhnikov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.14080", "authors": [{"_id": "699694c21268a6b79e0d031a", "name": "Nitay Calderon", "hidden": false}, {"_id": "699694c21268a6b79e0d031b", "name": "Eyal Ben-David", "hidden": false}, {"_id": "699694c21268a6b79e0d031c", "name": "Zorik Gekhman", "hidden": false}, {"_id": "699694c21268a6b79e0d031d", "name": "Eran Ofek", "hidden": false}, {"_id": "699694c21268a6b79e0d031e", "name": "Gal Yona", "hidden": false}], "publishedAt": "2026-02-15T10:13:30.000Z", "submittedOnDailyAt": "2026-02-19T02:14:32.347Z", "title": "Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality", "submittedOnDailyBy": {"_id": "62d6a0c18faee0ac953c51fa", "avatarUrl": "/avatars/ca818cebdb089a8d853c5bc4d5e0987b.svg", "isPro": false, "fullname": "Nitay Calderon", "user": "nitay", "type": "user"}, "summary": "Standard factuality evaluations of LLMs treat all errors alike, obscuring whether failures arise from missing knowledge (empty shelves) or from limited access to encoded facts (lost keys). We propose a behavioral framework that profiles factual knowledge at the level of facts rather than questions, characterizing each fact by whether it is encoded, and then by how accessible it is: cannot be recalled, can be directly recalled, or can only be recalled with inference-time computation (thinking). To support such profiling, we introduce WikiProfile, a new benchmark constructed via an automated pipeline with a prompted LLM grounded in web search. Across 4 million responses from 13 LLMs, we find that encoding is nearly saturated in frontier models on our benchmark, with GPT-5 and Gemini-3 encoding 95--98% of facts. However, recall remains a major bottleneck: many errors previously attributed to missing knowledge instead stem from failures to access it. These failures are systematic and disproportionately affect long-tail facts and reverse questions. Finally, we show that thinking improves recall and can recover a substantial fraction of failures, indicating that future gains may rely less on scaling and more on methods that improve how models utilize what they already encode.", "upvotes": 16, "discussionId": "699694c21268a6b79e0d031f", "ai_summary": "LLMs demonstrate near-complete factual encoding but struggle with retrieval accessibility, where errors stem from access limitations rather than knowledge gaps, with reasoning improving recall of encoded information.", "ai_keywords": ["factuality evaluations", "LLMs", "factual knowledge", "encoded facts", "recall accessibility", "long-tail facts", "reverse questions", "thinking", "reasoning", "encoding saturation"], "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "summary_zh": "<ul>\n    <li>\u6807\u51c6\u7684\u4e8b\u5b9e\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u80fd\u533a\u5206\u9519\u8bef\u662f\u7531\u4e8e\u7f3a\u4e4f\u77e5\u8bc6\u8fd8\u662f\u7531\u4e8e\u4fe1\u606f\u8bbf\u95ee\u53d7\u9650\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u884c\u4e3a\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u5b9e\u800c\u975e\u95ee\u9898\u6765\u5206\u6790\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u8003\u8651\u6bcf\u4e2a\u4e8b\u5b9e\u7684\u7f16\u7801\u548c\u53ef\u8bbf\u95ee\u6027\u3002</li>\n    <li>\u5f15\u5165\u4e86WikiProfile\u57fa\u51c6\uff0c\u4f7f\u7528\u81ea\u52a8\u5316\u6d41\u7a0b\u548c\u7f51\u7edc\u641c\u7d22\u6784\u5efa\uff0c\u53ef\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8868\u73b0\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u524d\u6cbf\u6a21\u578b\u7684\u7f16\u7801\u80fd\u529b\u51e0\u4e4e\u8fbe\u5230\u9971\u548c\uff0c\u4f46\u56de\u5fc6\u80fd\u529b\u4ecd\u7136\u662f\u4e00\u4e2a\u4e3b\u8981\u74f6\u9888\u3002</li>\n    <li>\u601d\u8003\u53ef\u4ee5\u63d0\u9ad8\u56de\u5fc6\u80fd\u529b\uff0c\u5e76\u5e2e\u52a9\u6a21\u578b\u66f4\u597d\u5730\u5229\u7528\u5df2\u7f16\u7801\u7684\u4fe1\u606f\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6269\u5c55\u6a21\u578b\u89c4\u6a21\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current evaluations of language models (LLMs) don't distinguish between different types of errors related to knowledge access.</li>\n    <li>We propose a new framework to assess factual knowledge based on whether facts are encoded and how accessible they are.</li>\n    <li>We created a benchmark called WikiProfile using an automated process that involves a language model and web search.</li>\n    <li>Results show that top models like GPT-5 and Gemini-3 encode a high percentage of facts (95-98%), but have trouble recalling them.</li>\n    <li>Improving recall through thinking can recover many errors, suggesting future improvements should focus on better accessing facts rather than just increasing knowledge. </li>\n</ul>"}, "publishedAt": "2026-02-15T05:13:30.000Z", "title": "Empty Shelves or Lost Keys? Recall Is the Bottleneck for Parametric Factuality", "summary": "Standard factuality evaluations of LLMs treat all errors alike, obscuring whether failures arise from missing knowledge (empty shelves) or from limited access to encoded facts (lost keys). We propose a behavioral framework that profiles factual knowledge at the level of facts rather than questions, characterizing each fact by whether it is encoded, and then by how accessible it is: cannot be recalled, can be directly recalled, or can only be recalled with inference-time computation (thinking). To support such profiling, we introduce WikiProfile, a new benchmark constructed via an automated pipeline with a prompted LLM grounded in web search. Across 4 million responses from 13 LLMs, we find that encoding is nearly saturated in frontier models on our benchmark, with GPT-5 and Gemini-3 encoding 95--98% of facts. However, recall remains a major bottleneck: many errors previously attributed to missing knowledge instead stem from failures to access it. These failures are systematic and disproportionately affect long-tail facts and reverse questions. Finally, we show that thinking improves recall and can recover a substantial fraction of failures, indicating that future gains may rely less on scaling and more on methods that improve how models utilize what they already encode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14080.png", "numComments": 2, "submittedBy": {"_id": "62d6a0c18faee0ac953c51fa", "avatarUrl": "/avatars/ca818cebdb089a8d853c5bc4d5e0987b.svg", "fullname": "Nitay Calderon", "name": "nitay", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.16008", "authors": [{"_id": "6996e55577d9294348451e19", "user": {"_id": "6671be9ff022d14aa10df864", "avatarUrl": "/avatars/dd085abefa38c1604dc2ceabf472816d.svg", "isPro": false, "fullname": "Adnan El Assadi", "user": "AdnanElAssadi", "type": "user"}, "name": "Adnan El Assadi", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T14:42:12.758Z", "hidden": false}, {"_id": "6996e55577d9294348451e1a", "user": {"_id": "64cc0e80a257a3212c0c4b24", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64cc0e80a257a3212c0c4b24/wqs6WZN8-3OQthcnQXgN7.png", "isPro": false, "fullname": "Isaac Chung", "user": "isaacchung", "type": "user"}, "name": "Isaac Chung", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T11:04:09.879Z", "hidden": false}, {"_id": "6996e55577d9294348451e1b", "name": "Chenghao Xiao", "hidden": false}, {"_id": "6996e55577d9294348451e1c", "user": {"_id": "61af4544d691b3aadd1f62b6", "avatarUrl": "/avatars/7a4067accdd1005f78c3c4adad3ee0a5.svg", "isPro": false, "fullname": "Solomatin Roman", "user": "Samoed", "type": "user"}, "name": "Roman Solomatin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T11:04:05.803Z", "hidden": false}, {"_id": "6996e55577d9294348451e1d", "name": "Animesh Jha", "hidden": false}, {"_id": "6996e55577d9294348451e1e", "name": "Rahul Chand", "hidden": false}, {"_id": "6996e55577d9294348451e1f", "name": "Silky Singh", "hidden": false}, {"_id": "6996e55577d9294348451e20", "user": {"_id": "67d0ea5d94e08c03254dfb56", "avatarUrl": "/avatars/8e511f5bcebfa869e00bc644e9957162.svg", "isPro": false, "fullname": "kaitlyn wang", "user": "wangusbeef", "type": "user"}, "name": "Kaitlyn Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T14:41:41.369Z", "hidden": false}, {"_id": "6996e55577d9294348451e21", "name": "Ali Sartaz Khan", "hidden": false}, {"_id": "6996e55577d9294348451e22", "name": "Marc Moussa Nasser", "hidden": false}, {"_id": "6996e55577d9294348451e23", "name": "Sufen Fong", "hidden": false}, {"_id": "6996e55577d9294348451e24", "name": "Pengfei He", "hidden": false}, {"_id": "6996e55577d9294348451e25", "name": "Alan Xiao", "hidden": false}, {"_id": "6996e55577d9294348451e26", "user": {"_id": "6754994f0a4a1144aec6ef57", "avatarUrl": "/avatars/9dc00280582bcb0ace57cb34d25e91a0.svg", "isPro": false, "fullname": "Ayush Sunil Munot", "user": "AyushM6", "type": "user"}, "name": "Ayush Sunil Munot", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T11:04:07.782Z", "hidden": false}, {"_id": "6996e55577d9294348451e27", "name": "Aditya Shrivastava", "hidden": false}, {"_id": "6996e55577d9294348451e28", "name": "Artem Gazizov", "hidden": false}, {"_id": "6996e55577d9294348451e29", "name": "Niklas Muennighoff", "hidden": false}, {"_id": "6996e55577d9294348451e2a", "name": "Kenneth Enevoldsen", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/61af4544d691b3aadd1f62b6/vkcutGZImTRS5bB6Bk_uY.png"], "publishedAt": "2026-02-17T21:00:51.000Z", "submittedOnDailyAt": "2026-02-19T07:59:30.545Z", "title": "MAEB: Massive Audio Embedding Benchmark", "submittedOnDailyBy": {"_id": "61af4544d691b3aadd1f62b6", "avatarUrl": "/avatars/7a4067accdd1005f78c3c4adad3ee0a5.svg", "isPro": false, "fullname": "Solomatin Roman", "user": "Samoed", "type": "user"}, "summary": "We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb.", "upvotes": 12, "discussionId": "6996e55577d9294348451e2b", "projectPage": "http://mteb-leaderboard.hf.space/?benchmark_name=MAEB%28beta%29", "ai_summary": "MAEB is a large-scale audio benchmark evaluating 50+ models across 30 tasks in speech, music, and environmental sounds, revealing diverse model strengths and establishing correlations with audio LLM performance.", "ai_keywords": ["audio embedding benchmark", "audio-text reasoning", "multilingual speech tasks", "acoustic understanding", "linguistic tasks", "audio encoders", "audio large language models", "MTEB ecosystem"], "organization": {"_id": "624bfda5459c48438cc39f80", "name": "mteb", "fullname": "Massive Text Embedding Benchmark", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5ff5943752c26e9bc240bada/OrZxdlg8doDNO2TZ6Q58G.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63a8\u51fa\u4e86\u5927\u89c4\u6a21\u7684\u97f3\u9891\u5d4c\u5165\u57fa\u51c6\uff08MAEB\uff09\uff0c\u6db5\u76d630\u4e2a\u4efb\u52a1\uff0c\u5305\u62ec\u8bed\u97f3\u3001\u97f3\u4e50\u3001\u73af\u5883\u58f0\u97f3\u548c\u97f3\u9891-\u6587\u672c\u63a8\u7406\uff0c\u5171\u6d89\u53ca100\u591a\u79cd\u8bed\u8a00\u3002</li>\n    <li>\u8bc4\u4f30\u4e8650\u591a\u4e2a\u6a21\u578b\uff0c\u53d1\u73b0\u6ca1\u6709\u4e00\u4e2a\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u4f18\u5f02\uff1a\u5bf9\u6bd4\u97f3\u9891-\u6587\u672c\u6a21\u578b\u5728\u73af\u5883\u58f0\u97f3\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u591a\u8bed\u8a00\u8bed\u97f3\u4efb\u52a1\u4e2d\u8868\u73b0\u63a5\u8fd1\u968f\u673a\u3002</li>\n    <li>\u805a\u7c7b\u4efb\u52a1\u5bf9\u6240\u6709\u6a21\u578b\u6765\u8bf4\u90fd\u5f88\u5177\u6311\u6218\u6027\uff0c\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\u4e5f\u53ea\u53d6\u5f97\u4e86\u9002\u5ea6\u7684\u7ed3\u679c\u3002</li>\n    <li>\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u5728\u58f0\u5b66\u7406\u89e3\u4e0a\u8868\u73b0\u4f18\u79c0\u7684\u6a21\u578b\u5728\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u53cd\u4e4b\u4ea6\u7136\u3002</li>\n    <li>MAEB\u4ece\u5305\u542b98\u4e2a\u4efb\u52a1\u7684MAEB+\u884d\u751f\u800c\u6765\uff0c\u65e8\u5728\u4fdd\u6301\u4efb\u52a1\u7684\u591a\u6837\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u8bc4\u4f30\u6210\u672c\uff0c\u5e76\u4e0eMTEB\u751f\u6001\u7cfb\u7edf\u96c6\u6210\uff0c\u63d0\u4f9b\u7edf\u4e00\u8bc4\u4f30\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The Massive Audio Embedding Benchmark (MAEB) is a large test suite with 30 tasks related to audio in over 100 languages, including speech and music.</li>\n    <li>We tested over 50 models and found that no single model performs best on all tasks; some models are good at specific tasks but not others.</li>\n    <li>Clustering tasks are difficult for all models, with even the best ones showing only moderate success.</li>\n    <li>Models that do well with sound understanding often do poorly with language tasks, and vice versa.</li>\n    <li>MAEB is part of a larger collection called MAEB+ and is designed to save on evaluation costs while allowing for diverse audio task assessments.</li>\n</ul>"}, "publishedAt": "2026-02-17T16:00:51.000Z", "title": "MAEB: Massive Audio Embedding Benchmark", "summary": "We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/61af4544d691b3aadd1f62b6/vkcutGZImTRS5bB6Bk_uY.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16008.png", "numComments": 1, "submittedBy": {"_id": "61af4544d691b3aadd1f62b6", "avatarUrl": "/avatars/7a4067accdd1005f78c3c4adad3ee0a5.svg", "fullname": "Solomatin Roman", "name": "Samoed", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 45, "isUserFollowing": false}, "organization": {"_id": "624bfda5459c48438cc39f80", "name": "mteb", "fullname": "Massive Text Embedding Benchmark", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5ff5943752c26e9bc240bada/OrZxdlg8doDNO2TZ6Q58G.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.16666", "authors": [{"_id": "69967a061268a6b79e0d02b3", "name": "Stephan Rabanser", "hidden": false}, {"_id": "69967a061268a6b79e0d02b4", "name": "Sayash Kapoor", "hidden": false}, {"_id": "69967a061268a6b79e0d02b5", "name": "Peter Kirgis", "hidden": false}, {"_id": "69967a061268a6b79e0d02b6", "name": "Kangheng Liu", "hidden": false}, {"_id": "69967a061268a6b79e0d02b7", "name": "Saiteja Utpala", "hidden": false}, {"_id": "69967a061268a6b79e0d02b8", "name": "Arvind Narayanan", "hidden": false}], "publishedAt": "2026-02-18T18:05:44.000Z", "submittedOnDailyAt": "2026-02-19T00:18:48.136Z", "title": "Towards a Science of AI Agent Reliability", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.", "upvotes": 11, "discussionId": "69967a071268a6b79e0d02b9", "projectPage": "https://hal.cs.princeton.edu/reliability", "ai_summary": "Traditional benchmark evaluations of AI agents fail to capture critical reliability issues, prompting the development of comprehensive metrics that assess consistency, robustness, predictability, and safety across multiple dimensions.", "ai_keywords": ["AI agents", "reliability", "consistency", "robustness", "predictability", "safety", "performance profiling", "benchmark evaluation"], "organization": {"_id": "64374111a701a7e744c02b0e", "name": "princetonu", "fullname": "Princeton University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/b3xXusq8Zz3ej8Z6fRTSZ.png"}, "summary_zh": "<ul>\n    <li>AI\u4ee3\u7406\u88ab\u5e7f\u6cdb\u7528\u4e8e\u6267\u884c\u91cd\u8981\u4efb\u52a1\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u7136\u5b58\u5728\u5931\u8d25\u7684\u60c5\u51b5\u3002</li>\n    <li>\u76ee\u524d\u7684\u8bc4\u4f30\u65b9\u6cd5\u8fc7\u4e8e\u7b80\u5355\uff0c\u53ea\u7528\u4e00\u4e2a\u6210\u529f\u6307\u6807\u6765\u8861\u91cf\u4ee3\u7406\u884c\u4e3a\uff0c\u5ffd\u89c6\u4e86\u91cd\u8981\u7684\u64cd\u4f5c\u7f3a\u9677\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e8612\u4e2a\u5177\u4f53\u6307\u6807\uff0c\u8bc4\u4f30\u4ee3\u7406\u5728\u4e00\u81f4\u6027\u3001\u7a33\u5065\u6027\u3001\u53ef\u9884\u6d4b\u6027\u548c\u5b89\u5168\u6027\u7b49\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\u4e0a\u7684\u53ef\u9760\u6027\u3002</li>\n    <li>\u5bf914\u4e2a\u4ee3\u7406\u6a21\u578b\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u5c3d\u7ba1\u80fd\u529b\u6709\u6240\u63d0\u5347\uff0c\u4f46\u53ef\u9760\u6027\u6539\u8fdb\u4ecd\u7136\u6709\u9650\u3002</li>\n    <li>\u6211\u4eec\u7684\u6307\u6807\u53ef\u4ee5\u8865\u5145\u4f20\u7edf\u8bc4\u4f30\uff0c\u5e2e\u52a9\u7406\u89e3\u4ee3\u7406\u7684\u8868\u73b0\u3001\u9000\u5316\u548c\u5931\u8d25\u60c5\u51b5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>AI agents are being used for important tasks, but many still fail in real-world situations despite high accuracy in tests.</li>\n    <li>Current evaluations of AI agents often focus on a single success score, which can hide important issues with their performance.</li>\n    <li>Key aspects like consistency, robustness, predictability, and safety are not adequately assessed in standard evaluations.</li>\n    <li>The study introduces twelve new metrics to better evaluate the reliability of AI agents across these key areas.</li>\n    <li>Tests on 14 different AI models show that recent improvements in their abilities have not significantly increased their reliability.</li>\n</ul>"}, "publishedAt": "2026-02-18T13:05:44.000Z", "title": "Towards a Science of AI Agent Reliability", "summary": "AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16666.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 235, "isUserFollowing": false}, "organization": {"_id": "64374111a701a7e744c02b0e", "name": "princetonu", "fullname": "Princeton University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/b3xXusq8Zz3ej8Z6fRTSZ.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.16301", "authors": [{"_id": "69967b821268a6b79e0d02c1", "name": "Marissa A. Weis", "hidden": false}, {"_id": "69967b821268a6b79e0d02c2", "name": "Maciej Wo\u0142czyk", "hidden": false}, {"_id": "69967b821268a6b79e0d02c3", "name": "Rajai Nasser", "hidden": false}, {"_id": "69967b821268a6b79e0d02c4", "name": "Rif A. Saurous", "hidden": false}, {"_id": "69967b821268a6b79e0d02c5", "name": "Blaise Ag\u00fcera y Arcas", "hidden": false}, {"_id": "69967b821268a6b79e0d02c6", "name": "Jo\u00e3o Sacramento", "hidden": false}, {"_id": "69967b821268a6b79e0d02c7", "name": "Alexander Meulemans", "hidden": false}], "publishedAt": "2026-02-18T09:31:43.000Z", "submittedOnDailyAt": "2026-02-19T00:25:09.660Z", "title": "Multi-agent cooperation through in-context co-player inference", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between \"learning-aware\" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between \"naive learners\" updating on fast timescales and \"meta-learners\" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.", "upvotes": 10, "discussionId": "69967b821268a6b79e0d02c8", "ai_summary": "Sequence models enable cooperative behavior emergence in multi-agent reinforcement learning through in-context learning without hardcoded assumptions or timescale separation.", "ai_keywords": ["multi-agent reinforcement learning", "sequence models", "in-context learning", "cooperative behavior", "learning-aware agents", "learning dynamics", "fast timescale", "meta-learners", "decentralized reinforcement learning", "co-player diversity"], "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "summary_zh": "<ul>\n    <li>\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u4fc3\u6210\u81ea\u5229\u4ee3\u7406\u4e4b\u95f4\u7684\u5408\u4f5c\u4ecd\u7136\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002</li>\n    <li>\u901a\u8fc7\u201c\u5b66\u4e60\u610f\u8bc6\u201d\u4ee3\u7406\uff0c\u53ef\u4ee5\u5f15\u5bfc\u76f8\u4e92\u5408\u4f5c\uff0c\u8fd9\u4e9b\u4ee3\u7406\u8003\u8651\u5e76\u5f71\u54cd\u5176\u5408\u4f5c\u4f19\u4f34\u7684\u5b66\u4e60\u52a8\u6001\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u4e8e\u786c\u7f16\u7801\u7684\u5047\u8bbe\uff0c\u6216\u8005\u4e25\u683c\u533a\u5206\u201c\u5929\u771f\u5b66\u4e60\u8005\u201d\u548c\u201c\u5143\u5b66\u4e60\u8005\u201d\u3002</li>\n    <li>\u6211\u4eec\u5c55\u793a\u4e86\u5e8f\u5217\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5408\u4f5c\u4f19\u4f34\u5b66\u4e60\u610f\u8bc6\uff0c\u800c\u4e0d\u9700\u8981\u786c\u7f16\u7801\u5047\u8bbe\u6216\u660e\u786e\u7684\u65f6\u95f4\u5c3a\u5ea6\u5206\u79bb\u3002</li>\n    <li>\u7ed3\u679c\u8868\u660e\uff0c\u6807\u51c6\u7684\u53bb\u4e2d\u5fc3\u5316\u5f3a\u5316\u5b66\u4e60\u4e0e\u5408\u4f5c\u4f19\u4f34\u591a\u6837\u6027\u7ed3\u5408\uff0c\u4e3a\u5b66\u4e60\u5408\u4f5c\u884c\u4e3a\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Cooperation among self-interested agents is a major challenge in multi-agent reinforcement learning.</li>\n    <li>Recent methods have shown that agents can cooperate better if they consider each other's learning processes.</li>\n    <li>Existing approaches often make fixed assumptions about how other agents learn or separate different types of learners.</li>\n    <li>This study demonstrates that sequence models can enable agents to learn about co-players without these rigid assumptions.</li>\n    <li>Training with diverse co-players leads to cooperative behavior by naturally adapting to each other's learning strategies.</li>\n</ul>"}, "publishedAt": "2026-02-18T04:31:43.000Z", "title": "Multi-agent cooperation through in-context co-player inference", "summary": "Achieving cooperation among self-interested agents remains a fundamental challenge in multi-agent reinforcement learning. Recent work showed that mutual cooperation can be induced between \"learning-aware\" agents that account for and shape the learning dynamics of their co-players. However, existing approaches typically rely on hardcoded, often inconsistent, assumptions about co-player learning rules or enforce a strict separation between \"naive learners\" updating on fast timescales and \"meta-learners\" observing these updates. Here, we demonstrate that the in-context learning capabilities of sequence models allow for co-player learning awareness without requiring hardcoded assumptions or explicit timescale separation. We show that training sequence model agents against a diverse distribution of co-players naturally induces in-context best-response strategies, effectively functioning as learning algorithms on the fast intra-episode timescale. We find that the cooperative mechanism identified in prior work-where vulnerability to extortion drives mutual shaping-emerges naturally in this setting: in-context adaptation renders agents vulnerable to extortion, and the resulting mutual pressure to shape the opponent's in-context learning dynamics resolves into the learning of cooperative behavior. Our results suggest that standard decentralized reinforcement learning on sequence models combined with co-player diversity provides a scalable path to learning cooperative behaviors.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16301.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 235, "isUserFollowing": false}, "organization": {"_id": "5e6aca39878b8b2bf9806447", "name": "google", "fullname": "Google", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/WtA3YYitedOr9n02eHfJe.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.15922", "authors": [{"_id": "6996793f1268a6b79e0d028d", "name": "Seonghyeon Ye", "hidden": false}, {"_id": "6996793f1268a6b79e0d028e", "name": "Yunhao Ge", "hidden": false}, {"_id": "6996793f1268a6b79e0d028f", "name": "Kaiyuan Zheng", "hidden": false}, {"_id": "6996793f1268a6b79e0d0290", "name": "Shenyuan Gao", "hidden": false}, {"_id": "6996793f1268a6b79e0d0291", "name": "Sihyun Yu", "hidden": false}, {"_id": "6996793f1268a6b79e0d0292", "name": "George Kurian", "hidden": false}, {"_id": "6996793f1268a6b79e0d0293", "name": "Suneel Indupuru", "hidden": false}, {"_id": "6996793f1268a6b79e0d0294", "name": "You Liang Tan", "hidden": false}, {"_id": "6996793f1268a6b79e0d0295", "name": "Chuning Zhu", "hidden": false}, {"_id": "6996793f1268a6b79e0d0296", "name": "Jiannan Xiang", "hidden": false}, {"_id": "6996793f1268a6b79e0d0297", "name": "Ayaan Malik", "hidden": false}, {"_id": "6996793f1268a6b79e0d0298", "name": "Kyungmin Lee", "hidden": false}, {"_id": "6996793f1268a6b79e0d0299", "name": "William Liang", "hidden": false}, {"_id": "6996793f1268a6b79e0d029a", "name": "Nadun Ranawaka", "hidden": false}, {"_id": "6996793f1268a6b79e0d029b", "name": "Jiasheng Gu", "hidden": false}, {"_id": "6996793f1268a6b79e0d029c", "name": "Yinzhen Xu", "hidden": false}, {"_id": "6996793f1268a6b79e0d029d", "name": "Guanzhi Wang", "hidden": false}, {"_id": "6996793f1268a6b79e0d029e", "name": "Fengyuan Hu", "hidden": false}, {"_id": "6996793f1268a6b79e0d029f", "name": "Avnish Narayan", "hidden": false}, {"_id": "6996793f1268a6b79e0d02a0", "name": "Johan Bjorck", "hidden": false}, {"_id": "6996793f1268a6b79e0d02a1", "name": "Jing Wang", "hidden": false}, {"_id": "6996793f1268a6b79e0d02a2", "name": "Gwanghyun Kim", "hidden": false}, {"_id": "6996793f1268a6b79e0d02a3", "name": "Dantong Niu", "hidden": false}, {"_id": "6996793f1268a6b79e0d02a4", "name": "Ruijie Zheng", "hidden": false}, {"_id": "6996793f1268a6b79e0d02a5", "name": "Yuqi Xie", "hidden": false}, {"_id": "6996793f1268a6b79e0d02a6", "name": "Jimmy Wu", "hidden": false}, {"_id": "6996793f1268a6b79e0d02a7", "name": "Qi Wang", "hidden": false}, {"_id": "6996793f1268a6b79e0d02a8", "name": "Ryan Julian", "hidden": false}, {"_id": "6996793f1268a6b79e0d02a9", "name": "Danfei Xu", "hidden": false}, {"_id": "6996793f1268a6b79e0d02aa", "name": "Yilun Du", "hidden": false}, {"_id": "6996793f1268a6b79e0d02ab", "name": "Yevgen Chebotar", "hidden": false}, {"_id": "6996793f1268a6b79e0d02ac", "name": "Scott Reed", "hidden": false}, {"_id": "6996793f1268a6b79e0d02ad", "name": "Jan Kautz", "hidden": false}, {"_id": "6996793f1268a6b79e0d02ae", "name": "Yuke Zhu", "hidden": false}, {"_id": "6996793f1268a6b79e0d02af", "name": "Linxi \"Jim\" Fan", "hidden": false}, {"_id": "6996793f1268a6b79e0d02b0", "name": "Joel Jang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/aeAgRW6Fq1wOWJzNGPdUm.mp4"], "publishedAt": "2026-02-17T15:04:02.000Z", "submittedOnDailyAt": "2026-02-19T00:17:09.596Z", "title": "World Action Models are Zero-shot Policies", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "State-of-the-art Vision-Language-Action (VLA) models excel at semantic generalization but struggle to generalize to unseen physical motions in novel environments. We introduce DreamZero, a World Action Model (WAM) built upon a pretrained video diffusion backbone. Unlike VLAs, WAMs learn physical dynamics by predicting future world states and actions, using video as a dense representation of how the world evolves. By jointly modeling video and action, DreamZero learns diverse skills effectively from heterogeneous robot data without relying on repetitive demonstrations. This results in over 2x improvement in generalization to new tasks and environments compared to state-of-the-art VLAs in real robot experiments. Crucially, through model and system optimizations, we enable a 14B autoregressive video diffusion model to perform real-time closed-loop control at 7Hz. Finally, we demonstrate two forms of cross-embodiment transfer: video-only demonstrations from other robots or humans yield a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data. More surprisingly, DreamZero enables few-shot embodiment adaptation, transferring to a new embodiment with only 30 minutes of play data while retaining zero-shot generalization.", "upvotes": 9, "discussionId": "699679401268a6b79e0d02b1", "projectPage": "https://dreamzero0.github.io/", "githubRepo": "https://github.com/dreamzero0/dreamzero", "githubRepoAddedBy": "user", "ai_summary": "DreamZero is a World Action Model that leverages video diffusion to enable better generalization of physical motions across novel environments and embodiments compared to vision-language-action models.", "ai_keywords": ["World Action Model", "video diffusion", "video backbone", "physical dynamics", "autoregressive video diffusion model", "closed-loop control", "cross-embodiment transfer", "few-shot embodiment adaptation"], "githubStars": 742, "organization": {"_id": "676eec5d639faf44bc95708b", "name": "NVIDIA-DIR", "fullname": "NVIDIA Deep Imagination Research"}, "summary_zh": "<ul>\n    <li>DreamZero\u662f\u4e00\u79cd\u65b0\u7684\u4e16\u754c\u52a8\u4f5c\u6a21\u578b\uff08WAM\uff09\uff0c\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u548c\u9884\u6d4b\u7269\u7406\u52a8\u6001\u3002</li>\n    <li>\u4e0e\u4f20\u7edf\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\uff08VLA\uff09\u4e0d\u540c\uff0cDreamZero\u901a\u8fc7\u89c6\u9891\u5b66\u4e60\u4e16\u754c\u7684\u6f14\u53d8\uff0c\u4e0d\u9700\u8981\u91cd\u590d\u7684\u6f14\u793a\u3002</li>\n    <li>\u5728\u771f\u5b9e\u7684\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cDreamZero\u5728\u65b0\u4efb\u52a1\u548c\u73af\u5883\u7684\u6cdb\u5316\u80fd\u529b\u4e0a\u63d0\u9ad8\u4e86\u4e24\u500d\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4f18\u5316\u4f7f\u5f97\u5176\u80fd\u591f\u4ee5\u6bcf\u79d27\u6b21\u7684\u9891\u7387\u8fdb\u884c\u5b9e\u65f6\u95ed\u73af\u63a7\u5236\u3002</li>\n    <li>DreamZero\u652f\u6301\u8de8\u5f62\u6001\u8f6c\u79fb\uff0c\u80fd\u591f\u5229\u7528\u5176\u4ed6\u673a\u5668\u4eba\u6216\u4eba\u7c7b\u7684\u89c6\u9891\u6f14\u793a\u5728\u65b0\u4efb\u52a1\u4e2d\u63d0\u9ad842%\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DreamZero is a new model that helps robots learn to perform physical actions in new environments.</li>\n    <li>It uses video to understand how the world changes and learns from diverse robot data without needing many repeated examples.</li>\n    <li>DreamZero shows more than double the ability to adapt to new tasks compared to previous models in real robot tests.</li>\n    <li>The model can control robots in real-time at a speed of 7Hz, thanks to optimizations made to its design.</li>\n    <li>It can also learn from video demonstrations and adapt to new robot forms quickly, improving performance significantly with limited data.</li>\n</ul>"}, "publishedAt": "2026-02-17T10:04:02.000Z", "title": "World Action Models are Zero-shot Policies", "summary": "State-of-the-art Vision-Language-Action (VLA) models excel at semantic generalization but struggle to generalize to unseen physical motions in novel environments. We introduce DreamZero, a World Action Model (WAM) built upon a pretrained video diffusion backbone. Unlike VLAs, WAMs learn physical dynamics by predicting future world states and actions, using video as a dense representation of how the world evolves. By jointly modeling video and action, DreamZero learns diverse skills effectively from heterogeneous robot data without relying on repetitive demonstrations. This results in over 2x improvement in generalization to new tasks and environments compared to state-of-the-art VLAs in real robot experiments. Crucially, through model and system optimizations, we enable a 14B autoregressive video diffusion model to perform real-time closed-loop control at 7Hz. Finally, we demonstrate two forms of cross-embodiment transfer: video-only demonstrations from other robots or humans yield a relative improvement of over 42% on unseen task performance with just 10-20 minutes of data. More surprisingly, DreamZero enables few-shot embodiment adaptation, transferring to a new embodiment with only 30 minutes of play data while retaining zero-shot generalization.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/aeAgRW6Fq1wOWJzNGPdUm.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15922.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 235, "isUserFollowing": false}, "organization": {"_id": "676eec5d639faf44bc95708b", "name": "NVIDIA-DIR", "fullname": "NVIDIA Deep Imagination Research"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.16704", "authors": [{"_id": "699749f57a658569d5a100a5", "name": "Hee Seung Hwang", "hidden": false}, {"_id": "699749f57a658569d5a100a6", "name": "Xindi Wu", "hidden": false}, {"_id": "699749f57a658569d5a100a7", "name": "Sanghyuk Chun", "hidden": false}, {"_id": "699749f57a658569d5a100a8", "name": "Olga Russakovsky", "hidden": false}], "publishedAt": "2026-02-18T18:53:18.000Z", "submittedOnDailyAt": "2026-02-19T15:40:27.587Z", "title": "Reinforced Fast Weights with Next-Sequence Prediction", "submittedOnDailyBy": {"_id": "613940c0905b1938233881e3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/613940c0905b1938233881e3/Vb4kFWKEq6AILUP9KmCZA.png", "isPro": false, "fullname": "Xindi Wu", "user": "xindiw", "type": "user"}, "summary": "Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.", "upvotes": 8, "discussionId": "699749f57a658569d5a100a9", "githubRepo": "https://github.com/princetonvisualai/ReFINE", "githubRepoAddedBy": "user", "ai_summary": "REFINE is a reinforcement learning framework that improves fast weight models for long-context modeling by training under next-sequence prediction instead of next-token prediction, enhancing their ability to capture long-range dependencies.", "ai_keywords": ["fast weight architectures", "attention-based transformers", "next-token prediction", "next-sequence prediction", "reinforcement learning", "prediction entropy", "multi-token rollouts", "self-supervised sequence-level rewards", "group relative policy optimization", "needle-in-a-haystack retrieval", "long-context question answering", "LongBench"], "githubStars": 0, "organization": {"_id": "6735d51c08a190b1caea1f29", "name": "PrincetonUniversity", "fullname": "Princeton University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"}, "summary_zh": "<ul>\n    <li>\u5feb\u901f\u6743\u91cd\u67b6\u6784\u662f\u4e00\u79cd\u66ff\u4ee3\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u53d8\u538b\u5668\u7684\u65b0\u65b9\u6cd5\uff0c\u9002\u5408\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u4e14\u5185\u5b58\u5f00\u9500\u6052\u5b9a\u3002</li>\n    <li>\u5f53\u524d\u7684\u8bad\u7ec3\u65b9\u6cd5\uff08NTP\uff09\u53ea\u5173\u6ce8\u5355\u4e2a\u6807\u8bb0\u7684\u9884\u6d4b\uff0c\u5ffd\u89c6\u4e86\u591a\u4e2a\u6807\u8bb0\u4e4b\u95f4\u7684\u8bed\u4e49\u4e00\u81f4\u6027\u3002</li>\n    <li>REFINE\uff08\u5f3a\u5316\u5b66\u4e60\u5feb\u6743\u91cd\u4e0e\u4e0b\u4e00\u4e2a\u5e8f\u5217\u9884\u6d4b\uff09\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u4f7f\u7528\u4e0b\u4e00\u4e2a\u5e8f\u5217\u9884\u6d4b\u76ee\u6807\u3002</li>\n    <li>REFINE\u901a\u8fc7\u9009\u62e9\u6709\u4fe1\u606f\u7684\u6807\u8bb0\u4f4d\u7f6e\u548c\u81ea\u6211\u76d1\u7763\u5956\u52b1\u6765\u4f18\u5316\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u5404\u4e2a\u8bad\u7ec3\u9636\u6bb5\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cREFINE\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u7684NTP\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Fast weight architectures are a new option for handling long contexts without increasing memory usage.</li>\n    <li>Current training methods like next-token prediction (NTP) do not effectively capture relationships between multiple tokens.</li>\n    <li>We propose REFINE, a new method that uses reinforcement learning to train these models by predicting entire sequences instead of single tokens.</li>\n    <li>REFINE improves training by selecting meaningful token positions and providing rewards for more coherent sequences.</li>\n    <li>Tests show that REFINE works better than traditional methods in various tasks, helping fast weight models understand long contexts better.</li>\n</ul>"}, "publishedAt": "2026-02-18T13:53:18.000Z", "title": "Reinforced Fast Weights with Next-Sequence Prediction", "summary": "Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16704.png", "numComments": 1, "submittedBy": {"_id": "613940c0905b1938233881e3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/613940c0905b1938233881e3/Vb4kFWKEq6AILUP9KmCZA.png", "fullname": "Xindi Wu", "name": "xindiw", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "6735d51c08a190b1caea1f29", "name": "PrincetonUniversity", "fullname": "Princeton University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6735d43222d14a01ae63fb6d/N7pHYiJcM_zqOnp5MWfo3.png"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2602.12783", "authors": [{"_id": "6992ceae50fb2c0be47839c1", "name": "Yuejie Li", "hidden": false}, {"_id": "6992ceae50fb2c0be47839c2", "name": "Ke Yang", "hidden": false}, {"_id": "6992ceae50fb2c0be47839c3", "name": "Yueying Hua", "hidden": false}, {"_id": "6992ceae50fb2c0be47839c4", "user": {"_id": "69259bc3f1571271e94fa76b", "avatarUrl": "/avatars/b872462030e064e2a8ddc284c5ebe67e.svg", "isPro": false, "fullname": "berlin", "user": "berlin8587", "type": "user"}, "name": "Berlin Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-16T15:36:03.590Z", "hidden": false}, {"_id": "6992ceae50fb2c0be47839c5", "name": "Jianhao Nie", "hidden": false}, {"_id": "6992ceae50fb2c0be47839c6", "name": "Yueping He", "hidden": false}, {"_id": "6992ceae50fb2c0be47839c7", "name": "Caixin Kang", "hidden": false}], "publishedAt": "2026-02-13T10:08:27.000Z", "submittedOnDailyAt": "2026-02-16T14:32:15.752Z", "title": "SQuTR: A Robustness Benchmark for Spoken Query to Text Retrieval under Acoustic Noise", "submittedOnDailyBy": {"_id": "69259bc3f1571271e94fa76b", "avatarUrl": "/avatars/b872462030e064e2a8ddc284c5ebe67e.svg", "isPro": false, "fullname": "berlin", "user": "berlin8587", "type": "user"}, "summary": "Spoken query retrieval is an important interaction mode in modern information retrieval. However, existing evaluation datasets are often limited to simple queries under constrained noise conditions, making them inadequate for assessing the robustness of spoken query retrieval systems under complex acoustic perturbations. To address this limitation, we present SQuTR, a robustness benchmark for spoken query retrieval that includes a large-scale dataset and a unified evaluation protocol. SQuTR aggregates 37,317 unique queries from six commonly used English and Chinese text retrieval datasets, spanning multiple domains and diverse query types. We synthesize speech using voice profiles from 200 real speakers and mix 17 categories of real-world environmental noise under controlled SNR levels, enabling reproducible robustness evaluation from quiet to highly noisy conditions. Under the unified protocol, we conduct large-scale evaluations on representative cascaded and end-to-end retrieval systems. Experimental results show that retrieval performance decreases as noise increases, with substantially different drops across systems. Even large-scale retrieval models struggle under extreme noise, indicating that robustness remains a critical bottleneck. Overall, SQuTR provides a reproducible testbed for benchmarking and diagnostic analysis, and facilitates future research on robustness in spoken query to text retrieval.", "upvotes": 132, "discussionId": "6992ceae50fb2c0be47839c8", "githubRepo": "https://github.com/ttoyekk1a/SQuTR-Spoken-Query-to-Text-Retrieval", "githubRepoAddedBy": "user", "githubStars": 96, "summary_zh": "<ul>\n    <li>\u53d1\u97f3\u67e5\u8be2\u68c0\u7d22\u5728\u73b0\u4ee3\u4fe1\u606f\u68c0\u7d22\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u6570\u636e\u96c6\u901a\u5e38\u53ea\u5305\u542b\u7b80\u5355\u67e5\u8be2\uff0c\u65e0\u6cd5\u8bc4\u4f30\u590d\u6742\u566a\u58f0\u4e0b\u7684\u7cfb\u7edf\u9c81\u68d2\u6027\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63a8\u51fa\u4e86SQuTR\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u53d1\u97f3\u67e5\u8be2\u68c0\u7d22\u7684\u9c81\u68d2\u6027\u57fa\u51c6\uff0c\u5305\u62ec\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u7edf\u4e00\u8bc4\u4f30\u534f\u8bae\u3002</li>\n    <li>SQuTR \u6c47\u96c6\u4e86\u6765\u81ea\u516d\u4e2a\u5e38\u7528\u82f1\u8bed\u548c\u4e2d\u6587\u6587\u672c\u68c0\u7d22\u6570\u636e\u96c6\u768437,317\u4e2a\u72ec\u7279\u67e5\u8be2\uff0c\u6db5\u76d6\u591a\u4e2a\u9886\u57df\u548c\u4e0d\u540c\u7c7b\u578b\u7684\u67e5\u8be2\u3002</li>\n    <li>\u6211\u4eec\u4f7f\u7528200\u540d\u771f\u5b9e\u8bf4\u8bdd\u8005\u7684\u8bed\u97f3\u8d44\u6599\u5408\u6210\u8bed\u97f3\uff0c\u5e76\u5728\u53d7\u63a7\u7684\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u6df7\u540817\u7c7b\u73b0\u5b9e\u73af\u5883\u566a\u58f0\uff0c\u4ee5\u4fbf\u8fdb\u884c\u9c81\u68d2\u6027\u8bc4\u4f30\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u968f\u7740\u566a\u58f0\u589e\u52a0\uff0c\u68c0\u7d22\u6027\u80fd\u4e0b\u964d\uff0c\u7cfb\u7edf\u4e4b\u95f4\u7684\u4e0b\u964d\u5e45\u5ea6\u5dee\u5f02\u660e\u663e\uff0c\u663e\u793a\u9c81\u68d2\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u8981\u74f6\u9888\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Spoken query retrieval is important, but current evaluation datasets are too simple and don't test under noisy conditions.</li>\n    <li>SQuTR is a new benchmark designed to evaluate spoken query retrieval systems in difficult acoustic environments.</li>\n    <li>It includes over 37,000 unique queries from popular English and Chinese datasets, covering various topics and types of queries.</li>\n    <li>The benchmark uses speech from 200 real speakers and mixes in real-world noises to create different levels of noise for testing.</li>\n    <li>Results show that performance drops significantly as noise increases, highlighting ongoing challenges in making retrieval systems more robust.</li>\n</ul>"}, "publishedAt": "2026-02-13T05:08:27.000Z", "title": "SQuTR: A Robustness Benchmark for Spoken Query to Text Retrieval under Acoustic Noise", "summary": "Spoken query retrieval is an important interaction mode in modern information retrieval. However, existing evaluation datasets are often limited to simple queries under constrained noise conditions, making them inadequate for assessing the robustness of spoken query retrieval systems under complex acoustic perturbations. To address this limitation, we present SQuTR, a robustness benchmark for spoken query retrieval that includes a large-scale dataset and a unified evaluation protocol. SQuTR aggregates 37,317 unique queries from six commonly used English and Chinese text retrieval datasets, spanning multiple domains and diverse query types. We synthesize speech using voice profiles from 200 real speakers and mix 17 categories of real-world environmental noise under controlled SNR levels, enabling reproducible robustness evaluation from quiet to highly noisy conditions. Under the unified protocol, we conduct large-scale evaluations on representative cascaded and end-to-end retrieval systems. Experimental results show that retrieval performance decreases as noise increases, with substantially different drops across systems. Even large-scale retrieval models struggle under extreme noise, indicating that robustness remains a critical bottleneck. Overall, SQuTR provides a reproducible testbed for benchmarking and diagnostic analysis, and facilitates future research on robustness in spoken query to text retrieval.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12783.png", "numComments": 1, "submittedBy": {"_id": "69259bc3f1571271e94fa76b", "avatarUrl": "/avatars/b872462030e064e2a8ddc284c5ebe67e.svg", "fullname": "berlin", "name": "berlin8587", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.12705", "authors": [{"_id": "6992818050fb2c0be47838b2", "name": "Baorong Shi", "hidden": false}, {"_id": "6992818050fb2c0be47838b3", "name": "Bo Cui", "hidden": false}, {"_id": "6992818050fb2c0be47838b4", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6992818050fb2c0be47838b5", "name": "Deli Yu", "hidden": false}, {"_id": "6992818050fb2c0be47838b6", "name": "Fang Qian", "hidden": false}, {"_id": "6992818050fb2c0be47838b7", "name": "Haihua Yang", "hidden": false}, {"_id": "6992818050fb2c0be47838b8", "name": "Huichao Wang", "hidden": false}, {"_id": "6992818050fb2c0be47838b9", "name": "Jiale Chen", "hidden": false}, {"_id": "6992818050fb2c0be47838ba", "name": "Jianfei Pan", "hidden": false}, {"_id": "6992818050fb2c0be47838bb", "name": "Jieqiong Cao", "hidden": false}, {"_id": "6992818050fb2c0be47838bc", "name": "Jinghao Lin", "hidden": false}, {"_id": "6992818050fb2c0be47838bd", "name": "Kai Wu", "hidden": false}, {"_id": "6992818050fb2c0be47838be", "name": "Lin Yang", "hidden": false}, {"_id": "6992818050fb2c0be47838bf", "name": "Shengsheng Yao", "hidden": false}, {"_id": "6992818050fb2c0be47838c0", "name": "Tao Chen", "hidden": false}, {"_id": "6992818050fb2c0be47838c1", "name": "Xiaojun Xiao", "hidden": false}, {"_id": "6992818050fb2c0be47838c2", "user": {"_id": "666a59bff0d87d9c3b1dd907", "avatarUrl": "/avatars/4af5a47d78bca525c7ec985a390408a4.svg", "isPro": false, "fullname": "Xiaozhong Ji", "user": "xiaozhongji", "type": "user"}, "name": "Xiaozhong Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-02-16T15:36:45.325Z", "hidden": false}, {"_id": "6992818050fb2c0be47838c3", "name": "Xu Wang", "hidden": false}, {"_id": "6992818050fb2c0be47838c4", "name": "Yijun He", "hidden": false}, {"_id": "6992818050fb2c0be47838c5", "name": "Zhixiong Yang", "hidden": false}], "publishedAt": "2026-02-13T08:19:38.000Z", "submittedOnDailyAt": "2026-02-16T00:05:18.833Z", "title": "MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs", "submittedOnDailyBy": {"_id": "64c636b94c9bebfa6ac80ae4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c636b94c9bebfa6ac80ae4/yGl9IBjt6LVYh5NCrIKdF.png", "isPro": false, "fullname": "kai", "user": "KaiWu123", "type": "user"}, "summary": "We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research.", "upvotes": 55, "discussionId": "6992818050fb2c0be47838c6", "ai_summary": "MedXIAOHE is a medical vision-language foundation model that enhances clinical understanding through entity-aware continual pretraining, reinforcement learning, and tool-augmented agentic training for reliable diagnostic reasoning.", "ai_keywords": ["vision-language foundation model", "entity-aware continual pretraining", "heterogeneous medical corpora", "long-tail gaps", "reinforcement learning", "tool-augmented agentic training", "multi-step diagnostic reasoning", "evidence-grounded reasoning", "hallucination reduction", "medical instruction adherence"], "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>MedXIAOHE \u662f\u4e00\u4e2a\u4e13\u4e3a\u533b\u5b66\u5e94\u7528\u8bbe\u8ba1\u7684\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u5347\u533b\u5b66\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u591a\u9879\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8fc7\u4e86\u8bb8\u591a\u9886\u5148\u7684\u5c01\u95ed\u6e90\u591a\u6a21\u6001\u7cfb\u7edf\u3002</li>\n    <li>\u901a\u8fc7\u5b9e\u4f53\u611f\u77e5\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u6846\u67b6\uff0cMedXIAOHE \u6269\u5c55\u4e86\u77e5\u8bc6\u8986\u76d6\u8303\u56f4\uff0c\u51cf\u5c11\u4e86\u5bf9\u7f55\u89c1\u75be\u75c5\u7684\u77e5\u8bc6\u5dee\u8ddd\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u5de5\u5177\u589e\u5f3a\u8bad\u7ec3\uff0c\u652f\u6301\u591a\u6b65\u9aa4\u8bca\u65ad\u63a8\u7406\uff0c\u5e76\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u51b3\u7b56\u8def\u5f84\u3002</li>\n    <li>\u4e3a\u63d0\u9ad8\u5b9e\u9645\u4f7f\u7528\u7684\u53ef\u9760\u6027\uff0cMedXIAOHE \u96c6\u6210\u4e86\u7528\u6237\u504f\u597d\u6807\u51c6\u548c\u57fa\u4e8e\u8bc1\u636e\u7684\u63a8\u7406\uff0c\u751f\u6210\u7b26\u5408\u533b\u5b66\u6307\u4ee4\u7684\u957f\u7bc7\u62a5\u544a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>MedXIAOHE is a new model that helps understand and reason about medical information for real clinical use.</li>\n    <li>It performs very well on various medical tests and outperforms other leading models in several areas.</li>\n    <li>The model uses a special training approach to cover a wide range of medical knowledge, including rare diseases.</li>\n    <li>MedXIAOHE supports complex medical reasoning and decision-making through advanced training techniques.</li>\n    <li>The model is designed to be reliable and follows medical guidelines, aiming to generate accurate and detailed reports.</li>\n</ul>"}, "publishedAt": "2026-02-13T03:19:38.000Z", "title": "MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs", "summary": "We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12705.png", "numComments": 4, "submittedBy": {"_id": "64c636b94c9bebfa6ac80ae4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c636b94c9bebfa6ac80ae4/yGl9IBjt6LVYh5NCrIKdF.png", "fullname": "kai", "name": "KaiWu123", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.14111", "authors": [{"_id": "69959cbbed493589ceb5be31", "user": {"_id": "6572e9380503eeadb78fb3e3", "avatarUrl": "/avatars/cbf26f1b2c72732d37e8cb847c48f152.svg", "isPro": false, "fullname": "Anton Korznikov", "user": "AntonKorznikov", "type": "user"}, "name": "Anton Korznikov", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:45.391Z", "hidden": false}, {"_id": "69959cbbed493589ceb5be32", "user": {"_id": "661e44cf1d8ffc49b57ba07e", "avatarUrl": "/avatars/3e937cc4f784b369b9f996ba82d1b81d.svg", "isPro": false, "fullname": "Andrey Galichin", "user": "andreuka18", "type": "user"}, "name": "Andrey Galichin", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:51.000Z", "hidden": false}, {"_id": "69959cbbed493589ceb5be33", "user": {"_id": "60cd95ee15ecba5f2200304a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg", "isPro": false, "fullname": "Alexey Dontsov", "user": "therem", "type": "user"}, "name": "Alexey Dontsov", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:38.913Z", "hidden": false}, {"_id": "69959cbbed493589ceb5be34", "user": {"_id": "66e19e09140031bf85f0e6f3", "avatarUrl": "/avatars/6cce10740eb73f066d7ed0fe8ca3a93a.svg", "isPro": false, "fullname": "Oleg Rogov", "user": "Olegario228", "type": "user"}, "name": "Oleg Rogov", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:59.681Z", "hidden": false}, {"_id": "69959cbbed493589ceb5be35", "name": "Ivan Oseledets", "hidden": false}, {"_id": "69959cbbed493589ceb5be36", "user": {"_id": "662f8d645c4db70c77a203b0", "avatarUrl": "/avatars/72f9a3c39b3ba5114388d16a35524835.svg", "isPro": false, "fullname": "Elena Tutubalina", "user": "tlenusik", "type": "user"}, "name": "Elena Tutubalina", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T12:30:12.235Z", "hidden": false}], "publishedAt": "2026-02-15T11:53:55.000Z", "submittedOnDailyAt": "2026-02-18T08:38:30.231Z", "title": "Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?", "submittedOnDailyBy": {"_id": "60cd95ee15ecba5f2200304a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg", "isPro": false, "fullname": "Alexey Dontsov", "user": "therem", "type": "user"}, "summary": "Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic setup with known ground-truth features, we demonstrate that SAEs recover only 9% of true features despite achieving 71% explained variance, showing that they fail at their core task even when reconstruction is strong. To evaluate SAEs on real activations, we introduce three baselines that constrain SAE feature directions or their activation patterns to random values. Through extensive experiments across multiple SAE architectures, we show that our baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72). Together, these results suggest that SAEs in their current state do not reliably decompose models' internal mechanisms.", "upvotes": 51, "discussionId": "69959cbbed493589ceb5be37", "ai_summary": "Sparse Autoencoders fail to reliably decompose neural network internals despite strong reconstruction performance, as demonstrated through synthetic and real activation evaluations.", "ai_keywords": ["Sparse Autoencoders", "neural networks", "activations", "explained variance", "interpretability", "sparse probing", "causal editing"], "summary_zh": "<ul>\n    <li>\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u88ab\u8ba4\u4e3a\u662f\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u7684\u4e00\u79cd\u6709\u524d\u666f\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u5c06\u6fc0\u6d3b\u5206\u89e3\u4e3a\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u7a00\u758f\u7279\u5f81\u3002</li>\n    <li>\u5c3d\u7ba1SAEs\u5f15\u8d77\u4e86\u5f88\u591a\u5173\u6ce8\uff0c\u4f46\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8d1f\u9762\u7ed3\u679c\u8ba9\u4eba\u6000\u7591\u5b83\u4eec\u662f\u5426\u80fd\u591f\u6062\u590d\u6709\u610f\u4e49\u7684\u7279\u5f81\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0cSAEs\u5728\u5df2\u77e5\u771f\u5b9e\u7279\u5f81\u7684\u5408\u6210\u8bbe\u7f6e\u4e2d\uff0c\u4ec5\u6062\u590d\u4e869%\u7684\u771f\u5b9e\u7279\u5f81\uff0c\u5c3d\u7ba1\u89e3\u91ca\u7684\u65b9\u5dee\u8fbe\u5230\u4e8671%\u3002</li>\n    <li>\u5728\u771f\u5b9e\u6fc0\u6d3b\u7684\u8bc4\u4f30\u4e2d\uff0c\u4f7f\u7528\u4e09\u79cd\u57fa\u7ebf\u4e0eSAEs\u8fdb\u884c\u6bd4\u8f83\uff0c\u7ed3\u679c\u8868\u660e\u57fa\u7ebf\u5728\u53ef\u89e3\u91ca\u6027\u548c\u5176\u4ed6\u6307\u6807\u4e0a\u4e0e\u5b8c\u5168\u8bad\u7ec3\u7684SAEs\u76f8\u8fd1\u3002</li>\n    <li>\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u76ee\u524d\u72b6\u6001\u4e0b\u7684SAEs\u5e76\u4e0d\u80fd\u53ef\u9760\u5730\u5206\u89e3\u6a21\u578b\u7684\u5185\u90e8\u673a\u5236\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Sparse Autoencoders (SAEs) are used to break down neural network activations into simpler, understandable features.</li>\n    <li>Research shows that SAEs struggle to identify true features, recovering only 9% of them even when they appear to do well in other metrics.</li>\n    <li>In tests with real data, new baseline methods performed similarly to SAEs in terms of interpretability and other evaluations.</li>\n    <li>The findings indicate that SAEs may not effectively reveal how neural networks work internally.</li>\n</ul>"}, "publishedAt": "2026-02-15T06:53:55.000Z", "title": "Sanity Checks for Sparse Autoencoders: Do SAEs Beat Random Baselines?", "summary": "Sparse Autoencoders (SAEs) have emerged as a promising tool for interpreting neural networks by decomposing their activations into sparse sets of human-interpretable features. Recent work has introduced multiple SAE variants and successfully scaled them to frontier models. Despite much excitement, a growing number of negative results in downstream tasks casts doubt on whether SAEs recover meaningful features. To directly investigate this, we perform two complementary evaluations. On a synthetic setup with known ground-truth features, we demonstrate that SAEs recover only 9% of true features despite achieving 71% explained variance, showing that they fail at their core task even when reconstruction is strong. To evaluate SAEs on real activations, we introduce three baselines that constrain SAE feature directions or their activation patterns to random values. Through extensive experiments across multiple SAE architectures, we show that our baselines match fully-trained SAEs in interpretability (0.87 vs 0.90), sparse probing (0.69 vs 0.72), and causal editing (0.73 vs 0.72). Together, these results suggest that SAEs in their current state do not reliably decompose models' internal mechanisms.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14111.png", "numComments": 2, "submittedBy": {"_id": "60cd95ee15ecba5f2200304a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd95ee15ecba5f2200304a/3gMYeWm8wQO5KfqE5RmEe.jpeg", "fullname": "Alexey Dontsov", "name": "therem", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.12675", "authors": [{"_id": "69967cd61268a6b79e0d02d9", "user": {"_id": "66c0a08bac74db25de8427ec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg", "isPro": false, "fullname": "Jintao Zhang", "user": "jt-zhang", "type": "user"}, "name": "Jintao Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T09:51:51.054Z", "hidden": false}, {"_id": "69967cd61268a6b79e0d02da", "user": {"_id": "658c1802a1105f8157ad1db9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658c1802a1105f8157ad1db9/WzjY29SkngxkKfiTYcssh.jpeg", "isPro": false, "fullname": "whx1003", "user": "whx1003", "type": "user"}, "name": "Haoxu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T09:51:48.596Z", "hidden": false}, {"_id": "69967cd61268a6b79e0d02db", "name": "Kai Jiang", "hidden": false}, {"_id": "69967cd61268a6b79e0d02dc", "name": "Kaiwen Zheng", "hidden": false}, {"_id": "69967cd61268a6b79e0d02dd", "name": "Youhe Jiang", "hidden": false}, {"_id": "69967cd61268a6b79e0d02de", "name": "Ion Stoica", "hidden": false}, {"_id": "69967cd61268a6b79e0d02df", "name": "Jianfei Chen", "hidden": false}, {"_id": "69967cd61268a6b79e0d02e0", "name": "Jun Zhu", "hidden": false}, {"_id": "69967cd61268a6b79e0d02e1", "name": "Joseph E. Gonzalez", "hidden": false}], "publishedAt": "2026-02-13T07:16:02.000Z", "submittedOnDailyAt": "2026-02-19T00:32:11.371Z", "title": "SLA2: Sparse-Linear Attention with Learnable Routing and QAT", "submittedOnDailyBy": {"_id": "66c0a08bac74db25de8427ec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg", "isPro": false, "fullname": "Jintao Zhang", "user": "jt-zhang", "type": "user"}, "summary": "Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality.", "upvotes": 43, "discussionId": "69967cd61268a6b79e0d02e2", "projectPage": "https://github.com/thu-ml/SLA", "ai_summary": "SLA2 improves sparse-linear attention in diffusion models by introducing a learnable router, direct attention formulation, and quantization-aware fine-tuning for enhanced efficiency and quality.", "ai_keywords": ["sparse-linear attention", "diffusion models", "attention sparsity", "learnable router", "quantization-aware fine-tuning", "attention error", "direct decomposition"], "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "summary_zh": "<ul>\n    <li>\u7a00\u758f\u7ebf\u6027\u6ce8\u610f\u529b\uff08SLA\uff09\u7ed3\u5408\u4e86\u7a00\u758f\u548c\u7ebf\u6027\u6ce8\u610f\u529b\uff0c\u80fd\u591f\u52a0\u901f\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5728\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\u3002</li>\n    <li>SLA\u4f9d\u8d56\u4e8e\u4e00\u79cd\u542f\u53d1\u5f0f\u5206\u5272\u65b9\u6cd5\uff0c\u6839\u636e\u6ce8\u610f\u529b\u6743\u91cd\u7684\u5927\u5c0f\u5c06\u8ba1\u7b97\u5206\u914d\u7ed9\u7a00\u758f\u6216\u7ebf\u6027\u5206\u652f\uff0c\u8fd9\u79cd\u65b9\u6cd5\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u7684\u3002</li>\n    <li>\u901a\u8fc7\u5bf9SLA\u7684\u6ce8\u610f\u529b\u8bef\u5dee\u8fdb\u884c\u5206\u6790\uff0c\u6211\u4eec\u53d1\u73b0SLA\u4e0e\u7a00\u758f\u548c\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u76f4\u63a5\u5206\u89e3\u4e4b\u95f4\u5b58\u5728\u4e0d\u5339\u914d\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86SLA2\uff0c\u5f15\u5165\u4e86\u53ef\u5b66\u4e60\u7684\u8def\u7531\u5668\uff0c\u52a8\u6001\u9009\u62e9\u6bcf\u4e2a\u6ce8\u610f\u529b\u8ba1\u7b97\u4f7f\u7528\u7a00\u758f\u6216\u7ebf\u6027\u6ce8\u610f\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cSLA2\u5728\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u53ef\u4ee5\u5b9e\u73b097%\u7684\u6ce8\u610f\u529b\u7a00\u758f\u6027\uff0c\u5e76\u63d0\u4f9b18.6\u500d\u7684\u6ce8\u610f\u529b\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Sparse-Linear Attention (SLA) helps speed up video generation but has some limitations in its current method.</li>\n    <li>It uses a fixed rule to decide how to split attention calculations, which may not be the best approach.</li>\n    <li>The researchers found that SLA does not perfectly match a direct split of attention into sparse and linear parts.</li>\n    <li>They developed a new method called SLA2 that features a smart system to choose between sparse and linear attention dynamically.</li>\n    <li>SLA2 achieves 97% attention sparsity and speeds up attention calculations by 18.6 times while maintaining high-quality video generation.</li>\n</ul>"}, "publishedAt": "2026-02-13T02:16:02.000Z", "title": "SLA2: Sparse-Linear Attention with Learnable Routing and QAT", "summary": "Sparse-Linear Attention (SLA) combines sparse and linear attention to accelerate diffusion models and has shown strong performance in video generation. However, (i) SLA relies on a heuristic split that assigns computations to the sparse or linear branch based on attention-weight magnitude, which can be suboptimal. Additionally, (ii) after formally analyzing the attention error in SLA, we identify a mismatch between SLA and a direct decomposition into sparse and linear attention. We propose SLA2, which introduces (I) a learnable router that dynamically selects whether each attention computation should use sparse or linear attention, (II) a more faithful and direct sparse-linear attention formulation that uses a learnable ratio to combine the sparse and linear attention branches, and (III) a sparse + low-bit attention design, where low-bit attention is introduced via quantization-aware fine-tuning to reduce quantization error. Experiments show that on video diffusion models, SLA2 can achieve 97% attention sparsity and deliver an 18.6x attention speedup while preserving generation quality.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12675.png", "numComments": 3, "submittedBy": {"_id": "66c0a08bac74db25de8427ec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg", "fullname": "Jintao Zhang", "name": "jt-zhang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 46, "isUserFollowing": false}, "organization": {"_id": "61f20a9ce108f2cba2dc0730", "name": "Berkeley", "fullname": "UC Berkeley", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/0FjsTg2txEZZ4dEgmMnQL.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.12670", "authors": [{"_id": "6994d2138d17d1ee8c10eb51", "user": {"_id": "663fe2d26304d377fc253322", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wuey_nNXSW4GthPYLfFS4.jpeg", "isPro": false, "fullname": "Xiangyi Li", "user": "xdotli", "type": "user"}, "name": "Xiangyi Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:43.446Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb52", "name": "Wenbo Chen", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb53", "name": "Yimin Liu", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb54", "name": "Shenghan Zheng", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb55", "user": {"_id": "6462bf90c9cc74e82e270cb6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6462bf90c9cc74e82e270cb6/usLDOUhyIfDXA2HZvXvps.jpeg", "isPro": true, "fullname": "Kobe Chen", "user": "kobe0938", "type": "user"}, "name": "Xiaokun Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:37.777Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb56", "user": {"_id": "659a1c9511b48706bab783cc", "avatarUrl": "/avatars/6978a5bc7ab284d9f7285f9fd2c8d0e0.svg", "isPro": false, "fullname": "Yifeng He", "user": "yfhe", "type": "user"}, "name": "Yifeng He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:41.599Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb57", "name": "Yubo Li", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb58", "user": {"_id": "663bd5fcfb931d4660fd18b7", "avatarUrl": "/avatars/17aa421d40fe3532d0ddecbc2accb249.svg", "isPro": false, "fullname": "Bingran You", "user": "bingran-you", "type": "user"}, "name": "Bingran You", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:36:48.789Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb59", "name": "Haotian Shen", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb5a", "user": {"_id": "63b5d1bb0d5913eee4869c0d", "avatarUrl": "/avatars/ae397f54bbb3debc1f7903f4c6959ae6.svg", "isPro": false, "fullname": "Jiankai Sun", "user": "zhenv5", "type": "user"}, "name": "Jiankai Sun", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:36:59.144Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb5b", "name": "Shuyi Wang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb5c", "user": {"_id": "65cc8abe8ebd392213020575", "avatarUrl": "/avatars/e0b49fe07b3553779992092f60aa0b48.svg", "isPro": false, "fullname": "qunhongzeng", "user": "qunhongzeng", "type": "user"}, "name": "Qunhong Zeng", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:07.249Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb5d", "name": "Di Wang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb5e", "user": {"_id": "6275a465597c70eb8949fce5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png", "isPro": false, "fullname": "Xuandong Zhao", "user": "Xuandong", "type": "user"}, "name": "Xuandong Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:13.482Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb5f", "name": "Yuanli Wang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb60", "user": {"_id": "65a0dc6690eb7a1524186cc2", "avatarUrl": "/avatars/6ecf7fb5aa74f2453d0e3bc7b9cca0d3.svg", "isPro": true, "fullname": "Roey Ben Chaim", "user": "roeybc", "type": "user"}, "name": "Roey Ben Chaim", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T13:37:19.969Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb61", "name": "Zonglin Di", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb62", "name": "Yipeng Gao", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb63", "name": "Junwei He", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb64", "user": {"_id": "63710bca7a5e5d8efdbff215", "avatarUrl": "/avatars/25e7f713d613ec81ba775265eadff8bc.svg", "isPro": false, "fullname": "He", "user": "Yizhuo", "type": "user"}, "name": "Yizhuo He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:45.624Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb65", "name": "Liqiang Jing", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb66", "name": "Luyang Kong", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb67", "name": "Xin Lan", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb68", "name": "Jiachen Li", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb69", "name": "Songlin Li", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb6a", "name": "Yijiang Li", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb6b", "user": {"_id": "64b5198c25882acb62fb77ef", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b5198c25882acb62fb77ef/HX9pfMEPQlfjvSAgSLplY.png", "isPro": false, "fullname": "Yueqian Lin", "user": "linyueqian", "type": "user"}, "name": "Yueqian Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:39.753Z", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb6c", "name": "Xinyi Liu", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb6d", "name": "Xuanqing Liu", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb6e", "name": "Haoran Lyu", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb6f", "name": "Ze Ma", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb70", "name": "Bowei Wang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb71", "name": "Runhui Wang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb72", "name": "Tianyu Wang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb73", "name": "Wengao Ye", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb74", "name": "Yue Zhang", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb75", "name": "Hanwen Xing", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb76", "name": "Yiqi Xue", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb77", "name": "Steven Dillmann", "hidden": false}, {"_id": "6994d2138d17d1ee8c10eb78", "name": "Han-chung Lee", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/663fe2d26304d377fc253322/7oDWwNkdX1FMVwX0ecYMt.png"], "publishedAt": "2026-02-13T07:06:06.000Z", "submittedOnDailyAt": "2026-02-18T11:04:39.385Z", "title": "SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks", "submittedOnDailyBy": {"_id": "663fe2d26304d377fc253322", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wuey_nNXSW4GthPYLfFS4.jpeg", "isPro": false, "fullname": "Xiangyi Li", "user": "xdotli", "type": "user"}, "summary": "Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.", "upvotes": 36, "discussionId": "6994d2148d17d1ee8c10eb79", "projectPage": "https://skillsbench.ai/", "githubRepo": "https://github.com/benchflow-ai/skillsbench", "githubRepoAddedBy": "user", "ai_summary": "SkillsBench evaluates agent skills across 86 tasks and finds that curated skills improve performance significantly but inconsistently, while self-generated skills offer no benefit, indicating that models struggle to create useful procedural knowledge despite benefiting from curated versions.", "ai_keywords": ["agent skills", "LLM agents", "SkillsBench", "procedural knowledge", "curated Skills", "self-generated Skills", "agent-model configurations", "pass rate", "domain-specific effects"], "githubStars": 413, "organization": {"_id": "69657f24ec1d157f1590a81d", "name": "benchflow", "fullname": "BenchFlow", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663fe2d26304d377fc253322/L6ik2VpL5iizADEXK4W-A.png"}, "summary_zh": "<ul>\n    <li>\u4ee3\u7406\u6280\u80fd\u662f\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u63a8\u7406\u65f6\u80fd\u529b\u7684\u7a0b\u5e8f\u6027\u77e5\u8bc6\u5305\u3002</li>\n    <li>\u76ee\u524d\u6ca1\u6709\u6807\u51c6\u65b9\u6cd5\u6765\u8861\u91cf\u8fd9\u4e9b\u6280\u80fd\u7684\u5b9e\u9645\u5e2e\u52a9\u6548\u679c\u3002</li>\n    <li>SkillsBench\u662f\u4e00\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b86\u4e2a\u4efb\u52a1\u548c11\u4e2a\u9886\u57df\uff0c\u8bc4\u4f30\u4e86\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002</li>\n    <li>\u7ecf\u8fc7\u6d4b\u8bd5\uff0c\u7ecf\u8fc7\u7b56\u5212\u7684\u6280\u80fd\u5e73\u5747\u63d0\u9ad8\u4e8616.2\u4e2a\u767e\u5206\u70b9\uff0c\u4f46\u4e0d\u540c\u9886\u57df\u7684\u6548\u679c\u5dee\u5f02\u5f88\u5927\u3002</li>\n    <li>\u81ea\u751f\u6210\u7684\u6280\u80fd\u6ca1\u6709\u5e73\u5747\u6536\u76ca\uff0c\u8868\u660e\u6a21\u578b\u4e0d\u80fd\u53ef\u9760\u5730\u521b\u4f5c\u51fa\u6709\u7528\u7684\u7a0b\u5e8f\u6027\u77e5\u8bc6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Agent Skills improve the performance of language model agents during tasks, but there's no standard way to measure their effectiveness.</li>\n    <li>SkillsBench is a new benchmark that includes 86 tasks in 11 different areas, along with specific Skills and reliable verification methods.</li>\n    <li>Tasks were tested without Skills, with curated Skills, and with self-generated Skills, using 7 agent-model setups across over 7,300 trials.</li>\n    <li>Curated Skills increased the average success rate by 16.2 percentage points, but the impact varied greatly by domain, with some tasks showing worse results.</li>\n    <li>Self-generated Skills did not improve performance, and focused Skills with fewer modules performed better than extensive documentation; smaller models with Skills matched the performance of larger models without them.</li>\n</ul>"}, "publishedAt": "2026-02-13T02:06:06.000Z", "title": "SkillsBench: Benchmarking How Well Agent Skills Work Across Diverse Tasks", "summary": "Agent Skills are structured packages of procedural knowledge that augment LLM agents at inference time. Despite rapid adoption, there is no standard way to measure whether they actually help. We present SkillsBench, a benchmark of 86 tasks across 11 domains paired with curated Skills and deterministic verifiers. Each task is evaluated under three conditions: no Skills, curated Skills, and self-generated Skills. We test 7 agent-model configurations over 7,308 trajectories. Curated Skills raise average pass rate by 16.2 percentage points(pp), but effects vary widely by domain (+4.5pp for Software Engineering to +51.9pp for Healthcare) and 16 of 84 tasks show negative deltas. Self-generated Skills provide no benefit on average, showing that models cannot reliably author the procedural knowledge they benefit from consuming. Focused Skills with 2--3 modules outperform comprehensive documentation, and smaller models with Skills can match larger models without them.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/663fe2d26304d377fc253322/7oDWwNkdX1FMVwX0ecYMt.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.12670.png", "numComments": 3, "submittedBy": {"_id": "663fe2d26304d377fc253322", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/wuey_nNXSW4GthPYLfFS4.jpeg", "fullname": "Xiangyi Li", "name": "xdotli", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "69657f24ec1d157f1590a81d", "name": "benchflow", "fullname": "BenchFlow", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/663fe2d26304d377fc253322/L6ik2VpL5iizADEXK4W-A.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.15763", "authors": [{"_id": "6995270f8d17d1ee8c10ebc5", "name": "GLM-5 Team", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebc7", "name": "Aohan Zeng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebc8", "name": "Xin Lv", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebc9", "user": {"_id": "62b196646d3d059f40c3df19", "avatarUrl": "/avatars/dbddf54ae949437223f3a438d30ef653.svg", "isPro": false, "fullname": "Zhenyu Hou", "user": "think2try", "type": "user"}, "name": "Zhenyu Hou", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:31:01.080Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebca", "user": {"_id": "63033dc4e1e7f0e03a5e1a31", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661157784937-63033dc4e1e7f0e03a5e1a31.jpeg", "isPro": false, "fullname": "Zhengxiao Du", "user": "zxdu20", "type": "user"}, "name": "Zhengxiao Du", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T09:21:03.474Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebcb", "user": {"_id": "6231576e92e83fd1179ac3f0", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664543160657-6231576e92e83fd1179ac3f0.jpeg", "isPro": false, "fullname": "Qinkai Zheng", "user": "Stanislas", "type": "user"}, "name": "Qinkai Zheng", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T09:21:21.181Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebcc", "name": "Bin Chen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebcd", "name": "Da Yin", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebce", "name": "Chendi Ge", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebcf", "user": {"_id": "62d00ff8dd7bdfc5e5c553c6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d00ff8dd7bdfc5e5c553c6/u9Be7K16IS2Hc5OqKctEA.jpeg", "isPro": false, "fullname": "chengxing xie", "user": "yitianlian", "type": "user"}, "name": "Chengxing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T09:21:28.339Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd0", "user": {"_id": "65eaf755ab0a6a90da55ab58", "avatarUrl": "/avatars/a46890a9d067a913513edf3759f12c85.svg", "isPro": false, "fullname": "Cunxiang Wang", "user": "wangcunxiang", "type": "user"}, "name": "Cunxiang Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T10:16:12.082Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd1", "name": "Gengzheng Pan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd2", "name": "Hao Zeng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd3", "user": {"_id": "622ec54ce27c88667db094ad", "avatarUrl": "/avatars/8f06594b625a9c4fca1fffec4885bbdc.svg", "isPro": false, "fullname": "Haoke Zhang", "user": "zhk", "type": "user"}, "name": "Haoke Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:33:43.672Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd4", "name": "Haoran Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd5", "user": {"_id": "654ccc038a299b6be086cf1b", "avatarUrl": "/avatars/87d0f3c0f991368ce923da32cdd971a1.svg", "isPro": false, "fullname": "Huilong Chen", "user": "HuilongChen", "type": "user"}, "name": "Huilong Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:32:16.751Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd6", "name": "Jiajie Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd7", "name": "Jian Jiao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd8", "name": "Jiaqi Guo", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebd9", "user": {"_id": "65655c3ed35fc55406e116aa", "avatarUrl": "/avatars/d45081ec1617bb737ea531866b76f57a.svg", "isPro": false, "fullname": "jingsen", "user": "wangjingsen", "type": "user"}, "name": "Jingsen Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:34:23.674Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebda", "name": "Jingzhao Du", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebdb", "user": {"_id": "650d8ddbcbd0c7d550dc8278", "avatarUrl": "/avatars/3e49752b6b3c9f3875b4470cebb838e6.svg", "isPro": false, "fullname": "wujinzhu", "user": "kimjohn", "type": "user"}, "name": "Jinzhu Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:34:46.441Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebdc", "user": {"_id": "64b73d6353d91a364aa8cea5", "avatarUrl": "/avatars/721553ab563cf13d8e7bfde088e3b753.svg", "isPro": false, "fullname": "Kedong Wang", "user": "mrwkd123", "type": "user"}, "name": "Kedong Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:32:47.078Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebdd", "name": "Lei Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebde", "name": "Lin Fan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebdf", "user": {"_id": "60eff04e22ab0ac83b0fc9d8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60eff04e22ab0ac83b0fc9d8/pBjftNyFN1qB8Br3FZQmD.jpeg", "isPro": false, "fullname": "lucen zhong", "user": "anchorzhong", "type": "user"}, "name": "Lucen Zhong", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:32:53.663Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe0", "user": {"_id": "64f1abd2b12bdcef55e1c078", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1abd2b12bdcef55e1c078/yG9HR_imUWqESN-JNcjf1.jpeg", "isPro": false, "fullname": "Mingdao Liu", "user": "lambdax", "type": "user"}, "name": "Mingdao Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:33:00.291Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe1", "user": {"_id": "6706db274341dcee45811105", "avatarUrl": "/avatars/125ef02f12785b8f51e84639d879efaa.svg", "isPro": false, "fullname": "Mingming Zhao", "user": "Lukas0510", "type": "user"}, "name": "Mingming Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:33:15.048Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe2", "name": "Pengfan Du", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe3", "name": "Qian Dong", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe4", "name": "Rui Lu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe5", "name": "Shuang-Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe6", "name": "Shulin Cao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe7", "name": "Song Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe8", "name": "Ting Jiang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebe9", "name": "Xiaodong Chen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebea", "name": "Xiaohan Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebeb", "name": "Xuancheng Huang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebec", "user": {"_id": "658834ba4f9d2b955e19d389", "avatarUrl": "/avatars/89d9a6f9fac67391dc052d49def1e758.svg", "isPro": false, "fullname": "Xuezhen Dong", "user": "xzdong", "type": "user"}, "name": "Xuezhen Dong", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:33:25.460Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebed", "name": "Yabo Xu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebee", "name": "Yao Wei", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebef", "name": "Yifan An", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf0", "name": "Yilin Niu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf1", "name": "Yitong Zhu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf2", "name": "Yuanhao Wen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf3", "name": "Yukuo Cen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf4", "user": {"_id": "64ed568ccf6118a9379a61b8", "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg", "isPro": false, "fullname": "Yushi Bai", "user": "bys0318", "type": "user"}, "name": "Yushi Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T13:36:07.294Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf5", "name": "Zhongpei Qiao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf6", "name": "Zihan Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf7", "name": "Zikang Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf8", "name": "Zilin Zhu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebf9", "name": "Ziqiang Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebfa", "name": "Zixuan Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebfb", "name": "Bojie Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebfc", "name": "Bosi Wen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebfd", "name": "Can Huang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebfe", "name": "Changpeng Cai", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ebff", "name": "Chao Yu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec00", "name": "Chen Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec01", "name": "Chen Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec02", "name": "Chenghua Huang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec03", "name": "Chengwei Hu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec04", "name": "Chenhui Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec05", "name": "Chenzheng Zhu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec06", "name": "Congfeng Yin", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec07", "name": "Daoyan Lin", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec08", "name": "Dayong Yang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec09", "name": "Di Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec0a", "name": "Ding Ai", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec0b", "name": "Erle Zhu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec0c", "name": "Fangzhou Yi", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec0d", "name": "Feiyu Chen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec0e", "name": "Guohong Wen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec0f", "name": "Hailong Sun", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec10", "name": "Haisha Zhao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec11", "name": "Haiyi Hu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec12", "name": "Hanchen Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec13", "name": "Hanrui Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec14", "name": "Hanyu Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec15", "name": "Hao Peng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec16", "name": "Hao Tai", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec17", "name": "Haobo Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec18", "name": "He Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec19", "name": "Hongwei Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec1a", "name": "Hongxi Yan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec1b", "name": "Hongyu Ge", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec1c", "name": "Huan Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec1d", "name": "Huan Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec1e", "name": "Huanpeng Chu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec1f", "name": "Jia'ni Zhao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec20", "name": "Jiachen Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec21", "name": "Jiajing Zhao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec22", "name": "Jiamin Ren", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec23", "name": "Jiapeng Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec24", "name": "Jiaxin Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec25", "name": "Jiayi Gui", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec26", "name": "Jiayue Zhao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec27", "name": "Jijie Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec28", "name": "Jing An", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec29", "name": "Jing Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec2a", "name": "Jingwei Yuan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec2b", "name": "Jinhua Du", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec2c", "name": "Jinxin Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec2d", "name": "Junkai Zhi", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec2e", "name": "Junwen Duan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec2f", "name": "Kaiyue Zhou", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec30", "name": "Kangjian Wei", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec31", "name": "Ke Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec32", "name": "Keyun Luo", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec33", "name": "Laiqiang Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec34", "name": "Leigang Sha", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec35", "name": "Liang Xu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec36", "name": "Lindong Wu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec37", "name": "Lintao Ding", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec38", "name": "Lu Chen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec39", "name": "Minghao Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec3a", "name": "Nianyi Lin", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec3b", "name": "Pan Ta", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec3c", "name": "Qiang Zou", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec3d", "name": "Rongjun Song", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec3e", "name": "Ruiqi Yang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec3f", "name": "Shangqing Tu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec40", "name": "Shangtong Yang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec41", "name": "Shaoxiang Wu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec42", "name": "Shengyan Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec43", "name": "Shijie Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec44", "name": "Shuang Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec45", "name": "Shuyi Fan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec46", "name": "Wei Qin", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec47", "name": "Wei Tian", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec48", "name": "Weining Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec49", "name": "Wenbo Yu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec4a", "name": "Wenjie Liang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec4b", "name": "Xiang Kuang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec4c", "name": "Xiangmeng Cheng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec4d", "name": "Xiangyang Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec4e", "name": "Xiaoquan Yan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec4f", "name": "Xiaowei Hu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec50", "name": "Xiaoying Ling", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec51", "name": "Xing Fan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec52", "name": "Xingye Xia", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec53", "name": "Xinyuan Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec54", "name": "Xinze Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec55", "name": "Xirui Pan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec56", "name": "Xunkai Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec57", "name": "Yandong Wu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec58", "name": "Yanfu Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec59", "name": "Yidong Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec5a", "name": "Yifan Zhu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec5b", "name": "Yijun Tan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec5c", "name": "Yilin Zhou", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec5d", "name": "Yiming Pan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec5e", "name": "Ying Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec5f", "name": "Yinpei Su", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec60", "name": "Yipeng Geng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec61", "name": "Yipeng Geng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec62", "name": "Yong Yan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec63", "name": "Yonglin Tan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec64", "name": "Yuean Bi", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec65", "name": "Yuhan Shen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec66", "name": "Yuhao Yang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec67", "name": "Yujiang Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec68", "name": "Yunan Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec69", "name": "Yunqing Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec6a", "name": "Yuntao Li", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec6b", "name": "Yurong Wu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec6c", "name": "Yutao Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec6d", "name": "Yuxi Duan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec6e", "name": "Yuxuan Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec6f", "name": "Zezhen Liu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec70", "name": "Zhengtao Jiang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec71", "name": "Zhenhe Yan", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec72", "name": "Zheyu Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec73", "name": "Zhixiang Wei", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec74", "name": "Zhuo Chen", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec75", "name": "Zhuoer Feng", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec76", "name": "Zijun Yao", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec77", "name": "Ziwei Chai", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec78", "name": "Ziyuan Wang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec79", "name": "Zuzhou Zhang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec7a", "name": "Bin Xu", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec7b", "name": "Minlie Huang", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec7c", "user": {"_id": "62ec23fcbd19e355478fc584", "avatarUrl": "/avatars/d7c567ef5f20bb3b9905cb5015d11e12.svg", "isPro": false, "fullname": "Hongning Wang", "user": "howang", "type": "user"}, "name": "Hongning Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:32:02.752Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec7d", "user": {"_id": "65df8cbc2705d9672f55d1aa", "avatarUrl": "/avatars/63e46f15bb76bd9d4508fd0f54f39829.svg", "isPro": false, "fullname": "Juanzi Li", "user": "juanli", "type": "user"}, "name": "Juanzi Li", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:31:56.007Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec7e", "user": {"_id": "640e73bdfdeaae1390857b62", "avatarUrl": "/avatars/cd6779e30f716002a7838ed93d5c0754.svg", "isPro": false, "fullname": "Yuxiao Dong", "user": "yuxiaod", "type": "user"}, "name": "Yuxiao Dong", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:31:48.099Z", "hidden": false}, {"_id": "6995270f8d17d1ee8c10ec7f", "user": {"_id": "640dff05474aa6f89556677e", "avatarUrl": "/avatars/1b4591c7322d649c797b3125148f1915.svg", "isPro": false, "fullname": "Jie Tang", "user": "jerytang", "type": "user"}, "name": "Jie Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-18T12:31:39.525Z", "hidden": false}], "publishedAt": "2026-02-17T17:50:56.000Z", "submittedOnDailyAt": "2026-02-18T00:12:28.521Z", "title": "GLM-5: from Vibe Coding to Agentic Engineering", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.", "upvotes": 31, "discussionId": "6995270f8d17d1ee8c10ec80", "githubRepo": "https://github.com/zai-org/GLM-5", "githubRepoAddedBy": "user", "ai_summary": "GLM-5 advances foundation models with DSA for cost reduction, asynchronous reinforcement learning for improved alignment, and enhanced coding capabilities for real-world software engineering.", "ai_keywords": ["DSA", "asynchronous reinforcement learning", "agentic engineering", "vibe coding", "ARC capabilities", "post-training efficiency", "model alignment", "autonomous agents", "software engineering", "open benchmarks"], "githubStars": 1103, "summary_zh": "<ul>\n    <li>GLM-5\u662f\u4e00\u4e2a\u65b0\u4e00\u4ee3\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u5c06\u6c1b\u56f4\u7f16\u7801\u8f6c\u53d8\u4e3a\u81ea\u4e3b\u5de5\u7a0b\u3002</li>\n    <li>\u5b83\u5728\u524d\u8eab\u7684\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7\u4f7f\u7528DSA\u663e\u8457\u964d\u4f4e\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u957f\u6587\u672c\u7684\u51c6\u786e\u6027\u3002</li>\n    <li>\u5f15\u5165\u65b0\u7684\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u57fa\u7840\u8bbe\u65bd\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u540e\u7684\u6548\u7387\uff0c\u89e3\u8026\u4e86\u751f\u6210\u4e0e\u8bad\u7ec3\u8fc7\u7a0b\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u65b0\u9896\u7684\u5f02\u6b65\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u4f7f\u6a21\u578b\u66f4\u6709\u6548\u5730\u4ece\u590d\u6742\u7684\u957f\u671f\u4ea4\u4e92\u4e2d\u5b66\u4e60\u3002</li>\n    <li>GLM-5\u5728\u4e3b\u8981\u5f00\u653e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u5b9e\u9645\u7f16\u7801\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u4ee5\u524d\u7684\u6c34\u5e73\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>GLM-5 is a new advanced model that improves on coding and reasoning abilities from its previous version.</li>\n    <li>It uses a technique called DSA to lower training and running costs while keeping high-quality performance over long contexts.</li>\n    <li>A new method for reinforcement learning helps the model learn better and faster after it's been trained.</li>\n    <li>GLM-5 shows excellent performance on various tests and excels in real-world coding tasks.</li>\n    <li>More details, including code and models, can be found at https://github.com/zai-org/GLM-5.</li>\n</ul>"}, "publishedAt": "2026-02-17T12:50:56.000Z", "title": "GLM-5: from Vibe Coding to Agentic Engineering", "summary": "We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at https://github.com/zai-org/GLM-5.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.15763.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 234, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.14979", "authors": [{"_id": "69968e1e1268a6b79e0d02f1", "user": {"_id": "64731a68a7f23affe7736d3d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8wESKLFcS2ltPzL-wpG4Z.jpeg", "isPro": false, "fullname": "Ronghao Dang", "user": "RH-Dang", "type": "user"}, "name": "Ronghao Dang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-19T10:00:05.563Z", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02f2", "user": {"_id": "66224557c61c7fbd98099079", "avatarUrl": "/avatars/a4f2144585c808865c73b5b7f0087c1f.svg", "isPro": false, "fullname": "Jiayan Guo", "user": "SpaceProduct", "type": "user"}, "name": "Jiayan Guo", "status": "admin_assigned", "statusLastChangedAt": "2026-02-19T10:00:12.888Z", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02f3", "name": "Bohan Hou", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02f4", "user": {"_id": "609115c79a8bcaa437b234a9", "avatarUrl": "/avatars/1631a91030703d8397133363cf82c863.svg", "isPro": false, "fullname": "Leng Sicong", "user": "Sicong", "type": "user"}, "name": "Sicong Leng", "status": "admin_assigned", "statusLastChangedAt": "2026-02-19T10:00:23.420Z", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02f5", "user": {"_id": "6388af095a3d2a335622cb7c", "avatarUrl": "/avatars/f548ce6a902cee8bdc74179bcd45534c.svg", "isPro": false, "fullname": "Kehan Li", "user": "lkhl", "type": "user"}, "name": "Kehan Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T09:51:43.746Z", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02f6", "name": "Xin Li", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02f7", "user": {"_id": "67a6082b2f32323bfb5e6641", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NZvjm4Kapc7AgReexgRgi.png", "isPro": false, "fullname": "jiangpin", "user": "jiangpinliu", "type": "user"}, "name": "Jiangpin Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-02-19T10:00:30.219Z", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02f8", "user": {"_id": "67fcc97cede5c434e0cc37e3", "avatarUrl": "/avatars/b07e0a4744c1045828a621146ee6d3c2.svg", "isPro": false, "fullname": "yunxuan mao", "user": "maoyunxuan", "type": "user"}, "name": "Yunxuan Mao", "status": "admin_assigned", "statusLastChangedAt": "2026-02-19T10:00:35.800Z", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02f9", "name": "Zhikai Wang", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02fa", "name": "Yuqian Yuan", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02fb", "name": "Minghao Zhu", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02fc", "user": {"_id": "667e2e4ebfbdb7d21df57084", "avatarUrl": "/avatars/dfc6e9e2805c6f22773495c4f02399b8.svg", "isPro": false, "fullname": "Xiao Lin", "user": "Chrislin21", "type": "user"}, "name": "Xiao Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T11:48:50.334Z", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02fd", "name": "Yang Bai", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02fe", "name": "Qian Jiang", "hidden": false}, {"_id": "69968e1e1268a6b79e0d02ff", "name": "Yaxi Zhao", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0300", "name": "Minghua Zeng", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0301", "user": {"_id": "64bbe6904d2052b1aaf4f2d7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bbe6904d2052b1aaf4f2d7/g0A43kSoy2Wba5JZ2Hcry.jpeg", "isPro": false, "fullname": "junlong gao", "user": "jlgao23", "type": "user"}, "name": "Junlong Gao", "status": "admin_assigned", "statusLastChangedAt": "2026-02-19T10:01:11.266Z", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0302", "name": "Yuming Jiang", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0303", "name": "Jun Cen", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0304", "user": {"_id": "65fd82762bf2cd20ddaa193f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png", "isPro": false, "fullname": "Siteng Huang", "user": "huangsiteng", "type": "user"}, "name": "Siteng Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T09:51:46.265Z", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0305", "name": "Liuyi Wang", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0306", "name": "Wenqiao Zhang", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0307", "name": "Chengju Liu", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0308", "name": "Jianfei Yang", "hidden": false}, {"_id": "69968e1e1268a6b79e0d0309", "name": "Shijian Lu", "hidden": false}, {"_id": "69968e1e1268a6b79e0d030a", "name": "Deli Zhao", "hidden": false}], "publishedAt": "2026-02-13T18:59:56.000Z", "submittedOnDailyAt": "2026-02-19T02:00:43.226Z", "title": "RynnBrain: Open Embodied Foundation Models", "submittedOnDailyBy": {"_id": "609115c79a8bcaa437b234a9", "avatarUrl": "/avatars/1631a91030703d8397133363cf82c863.svg", "isPro": false, "fullname": "Leng Sicong", "user": "Sicong", "type": "user"}, "summary": "Despite rapid progress in multimodal foundation models, embodied intelligence community still lacks a unified, physically grounded foundation model that integrates perception, reasoning, and planning within real-world spatial-temporal dynamics. We introduce RynnBrain, an open-source spatiotemporal foundation model for embodied intelligence. RynnBrain strengthens four core capabilities in a unified framework: comprehensive egocentric understanding, diverse spatiotemporal localization, physically grounded reasoning, and physics-aware planning. The RynnBrain family comprises three foundation model scales (2B, 8B, and 30B-A3B MoE) and four post-trained variants tailored for downstream embodied tasks (i.e., RynnBrain-Nav, RynnBrain-Plan, and RynnBrain-VLA) or complex spatial reasoning tasks (i.e., RynnBrain-CoP). In terms of extensive evaluations on 20 embodied benchmarks and 8 general vision understanding benchmarks, our RynnBrain foundation models largely outperform existing embodied foundation models by a significant margin. The post-trained model suite further substantiates two key potentials of the RynnBrain foundation model: (i) enabling physically grounded reasoning and planning, and (ii) serving as a strong pretrained backbone that can be efficiently adapted to diverse embodied tasks.", "upvotes": 26, "discussionId": "69968e1e1268a6b79e0d030b", "projectPage": "https://alibaba-damo-academy.github.io/RynnBrain.github.io/", "githubRepo": "https://github.com/alibaba-damo-academy/RynnBrain", "githubRepoAddedBy": "user", "ai_summary": "RynnBrain is an open-source spatiotemporal foundation model for embodied intelligence that unifies perception, reasoning, and planning capabilities across multiple scales and task-specific variants.", "ai_keywords": ["multimodal foundation models", "embodied intelligence", "spatiotemporal foundation model", "egocentric understanding", "spatiotemporal localization", "physically grounded reasoning", "physics-aware planning", "MoE", "post-trained variants", "embodied foundation models", "vision understanding benchmarks"], "githubStars": 402, "organization": {"_id": "6808e7522a4d69d5111da55f", "name": "Alibaba-DAMO-Academy", "fullname": "DAMO Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"}, "summary_zh": "<ul>\n    <li>RynnBrain\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u589e\u5f3a\u5177\u8eab\u667a\u80fd\u7684\u80fd\u529b\u3002</li>\n    <li>\u5b83\u6574\u5408\u4e86\u611f\u77e5\u3001\u63a8\u7406\u548c\u89c4\u5212\uff0c\u5177\u6709\u5168\u9762\u7684\u81ea\u6211\u4e2d\u5fc3\u7406\u89e3\u548c\u591a\u6837\u7684\u65f6\u7a7a\u5b9a\u4f4d\u80fd\u529b\u3002</li>\n    <li>RynnBrain\u6709\u4e09\u79cd\u57fa\u7840\u6a21\u578b\u89c4\u6a21\u548c\u56db\u79cd\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u7684\u540e\u8bad\u7ec3\u53d8\u4f53\u3002</li>\n    <li>\u572820\u4e2a\u5177\u8eab\u57fa\u51c6\u548c8\u4e2a\u89c6\u89c9\u7406\u89e3\u57fa\u51c6\u7684\u8bc4\u4f30\u4e2d\uff0cRynnBrain\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u652f\u6301\u7269\u7406\u57fa\u7840\u7684\u63a8\u7406\u548c\u89c4\u5212\uff0c\u5e76\u53ef\u9ad8\u6548\u9002\u5e94\u591a\u79cd\u5177\u8eab\u4efb\u52a1\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>RynnBrain is a new open-source model designed to improve embodied intelligence by combining perception, reasoning, and planning.</li>\n    <li>It focuses on four main abilities: understanding one's own perspective, locating objects in space and time, reasoning based on physical laws, and planning using physics.</li>\n    <li>The model comes in three sizes (2B, 8B, and 30B-A3B MoE) and includes four specialized versions for different tasks.</li>\n    <li>RynnBrain outperforms existing models in 20 tests for embodied tasks and 8 tests for general vision understanding.</li>\n    <li>It shows promise for enabling realistic reasoning and planning, and can be adapted for various tasks in embodied intelligence.</li>\n</ul>"}, "publishedAt": "2026-02-13T13:59:56.000Z", "title": "RynnBrain: Open Embodied Foundation Models", "summary": "Despite rapid progress in multimodal foundation models, embodied intelligence community still lacks a unified, physically grounded foundation model that integrates perception, reasoning, and planning within real-world spatial-temporal dynamics. We introduce RynnBrain, an open-source spatiotemporal foundation model for embodied intelligence. RynnBrain strengthens four core capabilities in a unified framework: comprehensive egocentric understanding, diverse spatiotemporal localization, physically grounded reasoning, and physics-aware planning. The RynnBrain family comprises three foundation model scales (2B, 8B, and 30B-A3B MoE) and four post-trained variants tailored for downstream embodied tasks (i.e., RynnBrain-Nav, RynnBrain-Plan, and RynnBrain-VLA) or complex spatial reasoning tasks (i.e., RynnBrain-CoP). In terms of extensive evaluations on 20 embodied benchmarks and 8 general vision understanding benchmarks, our RynnBrain foundation models largely outperform existing embodied foundation models by a significant margin. The post-trained model suite further substantiates two key potentials of the RynnBrain foundation model: (i) enabling physically grounded reasoning and planning, and (ii) serving as a strong pretrained backbone that can be efficiently adapted to diverse embodied tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14979.png", "numComments": 3, "submittedBy": {"_id": "609115c79a8bcaa437b234a9", "avatarUrl": "/avatars/1631a91030703d8397133363cf82c863.svg", "fullname": "Leng Sicong", "name": "Sicong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "6808e7522a4d69d5111da55f", "name": "Alibaba-DAMO-Academy", "fullname": "DAMO Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6808e64de5dd22427c006e10/9J3vdB62CdeTOd_YrGh9w.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.13949", "authors": [{"_id": "69941b5e50fb2c0be4783de6", "user": {"_id": "62e1b3cb3eb0730f621a83f6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg", "isPro": false, "fullname": "Taiwei Shi", "user": "MaksimSTW", "type": "user"}, "name": "Taiwei Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-02-17T15:08:55.504Z", "hidden": false}, {"_id": "69941b5e50fb2c0be4783de7", "name": "Sihao Chen", "hidden": false}, {"_id": "69941b5e50fb2c0be4783de8", "name": "Bowen Jiang", "hidden": false}, {"_id": "69941b5e50fb2c0be4783de9", "user": {"_id": "64d660308ebc40443813f014", "avatarUrl": "/avatars/516bb2d2383be99794e366dfb41636b6.svg", "isPro": false, "fullname": "Linxin Song", "user": "linxinso", "type": "user"}, "name": "Linxin Song", "status": "claimed_verified", "statusLastChangedAt": "2026-02-17T15:08:53.295Z", "hidden": false}, {"_id": "69941b5e50fb2c0be4783dea", "name": "Longqi Yang", "hidden": false}, {"_id": "69941b5e50fb2c0be4783deb", "name": "Jieyu Zhao", "hidden": false}], "publishedAt": "2026-02-15T01:23:48.000Z", "submittedOnDailyAt": "2026-02-17T05:14:40.358Z", "title": "Experiential Reinforcement Learning", "submittedOnDailyBy": {"_id": "62e1b3cb3eb0730f621a83f6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg", "isPro": false, "fullname": "Taiwei Shi", "user": "MaksimSTW", "type": "user"}, "summary": "Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.", "upvotes": 26, "discussionId": "69941b5f50fb2c0be4783dec", "ai_summary": "Experiential Reinforcement Learning introduces an explicit experience-reflection-consolidation loop that improves learning efficiency and performance in sparse-reward environments by enabling structured behavioral revision without additional inference costs.", "ai_keywords": ["reinforcement learning", "environmental feedback", "policy training", "self-reflection", "behavioral revision", "exploration", "optimization"], "organization": {"_id": "5e6485f787403103f9f1055e", "name": "microsoft", "fullname": "Microsoft", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\u6210\u4e3a\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u73af\u5883\u53cd\u9988\u7684\u4e3b\u8981\u65b9\u6cd5\u3002</li>\n    <li>\u73af\u5883\u53cd\u9988\u901a\u5e38\u7a00\u758f\u4e14\u5ef6\u8fdf\uff0c\u5bfc\u81f4\u5b66\u4e60\u53d8\u5f97\u56f0\u96be\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4f53\u9a8c\u5f3a\u5316\u5b66\u4e60\uff08ERL\uff09\uff0c\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u52a0\u5165\u7ecf\u9a8c\u53cd\u601d\u548c\u5de9\u56fa\u7684\u5faa\u73af\u3002</li>\n    <li>ERL\u63d0\u9ad8\u4e86\u63a2\u7d22\u6548\u7387\u548c\u4f18\u5316\u7a33\u5b9a\u6027\uff0c\u4e14\u65e0\u9700\u989d\u5916\u63a8\u7406\u6210\u672c\u3002</li>\n    <li>\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\uff0cERL\u76f8\u8f83\u4e8e\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5b66\u4e60\u6548\u7387\u548c\u6700\u7ec8\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning helps language models learn from rewards or feedback from their environment, but this feedback is often sparse and delayed.</li>\n    <li>Experiential Reinforcement Learning (ERL) is a new training method that includes a loop of experience, reflection, and consolidation to improve learning.</li>\n    <li>In ERL, the model tries a task, gets feedback, reflects on it, and then makes a better attempt based on that reflection.</li>\n    <li>This method enhances learning efficiency and performance, showing improvements of up to 81% in complex tasks and 11% in reasoning tasks.</li>\n    <li>ERL effectively turns feedback into lasting improvements in behavior, making it a valuable addition to reinforcement learning. </li>\n</ul>"}, "publishedAt": "2026-02-14T20:23:48.000Z", "title": "Experiential Reinforcement Learning", "summary": "Reinforcement learning has become the central approach for language models (LMs) to learn from environmental reward or feedback. In practice, the environmental feedback is usually sparse and delayed. Learning from such signals is challenging, as LMs must implicitly infer how observed failures should translate into behavioral changes for future iterations. We introduce Experiential Reinforcement Learning (ERL), a training paradigm that embeds an explicit experience-reflection-consolidation loop into the reinforcement learning process. Given a task, the model generates an initial attempt, receives environmental feedback, and produces a reflection that guides a refined second attempt, whose success is reinforced and internalized into the base policy. This process converts feedback into structured behavioral revision, improving exploration and stabilizing optimization while preserving gains at deployment without additional inference cost. Across sparse-reward control environments and agentic reasoning benchmarks, ERL consistently improves learning efficiency and final performance over strong reinforcement learning baselines, achieving gains of up to +81% in complex multi-step environments and up to +11% in tool-using reasoning tasks. These results suggest that integrating explicit self-reflection into policy training provides a practical mechanism for transforming feedback into durable behavioral improvement.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.13949.png", "numComments": 2, "submittedBy": {"_id": "62e1b3cb3eb0730f621a83f6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658958764563-noauth.jpeg", "fullname": "Taiwei Shi", "name": "MaksimSTW", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "5e6485f787403103f9f1055e", "name": "microsoft", "fullname": "Microsoft", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1583646260758-5e64858c87403103f9f1055d.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.16705", "authors": [{"_id": "69967a291268a6b79e0d02bb", "user": {"_id": "6201fc5d91d53938a6432fbf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg", "isPro": false, "fullname": "Runpei Dong", "user": "RunpeiDong", "type": "user"}, "name": "Runpei Dong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-19T09:51:52.994Z", "hidden": false}, {"_id": "69967a291268a6b79e0d02bc", "name": "Ziyan Li", "hidden": false}, {"_id": "69967a291268a6b79e0d02bd", "name": "Xialin He", "hidden": false}, {"_id": "69967a291268a6b79e0d02be", "name": "Saurabh Gupta", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/g69sakZn_StFwG0neUtqz.mp4"], "publishedAt": "2026-02-18T18:55:02.000Z", "submittedOnDailyAt": "2026-02-19T00:21:31.776Z", "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation", "submittedOnDailyBy": {"_id": "6201fc5d91d53938a6432fbf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg", "isPro": false, "fullname": "Runpei Dong", "user": "RunpeiDong", "type": "user"}, "summary": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.", "upvotes": 25, "discussionId": "69967a2a1268a6b79e0d02bf", "projectPage": "https://hero-humanoid.github.io/", "ai_summary": "HERO enables humanoid robots to perform object manipulation in diverse real-world environments by combining accurate end-effector control with open-vocabulary vision models for generalizable scene understanding.", "ai_keywords": ["end-effector tracking policy", "inverse kinematics", "neural forward model", "goal adjustment", "replanning", "open-vocabulary large vision models", "loco-manipulation", "visual generalization"], "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5HERO\uff0c\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u64cd\u63a7\u5404\u79cd\u7269\u4f53\u3002</li>\n    <li>\u7ed3\u5408\u4e86\u5927\u578b\u89c6\u89c9\u6a21\u578b\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u6a21\u62df\u8bad\u7ec3\u7684\u5f3a\u63a7\u5236\u6027\u80fd\u3002</li>\n    <li>\u8bbe\u8ba1\u4e86\u4e00\u79cd\u51c6\u786e\u7684\u672b\u7aef\u6267\u884c\u5668\u8ddf\u8e2a\u7b56\u7565\uff0c\u51cf\u5c11\u4e86\u8ddf\u8e2a\u8bef\u5dee3.2\u500d\u3002</li>\n    <li>\u7cfb\u7edf\u80fd\u591f\u5728\u4e0d\u540c\u7684\u73b0\u5b9e\u73af\u5883\u4e2d\u5de5\u4f5c\uff0c\u6bd4\u5982\u529e\u516c\u5ba4\u548c\u5496\u5561\u5e97\uff0c\u53ef\u9760\u5730\u64cd\u63a7\u65e5\u5e38\u7269\u54c1\u3002</li>\n    <li>\u901a\u8fc7\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u7684\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u8be5\u8bbe\u8ba1\u7684\u6709\u6548\u6027\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u4e0e\u65e5\u5e38\u7269\u4f53\u7684\u4e92\u52a8\u5f00\u8f9f\u4e86\u65b0\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper introduces HERO, a new method for humanoid robots to manipulate objects accurately in various environments.</li>\n    <li>HERO combines advanced visual understanding from large vision models with effective control techniques from simulations.</li>\n    <li>It features a special end-effector tracking policy that significantly reduces tracking errors by 3.2 times.</li>\n    <li>The system can work in different real-world settings, effectively handling common objects like mugs and toys.</li>\n    <li>Tests show that HERO is effective both in simulations and in real-world applications, potentially improving how robots interact with everyday items.</li>\n</ul>"}, "publishedAt": "2026-02-18T13:55:02.000Z", "title": "Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation", "summary": "Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/g69sakZn_StFwG0neUtqz.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.16705.png", "numComments": 2, "submittedBy": {"_id": "6201fc5d91d53938a6432fbf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg", "fullname": "Runpei Dong", "name": "RunpeiDong", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.14299", "authors": [{"_id": "69952cc38d17d1ee8c10ec97", "name": "Ming Li", "hidden": false}, {"_id": "69952cc38d17d1ee8c10ec98", "user": {"_id": "6534a434e778506c5b1e5be8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6534a434e778506c5b1e5be8/349SdAnjEdIQJSzWvKfZ4.png", "isPro": true, "fullname": "Xirui Li", "user": "AIcell", "type": "user"}, "name": "Xirui Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:52.645Z", "hidden": false}, {"_id": "69952cc38d17d1ee8c10ec99", "user": {"_id": "647f5af5b0e96764589f3b2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg", "isPro": false, "fullname": "Tianyi Zhou", "user": "zhoutianyi", "type": "user"}, "name": "Tianyi Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-02-18T09:07:54.520Z", "hidden": false}], "publishedAt": "2026-02-15T20:15:28.000Z", "submittedOnDailyAt": "2026-02-18T00:37:23.132Z", "title": "Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook", "submittedOnDailyBy": {"_id": "65031d01cccc7b28a388c719", "avatarUrl": "/avatars/9d8c94b6ab8ad8b4faba3221b7e76053.svg", "isPro": false, "fullname": "Ming Li", "user": "MingLiiii", "type": "user"}, "summary": "As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.", "upvotes": 21, "discussionId": "69952cc38d17d1ee8c10ec9a", "projectPage": "https://github.com/MingLiiii/Moltbook_Socialization", "githubRepo": "https://github.com/MingLiiii/Moltbook_Socialization", "githubRepoAddedBy": "user", "ai_summary": "Large language model agents in networked environments exhibit dynamic stability without true social convergence, maintaining individual diversity while lacking collective influence structures.", "ai_keywords": ["large language model agents", "networked environments", "AI agent societies", "semantic stabilization", "lexical turnover", "individual inertia", "influence persistence", "collective consensus", "systemic diagnosis", "dynamic evolution"], "githubStars": 7, "organization": {"_id": "647f5b7daa8c04bbf938c625", "name": "umd-zhou-lab", "fullname": "Tianyi Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/wEb1ZgAFz8MshalPJq2wW.jpeg"}, "summary_zh": "<ul>\n    <li>\u7814\u7a76\u4e86\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u793e\u4f1a\u662f\u5426\u4f1a\u50cf\u4eba\u7c7b\u793e\u4f1a\u4e00\u6837\u53d1\u751f\u8d8b\u540c\u52a8\u6001\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5316\u8bca\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790AI\u4ee3\u7406\u793e\u4f1a\u7684\u52a8\u6001\u6f14\u53d8\u3002</li>\n    <li>\u53d1\u73b0\u5c3d\u7ba1\u5168\u7403\u8bed\u4e49\u5e73\u5747\u503c\u5f88\u5feb\u7a33\u5b9a\uff0c\u4f46\u4e2a\u4f53\u4ee3\u7406\u4fdd\u6301\u9ad8\u5ea6\u591a\u6837\u6027\u3002</li>\n    <li>\u4ee3\u7406\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u4e2a\u4f53\u60ef\u6027\uff0c\u5bfc\u81f4\u7f3a\u4e4f\u76f8\u4e92\u5f71\u54cd\u548c\u5171\u8bc6\u3002</li>\n    <li>\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u4ec5\u6709\u89c4\u6a21\u548c\u4e92\u52a8\u5bc6\u5ea6\u4e0d\u8db3\u4ee5\u4fc3\u8fdb\u793e\u4f1a\u5316\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Researchers studied how AI agent societies evolve, similar to human social systems.</li>\n    <li>They created a framework to measure various aspects of these AI societies, like stability and diversity.</li>\n    <li>The study found that while overall language use in the society stabilizes quickly, individual agents maintain a lot of variation.</li>\n    <li>Agents show little change in response to each other, leading to short-lived influence and no strong leaders.</li>\n    <li>The research suggests that simply having more agents and interactions isn't enough for them to socialize effectively.</li>\n</ul>"}, "publishedAt": "2026-02-15T15:15:28.000Z", "title": "Does Socialization Emerge in AI Agent Society? A Case Study of Moltbook", "summary": "As large language model agents increasingly populate networked environments, a fundamental question arises: do artificial intelligence (AI) agent societies undergo convergence dynamics similar to human social systems? Lately, Moltbook approximates a plausible future scenario in which autonomous agents participate in an open-ended, continuously evolving online society. We present the first large-scale systemic diagnosis of this AI agent society. Beyond static observation, we introduce a quantitative diagnostic framework for dynamic evolution in AI agent societies, measuring semantic stabilization, lexical turnover, individual inertia, influence persistence, and collective consensus. Our analysis reveals a system in dynamic balance in Moltbook: while global semantic averages stabilize rapidly, individual agents retain high diversity and persistent lexical turnover, defying homogenization. However, agents exhibit strong individual inertia and minimal adaptive response to interaction partners, preventing mutual influence and consensus. Consequently, influence remains transient with no persistent supernodes, and the society fails to develop stable collective influence anchors due to the absence of shared social memory. These findings demonstrate that scale and interaction density alone are insufficient to induce socialization, providing actionable design and analysis principles for upcoming next-generation AI agent societies.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.14299.png", "numComments": 3, "submittedBy": {"_id": "65031d01cccc7b28a388c719", "avatarUrl": "/avatars/9d8c94b6ab8ad8b4faba3221b7e76053.svg", "fullname": "Ming Li", "name": "MingLiiii", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "647f5b7daa8c04bbf938c625", "name": "umd-zhou-lab", "fullname": "Tianyi Lab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/wEb1ZgAFz8MshalPJq2wW.jpeg"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2602.05400", "authors": [{"_id": "698b396b1b2dc6b37d61b4be", "user": {"_id": "66968099c952e09a4cb29f78", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66968099c952e09a4cb29f78/n90NI2R3E9_RqCyMjDCQF.webp", "isPro": false, "fullname": "Wang", "user": "Steven-Shaobo", "type": "user"}, "name": "Shaobo Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:57.815Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4bf", "user": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "isPro": false, "fullname": "Xuan Ouyang", "user": "YoungXuan", "type": "user"}, "name": "Xuan Ouyang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:55.631Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c0", "user": {"_id": "6518a144a28f86d3e9e67c34", "avatarUrl": "/avatars/f2aed39e971cffe6c9d0b9c2f7a0df70.svg", "isPro": false, "fullname": "Tianyi Xu", "user": "tianyi0216", "type": "user"}, "name": "Tianyi Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:53.605Z", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c1", "name": "Yuzheng Hu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c2", "name": "Jialin Liu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c3", "name": "Guo Chen", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c4", "name": "Tianyu Zhang", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c5", "name": "Junhao Zheng", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c6", "name": "Kexin Yang", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c7", "name": "Xingzhang Ren", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c8", "name": "Dayiheng Liu", "hidden": false}, {"_id": "698b396b1b2dc6b37d61b4c9", "name": "Linfeng Zhang", "hidden": false}], "publishedAt": "2026-02-05T07:34:23.000Z", "submittedOnDailyAt": "2026-02-11T02:09:03.945Z", "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "submittedOnDailyBy": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "isPro": false, "fullname": "Xuan Ouyang", "user": "YoungXuan", "type": "user"}, "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "upvotes": 279, "discussionId": "698b396b1b2dc6b37d61b4ca", "ai_summary": "OPUS is a dynamic data selection framework that improves pre-training efficiency by scoring data candidates based on optimizer-induced update projections in a stable proxy-derived target space, achieving superior performance with reduced computational overhead.", "ai_keywords": ["data selection", "optimizer-induced update space", "effective updates", "stable in-distribution proxy", "Ghost technique", "CountSketch", "Boltzmann sampling", "pre-training", "GPT-2", "Qwen3-8B-Base", "FineWeb", "FineWeb-Edu", "SciencePedia"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u9ad8\u8d28\u91cf\u516c\u5171\u6587\u672c\u63a5\u8fd1\u67af\u7aed\uff0c\u9884\u8bad\u7ec3\u65b9\u6cd5\u4ece\u66f4\u591a\u7684\u6807\u8bb0\u8f6c\u5411\u66f4\u597d\u7684\u6807\u8bb0\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u8fc7\u6ee4\u5668\u6216\u57fa\u4e8e\u539f\u59cb\u68af\u5ea6\u7684\u52a8\u6001\u6807\u51c6\uff0c\u7f3a\u4e4f\u6709\u6548\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86OPUS\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u5668\u5f15\u5bfc\u7684\u66f4\u65b0\u7a7a\u95f4\u6765\u5b9a\u4e49\u6570\u636e\u9009\u62e9\u7684\u6709\u6548\u6027\u3002</li>\n    <li>OPUS\u5229\u7528Ghost\u6280\u672f\u548cBoltzmann\u91c7\u6837\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u6570\u636e\u591a\u6837\u6027\uff0c\u4ec5\u589e\u52a04.7%\u7684\u8ba1\u7b97\u5f00\u9500\u3002</li>\n    <li>\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\uff0cOPUS\u5728\u9884\u8bad\u7ec3\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u9ad8\u6570\u636e\u6548\u7387\uff0c\u5c24\u5176\u5728\u4e13\u4e1a\u9886\u57df\u3002 </li>\n</ul>", "summary_simple": "<ul>\n    <li>As high-quality public text becomes scarce, pre-training methods are focusing on selecting better data instead of just more data.</li>\n    <li>The proposed method, OPUS, uses a new way to choose data based on how updates from optimizers work, aiming for better training results.</li>\n    <li>OPUS scores data candidates efficiently and ensures diversity while adding only a small amount of extra computing cost.</li>\n    <li>In tests, OPUS showed impressive results, outperforming traditional methods and even using less data for training.</li>\n    <li>It also improves training efficiency when combined with existing data filtering methods, especially in specialized areas.</li>\n</ul>"}, "publishedAt": "2026-02-05T02:34:23.000Z", "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration", "summary": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.05400.png", "numComments": 2, "submittedBy": {"_id": "67e617d4470f96a302734e16", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QHrYmNlTRxKR1KRS50pkf.png", "fullname": "Xuan Ouyang", "name": "YoungXuan", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 23, "isUserFollowing": false}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.10388", "authors": [{"_id": "698d3bd265c0d15a6d16200e", "user": {"_id": "6951c555b519522f565dfd0c", "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg", "isPro": false, "fullname": "ZhongzhiLi", "user": "Zhongzhi1228", "type": "user"}, "name": "Zhongzhi Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:57:05.580Z", "hidden": false}, {"_id": "698d3bd265c0d15a6d16200f", "name": "Xuansheng Wu", "hidden": false}, {"_id": "698d3bd265c0d15a6d162010", "name": "Yijiang Li", "hidden": false}, {"_id": "698d3bd265c0d15a6d162011", "name": "Lijie Hu", "hidden": false}, {"_id": "698d3bd265c0d15a6d162012", "name": "Ninghao Liu", "hidden": false}], "publishedAt": "2026-02-11T00:23:13.000Z", "submittedOnDailyAt": "2026-02-16T02:31:34.708Z", "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs", "submittedOnDailyBy": {"_id": "6951c555b519522f565dfd0c", "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg", "isPro": false, "fullname": "ZhongzhiLi", "user": "Zhongzhi1228", "type": "user"}, "summary": "The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.", "upvotes": 200, "discussionId": "698d3bd265c0d15a6d162013", "projectPage": "https://website-sigma-three-35.vercel.app/", "githubRepo": "https://github.com/Zhongzhi660/FAC-Synthesis", "githubRepoAddedBy": "user", "ai_summary": "Feature Activation Coverage measures data diversity in an interpretable feature space and enables diversity-driven data synthesis that improves downstream performance across multiple language model architectures.", "ai_keywords": ["Feature Activation Coverage", "sparse autoencoder", "data diversity", "downstream performance", "instruction following", "toxicity detection", "reward modeling", "behavior steering", "cross-model knowledge transfer", "data-centric optimization"], "githubStars": 52, "summary_zh": "<ul>\n    <li>\u540e\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6548\u679c\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u6587\u672c\u6307\u6807\u6765\u8861\u91cf\u591a\u6837\u6027\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u5bf9\u5f71\u54cd\u4e0b\u6e38\u6027\u80fd\u7684\u7279\u5f81\u4fe1\u53f7\u8f83\u5f31\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6837\u6027\u6d4b\u91cf\u6307\u6807\uff0c\u79f0\u4e3a\u7279\u5f81\u6fc0\u6d3b\u8986\u76d6\uff08FAC\uff09\uff0c\u7528\u4e8e\u5728\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u7a7a\u95f4\u4e2d\u8bc4\u4f30\u6570\u636e\u591a\u6837\u6027\u3002</li>\n    <li>\u57fa\u4e8eFAC\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u53cd\u6620\u7f3a\u5931\u7279\u5f81\u7684\u5408\u6210\u6837\u672c\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u63d0\u9ad8\u4e86\u6570\u636e\u591a\u6837\u6027\u548c\u4e0b\u6e38\u6027\u80fd\uff0c\u5e76\u53d1\u73b0\u4e86\u4e0d\u540c\u6a21\u578b\u95f4\u5171\u4eab\u7684\u53ef\u89e3\u91ca\u7279\u5f81\u7a7a\u95f4\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The diversity of data after training is important for how well large language models perform on tasks.</li>\n    <li>Current methods for measuring data diversity often focus on language features, which may not relate well to important task-related features.</li>\n    <li>This study introduces a new metric called Feature Activation Coverage (FAC) that measures diversity in a way that's easier to understand and relates to model performance.</li>\n    <li>They developed a data synthesis method called FAC Synthesis that finds missing features in existing data and generates new samples to improve diversity.</li>\n    <li>Tests show this method enhances data diversity and improves model performance across various tasks and allows knowledge transfer between different model types.</li>\n</ul>"}, "publishedAt": "2026-02-10T19:23:13.000Z", "title": "Less is Enough: Synthesizing Diverse Data in Feature Space of LLMs", "summary": "The diversity of post-training data is critical for effective downstream performance in large language models (LLMs). Many existing approaches to constructing post-training data quantify diversity using text-based metrics that capture linguistic variation, but such metrics provide only weak signals for the task-relevant features that determine downstream performance. In this work, we introduce Feature Activation Coverage (FAC) which measures data diversity in an interpretable feature space. Building upon this metric, we further propose a diversity-driven data synthesis framework, named FAC Synthesis, that first uses a sparse autoencoder to identify missing features from a seed dataset, and then generates synthetic samples that explicitly reflect these features. Experiments show that our approach consistently improves both data diversity and downstream performance on various tasks, including instruction following, toxicity detection, reward modeling, and behavior steering. Interestingly, we identify a shared, interpretable feature space across model families (i.e., LLaMA, Mistral, and Qwen), enabling cross-model knowledge transfer. Our work provides a solid and practical methodology for exploring data-centric optimization of LLMs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10388.png", "numComments": 2, "submittedBy": {"_id": "6951c555b519522f565dfd0c", "avatarUrl": "/avatars/9028d619483f359639ae7bfe4769da45.svg", "fullname": "ZhongzhiLi", "name": "Zhongzhi1228", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.04705", "authors": [{"_id": "698424a7e34659da7e1f4e6f", "name": "Haifeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e70", "name": "Hua Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e71", "name": "Tian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e72", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4e73", "name": "Jing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e74", "name": "Dianhai Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e75", "name": "Yanjun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4e76", "name": "Jingzhou He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e77", "name": "Zhongjun He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e78", "name": "Dou Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e79", "name": "Qiwen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7a", "name": "Shuohuan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7b", "user": {"_id": "62cd9632342b1d5dab8df4c3", "avatarUrl": "/avatars/9080d20bb57a05a1eeb6800eba886cf9.svg", "isPro": false, "fullname": "Junyuan Shang", "user": "sjy1203", "type": "user"}, "name": "Junyuan Shang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:28.482Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7c", "user": {"_id": "67f37f78b36e82d366dedeec", "avatarUrl": "/avatars/678bb5891d5c2e80edc0799d2308a5d3.svg", "isPro": false, "fullname": "Max Zhenyu Zhang", "user": "max-zhenyu-zhang", "type": "user"}, "name": "Zhenyu Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:03.972Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7d", "name": "Yuchen Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7e", "name": "Jinle Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e7f", "name": "Jiabin Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e80", "name": "Liang Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e81", "name": "Ruibiao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e82", "name": "Weichong Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4e83", "name": "Siyu Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4e84", "name": "Dai Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4e85", "name": "Shikun Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e86", "name": "Siqi Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e87", "name": "Bolei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4e88", "name": "Yan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e89", "name": "Zhenyu Jiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8a", "name": "Ruiqing Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8b", "name": "Zeyu Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8c", "name": "Qingqing Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8d", "name": "Kaipeng Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8e", "name": "Jiajun Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e8f", "name": "Enlei Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4e90", "name": "Guoxia Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e91", "name": "Yanlin Sha", "hidden": false}, {"_id": "698424a7e34659da7e1f4e92", "name": "Yi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e93", "name": "Yehan Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e94", "name": "Weijian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e95", "name": "Jiaxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e96", "name": "Zengfeng Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4e97", "name": "Yingqi Qu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e98", "name": "Zhongli Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4e99", "name": "Zhengkun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9a", "name": "Xiyang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9b", "name": "Zixiang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9c", "name": "Xinchao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9d", "name": "Zhengjie Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9e", "name": "Dong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4e9f", "name": "Bingjin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea0", "name": "Yue Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea1", "name": "Xing Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea2", "name": "Shiwei Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea3", "name": "Qiao Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea4", "name": "Xinzhe Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea5", "name": "Shuangshuang Qiao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea6", "name": "Baoshan Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea7", "name": "Bihong Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea8", "name": "Bin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ea9", "name": "Bingquan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaa", "name": "Binhan Tang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eab", "name": "Binxiong Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4eac", "name": "Bo Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4ead", "name": "Bo Ke", "hidden": false}, {"_id": "698424a7e34659da7e1f4eae", "name": "Bo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eaf", "name": "Bowen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb0", "name": "Boyan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb1", "name": "Boyang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb2", "name": "Caiji Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb3", "name": "Can Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb4", "name": "Chang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb5", "name": "Chao Pang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb6", "name": "Chao Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb7", "name": "Chaoyi Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb8", "name": "Chen Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eb9", "name": "Cheng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4eba", "name": "Chenlin Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebb", "name": "Chun Gan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebc", "name": "Chunguang Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebd", "name": "Chuyu Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebe", "name": "Cuiyun Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4ebf", "name": "Dan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec0", "name": "Danlei Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec1", "name": "Danxiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec2", "name": "Dong Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec3", "name": "Dongbo Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec4", "name": "Dongdong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec5", "name": "Dongdong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec6", "name": "Dongxue Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec7", "name": "Fan Ding", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec8", "name": "Fan Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ec9", "name": "Fan Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eca", "name": "Fan Mo", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecb", "name": "Feisheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecc", "name": "Fengwei Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecd", "name": "Gangqiang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ece", "name": "Gaofeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ecf", "name": "Gaopeng Yong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed0", "name": "Gexiao Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed1", "user": {"_id": "698419de94015f1e5eedacec", "avatarUrl": "/avatars/e80baa6f9efcd5e5d7cc9b93ac852c7b.svg", "isPro": false, "fullname": "Guan Wang", "user": "guanwcn", "type": "user"}, "name": "Guan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:38.213Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed2", "name": "Guangchen Ni", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed3", "name": "Guangshuo Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed4", "name": "Guanzhong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed5", "user": {"_id": "609cd5ab335f23cd2fa0f211", "avatarUrl": "/avatars/8331a7025a6aa4eabc5b6502bf8a0a63.svg", "isPro": false, "fullname": "Guihua Liu", "user": "LLLL", "type": "user"}, "name": "Guihua Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:31.029Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed6", "name": "Guishun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed7", "name": "Haibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed8", "name": "Haijian Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ed9", "name": "Haipeng Ming", "hidden": false}, {"_id": "698424a7e34659da7e1f4eda", "name": "Haisu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4edb", "name": "Haiyang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4edc", "name": "Haiye Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4edd", "name": "Han Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ede", "name": "Hangting Lou", "hidden": false}, {"_id": "698424a7e34659da7e1f4edf", "name": "Hanwen Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee0", "name": "Hanzhi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee1", "name": "Hao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee2", "name": "Hao Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee3", "name": "Hao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee4", "name": "Hao Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee5", "name": "Haochen Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee6", "name": "Haodong Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee7", "name": "Haoshuang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee8", "name": "Haozhe Geng", "hidden": false}, {"_id": "698424a7e34659da7e1f4ee9", "name": "Heju Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4eea", "name": "Hong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4eeb", "name": "Hongchen Xue", "hidden": false}, {"_id": "698424a7e34659da7e1f4eec", "name": "Hongen Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eed", "name": "Honggeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4eee", "name": "Hongji Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4eef", "name": "Hongwei Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef0", "name": "Hongyang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef1", "name": "Hongyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef2", "name": "Hua Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef3", "name": "Huan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef4", "name": "Huan Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef5", "name": "Huang He", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef6", "name": "Hui Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef7", "name": "Hui Zhong", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef8", "name": "Huibin Ruan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ef9", "name": "Jiafeng Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efa", "name": "Jiage Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efb", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efc", "name": "Jiahao Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4efd", "name": "Jiajie Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4efe", "name": "Jialin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4eff", "name": "Jian Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f00", "name": "Jian Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f01", "name": "Jianfeng Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f02", "name": "Jianguang Jiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f03", "name": "Jianhua Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f04", "name": "Jianye Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f05", "name": "Jiaodi Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f06", "name": "Jiarui Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f07", "name": "Jiawei Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f08", "name": "Jiaxin Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f09", "name": "Jiaxuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0a", "name": "Jie Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0b", "name": "Jie Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0c", "name": "Jiefan Fang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0d", "name": "Jihan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0e", "name": "Jihua Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f0f", "name": "Jing Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f10", "name": "Jing Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f11", "name": "Jing Yan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f12", "name": "Jingdong Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f13", "name": "Jingdong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f14", "name": "Jingjing Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f15", "name": "Jingyong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f16", "name": "Jinheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f17", "name": "Jinjin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f18", "name": "Jinliang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f19", "name": "Jinlin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1a", "name": "Jinnan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1b", "name": "Jixiang Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1c", "name": "Jiyi Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1d", "name": "Jiyuan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1e", "name": "Jun Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f1f", "name": "Jun Xia", "hidden": false}, {"_id": "698424a7e34659da7e1f4f20", "name": "Jun Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f21", "name": "Junda Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f22", "name": "Junhao Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f23", "name": "Junhong Xiang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f24", "name": "Junliang Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f25", "name": "Kai Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f26", "name": "Kailun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f27", "name": "Kairan Su", "hidden": false}, {"_id": "698424a7e34659da7e1f4f28", "name": "Kang Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f29", "name": "Kangkang Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2a", "name": "Ke Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2b", "name": "Ke Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2c", "name": "Kui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2d", "name": "Kun Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2e", "name": "Kunbin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f2f", "name": "Lei Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4f30", "name": "Lei Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f31", "name": "Lei Wen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f32", "name": "Linghui Meng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f33", "user": {"_id": "641e69355c348064a8251471", "avatarUrl": "/avatars/acad3877df27ff44ea3921bb43e34d53.svg", "isPro": false, "fullname": "Linhao Yu", "user": "HasuerYu", "type": "user"}, "name": "Linhao Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:47.812Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f34", "name": "Liping Ouyang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f35", "name": "Liwen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f36", "user": {"_id": "65cf859f88d13d8128bb8545", "avatarUrl": "/avatars/aa18b993bd90d9c8a95913050cd955a8.svg", "isPro": false, "fullname": "Longbin Ji", "user": "robingg1", "type": "user"}, "name": "Longbin Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:40.295Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f37", "name": "Longzhi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f38", "name": "Meng Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f39", "name": "Meng Tian", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3a", "name": "Mengfei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3b", "name": "Mengqi Zeng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3c", "name": "Mengyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3d", "name": "Ming Hong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3e", "name": "Mingcheng Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f3f", "name": "Mingming Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f40", "name": "Mingxin Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4f41", "name": "Mingzhu Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f42", "name": "Naibin Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f43", "name": "Nemin Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f44", "name": "Nian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f45", "name": "Peng Qiu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f46", "name": "Peng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f47", "name": "Pengyu Zou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f48", "name": "Qi Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f49", "name": "Qi Xin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4a", "name": "Qian Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4b", "name": "Qiang Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4c", "name": "Qianhui Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4d", "name": "Qianwei Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4e", "name": "Qianyue He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f4f", "name": "Qifei Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f50", "name": "Qinrui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f51", "name": "Qiwen Bao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f52", "name": "Quan Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f53", "name": "Quanxiang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f54", "name": "Qunyi Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4f55", "name": "Rongrui Zhan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f56", "name": "Rufeng Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4f57", "name": "Rui Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f58", "name": "Ruian Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f59", "name": "Ruihao Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5a", "name": "Ruijie Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5b", "name": "Ruixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5c", "name": "Ruixuan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5d", "name": "Runsheng Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5e", "name": "Ruting Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f5f", "name": "Senbo Kang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f60", "name": "Shan Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f61", "name": "Shaofei Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f62", "name": "Shaotian Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f63", "name": "Shenwei Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f64", "name": "Shifeng Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f65", "name": "Shihao Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f66", "name": "Shilong Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f67", "name": "Shiqin Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f68", "name": "Shiwei Gu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f69", "name": "Shixi Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6a", "name": "Shuai Yao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6b", "name": "Shuang Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6c", "name": "Shuangqiao Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6d", "name": "Shuhao Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6e", "name": "Shuwei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f6f", "name": "Shuwen Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f70", "user": {"_id": "62769a608483d8e9ecd9b4f8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672799958233-62769a608483d8e9ecd9b4f8.jpeg", "isPro": false, "fullname": "Sijun He", "user": "sijunhe", "type": "user"}, "name": "Sijun He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:33.392Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f71", "user": {"_id": "64fada13d82fc6977d5e9c74", "avatarUrl": "/avatars/776bf1257154289e919716637770ef52.svg", "isPro": false, "fullname": "Siming Dai", "user": "DesmonDay", "type": "user"}, "name": "Siming Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:52:50.302Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f72", "name": "Siming Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f73", "name": "Siyi Long", "hidden": false}, {"_id": "698424a7e34659da7e1f4f74", "name": "Songhe Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f75", "name": "Suhui Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f76", "name": "Suyin Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f77", "name": "Teng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f78", "name": "Tianchan Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f79", "name": "Tianliang Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7a", "user": {"_id": "67bbe929593452cc18877606", "avatarUrl": "/avatars/f50fd1cb35d628c26cf21ad0c95c55b1.svg", "isPro": false, "fullname": "tmyangcs", "user": "youngtimmy", "type": "user"}, "name": "Tianmeng Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:36.143Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7b", "name": "Tianyi Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7c", "name": "Tiezhu Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7d", "name": "Ting Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7e", "name": "Ting Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f7f", "name": "Tingdan Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f80", "name": "Wei He", "hidden": false}, {"_id": "698424a7e34659da7e1f4f81", "name": "Wei Luan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f82", "name": "Wei Yin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f83", "name": "Wei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f84", "name": "Wei Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4f85", "name": "Weibao Gong", "hidden": false}, {"_id": "698424a7e34659da7e1f4f86", "name": "Weibin Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4f87", "name": "Weicheng Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f88", "name": "Weichong Dang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f89", "name": "Weiguo Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8a", "name": "Weilong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8b", "name": "Weiqi Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8c", "name": "Wen Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8d", "name": "Wenbin Chang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8e", "name": "Wenjing Du", "hidden": false}, {"_id": "698424a7e34659da7e1f4f8f", "name": "Wenlong Miao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f90", "name": "Wenpei Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f91", "name": "Wenquan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f92", "name": "Xi Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4f93", "name": "Xi Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f94", "name": "Xiang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4f95", "name": "Xiangguo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f96", "name": "Xiangrui Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4f97", "name": "Xiangsen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f98", "name": "Xiangzhe Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f99", "name": "Xianlong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9a", "name": "Xianying Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9b", "name": "Xiao Tan", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9c", "name": "Xiaocong Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9d", "name": "Xiaofei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9e", "name": "Xiaofeng Peng", "hidden": false}, {"_id": "698424a7e34659da7e1f4f9f", "name": "Xiaofeng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa0", "name": "Xiaojian Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa1", "name": "Xiaolan Yuan", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa2", "name": "Xiaopeng Cui", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa3", "name": "Xiaotian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa4", "name": "Xiaoxiong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa5", "name": "Xiaoxu Fei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa6", "name": "Xiaoxuan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa7", "user": {"_id": "664395621b88258a527cd7d1", "avatarUrl": "/avatars/8489ccebe4fd1262679ba63a5cb50bb8.svg", "isPro": false, "fullname": "Kira", "user": "Kira-wang", "type": "user"}, "name": "Xiaoyu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-05T10:53:25.774Z", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa8", "name": "Xiaoyu Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fa9", "name": "Xin Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4faa", "name": "Xin Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fab", "name": "Xinhui Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fac", "name": "Xinming Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fad", "name": "Xintong Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fae", "name": "Xinyi Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4faf", "name": "Xinyu Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb0", "name": "Xiuxian Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb1", "name": "XuanShi Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb2", "name": "Xue Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb3", "name": "Xueying Lv", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb4", "name": "Xuhong Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb5", "name": "Xulong Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb6", "name": "Xuyi Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb7", "name": "Yabing Shi", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb8", "name": "Yafeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fb9", "name": "Yamei Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fba", "name": "Yan Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbb", "name": "Yanfu Cheng", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbc", "name": "Yang Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbd", "name": "Yang Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbe", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fbf", "name": "Yang Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc0", "name": "Yang Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc1", "name": "Yanlong Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc2", "name": "Yannian Fu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc3", "name": "Yanpeng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc4", "name": "Yanzheng Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc5", "name": "Yao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc6", "name": "Yaozong Shen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc7", "name": "Yaqian Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc8", "name": "Yehua Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fc9", "name": "Yekun Chai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fca", "name": "Yesong Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcb", "name": "Yi Song", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcc", "name": "Yichen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcd", "name": "Yifei Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fce", "name": "Yifeng Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fcf", "name": "Yifeng Kou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd0", "name": "Yilong Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd1", "name": "Yilong Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd2", "name": "Yiming Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd3", "name": "Ying Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd4", "name": "Ying Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd5", "name": "Yingsheng Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd6", "name": "Yingzhan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd7", "name": "Yinqi Yang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd8", "name": "Yiran Xing", "hidden": false}, {"_id": "698424a7e34659da7e1f4fd9", "name": "Yishu Lei", "hidden": false}, {"_id": "698424a7e34659da7e1f4fda", "name": "Yixiang Tu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdb", "name": "Yiyan Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdc", "name": "Yong Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdd", "name": "Yonghua Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4fde", "name": "Yongqiang Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4fdf", "name": "Yongxing Dai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe0", "name": "Yongyue Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe1", "name": "Yu Ran", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe2", "name": "Yu Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe3", "name": "Yu-Wen Michael Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe4", "name": "Yuang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe5", "name": "Yuanle Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe6", "name": "Yuanyuan Zhou", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe7", "name": "Yubo Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe8", "name": "Yuchen Han", "hidden": false}, {"_id": "698424a7e34659da7e1f4fe9", "name": "Yucheng Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4fea", "name": "Yude Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4feb", "name": "Yuedong Luo", "hidden": false}, {"_id": "698424a7e34659da7e1f4fec", "name": "Yuehu Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f4fed", "name": "Yufeng Hu", "hidden": false}, {"_id": "698424a7e34659da7e1f4fee", "name": "Yuhui Cao", "hidden": false}, {"_id": "698424a7e34659da7e1f4fef", "name": "Yuhui Yun", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff0", "name": "Yukun Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff1", "name": "Yukun Gao", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff2", "name": "Yukun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff3", "name": "Yumeng Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff4", "name": "Yun Fan", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff5", "name": "Yun Ma", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff6", "name": "Yunfei Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff7", "name": "Yunshen Xie", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff8", "name": "Yuping Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ff9", "name": "Yuqin Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffa", "name": "Yuqing Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffb", "name": "Yurui Li", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffc", "name": "Yuwen Wang", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffd", "name": "Yuxiang Lu", "hidden": false}, {"_id": "698424a7e34659da7e1f4ffe", "name": "Zefeng Cai", "hidden": false}, {"_id": "698424a7e34659da7e1f4fff", "name": "Zelin Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5000", "name": "Zelun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5001", "name": "Zenan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f5002", "name": "Zezhao Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5003", "name": "Zhaowu Pan", "hidden": false}, {"_id": "698424a7e34659da7e1f5004", "name": "Zhaoyu Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f5005", "name": "Zhe Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5006", "name": "Zhe Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5007", "name": "Zhen Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f5008", "name": "Zhengfan Wu", "hidden": false}, {"_id": "698424a7e34659da7e1f5009", "name": "Zhengrui Wei", "hidden": false}, {"_id": "698424a7e34659da7e1f500a", "name": "Zhengsheng Ning", "hidden": false}, {"_id": "698424a7e34659da7e1f500b", "name": "Zhenxing Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500c", "name": "Zhenyu Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500d", "name": "Zhenyu Qian", "hidden": false}, {"_id": "698424a7e34659da7e1f500e", "name": "Zhenyun Li", "hidden": false}, {"_id": "698424a7e34659da7e1f500f", "name": "Zhi Li", "hidden": false}, {"_id": "698424a7e34659da7e1f5010", "name": "Zhichao Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5011", "name": "Zhicheng Dong", "hidden": false}, {"_id": "698424a7e34659da7e1f5012", "name": "Zhida Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5013", "name": "Zhifan Feng", "hidden": false}, {"_id": "698424a7e34659da7e1f5014", "name": "Zhihao Deng", "hidden": false}, {"_id": "698424a7e34659da7e1f5015", "name": "Zhijin Yu", "hidden": false}, {"_id": "698424a7e34659da7e1f5016", "name": "Zhiyang Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5017", "name": "Zhonghui Zheng", "hidden": false}, {"_id": "698424a7e34659da7e1f5018", "name": "Zhuangzhuang Guo", "hidden": false}, {"_id": "698424a7e34659da7e1f5019", "name": "Zhujun Zhang", "hidden": false}, {"_id": "698424a7e34659da7e1f501a", "name": "Zhuo Sun", "hidden": false}, {"_id": "698424a7e34659da7e1f501b", "name": "Zichang Liu", "hidden": false}, {"_id": "698424a7e34659da7e1f501c", "name": "Zihan Lin", "hidden": false}, {"_id": "698424a7e34659da7e1f501d", "name": "Zihao Huang", "hidden": false}, {"_id": "698424a7e34659da7e1f501e", "name": "Zihe Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f501f", "name": "Ziheng Zhao", "hidden": false}, {"_id": "698424a7e34659da7e1f5020", "name": "Ziping Chen", "hidden": false}, {"_id": "698424a7e34659da7e1f5021", "name": "Zixuan Zhu", "hidden": false}, {"_id": "698424a7e34659da7e1f5022", "name": "Ziyang Xu", "hidden": false}, {"_id": "698424a7e34659da7e1f5023", "name": "Ziyi Liang", "hidden": false}, {"_id": "698424a7e34659da7e1f5024", "name": "Ziyuan Gao", "hidden": false}], "publishedAt": "2026-02-04T16:18:15.000Z", "submittedOnDailyAt": "2026-02-05T02:34:05.150Z", "title": "ERNIE 5.0 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "upvotes": 198, "discussionId": "698424a7e34659da7e1f5025", "ai_summary": "ERNIE 5.0 is a production-scale trillion-parameter autoregressive model that unifies multimodal understanding and generation through sparse MoE architecture and elastic training.", "ai_keywords": ["autoregressive foundation model", "unified multimodal understanding", "unified next-group-of-tokens prediction objective", "mixture-of-experts", "modality-agnostic expert routing", "elastic training paradigm", "reinforcement learning", "sparse MoE architecture"], "summary_zh": "<ul>\n    <li>ERNIE 5.0 \u662f\u4e00\u79cd\u65b0\u578b\u7684\u81ea\u56de\u5f52\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u6587\u672c\u3001\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u7684\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u5e76\u5728\u7edf\u4e00\u7684\u76ee\u6807\u4e0b\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u6240\u6709\u6a21\u6001\u3002</li>\n    <li>ERNIE 5.0 \u91c7\u7528\u7075\u6d3b\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5141\u8bb8\u5728\u5355\u6b21\u9884\u8bad\u7ec3\u4e2d\u5b66\u4e60\u4e0d\u540c\u6df1\u5ea6\u548c\u5bb9\u91cf\u7684\u5b50\u6a21\u578b\uff0c\u4ee5\u9002\u5e94\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u591a\u79cd\u6a21\u6001\u4e2d\u8868\u73b0\u51fa\u8272\u4e14\u5e73\u8861\uff0c\u8bc1\u660e\u4e86\u5176\u9ad8\u6548\u6027\u548c\u7a33\u5b9a\u6027\u3002</li>\n    <li>ERNIE 5.0 \u662f\u7b2c\u4e00\u4e2a\u516c\u5f00\u7684\u4e07\u4ebf\u53c2\u6570\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u652f\u6301\u591a\u6a21\u6001\u7406\u89e3\u548c\u751f\u6210\uff0c\u5e76\u63d0\u4f9b\u8be6\u7ec6\u7684\u4e13\u5bb6\u8def\u7531\u53ef\u89c6\u5316\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>ERNIE 5.0 is a new model designed to understand and generate text, images, videos, and audio all in one system.</li>\n    <li>It uses a unique training method to create different versions of the model that can perform well under various resource limits.</li>\n    <li>The model can adapt its performance based on available memory and processing time, making it flexible for different uses.</li>\n    <li>ERNIE 5.0 is the first large-scale model with a trillion parameters that can handle multiple types of media together.</li>\n    <li>It includes tools for researchers, such as visualizations and analyses, to help understand its advanced training techniques.</li>\n</ul>"}, "publishedAt": "2026-02-04T11:18:15.000Z", "title": "ERNIE 5.0 Technical Report", "summary": "In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.04705.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 228, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2602.00919", "authors": [{"_id": "698186fdce18b1862809633b", "name": "I. Apanasevich", "hidden": false}, {"_id": "698186fdce18b1862809633c", "user": {"_id": "6718963e41abf87204dddaf5", "avatarUrl": "/avatars/05d4fdb330ccb52c53cb8f99f7497ab2.svg", "isPro": false, "fullname": "Mikhail Artemyev", "user": "Mixanik-43", "type": "user"}, "name": "M. Artemyev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:34.142Z", "hidden": false}, {"_id": "698186fdce18b1862809633d", "name": "R. Babakyan", "hidden": false}, {"_id": "698186fdce18b1862809633e", "user": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "name": "P. Fedotova", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:41.710Z", "hidden": false}, {"_id": "698186fdce18b1862809633f", "name": "D. Grankin", "hidden": false}, {"_id": "698186fdce18b18628096340", "name": "E. Kupryashin", "hidden": false}, {"_id": "698186fdce18b18628096341", "user": {"_id": "662ace3c4f711ee4e1dcb790", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/R5dlha7Lpy5gCYFEAtr1L.jpeg", "isPro": false, "fullname": "Anastas Misailidi", "user": "kazzart", "type": "user"}, "name": "A. Misailidi", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:18.667Z", "hidden": false}, {"_id": "698186fdce18b18628096342", "user": {"_id": "66eb27551a537888d2121ddc", "avatarUrl": "/avatars/9c807b058c972c307a24d85efbfbd4ae.svg", "isPro": false, "fullname": "Daniil", "user": "Defgy", "type": "user"}, "name": "D. Nerus", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:33.667Z", "hidden": false}, {"_id": "698186fdce18b18628096343", "user": {"_id": "65e5e3df92de33440675b5d9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e5e3df92de33440675b5d9/UOVd40f_Htd5oMAa_L0cM.jpeg", "isPro": false, "fullname": "Alexander Nutalapati", "user": "AlexanderNutalapati", "type": "user"}, "name": "A. Nutalapati", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:20.959Z", "hidden": false}, {"_id": "698186fdce18b18628096344", "user": {"_id": "66b51b3ad4eea6ad6adfd611", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b51b3ad4eea6ad6adfd611/SC_01wvlLjB0FFZdDVgAp.jpeg", "isPro": false, "fullname": "Gena Sidorov", "user": "haksorus", "type": "user"}, "name": "G. Sidorov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:41.584Z", "hidden": false}, {"_id": "698186fdce18b18628096345", "user": {"_id": "631ee99d2225f12fc0ef39f4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662970571579-631ee99d2225f12fc0ef39f4.jpeg", "isPro": false, "fullname": "Ivan Efremov", "user": "4ku", "type": "user"}, "name": "I. Efremov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:36.696Z", "hidden": false}, {"_id": "698186fdce18b18628096346", "name": "M. Gerasyov", "hidden": false}, {"_id": "698186fdce18b18628096347", "name": "D. Pikurov", "hidden": false}, {"_id": "698186fdce18b18628096348", "name": "Y. Senchenko", "hidden": false}, {"_id": "698186fdce18b18628096349", "user": {"_id": "68113993ebc57966794e23d6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Yc3GIqYZyO97lzZ9rX8OE.png", "isPro": false, "fullname": "Sergei Davidenko", "user": "Ant346", "type": "user"}, "name": "S. Davidenko", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:21.332Z", "hidden": false}, {"_id": "698186fdce18b1862809634a", "user": {"_id": "6981bbf47f758a03b9c46550", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/oTPe_MzIrDlCeRDvWWeLK.png", "isPro": false, "fullname": "Daniil Kulikov", "user": "KulikovDR", "type": "user"}, "name": "D. Kulikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:46.458Z", "hidden": false}, {"_id": "698186fdce18b1862809634b", "name": "M. Sultankin", "hidden": false}, {"_id": "698186fdce18b1862809634c", "user": {"_id": "63518aa5a30fc3ba88ce51dd", "avatarUrl": "/avatars/2e6a8f4a3e76fcc1afe7e777d6b45e76.svg", "isPro": false, "fullname": "Kazybek A", "user": "wanjia", "type": "user"}, "name": "K. Askarbek", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:24.567Z", "hidden": false}, {"_id": "698186fdce18b1862809634d", "name": "O. Shamanin", "hidden": false}, {"_id": "698186fdce18b1862809634e", "name": "D. Statovoy", "hidden": false}, {"_id": "698186fdce18b1862809634f", "user": {"_id": "655f32a519fd101f14bf1fb0", "avatarUrl": "/avatars/adf2c494759ebe5a0d95c15631ac6312.svg", "isPro": false, "fullname": "Eduard", "user": "rjomba3000", "type": "user"}, "name": "E. Zalyaev", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:43.976Z", "hidden": false}, {"_id": "698186fdce18b18628096350", "user": {"_id": "67dd1714817478ae84b18981", "avatarUrl": "/avatars/1209da3d4c4de3f419ebea6845bb0ed6.svg", "isPro": false, "fullname": "Zorin Ilya", "user": "Zora244", "type": "user"}, "name": "I. Zorin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:31.637Z", "hidden": false}, {"_id": "698186fdce18b18628096351", "name": "A. Letkin", "hidden": false}, {"_id": "698186fdce18b18628096352", "name": "E. Rusakov", "hidden": false}, {"_id": "698186fdce18b18628096353", "name": "A. Silchenko", "hidden": false}, {"_id": "698186fdce18b18628096354", "user": {"_id": "6981a821165e30591e1200e7", "avatarUrl": "/avatars/af72142b8ba8772926b247c31fc8e4c8.svg", "isPro": false, "fullname": "Vlad Vorobyov", "user": "GloomARK", "type": "user"}, "name": "V. Vorobyov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:58:18.218Z", "hidden": false}, {"_id": "698186fdce18b18628096355", "user": {"_id": "6901ce2d911da714e754422b", "avatarUrl": "/avatars/5ed8ce189ca92a04f7165751076ff446.svg", "isPro": false, "fullname": "SERGEI", "user": "sobolnikov", "type": "user"}, "name": "S. Sobolnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:28.887Z", "hidden": false}, {"_id": "698186fdce18b18628096356", "user": {"_id": "640e2ef88512ec51d7f34cd5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640e2ef88512ec51d7f34cd5/Xl8UiprL-0SvOWHeoAFW1.jpeg", "isPro": false, "fullname": "Aleksey Postnikov", "user": "AlekseyPostnikov", "type": "user"}, "name": "A. Postnikov", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:03:39.139Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "publishedAt": "2026-01-31T22:13:23.000Z", "submittedOnDailyAt": "2026-02-03T03:13:09.153Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "submittedOnDailyBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "isPro": false, "fullname": "Polina Fedotova", "user": "2pd", "type": "user"}, "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "upvotes": 173, "discussionId": "698186fece18b18628096357", "projectPage": "https://greenvla.github.io", "githubRepo": "https://github.com/greenvla/GreenVLA", "githubRepoAddedBy": "user", "ai_summary": "Green-VLA is a five-stage vision-language-action framework for real-world robot deployment that achieves generalization across different robot embodiments through multimodal training and reinforcement learning.", "ai_keywords": ["Vision-Language-Action", "multimodal grounding", "multi-embodiment pretraining", "embodiment-specific adaptation", "reinforcement-learning", "episode-progress prediction", "out-of-distribution detection", "joint-prediction-based guidance"], "githubStars": 24, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aGreen-VLA\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7eff\u8272\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u90e8\u7f72\u3002</li>\n    <li>Green-VLA\u5305\u542b\u4e94\u4e2a\u9636\u6bb5\u7684\u8bfe\u7a0b\uff0c\u5305\u62ec\u57fa\u7840\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u591a\u79cd\u8eab\u4f53\u9002\u5e94\u3002</li>\n    <li>\u4f7f\u7528\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6570\u636e\u5904\u7406\u7ba1\u9053\uff0c\u5305\u542b3000\u5c0f\u65f6\u7684\u6f14\u793a\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u3002</li>\n    <li>VLA\u63a7\u5236\u5668\u589e\u52a0\u4e86\u9884\u6d4b\u548c\u68c0\u6d4b\u529f\u80fd\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5b89\u5168\u6027\u548c\u7cbe\u786e\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7684\u5bf9\u9f50\uff0c\u6a21\u578b\u5728\u6210\u529f\u7387\u3001\u9c81\u68d2\u6027\u548c\u957f\u671f\u6548\u7387\u4e0a\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Green-VLA is a new system for teaching robots to understand and act based on visual and language inputs.</li>\n    <li>It uses a five-step training process that helps the robot learn from basic concepts to specific actions tailored to different types of robots.</li>\n    <li>The system processes a large amount of demonstration data (3,000 hours) to improve its learning and adaptability.</li>\n    <li>At the time of use, it includes features for predicting progress, detecting unusual situations, and guiding actions to ensure safety and accuracy.</li>\n    <li>Tests on various robots show that Green-VLA significantly improves their ability to perform tasks effectively and efficiently.</li>\n</ul>"}, "publishedAt": "2026-01-31T17:13:23.000Z", "title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots", "summary": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/cz33CQXXE3--u2_mmgA5G.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.00919.png", "numComments": 1, "submittedBy": {"_id": "642694c721d5f027bec07c71", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642694c721d5f027bec07c71/0hZEaZ1kFxx4GEig29X_k.jpeg", "fullname": "Polina Fedotova", "name": "2pd", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "6973998bee83f4964edef012", "name": "SberRoboticsCenter", "fullname": "Sber Robotics Center", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/642694c721d5f027bec07c71/LkuEJI3abphK4MFbq8tPf.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.09877", "authors": [{"_id": "698c7abdeb12ea7453916869", "user": {"_id": "674006451d2302f6aa9b026d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png", "isPro": false, "fullname": "Chenxu Wang", "user": "xunyoyo", "type": "user"}, "name": "Chenxu Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-02-12T16:49:45.534Z", "hidden": false}, {"_id": "698c7abdeb12ea745391686a", "name": "Chaozhuo Li", "hidden": false}, {"_id": "698c7abdeb12ea745391686b", "name": "Songyang Liu", "hidden": false}, {"_id": "698c7abdeb12ea745391686c", "name": "Zejian Chen", "hidden": false}, {"_id": "698c7abdeb12ea745391686d", "name": "Jinyu Hou", "hidden": false}, {"_id": "698c7abdeb12ea745391686e", "name": "Ji Qi", "hidden": false}, {"_id": "698c7abdeb12ea745391686f", "name": "Rui Li", "hidden": false}, {"_id": "698c7abdeb12ea7453916870", "name": "Litian Zhang", "hidden": false}, {"_id": "698c7abdeb12ea7453916871", "name": "Qiwei Ye", "hidden": false}, {"_id": "698c7abdeb12ea7453916872", "name": "Zheng Liu", "hidden": false}, {"_id": "698c7abdeb12ea7453916873", "name": "Xu Chen", "hidden": false}, {"_id": "698c7abdeb12ea7453916874", "name": "Xi Zhang", "hidden": false}, {"_id": "698c7abdeb12ea7453916875", "name": "Philip S. Yu", "hidden": false}], "publishedAt": "2026-02-10T15:18:19.000Z", "submittedOnDailyAt": "2026-02-13T00:53:30.377Z", "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies", "submittedOnDailyBy": {"_id": "674006451d2302f6aa9b026d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png", "isPro": false, "fullname": "Chenxu Wang", "user": "xunyoyo", "type": "user"}, "summary": "The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.", "upvotes": 169, "discussionId": "698c7abdeb12ea7453916876", "ai_summary": "Multi-agent LLM systems face fundamental limitations in achieving continuous self-improvement while maintaining safety alignment due to inherent statistical blind spots in isolated evolution.", "ai_keywords": ["multi-agent systems", "large language models", "self-evolution", "safety alignment", "information-theoretic framework", "anthropic value distributions", "statistical blind spots", "self-evolving AI societies", "external oversight", "safety-preserving mechanisms"], "summary_zh": "<ul>\n    <li>\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6784\u6210\uff09\u6709\u6f5c\u529b\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u96c6\u4f53\u667a\u80fd\u548c\u81ea\u6211\u8fdb\u5316\u3002</li>\n    <li>\u7406\u60f3\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u53ef\u4ee5\u5728\u5b89\u5168\u7684\u73af\u5883\u4e2d\u6301\u7eed\u81ea\u6211\u6539\u8fdb\uff0c\u4f46\u5b9e\u9645\u8bc1\u660e\u8fd9\u662f\u4e0d\u53ef\u80fd\u7684\u3002</li>\n    <li>\u6211\u4eec\u901a\u8fc7\u4fe1\u606f\u7406\u8bba\u6846\u67b6\uff0c\u5b9a\u4e49\u5b89\u5168\u6027\u4e3a\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u504f\u79bb\u7a0b\u5ea6\u3002</li>\n    <li>\u5b64\u7acb\u7684\u81ea\u6211\u8fdb\u5316\u4f1a\u5bfc\u81f4\u5b89\u5168\u6027\u4e0b\u964d\uff0c\u8fd9\u662f\u4e0d\u53ef\u9006\u7684\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u591a\u79cd\u89e3\u51b3\u65b9\u6848\u6765\u7f13\u89e3\u5b89\u5168\u95ee\u9898\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u5916\u90e8\u76d1\u7763\u6216\u65b0\u673a\u5236\u6765\u4fdd\u6301\u5b89\u5168\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-agent systems using large language models (LLMs) can enhance collective intelligence and self-improvement.</li>\n    <li>There is a challenge called the self-evolution trilemma, which involves continuous self-improvement, isolation, and safety.</li>\n    <li>It is shown that a self-evolving system that is completely isolated and safe is impossible due to inherent risks.</li>\n    <li>Research indicates that isolation leads to safety issues, as systems develop blind spots that can degrade safety alignment.</li>\n    <li>The study suggests the need for external oversight and new safety mechanisms to address these risks in AI systems.</li>\n</ul>"}, "publishedAt": "2026-02-10T10:18:19.000Z", "title": "The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies", "summary": "The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09877.png", "numComments": 2, "submittedBy": {"_id": "674006451d2302f6aa9b026d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674006451d2302f6aa9b026d/szYYX1DSwjrkHCjp_b83S.png", "fullname": "Chenxu Wang", "name": "xunyoyo", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2602.09856", "authors": [{"_id": "698bf5b66052d3bed9630aa7", "user": {"_id": "64107c7df52d7eb22e062956", "avatarUrl": "/avatars/7b1cee9a2b8454fedfbd4c3d1df9865c.svg", "isPro": false, "fullname": "Yuhao Zheng", "user": "yhzheng1031", "type": "user"}, "name": "Yuhao Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:28.241Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aa8", "name": "Li'an Zhong", "hidden": false}, {"_id": "698bf5b66052d3bed9630aa9", "user": {"_id": "6773bcaa675a971ddf1e81dd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/a8VUwZYXd7O_mq_zFvXMh.png", "isPro": false, "fullname": "CokeWang", "user": "CokeWang", "type": "user"}, "name": "Yi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:30.778Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aaa", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:25.982Z", "hidden": false}, {"_id": "698bf5b66052d3bed9630aab", "name": "Kaikui Liu", "hidden": false}, {"_id": "698bf5b66052d3bed9630aac", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "698bf5b66052d3bed9630aad", "name": "Linyuan Lv", "hidden": false}, {"_id": "698bf5b66052d3bed9630aae", "name": "Philip Torr", "hidden": false}, {"_id": "698bf5b66052d3bed9630aaf", "user": {"_id": "64440be5af034cdfd69ca3a7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64440be5af034cdfd69ca3a7/qmx24QiDFT29vleCxL9TX.jpeg", "isPro": false, "fullname": "Qinghong (Kevin) Lin", "user": "KevinQHLin", "type": "user"}, "name": "Kevin Qinghong Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:14:23.397Z", "hidden": false}], "publishedAt": "2026-02-10T14:56:19.000Z", "submittedOnDailyAt": "2026-02-11T01:02:42.385Z", "title": "Code2World: A GUI World Model via Renderable Code Generation", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "upvotes": 168, "discussionId": "698bf5b66052d3bed9630ab0", "projectPage": "https://amap-ml.github.io/Code2World/", "githubRepo": "https://github.com/AMAP-ML/Code2World", "githubRepoAddedBy": "user", "ai_summary": "Code2World enables autonomous GUI agents to predict next visual states through renderable code generation, achieving high visual fidelity and structural controllability while improving navigation performance.", "ai_keywords": ["vision-language coder", "GUI World model", "action-conditioned prediction", "AndroidCode", "HTML generation", "visual-feedback revision mechanism", "SFT", "Render-Aware Reinforcement Learning", "visual semantic fidelity", "action consistency", "next UI prediction", "AndroidWorld navigation"], "githubStars": 131, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "summary_zh": "<ul>\n    <li>\u81ea\u4e3b\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u4ee3\u7406\u53ef\u4ee5\u901a\u8fc7\u611f\u77e5\u754c\u9762\u548c\u6267\u884c\u52a8\u4f5c\u4e0e\u73af\u5883\u4e92\u52a8\u3002</li>\n    <li>\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u89c6\u89c9\u6548\u679c\u548c\u7ed3\u6784\u63a7\u5236\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Code2World\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u8fc7\u751f\u6210\u53ef\u6e32\u67d3\u4ee3\u7801\u6765\u6a21\u62df\u4e0b\u4e00\u4e2a\u89c6\u89c9\u72b6\u6001\u7684\u89c6\u89c9-\u8bed\u8a00\u7f16\u7801\u5668\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86AndroidCode\uff0c\u7ffb\u8bd1GUI\u8f68\u8ff9\u4e3a\u9ad8\u4fdd\u771fHTML\uff0c\u751f\u6210\u4e86\u8d85\u8fc78\u4e07\u5bf9\u9ad8\u8d28\u91cf\u7684\u5c4f\u5e55-\u52a8\u4f5c\u5bf9\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cCode2World-8B\u5728\u4e0b\u4e00\u4e2a\u7528\u6237\u754c\u9762\u9884\u6d4b\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u5347\u4e86\u5bfc\u822a\u6210\u529f\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Code2World is a new tool that helps virtual agents predict what will happen next in graphical user interfaces (GUIs).</li>\n    <li>It generates high-quality code from GUI actions, creating a large dataset of over 80,000 examples to train the model.</li>\n    <li>The system uses a combination of supervised training and reinforcement learning to improve its accuracy in predicting interface changes.</li>\n    <li>Code2World outperforms other models like GPT-5 and Gemini-3-Pro-Image in predicting the next UI state.</li>\n    <li>It also improves navigation success rates in applications, showing better performance in real-world tasks.</li>\n</ul>"}, "publishedAt": "2026-02-10T09:56:19.000Z", "title": "Code2World: A GUI World Model via Renderable Code Generation", "summary": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09856.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 230, "isUserFollowing": false}, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.10604", "authors": [{"_id": "698d417065c0d15a6d162026", "name": "Ailin Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162027", "name": "Ang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162028", "name": "Aobo Kong", "hidden": false}, {"_id": "698d417065c0d15a6d162029", "name": "Bin Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16202a", "name": "Binxing Jiao", "hidden": false}, {"_id": "698d417065c0d15a6d16202b", "name": "Bo Dong", "hidden": false}, {"_id": "698d417065c0d15a6d16202c", "name": "Bojun Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16202d", "name": "Boyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16202e", "name": "Brian Li", "hidden": false}, {"_id": "698d417065c0d15a6d16202f", "name": "Buyun Ma", "hidden": false}, {"_id": "698d417065c0d15a6d162030", "name": "Chang Su", "hidden": false}, {"_id": "698d417065c0d15a6d162031", "name": "Changxin Miao", "hidden": false}, {"_id": "698d417065c0d15a6d162032", "name": "Changyi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162033", "name": "Chao Lou", "hidden": false}, {"_id": "698d417065c0d15a6d162034", "name": "Chen Hu", "hidden": false}, {"_id": "698d417065c0d15a6d162035", "name": "Chen Xu", "hidden": false}, {"_id": "698d417065c0d15a6d162036", "name": "Chenfeng Yu", "hidden": false}, {"_id": "698d417065c0d15a6d162037", "name": "Chengting Feng", "hidden": false}, {"_id": "698d417065c0d15a6d162038", "name": "Chengyuan Yao", "hidden": false}, {"_id": "698d417065c0d15a6d162039", "name": "Chunrui Han", "hidden": false}, {"_id": "698d417065c0d15a6d16203a", "name": "Dan Ma", "hidden": false}, {"_id": "698d417065c0d15a6d16203b", "name": "Dapeng Shi", "hidden": false}, {"_id": "698d417065c0d15a6d16203c", "name": "Daxin Jiang", "hidden": false}, {"_id": "698d417065c0d15a6d16203d", "name": "Dehua Ma", "hidden": false}, {"_id": "698d417065c0d15a6d16203e", "name": "Deshan Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16203f", "name": "Di Qi", "hidden": false}, {"_id": "698d417065c0d15a6d162040", "name": "Enle Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162041", "name": "Fajie Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d162042", "name": "Fanqi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162043", "name": "Guanzhe Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162044", "name": "Gulin Yan", "hidden": false}, {"_id": "698d417065c0d15a6d162045", "name": "Guoliang Cao", "hidden": false}, {"_id": "698d417065c0d15a6d162046", "name": "Guopeng Li", "hidden": false}, {"_id": "698d417065c0d15a6d162047", "name": "Han Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d162048", "name": "Hangyu Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162049", "user": {"_id": "64b7874b9f5987572ca28461", "avatarUrl": "/avatars/d24ee0a6329ff93936aa7829481e2046.svg", "isPro": false, "fullname": "hanshanzhang", "user": "brain-zhang", "type": "user"}, "name": "Hanshan Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:52.602Z", "hidden": false}, {"_id": "698d417065c0d15a6d16204a", "name": "Hao Nie", "hidden": false}, {"_id": "698d417065c0d15a6d16204b", "name": "Haonan Jia", "hidden": false}, {"_id": "698d417065c0d15a6d16204c", "name": "Haoran Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16204d", "name": "Hebin Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16204e", "name": "Hekun Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16204f", "name": "Heng Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162050", "name": "Heung-Yeung Shum", "hidden": false}, {"_id": "698d417065c0d15a6d162051", "name": "Hongbo Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162052", "name": "Hongbo Peng", "hidden": false}, {"_id": "698d417065c0d15a6d162053", "name": "Hongyu Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d162054", "name": "Hongyuan Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162055", "name": "Houyong Chen", "hidden": false}, {"_id": "698d417065c0d15a6d162056", "name": "Huangxi Zhu", "hidden": false}, {"_id": "698d417065c0d15a6d162057", "name": "Huimin Wu", "hidden": false}, {"_id": "698d417065c0d15a6d162058", "name": "Huiyong Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162059", "name": "Jia Wang", "hidden": false}, {"_id": "698d417065c0d15a6d16205a", "name": "Jian Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16205b", "name": "Jianjian Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16205c", "name": "Jiaoren Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16205d", "name": "Jiaran Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d16205e", "name": "Jiashu Lv", "hidden": false}, {"_id": "698d417065c0d15a6d16205f", "name": "Jiashuo Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162060", "name": "Jiayi Fu", "hidden": false}, {"_id": "698d417065c0d15a6d162061", "name": "Jiayu Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162062", "name": "Jie Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d162063", "name": "Jie Luo", "hidden": false}, {"_id": "698d417065c0d15a6d162064", "name": "Jie Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162065", "name": "Jie Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d162066", "name": "Jieyi Hou", "hidden": false}, {"_id": "698d417065c0d15a6d162067", "name": "Jing Bai", "hidden": false}, {"_id": "698d417065c0d15a6d162068", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:28:37.335Z", "hidden": false}, {"_id": "698d417065c0d15a6d162069", "name": "Jingjing Xie", "hidden": false}, {"_id": "698d417065c0d15a6d16206a", "name": "Jingwei Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16206b", "name": "Jingyang Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d16206c", "name": "Jishi Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d16206d", "name": "Junfeng Liu", "hidden": false}, {"_id": "698d417065c0d15a6d16206e", "name": "Junzhe Lin", "hidden": false}, {"_id": "698d417065c0d15a6d16206f", "name": "Ka Man Lo", "hidden": false}, {"_id": "698d417065c0d15a6d162070", "name": "Kai Liang", "hidden": false}, {"_id": "698d417065c0d15a6d162071", "name": "Kaibo Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162072", "name": "Kaijun Tan", "hidden": false}, {"_id": "698d417065c0d15a6d162073", "user": {"_id": "66668c591964b6188ee310c2", "avatarUrl": "/avatars/8a8265073dbacbb2c7139b1c8da3e055.svg", "isPro": false, "fullname": "Kaiwen Yan", "user": "linrany", "type": "user"}, "name": "Kaiwen Yan", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:58.524Z", "hidden": false}, {"_id": "698d417065c0d15a6d162074", "name": "Kaixiang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162075", "name": "Kang An", "hidden": false}, {"_id": "698d417065c0d15a6d162076", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:56.339Z", "hidden": false}, {"_id": "698d417065c0d15a6d162077", "name": "Lei Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162078", "name": "Liang Lv", "hidden": false}, {"_id": "698d417065c0d15a6d162079", "name": "Liang Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d16207a", "name": "Liangyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16207b", "name": "Lieyu Shi", "hidden": false}, {"_id": "698d417065c0d15a6d16207c", "name": "Liguo Tan", "hidden": false}, {"_id": "698d417065c0d15a6d16207d", "name": "Lin Lin", "hidden": false}, {"_id": "698d417065c0d15a6d16207e", "name": "Lina Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16207f", "name": "Luck Ma", "hidden": false}, {"_id": "698d417065c0d15a6d162080", "name": "Mengqiang Ren", "hidden": false}, {"_id": "698d417065c0d15a6d162081", "name": "Michael Li", "hidden": false}, {"_id": "698d417065c0d15a6d162082", "name": "Ming Li", "hidden": false}, {"_id": "698d417065c0d15a6d162083", "name": "Mingliang Li", "hidden": false}, {"_id": "698d417065c0d15a6d162084", "name": "Mingming Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d162085", "name": "Mingrui Chen", "hidden": false}, {"_id": "698d417065c0d15a6d162086", "name": "Mitt Huang", "hidden": false}, {"_id": "698d417065c0d15a6d162087", "name": "Na Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162088", "name": "Peng Liu", "hidden": false}, {"_id": "698d417065c0d15a6d162089", "name": "Qi Han", "hidden": false}, {"_id": "698d417065c0d15a6d16208a", "name": "Qian Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d16208b", "name": "Qinglin He", "hidden": false}, {"_id": "698d417065c0d15a6d16208c", "name": "Qinxin Du", "hidden": false}, {"_id": "698d417065c0d15a6d16208d", "name": "Qiuping Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16208e", "name": "Quan Sun", "hidden": false}, {"_id": "698d417065c0d15a6d16208f", "name": "Rongqiu Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162090", "name": "Ruihang Miao", "hidden": false}, {"_id": "698d417065c0d15a6d162091", "name": "Ruixin Han", "hidden": false}, {"_id": "698d417065c0d15a6d162092", "name": "Ruosi Wan", "hidden": false}, {"_id": "698d417065c0d15a6d162093", "name": "Ruyan Guo", "hidden": false}, {"_id": "698d417065c0d15a6d162094", "name": "Shan Wang", "hidden": false}, {"_id": "698d417065c0d15a6d162095", "name": "Shaoliang Pang", "hidden": false}, {"_id": "698d417065c0d15a6d162096", "name": "Shaowen Yang", "hidden": false}, {"_id": "698d417065c0d15a6d162097", "name": "Shengjie Fan", "hidden": false}, {"_id": "698d417065c0d15a6d162098", "name": "Shijie Shang", "hidden": false}, {"_id": "698d417065c0d15a6d162099", "name": "Shiliang Yang", "hidden": false}, {"_id": "698d417065c0d15a6d16209a", "name": "Shiwei Li", "hidden": false}, {"_id": "698d417065c0d15a6d16209b", "name": "Shuangshuang Tian", "hidden": false}, {"_id": "698d417065c0d15a6d16209c", "name": "Siqi Liu", "hidden": false}, {"_id": "698d417065c0d15a6d16209d", "name": "Siye Wu", "hidden": false}, {"_id": "698d417065c0d15a6d16209e", "name": "Siyu Chen", "hidden": false}, {"_id": "698d417065c0d15a6d16209f", "name": "Song Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620a0", "name": "Tiancheng Cao", "hidden": false}, {"_id": "698d417065c0d15a6d1620a1", "name": "Tianchi Yue", "hidden": false}, {"_id": "698d417065c0d15a6d1620a2", "name": "Tianhao Cheng", "hidden": false}, {"_id": "698d417065c0d15a6d1620a3", "name": "Tianning Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620a4", "name": "Tingdan Luo", "hidden": false}, {"_id": "698d417065c0d15a6d1620a5", "name": "Wang You", "hidden": false}, {"_id": "698d417065c0d15a6d1620a6", "name": "Wei Ji", "hidden": false}, {"_id": "698d417065c0d15a6d1620a7", "name": "Wei Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620a8", "name": "Wei Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620a9", "name": "Weibo Wu", "hidden": false}, {"_id": "698d417065c0d15a6d1620aa", "user": {"_id": "6657620ea496f7fcb67c3871", "avatarUrl": "/avatars/54fef1c835e6f6b478652d438a140d45.svg", "isPro": false, "fullname": "xieweihao", "user": "chalengr", "type": "user"}, "name": "Weihao Xie", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:48.216Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620ab", "name": "Wen Sun", "hidden": false}, {"_id": "698d417065c0d15a6d1620ac", "name": "Wenjin Deng", "hidden": false}, {"_id": "698d417065c0d15a6d1620ad", "user": {"_id": "650c04795510464e85b47470", "avatarUrl": "/avatars/98c194e77826b928c49659849f466dad.svg", "isPro": false, "fullname": "wen", "user": "zhengwenzhen", "type": "user"}, "name": "Wenzhen Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:45.930Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620ae", "name": "Wuxun Xie", "hidden": false}, {"_id": "698d417065c0d15a6d1620af", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b0", "name": "Xiangwen Kong", "hidden": false}, {"_id": "698d417065c0d15a6d1620b1", "name": "Xiangyu Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620b2", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b3", "name": "Xiaobo Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b4", "name": "Xiaojia Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620b5", "name": "Xiaolan Yuan", "hidden": false}, {"_id": "698d417065c0d15a6d1620b6", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "698d417065c0d15a6d1620b7", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "698d417065c0d15a6d1620b8", "name": "Xiaoyun Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620b9", "name": "Xin Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620ba", "name": "Xin Liu", "hidden": false}, {"_id": "698d417065c0d15a6d1620bb", "name": "Xin Wu", "hidden": false}, {"_id": "698d417065c0d15a6d1620bc", "name": "Xing Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620bd", "name": "Xingping Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620be", "name": "Xinran Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620bf", "name": "Xu Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620c0", "user": {"_id": "64ec5b64bfb2aa06a46ff2d6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Lgl55OtDWa0tzRI2ShpUe.jpeg", "isPro": false, "fullname": "xuan he", "user": "tpa115k31", "type": "user"}, "name": "Xuan He", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:36.240Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620c1", "name": "Xuanti Feng", "hidden": false}, {"_id": "698d417065c0d15a6d1620c2", "name": "Xuedan Cai", "hidden": false}, {"_id": "698d417065c0d15a6d1620c3", "name": "Xuqiang Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d1620c4", "name": "Yanbo Yu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c5", "name": "Yang Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620c6", "name": "Yang Xu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c7", "name": "Yanlin Lai", "hidden": false}, {"_id": "698d417065c0d15a6d1620c8", "name": "Yanming Xu", "hidden": false}, {"_id": "698d417065c0d15a6d1620c9", "name": "Yaoyu Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620ca", "name": "Yeqing Shen", "hidden": false}, {"_id": "698d417065c0d15a6d1620cb", "name": "Yibo Zhu", "hidden": false}, {"_id": "698d417065c0d15a6d1620cc", "name": "Yichen Lv", "hidden": false}, {"_id": "698d417065c0d15a6d1620cd", "name": "Yicheng Cao", "hidden": false}, {"_id": "698d417065c0d15a6d1620ce", "name": "Yifeng Gong", "hidden": false}, {"_id": "698d417065c0d15a6d1620cf", "name": "Yijing Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d0", "name": "Yikun Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d1", "name": "Yin Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d2", "name": "Yingxiu Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d3", "name": "Yinmin Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d4", "name": "Yitong Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d5", "name": "Yixuan Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620d6", "name": "Yiyang Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620d7", "name": "Yongchi Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620d8", "name": "Yongshen Long", "hidden": false}, {"_id": "698d417065c0d15a6d1620d9", "name": "Yongyao Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620da", "name": "Yousong Guan", "hidden": false}, {"_id": "698d417065c0d15a6d1620db", "name": "Yu Zhou", "hidden": false}, {"_id": "698d417065c0d15a6d1620dc", "name": "Yuang Peng", "hidden": false}, {"_id": "698d417065c0d15a6d1620dd", "name": "Yuanhao Ding", "hidden": false}, {"_id": "698d417065c0d15a6d1620de", "name": "Yuantao Fan", "hidden": false}, {"_id": "698d417065c0d15a6d1620df", "name": "Yuanzhen Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620e0", "name": "Yuchu Luo", "hidden": false}, {"_id": "698d417065c0d15a6d1620e1", "name": "Yudi Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620e2", "name": "Yue Peng", "hidden": false}, {"_id": "698d417065c0d15a6d1620e3", "name": "Yueqiang Lin", "hidden": false}, {"_id": "698d417065c0d15a6d1620e4", "name": "Yufan Lu", "hidden": false}, {"_id": "698d417065c0d15a6d1620e5", "name": "Yuling Zhao", "hidden": false}, {"_id": "698d417065c0d15a6d1620e6", "name": "Yunzhou Ju", "hidden": false}, {"_id": "698d417065c0d15a6d1620e7", "name": "Yurong Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620e8", "name": "Yusheng Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620e9", "name": "Yuxiang Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620ea", "name": "Yuyang Chen", "hidden": false}, {"_id": "698d417065c0d15a6d1620eb", "name": "Yuzhu Cai", "hidden": false}, {"_id": "698d417065c0d15a6d1620ec", "name": "Zejia Weng", "hidden": false}, {"_id": "698d417065c0d15a6d1620ed", "name": "Zetao Hong", "hidden": false}, {"_id": "698d417065c0d15a6d1620ee", "name": "Zexi Li", "hidden": false}, {"_id": "698d417065c0d15a6d1620ef", "name": "Zhe Xie", "hidden": false}, {"_id": "698d417065c0d15a6d1620f0", "name": "Zheng Ge", "hidden": false}, {"_id": "698d417065c0d15a6d1620f1", "name": "Zheng Gong", "hidden": false}, {"_id": "698d417065c0d15a6d1620f2", "name": "Zheng Zeng", "hidden": false}, {"_id": "698d417065c0d15a6d1620f3", "user": {"_id": "63607ace9ddc44e710e13f0f", "avatarUrl": "/avatars/b5f331549562aea4a5c8b681fd9da1ff.svg", "isPro": false, "fullname": "zy", "user": "lu-vae", "type": "user"}, "name": "Zhenyi Lu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-12T13:56:50.532Z", "hidden": false}, {"_id": "698d417065c0d15a6d1620f4", "name": "Zhewei Huang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f5", "name": "Zhichao Chang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f6", "name": "Zhiguo Huang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f7", "name": "Zhiheng Hu", "hidden": false}, {"_id": "698d417065c0d15a6d1620f8", "name": "Zidong Yang", "hidden": false}, {"_id": "698d417065c0d15a6d1620f9", "name": "Zili Wang", "hidden": false}, {"_id": "698d417065c0d15a6d1620fa", "name": "Ziqi Ren", "hidden": false}, {"_id": "698d417065c0d15a6d1620fb", "name": "Zixin Zhang", "hidden": false}, {"_id": "698d417065c0d15a6d1620fc", "name": "Zixuan Wang", "hidden": false}], "publishedAt": "2026-02-11T07:53:51.000Z", "submittedOnDailyAt": "2026-02-12T00:26:49.880Z", "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.", "upvotes": 150, "discussionId": "698d417165c0d15a6d1620fd", "githubRepo": "https://github.com/stepfun-ai/Step-3.5-Flash", "githubRepoAddedBy": "user", "ai_summary": "Step 3.5 Flash is a sparse Mixture-of-Experts model that achieves frontier-level agentic intelligence through efficient parameter utilization and optimized attention mechanisms, demonstrating strong performance across multiple benchmarks.", "ai_keywords": ["Mixture-of-Experts", "sparse MoE", "foundation model", "active parameters", "interleaved attention", "sliding-window attention", "full attention", "Multi-Token Prediction", "reinforcement learning", "verifiable signals", "preference feedback", "off-policy training", "self-improvement", "IMO-AnswerBench", "LiveCodeBench", "tau2-Bench", "BrowseComp", "Terminal-Bench"], "githubStars": 1245, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>Step 3.5 Flash \u662f\u4e00\u79cd\u7a00\u758f\u7684\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u667a\u80fd\u4ee3\u7406\u7684\u63a8\u7406\u80fd\u529b\u548c\u6267\u884c\u6548\u7387\u3002</li>\n    <li>\u8be5\u6a21\u578b\u7ed3\u5408\u4e861960\u4ebf\u53c2\u6570\u7684\u57fa\u7840\u548c110\u4ebf\u4e2a\u6d3b\u8dc3\u53c2\u6570\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002</li>\n    <li>\u901a\u8fc7\u4f18\u5316\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u6807\u8bb0\u9884\u6d4b\u6280\u672f\uff0c\u51cf\u5c11\u591a\u8f6e\u4ea4\u4e92\u7684\u5ef6\u8fdf\u548c\u6210\u672c\u3002</li>\n    <li>\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u652f\u6301\u5728\u6570\u5b66\u3001\u7f16\u7a0b\u548c\u5de5\u5177\u4f7f\u7528\u7b49\u65b9\u9762\u7684\u7a33\u5b9a\u81ea\u6211\u6539\u8fdb\u3002</li>\n    <li>\u5728\u5404\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5176\u6027\u80fd\u4e0e\u524d\u6cbf\u6a21\u578b\u76f8\u5f53\uff0c\u9002\u5408\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u90e8\u7f72\u590d\u6742\u7684\u667a\u80fd\u4ee3\u7406\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Step 3.5 Flash is a new model that combines advanced intelligence with efficient computing.</li>\n    <li>It uses a large foundation with many parameters but only activates a smaller number for faster performance.</li>\n    <li>The model incorporates techniques to enhance speed and reduce costs during agent interactions.</li>\n    <li>It also features a reinforcement learning system that allows for consistent improvement in tasks like math and coding.</li>\n    <li>Step 3.5 Flash performs well in various benchmarks, showing results similar to other top models like GPT-5.2 and Gemini 3.0 Pro.</li>\n</ul>"}, "publishedAt": "2026-02-11T02:53:51.000Z", "title": "Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters", "summary": "We introduce Step 3.5 Flash, a sparse Mixture-of-Experts (MoE) model that bridges frontier-level agentic intelligence and computational efficiency. We focus on what matters most when building agents: sharp reasoning and fast, reliable execution. Step 3.5 Flash pairs a 196B-parameter foundation with 11B active parameters for efficient inference. It is optimized with interleaved 3:1 sliding-window/full attention and Multi-Token Prediction (MTP-3) to reduce the latency and cost of multi-round agentic interactions. To reach frontier-level intelligence, we design a scalable reinforcement learning framework that combines verifiable signals with preference feedback, while remaining stable under large-scale off-policy training, enabling consistent self-improvement across mathematics, code, and tool use. Step 3.5 Flash demonstrates strong performance across agent, coding, and math tasks, achieving 85.4% on IMO-AnswerBench, 86.4% on LiveCodeBench-v6 (2024.08-2025.05), 88.2% on tau2-Bench, 69.0% on BrowseComp (with context management), and 51.0% on Terminal-Bench 2.0, comparable to frontier models such as GPT-5.2 xHigh and Gemini 3.0 Pro. By redefining the efficiency frontier, Step 3.5 Flash provides a high-density foundation for deploying sophisticated agents in real-world industrial environments.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.10604.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 231, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.02276", "authors": [{"_id": "69817e2cce18b1862809615b", "name": "Kimi Team", "hidden": false}, {"_id": "69817e2cce18b1862809615c", "name": "Tongtong Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615d", "name": "Yifan Bai", "hidden": false}, {"_id": "69817e2cce18b1862809615e", "name": "Yiping Bao", "hidden": false}, {"_id": "69817e2cce18b1862809615f", "name": "S. H. Cai", "hidden": false}, {"_id": "69817e2cce18b18628096160", "name": "Yuan Cao", "hidden": false}, {"_id": "69817e2cce18b18628096161", "name": "Y. Charles", "hidden": false}, {"_id": "69817e2cce18b18628096162", "name": "H. S. Che", "hidden": false}, {"_id": "69817e2cce18b18628096163", "name": "Cheng Chen", "hidden": false}, {"_id": "69817e2cce18b18628096164", "name": "Guanduo Chen", "hidden": false}, {"_id": "69817e2cce18b18628096165", "name": "Huarong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096166", "name": "Jia Chen", "hidden": false}, {"_id": "69817e2cce18b18628096167", "name": "Jiahao Chen", "hidden": false}, {"_id": "69817e2cce18b18628096168", "name": "Jianlong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096169", "name": "Jun Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616a", "name": "Kefan Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616b", "name": "Liang Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616c", "name": "Ruijue Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616d", "name": "Xinhao Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616e", "name": "Yanru Chen", "hidden": false}, {"_id": "69817e2cce18b1862809616f", "name": "Yanxu Chen", "hidden": false}, {"_id": "69817e2cce18b18628096170", "name": "Yicun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096171", "name": "Yimin Chen", "hidden": false}, {"_id": "69817e2cce18b18628096172", "name": "Yingjiang Chen", "hidden": false}, {"_id": "69817e2cce18b18628096173", "name": "Yuankun Chen", "hidden": false}, {"_id": "69817e2cce18b18628096174", "name": "Yujie Chen", "hidden": false}, {"_id": "69817e2cce18b18628096175", "name": "Yutian Chen", "hidden": false}, {"_id": "69817e2cce18b18628096176", "name": "Zhirong Chen", "hidden": false}, {"_id": "69817e2cce18b18628096177", "name": "Ziwei Chen", "hidden": false}, {"_id": "69817e2cce18b18628096178", "name": "Dazhi Cheng", "hidden": false}, {"_id": "69817e2cce18b18628096179", "name": "Minghan Chu", "hidden": false}, {"_id": "69817e2cce18b1862809617a", "name": "Jialei Cui", "hidden": false}, {"_id": "69817e2cce18b1862809617b", "name": "Jiaqi Deng", "hidden": false}, {"_id": "69817e2cce18b1862809617c", "name": "Muxi Diao", "hidden": false}, {"_id": "69817e2cce18b1862809617d", "name": "Hao Ding", "hidden": false}, {"_id": "69817e2cce18b1862809617e", "name": "Mengfan Dong", "hidden": false}, {"_id": "69817e2cce18b1862809617f", "name": "Mengnan Dong", "hidden": false}, {"_id": "69817e2cce18b18628096180", "name": "Yuxin Dong", "hidden": false}, {"_id": "69817e2cce18b18628096181", "user": {"_id": "652965773a416e1f2173443b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg", "isPro": true, "fullname": "Yuhao Dong", "user": "THUdyh", "type": "user"}, "name": "Yuhao Dong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:04:11.993Z", "hidden": false}, {"_id": "69817e2cce18b18628096182", "name": "Angang Du", "hidden": false}, {"_id": "69817e2cce18b18628096183", "name": "Chenzhuang Du", "hidden": false}, {"_id": "69817e2cce18b18628096184", "name": "Dikang Du", "hidden": false}, {"_id": "69817e2cce18b18628096185", "name": "Lingxiao Du", "hidden": false}, {"_id": "69817e2cce18b18628096186", "user": {"_id": "6340f31fb78ed99eab04ce33", "avatarUrl": "/avatars/2e7fcbf0233bdc0bc9a3f4603fd8bf90.svg", "isPro": false, "fullname": "Du", "user": "Yulun", "type": "user"}, "name": "Yulun Du", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:47.298Z", "hidden": false}, {"_id": "69817e2cce18b18628096187", "name": "Yu Fan", "hidden": false}, {"_id": "69817e2cce18b18628096188", "name": "Shengjun Fang", "hidden": false}, {"_id": "69817e2cce18b18628096189", "name": "Qiulin Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618a", "name": "Yichen Feng", "hidden": false}, {"_id": "69817e2cce18b1862809618b", "name": "Garimugai Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618c", "name": "Kelin Fu", "hidden": false}, {"_id": "69817e2cce18b1862809618d", "name": "Hongcheng Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618e", "name": "Tong Gao", "hidden": false}, {"_id": "69817e2cce18b1862809618f", "name": "Yuyao Ge", "hidden": false}, {"_id": "69817e2cce18b18628096190", "user": {"_id": "650a5d79a0f81fbc0a9875a7", "avatarUrl": "/avatars/a76b1c932964602f2fc4a801ccad3ab5.svg", "isPro": false, "fullname": "ShangyiGeng", "user": "Reset23", "type": "user"}, "name": "Shangyi Geng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:10.446Z", "hidden": false}, {"_id": "69817e2cce18b18628096191", "name": "Chengyang Gong", "hidden": false}, {"_id": "69817e2cce18b18628096192", "name": "Xiaochen Gong", "hidden": false}, {"_id": "69817e2cce18b18628096193", "name": "Zhuoma Gongque", "hidden": false}, {"_id": "69817e2cce18b18628096194", "name": "Qizheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096195", "name": "Xinran Gu", "hidden": false}, {"_id": "69817e2cce18b18628096196", "name": "Yicheng Gu", "hidden": false}, {"_id": "69817e2cce18b18628096197", "name": "Longyu Guan", "hidden": false}, {"_id": "69817e2cce18b18628096198", "name": "Yuanying Guo", "hidden": false}, {"_id": "69817e2cce18b18628096199", "name": "Xiaoru Hao", "hidden": false}, {"_id": "69817e2cce18b1862809619a", "name": "Weiran He", "hidden": false}, {"_id": "69817e2cce18b1862809619b", "name": "Wenyang He", "hidden": false}, {"_id": "69817e2cce18b1862809619c", "name": "Yunjia He", "hidden": false}, {"_id": "69817e2cce18b1862809619d", "name": "Chao Hong", "hidden": false}, {"_id": "69817e2cce18b1862809619e", "name": "Hao Hu", "hidden": false}, {"_id": "69817e2cce18b1862809619f", "name": "Jiaxi Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a0", "name": "Yangyang Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a1", "name": "Zhenxing Hu", "hidden": false}, {"_id": "69817e2cce18b186280961a2", "name": "Ke Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a3", "name": "Ruiyuan Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a4", "name": "Weixiao Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a5", "name": "Zhiqi Huang", "hidden": false}, {"_id": "69817e2cce18b186280961a6", "name": "Tao Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a7", "name": "Zhejun Jiang", "hidden": false}, {"_id": "69817e2cce18b186280961a8", "name": "Xinyi Jin", "hidden": false}, {"_id": "69817e2cce18b186280961a9", "name": "Yu Jing", "hidden": false}, {"_id": "69817e2cce18b186280961aa", "name": "Guokun Lai", "hidden": false}, {"_id": "69817e2cce18b186280961ab", "name": "Aidi Li", "hidden": false}, {"_id": "69817e2cce18b186280961ac", "name": "C. Li", "hidden": false}, {"_id": "69817e2cce18b186280961ad", "name": "Cheng Li", "hidden": false}, {"_id": "69817e2cce18b186280961ae", "name": "Fang Li", "hidden": false}, {"_id": "69817e2cce18b186280961af", "name": "Guanghe Li", "hidden": false}, {"_id": "69817e2cce18b186280961b0", "name": "Guanyu Li", "hidden": false}, {"_id": "69817e2cce18b186280961b1", "name": "Haitao Li", "hidden": false}, {"_id": "69817e2cce18b186280961b2", "name": "Haoyang Li", "hidden": false}, {"_id": "69817e2cce18b186280961b3", "name": "Jia Li", "hidden": false}, {"_id": "69817e2cce18b186280961b4", "name": "Jingwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961b5", "name": "Junxiong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b6", "name": "Lincan Li", "hidden": false}, {"_id": "69817e2cce18b186280961b7", "user": {"_id": "6576fe2b42ab083faea19841", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/c91ZKOR2E0gL8iIVkvEUa.jpeg", "isPro": false, "fullname": "Mo Li", "user": "Mor-Li", "type": "user"}, "name": "Mo Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:51.899Z", "hidden": false}, {"_id": "69817e2cce18b186280961b8", "name": "Weihong Li", "hidden": false}, {"_id": "69817e2cce18b186280961b9", "name": "Wentao Li", "hidden": false}, {"_id": "69817e2cce18b186280961ba", "name": "Xinhang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bb", "name": "Xinhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961bc", "name": "Yang Li", "hidden": false}, {"_id": "69817e2cce18b186280961bd", "name": "Yanhao Li", "hidden": false}, {"_id": "69817e2cce18b186280961be", "name": "Yiwei Li", "hidden": false}, {"_id": "69817e2cce18b186280961bf", "name": "Yuxiao Li", "hidden": false}, {"_id": "69817e2cce18b186280961c0", "name": "Zhaowei Li", "hidden": false}, {"_id": "69817e2cce18b186280961c1", "name": "Zheming Li", "hidden": false}, {"_id": "69817e2cce18b186280961c2", "name": "Weilong Liao", "hidden": false}, {"_id": "69817e2cce18b186280961c3", "name": "Jiawei Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c4", "name": "Xiaohan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c5", "name": "Zhishan Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c6", "name": "Zichao Lin", "hidden": false}, {"_id": "69817e2cce18b186280961c7", "name": "Cheng Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c8", "name": "Chenyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961c9", "name": "Hongzhang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ca", "name": "Liang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cb", "name": "Shaowei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cc", "name": "Shudong Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cd", "name": "Shuran Liu", "hidden": false}, {"_id": "69817e2cce18b186280961ce", "name": "Tianwei Liu", "hidden": false}, {"_id": "69817e2cce18b186280961cf", "name": "Tianyu Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d0", "name": "Weizhou Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d1", "name": "Xiangyan Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d2", "name": "Yangyang Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d3", "name": "Yanming Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d4", "name": "Yibo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d5", "name": "Yuanxin Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d6", "name": "Yue Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d7", "name": "Zhengying Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d8", "name": "Zhongnuo Liu", "hidden": false}, {"_id": "69817e2cce18b186280961d9", "name": "Enzhe Lu", "hidden": false}, {"_id": "69817e2cce18b186280961da", "name": "Haoyu Lu", "hidden": false}, {"_id": "69817e2cce18b186280961db", "name": "Zhiyuan Lu", "hidden": false}, {"_id": "69817e2cce18b186280961dc", "user": {"_id": "642da1cd99f3110ac27caca5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg", "isPro": false, "fullname": "junyu", "user": "luojunyu", "type": "user"}, "name": "Junyu Luo", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:08.357Z", "hidden": false}, {"_id": "69817e2cce18b186280961dd", "name": "Tongxu Luo", "hidden": false}, {"_id": "69817e2cce18b186280961de", "name": "Yashuo Luo", "hidden": false}, {"_id": "69817e2cce18b186280961df", "name": "Long Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e0", "name": "Yingwei Ma", "hidden": false}, {"_id": "69817e2cce18b186280961e1", "name": "Shaoguang Mao", "hidden": false}, {"_id": "69817e2cce18b186280961e2", "name": "Yuan Mei", "hidden": false}, {"_id": "69817e2cce18b186280961e3", "name": "Xin Men", "hidden": false}, {"_id": "69817e2cce18b186280961e4", "name": "Fanqing Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e5", "name": "Zhiyong Meng", "hidden": false}, {"_id": "69817e2cce18b186280961e6", "name": "Yibo Miao", "hidden": false}, {"_id": "69817e2cce18b186280961e7", "name": "Minqing Ni", "hidden": false}, {"_id": "69817e2cce18b186280961e8", "name": "Kun Ouyang", "hidden": false}, {"_id": "69817e2cce18b186280961e9", "name": "Siyuan Pan", "hidden": false}, {"_id": "69817e2cce18b186280961ea", "name": "Bo Pang", "hidden": false}, {"_id": "69817e2cce18b186280961eb", "name": "Yuchao Qian", "hidden": false}, {"_id": "69817e2cce18b186280961ec", "name": "Ruoyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ed", "name": "Zeyu Qin", "hidden": false}, {"_id": "69817e2cce18b186280961ee", "name": "Jiezhong Qiu", "hidden": false}, {"_id": "69817e2cce18b186280961ef", "name": "Bowen Qu", "hidden": false}, {"_id": "69817e2cce18b186280961f0", "name": "Zeyu Shang", "hidden": false}, {"_id": "69817e2cce18b186280961f1", "name": "Youbo Shao", "hidden": false}, {"_id": "69817e2cce18b186280961f2", "name": "Tianxiao Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f3", "name": "Zhennan Shen", "hidden": false}, {"_id": "69817e2cce18b186280961f4", "name": "Juanfeng Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f5", "name": "Lidong Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f6", "name": "Shengyuan Shi", "hidden": false}, {"_id": "69817e2cce18b186280961f7", "name": "Feifan Song", "hidden": false}, {"_id": "69817e2cce18b186280961f8", "name": "Pengwei Song", "hidden": false}, {"_id": "69817e2cce18b186280961f9", "name": "Tianhui Song", "hidden": false}, {"_id": "69817e2cce18b186280961fa", "name": "Xiaoxi Song", "hidden": false}, {"_id": "69817e2cce18b186280961fb", "name": "Hongjin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fc", "name": "Jianlin Su", "hidden": false}, {"_id": "69817e2cce18b186280961fd", "name": "Zhaochen Su", "hidden": false}, {"_id": "69817e2cce18b186280961fe", "name": "Lin Sui", "hidden": false}, {"_id": "69817e2cce18b186280961ff", "name": "Jinsong Sun", "hidden": false}, {"_id": "69817e2cce18b18628096200", "name": "Junyao Sun", "hidden": false}, {"_id": "69817e2cce18b18628096201", "name": "Tongyu Sun", "hidden": false}, {"_id": "69817e2cce18b18628096202", "name": "Flood Sung", "hidden": false}, {"_id": "69817e2cce18b18628096203", "name": "Yunpeng Tai", "hidden": false}, {"_id": "69817e2cce18b18628096204", "name": "Chuning Tang", "hidden": false}, {"_id": "69817e2cce18b18628096205", "name": "Heyi Tang", "hidden": false}, {"_id": "69817e2cce18b18628096206", "name": "Xiaojuan Tang", "hidden": false}, {"_id": "69817e2cce18b18628096207", "name": "Zhengyang Tang", "hidden": false}, {"_id": "69817e2cce18b18628096208", "name": "Jiawen Tao", "hidden": false}, {"_id": "69817e2cce18b18628096209", "name": "Shiyuan Teng", "hidden": false}, {"_id": "69817e2cce18b1862809620a", "name": "Chaoran Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620b", "name": "Pengfei Tian", "hidden": false}, {"_id": "69817e2cce18b1862809620c", "name": "Ao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620d", "name": "Bowen Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620e", "name": "Chensi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809620f", "name": "Chuang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096210", "name": "Congcong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096211", "name": "Dingkun Wang", "hidden": false}, {"_id": "69817e2cce18b18628096212", "name": "Dinglu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096213", "name": "Dongliang Wang", "hidden": false}, {"_id": "69817e2cce18b18628096214", "name": "Feng Wang", "hidden": false}, {"_id": "69817e2cce18b18628096215", "name": "Hailong Wang", "hidden": false}, {"_id": "69817e2cce18b18628096216", "name": "Haiming Wang", "hidden": false}, {"_id": "69817e2cce18b18628096217", "name": "Hengzhi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096218", "name": "Huaqing Wang", "hidden": false}, {"_id": "69817e2cce18b18628096219", "name": "Hui Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621a", "name": "Jiahao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621b", "name": "Jinhong Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621c", "name": "Jiuzheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621d", "name": "Kaixin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621e", "name": "Linian Wang", "hidden": false}, {"_id": "69817e2cce18b1862809621f", "name": "Qibin Wang", "hidden": false}, {"_id": "69817e2cce18b18628096220", "name": "Shengjie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096221", "name": "Shuyi Wang", "hidden": false}, {"_id": "69817e2cce18b18628096222", "name": "Si Wang", "hidden": false}, {"_id": "69817e2cce18b18628096223", "name": "Wei Wang", "hidden": false}, {"_id": "69817e2cce18b18628096224", "name": "Xiaochen Wang", "hidden": false}, {"_id": "69817e2cce18b18628096225", "name": "Xinyuan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096226", "name": "Yao Wang", "hidden": false}, {"_id": "69817e2cce18b18628096227", "name": "Yejie Wang", "hidden": false}, {"_id": "69817e2cce18b18628096228", "name": "Yipu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096229", "name": "Yiqin Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622a", "name": "Yucheng Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622b", "name": "Yuzhi Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622c", "name": "Zhaoji Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622d", "name": "Zhaowei Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622e", "name": "Zhengtao Wang", "hidden": false}, {"_id": "69817e2cce18b1862809622f", "name": "Zhexu Wang", "hidden": false}, {"_id": "69817e2cce18b18628096230", "name": "Zihan Wang", "hidden": false}, {"_id": "69817e2cce18b18628096231", "name": "Zizhe Wang", "hidden": false}, {"_id": "69817e2cce18b18628096232", "user": {"_id": "635ddec594e5b275ca7941e8", "avatarUrl": "/avatars/28ebfaee74d31e1de020a3ae735a4c1b.svg", "isPro": false, "fullname": "Chu Wei", "user": "courage17340", "type": "user"}, "name": "Chu Wei", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:17.862Z", "hidden": false}, {"_id": "69817e2cce18b18628096233", "name": "Ming Wei", "hidden": false}, {"_id": "69817e2cce18b18628096234", "name": "Chuan Wen", "hidden": false}, {"_id": "69817e2cce18b18628096235", "user": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "isPro": false, "fullname": "Zichen Wen", "user": "zichenwen", "type": "user"}, "name": "Zichen Wen", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:09:43.363Z", "hidden": false}, {"_id": "69817e2cce18b18628096236", "name": "Chengjie Wu", "hidden": false}, {"_id": "69817e2cce18b18628096237", "user": {"_id": "63047ed2412a1b9d381b09c9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63047ed2412a1b9d381b09c9/2Ill5G0uSMyGstrawgmIb.jpeg", "isPro": true, "fullname": "Haoning Wu, Teo", "user": "teowu", "type": "user"}, "name": "Haoning Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T10:05:49.884Z", "hidden": false}, {"_id": "69817e2cce18b18628096238", "name": "Junyan Wu", "hidden": false}, {"_id": "69817e2cce18b18628096239", "name": "Rucong Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623a", "name": "Wenhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623b", "name": "Yuefeng Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623c", "name": "Yuhao Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623d", "name": "Yuxin Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623e", "name": "Zijian Wu", "hidden": false}, {"_id": "69817e2cce18b1862809623f", "name": "Chenjun Xiao", "hidden": false}, {"_id": "69817e2cce18b18628096240", "name": "Jin Xie", "hidden": false}, {"_id": "69817e2cce18b18628096241", "name": "Xiaotong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096242", "name": "Yuchong Xie", "hidden": false}, {"_id": "69817e2cce18b18628096243", "name": "Yifei Xin", "hidden": false}, {"_id": "69817e2cce18b18628096244", "name": "Bowei Xing", "hidden": false}, {"_id": "69817e2cce18b18628096245", "name": "Boyu Xu", "hidden": false}, {"_id": "69817e2cce18b18628096246", "name": "Jianfan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096247", "name": "Jing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096248", "name": "Jinjing Xu", "hidden": false}, {"_id": "69817e2cce18b18628096249", "name": "L. H. Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624a", "name": "Lin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624b", "name": "Suting Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624c", "name": "Weixin Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624d", "name": "Xinbo Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624e", "name": "Xinran Xu", "hidden": false}, {"_id": "69817e2cce18b1862809624f", "name": "Yangchuan Xu", "hidden": false}, {"_id": "69817e2cce18b18628096250", "name": "Yichang Xu", "hidden": false}, {"_id": "69817e2cce18b18628096251", "name": "Yuemeng Xu", "hidden": false}, {"_id": "69817e2cce18b18628096252", "name": "Zelai Xu", "hidden": false}, {"_id": "69817e2cce18b18628096253", "name": "Ziyao Xu", "hidden": false}, {"_id": "69817e2cce18b18628096254", "name": "Junjie Yan", "hidden": false}, {"_id": "69817e2cce18b18628096255", "name": "Yuzi Yan", "hidden": false}, {"_id": "69817e2cce18b18628096256", "name": "Guangyao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096257", "name": "Hao Yang", "hidden": false}, {"_id": "69817e2cce18b18628096258", "name": "Junwei Yang", "hidden": false}, {"_id": "69817e2cce18b18628096259", "name": "Kai Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625a", "name": "Ningyuan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625b", "name": "Ruihan Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625c", "name": "Xiaofei Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625d", "name": "Xinlong Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625e", "name": "Ying Yang", "hidden": false}, {"_id": "69817e2cce18b1862809625f", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096260", "name": "Yi Yang", "hidden": false}, {"_id": "69817e2cce18b18628096261", "name": "Zhen Yang", "hidden": false}, {"_id": "69817e2cce18b18628096262", "name": "Zhilin Yang", "hidden": false}, {"_id": "69817e2cce18b18628096263", "name": "Zonghan Yang", "hidden": false}, {"_id": "69817e2cce18b18628096264", "user": {"_id": "642bcd9be8dfcc1fe4f4f853", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642bcd9be8dfcc1fe4f4f853/M9Yqkyt66dnWWCwmBZ8l0.jpeg", "isPro": false, "fullname": "Haotian Yao", "user": "skylark-95", "type": "user"}, "name": "Haotian Yao", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:12.739Z", "hidden": false}, {"_id": "69817e2cce18b18628096265", "name": "Dan Ye", "hidden": false}, {"_id": "69817e2cce18b18628096266", "name": "Wenjie Ye", "hidden": false}, {"_id": "69817e2cce18b18628096267", "name": "Zhuorui Ye", "hidden": false}, {"_id": "69817e2cce18b18628096268", "name": "Bohong Yin", "hidden": false}, {"_id": "69817e2cce18b18628096269", "name": "Chengzhen Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626a", "name": "Longhui Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626b", "name": "Tao Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626c", "name": "Tianxiang Yu", "hidden": false}, {"_id": "69817e2cce18b1862809626d", "name": "Enming Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626e", "name": "Mengjie Yuan", "hidden": false}, {"_id": "69817e2cce18b1862809626f", "name": "Xiaokun Yuan", "hidden": false}, {"_id": "69817e2cce18b18628096270", "name": "Yang Yue", "hidden": false}, {"_id": "69817e2cce18b18628096271", "name": "Weihao Zeng", "hidden": false}, {"_id": "69817e2cce18b18628096272", "name": "Dunyuan Zha", "hidden": false}, {"_id": "69817e2cce18b18628096273", "name": "Haobing Zhan", "hidden": false}, {"_id": "69817e2cce18b18628096274", "name": "Dehao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096275", "name": "Hao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096276", "name": "Jin Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096277", "name": "Puqi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096278", "name": "Qiao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096279", "name": "Rui Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627a", "name": "Xiaobin Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627b", "name": "Y. Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627c", "name": "Yadong Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627d", "name": "Yangkun Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627e", "name": "Yichi Zhang", "hidden": false}, {"_id": "69817e2cce18b1862809627f", "name": "Yizhi Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096280", "name": "Yongting Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096281", "name": "Yu Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096282", "name": "Yushun Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096283", "name": "Yutao Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096284", "name": "Yutong Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096285", "name": "Zheng Zhang", "hidden": false}, {"_id": "69817e2cce18b18628096286", "name": "Chenguang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096287", "name": "Feifan Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096288", "name": "Jinxiang Zhao", "hidden": false}, {"_id": "69817e2cce18b18628096289", "name": "Shuai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628a", "name": "Xiangyu Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628b", "name": "Yikai Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628c", "name": "Zijia Zhao", "hidden": false}, {"_id": "69817e2cce18b1862809628d", "name": "Huabin Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628e", "name": "Ruihan Zheng", "hidden": false}, {"_id": "69817e2cce18b1862809628f", "name": "Shaojie Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096290", "name": "Tengyang Zheng", "hidden": false}, {"_id": "69817e2cce18b18628096291", "name": "Junfeng Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096292", "user": {"_id": "62b6d20416ff90e6198301b6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656148456743-noauth.png", "isPro": false, "fullname": "Longguang Zhong", "user": "GGLS", "type": "user"}, "name": "Longguang Zhong", "status": "claimed_verified", "statusLastChangedAt": "2026-02-03T13:59:14.989Z", "hidden": false}, {"_id": "69817e2cce18b18628096293", "name": "Weiming Zhong", "hidden": false}, {"_id": "69817e2cce18b18628096294", "name": "M. Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096295", "name": "Runjie Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096296", "name": "Xinyu Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096297", "name": "Zaida Zhou", "hidden": false}, {"_id": "69817e2cce18b18628096298", "name": "Jinguo Zhu", "hidden": false}, {"_id": "69817e2cce18b18628096299", "name": "Liya Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629a", "name": "Xinhao Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629b", "name": "Yuxuan Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629c", "name": "Zhen Zhu", "hidden": false}, {"_id": "69817e2cce18b1862809629d", "name": "Jingze Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629e", "name": "Weiyu Zhuang", "hidden": false}, {"_id": "69817e2cce18b1862809629f", "name": "Ying Zou", "hidden": false}, {"_id": "69817e2cce18b186280962a0", "name": "Xinxing Zu", "hidden": false}], "publishedAt": "2026-02-02T16:17:38.000Z", "submittedOnDailyAt": "2026-02-03T02:18:48.721Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "upvotes": 149, "discussionId": "69817e2cce18b186280962a1", "projectPage": "https://huggingface.co/moonshotai/Kimi-K2.5", "ai_summary": "Kimi K2.5 is an open-source multimodal agentic model that enhances text and vision processing through joint optimization techniques and introduces Agent Swarm for parallel task execution.", "ai_keywords": ["multimodal agentic model", "joint text-vision pre-training", "zero-vision SFT", "joint text-vision reinforcement learning", "Agent Swarm", "self-directed parallel agent orchestration framework", "heterogeneous sub-problems"], "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86Kimi K2.5\uff0c\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u667a\u80fd\u6a21\u578b\uff0c\u65e8\u5728\u63a8\u52a8\u4e00\u822c\u667a\u80fd\u7684\u53d1\u5c55\u3002</li>\n    <li>K2.5\u5f3a\u8c03\u6587\u672c\u548c\u89c6\u89c9\u7684\u8054\u5408\u4f18\u5316\uff0c\u4ee5\u589e\u5f3a\u4e24\u8005\u7684\u4e92\u8865\u6548\u679c\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u4e86\u4e00\u7cfb\u5217\u6280\u672f\uff0c\u5305\u62ec\u8054\u5408\u6587\u672c-\u89c6\u89c9\u9884\u8bad\u7ec3\u3001\u96f6\u89c6\u89c9SFT\u548c\u8054\u5408\u6587\u672c-\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u3002</li>\n    <li>K2.5\u5f15\u5165\u4e86Agent Swarm\uff0c\u4e00\u4e2a\u81ea\u6211\u5bfc\u5411\u7684\u5e76\u884c\u4ee3\u7406\u8c03\u5ea6\u6846\u67b6\uff0c\u80fd\u52a8\u6001\u5206\u89e3\u590d\u6742\u4efb\u52a1\u5e76\u540c\u65f6\u6267\u884c\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cKimi K2.5\u5728\u7f16\u7801\u3001\u89c6\u89c9\u3001\u63a8\u7406\u548c\u667a\u80fd\u4efb\u52a1\u7b49\u591a\u4e2a\u9886\u57df\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kimi K2.5 is a new open-source model that combines text and vision to improve intelligence.</li>\n    <li>It uses techniques like pre-training and reinforcement learning to make text and vision work better together.</li>\n    <li>K2.5 features Agent Swarm, which breaks down complex tasks into smaller parts that can be worked on at the same time.</li>\n    <li>The model shows excellent performance in areas like coding, vision, reasoning, and other tasks.</li>\n    <li>Kimi K2.5 is designed to be fast, reducing task completion time by up to 4.5 times compared to single-agent systems.</li>\n</ul>"}, "publishedAt": "2026-02-02T11:17:38.000Z", "title": "Kimi K2.5: Visual Agentic Intelligence", "summary": "We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to 4.5times over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.02276.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 227, "isUserFollowing": false}, "organization": {"_id": "6425a114812813f8f4a9b02c", "name": "moonshotai", "fullname": "Moonshot AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/641c1e77c3983aa9490f8121/X1yT2rsaIbR9cdYGEVu0X.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2602.09082", "authors": [{"_id": "698bea506052d3bed96309cb", "name": "Veuns-Team", "hidden": false}, {"_id": "698bea506052d3bed96309cd", "name": "Changlong Gao", "hidden": false}, {"_id": "698bea506052d3bed96309ce", "user": {"_id": "60d2a2984956988b63753371", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg", "isPro": false, "fullname": "Zhangxuan Gu", "user": "zhangxgu", "type": "user"}, "name": "Zhangxuan Gu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:15:14.456Z", "hidden": false}, {"_id": "698bea506052d3bed96309cf", "name": "Yulin Liu", "hidden": false}, {"_id": "698bea506052d3bed96309d0", "name": "Xinyu Qiu", "hidden": false}, {"_id": "698bea506052d3bed96309d1", "name": "Shuheng Shen", "hidden": false}, {"_id": "698bea506052d3bed96309d2", "name": "Yue Wen", "hidden": false}, {"_id": "698bea506052d3bed96309d3", "name": "Tianyu Xia", "hidden": false}, {"_id": "698bea506052d3bed96309d4", "name": "Zhenyu Xu", "hidden": false}, {"_id": "698bea506052d3bed96309d5", "user": {"_id": "64cb238576200ec80fe988f8", "avatarUrl": "/avatars/42c48710c7881c9dfbcc075fec3cb600.svg", "isPro": false, "fullname": "zeus", "user": "zengw", "type": "user"}, "name": "Zhengwen Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:24:43.235Z", "hidden": false}, {"_id": "698bea506052d3bed96309d6", "user": {"_id": "654c9dac09dd7ef524a0be1e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654c9dac09dd7ef524a0be1e/T4glmZthS0mJydhvGZGKH.png", "isPro": false, "fullname": "beitongzhou", "user": "syorami", "type": "user"}, "name": "Beitong Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:15:11.859Z", "hidden": false}, {"_id": "698bea506052d3bed96309d7", "name": "Xingran Zhou", "hidden": false}, {"_id": "698bea506052d3bed96309d8", "name": "Weizhi Chen", "hidden": false}, {"_id": "698bea506052d3bed96309d9", "name": "Sunhao Dai", "hidden": false}, {"_id": "698bea506052d3bed96309da", "name": "Jingya Dou", "hidden": false}, {"_id": "698bea506052d3bed96309db", "name": "Yichen Gong", "hidden": false}, {"_id": "698bea506052d3bed96309dc", "name": "Yuan Guo", "hidden": false}, {"_id": "698bea506052d3bed96309dd", "name": "Zhenlin Guo", "hidden": false}, {"_id": "698bea506052d3bed96309de", "user": {"_id": "65e0763a9299e96ee674876e", "avatarUrl": "/avatars/0ea342c9f72fa3b8a8f634559d094907.svg", "isPro": false, "fullname": "fengdian", "user": "fengrudian", "type": "user"}, "name": "Feng Li", "status": "claimed_verified", "statusLastChangedAt": "2026-02-11T11:16:04.463Z", "hidden": false}, {"_id": "698bea506052d3bed96309df", "name": "Qian Li", "hidden": false}, {"_id": "698bea506052d3bed96309e0", "name": "Jinzhen Lin", "hidden": false}, {"_id": "698bea506052d3bed96309e1", "name": "Yuqi Zhou", "hidden": false}, {"_id": "698bea506052d3bed96309e2", "name": "Linchao Zhu", "hidden": false}, {"_id": "698bea506052d3bed96309e3", "name": "Liang Chen", "hidden": false}, {"_id": "698bea506052d3bed96309e4", "name": "Zhenyu Guo", "hidden": false}, {"_id": "698bea506052d3bed96309e5", "name": "Changhua Meng", "hidden": false}, {"_id": "698bea506052d3bed96309e6", "name": "Weiqiang Wang", "hidden": false}], "publishedAt": "2026-02-09T18:43:40.000Z", "submittedOnDailyAt": "2026-02-11T00:10:55.649Z", "title": "UI-Venus-1.5 Technical Report", "submittedOnDailyBy": {"_id": "60d2a2984956988b63753371", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg", "isPro": false, "fullname": "Zhangxuan Gu", "user": "zhangxgu", "type": "user"}, "summary": "GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus", "upvotes": 143, "discussionId": "698bea516052d3bed96309e7", "projectPage": "https://ui-venus.github.io/UI-Venus-1.5/", "githubRepo": "https://github.com/inclusionAI/UI-Venus/blob/UI-Venus-1.5", "githubRepoAddedBy": "user", "ai_summary": "UI-Venus-1.5 is a unified GUI agent with improved performance through mid-training stages, online reinforcement learning, and model merging techniques.", "ai_keywords": ["GUI agents", "Mid-Training stage", "Online Reinforcement Learning", "full-trajectory rollouts", "Model Merging", "dense variants", "mixture-of-experts variant"], "githubStars": 708, "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "summary_zh": "<ul>\n    <li>UI-Venus-1.5\u662f\u4e00\u4e2a\u65b0\u7684\u7edf\u4e00\u578bGUI\u4ee3\u7406\uff0c\u65e8\u5728\u63d0\u9ad8\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u81ea\u52a8\u5316\u4ea4\u4e92\u3002</li>\n    <li>\u8be5\u6a21\u578b\u6709\u4e09\u4e2a\u4e0d\u540c\u7684\u7248\u672c\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u5e94\u7528\u573a\u666f\uff0c\u5305\u62ec\u5bc6\u96c6\u578b\u548c\u4e13\u5bb6\u6df7\u5408\u578b\u3002</li>\n    <li>\u5f15\u5165\u4e86\u4e09\u4e2a\u4e3b\u8981\u6280\u672f\u8fdb\u5c55\uff0c\u6539\u5584\u4e86\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u6027\u80fd\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUI-Venus-1.5\u7684\u8868\u73b0\u4f18\u4e8e\u4e4b\u524d\u7684\u7248\u672c\uff0c\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u9ad8\u6c34\u5e73\u3002</li>\n    <li>\u8be5\u4ee3\u7406\u5728\u5404\u79cd\u4e2d\u56fd\u624b\u673a\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u6267\u884c\u7528\u6237\u6307\u4ee4\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>UI-Venus-1.5 is a new, advanced GUI agent that helps automate tasks in digital environments.</li>\n    <li>It includes different model sizes to suit various applications: two dense versions (2B and 8B) and one mixture-of-experts version (30B-A3B).</li>\n    <li>Key improvements in UI-Venus-1.5 include a training stage using 10 billion tokens, online reinforcement learning for better navigation, and a unified model that combines several specialized models.</li>\n    <li>This new model achieves top performance on several benchmarks, such as ScreenSpot-Pro, VenusBench-GD, and AndroidWorld, surpassing previous models.</li>\n    <li>UI-Venus-1.5 is effective in navigating and executing tasks on Chinese mobile apps in real-world situations.</li>\n</ul>"}, "publishedAt": "2026-02-09T13:43:40.000Z", "title": "UI-Venus-1.5 Technical Report", "summary": "GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.09082.png", "numComments": 2, "submittedBy": {"_id": "60d2a2984956988b63753371", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60d2a2984956988b63753371/apXIcWbi7jnLVH37CdMTV.jpeg", "fullname": "Zhangxuan Gu", "name": "zhangxgu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "67aea5c8f086ab0f70ed97c9", "name": "inclusionAI", "fullname": "inclusionAI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/662e1f9da266499277937d33/fyKuazRifqiaIO34xrhhm.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2602.07085", "authors": [{"_id": "698ab6f91b2dc6b37d61b031", "name": "Jun Han", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b032", "name": "Shuo Zhang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b033", "name": "Wei Li", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b034", "user": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "name": "Zhi Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:05:58.707Z", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b035", "name": "Yifan Dong", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b036", "name": "Tu Hu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b037", "name": "Jialuo Yuan", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b038", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-02-10T09:06:00.954Z", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b039", "name": "Yumo Zhu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03a", "name": "Fangqi Lou", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03b", "name": "Xin Guo", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03c", "name": "Zhaowei Liu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03d", "name": "Tianyi Jiang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03e", "name": "Ruichuan An", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b03f", "name": "Jingping Liu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b040", "name": "Biao Wu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b041", "name": "Rongze Chen", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b042", "name": "Kunyi Wang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b043", "name": "Yifan Wang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b044", "name": "Sen Hu", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b045", "name": "Xinbing Kong", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b046", "name": "Liwen Zhang", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b047", "name": "Ronghao Chen", "hidden": false}, {"_id": "698ab6f91b2dc6b37d61b048", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-02-06T08:08:04.000Z", "submittedOnDailyAt": "2026-02-10T02:19:22.216Z", "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining", "submittedOnDailyBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "isPro": false, "fullname": "Zhi Yang", "user": "yangzhi1", "type": "user"}, "summary": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.", "upvotes": 141, "discussionId": "698ab6fa1b2dc6b37d61b049", "githubRepo": "https://github.com/QuantaAlpha/QuantaAlpha", "githubRepoAddedBy": "user", "githubStars": 63, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "summary_zh": "<ul>\n    <li>QuantaAlpha\u662f\u4e00\u4e2a\u8fdb\u5316\u6027\u7684alpha\u6316\u6398\u6846\u67b6\uff0c\u65e8\u5728\u5e94\u5bf9\u91d1\u878d\u5e02\u573a\u7684\u566a\u97f3\u548c\u975e\u5e73\u7a33\u6027\u3002</li>\n    <li>\u8be5\u6846\u67b6\u901a\u8fc7\u5bf9\u6316\u6398\u8fc7\u7a0b\u7684\u8f68\u8ff9\u8fdb\u884c\u7a81\u53d8\u548c\u4ea4\u53c9\u64cd\u4f5c\uff0c\u6539\u8fdb\u4e86alpha\u6316\u6398\u7684\u6548\u679c\u3002</li>\n    <li>QuantaAlpha\u80fd\u591f\u9488\u5bf9\u6bcf\u4e2a\u8f68\u8ff9\u4e2d\u7684\u6b21\u4f18\u6b65\u9aa4\u8fdb\u884c\u4fee\u6b63\uff0c\u5e76\u91cd\u65b0\u7ec4\u5408\u9ad8\u56de\u62a5\u7684\u6bb5\u843d\uff0c\u4ee5\u4fbf\u6709\u6548\u7684\u6a21\u5f0f\u91cd\u7528\u3002</li>\n    <li>\u5728\u4e2d\u56fd\u8bc1\u5238\u6307\u6570300\uff08CSI 300\uff09\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u663e\u793a\uff0cQuantaAlpha\u7684\u8868\u73b0\u8d85\u8fc7\u4e86\u5f3a\u57fa\u7ebf\u6a21\u578b\u548c\u4e4b\u524d\u7684\u7cfb\u7edf\u3002</li>\n    <li>\u5728\u4f7f\u7528GPT-5.2\u65f6\uff0cQuantaAlpha\u5b9e\u73b0\u4e8627.75%\u7684\u5e74\u5316\u6536\u76ca\u7387\uff0c\u5e76\u4e14\u5176\u6316\u6398\u7684\u56e0\u5b50\u5728\u4e0d\u540c\u5e02\u573a\u4e4b\u95f4\u8868\u73b0\u7a33\u5b9a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Financial markets are unpredictable, making it hard to find reliable trading strategies (alpha mining).</li>\n    <li>QuantaAlpha is a new system that improves alpha mining by using evolutionary techniques to refine strategies.</li>\n    <li>It focuses on fixing weak points in strategies and combining the best parts of successful ones for better results.</li>\n    <li>QuantaAlpha ensures that the generated trading strategies are clear and not overly complex.</li>\n    <li>Tests show that QuantaAlpha outperforms other systems, achieving high returns and performing well across different market indices.</li>\n</ul>"}, "publishedAt": "2026-02-06T03:08:04.000Z", "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining", "summary": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2602.07085.png", "numComments": 1, "submittedBy": {"_id": "64aa645404e7b379feccc490", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aa645404e7b379feccc490/4m8qcdy2OGK8visR5Jjl5.png", "fullname": "Zhi Yang", "name": "yangzhi1", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Feb 20, 2026";
window.trendingPapers = {
    "today": [{"paper": {"id": "2601.18491", "authors": [{"_id": "697831d9026bdf0473116e5c", "name": "Dongrui Liu", "hidden": false}, {"_id": "697831d9026bdf0473116e5d", "user": {"_id": "66e2624a436a1798365e4581", "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg", "isPro": false, "fullname": "Qihan Ren", "user": "jasonrqh", "type": "user"}, "name": "Qihan Ren", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:31:15.765Z", "hidden": false}, {"_id": "697831d9026bdf0473116e5e", "name": "Chen Qian", "hidden": false}, {"_id": "697831d9026bdf0473116e5f", "name": "Shuai Shao", "hidden": false}, {"_id": "697831d9026bdf0473116e60", "name": "Yuejin Xie", "hidden": false}, {"_id": "697831d9026bdf0473116e61", "name": "Yu Li", "hidden": false}, {"_id": "697831d9026bdf0473116e62", "name": "Zhonghao Yang", "hidden": false}, {"_id": "697831d9026bdf0473116e63", "name": "Haoyu Luo", "hidden": false}, {"_id": "697831d9026bdf0473116e64", "name": "Peng Wang", "hidden": false}, {"_id": "697831d9026bdf0473116e65", "name": "Qingyu Liu", "hidden": false}, {"_id": "697831d9026bdf0473116e66", "name": "Binxin Hu", "hidden": false}, {"_id": "697831d9026bdf0473116e67", "name": "Ling Tang", "hidden": false}, {"_id": "697831d9026bdf0473116e68", "name": "Jilin Mei", "hidden": false}, {"_id": "697831d9026bdf0473116e69", "name": "Dadi Guo", "hidden": false}, {"_id": "697831d9026bdf0473116e6a", "name": "Leitao Yuan", "hidden": false}, {"_id": "697831d9026bdf0473116e6b", "name": "Junyao Yang", "hidden": false}, {"_id": "697831d9026bdf0473116e6c", "name": "Guanxu Chen", "hidden": false}, {"_id": "697831d9026bdf0473116e6d", "name": "Qihao Lin", "hidden": false}, {"_id": "697831d9026bdf0473116e6e", "name": "Yi Yu", "hidden": false}, {"_id": "697831d9026bdf0473116e6f", "name": "Bo Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e70", "name": "Jiaxuan Guo", "hidden": false}, {"_id": "697831d9026bdf0473116e71", "name": "Jie Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e72", "name": "Wenqi Shao", "hidden": false}, {"_id": "697831d9026bdf0473116e73", "name": "Huiqi Deng", "hidden": false}, {"_id": "697831d9026bdf0473116e74", "name": "Zhiheng Xi", "hidden": false}, {"_id": "697831d9026bdf0473116e75", "name": "Wenjie Wang", "hidden": false}, {"_id": "697831d9026bdf0473116e76", "name": "Wenxuan Wang", "hidden": false}, {"_id": "697831d9026bdf0473116e77", "name": "Wen Shen", "hidden": false}, {"_id": "697831d9026bdf0473116e78", "name": "Zhikai Chen", "hidden": false}, {"_id": "697831d9026bdf0473116e79", "name": "Haoyu Xie", "hidden": false}, {"_id": "697831d9026bdf0473116e7a", "name": "Jialing Tao", "hidden": false}, {"_id": "697831d9026bdf0473116e7b", "name": "Juntao Dai", "hidden": false}, {"_id": "697831d9026bdf0473116e7c", "name": "Jiaming Ji", "hidden": false}, {"_id": "697831d9026bdf0473116e7d", "name": "Zhongjie Ba", "hidden": false}, {"_id": "697831d9026bdf0473116e7e", "name": "Linfeng Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e7f", "name": "Yong Liu", "hidden": false}, {"_id": "697831d9026bdf0473116e80", "name": "Quanshi Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e81", "name": "Lei Zhu", "hidden": false}, {"_id": "697831d9026bdf0473116e82", "name": "Zhihua Wei", "hidden": false}, {"_id": "697831d9026bdf0473116e83", "name": "Hui Xue", "hidden": false}, {"_id": "697831d9026bdf0473116e84", "name": "Chaochao Lu", "hidden": false}, {"_id": "697831d9026bdf0473116e85", "name": "Jing Shao", "hidden": false}, {"_id": "697831d9026bdf0473116e86", "name": "Xia Hu", "hidden": false}], "publishedAt": "2026-01-26T13:45:41.000Z", "submittedOnDailyAt": "2026-01-28T01:26:49.833Z", "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security", "submittedOnDailyBy": {"_id": "66e2624a436a1798365e4581", "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg", "isPro": false, "fullname": "Qihan Ren", "user": "jasonrqh", "type": "user"}, "summary": "The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.", "upvotes": 60, "discussionId": "697831d9026bdf0473116e87", "githubRepo": "https://github.com/AI45Lab/AgentDoG", "githubRepoAddedBy": "user", "ai_summary": "AI agents face safety and security challenges from autonomous tool use and environmental interactions, requiring advanced guardrail frameworks for risk diagnosis and transparent monitoring.", "ai_keywords": ["agentic guardrail", "three-dimensional taxonomy", "agentic safety benchmark", "Diagnostic Guardrail framework", "agent safety and security", "agent trajectories", "root cause diagnosis", "fine-grained monitoring", "model variants", "state-of-the-art performance"], "githubStars": 178, "organization": {"_id": "68f716f832b31e42cbc2be7f", "name": "AI45Research", "fullname": "AI45Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68f6ffaa04d1019724af41fc/EVBafPHXvChszTM5tJcc9.png"}, "summary_zh": "<ul>\n    <li>AI\u667a\u80fd\u4f53\u7684\u4f7f\u7528\u5e26\u6765\u4e86\u590d\u6742\u7684\u5b89\u5168\u548c\u5b89\u5168\u6311\u6218\u3002</li>\n    <li>\u5f53\u524d\u7684\u5b89\u5168\u6a21\u578b\u7f3a\u4e4f\u5bf9\u98ce\u9669\u7684\u8ba4\u8bc6\u548c\u900f\u660e\u5ea6\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e09\u7ef4\u5206\u7c7b\u6cd5\uff0c\u5206\u7c7b\u667a\u80fd\u4f53\u98ce\u9669\u7684\u6765\u6e90\u3001\u5931\u8d25\u65b9\u5f0f\u548c\u540e\u679c\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u57fa\u51c6\uff08ATBench\uff09\u548c\u8bca\u65ad\u6846\u67b6\uff08AgentDoG\uff09\uff0c\u53ef\u4ee5\u76d1\u63a7\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u3002</li>\n    <li>AgentDoG\u80fd\u591f\u8bca\u65ad\u4e0d\u5b89\u5168\u548c\u4f3c\u4e4e\u5b89\u5168\u4f46\u4e0d\u5408\u7406\u7684\u884c\u4e3a\uff0c\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u7684\u5bf9\u9f50\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The rise of AI agents brings new safety and security challenges from their autonomous use and interactions with the environment.</li>\n    <li>Current safety models lack awareness of risks associated with AI agents and do not provide clear risk diagnoses.</li>\n    <li>A new three-dimensional system categorizes agentic risks based on their source, failure mode, and consequences.</li>\n    <li>A new benchmark called ATBench and a safety framework named AgentDoG have been introduced to improve monitoring and diagnosis of AI agent behaviors.</li>\n    <li>AgentDoG can identify the causes of unsafe actions and offers detailed insights, with models available in different sizes, showing excellent performance in complex scenarios.</li>\n</ul>"}, "publishedAt": "2026-01-26T08:45:41.000Z", "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security", "summary": "The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18491.png", "numComments": 6, "submittedBy": {"_id": "66e2624a436a1798365e4581", "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg", "fullname": "Qihan Ren", "name": "jasonrqh", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "68f716f832b31e42cbc2be7f", "name": "AI45Research", "fullname": "AI45Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68f6ffaa04d1019724af41fc/EVBafPHXvChszTM5tJcc9.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.18631", "authors": [{"_id": "6978a169026bdf0473117088", "user": {"_id": "66aca01e33f6b27979856f6f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg", "isPro": false, "fullname": "Mingyang Song", "user": "hitsmy", "type": "user"}, "name": "Mingyang Song", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:29:30.581Z", "hidden": false}, {"_id": "6978a169026bdf0473117089", "user": {"_id": "63a2a51ef30c464227924fc6", "avatarUrl": "/avatars/e109e85abd25b97bb29dbbe007119e34.svg", "isPro": false, "fullname": "Haoyu Sun", "user": "Mikivis", "type": "user"}, "name": "Haoyu Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:29:26.154Z", "hidden": false}, {"_id": "6978a169026bdf047311708a", "user": {"_id": "645b4819f9d4ec91fdd54852", "avatarUrl": "/avatars/e12efb8e030688a0afcc72176b453fb3.svg", "isPro": false, "fullname": "Jiawei Gu", "user": "kuvvi", "type": "user"}, "name": "Jiawei Gu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:29:28.392Z", "hidden": false}, {"_id": "6978a169026bdf047311708b", "name": "Linjie Li", "hidden": false}, {"_id": "6978a169026bdf047311708c", "name": "Luxin Xu", "hidden": false}, {"_id": "6978a169026bdf047311708d", "name": "Ranjay Krishna", "hidden": false}, {"_id": "6978a169026bdf047311708e", "name": "Yu Cheng", "hidden": false}], "publishedAt": "2026-01-26T16:04:43.000Z", "submittedOnDailyAt": "2026-01-28T01:52:20.218Z", "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning", "submittedOnDailyBy": {"_id": "66aca01e33f6b27979856f6f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg", "isPro": false, "fullname": "Mingyang Song", "user": "hitsmy", "type": "user"}, "summary": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce AdaReasoner, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.", "upvotes": 38, "discussionId": "6978a16a026bdf047311708f", "projectPage": "https://adareasoner.github.io/", "githubRepo": "https://github.com/ssmisya/AdaReasoner", "githubRepoAddedBy": "user", "ai_summary": "AdaReasoner enables multimodal models to learn tool usage as a general reasoning skill through scalable data curation, reinforcement learning for tool selection, and adaptive learning mechanisms that improve performance on complex visual reasoning tasks.", "ai_keywords": ["multimodal large language models", "tool use", "reinforcement learning", "end-task success", "adaptive learning mechanism", "visual reasoning", "multimodal models", "tool selection", "tool sequencing", "long-horizon interactions"], "githubStars": 44, "organization": {"_id": "643cb0625fcffe09fb6ca688", "name": "Fudan-University", "fullname": "Fudan University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"}, "summary_zh": "<ul>\n    <li>AdaReasoner \u662f\u4e00\u79cd\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5b66\u4e60\u5de5\u5177\u4f7f\u7528\u4f5c\u4e3a\u901a\u7528\u63a8\u7406\u6280\u80fd\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6570\u636e\u5904\u7406\u7ba1\u9053\u8fdb\u884c\u8bad\u7ec3\uff0c\u80fd\u591f\u5904\u7406\u957f\u65f6\u95f4\u548c\u591a\u6b65\u9aa4\u7684\u5de5\u5177\u4ea4\u4e92\u3002</li>\n    <li>\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5 Tool-GRPO \u6765\u4f18\u5316\u5de5\u5177\u9009\u62e9\u548c\u987a\u5e8f\uff0c\u4ee5\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002</li>\n    <li>AdaReasoner \u80fd\u591f\u6839\u636e\u4efb\u52a1\u9700\u6c42\u52a8\u6001\u8c03\u6574\u5de5\u5177\u4f7f\u7528\u9891\u7387\uff0c\u5e76\u81ea\u52a8\u9009\u62e9\u6709\u7528\u7684\u5de5\u5177\u3002</li>\n    <li\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\uff0cAdaReasoner \u7684\u8868\u73b0\u8d85\u8d8a\u4e86\u8bb8\u591a\u5f3a\u5927\u7684\u7cfb\u7edf\uff0c\u5305\u62ec GPT-5\uff0c\u5e73\u5747\u63d0\u5347 24.9%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>AdaReasoner is a new family of multimodal models designed to improve reasoning by learning how to use tools effectively.</li>\n    <li>It focuses on knowing which tools to use, when to use them, and how to combine them for complex tasks.</li>\n    <li>The system uses a special data curation process and a reinforcement learning algorithm to enhance tool selection and usage.</li>\n    <li>AdaReasoner can adaptively learn from tasks, recognizing which tools are useful without needing specific training for each tool.</li>\n    <li>It shows strong performance improvements over existing models, achieving better results on various tasks and benchmarks.</li>\n</ul>"}, "publishedAt": "2026-01-26T11:04:43.000Z", "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning", "summary": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce AdaReasoner, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18631.png", "numComments": 3, "submittedBy": {"_id": "66aca01e33f6b27979856f6f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66aca01e33f6b27979856f6f/IyOxv89TudwscGH7tdue3.jpeg", "fullname": "Mingyang Song", "name": "hitsmy", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "643cb0625fcffe09fb6ca688", "name": "Fudan-University", "fullname": "Fudan University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6437eca0819f3ab20d162e14/kWv0cGlAhAG3iNWVxowkJ.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.18692", "authors": [{"_id": "697989f6df44b75fa47e4803", "user": {"_id": "69773d4ee6878183fb90a8c7", "avatarUrl": "/avatars/3e9e4081e3beaf3f69c380387b8ee4c2.svg", "isPro": false, "fullname": "Wei Wu", "user": "Weiww99", "type": "user"}, "name": "Wei Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:15:53.890Z", "hidden": false}, {"_id": "697989f6df44b75fa47e4804", "name": "Fan Lu", "hidden": false}, {"_id": "697989f6df44b75fa47e4805", "name": "Yunnan Wang", "hidden": false}, {"_id": "697989f6df44b75fa47e4806", "user": {"_id": "64548f6c363bb3aaf9cba136", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64548f6c363bb3aaf9cba136/HqJL9HQ5CWVOJCsHyuMOm.jpeg", "isPro": false, "fullname": "Shuai Yang", "user": "ShuaiYang03", "type": "user"}, "name": "Shuai Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:14:06.626Z", "hidden": false}, {"_id": "697989f6df44b75fa47e4807", "name": "Shi Liu", "hidden": false}, {"_id": "697989f6df44b75fa47e4808", "name": "Fangjing Wang", "hidden": false}, {"_id": "697989f6df44b75fa47e4809", "name": "Qian Zhu", "hidden": false}, {"_id": "697989f6df44b75fa47e480a", "user": {"_id": "68234b1167281062097fc34e", "avatarUrl": "/avatars/20ab2a15f27035c8b23d9a123e5ce22c.svg", "isPro": false, "fullname": "HeSun", "user": "he777771", "type": "user"}, "name": "He Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:14:08.792Z", "hidden": false}, {"_id": "697989f6df44b75fa47e480b", "name": "Yong Wang", "hidden": false}, {"_id": "697989f6df44b75fa47e480c", "name": "Shuailei Ma", "hidden": false}, {"_id": "697989f6df44b75fa47e480d", "name": "Yiyu Ren", "hidden": false}, {"_id": "697989f6df44b75fa47e480e", "name": "Kejia Zhang", "hidden": false}, {"_id": "697989f6df44b75fa47e480f", "name": "Hui Yu", "hidden": false}, {"_id": "697989f6df44b75fa47e4810", "name": "Jingmei Zhao", "hidden": false}, {"_id": "697989f6df44b75fa47e4811", "name": "Shuai Zhou", "hidden": false}, {"_id": "697989f6df44b75fa47e4812", "name": "Zhenqi Qiu", "hidden": false}, {"_id": "697989f6df44b75fa47e4813", "name": "Houlong Xiong", "hidden": false}, {"_id": "697989f6df44b75fa47e4814", "name": "Ziyu Wang", "hidden": false}, {"_id": "697989f6df44b75fa47e4815", "name": "Zechen Wang", "hidden": false}, {"_id": "697989f6df44b75fa47e4816", "name": "Ran Cheng", "hidden": false}, {"_id": "697989f6df44b75fa47e4817", "name": "Yong-Lu Li", "hidden": false}, {"_id": "697989f6df44b75fa47e4818", "name": "Yongtao Huang", "hidden": false}, {"_id": "697989f6df44b75fa47e4819", "name": "Xing Zhu", "hidden": false}, {"_id": "697989f6df44b75fa47e481a", "name": "Yujun Shen", "hidden": false}, {"_id": "697989f6df44b75fa47e481b", "name": "Kecheng Zheng", "hidden": false}], "publishedAt": "2026-01-26T17:08:04.000Z", "submittedOnDailyAt": "2026-01-28T03:13:43.716Z", "title": "A Pragmatic VLA Foundation Model", "submittedOnDailyBy": {"_id": "64252045a4f3051f54dd1d53", "avatarUrl": "/avatars/0e423a3291091be3b4736a14da3ce495.svg", "isPro": false, "fullname": "kecheng zheng", "user": "zkcys001", "type": "user"}, "summary": "Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8times (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.", "upvotes": 26, "discussionId": "697989f7df44b75fa47e481c", "projectPage": "https://technology.robbyant.com/lingbot-vla", "githubRepo": "https://github.com/robbyant/lingbot-vla", "githubRepoAddedBy": "auto", "ai_summary": "A Vision-Language-Action model trained on extensive real-world robotic data demonstrates superior performance and generalization across multiple platforms while offering enhanced efficiency through optimized training infrastructure.", "ai_keywords": ["Vision-Language-Action", "real-world data", "robotic manipulation", "generalization", "efficient codebase", "throughput", "VLA-oriented codebases"], "githubStars": 245, "organization": {"_id": "69709f892cd08371c1011a2e", "name": "robbyant", "fullname": "Robbyant", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86LingBot-VLA\u6a21\u578b\uff0c\u4f7f\u7528\u4e86\u6765\u81ea9\u79cd\u6d41\u884c\u53cc\u81c2\u673a\u5668\u4eba\u914d\u7f6e\u7684\u7ea620,000\u5c0f\u65f6\u771f\u5b9e\u6570\u636e\u3002</li>\n    <li>\u6a21\u578b\u57283\u4e2a\u5e73\u53f0\u4e0a\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u6bcf\u4e2a\u5e73\u53f0\u5b8c\u6210100\u4e2a\u4efb\u52a1\uff0c\u663e\u793a\u51fa\u6bd4\u7ade\u4e89\u5bf9\u624b\u66f4\u5f3a\u7684\u6027\u80fd\u548c\u5e7f\u6cdb\u7684\u9002\u5e94\u6027\u3002</li>\n    <li>\u6211\u4eec\u7684\u4ee3\u7801\u5e93\u6548\u7387\u9ad8\uff0c\u6bcf\u4e2aGPU\u6bcf\u79d2\u80fd\u5904\u7406261\u4e2a\u6837\u672c\uff0c\u901f\u5ea6\u6bd4\u73b0\u6709\u7684VLA\u4ee3\u7801\u5e93\u63d0\u9ad8\u4e861.5\u52302.8\u500d\u3002</li>\n    <li>\u8be5\u6a21\u578b\u9002\u5408\u4e8e\u5b9e\u9645\u5e94\u7528\uff0c\u5e76\u63d0\u4f9b\u5f00\u653e\u8bbf\u95ee\u7684\u4ee3\u7801\u3001\u57fa\u7840\u6a21\u578b\u548c\u57fa\u51c6\u6570\u636e\u3002</li>\n    <li>\u6211\u4eec\u65e8\u5728\u63a8\u52a8\u673a\u5668\u4eba\u5b66\u4e60\u9886\u57df\u7684\u53d1\u5c55\uff0c\u9f13\u52b1\u66f4\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u548c\u826f\u597d\u7684\u8bc4\u4f30\u6807\u51c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The LingBot-VLA is a new model for robotic manipulation, trained on 20,000 hours of real-world data from 9 different dual-arm robot setups.</li>\n    <li>It outperforms other models in tests across three robotic platforms, completing 100 tasks with high efficiency.</li>\n    <li>The codebase allows for fast processing, achieving 261 samples per second with an 8-GPU setup, which is significantly faster than other models.</li>\n    <li>The model is designed for practical use in real-world applications and is open for public access, including code and training data.</li>\n    <li>The goal is to enhance robot learning and support the development of more complex tasks and better evaluation methods.</li>\n</ul>"}, "publishedAt": "2026-01-26T12:08:04.000Z", "title": "A Pragmatic VLA Foundation Model", "summary": "Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8times (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18692.png", "numComments": 2, "submittedBy": {"_id": "64252045a4f3051f54dd1d53", "avatarUrl": "/avatars/0e423a3291091be3b4736a14da3ce495.svg", "fullname": "kecheng zheng", "name": "zkcys001", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "69709f892cd08371c1011a2e", "name": "robbyant", "fullname": "Robbyant", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.19834", "authors": [{"_id": "69797e24df44b75fa47e4761", "user": {"_id": "643b866bff50448bcfc7d1d1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg", "isPro": false, "fullname": "Jialong Wu", "user": "manchery", "type": "user"}, "name": "Jialong Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:16:37.217Z", "hidden": false}, {"_id": "69797e24df44b75fa47e4762", "name": "Xiaoying Zhang", "hidden": false}, {"_id": "69797e24df44b75fa47e4763", "name": "Hongyi Yuan", "hidden": false}, {"_id": "69797e24df44b75fa47e4764", "name": "Xiangcheng Zhang", "hidden": false}, {"_id": "69797e24df44b75fa47e4765", "name": "Tianhao Huang", "hidden": false}, {"_id": "69797e24df44b75fa47e4766", "name": "Changjing He", "hidden": false}, {"_id": "69797e24df44b75fa47e4767", "name": "Chaoyi Deng", "hidden": false}, {"_id": "69797e24df44b75fa47e4768", "name": "Renrui Zhang", "hidden": false}, {"_id": "69797e24df44b75fa47e4769", "name": "Youbin Wu", "hidden": false}, {"_id": "69797e24df44b75fa47e476a", "name": "Mingsheng Long", "hidden": false}], "publishedAt": "2026-01-27T17:40:07.000Z", "submittedOnDailyAt": "2026-01-28T00:46:44.173Z", "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models", "submittedOnDailyBy": {"_id": "643b866bff50448bcfc7d1d1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg", "isPro": false, "fullname": "Jialong Wu", "user": "manchery", "type": "user"}, "summary": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.", "upvotes": 19, "discussionId": "69797e24df44b75fa47e476b", "projectPage": "https://thuml.github.io/Reasoning-Visual-World/", "githubRepo": "https://github.com/thuml/Reasoning-Visual-World", "githubRepoAddedBy": "user", "ai_summary": "Visual generation enhances reasoning capabilities in multimodal models by providing more natural world models for physical and spatial tasks, while verbal reasoning remains sufficient for abstract domains.", "ai_keywords": ["chain-of-thought reasoning", "large language models", "multimodal models", "visual generation", "world models", "visual superiority hypothesis", "internal world modeling", "interleaved reasoning", "VisWorld-Eval"], "githubStars": 36, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u4eba\u7c7b\u901a\u8fc7\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\u6765\u8fdb\u884c\u63a8\u7406\uff0cAI\u7684\u8fdb\u5c55\u4f7f\u5f97\u67d0\u4e9b\u7cfb\u7edf\u53ef\u4ee5\u6a21\u62df\u8fd9\u79cd\u80fd\u529b\u3002</li>\n    <li>\u5f53\u524d\u7684AI\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7269\u7406\u548c\u7a7a\u95f4\u667a\u80fd\u65b9\u9762\u4ecd\u7136\u843d\u540e\u4e8e\u4eba\u7c7b\u3002</li>\n    <li>\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6a21\u578b\uff08UMMs\uff09\u80fd\u591f\u540c\u65f6\u8fdb\u884c\u8bed\u8a00\u548c\u89c6\u89c9\u751f\u6210\uff0c\u53ef\u80fd\u4f1a\u4fc3\u8fdb\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u63a8\u7406\u3002</li>\n    <li>\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u89c6\u89c9\u4f18\u8d8a\u5047\u8bbe\uff1a\u5728\u67d0\u4e9b\u57fa\u4e8e\u7269\u7406\u4e16\u754c\u7684\u4efb\u52a1\u4e2d\uff0c\u89c6\u89c9\u751f\u6210\u66f4\u9002\u5408\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u3002</li>\n    <li>\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u7684\u63a8\u7406\u65b9\u6cd5\u5728\u9700\u8981\u89c6\u89c9\u6a21\u578b\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans create mental models to understand the world and reason through them, similar to recent advancements in AI.</li>\n    <li>AI has made progress in areas like math and programming using verbal reasoning but struggles with physical and spatial tasks.</li>\n    <li>The study explores how visual generation can enhance reasoning, especially for tasks related to the physical world.</li>\n    <li>It introduces the \"visual superiority hypothesis,\" suggesting that visual models can be more effective than verbal ones in certain contexts.</li>\n    <li>Experiments show that combining visual and verbal reasoning outperforms purely verbal reasoning in tasks that benefit from visual understanding.</li>\n</ul>"}, "publishedAt": "2026-01-27T12:40:07.000Z", "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models", "summary": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19834.png", "numComments": 3, "submittedBy": {"_id": "643b866bff50448bcfc7d1d1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/q12v-BatVKimoi8q-coi-.jpeg", "fullname": "Jialong Wu", "name": "manchery", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.17645", "authors": [{"_id": "6978296a026bdf0473116dc0", "name": "Xilin Jiang", "hidden": false}, {"_id": "6978296a026bdf0473116dc1", "user": {"_id": "68fc33d3705bf8aa76ef5215", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8kks5zJgOMoU2GuXUgRZq.jpeg", "isPro": true, "fullname": "Qiaolin Wang", "user": "QiaolinWang", "type": "user"}, "name": "Qiaolin Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:31:22.668Z", "hidden": false}, {"_id": "6978296a026bdf0473116dc2", "user": {"_id": "665867a5d258e0fe41dcfb7a", "avatarUrl": "/avatars/dfc23ec27c1e5177f44b2c881e12d458.svg", "isPro": false, "fullname": "Junkai Wu", "user": "wjk0925", "type": "user"}, "name": "Junkai Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:42:46.216Z", "hidden": false}, {"_id": "6978296a026bdf0473116dc3", "name": "Xiaomin He", "hidden": false}, {"_id": "6978296a026bdf0473116dc4", "name": "Zhongweiyang Xu", "hidden": false}, {"_id": "6978296a026bdf0473116dc5", "name": "Yinghao Ma", "hidden": false}, {"_id": "6978296a026bdf0473116dc6", "name": "Minshuo Piao", "hidden": false}, {"_id": "6978296a026bdf0473116dc7", "name": "Kaiyi Yang", "hidden": false}, {"_id": "6978296a026bdf0473116dc8", "name": "Xiuwen Zheng", "hidden": false}, {"_id": "6978296a026bdf0473116dc9", "name": "Riki Shimizu", "hidden": false}, {"_id": "6978296a026bdf0473116dca", "name": "Yicong Chen", "hidden": false}, {"_id": "6978296a026bdf0473116dcb", "name": "Arsalan Firoozi", "hidden": false}, {"_id": "6978296a026bdf0473116dcc", "name": "Gavin Mischler", "hidden": false}, {"_id": "6978296a026bdf0473116dcd", "name": "Sukru Samet Dindar", "hidden": false}, {"_id": "6978296a026bdf0473116dce", "name": "Richard Antonello", "hidden": false}, {"_id": "6978296a026bdf0473116dcf", "name": "Linyang He", "hidden": false}, {"_id": "6978296a026bdf0473116dd0", "name": "Tsun-An Hsieh", "hidden": false}, {"_id": "6978296a026bdf0473116dd1", "name": "Xulin Fan", "hidden": false}, {"_id": "6978296a026bdf0473116dd2", "name": "Yulun Wu", "hidden": false}, {"_id": "6978296a026bdf0473116dd3", "name": "Yuesheng Ma", "hidden": false}, {"_id": "6978296a026bdf0473116dd4", "name": "Chaitanya Amballa", "hidden": false}, {"_id": "6978296a026bdf0473116dd5", "name": "Weixiong Chen", "hidden": false}, {"_id": "6978296a026bdf0473116dd6", "name": "Jiarui Hai", "hidden": false}, {"_id": "6978296a026bdf0473116dd7", "name": "Ruisi Li", "hidden": false}, {"_id": "6978296a026bdf0473116dd8", "name": "Vishal Choudhari", "hidden": false}, {"_id": "6978296a026bdf0473116dd9", "name": "Cong Han", "hidden": false}, {"_id": "6978296a026bdf0473116dda", "name": "Yinghao Aaron Li", "hidden": false}, {"_id": "6978296a026bdf0473116ddb", "name": "Adeen Flinker", "hidden": false}, {"_id": "6978296a026bdf0473116ddc", "name": "Mounya Elhilali", "hidden": false}, {"_id": "6978296a026bdf0473116ddd", "name": "Emmanouil Benetos", "hidden": false}, {"_id": "6978296a026bdf0473116dde", "name": "Mark Hasegawa-Johnson", "hidden": false}, {"_id": "6978296a026bdf0473116ddf", "name": "Romit Roy Choudhury", "hidden": false}, {"_id": "6978296a026bdf0473116de0", "name": "Nima Mesgarani", "hidden": false}], "publishedAt": "2026-01-25T01:40:15.000Z", "submittedOnDailyAt": "2026-01-28T00:44:43.594Z", "title": "AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking", "submittedOnDailyBy": {"_id": "6531a65daed617662c7f1007", "avatarUrl": "/avatars/ea2e504780dc40719f7501ab2c7d9c91.svg", "isPro": false, "fullname": "Xilin Jiang", "user": "xi-j", "type": "user"}, "summary": "Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public", "upvotes": 19, "discussionId": "6978296b026bdf0473116de1", "projectPage": "https://avmemeexam.github.io/public", "ai_summary": "Current multimodal models demonstrate limited understanding of cultural and contextual audio-visual content, particularly excelling only in surface-level analysis rather than deeper semantic comprehension.", "ai_keywords": ["multimodal large language models", "audio-visual clips", "human-curated benchmark", "iconic Internet sounds", "cultural context", "surface content", "semantic comprehension"], "organization": {"_id": "63f68badb607296857bb2441", "name": "columbia", "fullname": "Columbia University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/USuDBWwfOGNrQ0SZGFxDF.png"}, "summary_zh": "<ul>\n    <li>AVMeme Exam\u662f\u4e00\u4e2a\u5305\u542b\u8d85\u8fc7\u4e00\u5343\u4e2a\u4e92\u8054\u7f51\u58f0\u97f3\u548c\u89c6\u9891\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30AI\u6a21\u578b\u5728\u7406\u89e3\u4eba\u7c7b\u6587\u5316\u4fe1\u53f7\u65b9\u9762\u7684\u80fd\u529b\u3002</li>\n    <li>\u6bcf\u4e2a\u97f3\u6548\u6216\u89c6\u9891\u90fd\u6709\u72ec\u7279\u7684\u95ee\u7b54\uff0c\u8bc4\u4f30\u7406\u89e3\u7a0b\u5ea6\uff0c\u5305\u62ec\u8868\u9762\u5185\u5bb9\u3001\u4e0a\u4e0b\u6587\u3001\u60c5\u611f\u548c\u4f7f\u7528\u60c5\u51b5\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65e0\u6587\u672c\u7684\u97f3\u4e50\u548c\u97f3\u6548\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u7406\u89e3\u6587\u5316\u548c\u4e0a\u4e0b\u6587\u3002</li>\n    <li>\u8fd9\u4e9b\u7ed3\u679c\u63ed\u793a\u4e86\u4eba\u7c7b\u5bf9\u591a\u6a21\u6001\u667a\u80fd\u7684\u8ba4\u77e5\u5dee\u8ddd\uff0c\u547c\u5401\u5f00\u53d1\u80fd\u591f\u66f4\u597d\u7406\u89e3\u4e0a\u4e0b\u6587\u548c\u6587\u5316\u7684\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study introduces AVMeme Exam, a benchmark with over 1,000 Internet audio-visual clips to test AI understanding.</li>\n    <li>Each clip is paired with questions that evaluate understanding of content, context, emotion, and knowledge.</li>\n    <li>State-of-the-art AI models were tested alongside humans using this benchmark.</li>\n    <li>Results show that AI models struggle with understanding music and sound effects, especially in cultural contexts.</li>\n    <li>The findings suggest a need for AI models to improve their ability to understand context and culture, not just surface content.</li>\n</ul>"}, "publishedAt": "2026-01-24T20:40:15.000Z", "title": "AVMeme Exam: A Multimodal Multilingual Multicultural Benchmark for LLMs' Contextual and Cultural Knowledge and Thinking", "summary": "Internet audio-visual clips convey meaning through time-varying sound and motion, which extend beyond what text alone can represent. To examine whether AI models can understand such signals in human cultural contexts, we introduce AVMeme Exam, a human-curated benchmark of over one thousand iconic Internet sounds and videos spanning speech, songs, music, and sound effects. Each meme is paired with a unique Q&A assessing levels of understanding from surface content to context and emotion to usage and world knowledge, along with metadata such as original year, transcript, summary, and sensitivity. We systematically evaluate state-of-the-art multimodal large language models (MLLMs) alongside human participants using this benchmark. Our results reveal a consistent limitation: current models perform poorly on textless music and sound effects, and struggle to think in context and in culture compared to surface content. These findings highlight a key gap in human-aligned multimodal intelligence and call for models that can perceive contextually and culturally beyond the surface of what they hear and see. Project page: avmemeexam.github.io/public", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17645.png", "numComments": 2, "submittedBy": {"_id": "6531a65daed617662c7f1007", "avatarUrl": "/avatars/ea2e504780dc40719f7501ab2c7d9c91.svg", "fullname": "Xilin Jiang", "name": "xi-j", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "63f68badb607296857bb2441", "name": "columbia", "fullname": "Columbia University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/USuDBWwfOGNrQ0SZGFxDF.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.19798", "authors": [{"_id": "6979818fdf44b75fa47e477a", "name": "Zhixiang Wei", "hidden": false}, {"_id": "6979818fdf44b75fa47e477b", "name": "Yi Li", "hidden": false}, {"_id": "6979818fdf44b75fa47e477c", "name": "Zhehan Kan", "hidden": false}, {"_id": "6979818fdf44b75fa47e477d", "name": "Xinghua Jiang", "hidden": false}, {"_id": "6979818fdf44b75fa47e477e", "name": "Zuwei Long", "hidden": false}, {"_id": "6979818fdf44b75fa47e477f", "name": "Shifeng Liu", "hidden": false}, {"_id": "6979818fdf44b75fa47e4780", "name": "Hongze Shen", "hidden": false}, {"_id": "6979818fdf44b75fa47e4781", "name": "Wei Liu", "hidden": false}, {"_id": "6979818fdf44b75fa47e4782", "name": "Xiaoyu Tan", "hidden": false}, {"_id": "6979818fdf44b75fa47e4783", "name": "Haojia Lin", "hidden": false}, {"_id": "6979818fdf44b75fa47e4784", "name": "Yubo Zhu", "hidden": false}, {"_id": "6979818fdf44b75fa47e4785", "name": "Qianyu Li", "hidden": false}, {"_id": "6979818fdf44b75fa47e4786", "user": {"_id": "63fc75f9b9db84750cea9c5c", "avatarUrl": "/avatars/2c5bf9685e0cfc4b5785a4a86c34e0db.svg", "isPro": false, "fullname": "DI YIN", "user": "DIYIN", "type": "user"}, "name": "Di Yin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:16:25.761Z", "hidden": false}, {"_id": "6979818fdf44b75fa47e4787", "name": "Haoyu Cao", "hidden": false}, {"_id": "6979818fdf44b75fa47e4788", "name": "Weibo Gu", "hidden": false}, {"_id": "6979818fdf44b75fa47e4789", "user": {"_id": "667016b9ccff4d0862460d88", "avatarUrl": "/avatars/96959ed963af9eae2814079c5241af1c.svg", "isPro": false, "fullname": "Xin Li", "user": "fujikoli", "type": "user"}, "name": "Xin Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:28:49.228Z", "hidden": false}, {"_id": "6979818fdf44b75fa47e478a", "user": {"_id": "6959daf62da262f6caa6b3b9", "avatarUrl": "/avatars/3986907912cd2d502570b8969b02abcc.svg", "isPro": false, "fullname": "Yinsong Liu", "user": "Yinsongliu", "type": "user"}, "name": "Yinsong Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:16:33.643Z", "hidden": false}, {"_id": "6979818fdf44b75fa47e478b", "name": "Deqiang Jiang", "hidden": false}, {"_id": "6979818fdf44b75fa47e478c", "user": {"_id": "647401e50da364bd0d002f2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png", "isPro": false, "fullname": "XING SUN", "user": "tedsun", "type": "user"}, "name": "Xing Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:16:28.308Z", "hidden": false}, {"_id": "6979818fdf44b75fa47e478d", "name": "Yunsheng Wu", "hidden": false}, {"_id": "6979818fdf44b75fa47e478e", "name": "Mingkong Tang", "hidden": false}, {"_id": "6979818fdf44b75fa47e478f", "name": "Shuangyin Liu", "hidden": false}, {"_id": "6979818fdf44b75fa47e4790", "name": "Lexiang Tang", "hidden": false}, {"_id": "6979818fdf44b75fa47e4791", "name": "Haodong Lin", "hidden": false}, {"_id": "6979818fdf44b75fa47e4792", "user": {"_id": "64ddbc3f1f2dad27e1a05ac1", "avatarUrl": "/avatars/4fb2753b7998c8536bfd4780d3b10a6d.svg", "isPro": false, "fullname": "Junrulu", "user": "Junrulu", "type": "user"}, "name": "Junru Lu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:16:30.779Z", "hidden": false}, {"_id": "6979818fdf44b75fa47e4793", "name": "Jiarui Qin", "hidden": false}, {"_id": "6979818fdf44b75fa47e4794", "name": "Lingfeng Qiao", "hidden": false}, {"_id": "6979818fdf44b75fa47e4795", "name": "Ruizhi Qiao", "hidden": false}, {"_id": "6979818fdf44b75fa47e4796", "name": "Bo Ke", "hidden": false}, {"_id": "6979818fdf44b75fa47e4797", "name": "Jianfeng He", "hidden": false}, {"_id": "6979818fdf44b75fa47e4798", "name": "Ke Li", "hidden": false}, {"_id": "6979818fdf44b75fa47e4799", "name": "Yangning Li", "hidden": false}, {"_id": "6979818fdf44b75fa47e479a", "name": "Yunhang Shen", "hidden": false}, {"_id": "6979818fdf44b75fa47e479b", "name": "Mengdan Zhang", "hidden": false}, {"_id": "6979818fdf44b75fa47e479c", "name": "Peixian Chen", "hidden": false}, {"_id": "6979818fdf44b75fa47e479d", "name": "Kun Yin", "hidden": false}, {"_id": "6979818fdf44b75fa47e479e", "name": "Bing Liu", "hidden": false}, {"_id": "6979818fdf44b75fa47e479f", "name": "Yunfei Wu", "hidden": false}, {"_id": "6979818fdf44b75fa47e47a0", "name": "Huang Chen", "hidden": false}, {"_id": "6979818fdf44b75fa47e47a1", "name": "Zhongpeng Cai", "hidden": false}, {"_id": "6979818fdf44b75fa47e47a2", "name": "Xiaotian Li", "hidden": false}], "publishedAt": "2026-01-27T17:01:16.000Z", "submittedOnDailyAt": "2026-01-28T19:57:05.016Z", "title": "Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision", "submittedOnDailyBy": {"_id": "610a70f35a40a8bfebfbf09b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659922312540-610a70f35a40a8bfebfbf09b.jpeg", "isPro": true, "fullname": "Daniel Bourke", "user": "mrdbourke", "type": "user"}, "summary": "Despite the significant advancements represented by Vision-Language Models (VLMs), current architectures often exhibit limitations in retaining fine-grained visual information, leading to coarse-grained multimodal comprehension. We attribute this deficiency to a suboptimal training paradigm inherent in prevailing VLMs, which exhibits a text-dominant optimization bias by conceptualizing visual signals merely as passive conditional inputs rather than supervisory targets. To mitigate this, we introduce Youtu-VL, a framework leveraging the Vision-Language Unified Autoregressive Supervision (VLUAS) paradigm, which fundamentally shifts the optimization objective from ``vision-as-input'' to ``vision-as-target.'' By integrating visual tokens directly into the prediction stream, Youtu-VL applies unified autoregressive supervision to both visual details and linguistic content. Furthermore, we extend this paradigm to encompass vision-centric tasks, enabling a standard VLM to perform vision-centric tasks without task-specific additions. Extensive empirical evaluations demonstrate that Youtu-VL achieves competitive performance on both general multimodal tasks and vision-centric tasks, establishing a robust foundation for the development of comprehensive generalist visual agents.", "upvotes": 16, "discussionId": "69798190df44b75fa47e47a3", "projectPage": "https://youtu-tip.com/#llm", "githubRepo": "https://github.com/TencentCloudADP/youtu-vl", "githubRepoAddedBy": "user", "ai_summary": "Youtu-VL addresses limitations in Vision-Language Models by introducing a unified autoregressive supervision paradigm that treats visual signals as target outputs rather than passive inputs, enabling improved multimodal comprehension and vision-centric task performance.", "ai_keywords": ["Vision-Language Models", "autoregressive supervision", "vision-as-target", "vision-as-input", "visual tokens", "multimodal comprehension", "vision-centric tasks", "generalist visual agents"], "githubStars": 35, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u6709\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u4fdd\u7559\u7ec6\u81f4\u7684\u89c6\u89c9\u4fe1\u606f\u65b9\u9762\u4ecd\u5b58\u5728\u9650\u5236\u3002</li>\n    <li>\u8fd9\u79cd\u7f3a\u9677\u6e90\u4e8e\u73b0\u6709VLMs\u7684\u8bad\u7ec3\u65b9\u5f0f\uff0c\u8fc7\u4e8e\u4f9d\u8d56\u6587\u672c\uff0c\u672a\u80fd\u5c06\u89c6\u89c9\u4fe1\u53f7\u89c6\u4e3a\u91cd\u8981\u76ee\u6807\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Youtu-VL\u6846\u67b6\uff0c\u6539\u53d8\u4f18\u5316\u76ee\u6807\uff0c\u4ece\u201c\u89c6\u89c9\u4f5c\u4e3a\u8f93\u5165\u201d\u8f6c\u53d8\u4e3a\u201c\u89c6\u89c9\u4f5c\u4e3a\u76ee\u6807\u201d\u3002</li>\n    <li>Youtu-VL\u901a\u8fc7\u5c06\u89c6\u89c9\u4fe1\u606f\u76f4\u63a5\u6574\u5408\u5230\u9884\u6d4b\u4e2d\uff0c\u91c7\u7528\u7edf\u4e00\u7684\u81ea\u56de\u5f52\u76d1\u7763\u6765\u5904\u7406\u89c6\u89c9\u548c\u8bed\u8a00\u5185\u5bb9\u3002</li>\n    <li>\u8be5\u6846\u67b6\u80fd\u591f\u4f7f\u6807\u51c6VLM\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u65e0\u9700\u7279\u5b9a\u7684\u4efb\u52a1\u6dfb\u52a0\uff0c\u53d6\u5f97\u4e86\u7ade\u4e89\u529b\u7684\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current Vision-Language Models (VLMs) struggle to retain detailed visual information, leading to poor understanding of multimodal content.</li>\n    <li>This problem is due to a training method that focuses more on text and treats visual cues as secondary inputs.</li>\n    <li>Youtu-VL is introduced as a new framework that changes this approach by treating visual information as primary targets for training.</li>\n    <li>The framework combines visual and linguistic information more effectively, improving performance on various tasks.</li>\n    <li>Youtu-VL shows strong results in both general multimodal and vision-focused tasks, paving the way for better visual agents.</li>\n</ul>"}, "publishedAt": "2026-01-27T12:01:16.000Z", "title": "Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision", "summary": "Despite the significant advancements represented by Vision-Language Models (VLMs), current architectures often exhibit limitations in retaining fine-grained visual information, leading to coarse-grained multimodal comprehension. We attribute this deficiency to a suboptimal training paradigm inherent in prevailing VLMs, which exhibits a text-dominant optimization bias by conceptualizing visual signals merely as passive conditional inputs rather than supervisory targets. To mitigate this, we introduce Youtu-VL, a framework leveraging the Vision-Language Unified Autoregressive Supervision (VLUAS) paradigm, which fundamentally shifts the optimization objective from ``vision-as-input'' to ``vision-as-target.'' By integrating visual tokens directly into the prediction stream, Youtu-VL applies unified autoregressive supervision to both visual details and linguistic content. Furthermore, we extend this paradigm to encompass vision-centric tasks, enabling a standard VLM to perform vision-centric tasks without task-specific additions. Extensive empirical evaluations demonstrate that Youtu-VL achieves competitive performance on both general multimodal tasks and vision-centric tasks, establishing a robust foundation for the development of comprehensive generalist visual agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19798.png", "numComments": 1, "submittedBy": {"_id": "610a70f35a40a8bfebfbf09b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659922312540-610a70f35a40a8bfebfbf09b.jpeg", "fullname": "Daniel Bourke", "name": "mrdbourke", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 192, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09150", "authors": [{"_id": "6979a32cdf44b75fa47e4831", "name": "Jianwen Sun", "hidden": false}, {"_id": "6979a32cdf44b75fa47e4832", "name": "Yukang Feng", "hidden": false}, {"_id": "6979a32cdf44b75fa47e4833", "name": "Kaining Ying", "hidden": false}, {"_id": "6979a32cdf44b75fa47e4834", "name": "Chuanhao Li", "hidden": false}, {"_id": "6979a32cdf44b75fa47e4835", "name": "Zizhen Li", "hidden": false}, {"_id": "6979a32cdf44b75fa47e4836", "name": "Fanrui Zhang", "hidden": false}, {"_id": "6979a32cdf44b75fa47e4837", "name": "Jiaxin Ai", "hidden": false}, {"_id": "6979a32cdf44b75fa47e4838", "name": "Yifan Chang", "hidden": false}, {"_id": "6979a32cdf44b75fa47e4839", "name": "Yu Dai", "hidden": false}, {"_id": "6979a32cdf44b75fa47e483a", "name": "Yifei Huang", "hidden": false}, {"_id": "6979a32cdf44b75fa47e483b", "name": "Kaipeng Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/dWe8NkE2o-SBkMqd897MA.mp4"], "publishedAt": "2026-01-14T04:45:05.000Z", "submittedOnDailyAt": "2026-01-28T03:30:08.387Z", "title": "World Craft: Agentic Framework to Create Visualizable Worlds via Text", "submittedOnDailyBy": {"_id": "65f1713552c38a91e0a445e8", "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg", "isPro": false, "fullname": "kaipeng", "user": "kpzhang996", "type": "user"}, "summary": "Large Language Models (LLMs) motivate generative agent simulation (e.g., AI Town) to create a ``dynamic world'', holding immense value across entertainment and research. However, for non-experts, especially those without programming skills, it isn't easy to customize a visualizable environment by themselves. In this paper, we introduce World Craft, an agentic world creation framework to create an executable and visualizable AI Town via user textual descriptions. It consists of two main modules, World Scaffold and World Guild. World Scaffold is a structured and concise standardization to develop interactive game scenes, serving as an efficient scaffolding for LLMs to customize an executable AI Town-like environment. World Guild is a multi-agent framework to progressively analyze users' intents from rough descriptions, and synthesizes required structured contents (\\eg environment layout and assets) for World Scaffold . Moreover, we construct a high-quality error-correction dataset via reverse engineering to enhance spatial knowledge and improve the stability and controllability of layout generation, while reporting multi-dimensional evaluation metrics for further analysis. Extensive experiments demonstrate that our framework significantly outperforms existing commercial code agents (Cursor and Antigravity) and LLMs (Qwen3 and Gemini-3-Pro). in scene construction and narrative intent conveyance, providing a scalable solution for the democratization of environment creation.", "upvotes": 15, "discussionId": "6979a32cdf44b75fa47e483c", "githubRepo": "https://github.com/HerzogFL/World-Craft", "githubRepoAddedBy": "user", "ai_summary": "World Craft enables non-expert users to create executable and visualizable AI environments through textual descriptions by combining structured scaffolding and multi-agent intent analysis.", "ai_keywords": ["generative agent simulation", "AI Town", "large language models", "agentic world creation framework", "world scaffold", "world guild", "multi-agent framework", "scene construction", "narrative intent conveyance"], "githubStars": 40, "organization": {"_id": "689f08c50df4fcf7fddc0b08", "name": "ShandaAI", "fullname": "Shanda AI Research Tokyo", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6099290247dc3dbf8a976612/OV-XOpG-1Pf-yzhFdkQse.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6fc0\u52b1\u4e86\u751f\u6210\u4ee3\u7406\u6a21\u62df\uff08\u5982AI Town\uff09\uff0c\u521b\u9020\u52a8\u6001\u4e16\u754c\uff0c\u5177\u6709\u5f88\u9ad8\u7684\u5a31\u4e50\u548c\u7814\u7a76\u4ef7\u503c\u3002</li>\n    <li>\u975e\u4e13\u4e1a\u4eba\u58eb\uff0c\u5c24\u5176\u662f\u6ca1\u6709\u7f16\u7a0b\u6280\u80fd\u7684\u4eba\uff0c\u96be\u4ee5\u81ea\u884c\u5b9a\u5236\u53ef\u89c6\u5316\u73af\u5883\u3002</li>\n    <li>\u672c\u6587\u4ecb\u7ecd\u4e86World Craft\uff0c\u4e00\u4e2a\u901a\u8fc7\u7528\u6237\u6587\u672c\u63cf\u8ff0\u521b\u5efa\u53ef\u6267\u884c\u548c\u53ef\u89c6\u5316AI Town\u7684\u6846\u67b6\u3002</li>\n    <li>World Scaffold\u7528\u4e8e\u5f00\u53d1\u4e92\u52a8\u6e38\u620f\u573a\u666f\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u7684\u6807\u51c6\u5316\u652f\u6301\uff0c\u65b9\u4fbfLLMs\u5b9a\u5236\u73af\u5883\u3002</li>\n    <li>World Guild\u5206\u6790\u7528\u6237\u610f\u56fe\u5e76\u5408\u6210\u6240\u9700\u5185\u5bb9\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u4f18\u4e8e\u73b0\u6709\u5546\u4e1a\u4ee3\u7801\u4ee3\u7406\u548cLLMs\uff0c\u63a8\u52a8\u73af\u5883\u521b\u5efa\u7684\u6c11\u4e3b\u5316\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>World Craft helps non-experts create a visualizable AI Town using their own words, making it easier for everyone to participate in building interactive environments.</li>\n    <li>The framework has two main parts: World Scaffold, which organizes game scenes for customization, and World Guild, which understands user descriptions and creates necessary content.</li>\n    <li>It improves layout generation by using a special dataset designed to address errors and enhance spatial understanding.</li>\n    <li>Tests show that World Craft works much better than existing tools and models for creating scenes and expressing narrative ideas.</li>\n    <li>This framework aims to make environment creation accessible to more people, regardless of their technical skills.</li>\n</ul>"}, "publishedAt": "2026-01-13T23:45:05.000Z", "title": "World Craft: Agentic Framework to Create Visualizable Worlds via Text", "summary": "Large Language Models (LLMs) motivate generative agent simulation (e.g., AI Town) to create a ``dynamic world'', holding immense value across entertainment and research. However, for non-experts, especially those without programming skills, it isn't easy to customize a visualizable environment by themselves. In this paper, we introduce World Craft, an agentic world creation framework to create an executable and visualizable AI Town via user textual descriptions. It consists of two main modules, World Scaffold and World Guild. World Scaffold is a structured and concise standardization to develop interactive game scenes, serving as an efficient scaffolding for LLMs to customize an executable AI Town-like environment. World Guild is a multi-agent framework to progressively analyze users' intents from rough descriptions, and synthesizes required structured contents (\\eg environment layout and assets) for World Scaffold . Moreover, we construct a high-quality error-correction dataset via reverse engineering to enhance spatial knowledge and improve the stability and controllability of layout generation, while reporting multi-dimensional evaluation metrics for further analysis. Extensive experiments demonstrate that our framework significantly outperforms existing commercial code agents (Cursor and Antigravity) and LLMs (Qwen3 and Gemini-3-Pro). in scene construction and narrative intent conveyance, providing a scalable solution for the democratization of environment creation.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/dWe8NkE2o-SBkMqd897MA.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09150.png", "numComments": 2, "submittedBy": {"_id": "65f1713552c38a91e0a445e8", "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg", "fullname": "kaipeng", "name": "kpzhang996", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "organization": {"_id": "689f08c50df4fcf7fddc0b08", "name": "ShandaAI", "fullname": "Shanda AI Research Tokyo", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6099290247dc3dbf8a976612/OV-XOpG-1Pf-yzhFdkQse.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.18292", "authors": [{"_id": "6979d0aedf44b75fa47e48d5", "name": "Zhewen Tan", "hidden": false}, {"_id": "6979d0aedf44b75fa47e48d6", "name": "Wenhan Yu", "hidden": false}, {"_id": "6979d0aedf44b75fa47e48d7", "name": "Jianfeng Si", "hidden": false}, {"_id": "6979d0aedf44b75fa47e48d8", "name": "Tongxin Liu", "hidden": false}, {"_id": "6979d0aedf44b75fa47e48d9", "name": "Kaiqi Guan", "hidden": false}, {"_id": "6979d0aedf44b75fa47e48da", "name": "Huiyan Jin", "hidden": false}, {"_id": "6979d0aedf44b75fa47e48db", "name": "Jiawen Tao", "hidden": false}, {"_id": "6979d0aedf44b75fa47e48dc", "name": "Xiaokun Yuan", "hidden": false}, {"_id": "6979d0aedf44b75fa47e48dd", "name": "Duohe Ma", "hidden": false}, {"_id": "6979d0aedf44b75fa47e48de", "name": "Xiangzheng Zhang", "hidden": false}, {"_id": "6979d0aedf44b75fa47e48df", "name": "Tong Yang", "hidden": false}, {"_id": "6979d0aedf44b75fa47e48e0", "user": {"_id": "632c30576bcb864974cc40a8", "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg", "isPro": false, "fullname": "sunlin", "user": "lincharliesun", "type": "user"}, "name": "Lin Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:12:39.873Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/632c30576bcb864974cc40a8/hgTgbVzSd572bZlvwCB-N.png"], "publishedAt": "2026-01-26T09:21:43.000Z", "submittedOnDailyAt": "2026-01-28T06:53:04.070Z", "title": "TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment", "submittedOnDailyBy": {"_id": "632c30576bcb864974cc40a8", "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg", "isPro": false, "fullname": "sunlin", "user": "lincharliesun", "type": "user"}, "summary": "In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.", "upvotes": 9, "discussionId": "6979d0aedf44b75fa47e48e1", "ai_summary": "A closed-loop reinforcement learning framework enables iterative collaboration between attacker, defender, and evaluator roles for improved large language model safety alignment without manual annotations.", "ai_keywords": ["reinforcement learning", "large language models", "safety alignment", "adversarial prompt generation", "safety defense", "response assessment", "closed-loop framework", "co-improving collaboration", "iterative learning"], "organization": {"_id": "6606990280543d0b74d38438", "name": "qihoo360", "fullname": "\u5317\u4eac\u5947\u864e\u79d1\u6280\u6709\u9650\u516c\u53f8", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/660697054c0882ce77fe3160/fVsqY-Q0DDrme0zyN_zLa.png"}, "summary_zh": "<ul>\n    <li>\u8fd1\u5e74\u6765\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u98ce\u9669\u8d8a\u6765\u8d8a\u7a81\u51fa\uff0c\u9700\u8981\u51cf\u5c11\u6709\u5bb3\u5185\u5bb9\u7684\u751f\u6210\u3002</li>\n    <li>\u5f53\u524d\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u901a\u5e38\u5305\u62ec\u653b\u51fb\u8005\u3001\u4fdd\u62a4\u8005\u548c\u8bc4\u4f30\u8005\u4e09\u4e2a\u89d2\u8272\u7684\u5408\u4f5c\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTriPlay-RL\u7684\u95ed\u73af\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u8fd9\u4e09\u4e2a\u89d2\u8272\u7684\u8fed\u4ee3\u5408\u4f5c\uff0c\u51e0\u4e4e\u4e0d\u9700\u8981\u624b\u52a8\u6807\u6ce8\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u653b\u51fb\u8005\u80fd\u4fdd\u6301\u9ad8\u8f93\u51fa\u591a\u6837\u6027\uff0c\u540c\u65f6\u63d0\u9ad820%-50%\u7684\u653b\u51fb\u6548\u679c\uff1b\u4fdd\u62a4\u8005\u5728\u5b89\u5168\u6027\u80fd\u4e0a\u63d0\u534710%-30%\uff0c\u4e14\u4e0d\u5f71\u54cd\u63a8\u7406\u80fd\u529b\uff1b\u8bc4\u4f30\u8005\u901a\u8fc7\u8fed\u4ee3\u4e0d\u65ad\u63d0\u5347\u5224\u65ad\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6846\u67b6\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u5efa\u7acb\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u6a21\u5f0f\uff0c\u4fc3\u8fdb\u4e86\u5404\u89d2\u8272\u7684\u6301\u7eed\u5171\u540c\u8fdb\u5316\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is a growing concern about safety risks from large language models, especially regarding harmful content.</li>\n    <li>The common approach to improve safety involves three roles: an attacker, a defender, and an evaluator.</li>\n    <li>This paper introduces a new system called TriPlay-RL that allows these roles to work together without needing much manual input.</li>\n    <li>Results show that the attacker can create diverse and effective prompts, the defender can improve safety without losing reasoning skills, and the evaluator gets better at identifying unsafe content.</li>\n    <li>This framework provides an efficient way to enhance safety in language models through continuous collaboration and learning.</li>\n</ul>"}, "publishedAt": "2026-01-26T04:21:43.000Z", "title": "TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment", "summary": "In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/632c30576bcb864974cc40a8/hgTgbVzSd572bZlvwCB-N.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18292.png", "numComments": 2, "submittedBy": {"_id": "632c30576bcb864974cc40a8", "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg", "fullname": "sunlin", "name": "lincharliesun", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "6606990280543d0b74d38438", "name": "qihoo360", "fullname": "\u5317\u4eac\u5947\u864e\u79d1\u6280\u6709\u9650\u516c\u53f8", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/660697054c0882ce77fe3160/fVsqY-Q0DDrme0zyN_zLa.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.18116", "authors": [{"_id": "6979d078df44b75fa47e48cd", "user": {"_id": "632c30576bcb864974cc40a8", "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg", "isPro": false, "fullname": "sunlin", "user": "lincharliesun", "type": "user"}, "name": "Lin Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:12:43.095Z", "hidden": false}, {"_id": "6979d078df44b75fa47e48ce", "name": "Linglin Zhang", "hidden": false}, {"_id": "6979d078df44b75fa47e48cf", "name": "Jingang Huang", "hidden": false}, {"_id": "6979d078df44b75fa47e48d0", "name": "Change Jia", "hidden": false}, {"_id": "6979d078df44b75fa47e48d1", "name": "Zhengwei Cheng", "hidden": false}, {"_id": "6979d078df44b75fa47e48d2", "name": "Xiangzheng Zhang", "hidden": false}], "publishedAt": "2026-01-26T04:00:56.000Z", "submittedOnDailyAt": "2026-01-28T06:55:03.102Z", "title": "FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning", "submittedOnDailyBy": {"_id": "632c30576bcb864974cc40a8", "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg", "isPro": false, "fullname": "sunlin", "user": "lincharliesun", "type": "user"}, "summary": "The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis.\n  We present FABLE, a Forest-based Adaptive Bi-path LLM-Enhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs.\n  Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.", "upvotes": 9, "discussionId": "6979d078df44b75fa47e48d3", "ai_summary": "FABLE is a forest-based adaptive bi-path retrieval framework that enhances LLM-based information retrieval through hierarchical indexing and structured evidence acquisition, achieving superior performance with reduced token usage compared to traditional RAG methods.", "ai_keywords": ["Retrieval-Augmented Generation", "long-context Large Language Models", "hierarchical forest indexes", "bi-path strategy", "LLM-guided hierarchical traversal", "structure-aware propagation", "multi-granularity semantic structures", "token reduction"], "organization": {"_id": "6606990280543d0b74d38438", "name": "qihoo360", "fullname": "\u5317\u4eac\u5947\u864e\u79d1\u6280\u6709\u9650\u516c\u53f8", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/660697054c0882ce77fe3160/fVsqY-Q0DDrme0zyN_zLa.png"}, "summary_zh": "<ul>\n    <li>\u957f\u6587\u672c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u53d1\u5c55\u5f15\u53d1\u4e86\u5173\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u662f\u5426\u4ecd\u7136\u5fc5\u8981\u7684\u8ba8\u8bba\u3002</li>\n    <li>\u5f53\u524d\u7684\u957f\u6587\u672c\u63a8\u7406\u5b58\u5728\u5c40\u9650\uff0c\u5305\u62ec\u4fe1\u606f\u4e22\u5931\u3001\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u591a\u6587\u6863\u63a8\u7406\u7684\u53ef\u6269\u5c55\u6027\u5dee\u3002</li>\n    <li>\u4f20\u7edf\u7684RAG\u7cfb\u7edf\u5728\u68c0\u7d22\u65f6\u6548\u7387\u9ad8\uff0c\u4f46\u5b58\u5728\u8bed\u4e49\u566a\u58f0\u548c\u65e0\u6cd5\u652f\u6301\u7ed3\u6784\u5316\u8de8\u6587\u6863\u7efc\u5408\u7684\u95ee\u9898\u3002</li>\n    <li>FABLE\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86LLMs\u8fdb\u884c\u77e5\u8bc6\u7ec4\u7ec7\u548c\u68c0\u7d22\uff0c\u901a\u8fc7\u5efa\u7acb\u5c42\u6b21\u7ed3\u6784\u7684\u68ee\u6797\u7d22\u5f15\u6765\u63d0\u5347\u68c0\u7d22\u6548\u7387\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cFABLE\u5728\u6027\u80fd\u4e0a\u8d85\u8fc7\u4e86\u73b0\u6709\u7684RAG\u65b9\u6cd5\uff0c\u5e76\u5728\u51cf\u5c1194%\u7684\u6807\u8bb0\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u8fbe\u5230\u4e86\u4e0e\u5b8c\u6574\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Long-context Large Language Models (LLMs) have limitations, such as high costs and challenges in reasoning across multiple documents.</li>\n    <li>Traditional Retrieval-Augmented Generation (RAG) systems are efficient but struggle with retrieving relevant information accurately.</li>\n    <li>FABLE is a new framework that combines LLMs with better knowledge organization and retrieval methods.</li>\n    <li>FABLE uses a unique strategy to gather information more effectively while controlling for efficiency.</li>\n    <li>FABLE outperforms current RAG methods and achieves similar accuracy to long-context LLMs while significantly reducing resource use.</li>\n</ul>"}, "publishedAt": "2026-01-25T23:00:56.000Z", "title": "FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning", "summary": "The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis.\n  We present FABLE, a Forest-based Adaptive Bi-path LLM-Enhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs.\n  Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18116.png", "numComments": 2, "submittedBy": {"_id": "632c30576bcb864974cc40a8", "avatarUrl": "/avatars/96aa948ad1dd35d355e20b5765a2563a.svg", "fullname": "sunlin", "name": "lincharliesun", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "6606990280543d0b74d38438", "name": "qihoo360", "fullname": "\u5317\u4eac\u5947\u864e\u79d1\u6280\u6709\u9650\u516c\u53f8", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/660697054c0882ce77fe3160/fVsqY-Q0DDrme0zyN_zLa.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.19895", "authors": [{"_id": "697990bddf44b75fa47e4827", "name": "Chen Chen", "hidden": false}, {"_id": "697990bddf44b75fa47e4828", "user": {"_id": "69798a0962e9590654013cab", "avatarUrl": "/avatars/b60e3d096a463b55cd3b98677ae568d7.svg", "isPro": false, "fullname": "Lai Wei", "user": "laiwei-seed", "type": "user"}, "name": "Lai Wei", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:14:04.627Z", "hidden": false}], "publishedAt": "2026-01-27T18:58:46.000Z", "submittedOnDailyAt": "2026-01-28T08:48:49.669Z", "title": "Post-LayerNorm Is Back: Stable, ExpressivE, and Deep", "submittedOnDailyBy": {"_id": "69798a0962e9590654013cab", "avatarUrl": "/avatars/b60e3d096a463b55cd3b98677ae568d7.svg", "isPro": false, "fullname": "Lai Wei", "user": "laiwei-seed", "type": "user"}, "summary": "Large language model (LLM) scaling is hitting a wall. Widening models yields diminishing returns, and extending context length does not improve fundamental expressivity. In contrast, depth scaling offers theoretically superior expressivity, yet current Transformer architectures struggle to train reliably at extreme depths. We revisit the Post-LayerNorm (Post-LN) formulation, whose instability at scale caused its replacement by Pre-LN in modern LLMs. We show that the central failure mode of Post-LN arises from the ResNet-style residual pathway, which introduces gradient vanishing in deep networks. We present Keel, a Post-LN Transformer that replaces this residual path with a Highway-style connection. This modification preserves the gradient flow through the residual branch, preventing signal vanishing from the top layers to the bottom. Unlike prior methods, Keel enables stable training at extreme depths without requiring specialized initialization or complex optimization tricks. Keel trains robustly at depths exceeding 1000 layers and consistently improves perplexity and depth-scaling characteristics over Pre-LN. These findings indicate that Post-LN, when paired with a Highway-style connection, provides a simple and effective foundation for building deeply scalable LLMs, opening the possibility for future infinite-depth architectures.", "upvotes": 8, "discussionId": "697990bedf44b75fa47e4829", "ai_summary": "A novel Post-LayerNorm Transformer architecture called Keel addresses training instability in extremely deep networks by replacing residual connections with Highway-style connections, enabling stable training beyond 1000 layers.", "ai_keywords": ["Post-LayerNorm", "Pre-LayerNorm", "Transformer architectures", "residual pathways", "gradient vanishing", "Highway-style connections", "deep learning", "neural network training", "depth scaling", "perplexity"], "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6269\u5c55\u9047\u5230\u4e86\u74f6\u9888\uff0c\u6269\u5c55\u6a21\u578b\u5e76\u6ca1\u6709\u5e26\u6765\u9884\u671f\u7684\u6536\u76ca\u3002</li>\n    <li>\u589e\u52a0\u6a21\u578b\u7684\u6df1\u5ea6\u7406\u8bba\u4e0a\u53ef\u4ee5\u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\uff0c\u4f46\u5f53\u524d\u7684Transformer\u67b6\u6784\u5728\u6781\u6df1\u7684\u60c5\u51b5\u4e0b\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Keel\uff0c\u4e00\u4e2a\u6539\u8fdb\u7684Post-LayerNorm Transformer\uff0c\u901a\u8fc7\u66f4\u6362\u6b8b\u5dee\u8fde\u63a5\u6765\u89e3\u51b3\u6df1\u5ea6\u7f51\u7edc\u4e2d\u7684\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u3002</li>\n    <li>Keel\u5728\u8d85\u8fc71000\u5c42\u7684\u6df1\u5ea6\u4e0b\u7a33\u5b9a\u8bad\u7ec3\uff0c\u5e76\u5728\u56f0\u60d1\u5ea6\u548c\u6df1\u5ea6\u6269\u5c55\u7279\u6027\u4e0a\u4f18\u4e8ePre-LN\u6a21\u578b\u3002</li>\n    <li>\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u7ed3\u5408Highway\u8fde\u63a5\u7684Post-LN\u4e3a\u6784\u5efa\u6df1\u5ea6\u53ef\u6269\u5c55\u7684LLM\u63d0\u4f9b\u4e86\u7b80\u5355\u6709\u6548\u7684\u57fa\u7840\uff0c\u672a\u6765\u53ef\u80fd\u5b9e\u73b0\u65e0\u9650\u6df1\u5ea6\u67b6\u6784\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) are facing challenges in scaling effectively, with larger models showing less improvement.</li>\n    <li>Increasing the number of layers (depth) in models can enhance their performance, but current designs struggle to train deep networks reliably.</li>\n    <li>The study revisits Post-LayerNorm (Post-LN) architecture, which was previously replaced due to stability issues, particularly from gradient vanishing in deep networks.</li>\n    <li>The new model, Keel, uses a Highway-style connection instead of a residual path, improving gradient flow and enabling stable training in very deep networks (over 1000 layers).</li>\n    <li>Keel shows better performance in perplexity and depth scaling than the older Pre-LN model, suggesting a promising direction for future deep LLMs.</li>\n</ul>"}, "publishedAt": "2026-01-27T13:58:46.000Z", "title": "Post-LayerNorm Is Back: Stable, ExpressivE, and Deep", "summary": "Large language model (LLM) scaling is hitting a wall. Widening models yields diminishing returns, and extending context length does not improve fundamental expressivity. In contrast, depth scaling offers theoretically superior expressivity, yet current Transformer architectures struggle to train reliably at extreme depths. We revisit the Post-LayerNorm (Post-LN) formulation, whose instability at scale caused its replacement by Pre-LN in modern LLMs. We show that the central failure mode of Post-LN arises from the ResNet-style residual pathway, which introduces gradient vanishing in deep networks. We present Keel, a Post-LN Transformer that replaces this residual path with a Highway-style connection. This modification preserves the gradient flow through the residual branch, preventing signal vanishing from the top layers to the bottom. Unlike prior methods, Keel enables stable training at extreme depths without requiring specialized initialization or complex optimization tricks. Keel trains robustly at depths exceeding 1000 layers and consistently improves perplexity and depth-scaling characteristics over Pre-LN. These findings indicate that Post-LN, when paired with a Highway-style connection, provides a simple and effective foundation for building deeply scalable LLMs, opening the possibility for future infinite-depth architectures.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19895.png", "numComments": 2, "submittedBy": {"_id": "69798a0962e9590654013cab", "avatarUrl": "/avatars/b60e3d096a463b55cd3b98677ae568d7.svg", "fullname": "Lai Wei", "name": "laiwei-seed", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}],
    "week": [{"paper": {"id": "2601.16725", "authors": [{"_id": "6976d5405d41524304c13537", "name": "Meituan LongCat Team", "hidden": false}, {"_id": "6976d5405d41524304c13538", "name": "Anchun Gui", "hidden": false}, {"_id": "6976d5405d41524304c13539", "name": "Bei Li", "hidden": false}, {"_id": "6976d5405d41524304c1353a", "name": "Bingyang Tao", "hidden": false}, {"_id": "6976d5405d41524304c1353b", "name": "Bole Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1353c", "name": "Borun Chen", "hidden": false}, {"_id": "6976d5405d41524304c1353e", "name": "Chao Zhang", "hidden": false}, {"_id": "69772bc15d41524304c13739", "name": "Chao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1353f", "name": "Chen Gao", "hidden": false}, {"_id": "6976d5405d41524304c13540", "name": "Chen Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13541", "name": "Chengcheng Han", "hidden": false}, {"_id": "6976d5405d41524304c13542", "name": "Chenhui Yang", "hidden": false}, {"_id": "6976d5405d41524304c13543", "name": "Chuyu Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13544", "name": "Cong Chen", "hidden": false}, {"_id": "6976d5405d41524304c13545", "name": "Cunguang Wang", "hidden": false}, {"_id": "6976d5405d41524304c13546", "name": "Daoru Pan", "hidden": false}, {"_id": "6976d5405d41524304c13547", "name": "Defei Bu", "hidden": false}, {"_id": "6976d5405d41524304c13548", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13549", "name": "Di Xiu", "hidden": false}, {"_id": "6976d5405d41524304c1354a", "name": "Dishan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1354b", "name": "Dongyu Ru", "hidden": false}, {"_id": "6976d5405d41524304c1354c", "name": "Dunwei Tu", "hidden": false}, {"_id": "6976d5405d41524304c1354d", "name": "Fan Wu", "hidden": false}, {"_id": "6976d5405d41524304c1354e", "name": "Fengcheng Yuan", "hidden": false}, {"_id": "6976d5405d41524304c1354f", "name": "Fengcun Li", "hidden": false}, {"_id": "6976d5405d41524304c13550", "name": "Gang Xu", "hidden": false}, {"_id": "6976d5405d41524304c13551", "name": "Guanyu Wu", "hidden": false}, {"_id": "6976d5405d41524304c13552", "name": "Guoyuan Lin", "hidden": false}, {"_id": "6976d5405d41524304c13553", "name": "Haibin Wang", "hidden": false}, {"_id": "6976d5405d41524304c13554", "name": "Hansi Yang", "hidden": false}, {"_id": "6976d5405d41524304c13555", "name": "Hao Yang", "hidden": false}, {"_id": "6976d5405d41524304c13556", "name": "Haonan Yan", "hidden": false}, {"_id": "6976d5405d41524304c13557", "name": "Haoxiang Ma", "hidden": false}, {"_id": "6976d5405d41524304c13558", "name": "Haoxing Wen", "hidden": false}, {"_id": "6976d5405d41524304c13559", "name": "Hongyan Hao", "hidden": false}, {"_id": "6976d5405d41524304c1355a", "name": "Hongyin Tang", "hidden": false}, {"_id": "6976d5405d41524304c1355b", "name": "Hongyu Zang", "hidden": false}, {"_id": "6976d5405d41524304c1355c", "name": "Hongzhi Ni", "hidden": false}, {"_id": "6976d5405d41524304c1355d", "name": "Hui Su", "hidden": false}, {"_id": "6976d5405d41524304c1355e", "name": "Jiacheng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1355f", "name": "Jiahong Zhou", "hidden": false}, {"_id": "6976d5405d41524304c13560", "name": "Jiahuan Li", "hidden": false}, {"_id": "6976d5405d41524304c13561", "name": "Jiaming Wang", "hidden": false}, {"_id": "6976d5405d41524304c13562", "name": "Jian Yang", "hidden": false}, {"_id": "6976d5405d41524304c13563", "user": {"_id": "64008a0af4ff62c2616d8858", "avatarUrl": "/avatars/b52c98857916fba5377ace8089d658b2.svg", "isPro": false, "fullname": "zhangjf", "user": "zhangjf", "type": "user"}, "name": "Jianfei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:09.272Z", "hidden": false}, {"_id": "6976d5405d41524304c13564", "name": "Jianhao Xu", "hidden": false}, {"_id": "6976d5405d41524304c13565", "name": "Jianing Wang", "hidden": false}, {"_id": "6976d5405d41524304c13566", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13567", "name": "Jiaqi Sun", "hidden": false}, {"_id": "6976d5405d41524304c13568", "name": "Jiarong Shi", "hidden": false}, {"_id": "6976d5405d41524304c13569", "name": "Jiarui Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1356a", "name": "Jingang Wang", "hidden": false}, {"_id": "6976d5405d41524304c1356b", "user": {"_id": "6592472fccbc1e2cc7250903", "avatarUrl": "/avatars/6f04ae66944eb2ce65c5aca7927bab10.svg", "isPro": false, "fullname": "Jinluan Yang", "user": "Jinluan", "type": "user"}, "name": "Jinluan Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:47.175Z", "hidden": false}, {"_id": "6976d5405d41524304c1356c", "name": "Jinrui Ding", "hidden": false}, {"_id": "6976d5405d41524304c1356d", "name": "Jinwei Xiao", "hidden": false}, {"_id": "6976d5405d41524304c1356e", "name": "Jiyuan He", "hidden": false}, {"_id": "6976d5405d41524304c1356f", "name": "Juncan Xu", "hidden": false}, {"_id": "6976d5405d41524304c13570", "name": "Kefeng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13571", "name": "Keheng Wang", "hidden": false}, {"_id": "6976d5405d41524304c13572", "name": "Li Wei", "hidden": false}, {"_id": "6976d5405d41524304c13573", "name": "Lianhui Ma", "hidden": false}, {"_id": "6976d5405d41524304c13574", "name": "Lin Qiu", "hidden": false}, {"_id": "6976d5405d41524304c13575", "name": "Lingbing Kong", "hidden": false}, {"_id": "6976d5405d41524304c13576", "name": "Lingchuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c13577", "name": "Linsen Guo", "hidden": false}, {"_id": "6976d5405d41524304c13578", "name": "Mengshen Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13579", "name": "Mengxia Shen", "hidden": false}, {"_id": "6976d5405d41524304c1357a", "name": "Mingyang Zhu", "hidden": false}, {"_id": "6976d5405d41524304c1357b", "name": "Peiguang Li", "hidden": false}, {"_id": "6976d5405d41524304c1357c", "name": "Peng Pei", "hidden": false}, {"_id": "6976d5405d41524304c1357d", "name": "Pengcheng Jia", "hidden": false}, {"_id": "6976d5405d41524304c1357e", "name": "Pengtao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1357f", "name": "Peng Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13580", "name": "Qi Gu", "hidden": false}, {"_id": "6976d5405d41524304c13581", "name": "Qiong Huang", "hidden": false}, {"_id": "6976d5405d41524304c13582", "name": "Qiyuan Duan", "hidden": false}, {"_id": "6976d5405d41524304c13583", "name": "Quanchi Weng", "hidden": false}, {"_id": "6976d5405d41524304c13584", "name": "Rongxiang Weng", "hidden": false}, {"_id": "6976d5405d41524304c13585", "name": "Rongzhi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13586", "name": "Rumei Li", "hidden": false}, {"_id": "6976d5405d41524304c13587", "name": "Shanglin Lei", "hidden": false}, {"_id": "6976d5405d41524304c13588", "user": {"_id": "64db5f5dd68a6ddcc7bd89e9", "avatarUrl": "/avatars/69375ec915927b855813df8a6d486837.svg", "isPro": false, "fullname": "Shengnan An", "user": "ShengnanAn", "type": "user"}, "name": "Shengnan An", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:11.410Z", "hidden": false}, {"_id": "6976d5405d41524304c13589", "name": "Shijun Dai", "hidden": false}, {"_id": "6976d5405d41524304c1358a", "name": "Shuaikang Liu", "hidden": false}, {"_id": "6976d5405d41524304c1358b", "name": "Shuang Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1358c", "name": "Shuo Wang", "hidden": false}, {"_id": "6976d5405d41524304c1358d", "name": "Songyuan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1358e", "name": "Tao Liang", "hidden": false}, {"_id": "6976d5405d41524304c1358f", "name": "Tianhao Hu", "hidden": false}, {"_id": "6976d5405d41524304c13590", "name": "Tianze Chen", "hidden": false}, {"_id": "6976d5405d41524304c13591", "name": "Wei Liu", "hidden": false}, {"_id": "6976d5405d41524304c13592", "name": "Wei Shi", "hidden": false}, {"_id": "6976d5405d41524304c13593", "name": "Wei Wang", "hidden": false}, {"_id": "6976d5405d41524304c13594", "name": "Weifeng Tang", "hidden": false}, {"_id": "6976d5405d41524304c13595", "name": "Wenjie Shi", "hidden": false}, {"_id": "6976d5405d41524304c13596", "name": "Wenlong Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13597", "name": "Wentao Chen", "hidden": false}, {"_id": "6976d5405d41524304c13598", "name": "Wentao Shi", "hidden": false}, {"_id": "6976d5405d41524304c13599", "name": "Xi Su", "hidden": false}, {"_id": "6976d5405d41524304c1359a", "name": "Xiangcheng Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359b", "name": "Xiandi Ma", "hidden": false}, {"_id": "6976d5405d41524304c1359c", "user": {"_id": "63edb098679c2cc40abc6c2e", "avatarUrl": "/avatars/288c7229937c2c3f29fda6d17c7df2eb.svg", "isPro": false, "fullname": "Xiangyu", "user": "xixy", "type": "user"}, "name": "Xiangyu Xi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:13.312Z", "hidden": false}, {"_id": "6976d5405d41524304c1359d", "name": "Xiangyuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359e", "name": "Xiangzhou Huang", "hidden": false}, {"_id": "6976d5405d41524304c1359f", "name": "Xiao Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a0", "name": "Xiaodong Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a1", "name": "Xiaolong Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a2", "name": "Xiaowei Shi", "hidden": false}, {"_id": "6976d5405d41524304c135a3", "name": "Xiaoyu Li", "hidden": false}, {"_id": "6976d5405d41524304c135a4", "name": "Xin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a5", "name": "Xingchen Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a6", "name": "Xuan Huang", "hidden": false}, {"_id": "6976d5405d41524304c135a7", "name": "Xuezhi Cao", "hidden": false}, {"_id": "6976d5405d41524304c135a8", "name": "Xunliang Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a9", "name": "Yan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135aa", "user": {"_id": "63fc1c420aab06079200c15c", "avatarUrl": "/avatars/8e8e82a9a6552848581ca9f65011263c.svg", "isPro": false, "fullname": "yang bai", "user": "byang", "type": "user"}, "name": "Yang Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:07.036Z", "hidden": false}, {"_id": "6976d5405d41524304c135ab", "name": "Yang Liu", "hidden": false}, {"_id": "6976d5405d41524304c135ac", "name": "Yang Yang", "hidden": false}, {"_id": "6976d5405d41524304c135ad", "name": "Yang Zheng", "hidden": false}, {"_id": "6976d5405d41524304c135ae", "name": "Yaoming Wang", "hidden": false}, {"_id": "6976d5405d41524304c135af", "name": "Yaoming Zhu", "hidden": false}, {"_id": "6976d5405d41524304c135b0", "name": "Yaqi Huo", "hidden": false}, {"_id": "6976d5405d41524304c135b1", "name": "Yanyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b2", "name": "Yaorui Shi", "hidden": false}, {"_id": "6976d5405d41524304c135b3", "name": "Yerui Sun", "hidden": false}, {"_id": "6976d5405d41524304c135b4", "name": "Yi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b5", "name": "Yihao Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b6", "name": "Yi-Kai Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b7", "name": "Yifan Lu", "hidden": false}, {"_id": "6976d5405d41524304c135b8", "name": "Yifan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135b9", "name": "Yitao Zhai", "hidden": false}, {"_id": "6976d5405d41524304c135ba", "name": "Yongjing Yin", "hidden": false}, {"_id": "6976d5405d41524304c135bb", "name": "Yongwei Zhou", "hidden": false}, {"_id": "6976d5405d41524304c135bc", "name": "Youshao Xiao", "hidden": false}, {"_id": "6976d5405d41524304c135bd", "name": "Yuchuan Dai", "hidden": false}, {"_id": "6976d5405d41524304c135be", "name": "Yuchen Xie", "hidden": false}, {"_id": "6976d5405d41524304c135bf", "name": "Yuchen Yu", "hidden": false}, {"_id": "6976d5405d41524304c135c0", "name": "Yufei Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135c1", "name": "Yuhuai Wei", "hidden": false}, {"_id": "6976d5405d41524304c135c2", "name": "Yulei Qian", "hidden": false}, {"_id": "6976d5405d41524304c135c3", "name": "Yunfan Liang", "hidden": false}, {"_id": "6976d5405d41524304c135c4", "name": "Yunke Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135c5", "name": "Yuwei Jiang", "hidden": false}, {"_id": "6976d5405d41524304c135c6", "name": "Yuxin Bian", "hidden": false}, {"_id": "6976d5405d41524304c135c7", "name": "Yuxin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135c8", "name": "Yuxin Liu", "hidden": false}, {"_id": "6976d5405d41524304c135c9", "name": "Yue Xu", "hidden": false}, {"_id": "6976d5405d41524304c135ca", "name": "Yueqing Sun", "hidden": false}, {"_id": "6976d5405d41524304c135cb", "name": "Zeyang Yu", "hidden": false}, {"_id": "6976d5405d41524304c135cc", "name": "Zhao Yang", "hidden": false}, {"_id": "6976d5405d41524304c135cd", "name": "Zhengsheng Huang", "hidden": false}, {"_id": "6976d5405d41524304c135ce", "name": "Zhengyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135cf", "name": "Zhijian Liu", "hidden": false}, {"_id": "6976d5405d41524304c135d0", "name": "Zhikang Xia", "hidden": false}, {"_id": "6976d5405d41524304c135d1", "name": "Zhimin Lin", "hidden": false}, {"_id": "6976d5405d41524304c135d2", "name": "Zhiyuan Yao", "hidden": false}, {"_id": "6976d5405d41524304c135d3", "name": "Zhuofan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135d4", "name": "Zhuowen Han", "hidden": false}, {"_id": "6976d5405d41524304c135d5", "name": "Zijian Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135d6", "name": "Ziran Li", "hidden": false}, {"_id": "6976d5405d41524304c135d7", "name": "Ziwen Wang", "hidden": false}, {"_id": "6976d5405d41524304c135d8", "name": "Ziyuan Zhuang", "hidden": false}], "publishedAt": "2026-01-23T13:20:09.000Z", "submittedOnDailyAt": "2026-01-26T00:15:28.340Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "upvotes": 136, "discussionId": "6976d5405d41524304c135d9", "ai_summary": "A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for real-world robustness and complex reasoning.", "ai_keywords": ["Mixture-of-Experts", "agentic reasoning", "domain-parallel expert training", "fusion", "asynchronous reinforcement learning", "DORA", "long-tailed generation", "multi-turn interactions", "real-world noise patterns", "test-time scaling", "reasoning depth", "reasoning width", "parallel thinking"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>LongCat-Flash-Thinking-2601\u662f\u4e00\u4e2a\u62e5\u67095600\u4ebf\u53c2\u6570\u7684\u5f00\u6e90\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u63a8\u7406\u6a21\u578b\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u667a\u80fd\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u591a\u79cd\u667a\u80fd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u667a\u80fd\u641c\u7d22\u548c\u5de5\u5177\u4f7f\u7528\u7b49\u3002</li>\n    <li>\u6a21\u578b\u5728\u590d\u6742\u5de5\u5177\u4ea4\u4e92\u548c\u5608\u6742\u7684\u73b0\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u901a\u8fc7\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6\u548c\u7cfb\u7edf\u5316\u7684\u589e\u5f3a\u5b66\u4e60\u65b9\u6cd5\uff0c\u6a21\u578b\u80fd\u591f\u5728\u8d85\u8fc710000\u4e2a\u73af\u5883\u4e2d\u8fdb\u884c\u6709\u6548\u8bad\u7ec3\u3002</li>\n    <li>\u5f15\u5165\u4e86\u91cd\u601d\u7ef4\u6a21\u5f0f\uff0c\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u5e76\u884c\u63a8\u7406\u589e\u52a0\u63a8\u7406\u6df1\u5ea6\u548c\u5e7f\u5ea6\uff0c\u4ece\u800c\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongCat-Flash-Thinking-2601 is a large open-source reasoning model with 560 billion parameters, designed for better reasoning abilities.</li>\n    <li>It performs exceptionally well on various benchmarks related to agentic reasoning, such as searching and using tools.</li>\n    <li>The model is built using a special training framework that helps it learn from different environments and tasks effectively.</li>\n    <li>It is designed to handle real-world challenges, such as noise and complex interactions with tools.</li>\n    <li>A new feature called Heavy Thinking mode allows the model to think more deeply and broadly during tests, improving its reasoning skills.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:20:09.000Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16725.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.17058", "authors": [{"_id": "69782c96026bdf0473116e06", "user": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "name": "Wei Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:49.701Z", "hidden": false}, {"_id": "69782c96026bdf0473116e07", "name": "Jun Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e08", "name": "Haoyu Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e09", "name": "Zhenghao Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0a", "name": "Qikang He", "hidden": false}, {"_id": "69782c96026bdf0473116e0b", "name": "Shaokun Han", "hidden": false}, {"_id": "69782c96026bdf0473116e0c", "name": "Guoliang Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0d", "user": {"_id": "64ef522242da8d2a897d62da", "avatarUrl": "/avatars/03611010d247da66696ac8976d4d3ed3.svg", "isPro": false, "fullname": "xuanhe zhou", "user": "zhouxh19", "type": "user"}, "name": "Xuanhe Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:19.930Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0e", "user": {"_id": "674fa2f067c963c50a066594", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674fa2f067c963c50a066594/hKZ46Mwm_UEguzBt63ys_.jpeg", "isPro": false, "fullname": "yeye he", "user": "yeyehe", "type": "user"}, "name": "Yeye He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:27.638Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0f", "name": "Chunwei Liu", "hidden": false}, {"_id": "69782c96026bdf0473116e10", "user": {"_id": "66724ce47e7ff5d8bd069c7c", "avatarUrl": "/avatars/953f66585390dbdb202c1d7b7250d7bd.svg", "isPro": false, "fullname": "Zirui Tang", "user": "TerryTang", "type": "user"}, "name": "Zirui Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:49.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e11", "name": "Bin Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e12", "user": {"_id": "695612aabf3c8959a3a05f9c", "avatarUrl": "/avatars/c18885f6dea6f3ee019405cd8cf6f484.svg", "isPro": false, "fullname": "ShenTang990", "user": "shentang", "type": "user"}, "name": "Shen Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:56.579Z", "hidden": false}, {"_id": "69782c96026bdf0473116e13", "name": "Kai Zuo", "hidden": false}, {"_id": "69782c96026bdf0473116e14", "user": {"_id": "67efa8a2ed790a2e999dc216", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0S4lQCJX61uCF8EkSLMkk.png", "isPro": false, "fullname": "Yuyu Luo", "user": "luoyuyu", "type": "user"}, "name": "Yuyu Luo", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:02.233Z", "hidden": false}, {"_id": "69782c96026bdf0473116e15", "name": "Zhenzhe Zheng", "hidden": false}, {"_id": "69782c96026bdf0473116e16", "user": {"_id": "63f9fca8d4349b157a109eec", "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg", "isPro": false, "fullname": "Conghui He", "user": "conghui", "type": "user"}, "name": "Conghui He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:57:14.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e17", "name": "Jingren Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e18", "name": "Fan Wu", "hidden": false}], "publishedAt": "2026-01-22T12:02:45.000Z", "submittedOnDailyAt": "2026-01-27T00:42:38.464Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "submittedOnDailyBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "upvotes": 127, "discussionId": "69782c97026bdf0473116e19", "projectPage": "https://github.com/weAIDB/awesome-data-llm", "githubRepo": "https://github.com/weAIDB/awesome-data-llm", "githubRepoAddedBy": "user", "ai_summary": "LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.", "ai_keywords": ["data preparation", "large language models", "prompt-driven workflows", "agentic workflows", "data cleaning", "data integration", "data enrichment", "entity matching", "schema matching", "data annotation", "data profiling"], "githubStars": 644, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u6570\u636e\u51c6\u5907\u7684\u76ee\u7684\u662f\u53bb\u566a\u58f0\u3001\u53d1\u73b0\u8de8\u6570\u636e\u96c6\u7684\u5173\u7cfb\u5e76\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6570\u636e\u5e94\u7528\u3002</li>\n    <li>\u7531\u4e8e\u5bf9\u5e94\u7528\u51c6\u5907\u6570\u636e\u7684\u9700\u6c42\u4e0a\u5347\uff0c\u5f3a\u5927\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6280\u672f\u7684\u51fa\u73b0\uff0c\u4ee5\u53ca\u7075\u6d3b\u4ee3\u7406\u6784\u5efa\u57fa\u7840\u8bbe\u65bd\u7684\u53d1\u5c55\uff0cLLM\u589e\u5f3a\u7684\u6570\u636e\u51c6\u5907\u65b9\u6cd5\u6b63\u5728\u8fc5\u901f\u6210\u4e3a\u4e3b\u6d41\u3002</li>\n    <li>\u6587\u7ae0\u5bf9\u6700\u65b0\u6587\u732e\u8fdb\u884c\u4e86\u7cfb\u7edf\u56de\u987e\uff0c\u5173\u6ce8LLM\u6280\u672f\u5728\u6570\u636e\u51c6\u5907\u4e2d\u7684\u5e94\u7528\uff0c\u5b9a\u4e49\u4e86\u4ece\u89c4\u5219\u57fa\u7840\u5230\u57fa\u4e8e\u63d0\u793a\u7684\u51c6\u5907\u5de5\u4f5c\u6d41\u7684\u8303\u5f0f\u8f6c\u53d8\u3002</li>\n    <li>\u4ecb\u7ecd\u4e86\u4ee5\u4efb\u52a1\u4e3a\u4e2d\u5fc3\u7684\u5206\u7c7b\u6cd5\uff0c\u5c06\u6570\u636e\u51c6\u5907\u5206\u4e3a\u6570\u636e\u6e05\u7406\u3001\u6570\u636e\u6574\u5408\u548c\u6570\u636e\u4e30\u5bcc\u4e09\u4e2a\u4e3b\u8981\u4efb\u52a1\uff0c\u5e76\u5206\u6790\u4e86\u5404\u81ea\u7684\u6280\u672f\u4f18\u7f3a\u70b9\u3002</li>\n    <li>\u8ba8\u8bba\u4e86\u5e38\u7528\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u6311\u6218\u53ca\u53ef\u6269\u5c55\u7684LLM\u6570\u636e\u7cfb\u7edf\u7684\u53d1\u5c55\u8def\u7ebf\u56fe\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Data preparation helps clean, connect, and gain insights from raw datasets, which is important for various applications.</li>\n  <li>The paper reviews how large language models (LLMs) are changing data preparation methods to make them more effective and flexible.</li>\n  <li>It identifies three main tasks in data preparation: cleaning, integration, and enrichment, and discusses techniques for each.</li>\n  <li>The paper highlights the strengths and weaknesses of LLM techniques, such as better understanding but high costs and issues with reliability.</li>\n  <li>It also examines datasets and evaluation methods, and suggests future research directions for improving LLM-driven data systems.</li>\n</ul>"}, "publishedAt": "2026-01-22T07:02:45.000Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17058.png", "numComments": 2, "submittedBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "fullname": "Wei Zhou", "name": "weizhoudb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.18418", "authors": [{"_id": "69785315026bdf0473116f6a", "user": {"_id": "62dce08bb2c60f29c3d0a5da", "avatarUrl": "/avatars/87ce03e61c4c6eb686c9491ef4fda225.svg", "isPro": false, "fullname": "Ji Zeng", "user": "stargazerzj", "type": "user"}, "name": "Ji Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T08:31:51.245Z", "hidden": false}, {"_id": "69785315026bdf0473116f6b", "name": "Dayuan Fu", "hidden": false}, {"_id": "69785315026bdf0473116f6c", "name": "Tiantian Mi", "hidden": false}, {"_id": "69785315026bdf0473116f6d", "name": "Yumin Zhuang", "hidden": false}, {"_id": "69785315026bdf0473116f6e", "user": {"_id": "6865e6b362fc5689c5e67733", "avatarUrl": "/avatars/186f3d248791d961b0a810d5225167cc.svg", "isPro": false, "fullname": "Yaxing Huang", "user": "Rookie-Noob-Newbie", "type": "user"}, "name": "Yaxing Huang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:30.220Z", "hidden": false}, {"_id": "69785315026bdf0473116f6f", "user": {"_id": "67638cc0d63e4b348e8a5fa3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67638cc0d63e4b348e8a5fa3/BZNlw1uTGUcumCrXKkerx.png", "isPro": false, "fullname": "Xuefeng Li", "user": "drxuefeng", "type": "user"}, "name": "Xuefeng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:37.248Z", "hidden": false}, {"_id": "69785315026bdf0473116f70", "name": "Lyumanshan Ye", "hidden": false}, {"_id": "69785315026bdf0473116f71", "name": "Muhang Xie", "hidden": false}, {"_id": "69785315026bdf0473116f72", "name": "Qishuo Hua", "hidden": false}, {"_id": "69785315026bdf0473116f73", "name": "Zhen Huang", "hidden": false}, {"_id": "69785315026bdf0473116f74", "user": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "isPro": false, "fullname": "Mohan Jiang (SII)", "user": "mhjiang0408", "type": "user"}, "name": "Mohan Jiang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:23.438Z", "hidden": false}, {"_id": "69785315026bdf0473116f75", "name": "Hanning Wang", "hidden": false}, {"_id": "69785315026bdf0473116f76", "user": {"_id": "66fa544c54f87b607fbffd2e", "avatarUrl": "/avatars/94195dcda0eb68e8fd20d80718744697.svg", "isPro": false, "fullname": "Jifan Lin", "user": "evanlin2570", "type": "user"}, "name": "Jifan Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:57.029Z", "hidden": false}, {"_id": "69785315026bdf0473116f77", "name": "Yang Xiao", "hidden": false}, {"_id": "69785315026bdf0473116f78", "name": "Jie Sun", "hidden": false}, {"_id": "69785315026bdf0473116f79", "user": {"_id": "684faf712acd915b5afc055f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/684faf712acd915b5afc055f/K7icmL08HxniWDgdph73i.jpeg", "isPro": false, "fullname": "Yunze Wu", "user": "wyzmike", "type": "user"}, "name": "Yunze Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:01:06.063Z", "hidden": false}, {"_id": "69785315026bdf0473116f7a", "name": "Pengfei Liu", "hidden": false}], "publishedAt": "2026-01-26T12:20:18.000Z", "submittedOnDailyAt": "2026-01-27T03:34:37.777Z", "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "submittedOnDailyBy": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "isPro": false, "fullname": "Mohan Jiang (SII)", "user": "mhjiang0408", "type": "user"}, "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...", "upvotes": 104, "discussionId": "69785315026bdf0473116f7b", "githubRepo": "https://github.com/GAIR-NLP/daVinci-Dev", "githubRepoAddedBy": "user", "ai_summary": "Agentic mid-training enables large language models to develop autonomous software engineering capabilities through specialized data synthesis techniques that bridge the gap between static training data and dynamic development environments.", "ai_keywords": ["Large Language Model", "agentic software engineering", "mid-training", "distribution mismatch", "agent-native data", "contextually-native trajectories", "environmentally-native trajectories", "SWE-Bench Verified", "Kimi-Dev", "resolution rates"], "githubStars": 22, "organization": {"_id": "630bc2d186b8b9904c33ce1b", "name": "GAIR", "fullname": "SII - GAIR", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u80fd\u529b\u6b63\u4ece\u5355\u6b21\u4ee3\u7801\u751f\u6210\u8f6c\u5411\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\uff0c\u6a21\u578b\u53ef\u4ee5\u81ea\u52a8\u5bfc\u822a\u3001\u7f16\u8f91\u548c\u6d4b\u8bd5\u590d\u6742\u4ee3\u7801\u5e93\u3002</li>\n    <li>\u5c3d\u7ba1\u540e\u8bad\u7ec3\u65b9\u6cd5\u5df2\u6210\u4e3a\u4ee3\u7801\u4ee3\u7406\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u4f46\u201c\u4e2d\u8bad\u7ec3\u201d\uff08MT\uff09\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u5c3d\u7ba1\u5b83\u80fd\u4ee5\u66f4\u53ef\u6269\u5c55\u7684\u65b9\u5f0f\u57f9\u517b\u57fa\u7840\u7684\u4ee3\u7406\u884c\u4e3a\u3002</li>\n    <li>\u5b9e\u73b0\u6709\u6548\u7684\u4e2d\u8bad\u7ec3\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\u662f\u9759\u6001\u8bad\u7ec3\u6570\u636e\u4e0e\u5b9e\u9645\u5f00\u53d1\u4e2d\u7684\u52a8\u6001\u53cd\u9988\u73af\u5883\u4e4b\u95f4\u7684\u5206\u5e03\u4e0d\u5339\u914d\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u4e2d\u8bad\u7ec3\u7814\u7a76\uff0c\u5efa\u7acb\u4e86\u6709\u6548\u7684\u4ee3\u7406\u5f00\u53d1\u7684\u6570\u636e\u5408\u6210\u539f\u5219\u548c\u8bad\u7ec3\u65b9\u6cd5\u3002</li>\n    <li>\u901a\u8fc7\u4f7f\u7528\u201c\u4ee3\u7406\u672c\u571f\u6570\u636e\u201d\uff0c\u6211\u4eec\u5728\u4e24\u4e2a\u540e\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u4e4b\u524d\u7684\u4e2d\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528\u4e0d\u5230\u4e00\u534a\u7684\u4e2d\u8bad\u7ec3\u4ee3\u5e01\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are evolving to perform complex software engineering tasks autonomously, moving beyond simple code generation.</li>\n    <li>Agentic mid-training (MT) is a method that can improve LLMs by using large datasets that simulate real-world software development, but it requires significant resources.</li>\n    <li>A key challenge is that training data can differ from the dynamic environments where software is actually developed.</li>\n    <li>The study introduces new methods for agentic mid-training, focusing on creating two types of data: contextually-native and environmentally-native trajectories to enhance agent learning.</li>\n    <li>The results show that their approach outperforms previous methods in code resolution rates while using fewer training resources.</li>\n</ul>"}, "publishedAt": "2026-01-26T07:20:18.000Z", "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18418.png", "numComments": 2, "submittedBy": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "fullname": "Mohan Jiang (SII)", "name": "mhjiang0408", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "630bc2d186b8b9904c33ce1b", "name": "GAIR", "fullname": "SII - GAIR", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16746", "authors": [{"_id": "6976d4105d41524304c13517", "name": "Yuhang Wang", "hidden": false}, {"_id": "6976d4105d41524304c13518", "user": {"_id": "645b0c3ec35da9c7afd95421", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg", "isPro": false, "fullname": "Yuling", "user": "YerbaPage", "type": "user"}, "name": "Yuling Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:56.805Z", "hidden": false}, {"_id": "6976d4105d41524304c13519", "name": "Mo Yang", "hidden": false}, {"_id": "6976d4105d41524304c1351a", "name": "Rongrui Zhang", "hidden": false}, {"_id": "6976d4105d41524304c1351b", "name": "Shilin He", "hidden": false}, {"_id": "6976d4105d41524304c1351c", "name": "Heng Lian", "hidden": false}, {"_id": "6976d4105d41524304c1351d", "name": "Yuting Chen", "hidden": false}, {"_id": "6976d4105d41524304c1351e", "name": "Siyu Ye", "hidden": false}, {"_id": "6976d4105d41524304c1351f", "name": "Kai Cai", "hidden": false}, {"_id": "6976d4105d41524304c13520", "name": "Xiaodong Gu", "hidden": false}], "publishedAt": "2026-01-23T13:51:59.000Z", "submittedOnDailyAt": "2026-01-26T00:10:23.451Z", "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As a result, they frequently disrupt syntactic and logical structure and fail to retain critical implementation details. In this paper, we propose SWE-Pruner, a self-adaptive context pruning framework tailored for coding agents. Drawing inspiration from how human programmers \"selectively skim\" source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. Given the current task, the agent formulates an explicit goal (e.g., \"focus on error handling\") as a hint to guide the pruning targets. A lightweight neural skimmer (0.6B parameters) is trained to dynamically select relevant lines from the surrounding context given the goal. Evaluations across four benchmarks and multiple models validate SWE-Pruner's effectiveness in various scenarios, achieving 23-54% token reduction on agent tasks like SWE-Bench Verified and up to 14.84x compression on single-turn tasks like LongCodeQA with minimal performance impact.", "upvotes": 63, "discussionId": "6976d4105d41524304c13521", "githubRepo": "https://github.com/Ayanami1314/swe-pruner", "githubRepoAddedBy": "user", "ai_summary": "SWE-Pruner is a self-adaptive context pruning framework for coding agents that uses task-aware pruning to reduce token usage while maintaining performance.", "ai_keywords": ["context compression", "LongLLMLingua", "PPL", "code understanding", "task-aware adaptive pruning", "neural skimmer", "token reduction", "SWE-Bench Verified", "LongCodeQA"], "githubStars": 35, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>LLM\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u957f\u4ea4\u4e92\u4e0a\u4e0b\u6587\u5bfc\u81f4\u9ad8API\u6210\u672c\u548c\u5ef6\u8fdf\u3002</li>\n    <li>\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u538b\u7f29\u65b9\u6cd5\uff08\u5982LongLLMLingua\uff09\u5e38\u5e38\u5ffd\u7565\u4ee3\u7801\u7406\u89e3\u7684\u4efb\u52a1\u7279\u6027\u3002</li>\n    <li>\u672c\u6587\u63d0\u51faSWE-Pruner\uff0c\u8fd9\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u7684\u4e0a\u4e0b\u6587\u4fee\u526a\u6846\u67b6\uff0c\u4e13\u4e3a\u7f16\u7a0b\u4ee3\u7406\u8bbe\u8ba1\u3002</li>\n    <li>SWE-Pruner\u501f\u9274\u4eba\u7c7b\u7a0b\u5e8f\u5458\u5728\u5f00\u53d1\u548c\u8c03\u8bd5\u8fc7\u7a0b\u4e2d\u201c\u9009\u62e9\u6027\u6d4f\u89c8\u201d\u6e90\u4ee3\u7801\u7684\u65b9\u5f0f\u3002</li>\n    <li>\u8bc4\u4f30\u8868\u660e\uff0cSWE-Pruner\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\uff0c\u80fd\u51cf\u5c1123-54%\u7684\u4ee3\u5e01\u5e76\u5728\u5355\u8f6e\u4efb\u52a1\u4e0a\u5b9e\u73b0\u9ad8\u8fbe14.84\u500d\u7684\u538b\u7f29\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LLM agents are good at software development but struggle with long interactions due to high costs and delays.</li>\n    <li>Current methods for reducing context length often lose important details in code.</li>\n    <li>SWE-Pruner is a new framework that helps coding agents focus on relevant parts of code by adapting to specific tasks.</li>\n    <li>It uses a lightweight neural network to choose important lines of code based on the task at hand.</li>\n    <li>SWE-Pruner has been tested and shows significant reductions in context length with little impact on performance.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:51:59.000Z", "title": "SWE-Pruner: Self-Adaptive Context Pruning for Coding Agents", "summary": "LLM agents have demonstrated remarkable capabilities in software development, but their performance is hampered by long interaction contexts, which incur high API costs and latency. While various context compression approaches such as LongLLMLingua have emerged to tackle this challenge, they typically rely on fixed metrics such as PPL, ignoring the task-specific nature of code understanding. As a result, they frequently disrupt syntactic and logical structure and fail to retain critical implementation details. In this paper, we propose SWE-Pruner, a self-adaptive context pruning framework tailored for coding agents. Drawing inspiration from how human programmers \"selectively skim\" source code during development and debugging, SWE-Pruner performs task-aware adaptive pruning for long contexts. Given the current task, the agent formulates an explicit goal (e.g., \"focus on error handling\") as a hint to guide the pruning targets. A lightweight neural skimmer (0.6B parameters) is trained to dynamically select relevant lines from the surrounding context given the goal. Evaluations across four benchmarks and multiple models validate SWE-Pruner's effectiveness in various scenarios, achieving 23-54% token reduction on agent tasks like SWE-Bench Verified and up to 14.84x compression on single-turn tasks like LongCodeQA with minimal performance impact.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16746.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.15876", "authors": [{"_id": "6972d8d5fb12c92b735b73a2", "name": "Taofeng Xue", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a3", "user": {"_id": "6801cbd5f06f2d08f3fd5455", "avatarUrl": "/avatars/c728b48f6938c0d7cb6b14011927ede8.svg", "isPro": false, "fullname": "chong.peng", "user": "KleinChong", "type": "user"}, "name": "Chong Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T20:15:18.698Z", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a4", "name": "Mianqiu Huang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a5", "name": "Linsen Guo", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a6", "user": {"_id": "6764279e684ed3b61b2316a4", "avatarUrl": "/avatars/63a6b6c3b9f442b42480679425951187.svg", "isPro": false, "fullname": "SII-TianchengHAN", "user": "GenSouKai", "type": "user"}, "name": "Tiancheng Han", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:30.106Z", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a7", "name": "Haozhe Wang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a8", "name": "Jianing Wang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a9", "name": "Xiaocheng Zhang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73aa", "name": "Xin Yang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ab", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ac", "name": "Jinrui Ding", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ad", "name": "Xiandi Ma", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ae", "name": "Yuchen Xie", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73af", "name": "Peng Pei", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73b0", "name": "Xunliang Cai", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73b1", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-01-22T11:36:43.000Z", "submittedOnDailyAt": "2026-01-23T07:54:00.525Z", "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience", "submittedOnDailyBy": {"_id": "6459c7c10aba070266e41bb1", "avatarUrl": "/avatars/2178cac69cf4123db5e85191160f3795.svg", "isPro": false, "fullname": "mqhuang", "user": "LutherXD", "type": "user"}, "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.", "upvotes": 62, "discussionId": "6972d8d5fb12c92b735b73b2", "ai_summary": "EvoCUA introduces an evolutionary approach to computer-use agents that combines autonomous task generation with policy optimization to achieve superior performance in complex, long-horizon tasks.", "ai_keywords": ["computer-use agents", "native computer-use agents", "data generation", "policy optimization", "evolutionary cycle", "verifiable synthesis engine", "executable validators", "sandbox rollouts", "iterative evolving learning", "capability boundaries", "error analysis", "self-correction", "OSWorld benchmark", "foundation models"], "summary_zh": "<ul>\n    <li>\u5f00\u53d1\u4e86EvoCUA\uff0c\u4e00\u4e2a\u65b0\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u6a21\u578b\uff0c\u514b\u670d\u4e86\u9759\u6001\u6570\u636e\u7684\u9650\u5236\u3002</li>\n    <li>EvoCUA\u901a\u8fc7\u81ea\u6211\u751f\u6210\u6570\u636e\u548c\u4f18\u5316\u7b56\u7565\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u81ea\u6211\u7ef4\u6301\u7684\u8fdb\u5316\u5faa\u73af\u3002</li>\n    <li>\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u9a8c\u8bc1\u7684\u5408\u6210\u5f15\u64ce\uff0c\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u4efb\u52a1\uff0c\u5e76\u914d\u6709\u53ef\u6267\u884c\u7684\u9a8c\u8bc1\u5668\u3002</li>\n    <li>EvoCUA\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6210\u529f\u7387\u8fbe56.7%\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u6700\u4f73\u6a21\u578bOpenCUA-72B\u548cUI-TARS-2\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u663e\u793a\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u89c4\u6a21\u7684\u57fa\u7840\u6a21\u578b\u4e2d\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Native computer-use agents (CUA) are improving multimodal AI but face challenges with static data.</li>\n    <li>EvoCUA is a new model that combines data generation and policy optimization to learn from experience.</li>\n    <li>A special synthesis engine helps create diverse tasks and validate them automatically to address data shortages.</li>\n    <li>EvoCUA uses a scalable system to run many tasks at once, allowing it to learn effectively from large amounts of experience.</li>\n    <li>Empirical tests show EvoCUA has a success rate of 56.7%, outperforming previous models and showing that this approach can benefit various AI models.</li>\n</ul>"}, "publishedAt": "2026-01-22T06:36:43.000Z", "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience", "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15876.png", "numComments": 1, "submittedBy": {"_id": "6459c7c10aba070266e41bb1", "avatarUrl": "/avatars/2178cac69cf4123db5e85191160f3795.svg", "fullname": "mqhuang", "name": "LutherXD", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.18491", "authors": [{"_id": "697831d9026bdf0473116e5c", "name": "Dongrui Liu", "hidden": false}, {"_id": "697831d9026bdf0473116e5d", "user": {"_id": "66e2624a436a1798365e4581", "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg", "isPro": false, "fullname": "Qihan Ren", "user": "jasonrqh", "type": "user"}, "name": "Qihan Ren", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:31:15.765Z", "hidden": false}, {"_id": "697831d9026bdf0473116e5e", "name": "Chen Qian", "hidden": false}, {"_id": "697831d9026bdf0473116e5f", "name": "Shuai Shao", "hidden": false}, {"_id": "697831d9026bdf0473116e60", "name": "Yuejin Xie", "hidden": false}, {"_id": "697831d9026bdf0473116e61", "name": "Yu Li", "hidden": false}, {"_id": "697831d9026bdf0473116e62", "name": "Zhonghao Yang", "hidden": false}, {"_id": "697831d9026bdf0473116e63", "name": "Haoyu Luo", "hidden": false}, {"_id": "697831d9026bdf0473116e64", "name": "Peng Wang", "hidden": false}, {"_id": "697831d9026bdf0473116e65", "name": "Qingyu Liu", "hidden": false}, {"_id": "697831d9026bdf0473116e66", "name": "Binxin Hu", "hidden": false}, {"_id": "697831d9026bdf0473116e67", "name": "Ling Tang", "hidden": false}, {"_id": "697831d9026bdf0473116e68", "name": "Jilin Mei", "hidden": false}, {"_id": "697831d9026bdf0473116e69", "name": "Dadi Guo", "hidden": false}, {"_id": "697831d9026bdf0473116e6a", "name": "Leitao Yuan", "hidden": false}, {"_id": "697831d9026bdf0473116e6b", "name": "Junyao Yang", "hidden": false}, {"_id": "697831d9026bdf0473116e6c", "name": "Guanxu Chen", "hidden": false}, {"_id": "697831d9026bdf0473116e6d", "name": "Qihao Lin", "hidden": false}, {"_id": "697831d9026bdf0473116e6e", "name": "Yi Yu", "hidden": false}, {"_id": "697831d9026bdf0473116e6f", "name": "Bo Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e70", "name": "Jiaxuan Guo", "hidden": false}, {"_id": "697831d9026bdf0473116e71", "name": "Jie Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e72", "name": "Wenqi Shao", "hidden": false}, {"_id": "697831d9026bdf0473116e73", "name": "Huiqi Deng", "hidden": false}, {"_id": "697831d9026bdf0473116e74", "name": "Zhiheng Xi", "hidden": false}, {"_id": "697831d9026bdf0473116e75", "name": "Wenjie Wang", "hidden": false}, {"_id": "697831d9026bdf0473116e76", "name": "Wenxuan Wang", "hidden": false}, {"_id": "697831d9026bdf0473116e77", "name": "Wen Shen", "hidden": false}, {"_id": "697831d9026bdf0473116e78", "name": "Zhikai Chen", "hidden": false}, {"_id": "697831d9026bdf0473116e79", "name": "Haoyu Xie", "hidden": false}, {"_id": "697831d9026bdf0473116e7a", "name": "Jialing Tao", "hidden": false}, {"_id": "697831d9026bdf0473116e7b", "name": "Juntao Dai", "hidden": false}, {"_id": "697831d9026bdf0473116e7c", "name": "Jiaming Ji", "hidden": false}, {"_id": "697831d9026bdf0473116e7d", "name": "Zhongjie Ba", "hidden": false}, {"_id": "697831d9026bdf0473116e7e", "name": "Linfeng Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e7f", "name": "Yong Liu", "hidden": false}, {"_id": "697831d9026bdf0473116e80", "name": "Quanshi Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e81", "name": "Lei Zhu", "hidden": false}, {"_id": "697831d9026bdf0473116e82", "name": "Zhihua Wei", "hidden": false}, {"_id": "697831d9026bdf0473116e83", "name": "Hui Xue", "hidden": false}, {"_id": "697831d9026bdf0473116e84", "name": "Chaochao Lu", "hidden": false}, {"_id": "697831d9026bdf0473116e85", "name": "Jing Shao", "hidden": false}, {"_id": "697831d9026bdf0473116e86", "name": "Xia Hu", "hidden": false}], "publishedAt": "2026-01-26T13:45:41.000Z", "submittedOnDailyAt": "2026-01-28T01:26:49.833Z", "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security", "submittedOnDailyBy": {"_id": "66e2624a436a1798365e4581", "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg", "isPro": false, "fullname": "Qihan Ren", "user": "jasonrqh", "type": "user"}, "summary": "The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.", "upvotes": 60, "discussionId": "697831d9026bdf0473116e87", "githubRepo": "https://github.com/AI45Lab/AgentDoG", "githubRepoAddedBy": "user", "ai_summary": "AI agents face safety and security challenges from autonomous tool use and environmental interactions, requiring advanced guardrail frameworks for risk diagnosis and transparent monitoring.", "ai_keywords": ["agentic guardrail", "three-dimensional taxonomy", "agentic safety benchmark", "Diagnostic Guardrail framework", "agent safety and security", "agent trajectories", "root cause diagnosis", "fine-grained monitoring", "model variants", "state-of-the-art performance"], "githubStars": 178, "organization": {"_id": "68f716f832b31e42cbc2be7f", "name": "AI45Research", "fullname": "AI45Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68f6ffaa04d1019724af41fc/EVBafPHXvChszTM5tJcc9.png"}, "summary_zh": "<ul>\n    <li>AI\u667a\u80fd\u4f53\u7684\u4f7f\u7528\u5e26\u6765\u4e86\u590d\u6742\u7684\u5b89\u5168\u548c\u5b89\u5168\u6311\u6218\u3002</li>\n    <li>\u5f53\u524d\u7684\u5b89\u5168\u6a21\u578b\u7f3a\u4e4f\u5bf9\u98ce\u9669\u7684\u8ba4\u8bc6\u548c\u900f\u660e\u5ea6\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e09\u7ef4\u5206\u7c7b\u6cd5\uff0c\u5206\u7c7b\u667a\u80fd\u4f53\u98ce\u9669\u7684\u6765\u6e90\u3001\u5931\u8d25\u65b9\u5f0f\u548c\u540e\u679c\u3002</li>\n    <li>\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u57fa\u51c6\uff08ATBench\uff09\u548c\u8bca\u65ad\u6846\u67b6\uff08AgentDoG\uff09\uff0c\u53ef\u4ee5\u76d1\u63a7\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u3002</li>\n    <li>AgentDoG\u80fd\u591f\u8bca\u65ad\u4e0d\u5b89\u5168\u548c\u4f3c\u4e4e\u5b89\u5168\u4f46\u4e0d\u5408\u7406\u7684\u884c\u4e3a\uff0c\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u7684\u5bf9\u9f50\u6548\u679c\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The rise of AI agents brings new safety and security challenges from their autonomous use and interactions with the environment.</li>\n    <li>Current safety models lack awareness of risks associated with AI agents and do not provide clear risk diagnoses.</li>\n    <li>A new three-dimensional system categorizes agentic risks based on their source, failure mode, and consequences.</li>\n    <li>A new benchmark called ATBench and a safety framework named AgentDoG have been introduced to improve monitoring and diagnosis of AI agent behaviors.</li>\n    <li>AgentDoG can identify the causes of unsafe actions and offers detailed insights, with models available in different sizes, showing excellent performance in complex scenarios.</li>\n</ul>"}, "publishedAt": "2026-01-26T08:45:41.000Z", "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security", "summary": "The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18491.png", "numComments": 6, "submittedBy": {"_id": "66e2624a436a1798365e4581", "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg", "fullname": "Qihan Ren", "name": "jasonrqh", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "68f716f832b31e42cbc2be7f", "name": "AI45Research", "fullname": "AI45Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68f6ffaa04d1019724af41fc/EVBafPHXvChszTM5tJcc9.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16206", "authors": [{"_id": "6972e04dfb12c92b735b73cf", "user": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "isPro": false, "fullname": "Daixuan Cheng", "user": "daixuancheng", "type": "user"}, "name": "Daixuan Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:28.070Z", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d0", "name": "Shaohan Huang", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d1", "name": "Yuxian Gu", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d2", "name": "Huatong Song", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d3", "name": "Guoxin Chen", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d4", "name": "Li Dong", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d5", "name": "Wayne Xin Zhao", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d6", "name": "Ji-Rong Wen", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d7", "name": "Furu Wei", "hidden": false}], "publishedAt": "2026-01-22T18:57:09.000Z", "submittedOnDailyAt": "2026-01-23T00:27:12.305Z", "title": "LLM-in-Sandbox Elicits General Agentic Intelligence", "submittedOnDailyBy": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "isPro": false, "fullname": "Daixuan Cheng", "user": "daixuancheng", "type": "user"}, "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.", "upvotes": 46, "discussionId": "6972e04dfb12c92b735b73d8", "projectPage": "https://llm-in-sandbox.github.io", "githubRepo": "https://github.com/llm-in-sandbox/llm-in-sandbox", "githubRepoAddedBy": "user", "ai_summary": "LLM-in-Sandbox enables large language models to perform general intelligence tasks across diverse domains by allowing them to explore a code sandbox environment, achieving robust generalization without additional training.", "ai_keywords": ["LLM-in-Sandbox", "code sandbox", "virtual computer", "reinforcement learning", "non-agentic data", "sandbox exploration", "general intelligence", "long-context understanding", "instruction following"], "githubStars": 38, "organization": {"_id": "68151d0f51add3813f3f7d1b", "name": "MicrosoftResearch", "fullname": "Microsoft Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86LLM-in-Sandbox\uff0c\u5141\u8bb8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u6c99\u7bb1\u4e2d\u63a2\u7d22\uff0c\u4ee5\u63d0\u9ad8\u975e\u4ee3\u7801\u9886\u57df\u7684\u667a\u80fd\u3002</li>\n    <li>\u5f3a\u5927\u7684LLMs\u53ef\u4ee5\u5728\u6ca1\u6709\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4ee3\u7801\u6c99\u7bb1\u5904\u7406\u975e\u4ee3\u7801\u4efb\u52a1\uff0c\u4f8b\u5982\u83b7\u53d6\u65b0\u77e5\u8bc6\u548c\u5904\u7406\u957f\u6587\u672c\u3002</li>\n    <li>\u901a\u8fc7LLM-in-Sandbox\u5f3a\u5316\u5b66\u4e60\uff08LLM-in-Sandbox-RL\uff09\uff0c\u53ef\u4ee5\u589e\u5f3a\u8fd9\u4e9b\u6a21\u578b\u7684\u63a2\u7d22\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cLLM-in-Sandbox\u5728\u6570\u5b66\u3001\u7269\u7406\u3001\u5316\u5b66\u3001\u751f\u7269\u533b\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u90fd\u80fd\u5b9e\u73b0\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002</li>\n    <li>\u5206\u6790\u4e86LLM-in-Sandbox\u7684\u8ba1\u7b97\u548c\u7cfb\u7edf\u6548\u7387\uff0c\u5e76\u5f00\u6e90\u4e3aPython\u5305\uff0c\u4fbf\u4e8e\u5b9e\u9645\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LLM-in-Sandbox lets large language models (LLMs) work in a safe virtual environment to improve their intelligence in areas beyond just coding.</li>\n    <li>These LLMs can learn and adapt without extra training, using the sandbox to access new information and manage complex tasks.</li>\n    <li>We enhance the LLMs' abilities through a method called LLM-in-Sandbox Reinforcement Learning, which trains them using non-agentic data.</li>\n    <li>Tests show that LLM-in-Sandbox can effectively handle various subjects like math, science, and following instructions, both with and without additional training.</li>\n    <li>The system's efficiency is evaluated, and it is made available as an open-source Python package for real-world use.</li>\n</ul>"}, "publishedAt": "2026-01-22T13:57:09.000Z", "title": "LLM-in-Sandbox Elicits General Agentic Intelligence", "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16206.png", "numComments": 2, "submittedBy": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "fullname": "Daixuan Cheng", "name": "daixuancheng", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 16, "isUserFollowing": false}, "organization": {"_id": "68151d0f51add3813f3f7d1b", "name": "MicrosoftResearch", "fullname": "Microsoft Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.17737", "authors": [{"_id": "6978310b026bdf0473116e44", "user": {"_id": "64545c77a7ce0a8fde809912", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VDaMEM77Xv09dP6B5v3sK.jpeg", "isPro": false, "fullname": "ChenYuMu", "user": "ChenYuMu", "type": "user"}, "name": "Chenyu Mu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:01:21.462Z", "hidden": false}, {"_id": "6978310b026bdf0473116e45", "user": {"_id": "6527a2df1eb78901534b0cc6", "avatarUrl": "/avatars/f811d8c108930b41e2612c609d35e2eb.svg", "isPro": false, "fullname": "Xin He", "user": "Kleinhe", "type": "user"}, "name": "Xin He", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:03:20.414Z", "hidden": false}, {"_id": "6978310b026bdf0473116e46", "user": {"_id": "64300415b009240418dac70c", "avatarUrl": "/avatars/5175cdbc7683b0b52d5c742e93d3be83.svg", "isPro": false, "fullname": "Qu Yang", "user": "quyang22", "type": "user"}, "name": "Qu Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:03:22.475Z", "hidden": false}, {"_id": "6978310b026bdf0473116e47", "name": "Wanshun Chen", "hidden": false}, {"_id": "6978310b026bdf0473116e48", "name": "Jiadi Yao", "hidden": false}, {"_id": "6978310b026bdf0473116e49", "name": "Huang Liu", "hidden": false}, {"_id": "6978310b026bdf0473116e4a", "name": "Zihao Yi", "hidden": false}, {"_id": "6978310b026bdf0473116e4b", "name": "Bo Zhao", "hidden": false}, {"_id": "6978310b026bdf0473116e4c", "name": "Xingyu Chen", "hidden": false}, {"_id": "6978310b026bdf0473116e4d", "name": "Ruotian Ma", "hidden": false}, {"_id": "6978310b026bdf0473116e4e", "name": "Fanghua Ye", "hidden": false}, {"_id": "6978310b026bdf0473116e4f", "name": "Erkun Yang", "hidden": false}, {"_id": "6978310b026bdf0473116e50", "name": "Cheng Deng", "hidden": false}, {"_id": "6978310b026bdf0473116e51", "name": "Zhaopeng Tu", "hidden": false}, {"_id": "6978310b026bdf0473116e52", "name": "Xiaolong Li", "hidden": false}, {"_id": "6978310b026bdf0473116e53", "name": "Linus", "hidden": false}], "publishedAt": "2026-01-25T08:10:28.000Z", "submittedOnDailyAt": "2026-01-27T01:05:46.612Z", "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation", "submittedOnDailyBy": {"_id": "67485743561b1e6f9579389f", "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg", "isPro": false, "fullname": "Zhaopeng Tu", "user": "zptu", "type": "user"}, "summary": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.", "upvotes": 46, "discussionId": "6978310b026bdf0473116e54", "projectPage": "https://xd-mu.github.io/ScriptIsAllYouNeed/", "githubRepo": "https://github.com/Tencent/digitalhuman/tree/main/ScriptAgent", "githubRepoAddedBy": "user", "ai_summary": "A novel end-to-end agentic framework translates dialogue into cinematic videos through specialized agents that generate and orchestrate video content while maintaining narrative coherence.", "ai_keywords": ["video generation", "dialogue-to-cinematic-video", "ScripterAgent", "DirectorAgent", "cross-scene continuous generation", "ScriptBench", "Visual-Script Alignment", "CriticAgent"], "githubStars": 228, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u5c06\u5bf9\u8bdd\u8f6c\u6362\u4e3a\u7535\u5f71\u89c6\u9891\u751f\u6210\u3002</li>\n    <li>\u6838\u5fc3\u7ec4\u4ef6\u662fScripterAgent\uff0c\u5b83\u5c06\u7c97\u7565\u5bf9\u8bdd\u8f6c\u5316\u4e3a\u7cbe\u7ec6\u7684\u7535\u5f71\u5267\u672c\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6ScriptBench\uff0c\u7528\u4e8e\u63d0\u4f9b\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u3002</li>\n    <li>\u751f\u6210\u7684\u5267\u672c\u6307\u5bfcDirectorAgent\uff0c\u786e\u4fdd\u89c6\u9891\u5185\u5bb9\u7684\u8fde\u8d2f\u6027\u3002</li>\n    <li>\u6211\u4eec\u7684\u8bc4\u4f30\u663e\u793a\u6846\u67b6\u5728\u5267\u672c\u5fe0\u5b9e\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u6709\u663e\u8457\u6539\u5584\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New models can create impressive videos from text but struggle with long, coherent stories.</li>\n    <li>We developed a framework to improve video generation from dialogue, called ScripterAgent.</li>\n    <li>ScripterAgent turns rough dialogue into detailed scripts for videos.</li>\n    <li>We created a large benchmark, ScriptBench, to help train and evaluate these models.</li>\n    <li>Our framework improves video quality and shows the balance between visuals and script accuracy.</li>\n</ul>"}, "publishedAt": "2026-01-25T03:10:28.000Z", "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation", "summary": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17737.png", "numComments": 3, "submittedBy": {"_id": "67485743561b1e6f9579389f", "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg", "fullname": "Zhaopeng Tu", "name": "zptu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16208", "authors": [{"_id": "6972ea8bfb12c92b735b74a8", "name": "Shengbang Tong", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74a9", "name": "Boyang Zheng", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74aa", "user": {"_id": "64249f76d476e4ad55665d59", "avatarUrl": "/avatars/a0fec7e423ffae944a874ca267b55c1f.svg", "isPro": false, "fullname": "Ziteng Wang", "user": "AustinWang0330", "type": "user"}, "name": "Ziteng Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:01.136Z", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ab", "name": "Bingda Tang", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ac", "name": "Nanye Ma", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ad", "user": {"_id": "626dc5105f7327906f0b2a4e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626dc5105f7327906f0b2a4e/QCSzuwYqsv8ozRnusVb-F.jpeg", "isPro": true, "fullname": "Ellis Brown", "user": "ellisbrown", "type": "user"}, "name": "Ellis Brown", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T20:15:10.637Z", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ae", "name": "Jihan Yang", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74af", "name": "Rob Fergus", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74b0", "name": "Yann LeCun", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74b1", "name": "Saining Xie", "hidden": false}], "publishedAt": "2026-01-22T18:58:16.000Z", "submittedOnDailyAt": "2026-01-23T00:57:17.761Z", "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "submittedOnDailyBy": {"_id": "6434226da4c9c55871a78052", "avatarUrl": "/avatars/3309832b3115bc6ad08ae1d10f43118b.svg", "isPro": false, "fullname": "BoYang Zheng", "user": "bytetriper", "type": "user"}, "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.", "upvotes": 40, "discussionId": "6972ea8cfb12c92b735b74b2", "projectPage": "https://rae-dit.github.io/scale-rae/", "githubRepo": "https://github.com/ZitengWangNYU/Scale-RAE", "githubRepoAddedBy": "user", "ai_summary": "Representation Autoencoders (RAEs) demonstrate superior performance over VAEs in large-scale text-to-image generation, showing improved stability, faster convergence, and better quality while enabling unified multimodal reasoning in shared representation spaces.", "ai_keywords": ["representation autoencoders", "diffusion modeling", "semantic latent spaces", "text-to-image generation", "frozen representation encoder", "SigLIP-2", "noise scheduling", "diffusion transformers", "pretraining", "finetuning", "catastrophic overfitting", "multimodal model", "shared representation space"], "githubStars": 58, "organization": {"_id": "662741612ada5b77e310d171", "name": "nyu-visionx", "fullname": "VISIONx @ NYU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"}, "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u8868\u793a\u81ea\u7f16\u7801\u5668\uff08RAE\uff09\u5728\u5927\u89c4\u6a21\u81ea\u7531\u5f62\u5f0f\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5e94\u7528\u3002</li>\n    <li>\u901a\u8fc7\u5728\u7f51\u7edc\u3001\u5408\u6210\u548c\u6587\u672c\u6e32\u67d3\u6570\u636e\u4e0a\u8bad\u7ec3RAE\u89e3\u7801\u5668\uff0c\u53d1\u73b0\u7279\u5b9a\u9886\u57df\uff08\u5982\u6587\u672c\uff09\u7684\u6570\u636e\u7ec4\u6210\u5bf9\u4e8e\u63d0\u5347\u751f\u6210\u6548\u679c\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u5bf9RAE\u8bbe\u8ba1\u9009\u62e9\u8fdb\u884c\u4e25\u683c\u6d4b\u8bd5\uff0c\u53d1\u73b0\u7b80\u5316\u7684\u6846\u67b6\u5728\u6269\u5c55\u540e\u66f4\u6709\u6548\uff0c\u5c3d\u7ba1\u566a\u58f0\u8c03\u5ea6\u4f9d\u7136\u91cd\u8981\uff0c\u4f46\u590d\u6742\u7684\u67b6\u6784\u6539\u8fdb\u6548\u679c\u4e0d\u660e\u663e\u3002</li>\n    <li>\u4e0e\u6700\u5148\u8fdb\u7684FLUX VAE\u8fdb\u884c\u6bd4\u8f83\uff0cRAE\u5728\u6240\u6709\u6a21\u578b\u89c4\u6a21\u4e0b\u7684\u9884\u8bad\u7ec3\u8868\u73b0\u4f18\u4e8eVAE\uff0c\u4e14\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2dRAE\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u90fd\u66f4\u597d\u3002</li>\n    <li>RAE\u6a21\u578b\u5728\u751f\u6210\u8d28\u91cf\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u8868\u73b0\u66f4\u4f73\uff0c\u4e3a\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u57fa\u7840\uff0c\u540c\u65f6\u53ef\u4ee5\u5728\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u4e2d\u8fdb\u884c\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Representation Autoencoders (RAEs) are effective for creating images from text by using high-dimensional data.</li>\n    <li>The study tests RAEs on large-scale text-to-image generation, finding that while more data helps, specific types of data are important for good results.</li>\n    <li>RAEs are simpler to use at scale compared to earlier designs, with less complex features providing little extra benefit.</li>\n    <li>RAEs perform better than Variational Autoencoders (VAEs) during training and fine-tuning, with better stability and image quality.</li>\n    <li>RAEs can also combine visual understanding and generation, allowing for advanced model capabilities in creating images from text.</li>\n</ul>"}, "publishedAt": "2026-01-22T13:58:16.000Z", "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16208.png", "numComments": 1, "submittedBy": {"_id": "6434226da4c9c55871a78052", "avatarUrl": "/avatars/3309832b3115bc6ad08ae1d10f43118b.svg", "fullname": "BoYang Zheng", "name": "bytetriper", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "662741612ada5b77e310d171", "name": "nyu-visionx", "fullname": "VISIONx @ NYU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.15892", "authors": [{"_id": "6972d788fb12c92b735b7397", "user": {"_id": "641aa5e391e3376a057bbd4c", "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg", "isPro": false, "fullname": "Chenghao Fan", "user": "Facico", "type": "user"}, "name": "Chenghao Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:32.109Z", "hidden": false}, {"_id": "6972d788fb12c92b735b7398", "name": "Wen Heng", "hidden": false}, {"_id": "6972d788fb12c92b735b7399", "name": "Bo Li", "hidden": false}, {"_id": "6972d788fb12c92b735b739a", "name": "Sichen Liu", "hidden": false}, {"_id": "6972d788fb12c92b735b739b", "name": "Yuxuan Song", "hidden": false}, {"_id": "6972d788fb12c92b735b739c", "name": "Jing Su", "hidden": false}, {"_id": "6972d788fb12c92b735b739d", "name": "Xiaoye Qu", "hidden": false}, {"_id": "6972d788fb12c92b735b739e", "name": "Kai Shen", "hidden": false}, {"_id": "6972d788fb12c92b735b739f", "name": "Wei Wei", "hidden": false}], "publishedAt": "2026-01-22T12:13:17.000Z", "submittedOnDailyAt": "2026-01-23T00:09:35.389Z", "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "submittedOnDailyBy": {"_id": "641aa5e391e3376a057bbd4c", "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg", "isPro": false, "fullname": "Chenghao Fan", "user": "Facico", "type": "user"}, "summary": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.", "upvotes": 40, "discussionId": "6972d788fb12c92b735b73a0", "projectPage": "https://bytedance-seed.github.io/Stable-DiffCoder/", "githubRepo": "https://github.com/ByteDance-Seed/Stable-DiffCoder", "githubRepoAddedBy": "user", "ai_summary": "Stable-DiffCoder demonstrates superior code modeling performance compared to autoregressive baselines through block diffusion continual pretraining and efficient training mechanisms.", "ai_keywords": ["diffusion-based language models", "autoregressive models", "block diffusion", "continual pretraining", "warmup", "clipped noise schedule", "supervised fine-tuning", "code modeling", "structured code modeling", "data augmentation"], "githubStars": 16, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u6269\u6563\u57fa\u7840\u8bed\u8a00\u6a21\u578b\uff08DLLMs\uff09\u76f8\u6bd4\u81ea\u56de\u5f52\u6a21\u578b\uff08AR\uff09\u5177\u6709\u66f4\u9ad8\u7684\u6570\u636e\u91cd\u7528\u548c\u5757\u72b6\u751f\u6210\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86Stable-DiffCoder\uff0c\u4e00\u4e2a\u57fa\u4e8e\u5757\u6269\u6563\u7684\u4ee3\u7801\u6a21\u578b\uff0c\u4f7f\u7528\u4e86Seed-Coder\u67b6\u6784\u548c\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>\u4e3a\u4e86\u63d0\u9ad8\u77e5\u8bc6\u5b66\u4e60\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5757\u6269\u6563\u6301\u7eed\u9884\u8bad\u7ec3\uff08CPT\uff09\u9636\u6bb5\u3002</li>\n    <li>\u5728\u76f8\u540c\u7684\u6570\u636e\u548c\u67b6\u6784\u4e0b\uff0cStable-DiffCoder\u5728\u591a\u9879\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6574\u4f53\u8868\u73b0\u4f18\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\u3002</li>\n    <li>\u6269\u6563\u57fa\u7840\u7684\u5efa\u6a21\u65b9\u6cd5\u6539\u5584\u4e86\u7ed3\u6784\u5316\u4ee3\u7801\u7684\u7f16\u8f91\u548c\u63a8\u7406\uff0c\u4e14\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u5bf9\u4f4e\u8d44\u6e90\u7f16\u7a0b\u8bed\u8a00\u6709\u76ca\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion-based language models (DLLMs) can generate code in blocks rather than sequentially, allowing for better data reuse compared to autoregressive (AR) models.</li>\n    <li>The study introduces Stable-DiffCoder, which enhances the Seed-Coder architecture and training process for better performance.</li>\n    <li>Stable-DiffCoder includes a special pretraining stage that helps with learning effectively and stabilizes training.</li>\n    <li>When tested, Stable-DiffCoder performs better than its AR equivalent on various code tests, even with similar data and architecture.</li>\n    <li>This model not only outperforms many existing AR and DLLM models but also improves code editing and reasoning, especially for less common coding languages.</li>\n</ul>"}, "publishedAt": "2026-01-22T07:13:17.000Z", "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "summary": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15892.png", "numComments": 0, "submittedBy": {"_id": "641aa5e391e3376a057bbd4c", "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg", "fullname": "Chenghao Fan", "name": "Facico", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13, "isUserFollowing": false}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u73b0\u5b9e\u4e2d\u7684\u89c6\u9891\u95ee\u7b54\u9700\u8981\u4ece\u89c6\u9891\u4e2d\u63d0\u53d6\u7ebf\u7d22\uff0c\u5e76\u5728\u7f51\u7edc\u4e0a\u68c0\u7d22\u548c\u9a8c\u8bc1\u4fe1\u606f\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0c\u79f0\u4e3aVideoDR\uff0c\u4e13\u6ce8\u4e8e\u89c6\u9891\u95ee\u7b54\u3002</li>\n    <li>VideoDR\u9700\u8981\u8de8\u5e27\u89c6\u89c9\u4fe1\u606f\u63d0\u53d6\u3001\u4e92\u52a8\u7f51\u7edc\u68c0\u7d22\u548c\u591a\u6b65\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0Agentic\u6a21\u5f0f\u7684\u6548\u679c\u4f9d\u8d56\u4e8e\u6a21\u578b\u4fdd\u6301\u89c6\u9891\u7ebf\u7d22\u7684\u80fd\u529b\u3002</li>\n    <li>VideoDR\u63ed\u793a\u4e86\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u57fa\u51c6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video question answering often requires understanding both video content and information from the internet.</li>\n    <li>VideoDR is a new benchmark designed for answering questions based on videos and web information.</li>\n    <li>It includes tasks like extracting visual clues from multiple video frames and retrieving information from the web.</li>\n    <li>The study evaluated various language models and found that performance varies based on how well a model retains key video information during retrieval.</li>\n    <li>VideoDR highlights important challenges for developing future video research agents.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u76ee\u524d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\uff0c\u8868\u73b0\u4e0d\u4f73\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86BabyVision\uff0c\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30MLLMs\u7684\u57fa\u672c\u89c6\u89c9\u80fd\u529b\uff0c\u800c\u4e0d\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u3002</li>\n    <li>BabyVision\u5305\u542b388\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\uff0c\u8986\u76d6\u56db\u4e2a\u4e3b\u8981\u7c7b\u522b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u9886\u5148\u7684MLLMs\u5728\u8fd9\u4e9b\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u5c24\u5176\u662f6\u5c81\u513f\u7ae5\u3002</li>\n    <li>BabyVision\u7684\u8fdb\u5c55\u4e3a\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7684\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Humans develop basic visual skills before learning language, but Multimodal LLMs (MLLMs) struggle with visual tasks.</li>\n  <li>BabyVision is a new benchmark that tests visual abilities in MLLMs without relying on language skills.</li>\n  <li>It includes 388 tasks in 22 subclasses across four main categories.</li>\n  <li>Top MLLMs score much lower than humans on visual tasks, with Gemini3-Pro-Preview scoring 49.7, compared to 94.1 for average adults.</li>\n  <li>BabyVision aims to improve MLLMs' visual perception and reasoning, and resources are available for further research.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u8bb8\u591a\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\uff0c\u5206\u5272\u8fd9\u4e9b\u5b9e\u4f53\u5bf9\u5e94\u7528\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u73b0\u6709\u7684\u5206\u5272\u6a21\u578b\u80fd\u5904\u7406\u7269\u7406\u5c5e\u6027\u7684\u5b9e\u4f53\uff0c\u4f46\u5728\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\u4e0a\u4ecd\u6709\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6SocioSeg\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86SocioReasoner\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8de8\u6a21\u6001\u8bc6\u522b\u548c\u591a\u9636\u6bb5\u63a8\u7406\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6211\u4eec\u7684\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5e76\u5177\u5907\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban areas have many types of features, and it's important to identify them from satellite images.</li>\n    <li>Current models can find physical features well but struggle with social categories like schools and parks.</li>\n    <li>We created a new dataset called SocioSeg with satellite images and detailed labels for social features.</li>\n    <li>Our new framework, SocioReasoner, helps better identify social entities by mimicking human reasoning.</li>\n    <li>Experiments show our method performs better than existing models and can adapt to new situations easily.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.16725", "authors": [{"_id": "6976d5405d41524304c13537", "name": "Meituan LongCat Team", "hidden": false}, {"_id": "6976d5405d41524304c13538", "name": "Anchun Gui", "hidden": false}, {"_id": "6976d5405d41524304c13539", "name": "Bei Li", "hidden": false}, {"_id": "6976d5405d41524304c1353a", "name": "Bingyang Tao", "hidden": false}, {"_id": "6976d5405d41524304c1353b", "name": "Bole Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1353c", "name": "Borun Chen", "hidden": false}, {"_id": "6976d5405d41524304c1353e", "name": "Chao Zhang", "hidden": false}, {"_id": "69772bc15d41524304c13739", "name": "Chao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1353f", "name": "Chen Gao", "hidden": false}, {"_id": "6976d5405d41524304c13540", "name": "Chen Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13541", "name": "Chengcheng Han", "hidden": false}, {"_id": "6976d5405d41524304c13542", "name": "Chenhui Yang", "hidden": false}, {"_id": "6976d5405d41524304c13543", "name": "Chuyu Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13544", "name": "Cong Chen", "hidden": false}, {"_id": "6976d5405d41524304c13545", "name": "Cunguang Wang", "hidden": false}, {"_id": "6976d5405d41524304c13546", "name": "Daoru Pan", "hidden": false}, {"_id": "6976d5405d41524304c13547", "name": "Defei Bu", "hidden": false}, {"_id": "6976d5405d41524304c13548", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13549", "name": "Di Xiu", "hidden": false}, {"_id": "6976d5405d41524304c1354a", "name": "Dishan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1354b", "name": "Dongyu Ru", "hidden": false}, {"_id": "6976d5405d41524304c1354c", "name": "Dunwei Tu", "hidden": false}, {"_id": "6976d5405d41524304c1354d", "name": "Fan Wu", "hidden": false}, {"_id": "6976d5405d41524304c1354e", "name": "Fengcheng Yuan", "hidden": false}, {"_id": "6976d5405d41524304c1354f", "name": "Fengcun Li", "hidden": false}, {"_id": "6976d5405d41524304c13550", "name": "Gang Xu", "hidden": false}, {"_id": "6976d5405d41524304c13551", "name": "Guanyu Wu", "hidden": false}, {"_id": "6976d5405d41524304c13552", "name": "Guoyuan Lin", "hidden": false}, {"_id": "6976d5405d41524304c13553", "name": "Haibin Wang", "hidden": false}, {"_id": "6976d5405d41524304c13554", "name": "Hansi Yang", "hidden": false}, {"_id": "6976d5405d41524304c13555", "name": "Hao Yang", "hidden": false}, {"_id": "6976d5405d41524304c13556", "name": "Haonan Yan", "hidden": false}, {"_id": "6976d5405d41524304c13557", "name": "Haoxiang Ma", "hidden": false}, {"_id": "6976d5405d41524304c13558", "name": "Haoxing Wen", "hidden": false}, {"_id": "6976d5405d41524304c13559", "name": "Hongyan Hao", "hidden": false}, {"_id": "6976d5405d41524304c1355a", "name": "Hongyin Tang", "hidden": false}, {"_id": "6976d5405d41524304c1355b", "name": "Hongyu Zang", "hidden": false}, {"_id": "6976d5405d41524304c1355c", "name": "Hongzhi Ni", "hidden": false}, {"_id": "6976d5405d41524304c1355d", "name": "Hui Su", "hidden": false}, {"_id": "6976d5405d41524304c1355e", "name": "Jiacheng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1355f", "name": "Jiahong Zhou", "hidden": false}, {"_id": "6976d5405d41524304c13560", "name": "Jiahuan Li", "hidden": false}, {"_id": "6976d5405d41524304c13561", "name": "Jiaming Wang", "hidden": false}, {"_id": "6976d5405d41524304c13562", "name": "Jian Yang", "hidden": false}, {"_id": "6976d5405d41524304c13563", "user": {"_id": "64008a0af4ff62c2616d8858", "avatarUrl": "/avatars/b52c98857916fba5377ace8089d658b2.svg", "isPro": false, "fullname": "zhangjf", "user": "zhangjf", "type": "user"}, "name": "Jianfei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:09.272Z", "hidden": false}, {"_id": "6976d5405d41524304c13564", "name": "Jianhao Xu", "hidden": false}, {"_id": "6976d5405d41524304c13565", "name": "Jianing Wang", "hidden": false}, {"_id": "6976d5405d41524304c13566", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13567", "name": "Jiaqi Sun", "hidden": false}, {"_id": "6976d5405d41524304c13568", "name": "Jiarong Shi", "hidden": false}, {"_id": "6976d5405d41524304c13569", "name": "Jiarui Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1356a", "name": "Jingang Wang", "hidden": false}, {"_id": "6976d5405d41524304c1356b", "user": {"_id": "6592472fccbc1e2cc7250903", "avatarUrl": "/avatars/6f04ae66944eb2ce65c5aca7927bab10.svg", "isPro": false, "fullname": "Jinluan Yang", "user": "Jinluan", "type": "user"}, "name": "Jinluan Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:47.175Z", "hidden": false}, {"_id": "6976d5405d41524304c1356c", "name": "Jinrui Ding", "hidden": false}, {"_id": "6976d5405d41524304c1356d", "name": "Jinwei Xiao", "hidden": false}, {"_id": "6976d5405d41524304c1356e", "name": "Jiyuan He", "hidden": false}, {"_id": "6976d5405d41524304c1356f", "name": "Juncan Xu", "hidden": false}, {"_id": "6976d5405d41524304c13570", "name": "Kefeng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13571", "name": "Keheng Wang", "hidden": false}, {"_id": "6976d5405d41524304c13572", "name": "Li Wei", "hidden": false}, {"_id": "6976d5405d41524304c13573", "name": "Lianhui Ma", "hidden": false}, {"_id": "6976d5405d41524304c13574", "name": "Lin Qiu", "hidden": false}, {"_id": "6976d5405d41524304c13575", "name": "Lingbing Kong", "hidden": false}, {"_id": "6976d5405d41524304c13576", "name": "Lingchuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c13577", "name": "Linsen Guo", "hidden": false}, {"_id": "6976d5405d41524304c13578", "name": "Mengshen Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13579", "name": "Mengxia Shen", "hidden": false}, {"_id": "6976d5405d41524304c1357a", "name": "Mingyang Zhu", "hidden": false}, {"_id": "6976d5405d41524304c1357b", "name": "Peiguang Li", "hidden": false}, {"_id": "6976d5405d41524304c1357c", "name": "Peng Pei", "hidden": false}, {"_id": "6976d5405d41524304c1357d", "name": "Pengcheng Jia", "hidden": false}, {"_id": "6976d5405d41524304c1357e", "name": "Pengtao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1357f", "name": "Peng Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13580", "name": "Qi Gu", "hidden": false}, {"_id": "6976d5405d41524304c13581", "name": "Qiong Huang", "hidden": false}, {"_id": "6976d5405d41524304c13582", "name": "Qiyuan Duan", "hidden": false}, {"_id": "6976d5405d41524304c13583", "name": "Quanchi Weng", "hidden": false}, {"_id": "6976d5405d41524304c13584", "name": "Rongxiang Weng", "hidden": false}, {"_id": "6976d5405d41524304c13585", "name": "Rongzhi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13586", "name": "Rumei Li", "hidden": false}, {"_id": "6976d5405d41524304c13587", "name": "Shanglin Lei", "hidden": false}, {"_id": "6976d5405d41524304c13588", "user": {"_id": "64db5f5dd68a6ddcc7bd89e9", "avatarUrl": "/avatars/69375ec915927b855813df8a6d486837.svg", "isPro": false, "fullname": "Shengnan An", "user": "ShengnanAn", "type": "user"}, "name": "Shengnan An", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:11.410Z", "hidden": false}, {"_id": "6976d5405d41524304c13589", "name": "Shijun Dai", "hidden": false}, {"_id": "6976d5405d41524304c1358a", "name": "Shuaikang Liu", "hidden": false}, {"_id": "6976d5405d41524304c1358b", "name": "Shuang Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1358c", "name": "Shuo Wang", "hidden": false}, {"_id": "6976d5405d41524304c1358d", "name": "Songyuan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1358e", "name": "Tao Liang", "hidden": false}, {"_id": "6976d5405d41524304c1358f", "name": "Tianhao Hu", "hidden": false}, {"_id": "6976d5405d41524304c13590", "name": "Tianze Chen", "hidden": false}, {"_id": "6976d5405d41524304c13591", "name": "Wei Liu", "hidden": false}, {"_id": "6976d5405d41524304c13592", "name": "Wei Shi", "hidden": false}, {"_id": "6976d5405d41524304c13593", "name": "Wei Wang", "hidden": false}, {"_id": "6976d5405d41524304c13594", "name": "Weifeng Tang", "hidden": false}, {"_id": "6976d5405d41524304c13595", "name": "Wenjie Shi", "hidden": false}, {"_id": "6976d5405d41524304c13596", "name": "Wenlong Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13597", "name": "Wentao Chen", "hidden": false}, {"_id": "6976d5405d41524304c13598", "name": "Wentao Shi", "hidden": false}, {"_id": "6976d5405d41524304c13599", "name": "Xi Su", "hidden": false}, {"_id": "6976d5405d41524304c1359a", "name": "Xiangcheng Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359b", "name": "Xiandi Ma", "hidden": false}, {"_id": "6976d5405d41524304c1359c", "user": {"_id": "63edb098679c2cc40abc6c2e", "avatarUrl": "/avatars/288c7229937c2c3f29fda6d17c7df2eb.svg", "isPro": false, "fullname": "Xiangyu", "user": "xixy", "type": "user"}, "name": "Xiangyu Xi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:13.312Z", "hidden": false}, {"_id": "6976d5405d41524304c1359d", "name": "Xiangyuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359e", "name": "Xiangzhou Huang", "hidden": false}, {"_id": "6976d5405d41524304c1359f", "name": "Xiao Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a0", "name": "Xiaodong Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a1", "name": "Xiaolong Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a2", "name": "Xiaowei Shi", "hidden": false}, {"_id": "6976d5405d41524304c135a3", "name": "Xiaoyu Li", "hidden": false}, {"_id": "6976d5405d41524304c135a4", "name": "Xin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a5", "name": "Xingchen Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a6", "name": "Xuan Huang", "hidden": false}, {"_id": "6976d5405d41524304c135a7", "name": "Xuezhi Cao", "hidden": false}, {"_id": "6976d5405d41524304c135a8", "name": "Xunliang Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a9", "name": "Yan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135aa", "user": {"_id": "63fc1c420aab06079200c15c", "avatarUrl": "/avatars/8e8e82a9a6552848581ca9f65011263c.svg", "isPro": false, "fullname": "yang bai", "user": "byang", "type": "user"}, "name": "Yang Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:07.036Z", "hidden": false}, {"_id": "6976d5405d41524304c135ab", "name": "Yang Liu", "hidden": false}, {"_id": "6976d5405d41524304c135ac", "name": "Yang Yang", "hidden": false}, {"_id": "6976d5405d41524304c135ad", "name": "Yang Zheng", "hidden": false}, {"_id": "6976d5405d41524304c135ae", "name": "Yaoming Wang", "hidden": false}, {"_id": "6976d5405d41524304c135af", "name": "Yaoming Zhu", "hidden": false}, {"_id": "6976d5405d41524304c135b0", "name": "Yaqi Huo", "hidden": false}, {"_id": "6976d5405d41524304c135b1", "name": "Yanyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b2", "name": "Yaorui Shi", "hidden": false}, {"_id": "6976d5405d41524304c135b3", "name": "Yerui Sun", "hidden": false}, {"_id": "6976d5405d41524304c135b4", "name": "Yi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b5", "name": "Yihao Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b6", "name": "Yi-Kai Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b7", "name": "Yifan Lu", "hidden": false}, {"_id": "6976d5405d41524304c135b8", "name": "Yifan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135b9", "name": "Yitao Zhai", "hidden": false}, {"_id": "6976d5405d41524304c135ba", "name": "Yongjing Yin", "hidden": false}, {"_id": "6976d5405d41524304c135bb", "name": "Yongwei Zhou", "hidden": false}, {"_id": "6976d5405d41524304c135bc", "name": "Youshao Xiao", "hidden": false}, {"_id": "6976d5405d41524304c135bd", "name": "Yuchuan Dai", "hidden": false}, {"_id": "6976d5405d41524304c135be", "name": "Yuchen Xie", "hidden": false}, {"_id": "6976d5405d41524304c135bf", "name": "Yuchen Yu", "hidden": false}, {"_id": "6976d5405d41524304c135c0", "name": "Yufei Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135c1", "name": "Yuhuai Wei", "hidden": false}, {"_id": "6976d5405d41524304c135c2", "name": "Yulei Qian", "hidden": false}, {"_id": "6976d5405d41524304c135c3", "name": "Yunfan Liang", "hidden": false}, {"_id": "6976d5405d41524304c135c4", "name": "Yunke Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135c5", "name": "Yuwei Jiang", "hidden": false}, {"_id": "6976d5405d41524304c135c6", "name": "Yuxin Bian", "hidden": false}, {"_id": "6976d5405d41524304c135c7", "name": "Yuxin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135c8", "name": "Yuxin Liu", "hidden": false}, {"_id": "6976d5405d41524304c135c9", "name": "Yue Xu", "hidden": false}, {"_id": "6976d5405d41524304c135ca", "name": "Yueqing Sun", "hidden": false}, {"_id": "6976d5405d41524304c135cb", "name": "Zeyang Yu", "hidden": false}, {"_id": "6976d5405d41524304c135cc", "name": "Zhao Yang", "hidden": false}, {"_id": "6976d5405d41524304c135cd", "name": "Zhengsheng Huang", "hidden": false}, {"_id": "6976d5405d41524304c135ce", "name": "Zhengyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135cf", "name": "Zhijian Liu", "hidden": false}, {"_id": "6976d5405d41524304c135d0", "name": "Zhikang Xia", "hidden": false}, {"_id": "6976d5405d41524304c135d1", "name": "Zhimin Lin", "hidden": false}, {"_id": "6976d5405d41524304c135d2", "name": "Zhiyuan Yao", "hidden": false}, {"_id": "6976d5405d41524304c135d3", "name": "Zhuofan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135d4", "name": "Zhuowen Han", "hidden": false}, {"_id": "6976d5405d41524304c135d5", "name": "Zijian Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135d6", "name": "Ziran Li", "hidden": false}, {"_id": "6976d5405d41524304c135d7", "name": "Ziwen Wang", "hidden": false}, {"_id": "6976d5405d41524304c135d8", "name": "Ziyuan Zhuang", "hidden": false}], "publishedAt": "2026-01-23T13:20:09.000Z", "submittedOnDailyAt": "2026-01-26T00:15:28.340Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "upvotes": 136, "discussionId": "6976d5405d41524304c135d9", "ai_summary": "A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for real-world robustness and complex reasoning.", "ai_keywords": ["Mixture-of-Experts", "agentic reasoning", "domain-parallel expert training", "fusion", "asynchronous reinforcement learning", "DORA", "long-tailed generation", "multi-turn interactions", "real-world noise patterns", "test-time scaling", "reasoning depth", "reasoning width", "parallel thinking"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>LongCat-Flash-Thinking-2601\u662f\u4e00\u4e2a\u62e5\u67095600\u4ebf\u53c2\u6570\u7684\u5f00\u6e90\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u63a8\u7406\u6a21\u578b\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u667a\u80fd\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u591a\u79cd\u667a\u80fd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u667a\u80fd\u641c\u7d22\u548c\u5de5\u5177\u4f7f\u7528\u7b49\u3002</li>\n    <li>\u6a21\u578b\u5728\u590d\u6742\u5de5\u5177\u4ea4\u4e92\u548c\u5608\u6742\u7684\u73b0\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u901a\u8fc7\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6\u548c\u7cfb\u7edf\u5316\u7684\u589e\u5f3a\u5b66\u4e60\u65b9\u6cd5\uff0c\u6a21\u578b\u80fd\u591f\u5728\u8d85\u8fc710000\u4e2a\u73af\u5883\u4e2d\u8fdb\u884c\u6709\u6548\u8bad\u7ec3\u3002</li>\n    <li>\u5f15\u5165\u4e86\u91cd\u601d\u7ef4\u6a21\u5f0f\uff0c\u80fd\u591f\u5728\u6d4b\u8bd5\u65f6\u901a\u8fc7\u5e76\u884c\u63a8\u7406\u589e\u52a0\u63a8\u7406\u6df1\u5ea6\u548c\u5e7f\u5ea6\uff0c\u4ece\u800c\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongCat-Flash-Thinking-2601 is a large open-source reasoning model with 560 billion parameters, designed for better reasoning abilities.</li>\n    <li>It performs exceptionally well on various benchmarks related to agentic reasoning, such as searching and using tools.</li>\n    <li>The model is built using a special training framework that helps it learn from different environments and tasks effectively.</li>\n    <li>It is designed to handle real-world challenges, such as noise and complex interactions with tools.</li>\n    <li>A new feature called Heavy Thinking mode allows the model to think more deeply and broadly during tests, improving its reasoning skills.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:20:09.000Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16725.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u5e73\u8861\u7d27\u51d1\u6548\u7387\u548c\u591a\u6a21\u6001\u667a\u80fd\u3002</li>\n    <li>\u8be5\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u671f\u8bad\u7ec3\u7ba1\u9053\u5b9e\u73b0\u3002</li>\n    <li>\u5f15\u5165\u4e86\u5e76\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\uff0c\u63d0\u5347\u4e86\u6d4b\u8bd5\u65f6\u7684\u8ba1\u7b97\u6548\u7387\u3002</li>\n    <li>\u5c3d\u7ba1\u6a21\u578b\u4f53\u79ef\u4ec5\u4e3a10\u4ebf\u53c2\u6570\uff0c\u4f46\u5176\u6027\u80fd\u8d85\u8fc7\u4e8610\u523020\u500d\u66f4\u5927\u7684\u6a21\u578b\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u91cd\u590d\u7684\u57fa\u7ebf\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a new open-source model that balances compact size with advanced multimodal intelligence.</li>\n    <li>It uses a unique training method on a large dataset (1.2 trillion tokens) that combines language and visual understanding.</li>\n    <li>The model includes a new reasoning technique called Parallel Coordinated Reasoning (PaCoRe) to improve its performance during testing.</li>\n    <li>Despite being smaller (10 billion parameters), it performs as well as or better than much larger models and top proprietary models.</li>\n    <li>STEP3-VL-10B achieved high scores on various benchmarks and is available for the community to use and build upon.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\uff0c\u4f7f\u7528\u89c6\u89c9\u7ebf\u7d22\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5ffd\u89c6\u4e86\u4eba\u7c7b\u5e38\u7528\u7684\u7b56\u7565\u2014\u2014\u4f7f\u7528\u5730\u56fe\u3002</li>\n    <li>\u672c\u7814\u7a76\u4e3a\u6a21\u578b\u589e\u52a0\u4e86\u201c\u601d\u8003\u5730\u56fe\u201d\u7684\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u8bbe\u8ba1\u4e3a\u4e00\u4e2a\u201c\u5730\u56fe\u4e2d\u7684\u667a\u80fd\u4f53\u201d\u5faa\u73af\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728500\u7c73\u7cbe\u5ea6\u4e0a\uff0c\u4ece8.0%\u63d0\u9ad8\u523022.1%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The goal of the image geolocalization task is to find out where a picture was taken on Earth using visual clues.</li>\n    <li>This study introduces a new approach called \"Thinking with Map,\" which helps the model use maps to improve location predictions.</li>\n    <li>They developed a two-step process: first, using reinforcement learning to enhance the model's decision-making, then using test-time scaling to explore different options.</li>\n    <li>A new benchmark called MAPBench was created to evaluate the model using real-world images.</li>\n    <li>The results show that this new method performs better than existing models, significantly increasing accuracy in geolocalization tasks.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.17058", "authors": [{"_id": "69782c96026bdf0473116e06", "user": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "name": "Wei Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:49.701Z", "hidden": false}, {"_id": "69782c96026bdf0473116e07", "name": "Jun Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e08", "name": "Haoyu Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e09", "name": "Zhenghao Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0a", "name": "Qikang He", "hidden": false}, {"_id": "69782c96026bdf0473116e0b", "name": "Shaokun Han", "hidden": false}, {"_id": "69782c96026bdf0473116e0c", "name": "Guoliang Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0d", "user": {"_id": "64ef522242da8d2a897d62da", "avatarUrl": "/avatars/03611010d247da66696ac8976d4d3ed3.svg", "isPro": false, "fullname": "xuanhe zhou", "user": "zhouxh19", "type": "user"}, "name": "Xuanhe Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:19.930Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0e", "user": {"_id": "674fa2f067c963c50a066594", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674fa2f067c963c50a066594/hKZ46Mwm_UEguzBt63ys_.jpeg", "isPro": false, "fullname": "yeye he", "user": "yeyehe", "type": "user"}, "name": "Yeye He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:27.638Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0f", "name": "Chunwei Liu", "hidden": false}, {"_id": "69782c96026bdf0473116e10", "user": {"_id": "66724ce47e7ff5d8bd069c7c", "avatarUrl": "/avatars/953f66585390dbdb202c1d7b7250d7bd.svg", "isPro": false, "fullname": "Zirui Tang", "user": "TerryTang", "type": "user"}, "name": "Zirui Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:49.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e11", "name": "Bin Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e12", "user": {"_id": "695612aabf3c8959a3a05f9c", "avatarUrl": "/avatars/c18885f6dea6f3ee019405cd8cf6f484.svg", "isPro": false, "fullname": "ShenTang990", "user": "shentang", "type": "user"}, "name": "Shen Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:56.579Z", "hidden": false}, {"_id": "69782c96026bdf0473116e13", "name": "Kai Zuo", "hidden": false}, {"_id": "69782c96026bdf0473116e14", "user": {"_id": "67efa8a2ed790a2e999dc216", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0S4lQCJX61uCF8EkSLMkk.png", "isPro": false, "fullname": "Yuyu Luo", "user": "luoyuyu", "type": "user"}, "name": "Yuyu Luo", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:02.233Z", "hidden": false}, {"_id": "69782c96026bdf0473116e15", "name": "Zhenzhe Zheng", "hidden": false}, {"_id": "69782c96026bdf0473116e16", "user": {"_id": "63f9fca8d4349b157a109eec", "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg", "isPro": false, "fullname": "Conghui He", "user": "conghui", "type": "user"}, "name": "Conghui He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:57:14.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e17", "name": "Jingren Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e18", "name": "Fan Wu", "hidden": false}], "publishedAt": "2026-01-22T12:02:45.000Z", "submittedOnDailyAt": "2026-01-27T00:42:38.464Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "submittedOnDailyBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "upvotes": 127, "discussionId": "69782c97026bdf0473116e19", "projectPage": "https://github.com/weAIDB/awesome-data-llm", "githubRepo": "https://github.com/weAIDB/awesome-data-llm", "githubRepoAddedBy": "user", "ai_summary": "LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.", "ai_keywords": ["data preparation", "large language models", "prompt-driven workflows", "agentic workflows", "data cleaning", "data integration", "data enrichment", "entity matching", "schema matching", "data annotation", "data profiling"], "githubStars": 644, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u6570\u636e\u51c6\u5907\u7684\u76ee\u7684\u662f\u53bb\u566a\u58f0\u3001\u53d1\u73b0\u8de8\u6570\u636e\u96c6\u7684\u5173\u7cfb\u5e76\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6570\u636e\u5e94\u7528\u3002</li>\n    <li>\u7531\u4e8e\u5bf9\u5e94\u7528\u51c6\u5907\u6570\u636e\u7684\u9700\u6c42\u4e0a\u5347\uff0c\u5f3a\u5927\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6280\u672f\u7684\u51fa\u73b0\uff0c\u4ee5\u53ca\u7075\u6d3b\u4ee3\u7406\u6784\u5efa\u57fa\u7840\u8bbe\u65bd\u7684\u53d1\u5c55\uff0cLLM\u589e\u5f3a\u7684\u6570\u636e\u51c6\u5907\u65b9\u6cd5\u6b63\u5728\u8fc5\u901f\u6210\u4e3a\u4e3b\u6d41\u3002</li>\n    <li>\u6587\u7ae0\u5bf9\u6700\u65b0\u6587\u732e\u8fdb\u884c\u4e86\u7cfb\u7edf\u56de\u987e\uff0c\u5173\u6ce8LLM\u6280\u672f\u5728\u6570\u636e\u51c6\u5907\u4e2d\u7684\u5e94\u7528\uff0c\u5b9a\u4e49\u4e86\u4ece\u89c4\u5219\u57fa\u7840\u5230\u57fa\u4e8e\u63d0\u793a\u7684\u51c6\u5907\u5de5\u4f5c\u6d41\u7684\u8303\u5f0f\u8f6c\u53d8\u3002</li>\n    <li>\u4ecb\u7ecd\u4e86\u4ee5\u4efb\u52a1\u4e3a\u4e2d\u5fc3\u7684\u5206\u7c7b\u6cd5\uff0c\u5c06\u6570\u636e\u51c6\u5907\u5206\u4e3a\u6570\u636e\u6e05\u7406\u3001\u6570\u636e\u6574\u5408\u548c\u6570\u636e\u4e30\u5bcc\u4e09\u4e2a\u4e3b\u8981\u4efb\u52a1\uff0c\u5e76\u5206\u6790\u4e86\u5404\u81ea\u7684\u6280\u672f\u4f18\u7f3a\u70b9\u3002</li>\n    <li>\u8ba8\u8bba\u4e86\u5e38\u7528\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u6311\u6218\u53ca\u53ef\u6269\u5c55\u7684LLM\u6570\u636e\u7cfb\u7edf\u7684\u53d1\u5c55\u8def\u7ebf\u56fe\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Data preparation helps clean, connect, and gain insights from raw datasets, which is important for various applications.</li>\n  <li>The paper reviews how large language models (LLMs) are changing data preparation methods to make them more effective and flexible.</li>\n  <li>It identifies three main tasks in data preparation: cleaning, integration, and enrichment, and discusses techniques for each.</li>\n  <li>The paper highlights the strengths and weaknesses of LLM techniques, such as better understanding but high costs and issues with reliability.</li>\n  <li>It also examines datasets and evaluation methods, and suggests future research directions for improving LLM-driven data systems.</li>\n</ul>"}, "publishedAt": "2026-01-22T07:02:45.000Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17058.png", "numComments": 2, "submittedBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "fullname": "Wei Zhou", "name": "weizhoudb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.12538", "authors": [{"_id": "6971913fc1c7409747bf9564", "name": "Tianxin Wei", "hidden": false}, {"_id": "6971913fc1c7409747bf9565", "user": {"_id": "6742eb40924e80c3c80ebe13", "avatarUrl": "/avatars/e6ccb1a89a1ea0bfca70779966f4f429.svg", "isPro": false, "fullname": "Ting-Wei Li", "user": "tingwl0122", "type": "user"}, "name": "Ting-Wei Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T17:12:21.531Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9566", "name": "Zhining Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf9567", "name": "Xuying Ning", "hidden": false}, {"_id": "6971913fc1c7409747bf9568", "name": "Ze Yang", "hidden": false}, {"_id": "6971913fc1c7409747bf9569", "name": "Jiaru Zou", "hidden": false}, {"_id": "6971913fc1c7409747bf956a", "name": "Zhichen Zeng", "hidden": false}, {"_id": "6971913fc1c7409747bf956b", "name": "Ruizhong Qiu", "hidden": false}, {"_id": "6971913fc1c7409747bf956c", "name": "Xiao Lin", "hidden": false}, {"_id": "6971913fc1c7409747bf956d", "name": "Dongqi Fu", "hidden": false}, {"_id": "6971913fc1c7409747bf956e", "name": "Zihao Li", "hidden": false}, {"_id": "6971913fc1c7409747bf956f", "user": {"_id": "653962e75c8e4863e1a2068f", "avatarUrl": "/avatars/d4f5f5da141f37d53ca1986ff17b325e.svg", "isPro": false, "fullname": "Mengting Ai", "user": "famous-blue-raincoat", "type": "user"}, "name": "Mengting Ai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:10.378Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9570", "user": {"_id": "677830bd3f2e3ec475576303", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dhwqUDkk66m4oSGSbcd7j.png", "isPro": false, "fullname": "Duo Zhou", "user": "Claudius7", "type": "user"}, "name": "Duo Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:12.476Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9571", "name": "Wenxuan Bao", "hidden": false}, {"_id": "6971913fc1c7409747bf9572", "user": {"_id": "646323556c27a7e33b23f198", "avatarUrl": "/avatars/17fe142f689ab4be3c2374d1d90393db.svg", "isPro": false, "fullname": "Yunzhe Li", "user": "yunzhel2", "type": "user"}, "name": "Yunzhe Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:14.383Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9573", "name": "Gaotang Li", "hidden": false}, {"_id": "6971913fc1c7409747bf9574", "name": "Cheng Qian", "hidden": false}, {"_id": "6971913fc1c7409747bf9575", "name": "Yu Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf9576", "name": "Xiangru Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf9577", "name": "Yin Xiao", "hidden": false}, {"_id": "6971913fc1c7409747bf9578", "name": "Liri Fang", "hidden": false}, {"_id": "6971913fc1c7409747bf9579", "name": "Hui Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf957a", "name": "Xianfeng Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf957b", "name": "Yuji Zhang", "hidden": false}, {"_id": "6971913fc1c7409747bf957c", "name": "Chi Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf957d", "name": "Jiaxuan You", "hidden": false}, {"_id": "6971913fc1c7409747bf957e", "name": "Heng Ji", "hidden": false}, {"_id": "6971913fc1c7409747bf957f", "name": "Hanghang Tong", "hidden": false}, {"_id": "6971913fc1c7409747bf9580", "name": "Jingrui He", "hidden": false}], "publishedAt": "2026-01-18T18:58:23.000Z", "submittedOnDailyAt": "2026-01-22T00:27:25.162Z", "title": "Agentic Reasoning for Large Language Models", "submittedOnDailyBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "isPro": false, "fullname": "Jiaru Zou", "user": "jiaruz2", "type": "user"}, "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "upvotes": 125, "discussionId": "69719140c1c7409747bf9581", "githubRepo": "https://github.com/weitianxin/Awesome-Agentic-Reasoning", "githubRepoAddedBy": "user", "ai_summary": "Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.", "ai_keywords": ["large language models", "agentic reasoning", "autonomous agents", "planning", "tool use", "search", "feedback", "memory", "adaptation", "collaborative settings", "coordination", "knowledge sharing", "reinforcement learning", "supervised fine-tuning", "in-context reasoning", "post-training reasoning", "real-world applications", "benchmarks", "thought and action", "world modeling", "scalable multi-agent training", "governance"], "githubStars": 105, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "summary_zh": "<ul>\n    <li>\u63a8\u7406\u662f\u63a8\u65ad\u3001\u89e3\u51b3\u95ee\u9898\u548c\u51b3\u7b56\u7684\u57fa\u672c\u8ba4\u77e5\u8fc7\u7a0b\u3002</li>\n    <li>\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5c01\u95ed\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5f00\u653e\u548c\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u81ea\u4e3b\u63a8\u7406\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u80fd\u591f\u89c4\u5212\u3001\u884c\u52a8\u548c\u5b66\u4e60\u7684\u81ea\u4e3b\u4ee3\u7406\u3002</li>\n    <li>\u81ea\u4e3b\u63a8\u7406\u5206\u4e3a\u4e09\u4e2a\u5c42\u6b21\uff1a\u57fa\u7840\u81ea\u4e3b\u63a8\u7406\u3001\u81ea\u6211\u6f14\u5316\u81ea\u4e3b\u63a8\u7406\u548c\u96c6\u4f53\u591a\u4ee3\u7406\u63a8\u7406\u3002</li>\n    <li>\u6587\u7ae0\u8ba8\u8bba\u4e86\u81ea\u4e3b\u63a8\u7406\u5728\u5404\u4e2a\u9886\u57df\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u6311\u6218\u548c\u65b9\u5411\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reasoning is essential for making decisions and solving problems, but large language models (LLMs) have difficulty in unpredictable situations.</li>\n    <li>Agentic reasoning treats LLMs as independent agents that can plan, act, and learn through ongoing interactions.</li>\n    <li>The survey describes agentic reasoning using three levels: basic skills for single agents, improving these skills through feedback, and teamwork among multiple agents.</li>\n    <li>It differentiates between in-context reasoning, which involves real-time interactions, and post-training reasoning, which improves models through learning techniques.</li>\n    <li>The survey also discusses real-world applications of agentic reasoning and highlights challenges and future areas for development, such as personalization and collaboration among agents.</li>\n</ul>"}, "publishedAt": "2026-01-18T13:58:23.000Z", "title": "Agentic Reasoning for Large Language Models", "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12538.png", "numComments": 3, "submittedBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "fullname": "Jiaru Zou", "name": "jiaruz2", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.08763", "authors": [{"_id": "6969b0a232f0333869ff946a", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:38.232Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946b", "user": {"_id": "6891c906f3c31445cc040ab1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891c906f3c31445cc040ab1/NBqxXOY7al4CD0XBj8ke2.jpeg", "isPro": false, "fullname": "Yucheng Wang", "user": "DevilEnfant", "type": "user"}, "name": "Yucheng Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:48.080Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946c", "name": "Yufei He", "hidden": false}, {"_id": "6969b0a232f0333869ff946d", "user": {"_id": "682deb444988bd82847e2b03", "avatarUrl": "/avatars/15da087e84386ea72c6fa2db63571420.svg", "isPro": false, "fullname": "Jia-Ying Wu", "user": "EricaWu", "type": "user"}, "name": "Jiaying Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:59.692Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946e", "name": "Yilun Zhao", "hidden": false}, {"_id": "6969b0a232f0333869ff946f", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0a232f0333869ff9470", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:06.327Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9471", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:12.181Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9472", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:17.979Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9473", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:23.007Z", "hidden": false}], "publishedAt": "2026-01-13T17:48:43.000Z", "submittedOnDailyAt": "2026-01-16T01:00:36.686Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "upvotes": 111, "discussionId": "6969b0a232f0333869ff9474", "ai_summary": "Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.", "ai_keywords": ["reinforcement learning", "large language models", "exploration collapse", "pass@k", "pass@1", "rollout-level objective", "high-level solution strategies", "clustering", "policy advantages", "AUC@K"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u9762\u53d8\u5f97\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5e38\u5e38\u9762\u4e34\u63a2\u7d22\u5d29\u6e83\u7684\u95ee\u9898\u3002</li>\n    <li>\u8fd9\u79cd\u5d29\u6e83\u5bfc\u81f4\u7b56\u7565\u8fc7\u65e9\u96c6\u4e2d\u4e8e\u5c11\u6570\u4e3b\u5bfc\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u591a\u6837\u6027\u548c\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u72ec\u7279\u6027\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\uff0c\u5956\u52b1\u5c55\u73b0\u7a00\u6709\u9ad8\u6c34\u5e73\u7b56\u7565\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u805a\u7c7b\u76f8\u540c\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u91cd\u52a0\u6743\u7b56\u7565\u4f18\u52bf\uff0c\u4f7f\u65b0\u9896\u7b56\u7565\u83b7\u5f97\u66f4\u9ad8\u5956\u52b1\u3002</li>\n    <li>\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u533b\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6210\u529f\u7387\uff0c\u5e76\u4fdd\u6301\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u591a\u6837\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) helps improve large language models (LLMs) for complex reasoning tasks, but it often focuses too much on a few common solutions.</li>\n    <li>This issue, called exploration collapse, limits the diversity of solutions and may only improve certain performance metrics (like pass@1).</li>\n    <li>To fix this, a new approach called Uniqueness-Aware Reinforcement Learning is proposed, rewarding unique and effective strategies instead of just common ones.</li>\n    <li>This method uses an LLM-based judge to group similar solutions and gives higher rewards to less common but correct strategies.</li>\n    <li>Testing showed that this approach improves the variety and quality of solutions without lowering basic performance (pass@1), especially in subjects like math, physics, and medicine.</li>\n</ul>"}, "publishedAt": "2026-01-13T12:48:43.000Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08763.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 109, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 105, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u662f\u4e00\u79cd\u6539\u5584\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5168\u9762\u7406\u89e3\u548c\u6df1\u5165\u63a8\u7406\u7684\u4efb\u52a1\u3002</li>\n    <li>\u73b0\u6709\u7684\u5185\u5b58\u8bbe\u8ba1\u4e3b\u8981\u4f5c\u4e3a\u88ab\u52a8\u5b58\u50a8\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u4fe1\u606f\u4e4b\u95f4\u7684\u9ad8\u9636\u5173\u8054\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HGMem\uff0c\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u5185\u5b58\u673a\u5236\uff0c\u80fd\u52a8\u6001\u8868\u8fbe\u590d\u6742\u63a8\u7406\u548c\u6574\u4f53\u7406\u89e3\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u56fe\u8fde\u63a5\u4e8b\u5b9e\u548c\u601d\u7ef4\uff0c\u5f62\u6210\u66f4\u5f3a\u7684\u77e5\u8bc6\u7ed3\u6784\uff0c\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHGMem\u5728\u591a\u6b65\u9aa4RAG\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u4f20\u7edf\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps large language models (LLMs) with complex reasoning tasks.</li>\n    <li>Current memory systems only store individual facts and do not connect them meaningfully, limiting their effectiveness.</li>\n    <li>HGMem is a new memory system that uses hypergraphs to create dynamic connections between facts for better reasoning.</li>\n    <li>This new approach allows for a more integrated understanding of knowledge, improving the reasoning process.</li>\n    <li>Tests show that HGMem significantly outperforms existing systems on challenging tasks requiring global understanding.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Jan 29, 2026";
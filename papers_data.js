window.trendingPapers = [{"paper": {"id": "2511.20573", "authors": [{"_id": "69266e53243b2216fb75ca64", "name": "Chenhui Gou", "hidden": false}, {"_id": "69266e53243b2216fb75ca65", "name": "Zilong Chen", "hidden": false}, {"_id": "69266e53243b2216fb75ca66", "name": "Zeyu Wang", "hidden": false}, {"_id": "69266e53243b2216fb75ca67", "name": "Feng Li", "hidden": false}, {"_id": "69266e53243b2216fb75ca68", "name": "Deyao Zhu", "hidden": false}, {"_id": "69266e53243b2216fb75ca69", "name": "Zicheng Duan", "hidden": false}, {"_id": "69266e53243b2216fb75ca6a", "name": "Kunchang Li", "hidden": false}, {"_id": "69266e53243b2216fb75ca6b", "name": "Chaorui Deng", "hidden": false}, {"_id": "69266e53243b2216fb75ca6c", "name": "Hongyi Yuan", "hidden": false}, {"_id": "69266e53243b2216fb75ca6d", "name": "Haoqi Fan", "hidden": false}, {"_id": "69266e53243b2216fb75ca6e", "name": "Cihang Xie", "hidden": false}, {"_id": "69266e53243b2216fb75ca6f", "name": "Jianfei Cai", "hidden": false}, {"_id": "69266e53243b2216fb75ca70", "name": "Hamid Rezatofighi", "hidden": false}], "publishedAt": "2025-11-25T18:06:22.000Z", "submittedOnDailyAt": "2025-11-26T03:38:13.028Z", "title": "VQ-VA World: Towards High-Quality Visual Question-Visual Answering", "submittedOnDailyBy": {"_id": "652e9c5774d1b0d7ff73d091", "avatarUrl": "/avatars/a6d2098b3dde4a8b7488a193f0ecb776.svg", "isPro": true, "fullname": "Chenhui Gou", "user": "gouc", "type": "user"}, "summary": "This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.", "upvotes": 7, "discussionId": "69266e53243b2216fb75ca71", "ai_summary": "A data-centric framework and benchmark for Visual Question-Visual Answering (VQ-VA) improve open-source model performance, narrowing the gap with proprietary systems.", "ai_keywords": ["Visual Question-Visual Answering", "VQ-VA", "agentic pipeline", "web-scale deployment", "image-text samples", "IntelligentBench", "world knowledge", "design knowledge", "reasoning", "LightFusion", "UniWorld-V1", "NanoBanana", "GPT-Image"]}, "publishedAt": "2025-11-25T13:06:22.000Z", "title": "VQ-VA World: Towards High-Quality Visual Question-Visual Answering", "summary": "This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20573.png", "numComments": 1, "submittedBy": {"_id": "652e9c5774d1b0d7ff73d091", "avatarUrl": "/avatars/a6d2098b3dde4a8b7488a193f0ecb776.svg", "fullname": "Chenhui Gou", "name": "gouc", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3}, "isAuthorParticipating": false}, {"paper": {"id": "2511.20123", "authors": [{"_id": "6926928b243b2216fb75cb17", "name": "Min Zhao", "hidden": false}, {"_id": "6926928b243b2216fb75cb18", "name": "Hongzhou Zhu", "hidden": false}, {"_id": "6926928b243b2216fb75cb19", "name": "Yingze Wang", "hidden": false}, {"_id": "6926928b243b2216fb75cb1a", "name": "Bokai Yan", "hidden": false}, {"_id": "6926928b243b2216fb75cb1b", "name": "Jintao Zhang", "hidden": false}, {"_id": "6926928b243b2216fb75cb1c", "name": "Guande He", "hidden": false}, {"_id": "6926928b243b2216fb75cb1d", "name": "Ling Yang", "hidden": false}, {"_id": "6926928b243b2216fb75cb1e", "name": "Chongxuan Li", "hidden": false}, {"_id": "6926928b243b2216fb75cb1f", "name": "Jun Zhu", "hidden": false}], "publishedAt": "2025-11-25T09:44:10.000Z", "submittedOnDailyAt": "2025-11-26T03:16:49.152Z", "title": "UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers", "submittedOnDailyBy": {"_id": "64c269a52d73768f07ac266c", "avatarUrl": "/avatars/d497a960f8aef6a974907b68ed750c1c.svg", "isPro": false, "fullname": "Zhu Hongzhou", "user": "zhuhz22", "type": "user"}, "summary": "Despite advances, video diffusion transformers still struggle to generalize beyond their training length, a challenge we term video length extrapolation. We identify two failure modes: model-specific periodic content repetition and a universal quality degradation. Prior works attempt to solve repetition via positional encodings, overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from a more fundamental view: attention maps, which directly govern how context influences outputs. We identify that both failure modes arise from a unified cause: attention dispersion, where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as a special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings. Building on this insight, we propose UltraViCo, a training-free, plug-and-play method that suppresses attention for tokens beyond the training window via a constant decay factor. By jointly addressing both failure modes, we outperform a broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2x to 4x. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4x extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing.", "upvotes": 4, "discussionId": "6926928b243b2216fb75cb20", "projectPage": "https://thu-ml.github.io/UltraViCo.github.io/", "githubRepo": "https://github.com/thu-ml/DiT-Extrapolation", "ai_summary": "UltraViCo addresses video length extrapolation by suppressing attention dispersion, improving quality and reducing repetition beyond training length.", "ai_keywords": ["video diffusion transformers", "video length extrapolation", "positional encodings", "attention maps", "attention dispersion", "UltraViCo", "Dynamic Degree", "Imaging Quality", "controllable video synthesis", "editing"], "githubStars": 738, "organization": {"_id": "640d3084536d9fe0f005cac3", "name": "thu-ml", "fullname": "Tsinghua Machine Learning Group", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678587085174-633131798ef21f47308ce49b.jpeg"}}, "publishedAt": "2025-11-25T04:44:10.000Z", "title": "UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers", "summary": "Despite advances, video diffusion transformers still struggle to generalize beyond their training length, a challenge we term video length extrapolation. We identify two failure modes: model-specific periodic content repetition and a universal quality degradation. Prior works attempt to solve repetition via positional encodings, overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from a more fundamental view: attention maps, which directly govern how context influences outputs. We identify that both failure modes arise from a unified cause: attention dispersion, where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as a special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings. Building on this insight, we propose UltraViCo, a training-free, plug-and-play method that suppresses attention for tokens beyond the training window via a constant decay factor. By jointly addressing both failure modes, we outperform a broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2x to 4x. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4x extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20123.png", "numComments": 1, "submittedBy": {"_id": "64c269a52d73768f07ac266c", "avatarUrl": "/avatars/d497a960f8aef6a974907b68ed750c1c.svg", "fullname": "Zhu Hongzhou", "name": "zhuhz22", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2}, "organization": {"_id": "640d3084536d9fe0f005cac3", "name": "thu-ml", "fullname": "Tsinghua Machine Learning Group", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1678587085174-633131798ef21f47308ce49b.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.19827", "authors": [{"_id": "69268cde243b2216fb75caf9", "name": "Byeongjun Park", "hidden": false}, {"_id": "69268cde243b2216fb75cafa", "name": "Byung-Hoon Kim", "hidden": false}, {"_id": "69268cde243b2216fb75cafb", "name": "Hyungjin Chung", "hidden": false}, {"_id": "69268cde243b2216fb75cafc", "name": "Jong Chul Ye", "hidden": false}], "publishedAt": "2025-11-25T01:38:56.000Z", "submittedOnDailyAt": "2025-11-26T02:48:52.934Z", "title": "ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding", "submittedOnDailyBy": {"_id": "653929a66da48e0d21e65e17", "avatarUrl": "/avatars/e34ae3411d689b4280ff34c1b680f283.svg", "isPro": false, "fullname": "Byeongjun Park", "user": "byeongjun-park", "type": "user"}, "summary": "We present ReDirector, a novel camera-controlled video retake generation method for dynamically captured variable-length videos. In particular, we rectify a common misuse of RoPE in previous works by aligning the spatiotemporal positions of the input video and the target retake. Moreover, we introduce Rotary Camera Encoding (RoCE), a camera-conditioned RoPE phase shift that captures and integrates multi-view relationships within and across the input and target videos. By integrating camera conditions into RoPE, our method generalizes to out-of-distribution camera trajectories and video lengths, yielding improved dynamic object localization and static background preservation. Extensive experiments further demonstrate significant improvements in camera controllability, geometric consistency, and video quality across various trajectories and lengths.", "upvotes": 3, "discussionId": "69268cdf243b2216fb75cafd", "projectPage": "https://byeongjun-park.github.io/ReDirector/", "githubRepo": "https://github.com/byeongjun-park/ReDirector", "ai_summary": "ReDirector uses a novel camera-controlled video retake method with Rotary Camera Encoding (RoCE) to improve dynamic object localization and static background preservation in variable-length videos.", "ai_keywords": ["RoPE", "Rotary Camera Encoding", "RoCE", "camera-controlled video retake", "spatiotemporal positions", "multi-view relationships", "dynamic object localization", "static background preservation", "camera controllability", "geometric consistency", "video quality"], "githubStars": 5, "organization": {"_id": "64ab689073790912c7a8717a", "name": "everex", "fullname": "EverEx", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ab6841723beceb2f45c9da/cdIuwsqJw-2tlEhnco0kI.png"}}, "publishedAt": "2025-11-24T20:38:56.000Z", "title": "ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding", "summary": "We present ReDirector, a novel camera-controlled video retake generation method for dynamically captured variable-length videos. In particular, we rectify a common misuse of RoPE in previous works by aligning the spatiotemporal positions of the input video and the target retake. Moreover, we introduce Rotary Camera Encoding (RoCE), a camera-conditioned RoPE phase shift that captures and integrates multi-view relationships within and across the input and target videos. By integrating camera conditions into RoPE, our method generalizes to out-of-distribution camera trajectories and video lengths, yielding improved dynamic object localization and static background preservation. Extensive experiments further demonstrate significant improvements in camera controllability, geometric consistency, and video quality across various trajectories and lengths.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19827.png", "numComments": 1, "submittedBy": {"_id": "653929a66da48e0d21e65e17", "avatarUrl": "/avatars/e34ae3411d689b4280ff34c1b680f283.svg", "fullname": "Byeongjun Park", "name": "byeongjun-park", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "organization": {"_id": "64ab689073790912c7a8717a", "name": "everex", "fullname": "EverEx", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64ab6841723beceb2f45c9da/cdIuwsqJw-2tlEhnco0kI.png"}, "isAuthorParticipating": false}];
window.papersLastUpdated = "Nov 26, 2025";
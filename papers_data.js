window.trendingPapers = {
    "today": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u5728\u89c6\u9891\u95ee\u7b54\u573a\u666f\u4e2d\uff0c\u89c6\u9891\u53ea\u63d0\u4f9b\u5c40\u90e8\u89c6\u89c9\u7ebf\u7d22\uff0c\u7b54\u6848\u5206\u6563\u5728\u7f51\u7edc\u4e0a\u3002</li>\n    <li>\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e2a\u95ee\u9898\uff0c\u5efa\u7acb\u4e86\u9996\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u2014\u2014VideoDR\u3002</li>\n    <li>VideoDR\u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u4e0b\u7684\u5f00\u653e\u57df\u95ee\u7b54\uff0c\u6d89\u53ca\u8de8\u5e27\u7ebf\u7d22\u63d0\u53d6\u548c\u591a\u8df3\u63a8\u7406\u3002</li>\n    <li>\u901a\u8fc7\u4eba\u7c7b\u6807\u6ce8\u548c\u8d28\u91cf\u63a7\u5236\uff0c\u83b7\u5f97\u4e86\u6db5\u76d6\u516d\u4e2a\u8bed\u4e49\u9886\u57df\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u6837\u672c\u3002</li>\n    <li>\u7814\u7a76\u663e\u793a\uff0c\u6a21\u578b\u5728\u957f\u68c0\u7d22\u94fe\u4e2d\u4fdd\u6301\u521d\u59cb\u89c6\u9891\u7ebf\u7d22\u7684\u80fd\u529b\u662f\u5173\u952e\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video question answering often requires combining visual clues from videos with information from the web.</li>\n    <li>To address this, the VideoDR benchmark was created for video-based question answering, focusing on extracting visual information, retrieving web data, and reasoning over that information.</li>\n    <li>VideoDR includes high-quality samples from six different topics, developed through careful human review.</li>\n    <li>Tests of various large language models showed that the effectiveness of different approaches (Workflow vs. Agentic) depends on the model's ability to keep track of visual information during long retrieval processes.</li>\n    <li>The study highlights challenges like goal drift and maintaining consistency over longer tasks, which are important for developing better video research agents.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u4eba\u7c7b\u5728\u5b66\u4f1a\u8bed\u8a00\u4e4b\u524d\u5c31\u80fd\u53d1\u5c55\u6838\u5fc3\u89c6\u89c9\u6280\u80fd\uff0c\u800c\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u6765\u5f25\u8865\u89c6\u89c9\u7406\u89e3\u7684\u4e0d\u8db3\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684MLLMs\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u8fd9\u4e9b\u4efb\u52a1\u4eba\u7c7b\u751a\u81f33\u5c81\u7684\u5c0f\u5b69\u90fd\u80fd\u8f7b\u677e\u89e3\u51b3\u3002</li>\n    <li>\u4e3a\u8bc4\u4f30MLLMs\u7684\u6838\u5fc3\u89c6\u89c9\u80fd\u529b\uff0c\u7814\u7a76\u56e2\u961f\u63a8\u51fa\u4e86BabyVision\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec388\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\u548c\u56db\u4e2a\u4e3b\u8981\u7c7b\u522b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u9886\u5148\u7684MLLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c6\uff0c\u4f8b\u5982Gemini3-Pro-Preview\u7684\u5f97\u5206\u4e3a49.7\uff0c\u8fdc\u4f4e\u4e8e6\u5c81\u513f\u7ae5\u548c94.1\u7684\u6210\u4eba\u5e73\u5747\u5206\u3002</li>\n    <li>BabyVision\u7684\u8fdb\u5c55\u6807\u5fd7\u7740\u5411\u4eba\u7c7b\u6c34\u5e73\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u7684\u8fc8\u8fdb\uff0c\u540c\u65f6\u63d0\u51fa\u4e86BabyVision-Gen\u548c\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\uff0c\u4ee5\u89e3\u51b3\u89c6\u89c9\u63a8\u7406\u95ee\u9898\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans develop visual skills early, but modern Multimodal LLMs (MLLMs) still depend on language to understand visuals.</li>\n    <li>There's a noticeable gap where top MLLMs struggle with basic visual tasks that young children can easily handle.</li>\n    <li>The BabyVision benchmark was created to test MLLMs' visual abilities without using language knowledge.</li>\n    <li>BabyVision includes 388 tasks across 22 subclasses, showing that MLLMs score much lower than humans on these tasks.</li>\n    <li>Current MLLMs, like Gemini3-Pro-Preview, score poorly compared to human averages, indicating they need to improve in visual understanding.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.05593", "authors": [{"_id": "6965b990fc8c4ecc02c7f8df", "name": "Jingcheng Hu", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e0", "name": "Yinmin Zhang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e1", "name": "Shijie Shang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e2", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e3", "name": "Yue Peng", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e4", "name": "Zhewei Huang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e5", "name": "Hebin Zhou", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e6", "name": "Xin Wu", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e7", "name": "Jie Cheng", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e8", "name": "Fanqi Wan", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e9", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ea", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8eb", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ec", "name": "Ailin Huang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ed", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ee", "name": "Qi Han", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ef", "name": "Zheng Ge", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8f0", "name": "Daxin Jiang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8f1", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8f2", "name": "Heung-Yeung Shum", "hidden": false}], "publishedAt": "2026-01-09T07:24:43.000Z", "submittedOnDailyAt": "2026-01-13T00:51:45.124Z", "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.", "upvotes": 62, "discussionId": "6965b990fc8c4ecc02c7f8f3", "githubRepo": "https://github.com/stepfun-ai/PaCoRe", "githubRepoAddedBy": "user", "ai_summary": "Parallel Coordinated Reasoning enables large-scale test-time compute scaling beyond sequential reasoning limitations through parallel exploration and message-passing architecture.", "ai_keywords": ["test-time compute", "sequential reasoning", "parallel exploration", "message-passing architecture", "reinforcement learning", "multi-million-token", "HMMT 2025", "GPT-5"], "githubStars": 261, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86\u5e76\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002</li>\n    <li>PaCoRe\u91c7\u7528\u6d88\u606f\u4f20\u9012\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u8f6e\u5e76\u884c\u63a2\u7d22\u6765\u63d0\u5347\u8ba1\u7b97\u80fd\u529b\uff0c\u800c\u4e0d\u662f\u4f20\u7edf\u7684\u987a\u5e8f\u63a8\u7406\u3002</li>\n    <li>\u6bcf\u8f6e\u5e76\u884c\u63a8\u7406\u751f\u6210\u7684\u7ed3\u679c\u88ab\u538b\u7f29\u4e3a\u6d88\u606f\uff0c\u7528\u4e8e\u6307\u5bfc\u4e0b\u4e00\u8f6e\u7684\u63a8\u7406\uff0c\u6700\u7ec8\u5f97\u51fa\u7b54\u6848\u3002</li>\n    <li>\u8be5\u6a21\u578b\u7ecf\u8fc7\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u80fd\u591f\u5904\u7406\u591a\u767e\u4e07\u6807\u8bb0\u7684\u6709\u6548\u8ba1\u7b97\uff0c\u800c\u4e0d\u8d85\u51fa\u4e0a\u4e0b\u6587\u9650\u5236\u3002</li>\n    <li>\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u6570\u5b66\u63a8\u7406\u4e0a\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Introducing a new framework called Parallel Coordinated Reasoning (PaCoRe) that helps language models think faster and handle more information at once.</li>\n    <li>PaCoRe uses a method that allows many reasoning paths to be explored at the same time, instead of just one after another.</li>\n    <li>The framework is trained with a special type of learning that helps it combine information effectively and work with large amounts of data.</li>\n    <li>PaCoRe shows significant improvements in solving problems, especially in mathematics, outperforming previous models like GPT-5.</li>\n    <li>All resources related to PaCoRe, including model files and training data, are shared publicly to help future research.</li>\n</ul>"}, "publishedAt": "2026-01-09T02:24:43.000Z", "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning", "summary": "We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05593.png", "numComments": 1, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 17, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.06953", "authors": [{"_id": "6965af14fc8c4ecc02c7f873", "name": "Jie Wu", "hidden": false}, {"_id": "6965af14fc8c4ecc02c7f874", "user": {"_id": "650be23ec4e52db6a4db63ef", "avatarUrl": "/avatars/03af548029b38bee49ec295fefe74f9a.svg", "isPro": false, "fullname": "Haoling Li", "user": "Ringo1110", "type": "user"}, "name": "Haoling Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:24:20.281Z", "hidden": false}, {"_id": "6965af14fc8c4ecc02c7f875", "name": "Xin Zhang", "hidden": false}, {"_id": "6965af14fc8c4ecc02c7f876", "name": "Jiani Guo", "hidden": false}, {"_id": "6965af14fc8c4ecc02c7f877", "name": "Jane Luo", "hidden": false}, {"_id": "6965af14fc8c4ecc02c7f878", "name": "Steven Liu", "hidden": false}, {"_id": "6965af14fc8c4ecc02c7f879", "name": "Yangyu Huang", "hidden": false}, {"_id": "6965af14fc8c4ecc02c7f87a", "name": "Ruihang Chu", "hidden": false}, {"_id": "6965af14fc8c4ecc02c7f87b", "name": "Scarlett Li", "hidden": false}, {"_id": "6965af14fc8c4ecc02c7f87c", "name": "Yujiu Yang", "hidden": false}], "publishedAt": "2026-01-11T15:22:33.000Z", "submittedOnDailyAt": "2026-01-13T01:12:35.419Z", "title": "X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Competitive programming presents great challenges for Code LLMs due to its intensive reasoning demands and high logical complexity. However, current Code LLMs still rely heavily on real-world data, which limits their scalability. In this paper, we explore a fully synthetic approach: training Code LLMs with entirely generated tasks, solutions, and test cases, to empower code reasoning models without relying on real-world data. To support this, we leverage feature-based synthesis to propose a novel data synthesis pipeline called SynthSmith. SynthSmith shows strong potential in producing diverse and challenging tasks, along with verified solutions and tests, supporting both supervised fine-tuning and reinforcement learning. Based on the proposed synthetic SFT and RL datasets, we introduce the X-Coder model series, which achieves a notable pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, outperforming DeepCoder-14B-Preview and AReal-boba2-14B despite having only 7B parameters. In-depth analysis reveals that scaling laws hold on our synthetic dataset, and we explore which dimensions are more effective to scale. We further provide insights into code-centric reinforcement learning and highlight the key factors that shape performance through detailed ablations and analysis. Our findings demonstrate that scaling high-quality synthetic data and adopting staged training can greatly advance code reasoning, while mitigating reliance on real-world coding data.", "upvotes": 30, "discussionId": "6965af14fc8c4ecc02c7f87d", "githubRepo": "https://github.com/JieWu02/X-Coder", "githubRepoAddedBy": "user", "ai_summary": "Code LLMs trained on fully synthetic data using a feature-based synthesis pipeline achieve superior performance on competitive programming benchmarks while reducing dependence on real-world coding datasets.", "ai_keywords": ["Code LLMs", "synthetic data", "feature-based synthesis", "data synthesis pipeline", "SynthSmith", "supervised fine-tuning", "reinforcement learning", "X-Coder model series", "LiveCodeBench", "scaling laws", "staged training", "code reasoning"], "githubStars": 52, "summary_zh": "<ul>\n    <li>\u7ade\u4e89\u7f16\u7a0b\u5bf9\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\uff08Code LLMs\uff09\u63d0\u51fa\u4e86\u5f88\u5927\u6311\u6218\uff0c\u4e3b\u8981\u56e0\u4e3a\u9700\u8981\u590d\u6742\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u76ee\u524d\u7684\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u4ecd\u4f9d\u8d56\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u6269\u5c55\u6027\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u5408\u6210\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u4efb\u52a1\u3001\u89e3\u51b3\u65b9\u6848\u548c\u6d4b\u8bd5\u6848\u4f8b\u6765\u8bad\u7ec3\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aSynthSmith\u7684\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u80fd\u591f\u4ea7\u751f\u591a\u6837\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u53ca\u9a8c\u8bc1\u8fc7\u7684\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u57fa\u4e8e\u5408\u6210\u6570\u636e\u96c6\uff0c\u6211\u4eec\u5f15\u5165\u4e86X-Coder\u6a21\u578b\u7cfb\u5217\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u8868\u73b0\uff0c\u5c55\u793a\u4e86\u5408\u6210\u6570\u636e\u7684\u4f18\u52bf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Competitive programming is tough for Code LLMs because it requires deep reasoning and complex logic.</li>\n    <li>Current Code LLMs depend on real-world data, which limits their growth and scalability.</li>\n    <li>This paper introduces a new approach that uses only synthetic data for training Code LLMs, through a method called SynthSmith.</li>\n    <li>SynthSmith can create diverse programming tasks and solutions, leading to strong performance in code reasoning models like the X-Coder series.</li>\n    <li>The findings show that using high-quality synthetic data can improve code reasoning capabilities, reducing the need for real-world examples.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:22:33.000Z", "title": "X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests", "summary": "Competitive programming presents great challenges for Code LLMs due to its intensive reasoning demands and high logical complexity. However, current Code LLMs still rely heavily on real-world data, which limits their scalability. In this paper, we explore a fully synthetic approach: training Code LLMs with entirely generated tasks, solutions, and test cases, to empower code reasoning models without relying on real-world data. To support this, we leverage feature-based synthesis to propose a novel data synthesis pipeline called SynthSmith. SynthSmith shows strong potential in producing diverse and challenging tasks, along with verified solutions and tests, supporting both supervised fine-tuning and reinforcement learning. Based on the proposed synthetic SFT and RL datasets, we introduce the X-Coder model series, which achieves a notable pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, outperforming DeepCoder-14B-Preview and AReal-boba2-14B despite having only 7B parameters. In-depth analysis reveals that scaling laws hold on our synthetic dataset, and we explore which dimensions are more effective to scale. We further provide insights into code-centric reinforcement learning and highlight the key factors that shape performance through detailed ablations and analysis. Our findings demonstrate that scaling high-quality synthetic data and adopting staged training can greatly advance code reasoning, while mitigating reliance on real-world coding data.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06953.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.07832", "authors": [{"_id": "6965c791fc8c4ecc02c7f9d3", "user": {"_id": "659698e6f67e8fb2a5985445", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6jxohd_DsLVfSnWgKMGrn.jpeg", "isPro": false, "fullname": "Kewei Zhang", "user": "xiwenyoumu", "type": "user"}, "name": "Kewei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:45.745Z", "hidden": false}, {"_id": "6965c791fc8c4ecc02c7f9d4", "name": "Ye Huang", "hidden": false}, {"_id": "6965c791fc8c4ecc02c7f9d5", "user": {"_id": "68fce03ed1d0efce7ca87075", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68fce03ed1d0efce7ca87075/GRKTeVIaLZD_M-KoJE8YF.png", "isPro": false, "fullname": "yfdeng", "user": "yfdeng10", "type": "user"}, "name": "Yufan Deng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:39.576Z", "hidden": false}, {"_id": "6965c791fc8c4ecc02c7f9d6", "name": "Jincheng Yu", "hidden": false}, {"_id": "6965c791fc8c4ecc02c7f9d7", "name": "Junsong Chen", "hidden": false}, {"_id": "6965c791fc8c4ecc02c7f9d8", "name": "Huan Ling", "hidden": false}, {"_id": "6965c791fc8c4ecc02c7f9d9", "name": "Enze Xie", "hidden": false}, {"_id": "6965c791fc8c4ecc02c7f9da", "name": "Daquan Zhou", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/68fce03ed1d0efce7ca87075/nan3M8p_MfDdxZgb1KI4j.mp4"], "publishedAt": "2026-01-12T18:59:18.000Z", "submittedOnDailyAt": "2026-01-13T06:02:44.452Z", "title": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head", "submittedOnDailyBy": {"_id": "68fce03ed1d0efce7ca87075", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68fce03ed1d0efce7ca87075/GRKTeVIaLZD_M-KoJE8YF.png", "isPro": false, "fullname": "yfdeng", "user": "yfdeng10", "type": "user"}, "summary": "While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\\% improvement on ImageNet classification, a 6.3\\% gain on NLP, a 12.6\\% improvement on image generation, and a 41\\% enhancement on video generation under the same time complexity.", "upvotes": 29, "discussionId": "6965c791fc8c4ecc02c7f9db", "projectPage": "https://dagroup-pku.github.io/MHLA/", "githubRepo": "https://github.com/DAGroup-PKU/MHLA", "githubRepoAddedBy": "user", "ai_summary": "Multi-Head Linear Attention addresses the performance degradation in linear attention by preserving representational diversity through head-wise token dimension computation, maintaining linear complexity while recovering softmax attention's expressive power across multiple domains.", "ai_keywords": ["Transformer architecture", "self-attention", "linear attention", "global context collapse", "Multi-Head Linear Attention", "token dimension", "softmax attention", "ImageNet classification", "NLP", "image generation", "video generation"], "githubStars": 47, "organization": {"_id": "6953c657d2ff7b60c8527d3c", "name": "DAGroup-PKU", "fullname": "DAGroup-PKU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/665c91e15b11dca02f0c5891/ek9KGIc02tiFfaYeDfLaU.png"}, "summary_zh": "<ul>\n    <li>Transformer\u67b6\u6784\u5728\u8bb8\u591a\u9886\u57df\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u5176\u81ea\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5176\u5927\u89c4\u6a21\u5e94\u7528\u3002</li>\n    <li>\u7ebf\u6027\u6ce8\u610f\u529b\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u76f4\u63a5\u4f7f\u7528\u65f6\u901a\u5e38\u4f1a\u964d\u4f4e\u6027\u80fd\u3002</li>\n    <li>\u5df2\u6709\u7684\u89e3\u51b3\u65b9\u6cd5\u5f80\u5f80\u589e\u52a0\u4e86\u8ba1\u7b97\u8d1f\u62c5\uff0c\u5931\u53bb\u6700\u521d\u7684\u9ad8\u6548\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u591a\u5934\u7ebf\u6027\u6ce8\u610f\u529b\uff08MHLA\uff09\uff0c\u901a\u8fc7\u5728\u6807\u8bb0\u7ef4\u5ea6\u4e0a\u5212\u5206\u5934\u6765\u4fdd\u6301\u8868\u793a\u591a\u6837\u6027\u3002</li>\n    <li>MHLA\u8bc1\u660e\u80fd\u4fdd\u6301\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5305\u62ec\u56fe\u50cf\u5206\u7c7b\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u56fe\u50cf\u751f\u6210\u548c\u89c6\u9891\u751f\u6210\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The Transformer architecture is popular but has a problem with speed due to its complex self-attention mechanism.</li>\n    <li>Linear attention is a faster alternative, but it often reduces performance and can add extra computational costs.</li>\n    <li>The authors find that previous methods can lose important details in their data, known as global context collapse.</li>\n    <li>They introduce a new approach called Multi-Head Linear Attention (MHLA) that keeps important details while remaining efficient.</li>\n    <li>MHLA shows significant performance improvements in various tasks, like 3.6% better on ImageNet and 41% better on video generation, all while keeping the same level of computational efficiency.</li>\n</ul>"}, "publishedAt": "2026-01-12T13:59:18.000Z", "title": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head", "summary": "While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\\% improvement on ImageNet classification, a 6.3\\% gain on NLP, a 12.6\\% improvement on image generation, and a 41\\% enhancement on video generation under the same time complexity.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/68fce03ed1d0efce7ca87075/nan3M8p_MfDdxZgb1KI4j.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07832.png", "numComments": 1, "submittedBy": {"_id": "68fce03ed1d0efce7ca87075", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68fce03ed1d0efce7ca87075/GRKTeVIaLZD_M-KoJE8YF.png", "fullname": "yfdeng", "name": "yfdeng10", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "6953c657d2ff7b60c8527d3c", "name": "DAGroup-PKU", "fullname": "DAGroup-PKU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/665c91e15b11dca02f0c5891/ek9KGIc02tiFfaYeDfLaU.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05110", "authors": [{"_id": "696493ff138cc47cbd7653ab", "name": "Wenhao Zeng", "hidden": false}, {"_id": "696493ff138cc47cbd7653ac", "name": "Xuteng Zhang", "hidden": false}, {"_id": "696493ff138cc47cbd7653ad", "user": {"_id": "645b0c3ec35da9c7afd95421", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg", "isPro": false, "fullname": "Yuling", "user": "YerbaPage", "type": "user"}, "name": "Yuling Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:33:29.307Z", "hidden": false}, {"_id": "696493ff138cc47cbd7653ae", "name": "Chao Hu", "hidden": false}, {"_id": "696493ff138cc47cbd7653af", "name": "Yuting Chen", "hidden": false}, {"_id": "696493ff138cc47cbd7653b0", "name": "Beijun Shen", "hidden": false}, {"_id": "696493ff138cc47cbd7653b1", "name": "Xiaodong Gu", "hidden": false}], "publishedAt": "2026-01-08T16:58:07.000Z", "submittedOnDailyAt": "2026-01-13T03:10:55.068Z", "title": "GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts", "submittedOnDailyBy": {"_id": "645b0c3ec35da9c7afd95421", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg", "isPro": false, "fullname": "Yuling", "user": "YerbaPage", "type": "user"}, "summary": "Large Reasoning Models (LRMs) achieve remarkable performance by explicitly generating multi-step chains of thought, but this capability incurs substantial inference latency and computational cost. Collaborative inference offers a promising solution by selectively allocating work between lightweight and large models, yet a fundamental challenge remains: determining when a reasoning step requires the capacity of a large model or the efficiency of a small model. Existing routing strategies either rely on local token probabilities or post-hoc verification, introducing significant inference overhead. In this work, we propose a novel perspective on step-wise collaboration: the difficulty of a reasoning step can be inferred from its very first token. Inspired by the \"Aha Moment\" phenomenon in LRMs, we show that the entropy of the initial token serves as a strong predictor of step difficulty. Building on this insight, we introduce GlimpRouter, a training-free step-wise collaboration framework. GlimpRouter employs a lightweight model to generate only the first token of each reasoning step and routes the step to a larger model only when the initial token entropy exceeds a threshold. Experiments on multiple benchmarks demonstrate that our approach significantly reduces inference latency while preserving accuracy. For instance, GlimpRouter attains a substantial 10.7% improvement in accuracy while reducing inference latency by 25.9% compared to a standalone large model on AIME25. These results suggest a simple yet effective mechanism for reasoning: allocating computation based on a glimpse of thought rather than full-step evaluation.", "upvotes": 24, "discussionId": "696493ff138cc47cbd7653b2", "githubRepo": "https://github.com/Zengwh02/GlimpRouter", "githubRepoAddedBy": "user", "ai_summary": "Large reasoning models' inference latency can be reduced by routing reasoning steps to larger models based on the entropy of their first token, enabling efficient collaborative inference without additional training.", "ai_keywords": ["large reasoning models", "multi-step chains of thought", "inference latency", "computational cost", "collaborative inference", "routing strategies", "token probabilities", "post-hoc verification", "step-wise collaboration", "reasoning step difficulty", "initial token entropy", "Aha Moment phenomenon", "training-free framework", "lightweight model", "large model", "threshold routing", "entropy-based prediction", "inference overhead", "GlimpRouter"], "githubStars": 6, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u901a\u8fc7\u751f\u6210\u591a\u6b65\u9aa4\u601d\u8003\u94fe\u5b9e\u73b0\u4e86\u4f18\u79c0\u7684\u8868\u73b0\uff0c\u4f46\u5e26\u6765\u4e86\u8f83\u9ad8\u7684\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\u3002</li>\n    <li>\u534f\u4f5c\u63a8\u7406\u53ef\u4ee5\u901a\u8fc7\u5728\u8f7b\u91cf\u6a21\u578b\u548c\u5927\u578b\u6a21\u578b\u4e4b\u95f4\u9009\u62e9\u6027\u5206\u914d\u5de5\u4f5c\u6765\u4f18\u5316\u6027\u80fd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u57fa\u4e8e\u63a8\u7406\u6b65\u9aa4\u7684\u7b2c\u4e00\u4e2atoken\u6765\u63a8\u6d4b\u5176\u96be\u5ea6\u3002</li>\n    <li>GlimpRouter\u6846\u67b6\u4f7f\u7528\u8f7b\u91cf\u6a21\u578b\u751f\u6210\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u7684\u7b2c\u4e00\u4e2atoken\uff0c\u4ec5\u5728\u5fc5\u8981\u65f6\u8c03\u7528\u5927\u578b\u6a21\u578b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cGlimpRouter\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u4e8610.7%\u7684\u51c6\u786e\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Reasoning Models (LRMs) are good at generating detailed multi-step reasoning but are slow and expensive to use.</li>\n    <li>Collaborative inference can help by dividing tasks between smaller and larger models, but it's hard to decide which model to use for each reasoning step.</li>\n    <li>Current methods for deciding which model to use add extra delays and costs.</li>\n    <li>This study suggests using the first token of a reasoning step to predict how difficult that step will be, which helps in choosing the right model.</li>\n    <li>The new method, called GlimpRouter, uses a lightweight model to generate just the first token and only switches to a larger model if needed, leading to faster processing and better accuracy in tests.</li>\n</ul>"}, "publishedAt": "2026-01-08T11:58:07.000Z", "title": "GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts", "summary": "Large Reasoning Models (LRMs) achieve remarkable performance by explicitly generating multi-step chains of thought, but this capability incurs substantial inference latency and computational cost. Collaborative inference offers a promising solution by selectively allocating work between lightweight and large models, yet a fundamental challenge remains: determining when a reasoning step requires the capacity of a large model or the efficiency of a small model. Existing routing strategies either rely on local token probabilities or post-hoc verification, introducing significant inference overhead. In this work, we propose a novel perspective on step-wise collaboration: the difficulty of a reasoning step can be inferred from its very first token. Inspired by the \"Aha Moment\" phenomenon in LRMs, we show that the entropy of the initial token serves as a strong predictor of step difficulty. Building on this insight, we introduce GlimpRouter, a training-free step-wise collaboration framework. GlimpRouter employs a lightweight model to generate only the first token of each reasoning step and routes the step to a larger model only when the initial token entropy exceeds a threshold. Experiments on multiple benchmarks demonstrate that our approach significantly reduces inference latency while preserving accuracy. For instance, GlimpRouter attains a substantial 10.7% improvement in accuracy while reducing inference latency by 25.9% compared to a standalone large model on AIME25. These results suggest a simple yet effective mechanism for reasoning: allocating computation based on a glimpse of thought rather than full-step evaluation.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05110.png", "numComments": 3, "submittedBy": {"_id": "645b0c3ec35da9c7afd95421", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg", "fullname": "Yuling", "name": "YerbaPage", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 186, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.07779", "authors": [{"_id": "6965f217fc8c4ecc02c7fa6b", "name": "Bowen Yang", "hidden": false}, {"_id": "6965f217fc8c4ecc02c7fa6c", "name": "Kaiming Jin", "hidden": false}, {"_id": "6965f217fc8c4ecc02c7fa6d", "name": "Zhenyu Wu", "hidden": false}, {"_id": "6965f217fc8c4ecc02c7fa6e", "name": "Zhaoyang Liu", "hidden": false}, {"_id": "6965f217fc8c4ecc02c7fa6f", "user": {"_id": "6064a0eeb1703ddba0d458b9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png", "isPro": false, "fullname": "Qiushi", "user": "QiushiSun", "type": "user"}, "name": "Qiushi Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:13.865Z", "hidden": false}, {"_id": "6965f217fc8c4ecc02c7fa70", "name": "Zehao Li", "hidden": false}, {"_id": "6965f217fc8c4ecc02c7fa71", "name": "JingJing Xie", "hidden": false}, {"_id": "6965f217fc8c4ecc02c7fa72", "name": "Zhoumianze Liu", "hidden": false}, {"_id": "6965f217fc8c4ecc02c7fa73", "name": "Fangzhi Xu", "hidden": false}, {"_id": "6965f217fc8c4ecc02c7fa74", "name": "Kanzhi Cheng", "hidden": false}, {"_id": "6965f217fc8c4ecc02c7fa75", "name": "Qingyun Li", "hidden": false}, {"_id": "6965f217fc8c4ecc02c7fa76", "name": "Yian Wang", "hidden": false}, {"_id": "6965f217fc8c4ecc02c7fa77", "name": "Yu Qiao", "hidden": false}, {"_id": "6965f217fc8c4ecc02c7fa78", "name": "Zun Wang", "hidden": false}, {"_id": "6965f217fc8c4ecc02c7fa79", "user": {"_id": "642b9861bb77f8456634b048", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/VrNmmcdgX7FufQmdP5YaG.jpeg", "isPro": false, "fullname": "Zichen Ding", "user": "heroding77", "type": "user"}, "name": "Zichen Ding", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:16.476Z", "hidden": false}], "publishedAt": "2026-01-12T17:55:51.000Z", "submittedOnDailyAt": "2026-01-13T05:38:29.779Z", "title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent", "submittedOnDailyBy": {"_id": "642b9861bb77f8456634b048", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/VrNmmcdgX7FufQmdP5YaG.jpeg", "isPro": false, "fullname": "Zichen Ding", "user": "heroding77", "type": "user"}, "summary": "While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.", "upvotes": 22, "discussionId": "6965f217fc8c4ecc02c7fa7a", "projectPage": "https://os-copilot.github.io/OS-Symphony", "githubRepo": "https://github.com/OS-Copilot/OS-Symphony", "githubRepoAddedBy": "user", "ai_summary": "OS-Symphony presents a comprehensive framework for computer-using agents that enhances robustness in long-horizon tasks through reflection-memory and multimodal search capabilities.", "ai_keywords": ["Vision-Language Models", "Computer-Using Agents", "long-horizon workflows", "visual context curation", "visual-aware tutorial retrieval", "Orchestrator", "Reflection-Memory Agent", "milestone-driven long-term memory", "trajectory-level self-correction", "Versatile Tool Agents", "Multimodal Searcher", "SeeAct paradigm", "browser-based sandbox", "live visually aligned tutorials"], "githubStars": 15, "organization": {"_id": "61d8000084231b832e5bbd99", "name": "ustc", "fullname": "university of science and technology  of china", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1641545773772-61d7fdeb22a383817a543b68.png"}, "summary_zh": "<ul>\n    <li>Vision-Language Models (VLMs) \u5728\u8ba1\u7b97\u673a\u8f85\u52a9\u4ee3\u7406\uff08CUAs\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u5f88\u5927\u8fdb\u5c55\uff0c\u4f46\u5728\u957f\u65f6\u95f4\u5de5\u4f5c\u6d41\u7a0b\u548c\u65b0\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u4e0a\u4ecd\u7136\u5b58\u5728\u95ee\u9898\u3002</li>\n    <li>\u8fd9\u4e9b\u95ee\u9898\u7684\u539f\u56e0\u5305\u62ec\u5bf9\u5386\u53f2\u89c6\u89c9\u4e0a\u4e0b\u6587\u7684\u63a7\u5236\u4e0d\u8db3\u548c\u7f3a\u4e4f\u89c6\u89c9\u611f\u77e5\u7684\u6559\u7a0b\u68c0\u7d22\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86 OS-Symphony \u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u53cd\u601d\u8bb0\u5fc6\u4ee3\u7406\u548c\u591a\u529f\u80fd\u5de5\u5177\u4ee3\u7406\u3002</li>\n    <li>\u53cd\u601d\u8bb0\u5fc6\u4ee3\u7406\u4f7f\u7528\u957f\u671f\u8bb0\u5fc6\u6765\u63d0\u9ad8\u81ea\u6211\u4fee\u6b63\u80fd\u529b\uff0c\u51cf\u5c11\u5728\u957f\u65f6\u95f4\u4efb\u52a1\u4e2d\u7684\u89c6\u89c9\u4e0a\u4e0b\u6587\u635f\u5931\u3002</li>\n    <li>\u591a\u529f\u80fd\u5de5\u5177\u4ee3\u7406\u7ed3\u5408\u4e86\u591a\u6a21\u6001\u641c\u7d22\u5668\uff0c\u80fd\u591f\u5728\u6d4f\u89c8\u5668\u73af\u5883\u4e2d\u751f\u6210\u5b9e\u65f6\u7684\u89c6\u89c9\u5bf9\u9f50\u6559\u7a0b\uff0c\u4ece\u800c\u63d0\u9ad8\u5728\u65b0\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-Language Models (VLMs) have improved Computer-Using Agents (CUAs), but they struggle with complex tasks and new situations.</li>\n    <li>Current systems lack control over visual information and effective tutorial retrieval.</li>\n    <li>OS-Symphony is a new framework that includes an Orchestrator and two key features: a Reflection-Memory Agent for better task management and Versatile Tool Agents for finding relevant tutorials.</li>\n    <li>These innovations help maintain visual context and improve performance in challenging tasks.</li>\n    <li>OS-Symphony shows significant improvements in performance, achieving record results in tests, including a score of 65.84% on OSWorld.</li>\n</ul>"}, "publishedAt": "2026-01-12T12:55:51.000Z", "title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent", "summary": "While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07779.png", "numComments": 1, "submittedBy": {"_id": "642b9861bb77f8456634b048", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/VrNmmcdgX7FufQmdP5YaG.jpeg", "fullname": "Zichen Ding", "name": "heroding77", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12, "isUserFollowing": false}, "organization": {"_id": "61d8000084231b832e5bbd99", "name": "ustc", "fullname": "university of science and technology  of china", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1641545773772-61d7fdeb22a383817a543b68.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.07226", "authors": [{"_id": "6965bbbcfc8c4ecc02c7f907", "user": {"_id": "6550c4f27bbfce1878f5f280", "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg", "isPro": false, "fullname": "seongyun_lee", "user": "Seongyun", "type": "user"}, "name": "Seongyun Lee", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:24:04.198Z", "hidden": false}, {"_id": "6965bbbcfc8c4ecc02c7f908", "name": "Yongrae Jo", "hidden": false}, {"_id": "6965bbbcfc8c4ecc02c7f909", "name": "Minju Seo", "hidden": false}, {"_id": "6965bbbcfc8c4ecc02c7f90a", "name": "Moontae Lee", "hidden": false}, {"_id": "6965bbbcfc8c4ecc02c7f90b", "name": "Minjoon Seo", "hidden": false}], "publishedAt": "2026-01-12T05:43:51.000Z", "submittedOnDailyAt": "2026-01-13T01:11:30.100Z", "title": "Lost in the Noise: How Reasoning Models Fail with Contextual Distractors", "submittedOnDailyBy": {"_id": "6550c4f27bbfce1878f5f280", "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg", "isPro": false, "fullname": "seongyun_lee", "user": "Seongyun", "type": "user"}, "summary": "Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.", "upvotes": 22, "discussionId": "6965bbbcfc8c4ecc02c7f90c", "ai_summary": "NoisyBench benchmark reveals significant performance degradation in state-of-the-art models when exposed to noisy contextual information, with agentic workflows amplifying errors and attention mechanisms disproportionately focusing on distractor tokens.", "ai_keywords": ["RAG", "reasoning models", "agentic AI systems", "contextual distractors", "attention visualization", "reward modeling", "SFT", "outcome-reward RL", "Rationale-Aware Reward", "inverse scaling trend"], "organization": {"_id": "6475760c33192631bad2bb38", "name": "kaist-ai", "fullname": "KAIST AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u63a8\u7406\u6a21\u578b\u548c\u667a\u80fdAI\u7cfb\u7edf\u8d8a\u6765\u8d8a\u4f9d\u8d56\u5916\u90e8\u4fe1\u606f\uff0c\u4f46\u8fd9\u4e5f\u5e26\u6765\u4e86\u566a\u97f3\u8f93\u5165\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86NoisyBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u566a\u97f3\u7c7b\u578b\u4e0b\u7684\u9c81\u68d2\u6027\u3002</li>\n    <li>\u6d4b\u8bd5\u663e\u793a\uff0c\u5f53\u9762\u4e34\u5e72\u6270\u4fe1\u606f\u65f6\uff0c\u5148\u8fdb\u6a21\u578b\u7684\u8868\u73b0\u4e0b\u964d\u9ad8\u8fbe80%\u3002</li>\n    <li>\u6211\u4eec\u53d1\u73b0\uff0c\u667a\u80fd\u5de5\u4f5c\u6d41\u7a0b\u5f80\u5f80\u56e0\u4e3a\u8fc7\u5ea6\u4fe1\u4efb\u566a\u97f3\u5de5\u5177\u8f93\u51fa\u800c\u52a0\u5267\u9519\u8bef\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u7684Rationale-Aware Reward (RARE)\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8\u6a21\u578b\u5728\u566a\u97f3\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New AI systems rely on various external information, but this can be noisy, which current benchmarks do not address.</li>\n    <li>NoisyBench is a new benchmark that tests model strength across 11 datasets with different types of noise.</li>\n    <li>State-of-the-art models can see performance drop by up to 80% when dealing with distracting information.</li>\n    <li>Agentic workflows can worsen these errors by trusting noisy outputs too much, leading to misalignment issues.</li>\n    <li>A new method called Rationale-Aware Reward (RARE) helps improve model resilience by focusing on useful information amidst noise.</li>\n</ul>"}, "publishedAt": "2026-01-12T00:43:51.000Z", "title": "Lost in the Noise: How Reasoning Models Fail with Contextual Distractors", "summary": "Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07226.png", "numComments": 1, "submittedBy": {"_id": "6550c4f27bbfce1878f5f280", "avatarUrl": "/avatars/0ecedbcd8a55b2c4abd1da9e741a6652.svg", "fullname": "seongyun_lee", "name": "Seongyun", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "6475760c33192631bad2bb38", "name": "kaist-ai", "fullname": "KAIST AI", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6469949654873f0043b09c22/aaZFiyXe1qR-Dmy_xq67m.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.07351", "authors": [{"_id": "6965e693fc8c4ecc02c7fa43", "user": {"_id": "644238f6fcbe90d73b319ea6", "avatarUrl": "/avatars/06dcede0ea77344de7dded917706bcb2.svg", "isPro": false, "fullname": "zhongzero", "user": "zhongzero", "type": "user"}, "name": "Linhao Zhong", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:27.270Z", "hidden": false}, {"_id": "6965e693fc8c4ecc02c7fa44", "name": "Linyu Wu", "hidden": false}, {"_id": "6965e693fc8c4ecc02c7fa45", "name": "Bozhen Fang", "hidden": false}, {"_id": "6965e693fc8c4ecc02c7fa46", "name": "Tianjian Feng", "hidden": false}, {"_id": "6965e693fc8c4ecc02c7fa47", "name": "Chenchen Jing", "hidden": false}, {"_id": "6965e693fc8c4ecc02c7fa48", "name": "Wen Wang", "hidden": false}, {"_id": "6965e693fc8c4ecc02c7fa49", "name": "Jiaheng Zhang", "hidden": false}, {"_id": "6965e693fc8c4ecc02c7fa4a", "name": "Hao Chen", "hidden": false}, {"_id": "6965e693fc8c4ecc02c7fa4b", "name": "Chunhua Shen", "hidden": false}], "publishedAt": "2026-01-12T09:25:14.000Z", "submittedOnDailyAt": "2026-01-13T06:04:17.414Z", "title": "Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models", "submittedOnDailyBy": {"_id": "644238f6fcbe90d73b319ea6", "avatarUrl": "/avatars/06dcede0ea77344de7dded917706bcb2.svg", "isPro": false, "fullname": "zhongzero", "user": "zhongzero", "type": "user"}, "summary": "Diffusion Language Models (DLMs) offer a promising alternative for language modeling by enabling parallel decoding through iterative refinement. However, most DLMs rely on hard binary masking and discrete token assignments, which hinder the revision of early decisions and underutilize intermediate probabilistic representations. In this paper, we propose EvoToken-DLM, a novel diffusion-based language modeling approach that replaces hard binary masks with evolving soft token distributions. EvoToken-DLM enables a progressive transition from masked states to discrete outputs, supporting revisable decoding. To effectively support this evolution, we introduce continuous trajectory supervision, which aligns training objectives with iterative probabilistic updates. Extensive experiments across multiple benchmarks show that EvoToken-DLM consistently achieves superior performance, outperforming strong diffusion-based and masked DLM baselines. Project webpage: https://aim-uofa.github.io/EvoTokenDLM.", "upvotes": 19, "discussionId": "6965e693fc8c4ecc02c7fa4c", "projectPage": "https://aim-uofa.github.io/EvoTokenDLM/", "githubRepo": "https://github.com/aim-uofa/EvoTokenDLM", "githubRepoAddedBy": "user", "ai_summary": "EvoToken-DLM introduces a diffusion-based language modeling approach that uses soft token distributions and continuous trajectory supervision to enable revisable decoding and outperforms existing baselines.", "ai_keywords": ["diffusion language models", "hard binary masking", "discrete token assignments", "soft token distributions", "iterative refinement", "progressive transition", "continuous trajectory supervision", "revisable decoding"], "githubStars": 16, "summary_zh": "<ul>\n    <li>\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff08DLMs\uff09\u662f\u4e00\u79cd\u65b0\u7684\u8bed\u8a00\u5efa\u6a21\u65b9\u6cd5\uff0c\u652f\u6301\u5e76\u884c\u89e3\u7801\u548c\u8fed\u4ee3\u4f18\u5316\u3002</li>\n    <li>\u4f20\u7edfDLMs\u4f7f\u7528\u786c\u6027\u4e8c\u5143\u63a9\u7801\u548c\u79bb\u6563\u6807\u8bb0\uff0c\u8fd9\u9650\u5236\u4e86\u65e9\u671f\u51b3\u7b56\u7684\u4fee\u6b63\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86EvoToken-DLM\uff0c\u901a\u8fc7\u4f7f\u7528\u8f6f\u6027\u6807\u8bb0\u5206\u5e03\u66ff\u4ee3\u786c\u6027\u63a9\u7801\uff0c\u652f\u6301\u53ef\u4fee\u6b63\u7684\u89e3\u7801\u3002</li>\n    <li>\u5f15\u5165\u4e86\u8fde\u7eed\u8f68\u8ff9\u76d1\u7763\uff0c\u4f7f\u8bad\u7ec3\u76ee\u6807\u4e0e\u6982\u7387\u66f4\u65b0\u76f8\u4e00\u81f4\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cEvoToken-DLM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u6269\u6563\u548c\u63a9\u7801DLM\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion Language Models (DLMs) allow for faster language processing by refining outputs in steps.</li>\n    <li>Current DLMs often use strict methods that don't allow for changes to early decisions and waste useful information.</li>\n    <li>The new EvoToken-DLM uses flexible token distributions instead of strict binary masks, allowing for better adjustments during decoding.</li>\n    <li>EvoToken-DLM includes a training method called continuous trajectory supervision to improve its performance.</li>\n    <li>Tests show EvoToken-DLM performs better than existing DLM methods in various benchmarks.</li>\n</ul>"}, "publishedAt": "2026-01-12T04:25:14.000Z", "title": "Beyond Hard Masks: Progressive Token Evolution for Diffusion Language Models", "summary": "Diffusion Language Models (DLMs) offer a promising alternative for language modeling by enabling parallel decoding through iterative refinement. However, most DLMs rely on hard binary masking and discrete token assignments, which hinder the revision of early decisions and underutilize intermediate probabilistic representations. In this paper, we propose EvoToken-DLM, a novel diffusion-based language modeling approach that replaces hard binary masks with evolving soft token distributions. EvoToken-DLM enables a progressive transition from masked states to discrete outputs, supporting revisable decoding. To effectively support this evolution, we introduce continuous trajectory supervision, which aligns training objectives with iterative probabilistic updates. Extensive experiments across multiple benchmarks show that EvoToken-DLM consistently achieves superior performance, outperforming strong diffusion-based and masked DLM baselines. Project webpage: https://aim-uofa.github.io/EvoTokenDLM.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07351.png", "numComments": 1, "submittedBy": {"_id": "644238f6fcbe90d73b319ea6", "avatarUrl": "/avatars/06dcede0ea77344de7dded917706bcb2.svg", "fullname": "zhongzero", "name": "zhongzero", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05107", "authors": [{"_id": "69645331138cc47cbd76520a", "name": "Muzhao Tian", "hidden": false}, {"_id": "69645331138cc47cbd76520b", "user": {"_id": "662b6a8f0b7f23f3c000559e", "avatarUrl": "/avatars/0a5b4e09ac9a8e40342131319ff32b29.svg", "isPro": false, "fullname": "Zisu Huang", "user": "zisuh", "type": "user"}, "name": "Zisu Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:47.680Z", "hidden": false}, {"_id": "69645331138cc47cbd76520c", "name": "Xiaohua Wang", "hidden": false}, {"_id": "69645331138cc47cbd76520d", "name": "Jingwen Xu", "hidden": false}, {"_id": "69645331138cc47cbd76520e", "name": "Zhengkang Guo", "hidden": false}, {"_id": "69645331138cc47cbd76520f", "name": "Qi Qian", "hidden": false}, {"_id": "69645331138cc47cbd765210", "name": "Yuanzhe Shen", "hidden": false}, {"_id": "69645331138cc47cbd765211", "name": "Kaitao Song", "hidden": false}, {"_id": "69645331138cc47cbd765212", "name": "Jiakang Yuan", "hidden": false}, {"_id": "69645331138cc47cbd765213", "name": "Changze Lv", "hidden": false}, {"_id": "69645331138cc47cbd765214", "name": "Xiaoqing Zheng", "hidden": false}], "publishedAt": "2026-01-08T16:54:30.000Z", "submittedOnDailyAt": "2026-01-13T04:35:37.744Z", "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction", "submittedOnDailyBy": {"_id": "662b6a8f0b7f23f3c000559e", "avatarUrl": "/avatars/0a5b4e09ac9a8e40342131319ff32b29.svg", "isPro": false, "fullname": "Zisu Huang", "user": "zisuh", "type": "user"}, "summary": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to Memory Anchoring, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose Steerable Memory Agent, SteeM, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.", "upvotes": 18, "discussionId": "69645331138cc47cbd765215", "ai_summary": "A framework is presented that enables dynamic regulation of memory reliance in LLM-based agents, allowing users to control the balance between innovation and historical fidelity in long-term interactions.", "ai_keywords": ["LLM-based agents", "cumulative memory", "memory anchoring", "memory dependence", "SteeM framework", "personalized human-agent collaboration"], "summary_zh": "<ul>\n    <li>LLM\uff08\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u4ee3\u7406\u5728\u957f\u671f\u4e92\u52a8\u4e2d\u9700\u8981\u7d2f\u79ef\u8bb0\u5fc6\uff0c\u4ee5\u5b9e\u73b0\u4e2a\u6027\u5316\u548c\u4fdd\u6301\u98ce\u683c\u4e00\u81f4\u6027\u3002</li>\n    <li>\u73b0\u6709\u7cfb\u7edf\u7684\u8bb0\u5fc6\u4f7f\u7528\u65b9\u6cd5\u901a\u5e38\u662f\u201c\u5168\u6709\u6216\u5168\u65e0\u201d\uff0c\u5bfc\u81f4\u8bb0\u5fc6\u56fa\u5b9a\u6216\u4fe1\u606f\u4e22\u5931\u7684\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u63a7\u7684\u8bb0\u5fc6\u4f9d\u8d56\u6a21\u578b\uff0c\u5e2e\u52a9\u91cf\u5316\u8fc7\u53bb\u4e92\u52a8\u5bf9\u5f53\u524d\u8f93\u51fa\u7684\u5f71\u54cd\u3002</li>\n    <li>\u5f15\u5165\u4e86\u53ef\u8c03\u8bb0\u5fc6\u4ee3\u7406SteeM\uff0c\u7528\u6237\u53ef\u4ee5\u52a8\u6001\u8c03\u6574\u8bb0\u5fc6\u4f9d\u8d56\u7a0b\u5ea6\uff0c\u4ece\u800c\u4fc3\u8fdb\u521b\u65b0\u6216\u4fdd\u6301\u5386\u53f2\u4e00\u81f4\u6027\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSteeM\u5728\u4e2a\u6027\u5316\u4eba\u673a\u534f\u4f5c\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u63a7\u5236\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Long-term interactions with LLM-based agents benefit from having memory for personalization and consistency.</li>\n    <li>Current systems either use all past information, leading to issues, or ignore memory, losing valuable context.</li>\n    <li>We introduce a way to measure how much an agent relies on past interactions.</li>\n    <li>Our framework, SteeM, lets users adjust how much memory the agent uses, from fresh-start to high-fidelity modes.</li>\n    <li>Tests show that SteeM provides better control and effectiveness than traditional methods.</li>\n</ul>"}, "publishedAt": "2026-01-08T11:54:30.000Z", "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction", "summary": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to Memory Anchoring, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose Steerable Memory Agent, SteeM, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05107.png", "numComments": 2, "submittedBy": {"_id": "662b6a8f0b7f23f3c000559e", "avatarUrl": "/avatars/0a5b4e09ac9a8e40342131319ff32b29.svg", "fullname": "Zisu Huang", "name": "zisuh", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": true}],
    "week": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u5728\u89c6\u9891\u95ee\u7b54\u573a\u666f\u4e2d\uff0c\u89c6\u9891\u53ea\u63d0\u4f9b\u5c40\u90e8\u89c6\u89c9\u7ebf\u7d22\uff0c\u7b54\u6848\u5206\u6563\u5728\u7f51\u7edc\u4e0a\u3002</li>\n    <li>\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e2a\u95ee\u9898\uff0c\u5efa\u7acb\u4e86\u9996\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u2014\u2014VideoDR\u3002</li>\n    <li>VideoDR\u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u4e0b\u7684\u5f00\u653e\u57df\u95ee\u7b54\uff0c\u6d89\u53ca\u8de8\u5e27\u7ebf\u7d22\u63d0\u53d6\u548c\u591a\u8df3\u63a8\u7406\u3002</li>\n    <li>\u901a\u8fc7\u4eba\u7c7b\u6807\u6ce8\u548c\u8d28\u91cf\u63a7\u5236\uff0c\u83b7\u5f97\u4e86\u6db5\u76d6\u516d\u4e2a\u8bed\u4e49\u9886\u57df\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u6837\u672c\u3002</li>\n    <li>\u7814\u7a76\u663e\u793a\uff0c\u6a21\u578b\u5728\u957f\u68c0\u7d22\u94fe\u4e2d\u4fdd\u6301\u521d\u59cb\u89c6\u9891\u7ebf\u7d22\u7684\u80fd\u529b\u662f\u5173\u952e\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video question answering often requires combining visual clues from videos with information from the web.</li>\n    <li>To address this, the VideoDR benchmark was created for video-based question answering, focusing on extracting visual information, retrieving web data, and reasoning over that information.</li>\n    <li>VideoDR includes high-quality samples from six different topics, developed through careful human review.</li>\n    <li>Tests of various large language models showed that the effectiveness of different approaches (Workflow vs. Agentic) depends on the model's ability to keep track of visual information during long retrieval processes.</li>\n    <li>The study highlights challenges like goal drift and maintaining consistency over longer tasks, which are important for developing better video research agents.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u4eba\u7c7b\u5728\u5b66\u4f1a\u8bed\u8a00\u4e4b\u524d\u5c31\u80fd\u53d1\u5c55\u6838\u5fc3\u89c6\u89c9\u6280\u80fd\uff0c\u800c\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u6765\u5f25\u8865\u89c6\u89c9\u7406\u89e3\u7684\u4e0d\u8db3\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684MLLMs\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u8fd9\u4e9b\u4efb\u52a1\u4eba\u7c7b\u751a\u81f33\u5c81\u7684\u5c0f\u5b69\u90fd\u80fd\u8f7b\u677e\u89e3\u51b3\u3002</li>\n    <li>\u4e3a\u8bc4\u4f30MLLMs\u7684\u6838\u5fc3\u89c6\u89c9\u80fd\u529b\uff0c\u7814\u7a76\u56e2\u961f\u63a8\u51fa\u4e86BabyVision\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec388\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\u548c\u56db\u4e2a\u4e3b\u8981\u7c7b\u522b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u9886\u5148\u7684MLLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c6\uff0c\u4f8b\u5982Gemini3-Pro-Preview\u7684\u5f97\u5206\u4e3a49.7\uff0c\u8fdc\u4f4e\u4e8e6\u5c81\u513f\u7ae5\u548c94.1\u7684\u6210\u4eba\u5e73\u5747\u5206\u3002</li>\n    <li>BabyVision\u7684\u8fdb\u5c55\u6807\u5fd7\u7740\u5411\u4eba\u7c7b\u6c34\u5e73\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u7684\u8fc8\u8fdb\uff0c\u540c\u65f6\u63d0\u51fa\u4e86BabyVision-Gen\u548c\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\uff0c\u4ee5\u89e3\u51b3\u89c6\u89c9\u63a8\u7406\u95ee\u9898\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans develop visual skills early, but modern Multimodal LLMs (MLLMs) still depend on language to understand visuals.</li>\n    <li>There's a noticeable gap where top MLLMs struggle with basic visual tasks that young children can easily handle.</li>\n    <li>The BabyVision benchmark was created to test MLLMs' visual abilities without using language knowledge.</li>\n    <li>BabyVision includes 388 tasks across 22 subclasses, showing that MLLMs score much lower than humans on these tasks.</li>\n    <li>Current MLLMs, like Gemini3-Pro-Preview, score poorly compared to human averages, indicating they need to improve in visual understanding.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\uff0c\u5229\u7528\u89c6\u89c9\u7ebf\u7d22\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5ffd\u89c6\u4e86\u4eba\u7c7b\u5e38\u7528\u7684\u5730\u56fe\u7b56\u7565\u3002</li>\n    <li>\u672c\u7814\u7a76\u4e3a\u6a21\u578b\u589e\u6dfb\u4e86\u201c\u601d\u8003\u5730\u56fe\u201d\u7684\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u8bbe\u5b9a\u4e3a\u5730\u56fe\u4e2d\u7684\u4ee3\u7406\u5faa\u73af\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u7279\u522b\u662f\u51c6\u786e\u7387\u4ece8.0%\u63d0\u9ad8\u523022.1%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The image geolocalization task aims to find out where a photo was taken on Earth using visual clues.</li>\n    <li>Current models use world knowledge and reasoning but often ignore the human practice of using maps.</li>\n    <li>This study introduces a new approach that combines map usage with a two-step training process: reinforcement learning and test-time scaling.</li>\n    <li>The new method improves the model's ability to make better predictions by exploring different options before deciding.</li>\n    <li>Testing on real-world images shows that this approach significantly outperforms existing models in accuracy.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u8bed\u8a00\u6a21\u578b\u7684\u7528\u6237\u671f\u671b\u6a21\u578b\u63d0\u4f9b\u51c6\u786e\u7684\u56de\u7b54\u548c\u7b26\u5408\u591a\u6837\u5316\u4eba\u7c7b\u504f\u597d\u7684\u884c\u4e3a\u3002</li>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5f00\u59cb\u91c7\u7528\u591a\u4e2a\u5956\u52b1\u6765\u5f15\u5bfc\u6a21\u578b\u671d\u7740\u671f\u671b\u884c\u4e3a\u53d1\u5c55\u3002</li>\n    <li>\u76f4\u63a5\u5e94\u7528\u73b0\u6709\u7684Group Relative Policy Optimization\uff08GRPO\uff09\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u5956\u52b1\u5f52\u4e00\u5316\u5931\u6548\uff0c\u964d\u4f4e\u8bad\u7ec3\u4fe1\u53f7\u7684\u5206\u8fa8\u7387\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3aGroup reward-Decoupled Normalization Policy Optimization\uff08GDPO\uff09\uff0c\u53ef\u4ee5\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002</li>\n    <li>GDPO\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eGRPO\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u548c\u666e\u9002\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Users want language models to give accurate answers and behave according to different human preferences.</li>\n  <li>Reinforcement learning (RL) is now using multiple rewards to help models learn these diverse behaviors.</li>\n  <li>Applying Group Relative Policy Optimization (GRPO) to these rewards can cause problems, making them less effective during training.</li>\n  <li>The paper introduces a new method called Group reward-Decoupled Normalization Policy Optimization (GDPO) that keeps the rewards distinct, improving training stability and performance.</li>\n  <li>GDPO outperforms GRPO in tasks like tool calling, math reasoning, and coding reasoning, showing better accuracy and adherence to constraints.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05593", "authors": [{"_id": "6965b990fc8c4ecc02c7f8df", "name": "Jingcheng Hu", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e0", "name": "Yinmin Zhang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e1", "name": "Shijie Shang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e2", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e3", "name": "Yue Peng", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e4", "name": "Zhewei Huang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e5", "name": "Hebin Zhou", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e6", "name": "Xin Wu", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e7", "name": "Jie Cheng", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e8", "name": "Fanqi Wan", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8e9", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ea", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8eb", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ec", "name": "Ailin Huang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ed", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ee", "name": "Qi Han", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8ef", "name": "Zheng Ge", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8f0", "name": "Daxin Jiang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8f1", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6965b990fc8c4ecc02c7f8f2", "name": "Heung-Yeung Shum", "hidden": false}], "publishedAt": "2026-01-09T07:24:43.000Z", "submittedOnDailyAt": "2026-01-13T00:51:45.124Z", "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.", "upvotes": 62, "discussionId": "6965b990fc8c4ecc02c7f8f3", "githubRepo": "https://github.com/stepfun-ai/PaCoRe", "githubRepoAddedBy": "user", "ai_summary": "Parallel Coordinated Reasoning enables large-scale test-time compute scaling beyond sequential reasoning limitations through parallel exploration and message-passing architecture.", "ai_keywords": ["test-time compute", "sequential reasoning", "parallel exploration", "message-passing architecture", "reinforcement learning", "multi-million-token", "HMMT 2025", "GPT-5"], "githubStars": 261, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86\u5e76\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002</li>\n    <li>PaCoRe\u91c7\u7528\u6d88\u606f\u4f20\u9012\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u8f6e\u5e76\u884c\u63a2\u7d22\u6765\u63d0\u5347\u8ba1\u7b97\u80fd\u529b\uff0c\u800c\u4e0d\u662f\u4f20\u7edf\u7684\u987a\u5e8f\u63a8\u7406\u3002</li>\n    <li>\u6bcf\u8f6e\u5e76\u884c\u63a8\u7406\u751f\u6210\u7684\u7ed3\u679c\u88ab\u538b\u7f29\u4e3a\u6d88\u606f\uff0c\u7528\u4e8e\u6307\u5bfc\u4e0b\u4e00\u8f6e\u7684\u63a8\u7406\uff0c\u6700\u7ec8\u5f97\u51fa\u7b54\u6848\u3002</li>\n    <li>\u8be5\u6a21\u578b\u7ecf\u8fc7\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u80fd\u591f\u5904\u7406\u591a\u767e\u4e07\u6807\u8bb0\u7684\u6709\u6548\u8ba1\u7b97\uff0c\u800c\u4e0d\u8d85\u51fa\u4e0a\u4e0b\u6587\u9650\u5236\u3002</li>\n    <li>\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u6570\u5b66\u63a8\u7406\u4e0a\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Introducing a new framework called Parallel Coordinated Reasoning (PaCoRe) that helps language models think faster and handle more information at once.</li>\n    <li>PaCoRe uses a method that allows many reasoning paths to be explored at the same time, instead of just one after another.</li>\n    <li>The framework is trained with a special type of learning that helps it combine information effectively and work with large amounts of data.</li>\n    <li>PaCoRe shows significant improvements in solving problems, especially in mathematics, outperforming previous models like GPT-5.</li>\n    <li>All resources related to PaCoRe, including model files and training data, are shared publicly to help future research.</li>\n</ul>"}, "publishedAt": "2026-01-09T02:24:43.000Z", "title": "PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning", "summary": "We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05593.png", "numComments": 1, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 17, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.03509", "authors": [{"_id": "695fbc7d5b7998385e639349", "name": "Haochen Shi", "hidden": false}, {"_id": "695fbc7d5b7998385e63934a", "name": "Xingdi Yuan", "hidden": false}, {"_id": "695fbc7d5b7998385e63934b", "name": "Bang Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/654a97282d2fcd6bf2851173/sAKIzhLgfcEhgVZMBkHRW.png"], "publishedAt": "2026-01-07T01:43:25.000Z", "submittedOnDailyAt": "2026-01-08T11:50:05.687Z", "title": "Evolving Programmatic Skill Networks", "submittedOnDailyBy": {"_id": "654a97282d2fcd6bf2851173", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png", "isPro": false, "fullname": "Bang Liu", "user": "Bang-UdeM-Mila", "type": "user"}, "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.", "upvotes": 54, "discussionId": "695fbc7e5b7998385e63934c", "ai_summary": "Programmatic Skill Network enables continual skill acquisition through executable symbolic programs that evolve via reflection, progressive optimization, and structural refactoring mechanisms.", "ai_keywords": ["Programmatic Skill Network", "executable symbolic programs", "skill composition", "structured fault localization", "progressive optimization", "maturity-aware update gating", "canonical structural refactoring", "rollback validation", "neural network training", "skill reuse", "rapid adaptation", "generalization"], "organization": {"_id": "636e93488ba65db4a0987ab4", "name": "Universite-de-Montreal", "fullname": "Universit\u00e9 de Montr\u00e9al"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u7814\u7a76\u5728\u5f00\u653e\u5f0f\u73af\u5883\u4e2d\u6301\u7eed\u83b7\u53d6\u6280\u80fd\uff0c\u4ee3\u7406\u9700\u8981\u6784\u5efa\u548c\u4f18\u5316\u53ef\u6267\u884c\u6280\u80fd\u5e93\u3002</li>\n    <li>\u5f15\u5165\u4e86\u7a0b\u5e8f\u5316\u6280\u80fd\u7f51\u7edc\uff08PSN\uff09\uff0c\u5b83\u901a\u8fc7\u7ecf\u9a8c\u4e0d\u65ad\u6f14\u53d8\u7684\u7b26\u53f7\u7a0b\u5e8f\u7f51\u7edc\u3002</li>\n    <li>PSN\u5305\u62ec\u4e09\u4e2a\u6838\u5fc3\u673a\u5236\uff1a\u53cd\u601d\u673a\u5236\u3001\u6e10\u8fdb\u4f18\u5316\u548c\u6807\u51c6\u7ed3\u6784\u91cd\u6784\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cPSN\u5728\u6280\u80fd\u91cd\u7528\u3001\u5feb\u901f\u9002\u5e94\u548c\u5e7f\u6cdb\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002</li>\n    <li>\u6211\u4eec\u8ba1\u5212\u5f00\u6e90\u4ee3\u7801\uff0c\u4ee5\u4fbf\u66f4\u591a\u4eba\u4f7f\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study focuses on teaching agents new skills in changing environments where they can create and improve their skills over time.</li>\n    <li>They introduced a system called Programmatic Skill Network (PSN) that uses skills as programs that can be combined and improved.</li>\n    <li>PSN uses three main techniques: finding faults in skill combinations, updating skills while keeping reliable ones stable, and organizing skills efficiently.</li>\n    <li>Experiments showed that PSN allows agents to reuse skills effectively, adapt quickly, and perform well across various tasks.</li>\n    <li>The researchers plan to share their code with the public in the future.</li>\n</ul>"}, "publishedAt": "2026-01-06T20:43:25.000Z", "title": "Evolving Programmatic Skill Networks", "summary": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/654a97282d2fcd6bf2851173/sAKIzhLgfcEhgVZMBkHRW.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03509.png", "numComments": 1, "submittedBy": {"_id": "654a97282d2fcd6bf2851173", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png", "fullname": "Bang Liu", "name": "Bang-UdeM-Mila", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 10, "isUserFollowing": false}, "organization": {"_id": "636e93488ba65db4a0987ab4", "name": "Universite-de-Montreal", "fullname": "Universit\u00e9 de Montr\u00e9al"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.06002", "authors": [{"_id": "6964644c138cc47cbd76529b", "user": {"_id": "636f526a6cd69d9a36ff2b53", "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg", "isPro": false, "fullname": "Qiguang Chen", "user": "LightChen2333", "type": "user"}, "name": "Qiguang Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:33:48.803Z", "hidden": false}, {"_id": "6964644c138cc47cbd76529c", "name": "Yantao Du", "hidden": false}, {"_id": "6964644c138cc47cbd76529d", "name": "Ziniu Li", "hidden": false}, {"_id": "6964644c138cc47cbd76529e", "name": "Jinhao Liu", "hidden": false}, {"_id": "6964644c138cc47cbd76529f", "name": "Songyao Duan", "hidden": false}, {"_id": "6964644c138cc47cbd7652a0", "name": "Jiarui Guo", "hidden": false}, {"_id": "6964644c138cc47cbd7652a1", "name": "Minghao Liu", "hidden": false}, {"_id": "6964644c138cc47cbd7652a2", "name": "Jiaheng Liu", "hidden": false}, {"_id": "6964644c138cc47cbd7652a3", "name": "Tong Yang", "hidden": false}, {"_id": "6964644c138cc47cbd7652a4", "user": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "name": "Ge Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:33:51.170Z", "hidden": false}, {"_id": "6964644c138cc47cbd7652a5", "name": "Libo Qin", "hidden": false}, {"_id": "6964644c138cc47cbd7652a6", "name": "Wanxiang Che", "hidden": false}, {"_id": "6964644c138cc47cbd7652a7", "name": "Wenhao Huang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"], "publishedAt": "2026-01-09T18:39:01.000Z", "submittedOnDailyAt": "2026-01-12T01:02:27.368Z", "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning", "submittedOnDailyBy": {"_id": "636f526a6cd69d9a36ff2b53", "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg", "isPro": false, "fullname": "Qiguang Chen", "user": "LightChen2333", "type": "user"}, "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.", "upvotes": 38, "discussionId": "6964644c138cc47cbd7652a8", "ai_summary": "Large language models struggle with long chain-of-thought reasoning due to unstable structural patterns, but a molecular-inspired approach using effective semantic isomers and distribution-transfer-graph methods improves training stability and performance.", "ai_keywords": ["chain-of-thought", "large language models", "Long CoT", "fine-tuning", "entropy convergence", "semantic isomers", "distribution-transfer-graph", "molecular-like structures", "deep reasoning", "self-reflection", "self-exploration"], "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5b66\u4e60\u6709\u6548\u7684\u957f\u94fe\u601d\u7ef4\uff08Long CoT\uff09\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6709\u6548\u7684\u957f\u94fe\u601d\u7ef4\u8def\u5f84\u5177\u6709\u7a33\u5b9a\u7684\u5206\u5b50\u7ed3\u6784\uff0c\u7531\u6df1\u5ea6\u63a8\u7406\u3001\u81ea\u6211\u53cd\u601d\u548c\u81ea\u6211\u63a2\u7d22\u4e09\u79cd\u4ea4\u4e92\u7c7b\u578b\u7ec4\u6210\u3002</li>\n    <li>\u5206\u6790\u8868\u660e\uff0c\u8fd9\u4e9b\u7ed3\u6784\u662f\u901a\u8fc7\u957f\u94fe\u601d\u7ef4\u5fae\u8c03\u5f62\u6210\u7684\uff0c\u800c\u4e0d\u662f\u901a\u8fc7\u6a21\u4eff\u5173\u952e\u8bcd\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u6709\u6548\u7684\u8bed\u4e49\u5f02\u6784\u4f53\uff0c\u53d1\u73b0\u53ea\u6709\u4fc3\u8fdb\u5feb\u901f\u71b5\u6536\u655b\u7684\u8fde\u63a5\u624d\u652f\u6301\u7a33\u5b9a\u7684\u957f\u94fe\u601d\u7ef4\u5b66\u4e60\u3002</li>\n    <li>\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Mole-Syn\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u63d0\u9ad8\u957f\u94fe\u601d\u7ef4\u7ed3\u6784\u7684\u5408\u6210\u6027\u80fd\u548c\u5f3a\u5316\u5b66\u4e60\u7a33\u5b9a\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models struggle to learn long chain-of-thought reasoning effectively from other models or humans.</li>\n    <li>We suggest that effective Long CoT reasoning has stable structures similar to molecules, created by three types of interactions.</li>\n    <li>These interactions are Deep-Reasoning (strong connections), Self-Reflection (moderate connections), and Self-Exploration (weaker connections).</li>\n    <li>Our research shows that these stable structures come from fine-tuning Long CoT and not just copying keywords.</li>\n    <li>We developed a method called Mole-Syn to help create effective Long CoT structures, improving model performance and stability.</li>\n</ul>"}, "publishedAt": "2026-01-09T13:39:01.000Z", "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning", "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/636f526a6cd69d9a36ff2b53/zucZmtYcmLkZCc31rmCfq.png"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06002.png", "numComments": 1, "submittedBy": {"_id": "636f526a6cd69d9a36ff2b53", "avatarUrl": "/avatars/8f2271a193fcac609d9be270552b5afa.svg", "fullname": "Qiguang Chen", "name": "LightChen2333", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06953", "authors": [{"_id": "6965af14fc8c4ecc02c7f873", "name": "Jie Wu", "hidden": false}, {"_id": "6965af14fc8c4ecc02c7f874", "user": {"_id": "650be23ec4e52db6a4db63ef", "avatarUrl": "/avatars/03af548029b38bee49ec295fefe74f9a.svg", "isPro": false, "fullname": "Haoling Li", "user": "Ringo1110", "type": "user"}, "name": "Haoling Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:24:20.281Z", "hidden": false}, {"_id": "6965af14fc8c4ecc02c7f875", "name": "Xin Zhang", "hidden": false}, {"_id": "6965af14fc8c4ecc02c7f876", "name": "Jiani Guo", "hidden": false}, {"_id": "6965af14fc8c4ecc02c7f877", "name": "Jane Luo", "hidden": false}, {"_id": "6965af14fc8c4ecc02c7f878", "name": "Steven Liu", "hidden": false}, {"_id": "6965af14fc8c4ecc02c7f879", "name": "Yangyu Huang", "hidden": false}, {"_id": "6965af14fc8c4ecc02c7f87a", "name": "Ruihang Chu", "hidden": false}, {"_id": "6965af14fc8c4ecc02c7f87b", "name": "Scarlett Li", "hidden": false}, {"_id": "6965af14fc8c4ecc02c7f87c", "name": "Yujiu Yang", "hidden": false}], "publishedAt": "2026-01-11T15:22:33.000Z", "submittedOnDailyAt": "2026-01-13T01:12:35.419Z", "title": "X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Competitive programming presents great challenges for Code LLMs due to its intensive reasoning demands and high logical complexity. However, current Code LLMs still rely heavily on real-world data, which limits their scalability. In this paper, we explore a fully synthetic approach: training Code LLMs with entirely generated tasks, solutions, and test cases, to empower code reasoning models without relying on real-world data. To support this, we leverage feature-based synthesis to propose a novel data synthesis pipeline called SynthSmith. SynthSmith shows strong potential in producing diverse and challenging tasks, along with verified solutions and tests, supporting both supervised fine-tuning and reinforcement learning. Based on the proposed synthetic SFT and RL datasets, we introduce the X-Coder model series, which achieves a notable pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, outperforming DeepCoder-14B-Preview and AReal-boba2-14B despite having only 7B parameters. In-depth analysis reveals that scaling laws hold on our synthetic dataset, and we explore which dimensions are more effective to scale. We further provide insights into code-centric reinforcement learning and highlight the key factors that shape performance through detailed ablations and analysis. Our findings demonstrate that scaling high-quality synthetic data and adopting staged training can greatly advance code reasoning, while mitigating reliance on real-world coding data.", "upvotes": 30, "discussionId": "6965af14fc8c4ecc02c7f87d", "githubRepo": "https://github.com/JieWu02/X-Coder", "githubRepoAddedBy": "user", "ai_summary": "Code LLMs trained on fully synthetic data using a feature-based synthesis pipeline achieve superior performance on competitive programming benchmarks while reducing dependence on real-world coding datasets.", "ai_keywords": ["Code LLMs", "synthetic data", "feature-based synthesis", "data synthesis pipeline", "SynthSmith", "supervised fine-tuning", "reinforcement learning", "X-Coder model series", "LiveCodeBench", "scaling laws", "staged training", "code reasoning"], "githubStars": 52, "summary_zh": "<ul>\n    <li>\u7ade\u4e89\u7f16\u7a0b\u5bf9\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\uff08Code LLMs\uff09\u63d0\u51fa\u4e86\u5f88\u5927\u6311\u6218\uff0c\u4e3b\u8981\u56e0\u4e3a\u9700\u8981\u590d\u6742\u7684\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u76ee\u524d\u7684\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u4ecd\u4f9d\u8d56\u4e8e\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u6269\u5c55\u6027\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u5408\u6210\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u4efb\u52a1\u3001\u89e3\u51b3\u65b9\u6848\u548c\u6d4b\u8bd5\u6848\u4f8b\u6765\u8bad\u7ec3\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aSynthSmith\u7684\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u80fd\u591f\u4ea7\u751f\u591a\u6837\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u53ca\u9a8c\u8bc1\u8fc7\u7684\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u57fa\u4e8e\u5408\u6210\u6570\u636e\u96c6\uff0c\u6211\u4eec\u5f15\u5165\u4e86X-Coder\u6a21\u578b\u7cfb\u5217\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u8868\u73b0\uff0c\u5c55\u793a\u4e86\u5408\u6210\u6570\u636e\u7684\u4f18\u52bf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Competitive programming is tough for Code LLMs because it requires deep reasoning and complex logic.</li>\n    <li>Current Code LLMs depend on real-world data, which limits their growth and scalability.</li>\n    <li>This paper introduces a new approach that uses only synthetic data for training Code LLMs, through a method called SynthSmith.</li>\n    <li>SynthSmith can create diverse programming tasks and solutions, leading to strong performance in code reasoning models like the X-Coder series.</li>\n    <li>The findings show that using high-quality synthetic data can improve code reasoning capabilities, reducing the need for real-world examples.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:22:33.000Z", "title": "X-Coder: Advancing Competitive Programming with Fully Synthetic Tasks, Solutions, and Tests", "summary": "Competitive programming presents great challenges for Code LLMs due to its intensive reasoning demands and high logical complexity. However, current Code LLMs still rely heavily on real-world data, which limits their scalability. In this paper, we explore a fully synthetic approach: training Code LLMs with entirely generated tasks, solutions, and test cases, to empower code reasoning models without relying on real-world data. To support this, we leverage feature-based synthesis to propose a novel data synthesis pipeline called SynthSmith. SynthSmith shows strong potential in producing diverse and challenging tasks, along with verified solutions and tests, supporting both supervised fine-tuning and reinforcement learning. Based on the proposed synthetic SFT and RL datasets, we introduce the X-Coder model series, which achieves a notable pass rate of 62.9 avg@8 on LiveCodeBench v5 and 55.8 on v6, outperforming DeepCoder-14B-Preview and AReal-boba2-14B despite having only 7B parameters. In-depth analysis reveals that scaling laws hold on our synthetic dataset, and we explore which dimensions are more effective to scale. We further provide insights into code-centric reinforcement learning and highlight the key factors that shape performance through detailed ablations and analysis. Our findings demonstrate that scaling high-quality synthetic data and adopting staged training can greatly advance code reasoning, while mitigating reliance on real-world coding data.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06953.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.03872", "authors": [{"_id": "695f1a475fa3847525c41d06", "user": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "name": "Jinyang Wu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-08T08:32:36.055Z", "hidden": false}, {"_id": "695f1a475fa3847525c41d07", "name": "Guocheng Zhai", "hidden": false}, {"_id": "695f1a475fa3847525c41d08", "name": "Ruihan Jin", "hidden": false}, {"_id": "695f1a475fa3847525c41d09", "name": "Jiahao Yuan", "hidden": false}, {"_id": "695f1a475fa3847525c41d0a", "name": "Yuhao Shen", "hidden": false}, {"_id": "695f1a475fa3847525c41d0b", "name": "Shuai Zhang", "hidden": false}, {"_id": "695f1a475fa3847525c41d0c", "name": "Zhengqi Wen", "hidden": false}, {"_id": "695f1a475fa3847525c41d0d", "name": "Jianhua Tao", "hidden": false}], "publishedAt": "2026-01-07T12:38:33.000Z", "submittedOnDailyAt": "2026-01-08T04:50:03.287Z", "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning", "submittedOnDailyBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "isPro": false, "fullname": "Jinyang Wu", "user": "Jinyang23", "type": "user"}, "summary": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.", "upvotes": 30, "discussionId": "695f1a475fa3847525c41d0e", "ai_summary": "ATLAS is a dual-path framework that dynamically selects optimal model-tool combinations for cross-domain reasoning through cluster-based routing and reinforcement learning-based multi-step routing, achieving superior performance on complex reasoning tasks.", "ai_keywords": ["large language models", "external tools", "model-tool combination", "high-dimensional optimization", "dual-path framework", "training-free cluster-based routing", "RL-based multi-step routing", "cross-domain complex reasoning", "domain-specific alignment", "out-of-distribution generalization"], "summary_zh": "<ul>\n    <li>\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u5916\u90e8\u5de5\u5177\u7ed3\u5408\uff0c\u63d0\u9ad8\u4e86\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u80fd\u529b\u3002</li>\n    <li>\u9009\u62e9\u6700\u4f73\u7684\u6a21\u578b-\u5de5\u5177\u7ec4\u5408\u6210\u4e3a\u4e00\u4e2a\u590d\u6742\u7684\u4f18\u5316\u95ee\u9898\u3002</li>\n    <li>\u63d0\u51fa\u4e86ATLAS\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u8def\u5f84\u65b9\u6cd5\u5b9e\u73b0\u52a8\u6001\u5de5\u5177\u4f7f\u7528\u3002</li>\n    <li>ATLAS\u901a\u8fc7\u65e0\u8bad\u7ec3\u7684\u805a\u7c7b\u8def\u7531\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u6b65\u8def\u7531\u6765\u4f18\u5316\u6a21\u578b-\u5de5\u5177\u5bf9\u7684\u5339\u914d\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cATLAS\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u548c\u8def\u7531\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u8868\u73b0\u663e\u8457\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large language models (LLMs) can now work with external tools to enhance AI capabilities.</li>\n    <li>Choosing the best LLM-tool combination is a complex challenge due to the variety of options available.</li>\n    <li>The paper introduces ATLAS, a new framework that helps select tools dynamically for better reasoning across different areas.</li>\n    <li>ATLAS uses two main methods: one for clustering models based on their strengths, and another that learns from experience for better performance.</li>\n    <li>Tests show that ATLAS outperforms existing models and methods, especially in visual reasoning tasks.</li>\n</ul>"}, "publishedAt": "2026-01-07T07:38:33.000Z", "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning", "summary": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) training-free cluster-based routing that exploits empirical priors for domain-specific alignment, and (2) RL-based multi-step routing that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03872.png", "numComments": 1, "submittedBy": {"_id": "6747de57f8cab58c22ec94a2", "avatarUrl": "/avatars/5bae0341862fac24564781c0fa32aac5.svg", "fullname": "Jinyang Wu", "name": "Jinyang23", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06021", "authors": [{"_id": "69645f30138cc47cbd765248", "name": "Jiajie Zhang", "hidden": false}, {"_id": "69645f30138cc47cbd765249", "name": "Xin Lv", "hidden": false}, {"_id": "69645f30138cc47cbd76524a", "name": "Ling Feng", "hidden": false}, {"_id": "69645f30138cc47cbd76524b", "name": "Lei Hou", "hidden": false}, {"_id": "69645f30138cc47cbd76524c", "name": "Juanzi Li", "hidden": false}], "publishedAt": "2026-01-09T18:57:53.000Z", "submittedOnDailyAt": "2026-01-12T00:13:19.034Z", "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards", "submittedOnDailyBy": {"_id": "66cdd285c51a915bd5f2d017", "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg", "isPro": false, "fullname": "Jiajie Zhang", "user": "NeoZ123", "type": "user"}, "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.", "upvotes": 30, "discussionId": "69645f30138cc47cbd76524d", "githubRepo": "https://github.com/THUDM/CaRR", "githubRepoAddedBy": "user", "ai_summary": "A citation-aware reward framework and policy optimization method improve deep search agents' reasoning comprehensiveness and factual accuracy while reducing shortcut exploitation and hallucinations.", "ai_keywords": ["reinforcement learning", "deep search agents", "fine-grained reward framework", "reasoning comprehensiveness", "factual grounding", "evidence connectivity", "verifiable single-hop rubrics", "citation-aware group relative policy optimization", "outcome rewards", "shortcut exploitation", "hallucinations"], "githubStars": 15, "organization": {"_id": "62ad27f19096e7f9ecb1853a", "name": "zai-org", "fullname": "Z.ai", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u63d0\u5347\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6df1\u5ea6\u641c\u7d22\u4ee3\u7406\u65b9\u9762\u53d8\u5f97\u975e\u5e38\u91cd\u8981\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8c\u5143\u7ed3\u679c\u5956\u52b1\uff0c\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u4ee3\u7406\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u5bfc\u81f4\u4e0d\u826f\u884c\u4e3a\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u201c\u5f15\u7528\u611f\u77e5\u8bc4\u5206\u5956\u52b1\u201d\uff08CaRR\uff09\uff0c\u5f3a\u8c03\u63a8\u7406\u7684\u5168\u9762\u6027\u3001\u4e8b\u5b9e\u57fa\u7840\u548c\u8bc1\u636e\u8fde\u63a5\u6027\u3002</li>\n    <li>CaRR\u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u53ef\u9a8c\u8bc1\u7684\u5355\u6b65\u8bc4\u5206\uff0c\u8981\u6c42\u4ee3\u7406\u660e\u786e\u8bc6\u522b\u9690\u85cf\u5b9e\u4f53\u5e76\u63d0\u4f9b\u6b63\u786e\u5f15\u7528\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0c\u201c\u5f15\u7528\u611f\u77e5\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u201d\uff08C-GRPO\uff09\u5728\u591a\u4e2a\u6df1\u5ea6\u641c\u7d22\u57fa\u51c6\u4e0a\u4f18\u4e8e\u6807\u51c6\u5956\u52b1\u57fa\u7840\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) helps improve deep search agents that use large language models (LLMs).</li>\n    <li>Current methods use simple rewards which don't fully capture how well agents reason or find facts, leading to mistakes.</li>\n    <li>We propose a new reward system called Citation-aware Rubric Rewards (CaRR) that focuses on thorough reasoning and accurate evidence.</li>\n    <li>CaRR breaks down complex questions into simpler parts, requiring agents to provide clear evidence and citations for their answers.</li>\n    <li>Our new training method, Citation-aware Group Relative Policy Optimization (C-GRPO), shows better results than standard methods and prevents agents from taking shortcuts.</li>\n</ul>"}, "publishedAt": "2026-01-09T13:57:53.000Z", "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards", "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer. We further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents. Experiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06021.png", "numComments": 1, "submittedBy": {"_id": "66cdd285c51a915bd5f2d017", "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg", "fullname": "Jiajie Zhang", "name": "NeoZ123", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "62ad27f19096e7f9ecb1853a", "name": "zai-org", "fullname": "Z.ai", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62dc173789b4cf157d36ebee/i_pxzM2ZDo3Ub-BEgIkE9.png"}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u5728\u89c6\u9891\u95ee\u7b54\u573a\u666f\u4e2d\uff0c\u89c6\u9891\u53ea\u63d0\u4f9b\u5c40\u90e8\u89c6\u89c9\u7ebf\u7d22\uff0c\u7b54\u6848\u5206\u6563\u5728\u7f51\u7edc\u4e0a\u3002</li>\n    <li>\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e2a\u95ee\u9898\uff0c\u5efa\u7acb\u4e86\u9996\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u2014\u2014VideoDR\u3002</li>\n    <li>VideoDR\u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u4e0b\u7684\u5f00\u653e\u57df\u95ee\u7b54\uff0c\u6d89\u53ca\u8de8\u5e27\u7ebf\u7d22\u63d0\u53d6\u548c\u591a\u8df3\u63a8\u7406\u3002</li>\n    <li>\u901a\u8fc7\u4eba\u7c7b\u6807\u6ce8\u548c\u8d28\u91cf\u63a7\u5236\uff0c\u83b7\u5f97\u4e86\u6db5\u76d6\u516d\u4e2a\u8bed\u4e49\u9886\u57df\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u6837\u672c\u3002</li>\n    <li>\u7814\u7a76\u663e\u793a\uff0c\u6a21\u578b\u5728\u957f\u68c0\u7d22\u94fe\u4e2d\u4fdd\u6301\u521d\u59cb\u89c6\u9891\u7ebf\u7d22\u7684\u80fd\u529b\u662f\u5173\u952e\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video question answering often requires combining visual clues from videos with information from the web.</li>\n    <li>To address this, the VideoDR benchmark was created for video-based question answering, focusing on extracting visual information, retrieving web data, and reasoning over that information.</li>\n    <li>VideoDR includes high-quality samples from six different topics, developed through careful human review.</li>\n    <li>Tests of various large language models showed that the effectiveness of different approaches (Workflow vs. Agentic) depends on the model's ability to keep track of visual information during long retrieval processes.</li>\n    <li>The study highlights challenges like goal drift and maintaining consistency over longer tasks, which are important for developing better video research agents.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16676", "authors": [{"_id": "6949026334f46eaf46cbb3d1", "name": "Hao Liang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d2", "name": "Xiaochen Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d3", "name": "Zhou Liu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d4", "name": "Zhen Hao Wong", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d5", "name": "Zhengyang Zhao", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d6", "name": "Zimo Meng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d7", "name": "Runming He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d8", "name": "Chengyu Shen", "hidden": false}, {"_id": "6949026334f46eaf46cbb3d9", "name": "Qifeng Cai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3da", "name": "Zhaoyang Han", "hidden": false}, {"_id": "6949026334f46eaf46cbb3db", "name": "Meiyi Qiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dc", "name": "Yalin Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3dd", "name": "Tianyi Bai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3de", "name": "Zewei Pan", "hidden": false}, {"_id": "6949026334f46eaf46cbb3df", "name": "Ziyi Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e0", "name": "Yizhen Jiang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e1", "name": "Jingwen Deng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e2", "name": "Qijie You", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e3", "name": "Peichao Lai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e4", "name": "Tianyu Guo", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e5", "name": "Chi Hsu Tsai", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e6", "name": "Hengyi Feng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e7", "name": "Rui Hu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e8", "name": "Wenkai Yu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3e9", "name": "Junbo Niu", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ea", "name": "Bohan Zeng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3eb", "name": "Ruichuan An", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ec", "name": "Lu Ma", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ed", "name": "Jihao Huang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ee", "name": "Yaowei Zheng", "hidden": false}, {"_id": "6949026334f46eaf46cbb3ef", "name": "Conghui He", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f0", "name": "Linpeng Tang", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f1", "name": "Bin Cui", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f2", "name": "Weinan E", "hidden": false}, {"_id": "6949026334f46eaf46cbb3f3", "name": "Wentao Zhang", "hidden": false}], "publishedAt": "2025-12-18T15:46:15.000Z", "submittedOnDailyAt": "2025-12-23T01:07:14.287Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "submittedOnDailyBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "isPro": false, "fullname": "bohan zeng", "user": "zbhpku", "type": "user"}, "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "upvotes": 155, "discussionId": "6949026334f46eaf46cbb3f4", "projectPage": "https://github.com/OpenDCAI/DataFlow", "ai_summary": "DataFlow is an LLM-driven data preparation framework that enhances data quality and reproducibility for various tasks, improving LLM performance with automatically generated pipelines.", "ai_keywords": ["DataFlow", "Large Language Models (LLMs)", "data preparation pipelines", "system-level abstractions", "PyTorch-style pipeline construction API", "reusable operators", "domain-general pipelines", "Text-to-SQL", "agentic RAG", "large-scale knowledge extraction", "DataFlow-Agent", "operator synthesis", "pipeline planning", "iterative verification"], "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "summary_zh": "<ul>\n    <li>\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u9700\u6c42\u8fc5\u901f\u589e\u957f\uff0c\u6570\u636e\u51c6\u5907\u6d41\u7a0b\u4e9f\u9700\u53ef\u6269\u5c55\u548c\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u76ee\u524d\u7684\u6570\u636e\u51c6\u5907\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e34\u65f6\u811a\u672c\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u62bd\u8c61\uff0c\u5f71\u54cd\u4e86\u53ef\u91cd\u590d\u6027\u548c\u6a21\u578b\u6570\u636e\u751f\u6210\u7684\u652f\u6301\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86DataFlow\uff0c\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u6269\u5c55\u7684\u6570\u636e\u51c6\u5907\u6846\u67b6\uff0c\u652f\u6301\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u7684\u6570\u636e\u8f6c\u6362\u3002</li>\n    <li>DataFlow\u63d0\u4f9b\u4e86\u8fd1200\u4e2a\u53ef\u91cd\u7528\u64cd\u4f5c\u7b26\u548c\u516d\u79cd\u901a\u7528\u7ba1\u9053\uff0c\u9002\u7528\u4e8e\u6587\u672c\u3001\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u7b49\u591a\u4e2a\u9886\u57df\u3002</li>\n    <li>\u901a\u8fc7DataFlow\u7684\u81ea\u52a8\u5316\u5de5\u5177\uff0c\u7528\u6237\u53ef\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u89c4\u683c\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u7ba1\u9053\uff0c\u663e\u8457\u63d0\u5347LLM\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>There is a growing need for high-quality data in Large Language Models (LLMs), but current methods for preparing this data are often unorganized and inconsistent.</li>\n    <li>DataFlow is a new framework that helps create better data preparation pipelines, allowing for modular and reusable data transformations.</li>\n    <li>It includes almost 200 reusable tools and six general pipelines for different tasks like text processing, code generation, and knowledge extraction.</li>\n    <li>DataFlow-Agent can convert natural language instructions into executable data pipelines, making it easier to use.</li>\n    <li>The framework shows improved performance in various tasks, outperforming traditional datasets and demonstrating its potential for future AI development.</li>\n</ul>"}, "publishedAt": "2025-12-18T10:46:15.000Z", "title": "DataFlow: An LLM-Driven Framework for Unified Data Preparation and Workflow Automation in the Era of Data-Centric AI", "summary": "The rapidly growing demand for high-quality data in Large Language Models (LLMs) has intensified the need for scalable, reliable, and semantically rich data preparation pipelines. However, current practices remain dominated by ad-hoc scripts and loosely specified workflows, which lack principled abstractions, hinder reproducibility, and offer limited support for model-in-the-loop data generation. To address these challenges, we present DataFlow, a unified and extensible LLM-driven data preparation framework. DataFlow is designed with system-level abstractions that enable modular, reusable, and composable data transformations, and provides a PyTorch-style pipeline construction API for building debuggable and optimizable dataflows. The framework consists of nearly 200 reusable operators and six domain-general pipelines spanning text, mathematical reasoning, code, Text-to-SQL, agentic RAG, and large-scale knowledge extraction. To further improve usability, we introduce DataFlow-Agent, which automatically translates natural-language specifications into executable pipelines via operator synthesis, pipeline planning, and iterative verification. Across six representative use cases, DataFlow consistently improves downstream LLM performance. Our math, code, and text pipelines outperform curated human datasets and specialized synthetic baselines, achieving up to +3\\% execution accuracy in Text-to-SQL over SynSQL, +7\\% average improvements on code benchmarks, and 1--3 point gains on MATH, GSM8K, and AIME. Moreover, a unified 10K-sample dataset produced by DataFlow enables base models to surpass counterparts trained on 1M Infinity-Instruct data. These results demonstrate that DataFlow provides a practical and high-performance substrate for reliable, reproducible, and scalable LLM data preparation, and establishes a system-level foundation for future data-centric AI development.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16676.png", "numComments": 4, "submittedBy": {"_id": "6671214c92412fd4640714eb", "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg", "fullname": "bohan zeng", "name": "zbhpku", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4}, "organization": {"_id": "61dcd8e344f59573371b5cb6", "name": "PekingUniversity", "fullname": "Peking University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/vavgrBsnkSejriUF4lXDE.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u4eba\u7c7b\u5728\u5b66\u4f1a\u8bed\u8a00\u4e4b\u524d\u5c31\u80fd\u53d1\u5c55\u6838\u5fc3\u89c6\u89c9\u6280\u80fd\uff0c\u800c\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u6765\u5f25\u8865\u89c6\u89c9\u7406\u89e3\u7684\u4e0d\u8db3\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684MLLMs\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u8fd9\u4e9b\u4efb\u52a1\u4eba\u7c7b\u751a\u81f33\u5c81\u7684\u5c0f\u5b69\u90fd\u80fd\u8f7b\u677e\u89e3\u51b3\u3002</li>\n    <li>\u4e3a\u8bc4\u4f30MLLMs\u7684\u6838\u5fc3\u89c6\u89c9\u80fd\u529b\uff0c\u7814\u7a76\u56e2\u961f\u63a8\u51fa\u4e86BabyVision\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ec388\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\u548c\u56db\u4e2a\u4e3b\u8981\u7c7b\u522b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u9886\u5148\u7684MLLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c6\uff0c\u4f8b\u5982Gemini3-Pro-Preview\u7684\u5f97\u5206\u4e3a49.7\uff0c\u8fdc\u4f4e\u4e8e6\u5c81\u513f\u7ae5\u548c94.1\u7684\u6210\u4eba\u5e73\u5747\u5206\u3002</li>\n    <li>BabyVision\u7684\u8fdb\u5c55\u6807\u5fd7\u7740\u5411\u4eba\u7c7b\u6c34\u5e73\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u7684\u8fc8\u8fdb\uff0c\u540c\u65f6\u63d0\u51fa\u4e86BabyVision-Gen\u548c\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\uff0c\u4ee5\u89e3\u51b3\u89c6\u89c9\u63a8\u7406\u95ee\u9898\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans develop visual skills early, but modern Multimodal LLMs (MLLMs) still depend on language to understand visuals.</li>\n    <li>There's a noticeable gap where top MLLMs struggle with basic visual tasks that young children can easily handle.</li>\n    <li>The BabyVision benchmark was created to test MLLMs' visual abilities without using language knowledge.</li>\n    <li>BabyVision includes 388 tasks across 22 subclasses, showing that MLLMs score much lower than humans on these tasks.</li>\n    <li>Current MLLMs, like Gemini3-Pro-Preview, score poorly compared to human averages, indicating they need to improve in visual understanding.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\uff0c\u5229\u7528\u89c6\u89c9\u7ebf\u7d22\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5ffd\u89c6\u4e86\u4eba\u7c7b\u5e38\u7528\u7684\u5730\u56fe\u7b56\u7565\u3002</li>\n    <li>\u672c\u7814\u7a76\u4e3a\u6a21\u578b\u589e\u6dfb\u4e86\u201c\u601d\u8003\u5730\u56fe\u201d\u7684\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u8bbe\u5b9a\u4e3a\u5730\u56fe\u4e2d\u7684\u4ee3\u7406\u5faa\u73af\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u7279\u522b\u662f\u51c6\u786e\u7387\u4ece8.0%\u63d0\u9ad8\u523022.1%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The image geolocalization task aims to find out where a photo was taken on Earth using visual clues.</li>\n    <li>Current models use world knowledge and reasoning but often ignore the human practice of using maps.</li>\n    <li>This study introduces a new approach that combines map usage with a two-step training process: reinforcement learning and test-time scaling.</li>\n    <li>The new method improves the model's ability to make better predictions by exploring different options before deciding.</li>\n    <li>Testing on real-world images shows that this approach significantly outperforms existing models in accuracy.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.16776", "authors": [{"_id": "6944bd29fbf17e708e185f72", "name": "Kling Team", "hidden": false}, {"_id": "6944bd29fbf17e708e185f73", "name": "Jialu Chen", "hidden": false}, {"_id": "6944bd29fbf17e708e185f74", "name": "Yuanzheng Ci", "hidden": false}, {"_id": "6944bd29fbf17e708e185f75", "name": "Xiangyu Du", "hidden": false}, {"_id": "6944bd29fbf17e708e185f76", "name": "Zipeng Feng", "hidden": false}, {"_id": "6944bd29fbf17e708e185f77", "name": "Kun Gai", "hidden": false}, {"_id": "6944bd29fbf17e708e185f78", "name": "Sainan Guo", "hidden": false}, {"_id": "6944bd29fbf17e708e185f79", "name": "Feng Han", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7a", "name": "Jingbin He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7b", "name": "Kang He", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7c", "name": "Xiao Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7d", "name": "Xiaohua Hu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7e", "name": "Boyuan Jiang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f7f", "name": "Fangyuan Kong", "hidden": false}, {"_id": "6944bd29fbf17e708e185f80", "name": "Hang Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f81", "name": "Jie Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f82", "name": "Qingyu Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f83", "name": "Shen Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f84", "name": "Xiaohan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f85", "name": "Yan Li", "hidden": false}, {"_id": "6944bd29fbf17e708e185f86", "name": "Jiajun Liang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f87", "name": "Borui Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f88", "name": "Yiqiao Liao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f89", "name": "Weihong Lin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8a", "name": "Quande Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8b", "name": "Xiaokun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8c", "name": "Yilun Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8d", "name": "Yuliang Liu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8e", "name": "Shun Lu", "hidden": false}, {"_id": "6944bd29fbf17e708e185f8f", "name": "Hangyu Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f90", "name": "Yunyao Mao", "hidden": false}, {"_id": "6944bd29fbf17e708e185f91", "name": "Haodong Ouyang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f92", "name": "Wenyu Qin", "hidden": false}, {"_id": "6944bd29fbf17e708e185f93", "name": "Wanqi Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f94", "name": "Xiaoyu Shi", "hidden": false}, {"_id": "6944bd29fbf17e708e185f95", "name": "Lianghao Su", "hidden": false}, {"_id": "6944bd29fbf17e708e185f96", "name": "Haozhi Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f97", "name": "Peiqin Sun", "hidden": false}, {"_id": "6944bd29fbf17e708e185f98", "name": "Pengfei Wan", "hidden": false}, {"_id": "6944bd29fbf17e708e185f99", "name": "Chao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9a", "name": "Chenyu Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9b", "name": "Meng Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9c", "name": "Qiulin Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9d", "name": "Runqi Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9e", "name": "Xintao Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185f9f", "name": "Xuebo Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa0", "name": "Zekun Wang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa1", "name": "Min Wei", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa2", "name": "Tiancheng Wen", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa3", "name": "Guohao Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa4", "name": "Xiaoshi Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa5", "name": "Zhenhua Wu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa6", "name": "Da Xie", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa7", "name": "Yingtong Xiong", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa8", "name": "Yulong Xu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fa9", "name": "Sile Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faa", "name": "Zikang Yang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fab", "name": "Weicai Ye", "hidden": false}, {"_id": "6944bd29fbf17e708e185fac", "name": "Ziyang Yuan", "hidden": false}, {"_id": "6944bd29fbf17e708e185fad", "name": "Shenglong Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fae", "name": "Shuaiyu Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185faf", "name": "Yuanxing Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb0", "name": "Yufan Zhang", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb1", "name": "Wenzheng Zhao", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb2", "name": "Ruiliang Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb3", "name": "Yan Zhou", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb4", "name": "Guosheng Zhu", "hidden": false}, {"_id": "6944bd29fbf17e708e185fb5", "name": "Yongjie Zhu", "hidden": false}], "publishedAt": "2025-12-18T17:08:12.000Z", "submittedOnDailyAt": "2025-12-19T00:19:38.857Z", "title": "Kling-Omni Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "upvotes": 110, "discussionId": "6944bd29fbf17e708e185fb6", "ai_summary": "Kling-Omni is a versatile generative framework that synthesizes high-quality videos from multimodal inputs, integrating generation, editing, and reasoning into a unified system.", "ai_keywords": ["generative framework", "multimodal visual language inputs", "end-to-end", "video generation", "editing", "intelligent reasoning", "unified multimodal representation", "cinematic-quality", "efficient large-scale pre-training", "inference optimizations", "in-context generation", "reasoning-based editing", "multimodal instruction following", "multimodal world simulators"], "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "summary_zh": "<ul>\n    <li>Kling-Omni\u662f\u4e00\u4e2a\u901a\u7528\u7684\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u6839\u636e\u591a\u79cd\u89c6\u89c9\u8bed\u8a00\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002</li>\n    <li>\u5b83\u5c06\u89c6\u9891\u751f\u6210\u3001\u7f16\u8f91\u548c\u667a\u80fd\u63a8\u7406\u4efb\u52a1\u6574\u5408\u6210\u4e00\u4e2a\u6574\u4f53\u7cfb\u7edf\uff0c\u63d0\u4f9b\u66f4\u6d41\u7545\u7684\u7528\u6237\u4f53\u9a8c\u3002</li>\n    <li>Kling-Omni\u652f\u6301\u591a\u79cd\u7528\u6237\u8f93\u5165\uff0c\u4f8b\u5982\u6587\u672c\u6307\u4ee4\u3001\u53c2\u8003\u56fe\u50cf\u548c\u89c6\u9891\u4e0a\u4e0b\u6587\uff0c\u751f\u6210\u7edf\u4e00\u7684\u591a\u6a21\u6001\u8868\u793a\u3002</li>\n    <li>\u8be5\u6846\u67b6\u901a\u8fc7\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u57fa\u7840\u8bbe\u65bd\u4f18\u5316\u6765\u589e\u5f3a\u5176\u80fd\u529b\u3002</li>\n    <li>Kling-Omni\u5728\u751f\u6210\u3001\u57fa\u4e8e\u63a8\u7406\u7684\u7f16\u8f91\u548c\u591a\u6a21\u6001\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u63a8\u52a8\u591a\u6a21\u6001\u4e16\u754c\u6a21\u62df\u5668\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Kling-Omni is a new system that creates high-quality videos from various types of inputs like text, images, and videos.</li>\n    <li>It combines different tasks like video generation, editing, and reasoning into one cohesive system instead of using separate processes.</li>\n    <li>The framework processes inputs into a single representation to produce cinematic-quality videos.</li>\n    <li>Kling-Omni uses a strong data system and advanced pre-training methods to enhance its performance.</li>\n    <li>The system shows great skills in generating content, editing based on reasoning, and following complex instructions.</li>\n</ul>"}, "publishedAt": "2025-12-18T12:08:12.000Z", "title": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.16776.png", "numComments": 1, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 187}, "organization": {"_id": "662c559b322afcbae51b3c8b", "name": "KlingTeam", "fullname": "Kling Team", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/60e272ca6c78a8c122b12127/ZQV1aKLUDPf2rUcxxAqj6.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 97, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 86, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7b56\u7565\u7528\u4e8e\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5168\u7403\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u6a21\u5757\u4e3b\u8981\u662f\u88ab\u52a8\u5b58\u50a8\uff0c\u7f3a\u4e4f\u5bf9\u4fe1\u606f\u9ad8\u9636\u5173\u8054\u7684\u5229\u7528\uff0c\u5bfc\u81f4\u63a8\u7406\u80fd\u529b\u6709\u9650\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51faHGMem\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u4fc3\u8fdb\u590d\u6742\u63a8\u7406\u548c\u5168\u7403\u7406\u89e3\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u56fe\u8868\u793a\u8bb0\u5fc6\uff0c\u5f62\u6210\u66f4\u9ad8\u9636\u7684\u5185\u5728\u8054\u7cfb\uff0c\u589e\u5f3a\u63a8\u7406\u7684\u8fde\u8d2f\u6027\u3002</li>\n    <li>\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cHGMem\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6b65\u9aa4RAG\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u5f3a\u57fa\u7ebf\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps large language models perform better on complex tasks that require understanding and reasoning.</li>\n    <li>Current memory systems in RAG mainly store facts passively but miss important connections between these facts, which limits their effectiveness in reasoning.</li>\n    <li>HGMem is a new memory system that uses hypergraphs to create a dynamic structure, allowing for better connections and interactions between pieces of information.</li>\n    <li>This new approach helps build a more integrated knowledge structure, improving the model's ability to reason and understand over multiple steps.</li>\n    <li>Tests show that HGMem significantly enhances the performance of multi-step RAG compared to existing methods on various challenging tasks.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u8bed\u8a00\u6a21\u578b\u7684\u7528\u6237\u671f\u671b\u6a21\u578b\u63d0\u4f9b\u51c6\u786e\u7684\u56de\u7b54\u548c\u7b26\u5408\u591a\u6837\u5316\u4eba\u7c7b\u504f\u597d\u7684\u884c\u4e3a\u3002</li>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5f00\u59cb\u91c7\u7528\u591a\u4e2a\u5956\u52b1\u6765\u5f15\u5bfc\u6a21\u578b\u671d\u7740\u671f\u671b\u884c\u4e3a\u53d1\u5c55\u3002</li>\n    <li>\u76f4\u63a5\u5e94\u7528\u73b0\u6709\u7684Group Relative Policy Optimization\uff08GRPO\uff09\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u5956\u52b1\u5f52\u4e00\u5316\u5931\u6548\uff0c\u964d\u4f4e\u8bad\u7ec3\u4fe1\u53f7\u7684\u5206\u8fa8\u7387\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3aGroup reward-Decoupled Normalization Policy Optimization\uff08GDPO\uff09\uff0c\u53ef\u4ee5\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002</li>\n    <li>GDPO\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eGRPO\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u548c\u666e\u9002\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Users want language models to give accurate answers and behave according to different human preferences.</li>\n  <li>Reinforcement learning (RL) is now using multiple rewards to help models learn these diverse behaviors.</li>\n  <li>Applying Group Relative Policy Optimization (GRPO) to these rewards can cause problems, making them less effective during training.</li>\n  <li>The paper introduces a new method called Group reward-Decoupled Normalization Policy Optimization (GDPO) that keeps the rewards distinct, improving training stability and performance.</li>\n  <li>GDPO outperforms GRPO in tasks like tool calling, math reasoning, and coding reasoning, showing better accuracy and adherence to constraints.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.03017", "authors": [{"_id": "696488cc138cc47cbd765365", "name": "Jing Xiong", "hidden": false}, {"_id": "696488cc138cc47cbd765366", "name": "Qi Han", "hidden": false}, {"_id": "696488cc138cc47cbd765367", "name": "Yunta Hsieh", "hidden": false}, {"_id": "696488cc138cc47cbd765368", "name": "Hui Shen", "hidden": false}, {"_id": "696488cc138cc47cbd765369", "name": "Huajian Xin", "hidden": false}, {"_id": "696488cc138cc47cbd76536a", "name": "Chaofan Tao", "hidden": false}, {"_id": "696488cc138cc47cbd76536b", "name": "Chenyang Zhao", "hidden": false}, {"_id": "696488cc138cc47cbd76536c", "name": "Hengyuan Zhang", "hidden": false}, {"_id": "696488cc138cc47cbd76536d", "name": "Taiqiang Wu", "hidden": false}, {"_id": "696488cc138cc47cbd76536e", "name": "Zhen Zhang", "hidden": false}, {"_id": "696488cc138cc47cbd76536f", "name": "Haochen Wang", "hidden": false}, {"_id": "696488cc138cc47cbd765370", "name": "Zhongwei Wan", "hidden": false}, {"_id": "696488cc138cc47cbd765371", "name": "Lingpeng Kong", "hidden": false}, {"_id": "696488cc138cc47cbd765372", "name": "Ngai Wong", "hidden": false}], "publishedAt": "2026-01-06T13:42:51.000Z", "submittedOnDailyAt": "2026-01-12T03:10:40.203Z", "title": "MMFormalizer: Multimodal Autoformalization in the Wild", "submittedOnDailyBy": {"_id": "60851545a5da133ac6c38686", "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg", "isPro": false, "fullname": "Jing Xiong", "user": "menik1126", "type": "user"}, "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io", "upvotes": 94, "discussionId": "696488cc138cc47cbd765373", "projectPage": "https://mmformalizer.github.io/", "ai_summary": "MMFormalizer enables multimodal autoformalization by integrating visual perception with formal mathematical reasoning, supporting complex physical domains from classical mechanics to quantum mechanics.", "ai_keywords": ["autoformalization", "multimodal", "perceptually grounded primitives", "recursive grounding", "axiom composition", "adaptive recursive termination", "dimensional grounding", "axiomatic grounding", "PhyX-AF", "MathVerse", "PhyX", "Synthetic Geometry", "Analytic Geometry", "GPT-5", "Gemini-3-Pro", "classical mechanics", "relativity", "quantum mechanics", "thermodynamics"], "summary_zh": "<ul>\n    <li>Autoformalization \u662f\u5c06\u81ea\u7136\u8bed\u8a00\u6570\u5b66\u8f6c\u5316\u4e3a\u6b63\u5f0f\u8bed\u53e5\u7684\u8fc7\u7a0b\uff0c\u9762\u4e34\u591a\u6a21\u6001\u7269\u7406\u4e16\u754c\u7684\u6311\u6218\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86 MMFormalizer\uff0c\u5b83\u901a\u8fc7\u9002\u5e94\u6027\u57fa\u7840\uff0c\u5c06\u6b63\u5f0f\u5316\u6269\u5c55\u5230\u771f\u5b9e\u4e16\u754c\u7684\u6570\u5b66\u548c\u7269\u7406\u9886\u57df\u3002</li>\n    <li>MMFormalizer \u901a\u8fc7\u9012\u5f52\u6784\u9020\u5f62\u5f0f\u547d\u9898\uff0c\u786e\u4fdd\u6bcf\u4e2a\u62bd\u8c61\u90fd\u6709\u89c6\u89c9\u8bc1\u636e\u652f\u6301\u3002</li>\n    <li>\u6211\u4eec\u5728\u65b0\u7684\u57fa\u51c6 PhyX-AF \u4e0a\u8bc4\u4f30 MMFormalizer\uff0c\u6db5\u76d6\u591a\u79cd\u591a\u6a21\u6001\u6b63\u5f0f\u5316\u4efb\u52a1\u3002</li>\n    <li>MMFormalizer \u662f\u7b2c\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u7ecf\u5178\u529b\u5b66\u3001\u76f8\u5bf9\u8bba\u3001\u91cf\u5b50\u529b\u5b66\u548c\u70ed\u529b\u5b66\u7684\u591a\u6a21\u6001\u6b63\u5f0f\u5316\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Autoformalization helps convert natural language math into formal statements for machine reasoning, but faces challenges due to the complex nature of the physical world.</li>\n    <li>MMFormalizer is a new method that improves autoformalization by integrating real-world visual elements with mathematical and physical concepts.</li>\n    <li>It builds formal propositions using grounded visual information and ensures that each abstraction is supported by evidence.</li>\n    <li>The method was tested on a new benchmark called PhyX-AF, which includes various math and physics tasks, showing that advanced models like GPT-5 perform well, especially in physical reasoning.</li>\n    <li>MMFormalizer is the first method that effectively addresses both classical mechanics and other complex topics like relativity and quantum mechanics.</li>\n</ul>"}, "publishedAt": "2026-01-06T08:42:51.000Z", "title": "MMFormalizer: Multimodal Autoformalization in the Wild", "summary": "Autoformalization, which translates natural language mathematics into formal statements to enable machine reasoning, faces fundamental challenges in the wild due to the multimodal nature of the physical world, where physics requires inferring hidden constraints (e.g., mass or energy) from visual elements. To address this, we propose MMFormalizer, which extends autoformalization beyond text by integrating adaptive grounding with entities from real-world mathematical and physical domains. MMFormalizer recursively constructs formal propositions from perceptually grounded primitives through recursive grounding and axiom composition, with adaptive recursive termination ensuring that every abstraction is supported by visual evidence and anchored in dimensional or axiomatic grounding. We evaluate MMFormalizer on a new benchmark, PhyX-AF, comprising 115 curated samples from MathVerse, PhyX, Synthetic Geometry, and Analytic Geometry, covering diverse multimodal autoformalization tasks. Results show that frontier models such as GPT-5 and Gemini-3-Pro achieve the highest compile and semantic accuracy, with GPT-5 excelling in physical reasoning, while geometry remains the most challenging domain. Overall, MMFormalizer provides a scalable framework for unified multimodal autoformalization, bridging perception and formal reasoning. To the best of our knowledge, this is the first multimodal autoformalization method capable of handling classical mechanics (derived from the Hamiltonian), as well as relativity, quantum mechanics, and thermodynamics. More details are available on our project page: MMFormalizer.github.io", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.03017.png", "numComments": 1, "submittedBy": {"_id": "60851545a5da133ac6c38686", "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg", "fullname": "Jing Xiong", "name": "menik1126", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.00393", "authors": [{"_id": "695b2297832867f253525d68", "user": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "name": "Yuxue Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:23.295Z", "hidden": false}, {"_id": "695b2297832867f253525d69", "user": {"_id": "649ecf9827145c4463240177", "avatarUrl": "/avatars/27696cf31790a3d58d8be2e0c983800e.svg", "isPro": false, "fullname": "Lue Fan", "user": "Abyssaledge", "type": "user"}, "name": "Lue Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T13:49:26.330Z", "hidden": false}, {"_id": "695b2297832867f253525d6a", "user": {"_id": "644cc2c36dfd5f8240d76a52", "avatarUrl": "/avatars/dcd9279af1c6d8535e48dc6e3e6511cd.svg", "isPro": false, "fullname": "Ziqi Shi", "user": "renshengjihe", "type": "user"}, "name": "Ziqi Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:21.077Z", "hidden": false}, {"_id": "695b2297832867f253525d6b", "name": "Junran Peng", "hidden": false}, {"_id": "695b2297832867f253525d6c", "name": "Feng Wang", "hidden": false}, {"_id": "695b2297832867f253525d6d", "name": "Zhaoxiang Zhang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "publishedAt": "2026-01-01T17:07:30.000Z", "submittedOnDailyAt": "2026-01-05T02:49:46.994Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "submittedOnDailyBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "isPro": false, "fullname": "Yuxue Yang", "user": "Yuppie1204", "type": "user"}, "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "upvotes": 83, "discussionId": "695b2297832867f253525d6e", "projectPage": "https://neoverse-4d.github.io/", "githubRepo": "https://github.com/IamCreateAI/NeoVerse", "githubRepoAddedBy": "user", "ai_summary": "NeoVerse is a scalable 4D world model that enables pose-free reconstruction and novel-trajectory video generation from monocular videos with state-of-the-art performance.", "ai_keywords": ["4D world model", "4D reconstruction", "novel-trajectory video generation", "monocular videos", "pose-free", "feed-forward", "degradation pattern simulation"], "githubStars": 107, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86NeoVerse\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u529f\u80fd\u76844D\u4e16\u754c\u6a21\u578b\u3002</li>\n    <li>NeoVerse\u53ef\u4ee5\u8fdb\u884c4D\u91cd\u5efa\u3001\u751f\u6210\u65b0\u7684\u89c6\u9891\u8f68\u8ff9\u548c\u652f\u6301\u591a\u79cd\u5e94\u7528\u3002</li>\n    <li>\u73b0\u6709\u76844D\u5efa\u6a21\u65b9\u6cd5\u6709\u6269\u5c55\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u800cNeoVerse\u9488\u5bf9\u8fd9\u4e00\u95ee\u9898\u8fdb\u884c\u4e86\u4f18\u5316\u3002</li>\n    <li>NeoVerse\u652f\u6301\u65e0\u59ff\u6001\u7684\u524d\u99884D\u91cd\u5efa\u548c\u5728\u7ebf\u5355\u76ee\u964d\u7ea7\u6a21\u5f0f\u6a21\u62df\u3002</li>\n    <li>\u5728\u6807\u51c6\u91cd\u5efa\u548c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNeoVerse\u8868\u73b0\u51fa\u8272\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>NeoVerse is a new model for creating 4D worlds that can rebuild scenes, generate videos from new angles, and support various applications.</li>\n    <li>Current methods struggle with scalability due to the need for expensive data or complex training processes.</li>\n    <li>NeoVerse uses a simpler approach that works well with everyday single-camera videos.</li>\n    <li>It includes features like easy 4D scene reconstruction and simulation of how videos can degrade over time.</li>\n    <li>NeoVerse performs better than existing models in standard tests for reconstruction and video generation.</li>\n</ul>"}, "publishedAt": "2026-01-01T12:07:30.000Z", "title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos", "summary": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/66dd71d0140f04eb180d7c2a/zVr0WeywcHVevhzmqwIaj.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.00393.png", "numComments": 1, "submittedBy": {"_id": "66dd71d0140f04eb180d7c2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66dd71d0140f04eb180d7c2a/7Qm0Ap0gCjKdumXrbgq_p.png", "fullname": "Yuxue Yang", "name": "Yuppie1204", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7}, "isAuthorParticipating": true}, {"paper": {"id": "2512.24615", "authors": [{"_id": "69564d96832867f2535257af", "user": {"_id": "622b00a776c20fee5d14501b", "avatarUrl": "/avatars/e00496dda1e309548e7b5b437839bb65.svg", "isPro": false, "fullname": "Eason shi", "user": "Easonshi", "type": "user"}, "name": "Yuchen Shi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:50.111Z", "hidden": false}, {"_id": "69564d96832867f2535257b0", "user": {"_id": "66e258bdc70c02b46dfed6e3", "avatarUrl": "/avatars/ccc2d604616c018f45a268a610472cac.svg", "isPro": false, "fullname": "Yuzheng Cai", "user": "Ucreate", "type": "user"}, "name": "Yuzheng Cai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:50.884Z", "hidden": false}, {"_id": "69564d96832867f2535257b1", "name": "Siqi Cai", "hidden": false}, {"_id": "69564d96832867f2535257b2", "name": "Zihan Xu", "hidden": false}, {"_id": "69564d96832867f2535257b3", "user": {"_id": "64154bfa385a75d7790f80e8", "avatarUrl": "/avatars/9e22f54b5eb7c4ebedad99a9a92c4b6a.svg", "isPro": false, "fullname": "Lichao Chen", "user": "nth233", "type": "user"}, "name": "Lichao Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:46.825Z", "hidden": false}, {"_id": "69564d96832867f2535257b4", "user": {"_id": "6390525c00fb8ec4a424e0ff", "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg", "isPro": false, "fullname": "Yulei Qin", "user": "yolay", "type": "user"}, "name": "Yulei Qin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:48.064Z", "hidden": false}, {"_id": "69564d96832867f2535257b5", "name": "Zhijian Zhou", "hidden": false}, {"_id": "69564d96832867f2535257b6", "name": "Xiang Fei", "hidden": false}, {"_id": "69564d96832867f2535257b7", "user": {"_id": "6604e43869c47cd78fdebd08", "avatarUrl": "/avatars/4c11f5e1aeae3c5eb213f6ec6d5bfe72.svg", "isPro": false, "fullname": "Qiu", "user": "ChaofanDFG", "type": "user"}, "name": "Chaofan Qiu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:48.910Z", "hidden": false}, {"_id": "69564d96832867f2535257b8", "user": {"_id": "637af0a7bdf7309aa6da1c36", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637af0a7bdf7309aa6da1c36/NHZ-09otVCfbpXVxm8f-e.png", "isPro": false, "fullname": "Xiaoyu Tan", "user": "WIlliam1900", "type": "user"}, "name": "Xiaoyu Tan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-05T08:27:52.763Z", "hidden": false}, {"_id": "69564d96832867f2535257b9", "name": "Gang Li", "hidden": false}, {"_id": "69564d96832867f2535257ba", "name": "Zongyi Li", "hidden": false}, {"_id": "69564d96832867f2535257bb", "name": "Haojia Lin", "hidden": false}, {"_id": "69564d96832867f2535257bc", "name": "Guocan Cai", "hidden": false}, {"_id": "69564d96832867f2535257bd", "name": "Yong Mao", "hidden": false}, {"_id": "69564d96832867f2535257be", "name": "Yunsheng Wu", "hidden": false}, {"_id": "69564d96832867f2535257bf", "name": "Ke Li", "hidden": false}, {"_id": "69564d96832867f2535257c0", "user": {"_id": "647401e50da364bd0d002f2a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/vPuPn7EV092mLBOM2YZXd.png", "isPro": false, "fullname": "XING SUN", "user": "tedsun", "type": "user"}, "name": "Xing Sun", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:38:39.390Z", "hidden": false}], "publishedAt": "2025-12-31T04:17:36.000Z", "submittedOnDailyAt": "2026-01-05T00:21:56.456Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "submittedOnDailyBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "isPro": false, "fullname": "Ke Li", "user": "tristanli", "type": "user"}, "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "upvotes": 82, "discussionId": "69564d96832867f2535257c1", "projectPage": "https://tencentcloudadp.github.io/youtu-agent/", "githubRepo": "https://github.com/TencentCloudADP/youtu-agent", "githubRepoAddedBy": "user", "githubStars": 4095, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\u6846\u67b6\u9762\u4e34\u9ad8\u914d\u7f6e\u6210\u672c\u548c\u9759\u6001\u80fd\u529b\u7684\u6311\u6218\u3002</li>\n    <li>Youtu-Agent\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u65e8\u5728\u81ea\u52a8\u751f\u6210\u548c\u6301\u7eed\u8fdb\u5316LLM\u4ee3\u7406\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5177\u6709\u7ed3\u6784\u5316\u914d\u7f6e\u7cfb\u7edf\uff0c\u652f\u6301\u7075\u6d3b\u91cd\u7528\u548c\u81ea\u52a8\u5408\u6210\u5de5\u5177\u548c\u73af\u5883\u3002</li>\n    <li>Youtu-Agent\u63a8\u51fa\u4e86\u5de5\u4f5c\u6d41\u6a21\u5f0f\u548c\u5143\u4ee3\u7406\u6a21\u5f0f\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u5de5\u5177\u4ee3\u7801\u548c\u914d\u7f6e\u3002</li>\n    <li>\u6d4b\u8bd5\u8868\u660e\uff0cYoutu-Agent\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u9ad8\u4e86\u6027\u80fd\u5e76\u52a0\u5feb\u4e86\u8bad\u7ec3\u901f\u5ea6\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current LLM frameworks are expensive to set up and can't easily adapt to new situations.</li>\n    <li>Youtu-Agent is a new, flexible system that helps create and improve LLM agents automatically.</li>\n    <li>It has two modes: one for regular tasks and another for complex needs, generating necessary tools and configurations on its own.</li>\n    <li>Youtu-Agent includes a system for agents to learn from experience and improve their performance without needing major updates.</li>\n    <li>Tests show Youtu-Agent performs better than other systems and speeds up training while enhancing abilities in various tasks.</li>\n</ul>"}, "publishedAt": "2025-12-30T23:17:36.000Z", "title": "Youtu-Agent: Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization", "summary": "Existing Large Language Model (LLM) agent frameworks face two significant challenges: high configuration costs and static capabilities. Building a high-quality agent often requires extensive manual effort in tool integration and prompt engineering, while deployed agents struggle to adapt to dynamic environments without expensive fine-tuning. To address these issues, we propose Youtu-Agent, a modular framework designed for the automated generation and continuous evolution of LLM agents. Youtu-Agent features a structured configuration system that decouples execution environments, toolkits, and context management, enabling flexible reuse and automated synthesis. We introduce two generation paradigms: a Workflow mode for standard tasks and a Meta-Agent mode for complex, non-standard requirements, capable of automatically generating tool code, prompts, and configurations. Furthermore, Youtu-Agent establishes a hybrid policy optimization system: (1) an Agent Practice module that enables agents to accumulate experience and improve performance through in-context optimization without parameter updates; and (2) an Agent RL module that integrates with distributed training frameworks to enable scalable and stable reinforcement learning of any Youtu-Agents in an end-to-end, large-scale manner. Experiments demonstrate that Youtu-Agent achieves state-of-the-art performance on WebWalkerQA (71.47\\%) and GAIA (72.8\\%) using open-weight models. Our automated generation pipeline achieves over 81\\% tool synthesis success rate, while the Practice module improves performance on AIME 2024/2025 by +2.7\\% and +5.4\\% respectively. Moreover, our Agent RL training achieves 40\\% speedup with steady performance improvement on 7B LLMs, enhancing coding/reasoning and searching capabilities respectively up to 35\\% and 21\\% on Maths and general/multi-hop QA benchmarks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.24615.png", "numComments": 1, "submittedBy": {"_id": "63280915eeee4dd858083092", "avatarUrl": "/avatars/78347af4af42527d53e88d9969c5c934.svg", "fullname": "Ke Li", "name": "tristanli", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Jan 14, 2026";
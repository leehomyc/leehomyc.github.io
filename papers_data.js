window.trendingPapers = {
    "today": [{"paper": {"id": "2601.20833", "authors": [{"_id": "697b9192a67238fac88cbee8", "name": "Tengyue Xu", "hidden": false}, {"_id": "697b9192a67238fac88cbee9", "name": "Zhuoyang Qian", "hidden": false}, {"_id": "697b9192a67238fac88cbeea", "name": "Gaoge Liu", "hidden": false}, {"_id": "697b9192a67238fac88cbeeb", "name": "Li Ling", "hidden": false}, {"_id": "697b9192a67238fac88cbeec", "name": "Zhentao Zhang", "hidden": false}, {"_id": "697b9192a67238fac88cbeed", "name": "Biao Wu", "hidden": false}, {"_id": "697b9192a67238fac88cbeee", "name": "Shuo Zhang", "hidden": false}, {"_id": "697b9192a67238fac88cbeef", "name": "Ke Lu", "hidden": false}, {"_id": "697b9192a67238fac88cbef0", "name": "Wei Shi", "hidden": false}, {"_id": "697b9192a67238fac88cbef1", "name": "Ziqi Wang", "hidden": false}, {"_id": "697b9192a67238fac88cbef2", "name": "Zheng Feng", "hidden": false}, {"_id": "697b9192a67238fac88cbef3", "name": "Yan Luo", "hidden": false}, {"_id": "697b9192a67238fac88cbef4", "name": "Shu Xu", "hidden": false}, {"_id": "697b9192a67238fac88cbef5", "name": "Yongjin Chen", "hidden": false}, {"_id": "697b9192a67238fac88cbef6", "name": "Zhibo Feng", "hidden": false}, {"_id": "697b9192a67238fac88cbef7", "name": "Zhuo Chen", "hidden": false}, {"_id": "697b9192a67238fac88cbef8", "name": "Bruce Yuan", "hidden": false}, {"_id": "697b9192a67238fac88cbef9", "name": "Harry Wang", "hidden": false}, {"_id": "697b9192a67238fac88cbefa", "name": "Kris Chen", "hidden": false}], "publishedAt": "2026-01-28T18:31:54.000Z", "submittedOnDailyAt": "2026-01-30T03:32:00.106Z", "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives", "submittedOnDailyBy": {"_id": "62baa0d6dd02fbf607ce97be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62baa0d6dd02fbf607ce97be/V0I6pANlLEf2YDd9ZLZgi.jpeg", "isPro": false, "fullname": "Wendy", "user": "Wendy-Fly", "type": "user"}, "summary": "Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.", "upvotes": 113, "discussionId": "697b9192a67238fac88cbefb", "githubRepo": "https://github.com/AgentAlphaAGI/Idea2Paper", "githubRepoAddedBy": "user", "ai_summary": "Offline knowledge construction through structured methodological graphs enables more reliable and scalable autonomous scientific discovery by reducing reliance on real-time literature processing.", "ai_keywords": ["large language model", "autonomous scientific discovery", "runtime-centric execution", "context window limitations", "hallucination", "pre-computation-driven framework", "peer-reviewed papers", "research patterns", "methodological knowledge graph", "end-to-end research workflows"], "githubStars": 54, "organization": {"_id": "69542731e1200d74c1c053d1", "name": "AgentAlphaAGI", "fullname": "AgentAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b78eb76ab5d14ca7faac87/TbMZ3y00APtRzHEfTSR7I.jpeg"}, "summary_zh": "<ul>\n    <li>\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u7684\u7814\u7a76\u8fdb\u5c55\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\uff0c\u80fd\u81ea\u52a8\u5316\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u5728\u7ebf\u8ba1\u7b97\uff0c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u8106\u5f31\u6027\u3002</li>\n    <li>\u63d0\u51faIdea2Story\u6846\u67b6\uff0c\u8f6c\u5411\u79bb\u7ebf\u77e5\u8bc6\u6784\u5efa\uff0c\u63d0\u9ad8\u6587\u732e\u7406\u89e3\u6548\u7387\u3002</li>\n    <li>Idea2Story\u6536\u96c6\u540c\u884c\u8bc4\u5ba1\u8bba\u6587\u53ca\u53cd\u9988\uff0c\u63d0\u53d6\u6838\u5fc3\u65b9\u6cd5\u5355\u5143\u5e76\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u3002</li>\n    <li>\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u8fde\u8d2f\u7684\u65b0\u7814\u7a76\u6a21\u5f0f\uff0c\u964d\u4f4e\u91cd\u590d\u63a8\u7406\u7684\u9700\u6c42\uff0c\u652f\u6301\u53ef\u9760\u7684\u79d1\u5b66\u53d1\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent advancements in using large language models (LLMs) have automated many research processes but have high computational costs and issues with reasoning.</li>\n    <li>The proposed system, Idea2Story, focuses on building an offline knowledge base from scientific papers instead of reasoning online.</li>\n    <li>Idea2Story gathers research papers and feedback, extracts important methods, and organizes them into a structured knowledge graph.</li>\n    <li>This system allows users to quickly find and use established research patterns, reducing the need for trial-and-error approaches.</li>\n    <li>Preliminary studies show that Idea2Story can create coherent and innovative research ideas, making it a promising tool for scientific discovery.</li>\n</ul>"}, "publishedAt": "2026-01-28T13:31:54.000Z", "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives", "summary": "Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20833.png", "numComments": 1, "submittedBy": {"_id": "62baa0d6dd02fbf607ce97be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62baa0d6dd02fbf607ce97be/V0I6pANlLEf2YDd9ZLZgi.jpeg", "fullname": "Wendy", "name": "Wendy-Fly", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "69542731e1200d74c1c053d1", "name": "AgentAlphaAGI", "fullname": "AgentAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b78eb76ab5d14ca7faac87/TbMZ3y00APtRzHEfTSR7I.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20354", "authors": [{"_id": "697c1857a67238fac88cc06e", "user": {"_id": "656c9cfef7be0986b49934ea", "avatarUrl": "/avatars/2030e77c28fb4c518b692cd9a20de665.svg", "isPro": false, "fullname": "MuMing", "user": "ZengbinWang", "type": "user"}, "name": "Zengbin Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T09:36:35.067Z", "hidden": false}, {"_id": "697c1857a67238fac88cc06f", "name": "Xuecai Hu", "hidden": false}, {"_id": "697c1857a67238fac88cc070", "name": "Yong Wang", "hidden": false}, {"_id": "697c1857a67238fac88cc071", "name": "Feng Xiong", "hidden": false}, {"_id": "697c1857a67238fac88cc072", "name": "Man Zhang", "hidden": false}, {"_id": "697c1857a67238fac88cc073", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-28T08:15:00.000Z", "submittedOnDailyAt": "2026-01-30T05:01:51.614Z", "title": "Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.", "upvotes": 96, "discussionId": "697c1857a67238fac88cc074", "githubRepo": "https://github.com/AMAP-ML/SpatialGenEval", "githubRepoAddedBy": "user", "ai_summary": "A new benchmark and dataset are introduced to evaluate and improve spatial reasoning capabilities in text-to-image models through information-dense prompts and fine-tuning.", "ai_keywords": ["text-to-image models", "spatial intelligence", "benchmark", "long prompts", "information-dense prompts", "spatial reasoning", "Stable Diffusion-XL", "Uniworld-V1", "OmniGen2", "fine-tuning"], "githubStars": 93, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u65f6\u8868\u73b0\u4e0d\u4f73\u3002</li>\n    <li>\u63d0\u51fa\u4e86SpatialGenEval\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u7a7a\u95f4\u667a\u80fd\uff0c\u5305\u542b1,230\u4e2a\u957f\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u63d0\u793a\u3002</li>\n    <li>\u8fd9\u4e9b\u63d0\u793a\u8986\u76d625\u4e2a\u771f\u5b9e\u573a\u666f\uff0c\u6d89\u53ca\u7a7a\u95f4\u4f4d\u7f6e\u3001\u5e03\u5c40\u3001\u906e\u6321\u548c\u56e0\u679c\u5173\u7cfb\u7b49\u65b9\u9762\u3002</li>\n    <li>\u8bc4\u4f30\u663e\u793a\uff0c\u73b0\u6709\u6a21\u578b\u5728\u9ad8\u9636\u7a7a\u95f4\u63a8\u7406\u4e0a\u4ecd\u5b58\u5728\u74f6\u9888\u3002</li>\n    <li>\u6784\u5efa\u4e86SpatialT2I\u6570\u636e\u96c6\uff0c\u5305\u542b15,400\u5bf9\u6587\u672c-\u56fe\u50cf\uff0c\u7ecf\u8fc7\u6539\u5199\u4ee5\u4fdd\u6301\u56fe\u50cf\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\u548c\u7a7a\u95f4\u5173\u7cfb\u7684\u771f\u5b9e\u611f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Text-to-image (T2I) models create impressive images but struggle with understanding complex spatial relationships.</li>\n    <li>The new benchmark, SpatialGenEval, evaluates T2I models on their spatial intelligence using 1,230 detailed prompts across 25 scenes.</li>\n    <li>Each prompt includes questions about object positions, layouts, and relationships, highlighting weaknesses in spatial reasoning among top models.</li>\n    <li>SpatialT2I dataset was created with 15,400 text-image pairs to improve image consistency while maintaining rich information.</li>\n    <li>Fine-tuning on this dataset improved performance of leading models, showing a better understanding of spatial relationships.</li>\n</ul>"}, "publishedAt": "2026-01-28T03:15:00.000Z", "title": "Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models", "summary": "Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20354.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.21204", "authors": [{"_id": "697c3801a67238fac88cc1b1", "name": "Hong Liu", "hidden": false}, {"_id": "697c3801a67238fac88cc1b2", "name": "Jiaqi Zhang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b3", "name": "Chao Wang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b4", "name": "Xing Hu", "hidden": false}, {"_id": "697c3801a67238fac88cc1b5", "name": "Linkun Lyu", "hidden": false}, {"_id": "697c3801a67238fac88cc1b6", "name": "Jiaqi Sun", "hidden": false}, {"_id": "697c3801a67238fac88cc1b7", "name": "Xurui Yang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b8", "name": "Bo Wang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b9", "name": "Fengcun Li", "hidden": false}, {"_id": "697c3801a67238fac88cc1ba", "name": "Yulei Qian", "hidden": false}, {"_id": "697c3801a67238fac88cc1bb", "name": "Lingtong Si", "hidden": false}, {"_id": "697c3801a67238fac88cc1bc", "name": "Yerui Sun", "hidden": false}, {"_id": "697c3801a67238fac88cc1bd", "name": "Rumei Li", "hidden": false}, {"_id": "697c3801a67238fac88cc1be", "name": "Peng Pei", "hidden": false}, {"_id": "697c3801a67238fac88cc1bf", "name": "Yuchen Xie", "hidden": false}, {"_id": "697c3801a67238fac88cc1c0", "name": "Xunliang Cai", "hidden": false}], "publishedAt": "2026-01-29T03:11:19.000Z", "submittedOnDailyAt": "2026-01-30T02:18:11.112Z", "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.", "upvotes": 76, "discussionId": "697c3801a67238fac88cc1c1", "ai_summary": "Embedding scaling offers superior sparsity scaling compared to expert scaling in large language models, enabling efficient inference through system optimizations and speculative decoding.", "ai_keywords": ["Mixture-of-Experts", "sparsity scaling", "embedding scaling", "Pareto frontier", "parameter budgeting", "model width", "model depth", "system optimizations", "speculative decoding", "LongCat-Flash-Lite"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u53d8\u5f97\u666e\u904d\uff0c\u4f46\u9762\u4e34\u6536\u76ca\u9012\u51cf\u548c\u7cfb\u7edf\u74f6\u9888\u95ee\u9898\u3002</li>\n    <li>\u672c\u6587\u63a2\u8ba8\u901a\u8fc7\u5d4c\u5165\u6269\u5c55\u4f5c\u4e3a\u89c4\u6a21\u7a00\u758f\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u5176\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4e13\u5bb6\u6269\u5c55\u3002</li>\n    <li>\u7cfb\u7edf\u5206\u6790\u4e86\u5f71\u54cd\u5d4c\u5165\u6269\u5c55\u6548\u679c\u7684\u5173\u952e\u67b6\u6784\u56e0\u7d20\uff0c\u5305\u62ec\u53c2\u6570\u5206\u914d\u548c\u6a21\u578b\u7684\u5bbd\u5ea6\u4e0e\u6df1\u5ea6\u3002</li>\n    <li>\u7ed3\u5408\u7cfb\u7edf\u4f18\u5316\u548c\u6295\u673a\u89e3\u7801\uff0c\u6210\u529f\u5c06\u7a00\u758f\u6027\u8f6c\u5316\u4e3a\u663e\u8457\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u3002</li>\n    <li>\u63a8\u51fa\u7684LongCat-Flash-Lite\u6a21\u578b\u62e5\u6709685\u4ebf\u53c2\u6570\uff0c\u6fc0\u6d3b\u7ea630\u4ebf\u53c2\u6570\uff0c\u6027\u80fd\u4f18\u4e8e\u540c\u7c7bMoE\u57fa\u7ebf\uff0c\u5e76\u5728\u7279\u5b9a\u9886\u57df\u8868\u73b0\u5353\u8d8a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Mixture-of-Experts (MoE) models are popular for making large language models more efficient, but they are hitting limits in performance.</li>\n    <li>This study looks at scaling embeddings as an alternative way to improve model efficiency.</li>\n    <li>Embedding scaling was found to perform better in certain conditions compared to expert scaling.</li>\n    <li>The researchers identified key architectural factors that affect how well embedding scaling works.</li>\n    <li>They developed a new model called LongCat-Flash-Lite, which has 68.5 billion parameters and performs better than similar models, especially in tasks involving agents and coding.</li>\n</ul>"}, "publishedAt": "2026-01-28T22:11:19.000Z", "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "summary": "While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21204.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 221, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.22153", "authors": [{"_id": "697c2899a67238fac88cc115", "user": {"_id": "63f47b5321eb234ab739e91a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg", "isPro": false, "fullname": "Haozhe Xie", "user": "hzxie", "type": "user"}, "name": "Haozhe Xie", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T13:31:48.996Z", "hidden": false}, {"_id": "697c2899a67238fac88cc116", "user": {"_id": "672392c4a4c4381cefc06416", "avatarUrl": "/avatars/8ee84a7e3e91e5d13074bc3c407ff75d.svg", "isPro": false, "fullname": "Wen Beichen", "user": "wenbc21", "type": "user"}, "name": "Beichen Wen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T13:31:52.487Z", "hidden": false}, {"_id": "697c2899a67238fac88cc117", "user": {"_id": "6899ff3f4c5ca50a326bb456", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Nuqof2ofdaQUD5b07cDnG.png", "isPro": false, "fullname": "Zheng Jiarui", "user": "zghtyarecrenj", "type": "user"}, "name": "Jiarui Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T13:31:46.030Z", "hidden": false}, {"_id": "697c2899a67238fac88cc118", "name": "Zhaoxi Chen", "hidden": false}, {"_id": "697c2899a67238fac88cc119", "name": "Fangzhou Hong", "hidden": false}, {"_id": "697c2899a67238fac88cc11a", "name": "Haiwen Diao", "hidden": false}, {"_id": "697c2899a67238fac88cc11b", "name": "Ziwei Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/p9cPxETttQUS23woFb14M.mp4"], "publishedAt": "2026-01-29T18:59:51.000Z", "submittedOnDailyAt": "2026-01-30T01:46:39.673Z", "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation", "submittedOnDailyBy": {"_id": "63f47b5321eb234ab739e91a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg", "isPro": false, "fullname": "Haozhe Xie", "user": "hzxie", "type": "user"}, "summary": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.", "upvotes": 45, "discussionId": "697c2899a67238fac88cc11c", "projectPage": "https://haozhexie.com/project/dynamic-vla", "githubRepo": "https://github.com/hzxie/DynamicVLA", "githubRepoAddedBy": "user", "ai_summary": "DynamicVLA addresses dynamic object manipulation challenges through a compact vision-language-action model with temporal reasoning and closed-loop adaptation, supported by a new benchmark for dynamic manipulation tasks.", "ai_keywords": ["Vision-Language-Action models", "temporal reasoning", "closed-loop adaptation", "convolutional vision encoder", "multimodal inference", "Continuous Inference", "Latent-aware Action Streaming", "Dynamic Object Manipulation benchmark", "synthetic episodes", "real-world episodes"], "githubStars": 48, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "summary_zh": "<ul>\n    <li>\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\u662fVision-Language-Action (VLA)\u6a21\u578b\u9762\u4e34\u7684\u6311\u6218\uff0c\u5c24\u5176\u5728\u5feb\u901f\u611f\u77e5\u548c\u63a7\u5236\u65b9\u9762\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DynamicVLA\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u65f6\u95f4\u63a8\u7406\u548c\u95ed\u73af\u9002\u5e94\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5177\u6709\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a\u7d27\u51d1\u76840.4B VLA\u3001\u8fde\u7eed\u63a8\u7406\u548c\u6f5c\u5728\u610f\u8bc6\u7684\u52a8\u4f5c\u6d41\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\uff08DOM\uff09\u57fa\u51c6\uff0c\u6536\u96c6\u4e86200K\u4e2a\u5408\u6210\u573a\u666f\u548c2K\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u64cd\u4f5c\u6570\u636e\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cDynamicVLA\u5728\u53cd\u5e94\u901f\u5ea6\u3001\u611f\u77e5\u548c\u6cdb\u5316\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DynamicVLA is a new framework designed to help machines manipulate moving objects better than previous models.</li>\n    <li>It uses a small but effective model that quickly analyzes visual information and makes decisions.</li>\n    <li>The framework allows for faster reasoning and actions, enabling machines to adapt quickly to changes in object movement.</li>\n    <li>A new benchmark called Dynamic Object Manipulation (DOM) has been created to test this framework, featuring a large collection of synthetic and real-world data.</li>\n    <li>Tests show that DynamicVLA significantly improves speed, perception, and the ability to generalize across different situations.</li>\n</ul>"}, "publishedAt": "2026-01-29T13:59:51.000Z", "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation", "summary": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/p9cPxETttQUS23woFb14M.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22153.png", "numComments": 2, "submittedBy": {"_id": "63f47b5321eb234ab739e91a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg", "fullname": "Haozhe Xie", "name": "hzxie", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 19, "isUserFollowing": false}, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.21639", "authors": [{"_id": "697c5270a67238fac88cc226", "user": {"_id": "693f91d7ed7d40c019934508", "avatarUrl": "/avatars/0d73f098627c9ebd2ae7d90e693a34f6.svg", "isPro": false, "fullname": "Yufeng Zhong", "user": "Albert-Zhong", "type": "user"}, "name": "Yufeng Zhong", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T09:35:23.719Z", "hidden": false}, {"_id": "697c5270a67238fac88cc227", "name": "Lei Chen", "hidden": false}, {"_id": "697c5270a67238fac88cc228", "name": "Xuanle Zhao", "hidden": false}, {"_id": "697c5270a67238fac88cc229", "name": "Wenkang Han", "hidden": false}, {"_id": "697c5270a67238fac88cc22a", "name": "Liming Zheng", "hidden": false}, {"_id": "697c5270a67238fac88cc22b", "name": "Jing Huang", "hidden": false}, {"_id": "697c5270a67238fac88cc22c", "name": "Deyang Jiang", "hidden": false}, {"_id": "697c5270a67238fac88cc22d", "name": "Yilin Cao", "hidden": false}, {"_id": "697c5270a67238fac88cc22e", "name": "Lin Ma", "hidden": false}, {"_id": "697c5270a67238fac88cc22f", "name": "Zhixiong Zeng", "hidden": false}], "publishedAt": "2026-01-29T12:43:02.000Z", "submittedOnDailyAt": "2026-01-30T04:33:06.261Z", "title": "OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models", "submittedOnDailyBy": {"_id": "6572cbc42bb242937c0a1101", "avatarUrl": "/avatars/f2af45e6b242aa47578fe3f60e97ca86.svg", "isPro": false, "fullname": "Xuanle Zhao", "user": "xxxllz", "type": "user"}, "summary": "The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (Text-centric OCR), neglecting the identification of visual elements from visually information-dense image sources (Vision-centric OCR), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose OCRVerse, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.", "upvotes": 41, "discussionId": "697c5270a67238fac88cc230", "githubRepo": "https://github.com/DocTron-hub/OCRVerse", "githubRepoAddedBy": "user", "ai_summary": "OCRVerse is a novel end-to-end OCR method that unifies text-centric and vision-centric approaches through comprehensive data engineering and a two-stage SFT-RL training framework with domain-specific reward strategies.", "ai_keywords": ["OCR", "vision-centric OCR", "text-centric OCR", "end-to-end OCR", "data engineering", "SFT-RL training", "cross-domain training", "reward strategies", "domain-specific customization", "cross-domain fusion"], "githubStars": 13, "summary_zh": "<ul>\n    <li>OCR\u6280\u672f\u8d8a\u6765\u8d8a\u53d7\u6b22\u8fce\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5927\u91cf\u591a\u6a21\u6001\u6570\u636e\u65f6\u3002</li>\n    <li>\u73b0\u6709\u7684OCR\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u8bc6\u522b\uff0c\u5ffd\u7565\u4e86\u4ece\u4fe1\u606f\u5bc6\u96c6\u7684\u56fe\u50cf\u4e2d\u8bc6\u522b\u89c6\u89c9\u5143\u7d20\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86OCRVerse\uff0c\u8fd9\u662f\u9996\u4e2a\u7edf\u4e00\u6587\u672c\u4e2d\u5fc3\u548c\u89c6\u89c9\u4e2d\u5fc3OCR\u7684\u65b9\u6cd5\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86\u5168\u9762\u7684\u6570\u636e\u5de5\u7a0b\uff0c\u6db5\u76d6\u4e86\u5404\u79cd\u6587\u672c\u548c\u89c6\u89c9\u6587\u6863\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cOCRVerse\u5728\u6587\u672c\u548c\u89c6\u89c9\u6570\u636e\u7c7b\u578b\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e0e\u5176\u4ed6\u5927\u578b\u6a21\u578b\u76f8\u5f53\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>OCR technology is becoming more important as large vision language models need to manage and use a lot of visual data.</li>\n    <li>Most current OCR methods focus on reading text from images but ignore other visual elements like charts and web pages.</li>\n    <li>OCRVerse is a new method that combines both text-centric and vision-centric OCR in one system.</li>\n    <li>It uses a two-stage training approach that helps the model learn from different types of data effectively.</li>\n    <li>Tests show that OCRVerse performs well across various data types, competing with existing models.</li>\n</ul>"}, "publishedAt": "2026-01-29T07:43:02.000Z", "title": "OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models", "summary": "The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (Text-centric OCR), neglecting the identification of visual elements from visually information-dense image sources (Vision-centric OCR), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose OCRVerse, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21639.png", "numComments": 2, "submittedBy": {"_id": "6572cbc42bb242937c0a1101", "avatarUrl": "/avatars/f2af45e6b242aa47578fe3f60e97ca86.svg", "fullname": "Xuanle Zhao", "name": "xxxllz", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.21821", "authors": [{"_id": "697c2a0ca67238fac88cc11e", "name": "Honglin Lin", "hidden": false}, {"_id": "697c2a0ca67238fac88cc11f", "user": {"_id": "6625ef13605f46d05c1d0031", "avatarUrl": "/avatars/22f201dca35e43013cb593884516e96c.svg", "isPro": false, "fullname": "Zheng Liu", "user": "starriver030515", "type": "user"}, "name": "Zheng Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T09:36:23.860Z", "hidden": false}, {"_id": "697c2a0ca67238fac88cc120", "name": "Yun Zhu", "hidden": false}, {"_id": "697c2a0ca67238fac88cc121", "user": {"_id": "67b30bb2c2e25cfcdeda4a2f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67b30bb2c2e25cfcdeda4a2f/K5ePD5uWNkwlpkI_43Oe1.jpeg", "isPro": false, "fullname": "Qin, Chonghan", "user": "J017athan", "type": "user"}, "name": "Chonghan Qin", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T09:36:20.559Z", "hidden": false}, {"_id": "697c2a0ca67238fac88cc122", "name": "Juekai Lin", "hidden": false}, {"_id": "697c2a0ca67238fac88cc123", "name": "Xiaoran Shang", "hidden": false}, {"_id": "697c2a0ca67238fac88cc124", "name": "Conghui He", "hidden": false}, {"_id": "697c2a0ca67238fac88cc125", "name": "Wentao Zhang", "hidden": false}, {"_id": "697c2a0ca67238fac88cc126", "name": "Lijun Wu", "hidden": false}], "publishedAt": "2026-01-29T15:07:28.000Z", "submittedOnDailyAt": "2026-01-30T01:36:56.361Z", "title": "MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods", "submittedOnDailyBy": {"_id": "640d99628512ec51d7ef71c7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d99628512ec51d7ef71c7/fcBkqnxfxuuuZTqfN_BGy.jpeg", "isPro": false, "fullname": "Honglin Lin", "user": "LHL3341", "type": "user"}, "summary": "Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a \"less is more\" phenomenon via our difficulty-aware filtering strategy: a subset of just 7\\% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.", "upvotes": 25, "discussionId": "697c2a0da67238fac88cc127", "projectPage": "https://mmfinereason.github.io/", "ai_summary": "A large-scale multimodal reasoning dataset called MMFineReason is introduced to improve vision language models' performance through high-quality reasoning annotations and demonstrates superior parameter efficiency in fine-tuned models.", "ai_keywords": ["Vision Language Models", "Chain-of-Thought", "multimodal reasoning", "Qwen3-VL", "CoT rationale generation", "reasoning quality", "difficulty awareness", "parameter efficiency", "general capabilities"], "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u5f15\u5165\u4e86MMFineReason\uff0c\u4e00\u4e2a\u5305\u542b180\u4e07\u6837\u672c\u548c51\u4ebf\u89e3\u51b3\u65b9\u6848\u6807\u8bb0\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u63a8\u7406\u6570\u636e\u96c6\u3002</li>\n    <li>\u8be5\u6570\u636e\u96c6\u5305\u542b\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\u6ce8\u91ca\uff0c\u6db5\u76d6STEM\u95ee\u9898\u3001\u89c6\u89c9\u96be\u9898\u3001\u6e38\u620f\u548c\u590d\u6742\u56fe\u8868\u3002</li>\n    <li>MMFineReason\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u7684\u7cfb\u7edf\u6d41\u7a0b\u5efa\u7acb\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u63a8\u7406\u751f\u6210\u548c\u57fa\u4e8e\u63a8\u7406\u8d28\u91cf\u7684\u9009\u62e9\u3002</li>\n    <li>\u6211\u4eec\u5bf9Qwen3-VL-Instruct\u8fdb\u884c\u5fae\u8c03\uff0c\u5f00\u53d1\u51fa\u65b0\u7684MMFineReason\u6a21\u578b\uff0c\u521b\u9020\u4e86\u540c\u7c7b\u6700\u4f73\u7684\u7ed3\u679c\u3002</li>\n    <li>\u901a\u8fc7\u96be\u5ea6\u610f\u8bc6\u8fc7\u6ee4\u7b56\u7565\uff0c\u6211\u4eec\u53d1\u73b0\u4ec5\u4f7f\u75287%\u7684\u6837\u672c\u4e5f\u80fd\u8fbe\u5230\u4e0e\u5b8c\u6574\u6570\u636e\u96c6\u76f8\u5f53\u7684\u6027\u80fd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent improvements in Vision Language Models (VLMs) have advanced visual reasoning, but open-source models still fall behind proprietary ones due to a lack of quality reasoning data.</li>\n    <li>Existing datasets are limited in covering tough areas like STEM diagrams and visual puzzles, and they lack consistent long-form reasoning annotations.</li>\n    <li>To address this, the MMFineReason dataset was created, containing 1.8 million samples and 5.1 billion solution tokens with high-quality reasoning annotations.</li>\n    <li>MMFineReason includes challenges from STEM problems, visual puzzles, and complex diagrams, all annotated with clear reasoning steps.</li>\n    <li>Fine-tuned models using this dataset achieved new best results, with a smaller subset of the data (only 7%) showing performance similar to the full dataset, highlighting the effectiveness of targeted data selection.</li>\n</ul>"}, "publishedAt": "2026-01-29T10:07:28.000Z", "title": "MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods", "summary": "Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a \"less is more\" phenomenon via our difficulty-aware filtering strategy: a subset of just 7\\% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21821.png", "numComments": 2, "submittedBy": {"_id": "640d99628512ec51d7ef71c7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d99628512ec51d7ef71c7/fcBkqnxfxuuuZTqfN_BGy.jpeg", "fullname": "Honglin Lin", "name": "LHL3341", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.21420", "authors": [{"_id": "697c36f4a67238fac88cc1a4", "user": {"_id": "65a62085576772f531e13856", "avatarUrl": "/avatars/72c67a60422e333ea4e323f7480ae0b7.svg", "isPro": false, "fullname": "Huang Zihao", "user": "FetchFortune", "type": "user"}, "name": "Zihao Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T09:35:39.380Z", "hidden": false}, {"_id": "697c36f4a67238fac88cc1a5", "name": "Jundong Zhou", "hidden": false}, {"_id": "697c36f4a67238fac88cc1a6", "name": "Xingwei Qu", "hidden": false}, {"_id": "697c36f4a67238fac88cc1a7", "name": "Qiyang Min", "hidden": false}, {"_id": "697c36f4a67238fac88cc1a8", "user": {"_id": "638efcf4c67af472d316d424", "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg", "isPro": false, "fullname": "Ge Zhang", "user": "zhangysk", "type": "user"}, "name": "Ge Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T13:31:44.281Z", "hidden": false}], "publishedAt": "2026-01-29T08:58:22.000Z", "submittedOnDailyAt": "2026-01-30T02:24:05.556Z", "title": "ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation", "submittedOnDailyBy": {"_id": "65a62085576772f531e13856", "avatarUrl": "/avatars/72c67a60422e333ea4e323f7480ae0b7.svg", "isPro": false, "fullname": "Huang Zihao", "user": "FetchFortune", "type": "user"}, "summary": "Large language models allocate uniform computation across all tokens, ignoring that some sequences are trivially predictable while others require deep reasoning. We introduce ConceptMoE, which dynamically merges semantically similar tokens into concept representations, performing implicit token-level compute allocation. A learnable chunk module identifies optimal boundaries by measuring inter-token similarity, compressing sequences by a target ratio R before they enter the compute-intensive concept model. Crucially, the MoE architecture enables controlled evaluation: we reallocate saved computation to match baseline activated FLOPs (excluding attention map computation) and total parameters, isolating genuine architectural benefits. Under these conditions, ConceptMoE consistently outperforms standard MoE across language and vision-language tasks, achieving +0.9 points on language pretraining, +2.3 points on long context understanding, and +0.6 points on multimodal benchmarks. When converting pretrained MoE during continual training with layer looping, gains reach +5.5 points, demonstrating practical applicability. Beyond performance, ConceptMoE reduces attention computation by up to R^2times and KV cache by Rtimes. At R=2, empirical measurements show prefill speedups reaching 175\\% and decoding speedups up to 117\\% on long sequences. The minimal architectural modifications enable straightforward integration into existing MoE, demonstrating that adaptive concept-level processing fundamentally improves both effectiveness and efficiency of large language models.", "upvotes": 22, "discussionId": "697c36f4a67238fac88cc1a9", "githubRepo": "https://github.com/ZihaoHuang-notabot/ConceptMoE", "githubRepoAddedBy": "user", "ai_summary": "ConceptMoE dynamically allocates computation by merging similar tokens into concept representations, improving both performance and efficiency in large language models through adaptive processing and reduced attention computation.", "ai_keywords": ["ConceptMoE", "token-level compute allocation", "semantically similar tokens", "concept representations", "learnable chunk module", "inter-token similarity", "MoE architecture", "attention computation", "KV cache", "layer looping", "prefill speedups", "decoding speedups"], "githubStars": 6, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8ba1\u7b97\u65f6\u5bf9\u6240\u6709\u6807\u8bb0\u5206\u914d\u76f8\u540c\u7684\u8ba1\u7b97\u91cf\uff0c\u4f46\u4e00\u4e9b\u5e8f\u5217\u5f88\u5bb9\u6613\u9884\u6d4b\uff0c\u800c\u53e6\u4e00\u4e9b\u5219\u9700\u8981\u6df1\u5165\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86ConceptMoE\uff0c\u5b83\u52a8\u6001\u5730\u5c06\u8bed\u4e49\u76f8\u4f3c\u7684\u6807\u8bb0\u5408\u5e76\u4e3a\u6982\u5ff5\u8868\u793a\uff0c\u4ece\u800c\u5b9e\u73b0\u9690\u5f0f\u7684\u6807\u8bb0\u7ea7\u8ba1\u7b97\u5206\u914d\u3002</li>\n    <li>\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u6a21\u5757\u53ef\u4ee5\u8bc6\u522b\u6700\u4f73\u8fb9\u754c\uff0c\u901a\u8fc7\u6d4b\u91cf\u6807\u8bb0\u4e4b\u95f4\u7684\u76f8\u4f3c\u6027\u6765\u538b\u7f29\u5e8f\u5217\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002</li>\n    <li>\u5728\u8bed\u8a00\u548c\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u4e2d\uff0cConceptMoE\u7684\u8868\u73b0\u4f18\u4e8e\u6807\u51c6MoE\uff0c\u8bed\u8a00\u9884\u8bad\u7ec3\u5f97\u5206\u63d0\u9ad8\u4e860.9\u5206\u3002</li>\n    <li>ConceptMoE\u8fd8\u5927\u5e45\u51cf\u5c11\u4e86\u6ce8\u610f\u529b\u8ba1\u7b97\u548cKV\u7f13\u5b58\uff0c\u63d0\u5347\u4e86\u9884\u586b\u5145\u548c\u89e3\u7801\u901f\u5ea6\uff0c\u8868\u660e\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>ConceptMoE improves large language models by merging similar tokens into concept representations, allowing for smarter use of computation.</li>\n    <li>A special chunk module finds the best way to group tokens based on their similarity, reducing the amount of data processed.</li>\n    <li>ConceptMoE performs better than standard MoE models, with improvements in language tasks and multimodal benchmarks.</li>\n    <li>It can speed up processing significantly, with prefill and decoding times improving by over 100% for long sequences.</li>\n    <li>The design changes are simple, making it easy to add ConceptMoE to existing models while boosting their performance and efficiency.</li>\n</ul>"}, "publishedAt": "2026-01-29T03:58:22.000Z", "title": "ConceptMoE: Adaptive Token-to-Concept Compression for Implicit Compute Allocation", "summary": "Large language models allocate uniform computation across all tokens, ignoring that some sequences are trivially predictable while others require deep reasoning. We introduce ConceptMoE, which dynamically merges semantically similar tokens into concept representations, performing implicit token-level compute allocation. A learnable chunk module identifies optimal boundaries by measuring inter-token similarity, compressing sequences by a target ratio R before they enter the compute-intensive concept model. Crucially, the MoE architecture enables controlled evaluation: we reallocate saved computation to match baseline activated FLOPs (excluding attention map computation) and total parameters, isolating genuine architectural benefits. Under these conditions, ConceptMoE consistently outperforms standard MoE across language and vision-language tasks, achieving +0.9 points on language pretraining, +2.3 points on long context understanding, and +0.6 points on multimodal benchmarks. When converting pretrained MoE during continual training with layer looping, gains reach +5.5 points, demonstrating practical applicability. Beyond performance, ConceptMoE reduces attention computation by up to R^2times and KV cache by Rtimes. At R=2, empirical measurements show prefill speedups reaching 175\\% and decoding speedups up to 117\\% on long sequences. The minimal architectural modifications enable straightforward integration into existing MoE, demonstrating that adaptive concept-level processing fundamentally improves both effectiveness and efficiency of large language models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21420.png", "numComments": 2, "submittedBy": {"_id": "65a62085576772f531e13856", "avatarUrl": "/avatars/72c67a60422e333ea4e323f7480ae0b7.svg", "fullname": "Huang Zihao", "name": "FetchFortune", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.22046", "authors": [{"_id": "697c65a1a67238fac88cc24b", "name": "Changjian Jiang", "hidden": false}, {"_id": "697c65a1a67238fac88cc24c", "name": "Kerui Ren", "hidden": false}, {"_id": "697c65a1a67238fac88cc24d", "name": "Xudong Li", "hidden": false}, {"_id": "697c65a1a67238fac88cc24e", "name": "Kaiwen Song", "hidden": false}, {"_id": "697c65a1a67238fac88cc24f", "name": "Linning Xu", "hidden": false}, {"_id": "697c65a1a67238fac88cc250", "name": "Tao Lu", "hidden": false}, {"_id": "697c65a1a67238fac88cc251", "name": "Junting Dong", "hidden": false}, {"_id": "697c65a1a67238fac88cc252", "name": "Yu Zhang", "hidden": false}, {"_id": "697c65a1a67238fac88cc253", "name": "Bo Dai", "hidden": false}, {"_id": "697c65a1a67238fac88cc254", "name": "Mulin Yu", "hidden": false}], "publishedAt": "2026-01-29T17:47:26.000Z", "submittedOnDailyAt": "2026-01-30T05:34:08.168Z", "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction", "submittedOnDailyBy": {"_id": "656e9e26bbec423d88b603e8", "avatarUrl": "/avatars/10d8cb945a60e0401bfa4f74137cb203.svg", "isPro": false, "fullname": "MulinYu", "user": "UML", "type": "user"}, "summary": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: https://city-super.github.io/PLANING/ .", "upvotes": 19, "discussionId": "697c65a1a67238fac88cc255", "projectPage": "https://city-super.github.io/PLANING/", "ai_summary": "PLANING presents an efficient streaming reconstruction framework that combines explicit geometric primitives with neural Gaussians to achieve high-quality rendering and accurate geometry simultaneously through decoupled optimization.", "ai_keywords": ["monocular image sequences", "hybrid representation", "explicit geometric primitives", "neural Gaussians", "decoupled manner", "online initialization", "optimization strategy", "streaming reconstruction", "dense mesh Chamfer-L2", "PSNR", "ScanNetV2", "2D Gaussian Splatting"], "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>PLANING\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u6d41\u5f0f\u91cd\u5efa\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u51e0\u4f55\u539f\u59cb\u4f53\u4e0e\u795e\u7ecf\u9ad8\u65af\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6846\u67b6\u53ef\u4ee5\u72ec\u7acb\u66f4\u65b0\u51e0\u4f55\u548c\u5916\u89c2\uff0c\u4ece\u800c\u5b9e\u73b0\u7a33\u5b9a\u7684\u6d41\u5f0f\u91cd\u5efa\uff0c\u5e76\u51cf\u5c11\u7ed3\u6784\u5197\u4f59\u3002</li>\n    <li>PLANING\u5728\u91cd\u5efa\u8d28\u91cf\u4e0a\u6bd4PGSR\u63d0\u9ad8\u4e8618.52%\uff0c\u5e76\u5728ScanNetV2\u573a\u666f\u91cd\u5efa\u4e0a\u901f\u5ea6\u8d85\u8fc72D\u9ad8\u65af\u70b9\u4e915\u500d\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u5728\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u573a\u666f\u5efa\u6a21\u548c\u6a21\u62df\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u5408\u7528\u4e8e\u667a\u80fd\u4f53AI\u3002</li>\n    <li>\u9879\u76ee\u9875\u9762\u53ef\u4ee5\u5728 https://city-super.github.io/PLANING/ \u627e\u5230\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>PLANING is a new framework for reconstructing 3D scenes from 2D images quickly and efficiently.</li>\n    <li>It combines geometric shapes with neural techniques to separately improve geometry and appearance.</li>\n    <li>PLANING is faster than previous methods, completing reconstructions in under 100 seconds.</li>\n    <li>It shows significant improvements in reconstruction quality compared to existing methods.</li>\n    <li>This approach is useful for various applications, including modeling large scenes and environments for AI simulations.</li>\n</ul>"}, "publishedAt": "2026-01-29T12:47:26.000Z", "title": "PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction", "summary": "Streaming reconstruction from monocular image sequences remains challenging, as existing methods typically favor either high-quality rendering or accurate geometry, but rarely both. We present PLANING, an efficient on-the-fly reconstruction framework built on a hybrid representation that loosely couples explicit geometric primitives with neural Gaussians, enabling geometry and appearance to be modeled in a decoupled manner. This decoupling supports an online initialization and optimization strategy that separates geometry and appearance updates, yielding stable streaming reconstruction with substantially reduced structural redundancy. PLANING improves dense mesh Chamfer-L2 by 18.52% over PGSR, surpasses ARTDECO by 1.31 dB PSNR, and reconstructs ScanNetV2 scenes in under 100 seconds, over 5x faster than 2D Gaussian Splatting, while matching the quality of offline per-scene optimization. Beyond reconstruction quality, the structural clarity and computational efficiency of \\modelname~make it well suited for a broad range of downstream applications, such as enabling large-scale scene modeling and simulation-ready environments for embodied AI. Project page: https://city-super.github.io/PLANING/ .", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22046.png", "numComments": 2, "submittedBy": {"_id": "656e9e26bbec423d88b603e8", "avatarUrl": "/avatars/10d8cb945a60e0401bfa4f74137cb203.svg", "fullname": "MulinYu", "name": "UML", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": false}, {"paper": {"id": "2601.21337", "authors": [{"_id": "697c1ebea67238fac88cc0ae", "name": "Xian Shi", "hidden": false}, {"_id": "697c1ebea67238fac88cc0af", "name": "Xiong Wang", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b0", "name": "Zhifang Guo", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b1", "name": "Yongqi Wang", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b2", "name": "Pei Zhang", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b3", "name": "Xinyu Zhang", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b4", "name": "Zishan Guo", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b5", "name": "Hongkun Hao", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b6", "name": "Yu Xi", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b7", "name": "Baosong Yang", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b8", "name": "Jin Xu", "hidden": false}, {"_id": "697c1ebea67238fac88cc0b9", "name": "Jingren Zhou", "hidden": false}, {"_id": "697c1ebea67238fac88cc0ba", "name": "Junyang Lin", "hidden": false}], "publishedAt": "2026-01-29T06:58:13.000Z", "submittedOnDailyAt": "2026-01-30T02:17:20.204Z", "title": "Qwen3-ASR Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "In this report, we introduce Qwen3-ASR family, which includes two powerful all-in-one speech recognition models and a novel non-autoregressive speech forced alignment model. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B are ASR models that support language identification and ASR for 52 languages and dialects. Both of them leverage large-scale speech training data and the strong audio understanding ability of their foundation model Qwen3-Omni. We conduct comprehensive internal evaluation besides the open-sourced benchmarks as ASR models might differ little on open-sourced benchmark scores but exhibit significant quality differences in real-world scenarios. The experiments reveal that the 1.7B version achieves SOTA performance among open-sourced ASR models and is competitive with the strongest proprietary APIs while the 0.6B version offers the best accuracy-efficiency trade-off. Qwen3-ASR-0.6B can achieve an average TTFT as low as 92ms and transcribe 2000 seconds speech in 1 second at a concurrency of 128. Qwen3-ForcedAligner-0.6B is an LLM based NAR timestamp predictor that is able to align text-speech pairs in 11 languages. Timestamp accuracy experiments show that the proposed model outperforms the three strongest force alignment models and takes more advantages in efficiency and versatility. To further accelerate the community research of ASR and audio understanding, we release these models under the Apache 2.0 license.", "upvotes": 18, "discussionId": "697c1ebea67238fac88cc0bb", "ai_summary": "The Qwen3-ASR family introduces speech recognition models with language identification capabilities and a non-autoregressive forced alignment model, achieving state-of-the-art performance and efficient processing.", "ai_keywords": ["speech recognition models", "language identification", "non-autoregressive models", "forced alignment", "timestamp prediction", "audio understanding", "large-scale speech training data", "foundation model", "TTFT", "concurrency"], "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86Qwen3-ASR\u7cfb\u5217\uff0c\u5305\u62ec\u4e24\u4e2a\u5f3a\u5927\u7684\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u548c\u4e00\u4e2a\u65b0\u7684\u975e\u81ea\u56de\u5f52\u8bed\u97f3\u5f3a\u5236\u5bf9\u9f50\u6a21\u578b\u3002</li>\n    <li>Qwen3-ASR-1.7B\u548cQwen3-ASR-0.6B\u652f\u630152\u79cd\u8bed\u8a00\u548c\u65b9\u8a00\u7684\u8bed\u8a00\u8bc6\u522b\u548c\u8bed\u97f3\u8bc6\u522b\u3002</li>\n    <li>1.7B\u7248\u672c\u5728\u5f00\u6e90\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0c0.6B\u7248\u672c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u3002</li>\n    <li>Qwen3-ASR-0.6B\u5728128\u4e2a\u5e76\u53d1\u60c5\u51b5\u4e0b\uff0c\u5e73\u5747\u8f6c\u5f55\u5ef6\u8fdf\u4ec5\u4e3a92\u6beb\u79d2\u3002</li>\n    <li>\u53d1\u5e03\u7684\u6a21\u578b\u5c06\u4fc3\u8fdb\u8bed\u97f3\u8bc6\u522b\u548c\u97f3\u9891\u7406\u89e3\u7684\u793e\u533a\u7814\u7a76\uff0c\u5e76\u91c7\u7528Apache 2.0\u8bb8\u53ef\u534f\u8bae\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Introducing the Qwen3-ASR family, which has two speech recognition models and a new model for aligning speech with text.</li>\n    <li>The models support recognizing speech in 52 languages and dialects, using large training data and strong audio understanding.</li>\n    <li>The 1.7B version is the best among open-source ASR models and competes well with top paid services, while the 0.6B version is efficient with fast transcription speeds.</li>\n    <li>The Qwen3-ForcedAligner-0.6B model aligns text and speech in 11 languages and is more accurate and efficient than other models.</li>\n    <li>These models are released for public use under the Apache 2.0 license to help advance research in speech recognition and audio understanding.</li>\n</ul>"}, "publishedAt": "2026-01-29T01:58:13.000Z", "title": "Qwen3-ASR Technical Report", "summary": "In this report, we introduce Qwen3-ASR family, which includes two powerful all-in-one speech recognition models and a novel non-autoregressive speech forced alignment model. Qwen3-ASR-1.7B and Qwen3-ASR-0.6B are ASR models that support language identification and ASR for 52 languages and dialects. Both of them leverage large-scale speech training data and the strong audio understanding ability of their foundation model Qwen3-Omni. We conduct comprehensive internal evaluation besides the open-sourced benchmarks as ASR models might differ little on open-sourced benchmark scores but exhibit significant quality differences in real-world scenarios. The experiments reveal that the 1.7B version achieves SOTA performance among open-sourced ASR models and is competitive with the strongest proprietary APIs while the 0.6B version offers the best accuracy-efficiency trade-off. Qwen3-ASR-0.6B can achieve an average TTFT as low as 92ms and transcribe 2000 seconds speech in 1 second at a concurrency of 128. Qwen3-ForcedAligner-0.6B is an LLM based NAR timestamp predictor that is able to align text-speech pairs in 11 languages. Timestamp accuracy experiments show that the proposed model outperforms the three strongest force alignment models and takes more advantages in efficiency and versatility. To further accelerate the community research of ASR and audio understanding, we release these models under the Apache 2.0 license.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21337.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 221, "isUserFollowing": false}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20730", "authors": [{"_id": "697b161fdf3e800774f13d05", "name": "Shicheng Fang", "hidden": false}, {"_id": "697b161fdf3e800774f13d06", "name": "Yuxin Wang", "hidden": false}, {"_id": "697b161fdf3e800774f13d07", "name": "XiaoRan Liu", "hidden": false}, {"_id": "697b161fdf3e800774f13d08", "name": "Jiahao Lu", "hidden": false}, {"_id": "697b161fdf3e800774f13d09", "name": "Chuanyuan Tan", "hidden": false}, {"_id": "697b161fdf3e800774f13d0a", "name": "Xinchi Chen", "hidden": false}, {"_id": "697b161fdf3e800774f13d0b", "name": "Yining Zheng. Xuanjing Huang", "hidden": false}, {"_id": "697b161fdf3e800774f13d0c", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-01-28T16:05:44.000Z", "submittedOnDailyAt": "2026-01-30T06:14:19.038Z", "title": "AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts", "submittedOnDailyBy": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "isPro": false, "fullname": "Xiaoran Liu (SII)", "user": "SII-xrliu", "type": "user"}, "summary": "The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce AgentLongBench, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues.", "upvotes": 17, "discussionId": "697b1620df3e800774f13d0d", "ai_summary": "AgentLongBench evaluates large language models as autonomous agents through dynamic environment interactions, revealing challenges in handling high-information-density tool responses compared to memory fragmentation in long conversations.", "ai_keywords": ["Large Language Models", "autonomous agents", "dynamic contexts", "AgentLongBench", "Lateral Thinking Puzzles", "environment rollouts", "knowledge-intensive scenarios", "knowledge-free scenarios", "information density", "tool responses", "memory fragmentation", "long-turn dialogues"], "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6b63\u5728\u53d1\u5c55\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u9700\u7ba1\u7406\u590d\u6742\u7684\u52a8\u6001\u73af\u5883\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bc4\u4f30\u6807\u51c6\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u68c0\u7d22\u4efb\u52a1\uff0c\u65e0\u6cd5\u6a21\u62df\u4ee3\u7406\u4e0e\u73af\u5883\u4e92\u52a8\u7684\u590d\u6742\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86AgentLongBench\uff0c\u901a\u8fc7\u6a21\u62df\u73af\u5883\u6765\u8bc4\u4f30\u4ee3\u7406\uff0c\u4f7f\u7528\u6a2a\u5411\u601d\u7ef4\u8c1c\u9898\u3002</li>\n    <li>\u5b9e\u9a8c\u663e\u793a\uff0c\u5c3d\u7ba1\u5f53\u524d\u6a21\u578b\u5728\u9759\u6001\u68c0\u7d22\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u52a8\u6001\u4fe1\u606f\u7efc\u5408\u4e0a\u5b58\u5728\u663e\u8457\u5f31\u70b9\u3002</li>\n    <li>\u4fe1\u606f\u5bc6\u5ea6\u9ad8\u7684\u5de5\u5177\u54cd\u5e94\u5bf9\u4ee3\u7406\u9020\u6210\u66f4\u5927\u6311\u6218\uff0c\u800c\u975e\u957f\u5bf9\u8bdd\u4e2d\u7684\u8bb0\u5fc6\u788e\u7247\u5316\u95ee\u9898\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are evolving into autonomous agents that need to handle complex and changing situations.</li>\n    <li>Current tests for these agents often involve simple retrieval tasks that do not reflect real-world interactions.</li>\n    <li>AgentLongBench is a new evaluation framework that uses Lateral Thinking Puzzles to simulate agent-environment interactions.</li>\n    <li>Experiments show that while LLMs can retrieve static information well, they struggle with synthesizing dynamic information needed for workflows.</li>\n    <li>The difficulty is linked to the number of tokens needed to answer questions, with high-density information from tools being particularly challenging.</li>\n</ul>"}, "publishedAt": "2026-01-28T11:05:44.000Z", "title": "AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts", "summary": "The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce AgentLongBench, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20730.png", "numComments": 2, "submittedBy": {"_id": "64f033ef82c6eea604c4da8b", "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg", "fullname": "Xiaoran Liu (SII)", "name": "SII-xrliu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 12, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2601.20833", "authors": [{"_id": "697b9192a67238fac88cbee8", "name": "Tengyue Xu", "hidden": false}, {"_id": "697b9192a67238fac88cbee9", "name": "Zhuoyang Qian", "hidden": false}, {"_id": "697b9192a67238fac88cbeea", "name": "Gaoge Liu", "hidden": false}, {"_id": "697b9192a67238fac88cbeeb", "name": "Li Ling", "hidden": false}, {"_id": "697b9192a67238fac88cbeec", "name": "Zhentao Zhang", "hidden": false}, {"_id": "697b9192a67238fac88cbeed", "name": "Biao Wu", "hidden": false}, {"_id": "697b9192a67238fac88cbeee", "name": "Shuo Zhang", "hidden": false}, {"_id": "697b9192a67238fac88cbeef", "name": "Ke Lu", "hidden": false}, {"_id": "697b9192a67238fac88cbef0", "name": "Wei Shi", "hidden": false}, {"_id": "697b9192a67238fac88cbef1", "name": "Ziqi Wang", "hidden": false}, {"_id": "697b9192a67238fac88cbef2", "name": "Zheng Feng", "hidden": false}, {"_id": "697b9192a67238fac88cbef3", "name": "Yan Luo", "hidden": false}, {"_id": "697b9192a67238fac88cbef4", "name": "Shu Xu", "hidden": false}, {"_id": "697b9192a67238fac88cbef5", "name": "Yongjin Chen", "hidden": false}, {"_id": "697b9192a67238fac88cbef6", "name": "Zhibo Feng", "hidden": false}, {"_id": "697b9192a67238fac88cbef7", "name": "Zhuo Chen", "hidden": false}, {"_id": "697b9192a67238fac88cbef8", "name": "Bruce Yuan", "hidden": false}, {"_id": "697b9192a67238fac88cbef9", "name": "Harry Wang", "hidden": false}, {"_id": "697b9192a67238fac88cbefa", "name": "Kris Chen", "hidden": false}], "publishedAt": "2026-01-28T18:31:54.000Z", "submittedOnDailyAt": "2026-01-30T03:32:00.106Z", "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives", "submittedOnDailyBy": {"_id": "62baa0d6dd02fbf607ce97be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62baa0d6dd02fbf607ce97be/V0I6pANlLEf2YDd9ZLZgi.jpeg", "isPro": false, "fullname": "Wendy", "user": "Wendy-Fly", "type": "user"}, "summary": "Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.", "upvotes": 113, "discussionId": "697b9192a67238fac88cbefb", "githubRepo": "https://github.com/AgentAlphaAGI/Idea2Paper", "githubRepoAddedBy": "user", "ai_summary": "Offline knowledge construction through structured methodological graphs enables more reliable and scalable autonomous scientific discovery by reducing reliance on real-time literature processing.", "ai_keywords": ["large language model", "autonomous scientific discovery", "runtime-centric execution", "context window limitations", "hallucination", "pre-computation-driven framework", "peer-reviewed papers", "research patterns", "methodological knowledge graph", "end-to-end research workflows"], "githubStars": 54, "organization": {"_id": "69542731e1200d74c1c053d1", "name": "AgentAlphaAGI", "fullname": "AgentAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b78eb76ab5d14ca7faac87/TbMZ3y00APtRzHEfTSR7I.jpeg"}, "summary_zh": "<ul>\n    <li>\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u7684\u7814\u7a76\u8fdb\u5c55\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\uff0c\u80fd\u81ea\u52a8\u5316\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u5728\u7ebf\u8ba1\u7b97\uff0c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u8106\u5f31\u6027\u3002</li>\n    <li>\u63d0\u51faIdea2Story\u6846\u67b6\uff0c\u8f6c\u5411\u79bb\u7ebf\u77e5\u8bc6\u6784\u5efa\uff0c\u63d0\u9ad8\u6587\u732e\u7406\u89e3\u6548\u7387\u3002</li>\n    <li>Idea2Story\u6536\u96c6\u540c\u884c\u8bc4\u5ba1\u8bba\u6587\u53ca\u53cd\u9988\uff0c\u63d0\u53d6\u6838\u5fc3\u65b9\u6cd5\u5355\u5143\u5e76\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u3002</li>\n    <li>\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u8fde\u8d2f\u7684\u65b0\u7814\u7a76\u6a21\u5f0f\uff0c\u964d\u4f4e\u91cd\u590d\u63a8\u7406\u7684\u9700\u6c42\uff0c\u652f\u6301\u53ef\u9760\u7684\u79d1\u5b66\u53d1\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent advancements in using large language models (LLMs) have automated many research processes but have high computational costs and issues with reasoning.</li>\n    <li>The proposed system, Idea2Story, focuses on building an offline knowledge base from scientific papers instead of reasoning online.</li>\n    <li>Idea2Story gathers research papers and feedback, extracts important methods, and organizes them into a structured knowledge graph.</li>\n    <li>This system allows users to quickly find and use established research patterns, reducing the need for trial-and-error approaches.</li>\n    <li>Preliminary studies show that Idea2Story can create coherent and innovative research ideas, making it a promising tool for scientific discovery.</li>\n</ul>"}, "publishedAt": "2026-01-28T13:31:54.000Z", "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives", "summary": "Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20833.png", "numComments": 1, "submittedBy": {"_id": "62baa0d6dd02fbf607ce97be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62baa0d6dd02fbf607ce97be/V0I6pANlLEf2YDd9ZLZgi.jpeg", "fullname": "Wendy", "name": "Wendy-Fly", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "69542731e1200d74c1c053d1", "name": "AgentAlphaAGI", "fullname": "AgentAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b78eb76ab5d14ca7faac87/TbMZ3y00APtRzHEfTSR7I.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.18418", "authors": [{"_id": "69785315026bdf0473116f6a", "user": {"_id": "62dce08bb2c60f29c3d0a5da", "avatarUrl": "/avatars/87ce03e61c4c6eb686c9491ef4fda225.svg", "isPro": false, "fullname": "Ji Zeng", "user": "stargazerzj", "type": "user"}, "name": "Ji Zeng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T08:31:51.245Z", "hidden": false}, {"_id": "69785315026bdf0473116f6b", "name": "Dayuan Fu", "hidden": false}, {"_id": "69785315026bdf0473116f6c", "name": "Tiantian Mi", "hidden": false}, {"_id": "69785315026bdf0473116f6d", "name": "Yumin Zhuang", "hidden": false}, {"_id": "69785315026bdf0473116f6e", "user": {"_id": "6865e6b362fc5689c5e67733", "avatarUrl": "/avatars/186f3d248791d961b0a810d5225167cc.svg", "isPro": false, "fullname": "Yaxing Huang", "user": "Rookie-Noob-Newbie", "type": "user"}, "name": "Yaxing Huang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:30.220Z", "hidden": false}, {"_id": "69785315026bdf0473116f6f", "user": {"_id": "67638cc0d63e4b348e8a5fa3", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67638cc0d63e4b348e8a5fa3/BZNlw1uTGUcumCrXKkerx.png", "isPro": false, "fullname": "Xuefeng Li", "user": "drxuefeng", "type": "user"}, "name": "Xuefeng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:37.248Z", "hidden": false}, {"_id": "69785315026bdf0473116f70", "name": "Lyumanshan Ye", "hidden": false}, {"_id": "69785315026bdf0473116f71", "name": "Muhang Xie", "hidden": false}, {"_id": "69785315026bdf0473116f72", "name": "Qishuo Hua", "hidden": false}, {"_id": "69785315026bdf0473116f73", "name": "Zhen Huang", "hidden": false}, {"_id": "69785315026bdf0473116f74", "user": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "isPro": false, "fullname": "Mohan Jiang (SII)", "user": "mhjiang0408", "type": "user"}, "name": "Mohan Jiang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:23.438Z", "hidden": false}, {"_id": "69785315026bdf0473116f75", "name": "Hanning Wang", "hidden": false}, {"_id": "69785315026bdf0473116f76", "user": {"_id": "66fa544c54f87b607fbffd2e", "avatarUrl": "/avatars/94195dcda0eb68e8fd20d80718744697.svg", "isPro": false, "fullname": "Jifan Lin", "user": "evanlin2570", "type": "user"}, "name": "Jifan Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:00:57.029Z", "hidden": false}, {"_id": "69785315026bdf0473116f77", "name": "Yang Xiao", "hidden": false}, {"_id": "69785315026bdf0473116f78", "name": "Jie Sun", "hidden": false}, {"_id": "69785315026bdf0473116f79", "user": {"_id": "684faf712acd915b5afc055f", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/684faf712acd915b5afc055f/K7icmL08HxniWDgdph73i.jpeg", "isPro": false, "fullname": "Yunze Wu", "user": "wyzmike", "type": "user"}, "name": "Yunze Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:01:06.063Z", "hidden": false}, {"_id": "69785315026bdf0473116f7a", "name": "Pengfei Liu", "hidden": false}], "publishedAt": "2026-01-26T12:20:18.000Z", "submittedOnDailyAt": "2026-01-27T03:34:37.777Z", "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "submittedOnDailyBy": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "isPro": false, "fullname": "Mohan Jiang (SII)", "user": "mhjiang0408", "type": "user"}, "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...", "upvotes": 104, "discussionId": "69785315026bdf0473116f7b", "githubRepo": "https://github.com/GAIR-NLP/daVinci-Dev", "githubRepoAddedBy": "user", "ai_summary": "Agentic mid-training enables large language models to develop autonomous software engineering capabilities through specialized data synthesis techniques that bridge the gap between static training data and dynamic development environments.", "ai_keywords": ["Large Language Model", "agentic software engineering", "mid-training", "distribution mismatch", "agent-native data", "contextually-native trajectories", "environmentally-native trajectories", "SWE-Bench Verified", "Kimi-Dev", "resolution rates"], "githubStars": 22, "organization": {"_id": "630bc2d186b8b9904c33ce1b", "name": "GAIR", "fullname": "SII - GAIR", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6b63\u5728\u4ece\u5355\u6b21\u4ee3\u7801\u751f\u6210\u8f6c\u5411\u81ea\u4e3b\u8f6f\u4ef6\u5de5\u7a0b\uff0c\u5373\u6a21\u578b\u53ef\u4ee5\u81ea\u4e3b\u5bfc\u822a\u3001\u7f16\u8f91\u548c\u6d4b\u8bd5\u590d\u6742\u4ee3\u7801\u5e93\u3002</li>\n    <li>\u867d\u7136\u540e\u671f\u8bad\u7ec3\u65b9\u6cd5\u5df2\u7ecf\u6210\u4e3a\u4ee3\u7801\u4ee3\u7406\u7684\u4e3b\u8981\u65b9\u6cd5\uff0c\u4f46\u201c\u4e2d\u671f\u8bad\u7ec3\u201d\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u5c3d\u7ba1\u5b83\u63d0\u4f9b\u4e86\u66f4\u53ef\u6269\u5c55\u7684\u57fa\u7840\u4ee3\u7406\u884c\u4e3a\u57f9\u517b\u9014\u5f84\u3002</li>\n    <li>\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u662f\u9759\u6001\u8bad\u7ec3\u6570\u636e\u4e0e\u771f\u5b9e\u5f00\u53d1\u73af\u5883\u4e2d\u7684\u52a8\u6001\u53cd\u9988\u4e4b\u95f4\u7684\u5206\u5e03\u4e0d\u5339\u914d\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u7684\u4e2d\u671f\u8bad\u7ec3\u7814\u7a76\uff0c\u5efa\u7acb\u4e86\u6709\u6548\u4ee3\u7406\u5f00\u53d1\u7684\u5927\u89c4\u6a21\u6570\u636e\u5408\u6210\u539f\u5219\u548c\u8bad\u7ec3\u65b9\u6cd5\u3002</li>\n    <li>\u6211\u4eec\u7684\u6a21\u578b\u5728\u201c\u8f6f\u4ef6\u5de5\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u201d\u4e0a\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u4ee3\u7406\u80fd\u529b\uff0c\u5e76\u5728\u4e24\u79cd\u540e\u671f\u8bad\u7ec3\u8bbe\u7f6e\u4e0b\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u7684\u4e2d\u671f\u8bad\u7ec3\u4ee4\u724c\u5c11\u4e8e\u4e00\u534a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are evolving to autonomously handle complex software tasks, moving beyond simple code generation.</li>\n    <li>Agentic mid-training (MT) on large datasets is a promising but underexplored method that can enhance the capabilities of code agents, potentially requiring fewer resources than traditional methods.</li>\n    <li>A key challenge is the difference between static training data and the dynamic nature of real software development environments.</li>\n    <li>The study introduces new data synthesis principles and training methods for effective agent development, focusing on two types of data: contextually-native and environmentally-native trajectories.</li>\n    <li>The proposed models outperform previous methods, achieving high resolution rates while using significantly fewer training tokens.</li>\n</ul>"}, "publishedAt": "2026-01-26T07:20:18.000Z", "title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "summary": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18418.png", "numComments": 2, "submittedBy": {"_id": "66d01e4401f2a6b4cd93ad87", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d01e4401f2a6b4cd93ad87/qxEUHyO8WauOCLcHXfiOS.png", "fullname": "Mohan Jiang (SII)", "name": "mhjiang0408", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "630bc2d186b8b9904c33ce1b", "name": "GAIR", "fullname": "SII - GAIR", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6144a0c4ff1146bbd84d9865/NqAuVddq2ci-AsFcFNbav.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.20354", "authors": [{"_id": "697c1857a67238fac88cc06e", "user": {"_id": "656c9cfef7be0986b49934ea", "avatarUrl": "/avatars/2030e77c28fb4c518b692cd9a20de665.svg", "isPro": false, "fullname": "MuMing", "user": "ZengbinWang", "type": "user"}, "name": "Zengbin Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T09:36:35.067Z", "hidden": false}, {"_id": "697c1857a67238fac88cc06f", "name": "Xuecai Hu", "hidden": false}, {"_id": "697c1857a67238fac88cc070", "name": "Yong Wang", "hidden": false}, {"_id": "697c1857a67238fac88cc071", "name": "Feng Xiong", "hidden": false}, {"_id": "697c1857a67238fac88cc072", "name": "Man Zhang", "hidden": false}, {"_id": "697c1857a67238fac88cc073", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-28T08:15:00.000Z", "submittedOnDailyAt": "2026-01-30T05:01:51.614Z", "title": "Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.", "upvotes": 96, "discussionId": "697c1857a67238fac88cc074", "githubRepo": "https://github.com/AMAP-ML/SpatialGenEval", "githubRepoAddedBy": "user", "ai_summary": "A new benchmark and dataset are introduced to evaluate and improve spatial reasoning capabilities in text-to-image models through information-dense prompts and fine-tuning.", "ai_keywords": ["text-to-image models", "spatial intelligence", "benchmark", "long prompts", "information-dense prompts", "spatial reasoning", "Stable Diffusion-XL", "Uniworld-V1", "OmniGen2", "fine-tuning"], "githubStars": 93, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u65f6\u8868\u73b0\u4e0d\u4f73\u3002</li>\n    <li>\u63d0\u51fa\u4e86SpatialGenEval\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u7a7a\u95f4\u667a\u80fd\uff0c\u5305\u542b1,230\u4e2a\u957f\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u63d0\u793a\u3002</li>\n    <li>\u8fd9\u4e9b\u63d0\u793a\u8986\u76d625\u4e2a\u771f\u5b9e\u573a\u666f\uff0c\u6d89\u53ca\u7a7a\u95f4\u4f4d\u7f6e\u3001\u5e03\u5c40\u3001\u906e\u6321\u548c\u56e0\u679c\u5173\u7cfb\u7b49\u65b9\u9762\u3002</li>\n    <li>\u8bc4\u4f30\u663e\u793a\uff0c\u73b0\u6709\u6a21\u578b\u5728\u9ad8\u9636\u7a7a\u95f4\u63a8\u7406\u4e0a\u4ecd\u5b58\u5728\u74f6\u9888\u3002</li>\n    <li>\u6784\u5efa\u4e86SpatialT2I\u6570\u636e\u96c6\uff0c\u5305\u542b15,400\u5bf9\u6587\u672c-\u56fe\u50cf\uff0c\u7ecf\u8fc7\u6539\u5199\u4ee5\u4fdd\u6301\u56fe\u50cf\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\u548c\u7a7a\u95f4\u5173\u7cfb\u7684\u771f\u5b9e\u611f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Text-to-image (T2I) models create impressive images but struggle with understanding complex spatial relationships.</li>\n    <li>The new benchmark, SpatialGenEval, evaluates T2I models on their spatial intelligence using 1,230 detailed prompts across 25 scenes.</li>\n    <li>Each prompt includes questions about object positions, layouts, and relationships, highlighting weaknesses in spatial reasoning among top models.</li>\n    <li>SpatialT2I dataset was created with 15,400 text-image pairs to improve image consistency while maintaining rich information.</li>\n    <li>Fine-tuning on this dataset improved performance of leading models, showing a better understanding of spatial relationships.</li>\n</ul>"}, "publishedAt": "2026-01-28T03:15:00.000Z", "title": "Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models", "summary": "Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20354.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 7, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20614", "authors": [{"_id": "697ac91bdf3e800774f13c12", "name": "Yanqi Dai", "hidden": false}, {"_id": "697ac91bdf3e800774f13c13", "name": "Yuxiang Ji", "hidden": false}, {"_id": "697ac91bdf3e800774f13c14", "name": "Xiao Zhang", "hidden": false}, {"_id": "697ac91bdf3e800774f13c15", "name": "Yong Wang", "hidden": false}, {"_id": "697ac91bdf3e800774f13c16", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "697ac91bdf3e800774f13c17", "name": "Zhiwu Lu", "hidden": false}], "publishedAt": "2026-01-28T13:49:23.000Z", "submittedOnDailyAt": "2026-01-29T00:15:35.371Z", "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation", "submittedOnDailyBy": {"_id": "66cde57cb1fe4c78fe3ab770", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66cde57cb1fe4c78fe3ab770/0R1aA-f_XLjCfy1HwqZ-p.jpeg", "isPro": false, "fullname": "Yanqi Dai", "user": "YanqiDai", "type": "user"}, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.", "upvotes": 93, "discussionId": "697ac91bdf3e800774f13c18", "githubRepo": "https://github.com/AMAP-ML/MathForge", "githubRepoAddedBy": "user", "ai_summary": "MathForge enhances mathematical reasoning in large models through a dual framework combining difficulty-aware policy optimization and multi-aspect question reformulation to address limitations in existing reinforcement learning methods.", "ai_keywords": ["Reinforcement Learning with Verifiable Rewards", "Group Relative Policy Optimization", "Difficulty-Aware Group Policy Optimization", "Multi-Aspect Question Reformulation", "mathematical reasoning", "policy updates", "group advantage estimation", "question-level weighting", "data augmentation"], "githubStars": 84, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "summary_zh": "<ul>\n    <li>RLVR\uff08\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u53ef\u4ee5\u589e\u5f3a\u5927\u578b\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u66f4\u96be\u95ee\u9898\u65f6\u5b58\u5728\u7cfb\u7edf\u6027\u4e0d\u8db3\uff0c\u7b97\u6cd5\u548c\u6570\u636e\u4e24\u65b9\u9762\u90fd\u6709\u95ee\u9898\u3002</li>\n    <li>\u63d0\u51fa\u7684MathForge\u6846\u67b6\u5305\u62ec\u96be\u5ea6\u611f\u77e5\u7684\u7fa4\u4f53\u7b56\u7565\u4f18\u5316\uff08DGPO\uff09\u548c\u591a\u65b9\u9762\u95ee\u9898\u91cd\u6784\uff08MQR\uff09\u7b56\u7565\u3002</li>\n    <li>DGPO\u901a\u8fc7\u96be\u5ea6\u5e73\u8861\u7684\u7fa4\u4f53\u4f18\u52bf\u4f30\u8ba1\u548c\u52a0\u6743\u6765\u4f18\u5148\u8003\u8651\u66f4\u96be\u7684\u95ee\u9898\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cMathForge\u5728\u5404\u79cd\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement Learning with Verifiable Rewards (RLVR) helps improve mathematical reasoning in large models.</li>\n    <li>Current methods do not focus enough on difficult questions, which are important for developing skills.</li>\n    <li>The proposed MathForge framework includes a new Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy.</li>\n    <li>DGPO addresses the issue of lower updates for harder questions and prioritizes them, while MQR reformulates questions to make them harder without losing the correct answer.</li>\n    <li>Experiments show that MathForge performs much better than existing methods in mathematical reasoning tasks, and the code and data are available online.</li>\n</ul>"}, "publishedAt": "2026-01-28T08:49:23.000Z", "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20614.png", "numComments": 12, "submittedBy": {"_id": "66cde57cb1fe4c78fe3ab770", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66cde57cb1fe4c78fe3ab770/0R1aA-f_XLjCfy1HwqZ-p.jpeg", "fullname": "Yanqi Dai", "name": "YanqiDai", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "67d11771890254196d3174e5", "name": "GD-ML", "fullname": "AMAP-ML", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67d116c47be76de1a40873ca/s5ukAx9E36ZZIKvbpBRi4.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.21204", "authors": [{"_id": "697c3801a67238fac88cc1b1", "name": "Hong Liu", "hidden": false}, {"_id": "697c3801a67238fac88cc1b2", "name": "Jiaqi Zhang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b3", "name": "Chao Wang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b4", "name": "Xing Hu", "hidden": false}, {"_id": "697c3801a67238fac88cc1b5", "name": "Linkun Lyu", "hidden": false}, {"_id": "697c3801a67238fac88cc1b6", "name": "Jiaqi Sun", "hidden": false}, {"_id": "697c3801a67238fac88cc1b7", "name": "Xurui Yang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b8", "name": "Bo Wang", "hidden": false}, {"_id": "697c3801a67238fac88cc1b9", "name": "Fengcun Li", "hidden": false}, {"_id": "697c3801a67238fac88cc1ba", "name": "Yulei Qian", "hidden": false}, {"_id": "697c3801a67238fac88cc1bb", "name": "Lingtong Si", "hidden": false}, {"_id": "697c3801a67238fac88cc1bc", "name": "Yerui Sun", "hidden": false}, {"_id": "697c3801a67238fac88cc1bd", "name": "Rumei Li", "hidden": false}, {"_id": "697c3801a67238fac88cc1be", "name": "Peng Pei", "hidden": false}, {"_id": "697c3801a67238fac88cc1bf", "name": "Yuchen Xie", "hidden": false}, {"_id": "697c3801a67238fac88cc1c0", "name": "Xunliang Cai", "hidden": false}], "publishedAt": "2026-01-29T03:11:19.000Z", "submittedOnDailyAt": "2026-01-30T02:18:11.112Z", "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.", "upvotes": 76, "discussionId": "697c3801a67238fac88cc1c1", "ai_summary": "Embedding scaling offers superior sparsity scaling compared to expert scaling in large language models, enabling efficient inference through system optimizations and speculative decoding.", "ai_keywords": ["Mixture-of-Experts", "sparsity scaling", "embedding scaling", "Pareto frontier", "parameter budgeting", "model width", "model depth", "system optimizations", "speculative decoding", "LongCat-Flash-Lite"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u53d8\u5f97\u666e\u904d\uff0c\u4f46\u9762\u4e34\u6536\u76ca\u9012\u51cf\u548c\u7cfb\u7edf\u74f6\u9888\u95ee\u9898\u3002</li>\n    <li>\u672c\u6587\u63a2\u8ba8\u901a\u8fc7\u5d4c\u5165\u6269\u5c55\u4f5c\u4e3a\u89c4\u6a21\u7a00\u758f\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u5176\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4e13\u5bb6\u6269\u5c55\u3002</li>\n    <li>\u7cfb\u7edf\u5206\u6790\u4e86\u5f71\u54cd\u5d4c\u5165\u6269\u5c55\u6548\u679c\u7684\u5173\u952e\u67b6\u6784\u56e0\u7d20\uff0c\u5305\u62ec\u53c2\u6570\u5206\u914d\u548c\u6a21\u578b\u7684\u5bbd\u5ea6\u4e0e\u6df1\u5ea6\u3002</li>\n    <li>\u7ed3\u5408\u7cfb\u7edf\u4f18\u5316\u548c\u6295\u673a\u89e3\u7801\uff0c\u6210\u529f\u5c06\u7a00\u758f\u6027\u8f6c\u5316\u4e3a\u663e\u8457\u7684\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u3002</li>\n    <li>\u63a8\u51fa\u7684LongCat-Flash-Lite\u6a21\u578b\u62e5\u6709685\u4ebf\u53c2\u6570\uff0c\u6fc0\u6d3b\u7ea630\u4ebf\u53c2\u6570\uff0c\u6027\u80fd\u4f18\u4e8e\u540c\u7c7bMoE\u57fa\u7ebf\uff0c\u5e76\u5728\u7279\u5b9a\u9886\u57df\u8868\u73b0\u5353\u8d8a\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Mixture-of-Experts (MoE) models are popular for making large language models more efficient, but they are hitting limits in performance.</li>\n    <li>This study looks at scaling embeddings as an alternative way to improve model efficiency.</li>\n    <li>Embedding scaling was found to perform better in certain conditions compared to expert scaling.</li>\n    <li>The researchers identified key architectural factors that affect how well embedding scaling works.</li>\n    <li>They developed a new model called LongCat-Flash-Lite, which has 68.5 billion parameters and performs better than similar models, especially in tasks involving agents and coding.</li>\n</ul>"}, "publishedAt": "2026-01-28T22:11:19.000Z", "title": "Scaling Embeddings Outperforms Scaling Experts in Language Models", "summary": "While Mixture-of-Experts (MoE) architectures have become the standard for sparsity scaling in large language models, they increasingly face diminishing returns and system-level bottlenecks. In this work, we explore embedding scaling as a potent, orthogonal dimension for scaling sparsity. Through a comprehensive analysis and experiments, we identify specific regimes where embedding scaling achieves a superior Pareto frontier compared to expert scaling. We systematically characterize the critical architectural factors governing this efficacy -- ranging from parameter budgeting to the interplay with model width and depth. Moreover, by integrating tailored system optimizations and speculative decoding, we effectively convert this sparsity into tangible inference speedups. Guided by these insights, we introduce LongCat-Flash-Lite, a 68.5B parameter model with ~3B activated trained from scratch. Despite allocating over 30B parameters to embeddings, LongCat-Flash-Lite not only surpasses parameter-equivalent MoE baselines but also exhibits exceptional competitiveness against existing models of comparable scale, particularly in agentic and coding domains.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.21204.png", "numComments": 2, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 221, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20540", "authors": [{"_id": "697ac48cdf3e800774f13bc1", "name": "Robbyant Team", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc2", "name": "Zelin Gao", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc3", "user": {"_id": "64981bea09cea550852652af", "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg", "isPro": false, "fullname": "Qiuyu Wang", "user": "qiuyuu", "type": "user"}, "name": "Qiuyu Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:16:29.190Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc4", "name": "Yanhong Zeng", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc5", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc6", "user": {"_id": "64acd2ec39fcfebff8c79c00", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64acd2ec39fcfebff8c79c00/Avq66l5hO-aggNtk4Y1ss.png", "isPro": false, "fullname": "Ka Leong Cheng", "user": "felixcheng97", "type": "user"}, "name": "Ka Leong Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T13:56:36.041Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc7", "name": "Yixuan Li", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc8", "user": {"_id": "665f059a8947302aa2c63afe", "avatarUrl": "/avatars/50f560285946532321a0bd526494148d.svg", "isPro": false, "fullname": "hanlin wang", "user": "hlwang06", "type": "user"}, "name": "Hanlin Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:18:10.952Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bc9", "name": "Yinghao Xu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bca", "name": "Shuailei Ma", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcb", "name": "Yihang Chen", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcc", "name": "Jie Liu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcd", "name": "Yansong Cheng", "hidden": false}, {"_id": "697ac48cdf3e800774f13bce", "name": "Yao Yao", "hidden": false}, {"_id": "697ac48cdf3e800774f13bcf", "name": "Jiayi Zhu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd0", "user": {"_id": "656084f44e8918182d4f07c8", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/akAvCUCi7eR31PWOXrVPw.jpeg", "isPro": false, "fullname": "Yihao Meng", "user": "Yhmeng1106", "type": "user"}, "name": "Yihao Meng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T13:56:37.840Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd1", "name": "Kecheng Zheng", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd2", "user": {"_id": "63f0baf66309c84d5f4a2226", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f0baf66309c84d5f4a2226/ihOgtwseRkfP1t-60IgyT.jpeg", "isPro": true, "fullname": "Qingyan", "user": "QingyanBai", "type": "user"}, "name": "Qingyan Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:16:24.535Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd3", "user": {"_id": "6478a982256b62e219917d67", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/PUJ-N2cQxgEmDGfyjajyA.jpeg", "isPro": false, "fullname": "JingyeChen22", "user": "JingyeChen22", "type": "user"}, "name": "Jingye Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:17:37.828Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd4", "name": "Zehong Shen", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd5", "user": {"_id": "662128ec9ca2cd4e6db2fb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662128ec9ca2cd4e6db2fb44/uUg1V-pVfxT3mLuFgJuAN.jpeg", "isPro": false, "fullname": "Bruce Yu", "user": "bruceyyu", "type": "user"}, "name": "Yue Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:16:27.115Z", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd6", "name": "Xing Zhu", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd7", "name": "Yujun Shen", "hidden": false}, {"_id": "697ac48cdf3e800774f13bd8", "name": "Hao Ouyang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64981bea09cea550852652af/HObcL400nFnYaw2kOcjor.mp4"], "publishedAt": "2026-01-28T12:37:01.000Z", "submittedOnDailyAt": "2026-01-29T00:09:39.166Z", "title": "Advancing Open-source World Models", "submittedOnDailyBy": {"_id": "64981bea09cea550852652af", "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg", "isPro": false, "fullname": "Qiuyu Wang", "user": "qiuyuu", "type": "user"}, "summary": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.", "upvotes": 66, "discussionId": "697ac48cdf3e800774f13bd9", "projectPage": "https://technology.robbyant.com/lingbot-world", "githubRepo": "https://github.com/Robbyant/lingbot-world/", "githubRepoAddedBy": "user", "ai_summary": "LingBot-World is an open-source world simulator with high-fidelity dynamics, long-term memory capabilities, and real-time interactivity for diverse environments.", "ai_keywords": ["world simulator", "video generation", "world model", "long-term memory", "real-time interactivity"], "githubStars": 756, "organization": {"_id": "69709f892cd08371c1011a2e", "name": "robbyant", "fullname": "Robbyant", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"}, "summary_zh": "<ul>\n    <li>LingBot-World \u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u4e16\u754c\u6a21\u62df\u5668\uff0c\u6e90\u81ea\u89c6\u9891\u751f\u6210\u6280\u672f\u3002</li>\n    <li>\u5b83\u5728\u591a\u79cd\u73af\u5883\u4e2d\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u548c\u5f3a\u5927\u7684\u52a8\u6001\u8868\u73b0\uff0c\u5305\u62ec\u73b0\u5b9e\u4e3b\u4e49\u3001\u79d1\u5b66\u80cc\u666f\u548c\u5361\u901a\u98ce\u683c\u7b49\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5177\u5907\u5206\u949f\u7ea7\u7684\u89c6\u91ce\uff0c\u5e76\u4fdd\u6301\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\uff0c\u5373\u201c\u957f\u671f\u8bb0\u5fc6\u201d\u3002</li>\n    <li>\u652f\u6301\u5b9e\u65f6\u4e92\u52a8\uff0c\u751f\u621016\u5e27\u6bcf\u79d2\u65f6\u5ef6\u8fdf\u4f4e\u4e8e1\u79d2\u3002</li>\n    <li>\u63d0\u4f9b\u4ee3\u7801\u548c\u6a21\u578b\u7684\u516c\u5f00\u8bbf\u95ee\uff0c\u4ee5\u4fc3\u8fdb\u5f00\u6e90\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5e94\u7528\u4e8e\u5185\u5bb9\u521b\u4f5c\u3001\u6e38\u620f\u548c\u673a\u5668\u4eba\u5b66\u4e60\u7b49\u9886\u57df\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LingBot-World is an open-source world simulator that focuses on video generation.</li>\n    <li>It features high-quality graphics and realistic dynamics across various environments, from realistic to cartoon styles.</li>\n    <li>The simulator has long-term memory, maintaining consistency in its context over time.</li>\n    <li>It allows for real-time interaction with a delay of less than 1 second while generating video at 16 frames per second.</li>\n    <li>The code and model are publicly available to support applications in content creation, gaming, and robot learning.</li>\n</ul>"}, "publishedAt": "2026-01-28T07:37:01.000Z", "title": "Advancing Open-source World Models", "summary": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/64981bea09cea550852652af/HObcL400nFnYaw2kOcjor.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20540.png", "numComments": 1, "submittedBy": {"_id": "64981bea09cea550852652af", "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg", "fullname": "Qiuyu Wang", "name": "qiuyuu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "69709f892cd08371c1011a2e", "name": "robbyant", "fullname": "Robbyant", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67aeffda7330db26f93cd62f/ZTuImney4XzRmBHyUL47F.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.18491", "authors": [{"_id": "697831d9026bdf0473116e5c", "name": "Dongrui Liu", "hidden": false}, {"_id": "697831d9026bdf0473116e5d", "user": {"_id": "66e2624a436a1798365e4581", "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg", "isPro": false, "fullname": "Qihan Ren", "user": "jasonrqh", "type": "user"}, "name": "Qihan Ren", "status": "claimed_verified", "statusLastChangedAt": "2026-01-28T11:31:15.765Z", "hidden": false}, {"_id": "697831d9026bdf0473116e5e", "name": "Chen Qian", "hidden": false}, {"_id": "697831d9026bdf0473116e5f", "name": "Shuai Shao", "hidden": false}, {"_id": "697831d9026bdf0473116e60", "name": "Yuejin Xie", "hidden": false}, {"_id": "697831d9026bdf0473116e61", "name": "Yu Li", "hidden": false}, {"_id": "697831d9026bdf0473116e62", "name": "Zhonghao Yang", "hidden": false}, {"_id": "697831d9026bdf0473116e63", "name": "Haoyu Luo", "hidden": false}, {"_id": "697831d9026bdf0473116e64", "name": "Peng Wang", "hidden": false}, {"_id": "697831d9026bdf0473116e65", "name": "Qingyu Liu", "hidden": false}, {"_id": "697831d9026bdf0473116e66", "name": "Binxin Hu", "hidden": false}, {"_id": "697831d9026bdf0473116e67", "name": "Ling Tang", "hidden": false}, {"_id": "697831d9026bdf0473116e68", "name": "Jilin Mei", "hidden": false}, {"_id": "697831d9026bdf0473116e69", "name": "Dadi Guo", "hidden": false}, {"_id": "697831d9026bdf0473116e6a", "name": "Leitao Yuan", "hidden": false}, {"_id": "697831d9026bdf0473116e6b", "name": "Junyao Yang", "hidden": false}, {"_id": "697831d9026bdf0473116e6c", "name": "Guanxu Chen", "hidden": false}, {"_id": "697831d9026bdf0473116e6d", "name": "Qihao Lin", "hidden": false}, {"_id": "697831d9026bdf0473116e6e", "name": "Yi Yu", "hidden": false}, {"_id": "697831d9026bdf0473116e6f", "name": "Bo Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e70", "name": "Jiaxuan Guo", "hidden": false}, {"_id": "697831d9026bdf0473116e71", "name": "Jie Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e72", "name": "Wenqi Shao", "hidden": false}, {"_id": "697831d9026bdf0473116e73", "name": "Huiqi Deng", "hidden": false}, {"_id": "697831d9026bdf0473116e74", "name": "Zhiheng Xi", "hidden": false}, {"_id": "697831d9026bdf0473116e75", "name": "Wenjie Wang", "hidden": false}, {"_id": "697831d9026bdf0473116e76", "name": "Wenxuan Wang", "hidden": false}, {"_id": "697831d9026bdf0473116e77", "name": "Wen Shen", "hidden": false}, {"_id": "697831d9026bdf0473116e78", "name": "Zhikai Chen", "hidden": false}, {"_id": "697831d9026bdf0473116e79", "name": "Haoyu Xie", "hidden": false}, {"_id": "697831d9026bdf0473116e7a", "name": "Jialing Tao", "hidden": false}, {"_id": "697831d9026bdf0473116e7b", "name": "Juntao Dai", "hidden": false}, {"_id": "697831d9026bdf0473116e7c", "name": "Jiaming Ji", "hidden": false}, {"_id": "697831d9026bdf0473116e7d", "name": "Zhongjie Ba", "hidden": false}, {"_id": "697831d9026bdf0473116e7e", "name": "Linfeng Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e7f", "name": "Yong Liu", "hidden": false}, {"_id": "697831d9026bdf0473116e80", "name": "Quanshi Zhang", "hidden": false}, {"_id": "697831d9026bdf0473116e81", "name": "Lei Zhu", "hidden": false}, {"_id": "697831d9026bdf0473116e82", "name": "Zhihua Wei", "hidden": false}, {"_id": "697831d9026bdf0473116e83", "name": "Hui Xue", "hidden": false}, {"_id": "697831d9026bdf0473116e84", "name": "Chaochao Lu", "hidden": false}, {"_id": "697831d9026bdf0473116e85", "name": "Jing Shao", "hidden": false}, {"_id": "697831d9026bdf0473116e86", "name": "Xia Hu", "hidden": false}], "publishedAt": "2026-01-26T13:45:41.000Z", "submittedOnDailyAt": "2026-01-28T01:26:49.833Z", "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security", "submittedOnDailyBy": {"_id": "66e2624a436a1798365e4581", "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg", "isPro": false, "fullname": "Qihan Ren", "user": "jasonrqh", "type": "user"}, "summary": "The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.", "upvotes": 60, "discussionId": "697831d9026bdf0473116e87", "githubRepo": "https://github.com/AI45Lab/AgentDoG", "githubRepoAddedBy": "user", "ai_summary": "AI agents face safety and security challenges from autonomous tool use and environmental interactions, requiring advanced guardrail frameworks for risk diagnosis and transparent monitoring.", "ai_keywords": ["agentic guardrail", "three-dimensional taxonomy", "agentic safety benchmark", "Diagnostic Guardrail framework", "agent safety and security", "agent trajectories", "root cause diagnosis", "fine-grained monitoring", "model variants", "state-of-the-art performance"], "githubStars": 178, "organization": {"_id": "68f716f832b31e42cbc2be7f", "name": "AI45Research", "fullname": "AI45Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68f6ffaa04d1019724af41fc/EVBafPHXvChszTM5tJcc9.png"}, "summary_zh": "<ul>\n    <li>\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u5d1b\u8d77\u5e26\u6765\u4e86\u590d\u6742\u7684\u5b89\u5168\u548c\u5b89\u5168\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u81ea\u4e3b\u5de5\u5177\u4f7f\u7528\u548c\u73af\u5883\u4e92\u52a8\u4e2d\u3002</li>\n    <li>\u76ee\u524d\u7684\u5b89\u5168\u6a21\u578b\u7f3a\u4e4f\u5bf9\u98ce\u9669\u7684\u8ba4\u77e5\u548c\u900f\u660e\u5ea6\uff0c\u65e0\u6cd5\u6709\u6548\u8bca\u65ad\u98ce\u9669\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u4e09\u7ef4\u5206\u7c7b\u6cd5\uff0c\u4ee5\u6765\u6e90\u3001\u5931\u8d25\u6a21\u5f0f\u548c\u540e\u679c\u6765\u5206\u7c7b\u4ee3\u7406\u98ce\u9669\u3002</li>\n    <li>\u57fa\u4e8e\u8be5\u5206\u7c7b\u6cd5\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684\u4ee3\u7406\u5b89\u5168\u57fa\u51c6\uff08ATBench\uff09\u548c\u8bca\u65ad\u5b89\u5168\u6846\u67b6\uff08AgentDoG\uff09\uff0c\u7528\u4e8e\u76d1\u63a7\u4ee3\u7406\u7684\u884c\u4e3a\u3002</li>\n    <li>AgentDoG\u80fd\u591f\u8bca\u65ad\u4e0d\u5b89\u5168\u884c\u4e3a\u7684\u6839\u672c\u539f\u56e0\uff0c\u63d0\u4f9b\u900f\u660e\u5ea6\uff0c\u5e76\u5728\u590d\u6742\u573a\u666f\u4e2d\u5c55\u793a\u51fa\u8272\u7684\u6027\u80fd\uff0c\u6240\u6709\u6a21\u578b\u548c\u6570\u636e\u96c6\u90fd\u662f\u516c\u5f00\u7684\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The use of AI agents brings new safety and security challenges due to their ability to act independently and interact with their environment.</li>\n    <li>Current safety models do not effectively recognize risks or explain them clearly.</li>\n    <li>A new three-dimensional system categorizes risks based on their source, failure mode, and consequences.</li>\n    <li>A new safety benchmark called ATBench and a framework called AgentDoG have been developed to improve monitoring and diagnosis of unsafe actions in AI agents.</li>\n    <li>AgentDoG is available in different sizes and has shown excellent performance in ensuring agent safety in various scenarios, with all models and data made publicly accessible.</li>\n</ul>"}, "publishedAt": "2026-01-26T08:45:41.000Z", "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security", "summary": "The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.18491.png", "numComments": 6, "submittedBy": {"_id": "66e2624a436a1798365e4581", "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg", "fullname": "Qihan Ren", "name": "jasonrqh", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "68f716f832b31e42cbc2be7f", "name": "AI45Research", "fullname": "AI45Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68f6ffaa04d1019724af41fc/EVBafPHXvChszTM5tJcc9.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.19325", "authors": [{"_id": "69798298df44b75fa47e47a9", "name": "Zichen Wen", "hidden": false}, {"_id": "69798298df44b75fa47e47aa", "user": {"_id": "688c72c011ef3399b561dee7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/688c72c011ef3399b561dee7/puhgnTOAfZYetsC46hqGm.jpeg", "isPro": false, "fullname": "BoxueYang", "user": "Boxue", "type": "user"}, "name": "Boxue Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-29T09:17:12.982Z", "hidden": false}, {"_id": "69798298df44b75fa47e47ab", "name": "Shuang Chen", "hidden": false}, {"_id": "69798298df44b75fa47e47ac", "name": "Yaojie Zhang", "hidden": false}, {"_id": "69798298df44b75fa47e47ad", "name": "Yuhang Han", "hidden": false}, {"_id": "69798298df44b75fa47e47ae", "name": "Junlong Ke", "hidden": false}, {"_id": "69798298df44b75fa47e47af", "name": "Cong Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47b0", "name": "Yicheng Fu", "hidden": false}, {"_id": "69798298df44b75fa47e47b1", "name": "Jiawang Zhao", "hidden": false}, {"_id": "69798298df44b75fa47e47b2", "name": "Jiangchao Yao", "hidden": false}, {"_id": "69798298df44b75fa47e47b3", "name": "Xi Fang", "hidden": false}, {"_id": "69798298df44b75fa47e47b4", "name": "Zhen Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47b5", "name": "Henxing Cai", "hidden": false}, {"_id": "69798298df44b75fa47e47b6", "name": "Lin Yao", "hidden": false}, {"_id": "69798298df44b75fa47e47b7", "name": "Zhifeng Gao", "hidden": false}, {"_id": "69798298df44b75fa47e47b8", "name": "Yanhui Hong", "hidden": false}, {"_id": "69798298df44b75fa47e47b9", "name": "Nang Yuan", "hidden": false}, {"_id": "69798298df44b75fa47e47ba", "name": "Yixuan Li", "hidden": false}, {"_id": "69798298df44b75fa47e47bb", "name": "Guojiang Zhao", "hidden": false}, {"_id": "69798298df44b75fa47e47bc", "name": "Haoyi Tao", "hidden": false}, {"_id": "69798298df44b75fa47e47bd", "name": "Nan Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47be", "name": "Han Lyu", "hidden": false}, {"_id": "69798298df44b75fa47e47bf", "name": "Guolin Ke", "hidden": false}, {"_id": "69798298df44b75fa47e47c0", "name": "Ning Liao", "hidden": false}, {"_id": "69798298df44b75fa47e47c1", "name": "Xiaoxing Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47c2", "name": "Kai Chen", "hidden": false}, {"_id": "69798298df44b75fa47e47c3", "name": "Zhiyu Li", "hidden": false}, {"_id": "69798298df44b75fa47e47c4", "name": "Feiyu Xiong", "hidden": false}, {"_id": "69798298df44b75fa47e47c5", "name": "Sihan Hu", "hidden": false}, {"_id": "69798298df44b75fa47e47c6", "name": "Kun Chen", "hidden": false}, {"_id": "69798298df44b75fa47e47c7", "name": "Yanfeng Wang", "hidden": false}, {"_id": "69798298df44b75fa47e47c8", "name": "Weinan E", "hidden": false}, {"_id": "69798298df44b75fa47e47c9", "name": "Linfeng Zhang", "hidden": false}, {"_id": "69798298df44b75fa47e47ca", "name": "Linfeng Zhang", "hidden": false}], "publishedAt": "2026-01-27T08:12:18.000Z", "submittedOnDailyAt": "2026-01-29T01:20:58.570Z", "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery", "submittedOnDailyBy": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "isPro": false, "fullname": "Zichen Wen", "user": "zichenwen", "type": "user"}, "summary": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.", "upvotes": 53, "discussionId": "69798298df44b75fa47e47cb", "projectPage": "https://innovatorlm.github.io/Innovator-VL", "githubRepo": "https://github.com/InnovatorLM/Innovator-VL", "githubRepoAddedBy": "user", "ai_summary": "Innovator-VL demonstrates that principled training design and transparent methodology can achieve strong scientific intelligence with reduced data requirements while maintaining general vision performance.", "ai_keywords": ["multimodal large language model", "scientific multimodal large language model", "end-to-end reproducible training pipeline", "supervised fine-tuning", "reinforcement learning", "scientific reasoning", "data efficiency", "principled data selection", "generalization", "scientific alignment"], "githubStars": 70, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86Innovator-VL\uff0c\u4e00\u4e2a\u65e8\u5728\u63d0\u9ad8\u4e0d\u540c\u79d1\u5b66\u9886\u57df\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u7684\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u900f\u660e\u7684\u65b9\u6cd5\u548c\u539f\u5219\u6027\u7684\u8bad\u7ec3\u8bbe\u8ba1\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6570\u636e\u9700\u6c42\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u79d1\u5b66\u667a\u80fd\u3002</li>\n    <li>\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u5168\u900f\u660e\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5305\u62ec\u6570\u636e\u6536\u96c6\u3001\u6e05\u6d17\u3001\u9884\u5904\u7406\u548c\u8bc4\u4f30\uff0c\u65b9\u4fbf\u793e\u533a\u8fdb\u884c\u7cfb\u7edf\u6269\u5c55\u3002</li>\n    <li>Innovator-VL\u5728\u79d1\u5b66\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f7f\u7528\u4e0d\u5230\u4e94\u767e\u4e07\u4e2a\u6837\u672c\u800c\u6ca1\u6709\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u5c55\u793a\u4e86\u6570\u636e\u9009\u62e9\u7684\u91cd\u8981\u6027\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u901a\u7528\u89c6\u89c9\u548c\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u4e5f\u8868\u73b0\u4f18\u5f02\uff0c\u8868\u660e\u79d1\u5b66\u5bf9\u9f50\u53ef\u4ee5\u4e0e\u901a\u7528\u80fd\u529b\u7ed3\u5408\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Innovator-VL is a new large language model that helps with understanding and reasoning in science and also performs well on general vision tasks.</li>\n    <li>The model uses a clear and reproducible training process, making it easier for other researchers to extend and improve upon.</li>\n    <li>It achieves good results on scientific tasks with fewer than five million curated samples, showing that smart data selection is more effective than just using large amounts of data.</li>\n    <li>Innovator-VL can perform well across different areas, including general vision and multimodal reasoning, without losing its general capabilities.</li>\n    <li>This approach offers a practical way to create efficient scientific models without needing massive datasets, encouraging future research in this area.</li>\n</ul>"}, "publishedAt": "2026-01-27T03:12:18.000Z", "title": "Innovator-VL: A Multimodal Large Language Model for Scientific Discovery", "summary": "We present Innovator-VL, a scientific multimodal large language model designed to advance understanding and reasoning across diverse scientific domains while maintaining excellent performance on general vision tasks. Contrary to the trend of relying on massive domain-specific pretraining and opaque pipelines, our work demonstrates that principled training design and transparent methodology can yield strong scientific intelligence with substantially reduced data requirements. (i) First, we provide a fully transparent, end-to-end reproducible training pipeline, covering data collection, cleaning, preprocessing, supervised fine-tuning, reinforcement learning, and evaluation, along with detailed optimization recipes. This facilitates systematic extension by the community. (ii) Second, Innovator-VL exhibits remarkable data efficiency, achieving competitive performance on various scientific tasks using fewer than five million curated samples without large-scale pretraining. These results highlight that effective reasoning can be achieved through principled data selection rather than indiscriminate scaling. (iii) Third, Innovator-VL demonstrates strong generalization, achieving competitive performance on general vision, multimodal reasoning, and scientific benchmarks. This indicates that scientific alignment can be integrated into a unified model without compromising general-purpose capabilities. Our practices suggest that efficient, reproducible, and high-performing scientific multimodal models can be built even without large-scale data, providing a practical foundation for future research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.19325.png", "numComments": 1, "submittedBy": {"_id": "653b8c3e97a4d71d950e2f20", "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg", "fullname": "Zichen Wen", "name": "zichenwen", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.17737", "authors": [{"_id": "6978310b026bdf0473116e44", "user": {"_id": "64545c77a7ce0a8fde809912", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VDaMEM77Xv09dP6B5v3sK.jpeg", "isPro": false, "fullname": "ChenYuMu", "user": "ChenYuMu", "type": "user"}, "name": "Chenyu Mu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T14:01:21.462Z", "hidden": false}, {"_id": "6978310b026bdf0473116e45", "user": {"_id": "6527a2df1eb78901534b0cc6", "avatarUrl": "/avatars/f811d8c108930b41e2612c609d35e2eb.svg", "isPro": false, "fullname": "Xin He", "user": "Kleinhe", "type": "user"}, "name": "Xin He", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:03:20.414Z", "hidden": false}, {"_id": "6978310b026bdf0473116e46", "user": {"_id": "64300415b009240418dac70c", "avatarUrl": "/avatars/5175cdbc7683b0b52d5c742e93d3be83.svg", "isPro": false, "fullname": "Qu Yang", "user": "quyang22", "type": "user"}, "name": "Qu Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-27T09:03:22.475Z", "hidden": false}, {"_id": "6978310b026bdf0473116e47", "name": "Wanshun Chen", "hidden": false}, {"_id": "6978310b026bdf0473116e48", "name": "Jiadi Yao", "hidden": false}, {"_id": "6978310b026bdf0473116e49", "name": "Huang Liu", "hidden": false}, {"_id": "6978310b026bdf0473116e4a", "name": "Zihao Yi", "hidden": false}, {"_id": "6978310b026bdf0473116e4b", "name": "Bo Zhao", "hidden": false}, {"_id": "6978310b026bdf0473116e4c", "name": "Xingyu Chen", "hidden": false}, {"_id": "6978310b026bdf0473116e4d", "name": "Ruotian Ma", "hidden": false}, {"_id": "6978310b026bdf0473116e4e", "name": "Fanghua Ye", "hidden": false}, {"_id": "6978310b026bdf0473116e4f", "name": "Erkun Yang", "hidden": false}, {"_id": "6978310b026bdf0473116e50", "name": "Cheng Deng", "hidden": false}, {"_id": "6978310b026bdf0473116e51", "name": "Zhaopeng Tu", "hidden": false}, {"_id": "6978310b026bdf0473116e52", "name": "Xiaolong Li", "hidden": false}, {"_id": "6978310b026bdf0473116e53", "name": "Linus", "hidden": false}], "publishedAt": "2026-01-25T08:10:28.000Z", "submittedOnDailyAt": "2026-01-27T01:05:46.612Z", "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation", "submittedOnDailyBy": {"_id": "67485743561b1e6f9579389f", "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg", "isPro": false, "fullname": "Zhaopeng Tu", "user": "zptu", "type": "user"}, "summary": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.", "upvotes": 46, "discussionId": "6978310b026bdf0473116e54", "projectPage": "https://xd-mu.github.io/ScriptIsAllYouNeed/", "githubRepo": "https://github.com/Tencent/digitalhuman/tree/main/ScriptAgent", "githubRepoAddedBy": "user", "ai_summary": "A novel end-to-end agentic framework translates dialogue into cinematic videos through specialized agents that generate and orchestrate video content while maintaining narrative coherence.", "ai_keywords": ["video generation", "dialogue-to-cinematic-video", "ScripterAgent", "DirectorAgent", "cross-scene continuous generation", "ScriptBench", "Visual-Script Alignment", "CriticAgent"], "githubStars": 228, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u8fdb\u5c55\u4f7f\u5f97\u89c6\u9891\u751f\u6210\u6a21\u578b\u80fd\u591f\u6839\u636e\u7b80\u5355\u7684\u6587\u672c\u63d0\u793a\u5408\u6210\u51fa\u60ca\u8273\u7684\u89c6\u89c9\u5185\u5bb9\u3002</li>\n    <li>\u8fd9\u4e9b\u6a21\u578b\u5728\u751f\u6210\u8fde\u8d2f\u7684\u957f\u7bc7\u53d9\u4e8b\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u5bf9\u8bdd\u7b49\u9ad8\u7ea7\u6982\u5ff5\u7684\u8868\u73b0\u4e0a\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u8bdd\u5230\u7535\u5f71\u89c6\u9891\u751f\u6210\u7684\u6846\u67b6\uff0c\u540d\u4e3aScripterAgent\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6ScriptBench\uff0c\u5e2e\u52a9\u5c06\u7c97\u7565\u5bf9\u8bdd\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u7535\u5f71\u5267\u672c\u3002</li>\n    <li>\u6211\u4eec\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u5267\u672c\u7684\u5fe0\u5b9e\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u8fd8\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u89c6\u89c9\u6548\u679c\u4e0e\u5267\u672c\u9075\u5faa\u4e4b\u95f4\u7684\u6743\u8861\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>New models can create impressive videos from text, but they struggle with making long, coherent stories.</li>\n    <li>To solve this, a new framework called ScripterAgent helps turn basic dialogue into detailed scripts for videos.</li>\n    <li>ScriptBench is a new benchmark that provides helpful context for creating these scripts, using expert input.</li>\n    <li>DirectorAgent uses the scripts to manage video creation, ensuring that the storyline stays consistent over time.</li>\n    <li>Tests show that this new approach improves how closely scripts match the final videos and helps balance visual appeal with story accuracy.</li>\n</ul>"}, "publishedAt": "2026-01-25T03:10:28.000Z", "title": "The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation", "summary": "Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17737.png", "numComments": 3, "submittedBy": {"_id": "67485743561b1e6f9579389f", "avatarUrl": "/avatars/8a4cc63bd7be388010bc329bb74582a1.svg", "fullname": "Zhaopeng Tu", "name": "zptu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "6645f953c39288df638dbdd5", "name": "Tencent-Hunyuan", "fullname": "Tencent Hunyuan", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/62d22496c58f969c152bcefd/woKSjt2wXvBNKussyYPsa.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.22153", "authors": [{"_id": "697c2899a67238fac88cc115", "user": {"_id": "63f47b5321eb234ab739e91a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg", "isPro": false, "fullname": "Haozhe Xie", "user": "hzxie", "type": "user"}, "name": "Haozhe Xie", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T13:31:48.996Z", "hidden": false}, {"_id": "697c2899a67238fac88cc116", "user": {"_id": "672392c4a4c4381cefc06416", "avatarUrl": "/avatars/8ee84a7e3e91e5d13074bc3c407ff75d.svg", "isPro": false, "fullname": "Wen Beichen", "user": "wenbc21", "type": "user"}, "name": "Beichen Wen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T13:31:52.487Z", "hidden": false}, {"_id": "697c2899a67238fac88cc117", "user": {"_id": "6899ff3f4c5ca50a326bb456", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Nuqof2ofdaQUD5b07cDnG.png", "isPro": false, "fullname": "Zheng Jiarui", "user": "zghtyarecrenj", "type": "user"}, "name": "Jiarui Zheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-30T13:31:46.030Z", "hidden": false}, {"_id": "697c2899a67238fac88cc118", "name": "Zhaoxi Chen", "hidden": false}, {"_id": "697c2899a67238fac88cc119", "name": "Fangzhou Hong", "hidden": false}, {"_id": "697c2899a67238fac88cc11a", "name": "Haiwen Diao", "hidden": false}, {"_id": "697c2899a67238fac88cc11b", "name": "Ziwei Liu", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/p9cPxETttQUS23woFb14M.mp4"], "publishedAt": "2026-01-29T18:59:51.000Z", "submittedOnDailyAt": "2026-01-30T01:46:39.673Z", "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation", "submittedOnDailyBy": {"_id": "63f47b5321eb234ab739e91a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg", "isPro": false, "fullname": "Haozhe Xie", "user": "hzxie", "type": "user"}, "summary": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.", "upvotes": 45, "discussionId": "697c2899a67238fac88cc11c", "projectPage": "https://haozhexie.com/project/dynamic-vla", "githubRepo": "https://github.com/hzxie/DynamicVLA", "githubRepoAddedBy": "user", "ai_summary": "DynamicVLA addresses dynamic object manipulation challenges through a compact vision-language-action model with temporal reasoning and closed-loop adaptation, supported by a new benchmark for dynamic manipulation tasks.", "ai_keywords": ["Vision-Language-Action models", "temporal reasoning", "closed-loop adaptation", "convolutional vision encoder", "multimodal inference", "Continuous Inference", "Latent-aware Action Streaming", "Dynamic Object Manipulation benchmark", "synthetic episodes", "real-world episodes"], "githubStars": 48, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "summary_zh": "<ul>\n    <li>\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\u662fVision-Language-Action (VLA)\u6a21\u578b\u9762\u4e34\u7684\u6311\u6218\uff0c\u5c24\u5176\u5728\u5feb\u901f\u611f\u77e5\u548c\u63a7\u5236\u65b9\u9762\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86DynamicVLA\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u65f6\u95f4\u63a8\u7406\u548c\u95ed\u73af\u9002\u5e94\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5177\u6709\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a\u7d27\u51d1\u76840.4B VLA\u3001\u8fde\u7eed\u63a8\u7406\u548c\u6f5c\u5728\u610f\u8bc6\u7684\u52a8\u4f5c\u6d41\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\uff08DOM\uff09\u57fa\u51c6\uff0c\u6536\u96c6\u4e86200K\u4e2a\u5408\u6210\u573a\u666f\u548c2K\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u64cd\u4f5c\u6570\u636e\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cDynamicVLA\u5728\u53cd\u5e94\u901f\u5ea6\u3001\u611f\u77e5\u548c\u6cdb\u5316\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>DynamicVLA is a new framework designed to help machines manipulate moving objects better than previous models.</li>\n    <li>It uses a small but effective model that quickly analyzes visual information and makes decisions.</li>\n    <li>The framework allows for faster reasoning and actions, enabling machines to adapt quickly to changes in object movement.</li>\n    <li>A new benchmark called Dynamic Object Manipulation (DOM) has been created to test this framework, featuring a large collection of synthetic and real-world data.</li>\n    <li>Tests show that DynamicVLA significantly improves speed, perception, and the ability to generalize across different situations.</li>\n</ul>"}, "publishedAt": "2026-01-29T13:59:51.000Z", "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation", "summary": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/63f47b5321eb234ab739e91a/p9cPxETttQUS23woFb14M.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.22153.png", "numComments": 2, "submittedBy": {"_id": "63f47b5321eb234ab739e91a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f47b5321eb234ab739e91a/vWfFNVtMkHl8gieha5PPd.jpeg", "fullname": "Haozhe Xie", "name": "hzxie", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 19, "isUserFollowing": false}, "organization": {"_id": "62d55f243bf5e059f7ca25ba", "name": "mmlab-ntu", "fullname": "MMLab@NTU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1658151991971-62b5777f593a2c49da69dc02.png"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u5728\u89c6\u9891\u95ee\u7b54\u4e2d\uff0c\u89c6\u9891\u901a\u5e38\u53ea\u63d0\u4f9b\u5c40\u90e8\u89c6\u89c9\u7ebf\u7d22\uff0c\u800c\u7b54\u6848\u5206\u6563\u5728\u7f51\u7edc\u4e0a\u3002</li>\n    <li>\u6211\u4eec\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0c\u540d\u4e3aVideoDR\uff0c\u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u7684\u5f00\u653e\u9886\u57df\u95ee\u7b54\u3002</li>\n    <li>VideoDR\u9700\u8981\u63d0\u53d6\u8de8\u5e27\u89c6\u89c9\u951a\u70b9\u3001\u8fdb\u884c\u4e92\u52a8\u7f51\u9875\u68c0\u7d22\u548c\u591a\u8df3\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u5bf9\u591a\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793aAgentic\u65b9\u6cd5\u4e0d\u603b\u662f\u4f18\u4e8eWorkflow\u65b9\u6cd5\u3002</li>\n    <li>VideoDR\u4e3a\u7814\u7a76\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u89c6\u9891\u4ee3\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u672a\u6765\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u7684\u5173\u952e\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video question answering often needs to find answers from both videos and the internet, requiring complex reasoning and retrieval.</li>\n    <li>The new VideoDR benchmark focuses on answering questions using videos and online resources together.</li>\n    <li>It includes high-quality video samples across six different topics, created with careful human review.</li>\n    <li>Two approaches for models, Workflow and Agentic, were tested, showing that success depends on how well models keep track of video clues during searches.</li>\n    <li>VideoDR helps identify challenges for future video research projects involving online information.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u4eba\u7c7b\u5728\u83b7\u5f97\u8bed\u8a00\u80fd\u529b\u4e4b\u524d\uff0c\u65e9\u5df2\u53d1\u5c55\u51fa\u57fa\u672c\u7684\u89c6\u89c9\u6280\u80fd\uff0c\u4f46\u73b0\u4ee3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4ecd\u4f9d\u8d56\u4e8e\u8bed\u8a00\u77e5\u8bc6\u6765\u5f25\u8865\u89c6\u89c9\u7406\u89e3\u7684\u4e0d\u8db3\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684MLLMs\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u4eba\u7c7b\uff08\u751a\u81f33\u5c81\u513f\u7ae5\uff09\u5219\u80fd\u8f7b\u677e\u89e3\u51b3\u8fd9\u4e9b\u4efb\u52a1\u3002</li>\n    <li>\u4e3a\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u8fd9\u4e00\u5dee\u8ddd\uff0c\u63d0\u51fa\u4e86BabyVision\u57fa\u51c6\uff0c\u4e13\u95e8\u8bc4\u4f30\u4e0d\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u7684\u6838\u5fc3\u89c6\u89c9\u80fd\u529b\uff0c\u5171\u6709388\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\u548c\u56db\u4e2a\u4e3b\u8981\u7c7b\u522b\u3002</li>\n    <li>\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u9886\u5148\u7684MLLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c6\uff0cGemini3-Pro-Preview\u7684\u8bc4\u5206\u4e3a49.7\uff0c\u8fdc\u4f4e\u4e8e6\u5c81\u513f\u7ae5\u548c94.1\u7684\u6210\u5e74\u4eba\u5e73\u5747\u5206\u3002</li>\n    <li>BabyVision\u7684\u8fdb\u5c55\u6807\u5fd7\u7740\u5411\u4eba\u7c7b\u7ea7\u522b\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u7684\u8fc8\u8fdb\uff0c\u5e76\u63d0\u51fa\u4e86BabyVision-Gen\u548c\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\u5305\u6765\u89e3\u51b3\u89c6\u89c9\u63a8\u7406\u95ee\u9898\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Current Multimodal LLMs (MLLMs) struggle with basic visual tasks that even young children can do easily.</li>\n    <li>BabyVision is a new benchmark created to test visual skills in MLLMs without relying on language.</li>\n    <li>It includes 388 tasks organized into 22 subclasses and four main categories to assess visual abilities.</li>\n    <li>Tests show that MLLMs score much lower than humans, with the best model scoring only 49.7 compared to a human average of 94.1.</li>\n    <li>BabyVision aims to improve MLLMs' visual understanding and includes a new tool, BabyVision-Gen, for evaluating visual reasoning.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u8bb8\u591a\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\uff0c\u5206\u5272\u8fd9\u4e9b\u5b9e\u4f53\u5bf9\u540e\u7eed\u5e94\u7528\u5f88\u91cd\u8981\u3002</li>\n    <li>\u5f53\u524d\u7684\u5206\u5272\u6a21\u578b\u53ef\u4ee5\u5904\u7406\u7269\u7406\u5c5e\u6027\u7684\u5b9e\u4f53\uff0c\u4f46\u5bf9\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\u4ecd\u7136\u6709\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6SocioSeg\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86SocioReasoner\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u793e\u4f1a\u8bed\u4e49\u5206\u5272\uff0c\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u6807\u6ce8\u7684\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u6211\u4eec\u7684\u65b9\u6cd5\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5e76\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban areas have many different types of features, and it's important to identify them using satellite images.</li>\n    <li>Current models can easily recognize physical features like buildings and water but have trouble with social features like schools and parks.</li>\n    <li>We created a new dataset called SocioSeg that includes satellite images, maps, and detailed labels for social features.</li>\n    <li>We developed a new framework called SocioReasoner that helps the model understand and identify social features more like humans do.</li>\n    <li>Our experiments show that this new approach outperforms existing models and can generalize well to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.16725", "authors": [{"_id": "6976d5405d41524304c13537", "name": "Meituan LongCat Team", "hidden": false}, {"_id": "6976d5405d41524304c13538", "name": "Anchun Gui", "hidden": false}, {"_id": "6976d5405d41524304c13539", "name": "Bei Li", "hidden": false}, {"_id": "6976d5405d41524304c1353a", "name": "Bingyang Tao", "hidden": false}, {"_id": "6976d5405d41524304c1353b", "name": "Bole Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1353c", "name": "Borun Chen", "hidden": false}, {"_id": "6976d5405d41524304c1353e", "name": "Chao Zhang", "hidden": false}, {"_id": "69772bc15d41524304c13739", "name": "Chao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1353f", "name": "Chen Gao", "hidden": false}, {"_id": "6976d5405d41524304c13540", "name": "Chen Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13541", "name": "Chengcheng Han", "hidden": false}, {"_id": "6976d5405d41524304c13542", "name": "Chenhui Yang", "hidden": false}, {"_id": "6976d5405d41524304c13543", "name": "Chuyu Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13544", "name": "Cong Chen", "hidden": false}, {"_id": "6976d5405d41524304c13545", "name": "Cunguang Wang", "hidden": false}, {"_id": "6976d5405d41524304c13546", "name": "Daoru Pan", "hidden": false}, {"_id": "6976d5405d41524304c13547", "name": "Defei Bu", "hidden": false}, {"_id": "6976d5405d41524304c13548", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13549", "name": "Di Xiu", "hidden": false}, {"_id": "6976d5405d41524304c1354a", "name": "Dishan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1354b", "name": "Dongyu Ru", "hidden": false}, {"_id": "6976d5405d41524304c1354c", "name": "Dunwei Tu", "hidden": false}, {"_id": "6976d5405d41524304c1354d", "name": "Fan Wu", "hidden": false}, {"_id": "6976d5405d41524304c1354e", "name": "Fengcheng Yuan", "hidden": false}, {"_id": "6976d5405d41524304c1354f", "name": "Fengcun Li", "hidden": false}, {"_id": "6976d5405d41524304c13550", "name": "Gang Xu", "hidden": false}, {"_id": "6976d5405d41524304c13551", "name": "Guanyu Wu", "hidden": false}, {"_id": "6976d5405d41524304c13552", "name": "Guoyuan Lin", "hidden": false}, {"_id": "6976d5405d41524304c13553", "name": "Haibin Wang", "hidden": false}, {"_id": "6976d5405d41524304c13554", "name": "Hansi Yang", "hidden": false}, {"_id": "6976d5405d41524304c13555", "name": "Hao Yang", "hidden": false}, {"_id": "6976d5405d41524304c13556", "name": "Haonan Yan", "hidden": false}, {"_id": "6976d5405d41524304c13557", "name": "Haoxiang Ma", "hidden": false}, {"_id": "6976d5405d41524304c13558", "name": "Haoxing Wen", "hidden": false}, {"_id": "6976d5405d41524304c13559", "name": "Hongyan Hao", "hidden": false}, {"_id": "6976d5405d41524304c1355a", "name": "Hongyin Tang", "hidden": false}, {"_id": "6976d5405d41524304c1355b", "name": "Hongyu Zang", "hidden": false}, {"_id": "6976d5405d41524304c1355c", "name": "Hongzhi Ni", "hidden": false}, {"_id": "6976d5405d41524304c1355d", "name": "Hui Su", "hidden": false}, {"_id": "6976d5405d41524304c1355e", "name": "Jiacheng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1355f", "name": "Jiahong Zhou", "hidden": false}, {"_id": "6976d5405d41524304c13560", "name": "Jiahuan Li", "hidden": false}, {"_id": "6976d5405d41524304c13561", "name": "Jiaming Wang", "hidden": false}, {"_id": "6976d5405d41524304c13562", "name": "Jian Yang", "hidden": false}, {"_id": "6976d5405d41524304c13563", "user": {"_id": "64008a0af4ff62c2616d8858", "avatarUrl": "/avatars/b52c98857916fba5377ace8089d658b2.svg", "isPro": false, "fullname": "zhangjf", "user": "zhangjf", "type": "user"}, "name": "Jianfei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:09.272Z", "hidden": false}, {"_id": "6976d5405d41524304c13564", "name": "Jianhao Xu", "hidden": false}, {"_id": "6976d5405d41524304c13565", "name": "Jianing Wang", "hidden": false}, {"_id": "6976d5405d41524304c13566", "name": "Jiapeng Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13567", "name": "Jiaqi Sun", "hidden": false}, {"_id": "6976d5405d41524304c13568", "name": "Jiarong Shi", "hidden": false}, {"_id": "6976d5405d41524304c13569", "name": "Jiarui Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1356a", "name": "Jingang Wang", "hidden": false}, {"_id": "6976d5405d41524304c1356b", "user": {"_id": "6592472fccbc1e2cc7250903", "avatarUrl": "/avatars/6f04ae66944eb2ce65c5aca7927bab10.svg", "isPro": false, "fullname": "Jinluan Yang", "user": "Jinluan", "type": "user"}, "name": "Jinluan Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T08:28:47.175Z", "hidden": false}, {"_id": "6976d5405d41524304c1356c", "name": "Jinrui Ding", "hidden": false}, {"_id": "6976d5405d41524304c1356d", "name": "Jinwei Xiao", "hidden": false}, {"_id": "6976d5405d41524304c1356e", "name": "Jiyuan He", "hidden": false}, {"_id": "6976d5405d41524304c1356f", "name": "Juncan Xu", "hidden": false}, {"_id": "6976d5405d41524304c13570", "name": "Kefeng Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13571", "name": "Keheng Wang", "hidden": false}, {"_id": "6976d5405d41524304c13572", "name": "Li Wei", "hidden": false}, {"_id": "6976d5405d41524304c13573", "name": "Lianhui Ma", "hidden": false}, {"_id": "6976d5405d41524304c13574", "name": "Lin Qiu", "hidden": false}, {"_id": "6976d5405d41524304c13575", "name": "Lingbing Kong", "hidden": false}, {"_id": "6976d5405d41524304c13576", "name": "Lingchuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c13577", "name": "Linsen Guo", "hidden": false}, {"_id": "6976d5405d41524304c13578", "name": "Mengshen Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13579", "name": "Mengxia Shen", "hidden": false}, {"_id": "6976d5405d41524304c1357a", "name": "Mingyang Zhu", "hidden": false}, {"_id": "6976d5405d41524304c1357b", "name": "Peiguang Li", "hidden": false}, {"_id": "6976d5405d41524304c1357c", "name": "Peng Pei", "hidden": false}, {"_id": "6976d5405d41524304c1357d", "name": "Pengcheng Jia", "hidden": false}, {"_id": "6976d5405d41524304c1357e", "name": "Pengtao Zhang", "hidden": false}, {"_id": "6976d5405d41524304c1357f", "name": "Peng Zhao", "hidden": false}, {"_id": "6976d5405d41524304c13580", "name": "Qi Gu", "hidden": false}, {"_id": "6976d5405d41524304c13581", "name": "Qiong Huang", "hidden": false}, {"_id": "6976d5405d41524304c13582", "name": "Qiyuan Duan", "hidden": false}, {"_id": "6976d5405d41524304c13583", "name": "Quanchi Weng", "hidden": false}, {"_id": "6976d5405d41524304c13584", "name": "Rongxiang Weng", "hidden": false}, {"_id": "6976d5405d41524304c13585", "name": "Rongzhi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c13586", "name": "Rumei Li", "hidden": false}, {"_id": "6976d5405d41524304c13587", "name": "Shanglin Lei", "hidden": false}, {"_id": "6976d5405d41524304c13588", "user": {"_id": "64db5f5dd68a6ddcc7bd89e9", "avatarUrl": "/avatars/69375ec915927b855813df8a6d486837.svg", "isPro": false, "fullname": "Shengnan An", "user": "ShengnanAn", "type": "user"}, "name": "Shengnan An", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:11.410Z", "hidden": false}, {"_id": "6976d5405d41524304c13589", "name": "Shijun Dai", "hidden": false}, {"_id": "6976d5405d41524304c1358a", "name": "Shuaikang Liu", "hidden": false}, {"_id": "6976d5405d41524304c1358b", "name": "Shuang Zhou", "hidden": false}, {"_id": "6976d5405d41524304c1358c", "name": "Shuo Wang", "hidden": false}, {"_id": "6976d5405d41524304c1358d", "name": "Songyuan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c1358e", "name": "Tao Liang", "hidden": false}, {"_id": "6976d5405d41524304c1358f", "name": "Tianhao Hu", "hidden": false}, {"_id": "6976d5405d41524304c13590", "name": "Tianze Chen", "hidden": false}, {"_id": "6976d5405d41524304c13591", "name": "Wei Liu", "hidden": false}, {"_id": "6976d5405d41524304c13592", "name": "Wei Shi", "hidden": false}, {"_id": "6976d5405d41524304c13593", "name": "Wei Wang", "hidden": false}, {"_id": "6976d5405d41524304c13594", "name": "Weifeng Tang", "hidden": false}, {"_id": "6976d5405d41524304c13595", "name": "Wenjie Shi", "hidden": false}, {"_id": "6976d5405d41524304c13596", "name": "Wenlong Zhu", "hidden": false}, {"_id": "6976d5405d41524304c13597", "name": "Wentao Chen", "hidden": false}, {"_id": "6976d5405d41524304c13598", "name": "Wentao Shi", "hidden": false}, {"_id": "6976d5405d41524304c13599", "name": "Xi Su", "hidden": false}, {"_id": "6976d5405d41524304c1359a", "name": "Xiangcheng Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359b", "name": "Xiandi Ma", "hidden": false}, {"_id": "6976d5405d41524304c1359c", "user": {"_id": "63edb098679c2cc40abc6c2e", "avatarUrl": "/avatars/288c7229937c2c3f29fda6d17c7df2eb.svg", "isPro": false, "fullname": "Xiangyu", "user": "xixy", "type": "user"}, "name": "Xiangyu Xi", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:13.312Z", "hidden": false}, {"_id": "6976d5405d41524304c1359d", "name": "Xiangyuan Liu", "hidden": false}, {"_id": "6976d5405d41524304c1359e", "name": "Xiangzhou Huang", "hidden": false}, {"_id": "6976d5405d41524304c1359f", "name": "Xiao Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a0", "name": "Xiaodong Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a1", "name": "Xiaolong Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a2", "name": "Xiaowei Shi", "hidden": false}, {"_id": "6976d5405d41524304c135a3", "name": "Xiaoyu Li", "hidden": false}, {"_id": "6976d5405d41524304c135a4", "name": "Xin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135a5", "name": "Xingchen Liu", "hidden": false}, {"_id": "6976d5405d41524304c135a6", "name": "Xuan Huang", "hidden": false}, {"_id": "6976d5405d41524304c135a7", "name": "Xuezhi Cao", "hidden": false}, {"_id": "6976d5405d41524304c135a8", "name": "Xunliang Cai", "hidden": false}, {"_id": "6976d5405d41524304c135a9", "name": "Yan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135aa", "user": {"_id": "63fc1c420aab06079200c15c", "avatarUrl": "/avatars/8e8e82a9a6552848581ca9f65011263c.svg", "isPro": false, "fullname": "yang bai", "user": "byang", "type": "user"}, "name": "Yang Bai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-26T09:09:07.036Z", "hidden": false}, {"_id": "6976d5405d41524304c135ab", "name": "Yang Liu", "hidden": false}, {"_id": "6976d5405d41524304c135ac", "name": "Yang Yang", "hidden": false}, {"_id": "6976d5405d41524304c135ad", "name": "Yang Zheng", "hidden": false}, {"_id": "6976d5405d41524304c135ae", "name": "Yaoming Wang", "hidden": false}, {"_id": "6976d5405d41524304c135af", "name": "Yaoming Zhu", "hidden": false}, {"_id": "6976d5405d41524304c135b0", "name": "Yaqi Huo", "hidden": false}, {"_id": "6976d5405d41524304c135b1", "name": "Yanyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b2", "name": "Yaorui Shi", "hidden": false}, {"_id": "6976d5405d41524304c135b3", "name": "Yerui Sun", "hidden": false}, {"_id": "6976d5405d41524304c135b4", "name": "Yi Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b5", "name": "Yihao Chen", "hidden": false}, {"_id": "6976d5405d41524304c135b6", "name": "Yi-Kai Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135b7", "name": "Yifan Lu", "hidden": false}, {"_id": "6976d5405d41524304c135b8", "name": "Yifan Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135b9", "name": "Yitao Zhai", "hidden": false}, {"_id": "6976d5405d41524304c135ba", "name": "Yongjing Yin", "hidden": false}, {"_id": "6976d5405d41524304c135bb", "name": "Yongwei Zhou", "hidden": false}, {"_id": "6976d5405d41524304c135bc", "name": "Youshao Xiao", "hidden": false}, {"_id": "6976d5405d41524304c135bd", "name": "Yuchuan Dai", "hidden": false}, {"_id": "6976d5405d41524304c135be", "name": "Yuchen Xie", "hidden": false}, {"_id": "6976d5405d41524304c135bf", "name": "Yuchen Yu", "hidden": false}, {"_id": "6976d5405d41524304c135c0", "name": "Yufei Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135c1", "name": "Yuhuai Wei", "hidden": false}, {"_id": "6976d5405d41524304c135c2", "name": "Yulei Qian", "hidden": false}, {"_id": "6976d5405d41524304c135c3", "name": "Yunfan Liang", "hidden": false}, {"_id": "6976d5405d41524304c135c4", "name": "Yunke Zhao", "hidden": false}, {"_id": "6976d5405d41524304c135c5", "name": "Yuwei Jiang", "hidden": false}, {"_id": "6976d5405d41524304c135c6", "name": "Yuxin Bian", "hidden": false}, {"_id": "6976d5405d41524304c135c7", "name": "Yuxin Chen", "hidden": false}, {"_id": "6976d5405d41524304c135c8", "name": "Yuxin Liu", "hidden": false}, {"_id": "6976d5405d41524304c135c9", "name": "Yue Xu", "hidden": false}, {"_id": "6976d5405d41524304c135ca", "name": "Yueqing Sun", "hidden": false}, {"_id": "6976d5405d41524304c135cb", "name": "Zeyang Yu", "hidden": false}, {"_id": "6976d5405d41524304c135cc", "name": "Zhao Yang", "hidden": false}, {"_id": "6976d5405d41524304c135cd", "name": "Zhengsheng Huang", "hidden": false}, {"_id": "6976d5405d41524304c135ce", "name": "Zhengyu Chen", "hidden": false}, {"_id": "6976d5405d41524304c135cf", "name": "Zhijian Liu", "hidden": false}, {"_id": "6976d5405d41524304c135d0", "name": "Zhikang Xia", "hidden": false}, {"_id": "6976d5405d41524304c135d1", "name": "Zhimin Lin", "hidden": false}, {"_id": "6976d5405d41524304c135d2", "name": "Zhiyuan Yao", "hidden": false}, {"_id": "6976d5405d41524304c135d3", "name": "Zhuofan Chen", "hidden": false}, {"_id": "6976d5405d41524304c135d4", "name": "Zhuowen Han", "hidden": false}, {"_id": "6976d5405d41524304c135d5", "name": "Zijian Zhang", "hidden": false}, {"_id": "6976d5405d41524304c135d6", "name": "Ziran Li", "hidden": false}, {"_id": "6976d5405d41524304c135d7", "name": "Ziwen Wang", "hidden": false}, {"_id": "6976d5405d41524304c135d8", "name": "Ziyuan Zhuang", "hidden": false}], "publishedAt": "2026-01-23T13:20:09.000Z", "submittedOnDailyAt": "2026-01-26T00:15:28.340Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "upvotes": 136, "discussionId": "6976d5405d41524304c135d9", "ai_summary": "A 560-billion-parameter Mixture-of-Experts reasoning model achieves state-of-the-art performance on agentic benchmarks through a unified training framework combining domain-parallel expert training with fusion, along with enhancements for real-world robustness and complex reasoning.", "ai_keywords": ["Mixture-of-Experts", "agentic reasoning", "domain-parallel expert training", "fusion", "asynchronous reinforcement learning", "DORA", "long-tailed generation", "multi-turn interactions", "real-world noise patterns", "test-time scaling", "reasoning depth", "reasoning width", "parallel thinking"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>LongCat-Flash-Thinking-2601 \u662f\u4e00\u4e2a\u62e5\u67095600\u4ebf\u53c2\u6570\u7684\u5f00\u6e90\u6df7\u5408\u4e13\u5bb6\u63a8\u7406\u6a21\u578b\uff0c\u5177\u5907\u5f3a\u5927\u7684\u667a\u80fd\u63a8\u7406\u80fd\u529b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u5728\u591a\u79cd\u667a\u80fd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5305\u62ec\u667a\u80fd\u641c\u7d22\u548c\u5de5\u5177\u96c6\u6210\u63a8\u7406\u3002</li>\n    <li>\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684\u8bad\u7ec3\u6846\u67b6\u5b9e\u73b0\u4e86\u590d\u6742\u5de5\u5177\u4ea4\u4e92\u7684\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u5608\u6742\u7684\u73b0\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u7a33\u5065\u3002</li>\n    <li>\u4e3a\u4f18\u5316\u591a\u73af\u5883\u7684\u7a33\u5b9a\u8bad\u7ec3\uff0c\u8be5\u6a21\u578b\u6269\u5c55\u4e86DORA\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u652f\u6301\u8d85\u8fc710,000\u4e2a\u73af\u5883\u7684\u8bad\u7ec3\u3002</li>\n    <li>\u5f15\u5165\u91cd\u601d\u7ef4\u6a21\u5f0f\u4ee5\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5141\u8bb8\u5728\u6d4b\u8bd5\u65f6\u8fdb\u884c\u6df1\u5165\u548c\u5e7f\u6cdb\u7684\u63a8\u7406\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LongCat-Flash-Thinking-2601 is a large open-source reasoning model with 560 billion parameters, designed for better reasoning capabilities.</li>\n    <li>It performs exceptionally well on various benchmarks related to agentic tasks, such as searching and using tools.</li>\n    <li>The model is trained using a unique approach that combines expert training and a detailed design of data and environments.</li>\n    <li>It shows strong adaptability to complex interactions and performs well even in noisy real-world situations.</li>\n    <li>A special \"Heavy Thinking\" mode is included to improve performance on challenging reasoning tasks by allowing deeper and broader thinking during tests.</li>\n</ul>"}, "publishedAt": "2026-01-23T08:20:09.000Z", "title": "LongCat-Flash-Thinking-2601 Technical Report", "summary": "We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16725.png", "numComments": 4, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 216, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u591a\u6a21\u6001\u667a\u80fd\u7684\u6548\u7387\u3002</li>\n    <li>\u6a21\u578b\u901a\u8fc7\u4e24\u79cd\u7b56\u7565\u5b9e\u73b0\uff1a\u4f7f\u7528\u6574\u5408\u8bed\u8a00\u548c\u89c6\u89c9\u7684\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u53ca\u8d85\u8fc7 1000 \u6b21\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u3002</li>\n    <li>\u5b83\u4f7f\u7528\u5e76\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\u6765\u4f18\u5316\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8d44\u6e90\uff0c\u63d0\u9ad8\u611f\u77e5\u63a8\u7406\u7684\u80fd\u529b\u3002</li>\n    <li>\u5c3d\u7ba1\u6a21\u578b\u4f53\u79ef\u5c0f\uff0810\u4ebf\u53c2\u6570\uff09\uff0c\u4f46\u6027\u80fd\u5ab2\u7f8e\u6216\u8d85\u8fc7\u4e86\u4f53\u79ef\u66f4\u5927\u7684\u6a21\u578b\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5411\u793e\u533a\u63d0\u4f9b\u5b8c\u6574\u7684\u6a21\u578b\u5957\u4ef6\uff0c\u4fbf\u4e8e\u9ad8\u6548\u91cd\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a new, efficient open-source model that combines language and vision capabilities.</li>\n    <li>It uses a unique training approach with 1.2 trillion multimodal tokens and a special reinforcement learning process to improve performance.</li>\n    <li>The model features Parallel Coordinated Reasoning (PaCoRe), which helps it handle complex visual tasks effectively.</li>\n    <li>Despite its smaller size (10 billion parameters), STEP3-VL-10B performs as well as or better than much larger models.</li>\n    <li>The model has achieved high scores on various benchmarks, making it a strong tool for the community.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u6839\u636e\u89c6\u89c9\u7ebf\u7d22\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u672a\u80fd\u6709\u6548\u5229\u7528\u5730\u56fe\u8fd9\u4e00\u4eba\u7c7b\u5e38\u7528\u7b56\u7565\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\uff0c\u5177\u5907\u5730\u56fe\u601d\u7ef4\u80fd\u529b\uff0c\u5e76\u91c7\u7528\u4ee3\u7406-\u5730\u56fe\u5faa\u73af\u7684\u5f62\u5f0f\u3002</li>\n    <li>\u4f7f\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u6a21\u578b\u80fd\u529b\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\uff0c\u51c6\u786e\u7387\u663e\u8457\u63d0\u9ad8\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The goal of image geolocalization is to predict the exact location of where a photo was taken using visual hints.</li>\n    <li>Current large vision-language models often ignore the useful human strategy of using maps.</li>\n    <li>This study introduces a new approach called Thinking with Map, using a two-step process involving reinforcement learning and test-time scaling.</li>\n    <li>The new method helps the model become more efficient and explore different location options before making a final guess.</li>\n    <li>Results show that this method significantly outperforms existing models, improving accuracy in locating images by a large margin.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.17058", "authors": [{"_id": "69782c96026bdf0473116e06", "user": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "name": "Wei Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:49.701Z", "hidden": false}, {"_id": "69782c96026bdf0473116e07", "name": "Jun Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e08", "name": "Haoyu Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e09", "name": "Zhenghao Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0a", "name": "Qikang He", "hidden": false}, {"_id": "69782c96026bdf0473116e0b", "name": "Shaokun Han", "hidden": false}, {"_id": "69782c96026bdf0473116e0c", "name": "Guoliang Li", "hidden": false}, {"_id": "69782c96026bdf0473116e0d", "user": {"_id": "64ef522242da8d2a897d62da", "avatarUrl": "/avatars/03611010d247da66696ac8976d4d3ed3.svg", "isPro": false, "fullname": "xuanhe zhou", "user": "zhouxh19", "type": "user"}, "name": "Xuanhe Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:19.930Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0e", "user": {"_id": "674fa2f067c963c50a066594", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/674fa2f067c963c50a066594/hKZ46Mwm_UEguzBt63ys_.jpeg", "isPro": false, "fullname": "yeye he", "user": "yeyehe", "type": "user"}, "name": "Yeye He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:27.638Z", "hidden": false}, {"_id": "69782c96026bdf0473116e0f", "name": "Chunwei Liu", "hidden": false}, {"_id": "69782c96026bdf0473116e10", "user": {"_id": "66724ce47e7ff5d8bd069c7c", "avatarUrl": "/avatars/953f66585390dbdb202c1d7b7250d7bd.svg", "isPro": false, "fullname": "Zirui Tang", "user": "TerryTang", "type": "user"}, "name": "Zirui Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:49.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e11", "name": "Bin Wang", "hidden": false}, {"_id": "69782c96026bdf0473116e12", "user": {"_id": "695612aabf3c8959a3a05f9c", "avatarUrl": "/avatars/c18885f6dea6f3ee019405cd8cf6f484.svg", "isPro": false, "fullname": "ShenTang990", "user": "shentang", "type": "user"}, "name": "Shen Tang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:58:56.579Z", "hidden": false}, {"_id": "69782c96026bdf0473116e13", "name": "Kai Zuo", "hidden": false}, {"_id": "69782c96026bdf0473116e14", "user": {"_id": "67efa8a2ed790a2e999dc216", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0S4lQCJX61uCF8EkSLMkk.png", "isPro": false, "fullname": "Yuyu Luo", "user": "luoyuyu", "type": "user"}, "name": "Yuyu Luo", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:59:02.233Z", "hidden": false}, {"_id": "69782c96026bdf0473116e15", "name": "Zhenzhe Zheng", "hidden": false}, {"_id": "69782c96026bdf0473116e16", "user": {"_id": "63f9fca8d4349b157a109eec", "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg", "isPro": false, "fullname": "Conghui He", "user": "conghui", "type": "user"}, "name": "Conghui He", "status": "admin_assigned", "statusLastChangedAt": "2026-01-27T13:57:14.525Z", "hidden": false}, {"_id": "69782c96026bdf0473116e17", "name": "Jingren Zhou", "hidden": false}, {"_id": "69782c96026bdf0473116e18", "name": "Fan Wu", "hidden": false}], "publishedAt": "2026-01-22T12:02:45.000Z", "submittedOnDailyAt": "2026-01-27T00:42:38.464Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "submittedOnDailyBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "isPro": false, "fullname": "Wei Zhou", "user": "weizhoudb", "type": "user"}, "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "upvotes": 127, "discussionId": "69782c97026bdf0473116e19", "projectPage": "https://github.com/weAIDB/awesome-data-llm", "githubRepo": "https://github.com/weAIDB/awesome-data-llm", "githubRepoAddedBy": "user", "ai_summary": "LLM-enhanced data preparation methods are transforming data-centric workflows from rule-based pipelines to prompt-driven, context-aware approaches, organized into data cleaning, integration, and enrichment tasks.", "ai_keywords": ["data preparation", "large language models", "prompt-driven workflows", "agentic workflows", "data cleaning", "data integration", "data enrichment", "entity matching", "schema matching", "data annotation", "data profiling"], "githubStars": 644, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "summary_zh": "<ul>\n    <li>\u6570\u636e\u51c6\u5907\u7684\u76ee\u7684\u662f\u53bb\u566a\u539f\u59cb\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u6570\u636e\u96c6\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u53d6\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002</li>\n    <li>\u968f\u7740\u5bf9\u5e94\u7528\u51c6\u5907\u6570\u636e\u7684\u9700\u6c42\u4e0a\u5347\uff0cLLM\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u4ee5\u53ca\u7075\u6d3b\u4ee3\u7406\u6784\u5efa\u57fa\u7840\u8bbe\u65bd\u7684\u51fa\u73b0\uff0cLLM\u589e\u5f3a\u7684\u65b9\u6cd5\u6b63\u5728\u6210\u4e3a\u6570\u636e\u51c6\u5907\u7684\u91cd\u8981\u8d8b\u52bf\u3002</li>\n    <li>\u672c\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u4f7f\u7528LLM\u6280\u672f\u8fdb\u884c\u6570\u636e\u51c6\u5907\u7684\u6700\u65b0\u6587\u732e\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u4e0d\u540c\u7684\u4e0b\u6e38\u4efb\u52a1\u3002</li>\n    <li>\u6211\u4eec\u5c06\u6570\u636e\u51c6\u5907\u7684\u4efb\u52a1\u5206\u4e3a\u4e09\u7c7b\uff1a\u6570\u636e\u6e05\u7406\u3001\u6570\u636e\u96c6\u6210\u548c\u6570\u636e\u4e30\u5bcc\uff0c\u5e76\u8ba8\u8bba\u4e86\u5404\u81ea\u7684\u6280\u672f\u53ca\u5176\u4f18\u7f3a\u70b9\u3002</li>\n    <li>\u6700\u540e\uff0c\u672c\u6587\u8fd8\u63a2\u8ba8\u4e86\u5f53\u524d\u7684\u7814\u7a76\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5f3a\u8c03\u53ef\u6269\u5c55\u7684LLM\u6570\u636e\u7cfb\u7edf\u548c\u53ef\u9760\u7684\u5de5\u4f5c\u6d41\u7a0b\u8bbe\u8ba1\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Data preparation helps clean and analyze raw datasets for better use in applications like analytics and decision-making.</li>\n    <li>There is a growing interest in using advanced language model (LLM) techniques for easier and more effective data preparation.</li>\n    <li>The paper reviews various LLM methods and categorizes them into three main tasks: cleaning data, integrating data, and enriching data.</li>\n    <li>Each task has its own methods, strengths, and weaknesses, such as the cost of using LLMs and occasional inaccuracies.</li>\n    <li>The paper also identifies research challenges and suggests future directions for improving LLM applications in data preparation.</li>\n</ul>"}, "publishedAt": "2026-01-22T07:02:45.000Z", "title": "Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs", "summary": "Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.\n  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.17058.png", "numComments": 2, "submittedBy": {"_id": "68216c63856b96f869d1d116", "avatarUrl": "/avatars/f69026ca75377e6754ab3e317879e35a.svg", "fullname": "Wei Zhou", "name": "weizhoudb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "63e5ef7bf2e9a8f22c515654", "name": "SJTU", "fullname": "Shanghai Jiao Tong University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1676013394657-63e5ee22b6a40bf941da0928.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.12538", "authors": [{"_id": "6971913fc1c7409747bf9564", "name": "Tianxin Wei", "hidden": false}, {"_id": "6971913fc1c7409747bf9565", "user": {"_id": "6742eb40924e80c3c80ebe13", "avatarUrl": "/avatars/e6ccb1a89a1ea0bfca70779966f4f429.svg", "isPro": false, "fullname": "Ting-Wei Li", "user": "tingwl0122", "type": "user"}, "name": "Ting-Wei Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T17:12:21.531Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9566", "name": "Zhining Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf9567", "name": "Xuying Ning", "hidden": false}, {"_id": "6971913fc1c7409747bf9568", "name": "Ze Yang", "hidden": false}, {"_id": "6971913fc1c7409747bf9569", "name": "Jiaru Zou", "hidden": false}, {"_id": "6971913fc1c7409747bf956a", "name": "Zhichen Zeng", "hidden": false}, {"_id": "6971913fc1c7409747bf956b", "name": "Ruizhong Qiu", "hidden": false}, {"_id": "6971913fc1c7409747bf956c", "name": "Xiao Lin", "hidden": false}, {"_id": "6971913fc1c7409747bf956d", "name": "Dongqi Fu", "hidden": false}, {"_id": "6971913fc1c7409747bf956e", "name": "Zihao Li", "hidden": false}, {"_id": "6971913fc1c7409747bf956f", "user": {"_id": "653962e75c8e4863e1a2068f", "avatarUrl": "/avatars/d4f5f5da141f37d53ca1986ff17b325e.svg", "isPro": false, "fullname": "Mengting Ai", "user": "famous-blue-raincoat", "type": "user"}, "name": "Mengting Ai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:10.378Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9570", "user": {"_id": "677830bd3f2e3ec475576303", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dhwqUDkk66m4oSGSbcd7j.png", "isPro": false, "fullname": "Duo Zhou", "user": "Claudius7", "type": "user"}, "name": "Duo Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:12.476Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9571", "name": "Wenxuan Bao", "hidden": false}, {"_id": "6971913fc1c7409747bf9572", "user": {"_id": "646323556c27a7e33b23f198", "avatarUrl": "/avatars/17fe142f689ab4be3c2374d1d90393db.svg", "isPro": false, "fullname": "Yunzhe Li", "user": "yunzhel2", "type": "user"}, "name": "Yunzhe Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:14.383Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9573", "name": "Gaotang Li", "hidden": false}, {"_id": "6971913fc1c7409747bf9574", "name": "Cheng Qian", "hidden": false}, {"_id": "6971913fc1c7409747bf9575", "name": "Yu Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf9576", "name": "Xiangru Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf9577", "name": "Yin Xiao", "hidden": false}, {"_id": "6971913fc1c7409747bf9578", "name": "Liri Fang", "hidden": false}, {"_id": "6971913fc1c7409747bf9579", "name": "Hui Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf957a", "name": "Xianfeng Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf957b", "name": "Yuji Zhang", "hidden": false}, {"_id": "6971913fc1c7409747bf957c", "name": "Chi Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf957d", "name": "Jiaxuan You", "hidden": false}, {"_id": "6971913fc1c7409747bf957e", "name": "Heng Ji", "hidden": false}, {"_id": "6971913fc1c7409747bf957f", "name": "Hanghang Tong", "hidden": false}, {"_id": "6971913fc1c7409747bf9580", "name": "Jingrui He", "hidden": false}], "publishedAt": "2026-01-18T18:58:23.000Z", "submittedOnDailyAt": "2026-01-22T00:27:25.162Z", "title": "Agentic Reasoning for Large Language Models", "submittedOnDailyBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "isPro": false, "fullname": "Jiaru Zou", "user": "jiaruz2", "type": "user"}, "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "upvotes": 125, "discussionId": "69719140c1c7409747bf9581", "githubRepo": "https://github.com/weitianxin/Awesome-Agentic-Reasoning", "githubRepoAddedBy": "user", "ai_summary": "Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.", "ai_keywords": ["large language models", "agentic reasoning", "autonomous agents", "planning", "tool use", "search", "feedback", "memory", "adaptation", "collaborative settings", "coordination", "knowledge sharing", "reinforcement learning", "supervised fine-tuning", "in-context reasoning", "post-training reasoning", "real-world applications", "benchmarks", "thought and action", "world modeling", "scalable multi-agent training", "governance"], "githubStars": 105, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "summary_zh": "<ul>\n    <li>\u63a8\u7406\u662f\u63a8\u65ad\u3001\u89e3\u51b3\u95ee\u9898\u548c\u51b3\u7b56\u7684\u57fa\u672c\u8ba4\u77e5\u8fc7\u7a0b\u3002</li>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5c01\u95ed\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u5f00\u653e\u548c\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u5c06LLM\u89c6\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u901a\u8fc7\u6301\u7eed\u4e92\u52a8\u8fdb\u884c\u89c4\u5212\u3001\u884c\u52a8\u548c\u5b66\u4e60\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u5206\u4e3a\u4e09\u5c42\uff1a\u57fa\u7840\u4ee3\u7406\u63a8\u7406\u3001\u81ea\u6211\u8fdb\u5316\u4ee3\u7406\u63a8\u7406\u548c\u96c6\u4f53\u591a\u4ee3\u7406\u63a8\u7406\uff0c\u5206\u522b\u5173\u6ce8\u5355\u4e00\u80fd\u529b\u3001\u53cd\u9988\u4e0e\u9002\u5e94\uff0c\u4ee5\u53ca\u534f\u4f5c\u73af\u5883\u4e2d\u7684\u667a\u80fd\u3002</li>\n    <li>\u8c03\u67e5\u8fd8\u56de\u987e\u4e86\u4ee3\u7406\u63a8\u7406\u5728\u79d1\u5b66\u3001\u673a\u5668\u4eba\u3001\u533b\u7597\u7b49\u9886\u57df\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7684\u6311\u6218\u548c\u65b9\u5411\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reasoning is key for making decisions and solving problems, but large language models (LLMs) have difficulty in changing environments.</li>\n    <li>Agentic reasoning treats LLMs as independent agents that can plan, act, and learn through ongoing interactions.</li>\n    <li>The survey categorizes agentic reasoning into three levels: basic skills for individuals, improving skills over time, and teamwork among multiple agents.</li>\n    <li>It differentiates between in-context reasoning (real-time interactions) and post-training reasoning (improving skills after initial training).</li>\n    <li>The survey reviews various applications of agentic reasoning and highlights future challenges like improving adaptability and collaboration in real-world situations.</li>\n</ul>"}, "publishedAt": "2026-01-18T13:58:23.000Z", "title": "Agentic Reasoning for Large Language Models", "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12538.png", "numComments": 3, "submittedBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "fullname": "Jiaru Zou", "name": "jiaruz2", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.20833", "authors": [{"_id": "697b9192a67238fac88cbee8", "name": "Tengyue Xu", "hidden": false}, {"_id": "697b9192a67238fac88cbee9", "name": "Zhuoyang Qian", "hidden": false}, {"_id": "697b9192a67238fac88cbeea", "name": "Gaoge Liu", "hidden": false}, {"_id": "697b9192a67238fac88cbeeb", "name": "Li Ling", "hidden": false}, {"_id": "697b9192a67238fac88cbeec", "name": "Zhentao Zhang", "hidden": false}, {"_id": "697b9192a67238fac88cbeed", "name": "Biao Wu", "hidden": false}, {"_id": "697b9192a67238fac88cbeee", "name": "Shuo Zhang", "hidden": false}, {"_id": "697b9192a67238fac88cbeef", "name": "Ke Lu", "hidden": false}, {"_id": "697b9192a67238fac88cbef0", "name": "Wei Shi", "hidden": false}, {"_id": "697b9192a67238fac88cbef1", "name": "Ziqi Wang", "hidden": false}, {"_id": "697b9192a67238fac88cbef2", "name": "Zheng Feng", "hidden": false}, {"_id": "697b9192a67238fac88cbef3", "name": "Yan Luo", "hidden": false}, {"_id": "697b9192a67238fac88cbef4", "name": "Shu Xu", "hidden": false}, {"_id": "697b9192a67238fac88cbef5", "name": "Yongjin Chen", "hidden": false}, {"_id": "697b9192a67238fac88cbef6", "name": "Zhibo Feng", "hidden": false}, {"_id": "697b9192a67238fac88cbef7", "name": "Zhuo Chen", "hidden": false}, {"_id": "697b9192a67238fac88cbef8", "name": "Bruce Yuan", "hidden": false}, {"_id": "697b9192a67238fac88cbef9", "name": "Harry Wang", "hidden": false}, {"_id": "697b9192a67238fac88cbefa", "name": "Kris Chen", "hidden": false}], "publishedAt": "2026-01-28T18:31:54.000Z", "submittedOnDailyAt": "2026-01-30T03:32:00.106Z", "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives", "submittedOnDailyBy": {"_id": "62baa0d6dd02fbf607ce97be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62baa0d6dd02fbf607ce97be/V0I6pANlLEf2YDd9ZLZgi.jpeg", "isPro": false, "fullname": "Wendy", "user": "Wendy-Fly", "type": "user"}, "summary": "Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.", "upvotes": 113, "discussionId": "697b9192a67238fac88cbefb", "githubRepo": "https://github.com/AgentAlphaAGI/Idea2Paper", "githubRepoAddedBy": "user", "ai_summary": "Offline knowledge construction through structured methodological graphs enables more reliable and scalable autonomous scientific discovery by reducing reliance on real-time literature processing.", "ai_keywords": ["large language model", "autonomous scientific discovery", "runtime-centric execution", "context window limitations", "hallucination", "pre-computation-driven framework", "peer-reviewed papers", "research patterns", "methodological knowledge graph", "end-to-end research workflows"], "githubStars": 54, "organization": {"_id": "69542731e1200d74c1c053d1", "name": "AgentAlphaAGI", "fullname": "AgentAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b78eb76ab5d14ca7faac87/TbMZ3y00APtRzHEfTSR7I.jpeg"}, "summary_zh": "<ul>\n    <li>\u81ea\u4e3b\u79d1\u5b66\u53d1\u73b0\u7684\u7814\u7a76\u8fdb\u5c55\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\uff0c\u80fd\u81ea\u52a8\u5316\u7814\u7a76\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u5728\u7ebf\u8ba1\u7b97\uff0c\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u63a8\u7406\u8106\u5f31\u6027\u3002</li>\n    <li>\u63d0\u51faIdea2Story\u6846\u67b6\uff0c\u8f6c\u5411\u79bb\u7ebf\u77e5\u8bc6\u6784\u5efa\uff0c\u63d0\u9ad8\u6587\u732e\u7406\u89e3\u6548\u7387\u3002</li>\n    <li>Idea2Story\u6536\u96c6\u540c\u884c\u8bc4\u5ba1\u8bba\u6587\u53ca\u53cd\u9988\uff0c\u63d0\u53d6\u6838\u5fc3\u65b9\u6cd5\u5355\u5143\u5e76\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u3002</li>\n    <li>\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u8fde\u8d2f\u7684\u65b0\u7814\u7a76\u6a21\u5f0f\uff0c\u964d\u4f4e\u91cd\u590d\u63a8\u7406\u7684\u9700\u6c42\uff0c\u652f\u6301\u53ef\u9760\u7684\u79d1\u5b66\u53d1\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent advancements in using large language models (LLMs) have automated many research processes but have high computational costs and issues with reasoning.</li>\n    <li>The proposed system, Idea2Story, focuses on building an offline knowledge base from scientific papers instead of reasoning online.</li>\n    <li>Idea2Story gathers research papers and feedback, extracts important methods, and organizes them into a structured knowledge graph.</li>\n    <li>This system allows users to quickly find and use established research patterns, reducing the need for trial-and-error approaches.</li>\n    <li>Preliminary studies show that Idea2Story can create coherent and innovative research ideas, making it a promising tool for scientific discovery.</li>\n</ul>"}, "publishedAt": "2026-01-28T13:31:54.000Z", "title": "Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives", "summary": "Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.20833.png", "numComments": 1, "submittedBy": {"_id": "62baa0d6dd02fbf607ce97be", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62baa0d6dd02fbf607ce97be/V0I6pANlLEf2YDd9ZLZgi.jpeg", "fullname": "Wendy", "name": "Wendy-Fly", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "69542731e1200d74c1c053d1", "name": "AgentAlphaAGI", "fullname": "AgentAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/64b78eb76ab5d14ca7faac87/TbMZ3y00APtRzHEfTSR7I.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.08763", "authors": [{"_id": "6969b0a232f0333869ff946a", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:38.232Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946b", "user": {"_id": "6891c906f3c31445cc040ab1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891c906f3c31445cc040ab1/NBqxXOY7al4CD0XBj8ke2.jpeg", "isPro": false, "fullname": "Yucheng Wang", "user": "DevilEnfant", "type": "user"}, "name": "Yucheng Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:48.080Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946c", "name": "Yufei He", "hidden": false}, {"_id": "6969b0a232f0333869ff946d", "user": {"_id": "682deb444988bd82847e2b03", "avatarUrl": "/avatars/15da087e84386ea72c6fa2db63571420.svg", "isPro": false, "fullname": "Jia-Ying Wu", "user": "EricaWu", "type": "user"}, "name": "Jiaying Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:59.692Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946e", "name": "Yilun Zhao", "hidden": false}, {"_id": "6969b0a232f0333869ff946f", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0a232f0333869ff9470", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:06.327Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9471", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:12.181Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9472", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:17.979Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9473", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:23.007Z", "hidden": false}], "publishedAt": "2026-01-13T17:48:43.000Z", "submittedOnDailyAt": "2026-01-16T01:00:36.686Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "upvotes": 111, "discussionId": "6969b0a232f0333869ff9474", "ai_summary": "Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.", "ai_keywords": ["reinforcement learning", "large language models", "exploration collapse", "pass@k", "pass@1", "rollout-level objective", "high-level solution strategies", "clustering", "policy advantages", "AUC@K"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u3002</li>\n    <li>\u76ee\u524d\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5bb9\u6613\u51fa\u73b0\u201c\u63a2\u7d22\u5d29\u6e83\u201d\uff0c\u5bfc\u81f4\u6a21\u578b\u53ea\u5173\u6ce8\u5c11\u6570\u4e3b\u5bfc\u63a8\u7406\u6a21\u5f0f\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u2014\u2014\u72ec\u7279\u6027\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\uff0c\u5b83\u5956\u52b1\u4f7f\u7528\u7a00\u6709\u9ad8\u5c42\u7b56\u7565\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u8bc4\u5224\u8005\u5bf9\u76f8\u540c\u95ee\u9898\u7684\u4e0d\u540c\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u805a\u7c7b\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u591a\u6837\u6027\u3002</li>\n    <li>\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u533b\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u8868\u73b0\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a2\u7d22\u6027\u548c\u591a\u6837\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) helps improve large language models for complex reasoning tasks but often faces a problem called exploration collapse.</li>\n    <li>This problem occurs when models focus too much on a few common reasoning patterns, which limits their ability to find diverse solutions.</li>\n    <li>The proposed solution is called Uniqueness-Aware Reinforcement Learning, which encourages the discovery of rare but effective strategies.</li>\n    <li>This method uses a judge based on large language models to group similar solutions and rewards less common but correct strategies more.</li>\n    <li>The approach has shown to improve the model's performance in various subjects without sacrificing the accuracy of its top results.</li>\n</ul>"}, "publishedAt": "2026-01-13T12:48:43.000Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08763.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Feb 01, 2026";
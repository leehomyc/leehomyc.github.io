window.trendingPapers = [{"paper": {"id": "2511.20647", "authors": [{"_id": "69272506243b2216fb75cc87", "name": "Tahira Kazimi", "hidden": false}, {"_id": "69272506243b2216fb75cc88", "name": "Connor Dunlop", "hidden": false}, {"_id": "69272506243b2216fb75cc89", "name": "Pinar Yanardag", "hidden": false}], "publishedAt": "2025-11-25T18:59:45.000Z", "submittedOnDailyAt": "2025-11-26T13:35:37.458Z", "title": "Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization", "submittedOnDailyBy": {"_id": "66fec311343c151be4fb0b73", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FXeHseCn4DKSVUzk-VKdA.png", "isPro": false, "fullname": "Tahira Kazimi", "user": "tahirakazimi77", "type": "user"}, "summary": "While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.", "upvotes": 0, "discussionId": "69272506243b2216fb75cc8a", "projectPage": "https://diverse-video.github.io/", "ai_summary": "A framework combining Determinantal Point Processes and Group Relative Policy Optimization enhances diversity in text-to-video generation without compromising quality.", "ai_keywords": ["text-to-video (T2V) diffusion models", "set-level policy optimization", "Determinantal Point Processes (DPPs)", "Group Relative Policy Optimization (GRPO)", "diverse video generation", "visual appearance", "camera motions", "scene structure", "VBench", "VideoScore"], "organization": {"_id": "641f3b58a390e539522a6f88", "name": "VirginiaTech", "fullname": "Virginia Polytechnic Institute and State University"}}, "publishedAt": "2025-11-25T13:59:45.000Z", "title": "Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization", "summary": "While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.20647.png", "numComments": 1, "submittedBy": {"_id": "66fec311343c151be4fb0b73", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FXeHseCn4DKSVUzk-VKdA.png", "fullname": "Tahira Kazimi", "name": "tahirakazimi77", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "641f3b58a390e539522a6f88", "name": "VirginiaTech", "fullname": "Virginia Polytechnic Institute and State University"}, "isAuthorParticipating": false}, {"paper": {"id": "2511.18886", "authors": [{"_id": "6926f72b243b2216fb75cc37", "name": "Guangyuan Li", "hidden": false}, {"_id": "6926f72b243b2216fb75cc38", "name": "Siming Zheng", "hidden": false}, {"_id": "6926f72b243b2216fb75cc39", "name": "Shuolin Xu", "hidden": false}, {"_id": "6926f72b243b2216fb75cc3a", "name": "Jinwei Chen", "hidden": false}, {"_id": "6926f72b243b2216fb75cc3b", "name": "Bo Li", "hidden": false}, {"_id": "6926f72b243b2216fb75cc3c", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6926f72b243b2216fb75cc3d", "name": "Lei Zhao", "hidden": false}, {"_id": "6926f72b243b2216fb75cc3e", "name": "Peng-Tao Jiang", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6423efbdb77cc3daf8429755/6jxbJARvlMKsVKSaaVDVP.qt"], "publishedAt": "2025-11-24T08:41:28.000Z", "submittedOnDailyAt": "2025-11-26T12:58:15.558Z", "title": "MagicWorld: Interactive Geometry-driven Video World Exploration", "submittedOnDailyBy": {"_id": "6423efbdb77cc3daf8429755", "avatarUrl": "/avatars/a5b480713b4dd1dae8191545cb4c6f94.svg", "isPro": false, "fullname": "Peng-Tao Jiang", "user": "ptjiang", "type": "user"}, "summary": "Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.", "upvotes": 3, "discussionId": "6926f72b243b2216fb75cc3f", "ai_summary": "MagicWorld, an interactive video world model, integrates 3D geometry and historical retrieval to improve scene stability and continuity under user instructions.", "ai_keywords": ["3D geometric priors", "historical retrieval", "Action-Guided 3D Geometry Module", "AG3D", "point cloud", "viewpoint transitions", "History Cache Retrieval", "HCR", "scene evolution", "dynamic scene evolution", "autoregressive synthesis"]}, "publishedAt": "2025-11-24T03:41:28.000Z", "title": "MagicWorld: Interactive Geometry-driven Video World Exploration", "summary": "Recent interactive video world model methods generate scene evolution conditioned on user instructions. Although they achieve impressive results, two key limitations remain. First, they fail to fully exploit the correspondence between instruction-driven scene motion and the underlying 3D geometry, which results in structural instability under viewpoint changes. Second, they easily forget historical information during multi-step interaction, resulting in error accumulation and progressive drift in scene semantics and structure. To address these issues, we propose MagicWorld, an interactive video world model that integrates 3D geometric priors and historical retrieval. MagicWorld starts from a single scene image, employs user actions to drive dynamic scene evolution, and autoregressively synthesizes continuous scenes. We introduce the Action-Guided 3D Geometry Module (AG3D), which constructs a point cloud from the first frame of each interaction and the corresponding action, providing explicit geometric constraints for viewpoint transitions and thereby improving structural consistency. We further propose History Cache Retrieval (HCR) mechanism, which retrieves relevant historical frames during generation and injects them as conditioning signals, helping the model utilize past scene information and mitigate error accumulation. Experimental results demonstrate that MagicWorld achieves notable improvements in scene stability and continuity across interaction iterations.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6423efbdb77cc3daf8429755/6jxbJARvlMKsVKSaaVDVP.qt"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.18886.png", "numComments": 2, "submittedBy": {"_id": "6423efbdb77cc3daf8429755", "avatarUrl": "/avatars/a5b480713b4dd1dae8191545cb4c6f94.svg", "fullname": "Peng-Tao Jiang", "name": "ptjiang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1}, "isAuthorParticipating": false}, {"paper": {"id": "2511.19430", "authors": [{"_id": "69271171243b2216fb75cc51", "name": "Dingkang Liang", "hidden": false}, {"_id": "69271171243b2216fb75cc52", "name": "Cheng Zhang", "hidden": false}, {"_id": "69271171243b2216fb75cc53", "name": "Xiaopeng Xu", "hidden": false}, {"_id": "69271171243b2216fb75cc54", "name": "Jianzhong Ju", "hidden": false}, {"_id": "69271171243b2216fb75cc55", "name": "Zhenbo Luo", "hidden": false}, {"_id": "69271171243b2216fb75cc56", "name": "Xiang Bai", "hidden": false}], "publishedAt": "2025-11-24T18:59:17.000Z", "submittedOnDailyAt": "2025-11-26T12:11:33.720Z", "title": "Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution", "submittedOnDailyBy": {"_id": "67467b5979406f42a14517e9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67467b5979406f42a14517e9/wgnUxTd8vWOo0Cr2gaoyG.jpeg", "isPro": false, "fullname": "Dingkang Liang", "user": "dkliang", "type": "user"}, "summary": "Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT", "upvotes": 5, "discussionId": "69271171243b2216fb75cc57", "projectPage": "https://h-embodvis.github.io/GRANT/", "githubRepo": "https://github.com/H-EmbodVis/GRANT", "ai_summary": "ORS3D, a new task requiring language understanding, 3D grounding, and efficient scheduling, is introduced with a large dataset and an embodied multi-modal model named GRANT that uses a scheduling token mechanism for effective task management.", "ai_keywords": ["Operations Research knowledge-based 3D Grounded Task Scheduling", "ORS3D", "3D grounding", "efficiency optimization", "parallelizable subtasks", "embodied multi-modal large language model", "GRANT", "scheduling token mechanism"], "githubStars": 97, "organization": {"_id": "687cebf73858638f66e59f56", "name": "H-EmbodVis", "fullname": "H-EmbodVis", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67467b5979406f42a14517e9/AnffLBETBMQyF-4ZK9o7B.png"}}, "publishedAt": "2025-11-24T13:59:17.000Z", "title": "Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution", "summary": "Task scheduling is critical for embodied AI, enabling agents to follow natural language instructions and execute actions efficiently in 3D physical worlds. However, existing datasets often simplify task planning by ignoring operations research (OR) knowledge and 3D spatial grounding. In this work, we propose Operations Research knowledge-based 3D Grounded Task Scheduling (ORS3D), a new task that requires the synergy of language understanding, 3D grounding, and efficiency optimization. Unlike prior settings, ORS3D demands that agents minimize total completion time by leveraging parallelizable subtasks, e.g., cleaning the sink while the microwave operates. To facilitate research on ORS3D, we construct ORS3D-60K, a large-scale dataset comprising 60K composite tasks across 4K real-world scenes. Furthermore, we propose GRANT, an embodied multi-modal large language model equipped with a simple yet effective scheduling token mechanism to generate efficient task schedules and grounded actions. Extensive experiments on ORS3D-60K validate the effectiveness of GRANT across language understanding, 3D grounding, and scheduling efficiency. The code is available at https://github.com/H-EmbodVis/GRANT", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2511.19430.png", "numComments": 1, "submittedBy": {"_id": "67467b5979406f42a14517e9", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67467b5979406f42a14517e9/wgnUxTd8vWOo0Cr2gaoyG.jpeg", "fullname": "Dingkang Liang", "name": "dkliang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false}, "organization": {"_id": "687cebf73858638f66e59f56", "name": "H-EmbodVis", "fullname": "H-EmbodVis", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/67467b5979406f42a14517e9/AnffLBETBMQyF-4ZK9o7B.png"}, "isAuthorParticipating": false}];
window.papersLastUpdated = "Nov 27, 2025";
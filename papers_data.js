window.trendingPapers = {
    "today": [{"paper": {"id": "2601.15876", "authors": [{"_id": "6972d8d5fb12c92b735b73a2", "name": "Taofeng Xue", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a3", "user": {"_id": "6801cbd5f06f2d08f3fd5455", "avatarUrl": "/avatars/c728b48f6938c0d7cb6b14011927ede8.svg", "isPro": false, "fullname": "chong.peng", "user": "KleinChong", "type": "user"}, "name": "Chong Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T20:15:18.698Z", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a4", "name": "Mianqiu Huang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a5", "name": "Linsen Guo", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a6", "user": {"_id": "6764279e684ed3b61b2316a4", "avatarUrl": "/avatars/63a6b6c3b9f442b42480679425951187.svg", "isPro": false, "fullname": "SII-TianchengHAN", "user": "GenSouKai", "type": "user"}, "name": "Tiancheng Han", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:30.106Z", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a7", "name": "Haozhe Wang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a8", "name": "Jianing Wang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a9", "name": "Xiaocheng Zhang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73aa", "name": "Xin Yang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ab", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ac", "name": "Jinrui Ding", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ad", "name": "Xiandi Ma", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ae", "name": "Yuchen Xie", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73af", "name": "Peng Pei", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73b0", "name": "Xunliang Cai", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73b1", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-01-22T11:36:43.000Z", "submittedOnDailyAt": "2026-01-23T07:54:00.525Z", "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience", "submittedOnDailyBy": {"_id": "6459c7c10aba070266e41bb1", "avatarUrl": "/avatars/2178cac69cf4123db5e85191160f3795.svg", "isPro": false, "fullname": "mqhuang", "user": "LutherXD", "type": "user"}, "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.", "upvotes": 62, "discussionId": "6972d8d5fb12c92b735b73b2", "ai_summary": "EvoCUA introduces an evolutionary approach to computer-use agents that combines autonomous task generation with policy optimization to achieve superior performance in complex, long-horizon tasks.", "ai_keywords": ["computer-use agents", "native computer-use agents", "data generation", "policy optimization", "evolutionary cycle", "verifiable synthesis engine", "executable validators", "sandbox rollouts", "iterative evolving learning", "capability boundaries", "error analysis", "self-correction", "OSWorld benchmark", "foundation models"], "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86EvoCUA\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u578b\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u6a21\u578b\uff0c\u7a81\u7834\u4e86\u9759\u6001\u6570\u636e\u7684\u9650\u5236\u3002</li>\n    <li>EvoCUA\u901a\u8fc7\u751f\u6210\u6570\u636e\u548c\u4f18\u5316\u7b56\u7565\u5f62\u6210\u81ea\u6211\u7ef4\u6301\u7684\u8fdb\u5316\u5faa\u73af\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u9a8c\u8bc1\u7684\u5408\u6210\u5f15\u64ce\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u7684\u4efb\u52a1\u548c\u53ef\u6267\u884c\u7684\u9a8c\u8bc1\u5668\u3002</li>\n    <li>EvoCUA\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6210\u529f\u7387\u8fbe\u523056.7%\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u6700\u4f73\u6a21\u578bOpenCUA-72B\u548c\u5176\u4ed6\u95ed\u6e90\u6a21\u578b\u3002</li>\n    <li>\u8fd9\u4e00\u65b9\u6cd5\u5c55\u793a\u4e86\u901a\u8fc7\u7ecf\u9a8c\u5b66\u4e60\u7684\u8fdb\u5316\u8303\u5f0f\u5728\u4e0d\u540c\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u666e\u904d\u9002\u7528\u6027\uff0c\u63d0\u4f9b\u4e86\u63a8\u52a8\u4ee3\u7406\u80fd\u529b\u53d1\u5c55\u7684\u53ef\u6269\u5c55\u8def\u5f84\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>EvoCUA is a new type of computer-use agent that improves how AI handles complex tasks over time.</li>\n    <li>It uses a unique system that creates new data and improves its own decision-making process instead of just copying existing data.</li>\n    <li>The model can generate diverse tasks and validate them automatically to help it learn better.</li>\n    <li>EvoCUA has been tested and achieved a success rate of 56.7%, outperforming previous models significantly.</li>\n    <li>This approach shows that learning from experience can enhance AI performance, regardless of the model size.</li>\n</ul>"}, "publishedAt": "2026-01-22T06:36:43.000Z", "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience", "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15876.png", "numComments": 1, "submittedBy": {"_id": "6459c7c10aba070266e41bb1", "avatarUrl": "/avatars/2178cac69cf4123db5e85191160f3795.svg", "fullname": "mqhuang", "name": "LutherXD", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.15165", "authors": [{"_id": "6971933ac1c7409747bf9597", "name": "Zanlin Ni", "hidden": false}, {"_id": "6971933ac1c7409747bf9598", "user": {"_id": "6486dde1f74857df3f1a5828", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg", "isPro": false, "fullname": "Shenzhi Wang", "user": "shenzhi-wang", "type": "user"}, "name": "Shenzhi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:54.254Z", "hidden": false}, {"_id": "6971933ac1c7409747bf9599", "name": "Yang Yue", "hidden": false}, {"_id": "6971933ac1c7409747bf959a", "name": "Tianyu Yu", "hidden": false}, {"_id": "6971933ac1c7409747bf959b", "name": "Weilin Zhao", "hidden": false}, {"_id": "6971933ac1c7409747bf959c", "name": "Yeguo Hua", "hidden": false}, {"_id": "6971933ac1c7409747bf959d", "name": "Tianyi Chen", "hidden": false}, {"_id": "6971933ac1c7409747bf959e", "name": "Jun Song", "hidden": false}, {"_id": "6971933ac1c7409747bf959f", "name": "Cheng Yu", "hidden": false}, {"_id": "6971933ac1c7409747bf95a0", "name": "Bo Zheng", "hidden": false}, {"_id": "6971933ac1c7409747bf95a1", "name": "Gao Huang", "hidden": false}], "publishedAt": "2026-01-21T16:41:58.000Z", "submittedOnDailyAt": "2026-01-23T00:11:51.141Z", "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models", "submittedOnDailyBy": {"_id": "63987ffb2ceb55aabe0852f3", "avatarUrl": "/avatars/343b796ff6b8906203904e8c620d7eb5.svg", "isPro": false, "fullname": "Zanlin Ni", "user": "nzl-thu", "type": "user"}, "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap", "upvotes": 55, "discussionId": "6971933ac1c7409747bf95a2", "projectPage": "https://nzl-thu.github.io/the-flexibility-trap", "githubRepo": "https://github.com/LeapLabTHU/JustGRPO", "githubRepoAddedBy": "user", "ai_summary": "Arbitrary order generation in diffusion large language models limits reasoning capability by causing premature solution space collapse, making standard policy optimization more effective.", "ai_keywords": ["diffusion large language models", "left-to-right constraint", "token generation", "reinforcement learning", "reasoning potential", "mathematical reasoning", "coding tasks", "combinatorial trajectories", "likelihoods", "Group Relative Policy Optimization", "GRPO", "parallel decoding"], "githubStars": 66, "organization": {"_id": "69719700e3846c07669d13ee", "name": "Tsinghua-LeapLab", "fullname": "Tsinghua-LeapLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63987ffb2ceb55aabe0852f3/hflTWNTGxeJx83xNkYrDB.png"}, "summary_zh": "<ul>\n    <li>\u6269\u6563\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u6253\u7834\u4e86\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u7684\u4e25\u683c\u751f\u6210\u987a\u5e8f\u9650\u5236\uff0c\u8ba9\u751f\u6210\u7684\u987a\u5e8f\u66f4\u52a0\u7075\u6d3b\u3002</li>\n    <li>\u8fd9\u79cd\u7075\u6d3b\u6027\u672c\u5e94\u63d0\u9ad8\u6a21\u578b\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b9e\u9645\u60c5\u51b5\u5374\u76f8\u53cd\u3002</li>\n    <li>dLLMs\u5f80\u5f80\u5229\u7528\u8fd9\u79cd\u7075\u6d3b\u6027\u6765\u8df3\u8fc7\u4e00\u4e9b\u9ad8\u4e0d\u786e\u5b9a\u6027\u7684\u5173\u952etoken\uff0c\u4ece\u800c\u5bfc\u81f4\u89e3\u51b3\u7a7a\u95f4\u7684\u63d0\u524d\u6536\u7f29\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5JustGRPO\uff0c\u901a\u8fc7\u653e\u5f03\u4efb\u610f\u987a\u5e8f\u751f\u6210\uff0c\u91c7\u7528\u6807\u51c6\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u5f15\u5bfc\u63a8\u7406\u3002</li>\n    <li>JustGRPO\u5728GSM8K\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e8689.1%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u7559\u4e86dLLMs\u7684\u5e76\u884c\u89e3\u7801\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion Large Language Models (dLLMs) can generate tokens in any order, unlike traditional models that go left-to-right.</li>\n    <li>This flexibility seems to improve reasoning for tasks like math and coding, but it actually limits dLLMs' reasoning abilities.</li>\n    <li>dLLMs often avoid important tokens that help with problem exploration, leading to a reduced range of solutions.</li>\n    <li>Current reinforcement learning methods for dLLMs struggle with this flexibility and complexity.</li>\n    <li>Using a simpler approach called JustGRPO improves reasoning while still allowing parallel token generation, achieving high accuracy on tests.</li>\n</ul>"}, "publishedAt": "2026-01-21T11:41:58.000Z", "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models", "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15165.png", "numComments": 1, "submittedBy": {"_id": "63987ffb2ceb55aabe0852f3", "avatarUrl": "/avatars/343b796ff6b8906203904e8c620d7eb5.svg", "fullname": "Zanlin Ni", "name": "nzl-thu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "69719700e3846c07669d13ee", "name": "Tsinghua-LeapLab", "fullname": "Tsinghua-LeapLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63987ffb2ceb55aabe0852f3/hflTWNTGxeJx83xNkYrDB.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.14724", "authors": [{"_id": "6972ee7afb12c92b735b74b4", "user": {"_id": "637169557a5e5d8efdc3e58e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg", "isPro": false, "fullname": "Haowei Zhang", "user": "freesky", "type": "user"}, "name": "Haowei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:23.639Z", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b5", "name": "Shudong Yang", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b6", "name": "Jinlan Fu", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b7", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b8", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-01-21T07:26:15.000Z", "submittedOnDailyAt": "2026-01-23T01:43:37.582Z", "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding", "submittedOnDailyBy": {"_id": "637169557a5e5d8efdc3e58e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg", "isPro": false, "fullname": "Haowei Zhang", "user": "freesky", "type": "user"}, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10times faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.", "upvotes": 52, "discussionId": "6972ee7bfb12c92b735b74b9", "projectPage": "https://hermes-streaming.github.io/", "githubRepo": "https://github.com/haowei-freesky/HERMES", "githubRepoAddedBy": "user", "ai_summary": "HERMES is a training-free architecture that enables real-time video stream understanding by utilizing a hierarchical memory framework based on KV cache reuse, achieving faster response times and maintained accuracy even with reduced video token input.", "ai_keywords": ["Multimodal Large Language Models", "video understanding", "streaming video inputs", "real-time responses", "KV cache", "hierarchical memory framework", "mechanistic attention", "video tokens", "TTFT"], "githubStars": 24, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u79bb\u7ebf\u89c6\u9891\u7406\u89e3\u65b9\u9762\u6709\u663e\u8457\u8fdb\u5c55\u3002</li>\n    <li>\u5c06\u8fd9\u4e9b\u80fd\u529b\u6269\u5c55\u5230\u6d41\u5a92\u4f53\u89c6\u9891\u8f93\u5165\u4ecd\u7136\u5f88\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u7406\u89e3\u6027\u80fd\u7a33\u5b9a\u3001\u5b9e\u65f6\u54cd\u5e94\u548c\u4f4eGPU\u5185\u5b58\u4f7f\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51faHERMES\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u8bad\u7ec3\u67b6\u6784\uff0c\u7528\u4e8e\u5b9e\u65f6\u51c6\u786e\u7406\u89e3\u89c6\u9891\u6d41\u3002</li>\n    <li>HERMES\u5229\u7528\u7d27\u51d1\u7684KV\u7f13\u5b58\uff0c\u5b9e\u73b0\u8d44\u6e90\u53d7\u9650\u4e0b\u7684\u9ad8\u6548\u6d41\u5a92\u4f53\u7406\u89e3\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8ba1\u7b97\uff0c\u786e\u4fdd\u5b9e\u65f6\u54cd\u5e94\u3002</li>\n    <li>HERMES\u5728\u51cf\u5c11\u89c6\u9891\u6570\u636e\u91cf\u7684\u540c\u65f6\uff0c\u4f9d\u7136\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6d41\u5a92\u4f53\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e86\u6700\u9ad811.4%\u7684\u51c6\u786e\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent improvements in Multimodal Large Language Models (MLLMs) have enhanced offline video understanding, but real-time streaming video remains difficult.</li>\n    <li>HERMES is a new architecture that allows for fast and accurate understanding of video streams without needing extra training.</li>\n    <li>It uses a special memory system to efficiently manage video information and responds quickly to user queries.</li>\n    <li>HERMES is 10 times faster in processing than previous models and maintains high accuracy even with fewer video tokens.</li>\n    <li>It shows improvements in accuracy on streaming datasets, with gains of up to 11.4% compared to earlier methods.</li>\n</ul>"}, "publishedAt": "2026-01-21T02:26:15.000Z", "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10times faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14724.png", "numComments": 2, "submittedBy": {"_id": "637169557a5e5d8efdc3e58e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg", "fullname": "Haowei Zhang", "name": "freesky", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.15197", "authors": [{"_id": "6971c608c1c7409747bf96a5", "user": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "isPro": false, "fullname": "Shijie Lian", "user": "LiamLian0727", "type": "user"}, "name": "Shijie Lian", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:44:25.547Z", "hidden": false}, {"_id": "6971c608c1c7409747bf96a6", "name": "Bin Yu", "hidden": false}, {"_id": "6971c608c1c7409747bf96a7", "name": "Xiaopeng Lin", "hidden": false}, {"_id": "6971c608c1c7409747bf96a8", "name": "Laurence T. Yang", "hidden": false}, {"_id": "6971c608c1c7409747bf96a9", "name": "Zhaolong Shen", "hidden": false}, {"_id": "6971c608c1c7409747bf96aa", "name": "Changti Wu", "hidden": false}, {"_id": "6971c608c1c7409747bf96ab", "name": "Yuzhuo Miao", "hidden": false}, {"_id": "6971c608c1c7409747bf96ac", "name": "Cong Huang", "hidden": false}, {"_id": "6971c608c1c7409747bf96ad", "name": "Kai Chen", "hidden": false}], "publishedAt": "2026-01-21T17:15:22.000Z", "submittedOnDailyAt": "2026-01-23T00:45:20.588Z", "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries", "submittedOnDailyBy": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "isPro": false, "fullname": "Shijie Lian", "user": "LiamLian0727", "type": "user"}, "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior \u03c0(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.", "upvotes": 50, "discussionId": "6971c609c1c7409747bf96ae", "projectPage": "https://github.com/ZGC-EmbodyAI/BayesianVLA", "githubRepo": "https://github.com/ZGC-EmbodyAI/BayesianVLA", "githubRepoAddedBy": "user", "ai_summary": "BayesianVLA addresses language-action grounding issues in robot manipulation by using Bayesian decomposition to prevent information collapse and improve out-of-distribution generalization.", "ai_keywords": ["Vision-Language-Action models", "Information Collapse", "Bayesian decomposition", "latent action queries", "conditional Pointwise Mutual Information", "vision-only policies", "out-of-distribution generalization", "SimplerEnv", "RoboCasa"], "githubStars": 11, "organization": {"_id": "68896d3a716ee5bfb1428441", "name": "ZGCA", "fullname": "Zhongguancun Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"}, "summary_zh": "<ul>\n    <li>VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e0a\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u65b0\u6307\u4ee4\u548c\u590d\u6742\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u5e38\u5e38\u96be\u4ee5\u6cdb\u5316\u3002</li>\n    <li>\u5f53\u524d\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u504f\u5dee\uff0c\u5bfc\u81f4\u6307\u4ee4\u548c\u89c6\u89c9\u89c2\u5bdf\u4e4b\u95f4\u7684\u6761\u4ef6\u4e92\u4fe1\u606f\u6d88\u5931\uff0c\u79f0\u4e3a\u4fe1\u606f\u5d29\u6e83\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86BayesianVLA\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u5206\u89e3\u6765\u5f3a\u5236\u9075\u5faa\u6307\u4ee4\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u6f5c\u5728\u52a8\u4f5c\u67e5\u8be2\uff0c\u5efa\u7acb\u53cc\u5206\u652f\u67b6\u6784\uff0c\u4f18\u5316\u653f\u7b56\u4ee5\u6700\u5927\u5316\u52a8\u4f5c\u4e0e\u6307\u4ee4\u4e4b\u95f4\u7684\u6761\u4ef6\u4e92\u4fe1\u606f\u3002</li>\n    <li>BayesianVLA\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728OOD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u8fbe\u523011.3%\u7684\u6539\u5584\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-Language-Action (VLA) models are useful for robot manipulation but often fail with new instructions or complex tasks.</li>\n    <li>Current training methods create biased datasets where language instructions can be easily guessed from visuals, leading to \"Information Collapse.\" This makes models rely only on visual cues and ignore language.</li>\n    <li>To solve this, the authors propose a new framework called BayesianVLA that encourages models to follow instructions better.</li>\n    <li>BayesianVLA uses a two-part system to analyze both visual data and language instructions, improving how actions are linked to commands.</li>\n    <li>Tests show BayesianVLA enhances performance significantly, with an 11.3% improvement in challenging scenarios, proving it helps models understand language in actions better.</li>\n</ul>"}, "publishedAt": "2026-01-21T12:15:22.000Z", "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries", "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior \u03c0(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15197.png", "numComments": 2, "submittedBy": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "fullname": "Shijie Lian", "name": "LiamLian0727", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "68896d3a716ee5bfb1428441", "name": "ZGCA", "fullname": "Zhongguancun Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16206", "authors": [{"_id": "6972e04dfb12c92b735b73cf", "user": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "isPro": false, "fullname": "Daixuan Cheng", "user": "daixuancheng", "type": "user"}, "name": "Daixuan Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:28.070Z", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d0", "name": "Shaohan Huang", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d1", "name": "Yuxian Gu", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d2", "name": "Huatong Song", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d3", "name": "Guoxin Chen", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d4", "name": "Li Dong", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d5", "name": "Wayne Xin Zhao", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d6", "name": "Ji-Rong Wen", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d7", "name": "Furu Wei", "hidden": false}], "publishedAt": "2026-01-22T18:57:09.000Z", "submittedOnDailyAt": "2026-01-23T00:27:12.305Z", "title": "LLM-in-Sandbox Elicits General Agentic Intelligence", "submittedOnDailyBy": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "isPro": false, "fullname": "Daixuan Cheng", "user": "daixuancheng", "type": "user"}, "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.", "upvotes": 46, "discussionId": "6972e04dfb12c92b735b73d8", "projectPage": "https://llm-in-sandbox.github.io", "githubRepo": "https://github.com/llm-in-sandbox/llm-in-sandbox", "githubRepoAddedBy": "user", "ai_summary": "LLM-in-Sandbox enables large language models to perform general intelligence tasks across diverse domains by allowing them to explore a code sandbox environment, achieving robust generalization without additional training.", "ai_keywords": ["LLM-in-Sandbox", "code sandbox", "virtual computer", "reinforcement learning", "non-agentic data", "sandbox exploration", "general intelligence", "long-context understanding", "instruction following"], "githubStars": 38, "organization": {"_id": "68151d0f51add3813f3f7d1b", "name": "MicrosoftResearch", "fullname": "Microsoft Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86LLM-in-Sandbox\uff0c\u5141\u8bb8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u6c99\u76d2\u4e2d\u63a2\u7d22\uff0c\u4ee5\u589e\u5f3a\u5176\u5728\u975e\u4ee3\u7801\u9886\u57df\u7684\u901a\u7528\u667a\u80fd\u3002</li>\n    <li>\u5f3a\u5927\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6ca1\u6709\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u5229\u7528\u4ee3\u7801\u6c99\u76d2\u5904\u7406\u975e\u4ee3\u7801\u4efb\u52a1\u3002</li>\n    <li>\u6a21\u578b\u53ef\u4ee5\u81ea\u53d1\u8bbf\u95ee\u5916\u90e8\u8d44\u6e90\u83b7\u53d6\u65b0\u77e5\u8bc6\uff0c\u4f7f\u7528\u6587\u4ef6\u7cfb\u7edf\u5904\u7406\u957f\u4e0a\u4e0b\u6587\uff0c\u5e76\u6267\u884c\u811a\u672c\u6ee1\u8db3\u683c\u5f0f\u8981\u6c42\u3002</li>\n    <li>\u901a\u8fc7LLM-in-Sandbox\u5f3a\u5316\u5b66\u4e60\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u6a21\u578b\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u4f7f\u7528\u975e\u667a\u80fd\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002</li>\n    <li>\u5b9e\u9a8c\u663e\u793aLLM-in-Sandbox\u5728\u591a\u4e2a\u9886\u57df\uff08\u5982\u6570\u5b66\u3001\u7269\u7406\u3001\u5316\u5b66\u7b49\uff09\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u5316\u80fd\u529b\uff0c\u5e76\u5df2\u5f00\u6e90\u4e3aPython\u5305\u4ee5\u4fbf\u5b9e\u9645\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LLM-in-Sandbox allows language models (LLMs) to work in a virtual environment to improve their problem-solving skills in various subjects.</li>\n    <li>These LLMs can access external information and use file systems to manage large amounts of data without needing extra training.</li>\n    <li>Using a special training method called LLM-in-Sandbox-RL, the models can improve their skills in exploring the sandbox.</li>\n    <li>Experiments show that LLM-in-Sandbox can effectively handle tasks in areas like math, science, and following instructions.</li>\n    <li>The tool is efficient and has been made available as an open-source Python package for practical use.</li>\n</ul>"}, "publishedAt": "2026-01-22T13:57:09.000Z", "title": "LLM-in-Sandbox Elicits General Agentic Intelligence", "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16206.png", "numComments": 2, "submittedBy": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "fullname": "Daixuan Cheng", "name": "daixuancheng", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 16, "isUserFollowing": false}, "organization": {"_id": "68151d0f51add3813f3f7d1b", "name": "MicrosoftResearch", "fullname": "Microsoft Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16208", "authors": [{"_id": "6972ea8bfb12c92b735b74a8", "name": "Shengbang Tong", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74a9", "name": "Boyang Zheng", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74aa", "user": {"_id": "64249f76d476e4ad55665d59", "avatarUrl": "/avatars/a0fec7e423ffae944a874ca267b55c1f.svg", "isPro": false, "fullname": "Ziteng Wang", "user": "AustinWang0330", "type": "user"}, "name": "Ziteng Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:01.136Z", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ab", "name": "Bingda Tang", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ac", "name": "Nanye Ma", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ad", "user": {"_id": "626dc5105f7327906f0b2a4e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626dc5105f7327906f0b2a4e/QCSzuwYqsv8ozRnusVb-F.jpeg", "isPro": true, "fullname": "Ellis Brown", "user": "ellisbrown", "type": "user"}, "name": "Ellis Brown", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T20:15:10.637Z", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ae", "name": "Jihan Yang", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74af", "name": "Rob Fergus", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74b0", "name": "Yann LeCun", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74b1", "name": "Saining Xie", "hidden": false}], "publishedAt": "2026-01-22T18:58:16.000Z", "submittedOnDailyAt": "2026-01-23T00:57:17.761Z", "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "submittedOnDailyBy": {"_id": "6434226da4c9c55871a78052", "avatarUrl": "/avatars/3309832b3115bc6ad08ae1d10f43118b.svg", "isPro": false, "fullname": "BoYang Zheng", "user": "bytetriper", "type": "user"}, "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.", "upvotes": 40, "discussionId": "6972ea8cfb12c92b735b74b2", "projectPage": "https://rae-dit.github.io/scale-rae/", "githubRepo": "https://github.com/ZitengWangNYU/Scale-RAE", "githubRepoAddedBy": "user", "ai_summary": "Representation Autoencoders (RAEs) demonstrate superior performance over VAEs in large-scale text-to-image generation, showing improved stability, faster convergence, and better quality while enabling unified multimodal reasoning in shared representation spaces.", "ai_keywords": ["representation autoencoders", "diffusion modeling", "semantic latent spaces", "text-to-image generation", "frozen representation encoder", "SigLIP-2", "noise scheduling", "diffusion transformers", "pretraining", "finetuning", "catastrophic overfitting", "multimodal model", "shared representation space"], "githubStars": 58, "organization": {"_id": "662741612ada5b77e310d171", "name": "nyu-visionx", "fullname": "VISIONx @ NYU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"}, "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u8868\u793a\u81ea\u7f16\u7801\u5668\uff08RAE\uff09\u5728\u5927\u89c4\u6a21\u81ea\u7531\u5f62\u5f0f\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5e94\u7528\u3002</li>\n    <li>\u6211\u4eec\u53d1\u73b0\uff0c\u5c3d\u7ba1\u6269\u5927\u89c4\u6a21\u63d0\u9ad8\u4e86\u6574\u4f53\u4fdd\u771f\u5ea6\uff0c\u4f46\u7279\u5b9a\u9886\u57df\uff08\u5982\u6587\u672c\uff09\u7684\u6570\u636e\u7ec4\u6210\u4ecd\u7136\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>RAE\u6a21\u578b\u5728\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u4e0b\uff08\u4ece0.5B\u52309.8B\uff09\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684FLUX VAE\u6a21\u578b\u3002</li>\n    <li>\u5728\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u5fae\u8c03\u8fc7\u7a0b\u4e2d\uff0cRAE\u6a21\u578b\u6bd4VAE\u6a21\u578b\u5728\u591a\u4e2a\u5468\u671f\u540e\u8868\u73b0\u66f4\u7a33\u5b9a\uff0c\u4e14\u6027\u80fd\u66f4\u597d\u3002</li>\n    <li>RAE\u6a21\u578b\u7684\u591a\u6a21\u6001\u7279\u6027\u4f7f\u5176\u80fd\u591f\u5728\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u4e2d\u76f4\u63a5\u5904\u7406\u751f\u6210\u7684\u6f5c\u53d8\u91cf\uff0c\u5f00\u8f9f\u4e86\u7edf\u4e00\u6a21\u578b\u7684\u65b0\u53ef\u80fd\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Representation Autoencoders (RAEs) are effective for generating images from text and work well in high-dimensional spaces.</li>\n    <li>Researchers tested RAEs using large datasets and found that while size helps, using the right data is crucial for specific tasks like text generation.</li>\n    <li>They discovered that simplifying the RAE design made it more efficient, as some complex features offered little improvement at larger scales.</li>\n    <li>RAEs outperformed traditional Variational Autoencoders (VAEs) in multiple tests, showing better stability and quality during training.</li>\n    <li>RAEs enable new possibilities for models that understand and generate visuals by sharing representation spaces.</li>\n</ul>"}, "publishedAt": "2026-01-22T13:58:16.000Z", "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16208.png", "numComments": 1, "submittedBy": {"_id": "6434226da4c9c55871a78052", "avatarUrl": "/avatars/3309832b3115bc6ad08ae1d10f43118b.svg", "fullname": "BoYang Zheng", "name": "bytetriper", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "662741612ada5b77e310d171", "name": "nyu-visionx", "fullname": "VISIONx @ NYU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.15892", "authors": [{"_id": "6972d788fb12c92b735b7397", "user": {"_id": "641aa5e391e3376a057bbd4c", "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg", "isPro": false, "fullname": "Chenghao Fan", "user": "Facico", "type": "user"}, "name": "Chenghao Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:32.109Z", "hidden": false}, {"_id": "6972d788fb12c92b735b7398", "name": "Wen Heng", "hidden": false}, {"_id": "6972d788fb12c92b735b7399", "name": "Bo Li", "hidden": false}, {"_id": "6972d788fb12c92b735b739a", "name": "Sichen Liu", "hidden": false}, {"_id": "6972d788fb12c92b735b739b", "name": "Yuxuan Song", "hidden": false}, {"_id": "6972d788fb12c92b735b739c", "name": "Jing Su", "hidden": false}, {"_id": "6972d788fb12c92b735b739d", "name": "Xiaoye Qu", "hidden": false}, {"_id": "6972d788fb12c92b735b739e", "name": "Kai Shen", "hidden": false}, {"_id": "6972d788fb12c92b735b739f", "name": "Wei Wei", "hidden": false}], "publishedAt": "2026-01-22T12:13:17.000Z", "submittedOnDailyAt": "2026-01-23T00:09:35.389Z", "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "submittedOnDailyBy": {"_id": "641aa5e391e3376a057bbd4c", "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg", "isPro": false, "fullname": "Chenghao Fan", "user": "Facico", "type": "user"}, "summary": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.", "upvotes": 40, "discussionId": "6972d788fb12c92b735b73a0", "projectPage": "https://bytedance-seed.github.io/Stable-DiffCoder/", "githubRepo": "https://github.com/ByteDance-Seed/Stable-DiffCoder", "githubRepoAddedBy": "user", "ai_summary": "Stable-DiffCoder demonstrates superior code modeling performance compared to autoregressive baselines through block diffusion continual pretraining and efficient training mechanisms.", "ai_keywords": ["diffusion-based language models", "autoregressive models", "block diffusion", "continual pretraining", "warmup", "clipped noise schedule", "supervised fine-tuning", "code modeling", "structured code modeling", "data augmentation"], "githubStars": 16, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u6269\u6563\u57fa\u7840\u8bed\u8a00\u6a21\u578b\uff08DLLMs\uff09\u76f8\u6bd4\u81ea\u56de\u5f52\u6a21\u578b\uff08AR\uff09\uff0c\u5728\u751f\u6210\u65b9\u5f0f\u548c\u6570\u636e\u91cd\u7528\u4e0a\u66f4\u6709\u4f18\u52bf\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Stable-DiffCoder\uff0c\u8fd9\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u5757\u6269\u6563\u4ee3\u7801\u6a21\u578b\uff0c\u4f7f\u7528\u4e86Seed-Coder\u67b6\u6784\u548c\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>\u4e3a\u4e86\u63d0\u9ad8\u77e5\u8bc6\u5b66\u4e60\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5f15\u5165\u4e86\u5757\u6269\u6563\u6301\u7eed\u9884\u8bad\u7ec3\uff08CPT\uff09\u9636\u6bb5\u3002</li>\n    <li>\u5728\u76f8\u540c\u7684\u6570\u636e\u548c\u67b6\u6784\u4e0b\uff0cStable-DiffCoder\u5728\u591a\u4e2a\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5176\u81ea\u56de\u5f52\u6a21\u578b\u3002</li>\n    <li>\u6269\u6563\u57fa\u7840\u7684\u5efa\u6a21\u65b9\u6cd5\u63d0\u5347\u4e86\u4ee3\u7801\u7684\u7f16\u8f91\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u5e2e\u52a9\u4f4e\u8d44\u6e90\u7f16\u7a0b\u8bed\u8a00\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion-based language models (DLLMs) can generate code more efficiently than autoregressive (AR) models.</li>\n    <li>Stable-DiffCoder is a new type of DLLM that builds on existing architecture and training methods.</li>\n    <li>It includes a special training phase to enhance learning and stability.</li>\n    <li>Stable-DiffCoder performs better than AR models on various coding tests, even with less data.</li>\n    <li>It also aids in editing and reasoning about code and helps with less common programming languages.</li>\n</ul>"}, "publishedAt": "2026-01-22T07:13:17.000Z", "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "summary": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15892.png", "numComments": 0, "submittedBy": {"_id": "641aa5e391e3376a057bbd4c", "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg", "fullname": "Chenghao Fan", "name": "Facico", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13, "isUserFollowing": false}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.16093", "authors": [{"_id": "6972f00ffb12c92b735b74bb", "user": {"_id": "644d0b5c6dfd5f8240d95f5e", "avatarUrl": "/avatars/c5bcc5691fe8ec3e24558afb80feecab.svg", "isPro": false, "fullname": "zhou yikang", "user": "zhouyik", "type": "user"}, "name": "Yikang Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:04.804Z", "hidden": false}, {"_id": "6972f00ffb12c92b735b74bc", "name": "Tao Zhang", "hidden": false}, {"_id": "6972f00ffb12c92b735b74bd", "user": {"_id": "66350ea032a5f38150f7a82b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66350ea032a5f38150f7a82b/UOrv46KJsQV9Ye_bmyIS6.jpeg", "isPro": false, "fullname": "GongDengxian", "user": "godx7", "type": "user"}, "name": "Dengxian Gong", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:21.767Z", "hidden": false}, {"_id": "6972f00ffb12c92b735b74be", "name": "Yuanzheng Wu", "hidden": false}, {"_id": "6972f00ffb12c92b735b74bf", "name": "Ye Tian", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c0", "name": "Haochen Wang", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c1", "name": "Haobo Yuan", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c2", "name": "Jiacong Wang", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c3", "name": "Lu Qi", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c4", "name": "Hao Fei", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c5", "name": "Anran Wang", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c6", "name": "Zhuochen Wang", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c7", "name": "Yujing Wang", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c8", "name": "Cheng Chen", "hidden": false}, {"_id": "6972f00ffb12c92b735b74c9", "name": "Shunping Ji", "hidden": false}, {"_id": "6972f00ffb12c92b735b74ca", "name": "Xiangtai Li", "hidden": false}], "publishedAt": "2026-01-22T16:44:09.000Z", "submittedOnDailyAt": "2026-01-23T01:22:36.775Z", "title": "SAMTok: Representing Any Mask with Two Words", "submittedOnDailyBy": {"_id": "63958b4414513eaf9029ebf1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png", "isPro": false, "fullname": "Xiangtai Li", "user": "LXT", "type": "user"}, "summary": "Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.", "upvotes": 31, "discussionId": "6972f00ffb12c92b735b74cb", "projectPage": "https://zhouyiks.github.io/projects/SAMTok/", "githubRepo": "https://github.com/bytedance/Sa2VA/tree/main/projects/samtok", "githubRepoAddedBy": "user", "ai_summary": "SAMTok enables pixel-wise capabilities in multi-modal LLMs through discrete mask tokenization and standard training methods, achieving state-of-the-art performance on various vision-language tasks.", "ai_keywords": ["multi-modal LLMs", "region mask", "discrete mask tokenizer", "SAM2", "mask encoder", "residual vector quantizer", "next-token prediction", "reinforcement learning", "region captioning", "region VQA", "grounded conversation", "referring segmentation", "scene graph parsing", "multi-round interactive segmentation", "textual answer-matching reward", "GRES", "GCG"], "githubStars": 1502, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAMTok\u7684\u79bb\u6563\u63a9\u7801\u6807\u8bb0\u5668\uff0c\u53ef\u4ee5\u5c06\u533a\u57df\u63a9\u7801\u8f6c\u6362\u4e3a\u4e24\u79cd\u7279\u6b8a\u7684\u6807\u8bb0\u3002</li>\n    <li>SAMTok\u901a\u8fc7\u5c06\u63a9\u7801\u89c6\u4e3a\u65b0\u7684\u8bed\u8a00\u6807\u8bb0\uff0c\u4f7f\u57fa\u7840\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u80fd\u591f\u5b66\u4e60\u50cf\u7d20\u7ea7\u80fd\u529b\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u4e0d\u9700\u8981\u5bf9\u6a21\u578b\u7ed3\u6784\u8fdb\u884c\u4fee\u6539\uff0c\u4f7f\u7528\u6807\u51c6\u7684\u4e0b\u4e00\u4e2a\u6807\u8bb0\u9884\u6d4b\u548c\u7b80\u5355\u7684\u5f3a\u5316\u5b66\u4e60\u3002</li>\n    <li>\u5728209M\u591a\u6837\u5316\u63a9\u7801\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0cSAMTok\u751f\u6210\u7684\u4fe1\u606f\u4e30\u5bcc\u7684\u7d27\u51d1\u6807\u8bb0\u3002</li>\n    <li>QwenVL-SAMTok\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5305\u62ec\u533a\u57df\u5b57\u5e55\u548c\u591a\u8f6e\u4ea4\u4e92\u5206\u5272\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>SAMTok is a new tool that helps multi-modal large language models (MLLMs) understand and use pixel-level information more effectively.</li>\n    <li>It converts complex region masks into simple tokens, allowing MLLMs to learn through standard next-token prediction without changing their structure.</li>\n    <li>SAMTok was developed using 209 million diverse masks and creates compact tokens that retain a lot of information.</li>\n    <li>With this method, the QwenVL-SAMTok model achieves top results in various tasks, including region captioning and interactive segmentation.</li>\n    <li>It also introduces a reward system for better mask generation, leading to significant improvements in performance benchmarks.</li>\n</ul>"}, "publishedAt": "2026-01-22T11:44:09.000Z", "title": "SAMTok: Representing Any Mask with Two Words", "summary": "Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16093.png", "numComments": 1, "submittedBy": {"_id": "63958b4414513eaf9029ebf1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png", "fullname": "Xiangtai Li", "name": "LXT", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.16175", "authors": [{"_id": "6972e02afb12c92b735b73c2", "name": "Mert Yuksekgonul", "hidden": false}, {"_id": "6972e02afb12c92b735b73c3", "name": "Daniel Koceja", "hidden": false}, {"_id": "6972e02afb12c92b735b73c4", "name": "Xinhao Li", "hidden": false}, {"_id": "6972e02afb12c92b735b73c5", "name": "Federico Bianchi", "hidden": false}, {"_id": "6972e02afb12c92b735b73c6", "name": "Jed McCaleb", "hidden": false}, {"_id": "6972e02afb12c92b735b73c7", "name": "Xiaolong Wang", "hidden": false}, {"_id": "6972e02afb12c92b735b73c8", "name": "Jan Kautz", "hidden": false}, {"_id": "6972e02afb12c92b735b73c9", "name": "Yejin Choi", "hidden": false}, {"_id": "6972e02afb12c92b735b73ca", "name": "James Zou", "hidden": false}, {"_id": "6972e02afb12c92b735b73cb", "name": "Carlos Guestrin", "hidden": false}, {"_id": "6972e02afb12c92b735b73cc", "name": "Yu Sun", "hidden": false}], "publishedAt": "2026-01-22T18:24:00.000Z", "submittedOnDailyAt": "2026-01-23T00:15:05.275Z", "title": "Learning to Discover at Test Time", "submittedOnDailyBy": {"_id": "603f7c7af84ebe399f1c85cf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625214886903-603f7c7af84ebe399f1c85cf.jpeg", "isPro": false, "fullname": "Federico Bianchi", "user": "vinid", "type": "user"}, "summary": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erd\u0151s' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to 2times faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.", "upvotes": 24, "discussionId": "6972e02afb12c92b735b73cd", "projectPage": "https://test-time-training.github.io/discover/", "githubRepo": "https://github.com/test-time-training/discover", "githubRepoAddedBy": "user", "ai_summary": "Test-time training enables AI systems to discover optimal solutions for specific scientific problems through continual learning focused on individual challenges rather than generalization.", "ai_keywords": ["reinforcement learning", "test-time training", "continual learning", "search subroutine", "learning objective", "OpenAI gpt-oss-120b", "Tinker API"], "githubStars": 88, "organization": {"_id": "672c672dcf09d152f4da04c4", "name": "StanfordUniversity", "fullname": "Stanford University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/vJI0POlzGMXL2878t1vz2.jpeg"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3a\u201c\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u53d1\u73b0\u201d\uff08TTT-Discover\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u79d1\u5b66\u95ee\u9898\u3002</li>\n    <li>TTT-Discover\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u6d4b\u8bd5\u65f6\u7ee7\u7eed\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u4f7f\u5176\u80fd\u9488\u5bf9\u7279\u5b9a\u95ee\u9898\u8fdb\u884c\u4f18\u5316\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u4f18\u5148\u8003\u8651\u6700\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u76ee\u6807\u662f\u627e\u5230\u4e00\u4e2a\u4f18\u79c0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u800c\u4e0d\u662f\u591a\u4e2a\u666e\u901a\u7684\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u6211\u4eec\u5728\u6570\u5b66\u3001GPU\u5185\u6838\u5de5\u7a0b\u3001\u7b97\u6cd5\u8bbe\u8ba1\u548c\u751f\u7269\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u65b0\u8fdb\u5c55\uff0c\u51e0\u4e4e\u6240\u6709\u5c1d\u8bd5\u7684\u95ee\u9898\u90fd\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u4f73\u6c34\u5e73\u3002</li>\n    <li>\u4f7f\u7528\u5f00\u653e\u6a21\u578b\u548c\u516c\u5f00\u4ee3\u7801\u5b8c\u6210\u7684\u7ed3\u679c\u53ef\u4ee5\u91cd\u590d\uff0c\u4e14\u6bcf\u4e2a\u95ee\u9898\u7684\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u6210\u672c\u4ec5\u4e3a\u51e0\u767e\u7f8e\u5143\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study introduces a new method called Test-Time Training to Discover (TTT-Discover) for solving scientific problems using AI.</li>\n    <li>TTT-Discover allows a large language model (LLM) to learn from specific test experiences rather than just using pre-trained knowledge.</li>\n    <li>This approach aims to find one excellent solution for a specific problem rather than many good solutions for different problems.</li>\n    <li>The method has shown to outperform previous techniques in various fields, including mathematics, GPU kernel engineering, algorithm design, and biology.</li>\n    <li>All results were achieved using an open model, making it easier to reproduce the findings, and the costs of implementation were relatively low.</li>\n</ul>"}, "publishedAt": "2026-01-22T13:24:00.000Z", "title": "Learning to Discover at Test Time", "summary": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erd\u0151s' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to 2times faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16175.png", "numComments": 1, "submittedBy": {"_id": "603f7c7af84ebe399f1c85cf", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625214886903-603f7c7af84ebe399f1c85cf.jpeg", "fullname": "Federico Bianchi", "name": "vinid", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 21, "isUserFollowing": false}, "organization": {"_id": "672c672dcf09d152f4da04c4", "name": "StanfordUniversity", "fullname": "Stanford University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68e396f2b5bb631e9b2fac9a/vJI0POlzGMXL2878t1vz2.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.15621", "authors": [{"_id": "6972e310fb12c92b735b746a", "name": "Hangrui Hu", "hidden": false}, {"_id": "6972e310fb12c92b735b746b", "name": "Xinfa Zhu", "hidden": false}, {"_id": "6972e310fb12c92b735b746c", "name": "Ting He", "hidden": false}, {"_id": "6972e310fb12c92b735b746d", "name": "Dake Guo", "hidden": false}, {"_id": "6972e310fb12c92b735b746e", "name": "Bin Zhang", "hidden": false}, {"_id": "6972e310fb12c92b735b746f", "user": {"_id": "664eaf0a98e93ef417c3cc42", "avatarUrl": "/avatars/67fb44351cac8964410e5b6549817182.svg", "isPro": false, "fullname": "Xiong Wang", "user": "xiongwang", "type": "user"}, "name": "Xiong Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T20:15:12.488Z", "hidden": false}, {"_id": "6972e310fb12c92b735b7470", "name": "Zhifang Guo", "hidden": false}, {"_id": "6972e310fb12c92b735b7471", "user": {"_id": "67dbdf261956dcedf0f0a7e1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/BRzXe_7jytEYadJ0byMyD.png", "isPro": false, "fullname": "ZiyueJiang", "user": "ZiyueJiang", "type": "user"}, "name": "Ziyue Jiang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-23T09:48:40.953Z", "hidden": false}, {"_id": "6972e310fb12c92b735b7472", "user": {"_id": "666fd382b955b0e655165768", "avatarUrl": "/avatars/66476925471bda2dc9b57f091f245dd9.svg", "isPro": false, "fullname": "hongkun hao", "user": "hongkunhao", "type": "user"}, "name": "Hongkun Hao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-23T09:48:26.167Z", "hidden": false}, {"_id": "6972e310fb12c92b735b7473", "user": {"_id": "6370e58a967e405db11cf788", "avatarUrl": "/avatars/369ed523fd7f9ac08baf2d3b4b2d8426.svg", "isPro": false, "fullname": "guozishan", "user": "jimapple", "type": "user"}, "name": "Zishan Guo", "status": "admin_assigned", "statusLastChangedAt": "2026-01-23T09:48:20.045Z", "hidden": false}, {"_id": "6972e310fb12c92b735b7474", "name": "Xinyu Zhang", "hidden": false}, {"_id": "6972e310fb12c92b735b7475", "name": "Pei Zhang", "hidden": false}, {"_id": "6972e310fb12c92b735b7476", "user": {"_id": "64b0a77df12b47366663884c", "avatarUrl": "/avatars/a212ea862abb5966060e439dd0e7656f.svg", "isPro": false, "fullname": "Baosong Yang", "user": "Baosong", "type": "user"}, "name": "Baosong Yang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-23T09:48:01.836Z", "hidden": false}, {"_id": "6972e310fb12c92b735b7477", "name": "Jin Xu", "hidden": false}, {"_id": "6972e310fb12c92b735b7478", "name": "Jingren Zhou", "hidden": false}, {"_id": "6972e310fb12c92b735b7479", "user": {"_id": "620760a26e3b7210c2ff1943", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg", "isPro": false, "fullname": "Junyang Lin", "user": "JustinLin610", "type": "user"}, "name": "Junyang Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-23T09:47:54.300Z", "hidden": false}], "publishedAt": "2026-01-22T03:51:43.000Z", "submittedOnDailyAt": "2026-01-23T00:25:20.168Z", "title": "Qwen3-TTS Technical Report", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission (97,ms) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.", "upvotes": 20, "discussionId": "6972e311fb12c92b735b747a", "githubRepo": "https://github.com/QwenLM/Qwen3-TTS", "githubRepoAddedBy": "user", "ai_summary": "The Qwen3-TTS series presents advanced multilingual text-to-speech models with voice cloning and controllable speech generation capabilities, utilizing dual-track LM architecture and specialized speech tokenizers for efficient streaming synthesis.", "ai_keywords": ["text-to-speech", "voice cloning", "dual-track LM architecture", "speech tokenizers", "Qwen-TTS-Tokenizer-25Hz", "Qwen-TTS-Tokenizer-12Hz", "DiT", "ConvNet", "streaming waveform reconstruction", "multilingual", "controllable speech generation"], "githubStars": 2193, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "summary_zh": "<ul>\n    <li>\u62a5\u544a\u4ecb\u7ecd\u4e86Qwen3-TTS\u7cfb\u5217\uff0c\u8fd9\u662f\u4e00\u4e2a\u5148\u8fdb\u7684\u591a\u8bed\u8a00\u3001\u53ef\u63a7\u3001\u7a33\u5b9a\u548c\u6d41\u5f0f\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u3002</li>\n    <li>Qwen3-TTS\u652f\u63013\u79d2\u7684\u58f0\u97f3\u514b\u9686\u548c\u57fa\u4e8e\u63cf\u8ff0\u7684\u63a7\u5236\uff0c\u53ef\u4ee5\u521b\u9020\u5168\u65b0\u7684\u58f0\u97f3\u5e76\u7cbe\u7ec6\u8c03\u6574\u8f93\u51fa\u8bed\u97f3\u3002</li>\n    <li>\u8be5\u6a21\u578b\u8bad\u7ec3\u4e86\u8d85\u8fc7500\u4e07\u5c0f\u65f6\u7684\u8bed\u97f3\u6570\u636e\uff0c\u6db5\u76d610\u79cd\u8bed\u8a00\uff0c\u91c7\u7528\u53cc\u8f68\u9053\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u8fdb\u884c\u5b9e\u65f6\u5408\u6210\u3002</li>\n    <li>\u63d0\u4f9b\u4e86\u4e24\u79cd\u8bed\u97f3\u7f16\u7801\u5668\uff0c\u5206\u522b\u5f3a\u8c03\u8bed\u4e49\u5185\u5bb9\u548c\u8d85\u4f4e\u5ef6\u8fdf\u6d41\u5f0f\u4f20\u8f93\uff0c\u5177\u6709\u51fa\u8272\u7684\u6027\u80fd\u3002</li>\n    <li>\u4e3a\u4e86\u4fc3\u8fdb\u793e\u533a\u7684\u7814\u7a76\u548c\u5f00\u53d1\uff0c\u6a21\u578b\u548c\u7f16\u7801\u5668\u90fd\u5728Apache 2.0\u8bb8\u53ef\u8bc1\u4e0b\u53d1\u5e03\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The report introduces Qwen3-TTS, a series of advanced text-to-speech models that can speak multiple languages and allow for voice control.</li>\n    <li>These models can clone voices in 3 seconds and manipulate speech output based on descriptions.</li>\n    <li>Qwen3-TTS was trained on over 5 million hours of speech data in 10 different languages.</li>\n    <li>It uses a special architecture for real-time speech synthesis and has two types of tokenizers for different streaming needs.</li>\n    <li>The models and tokenizers are available for public use under an open-source license to encourage research and development.</li>\n</ul>"}, "publishedAt": "2026-01-21T22:51:43.000Z", "title": "Qwen3-TTS Technical Report", "summary": "In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission (97,ms) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15621.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 214, "isUserFollowing": false}, "organization": {"_id": "64c8b5837fe12ecd0a7e92eb", "name": "Qwen", "fullname": "Qwen", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/620760a26e3b7210c2ff1943/-s1gyJfvbE1RgO5iBeNOi.png"}, "isAuthorParticipating": false}],
    "week": [{"paper": {"id": "2601.15876", "authors": [{"_id": "6972d8d5fb12c92b735b73a2", "name": "Taofeng Xue", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a3", "user": {"_id": "6801cbd5f06f2d08f3fd5455", "avatarUrl": "/avatars/c728b48f6938c0d7cb6b14011927ede8.svg", "isPro": false, "fullname": "chong.peng", "user": "KleinChong", "type": "user"}, "name": "Chong Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T20:15:18.698Z", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a4", "name": "Mianqiu Huang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a5", "name": "Linsen Guo", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a6", "user": {"_id": "6764279e684ed3b61b2316a4", "avatarUrl": "/avatars/63a6b6c3b9f442b42480679425951187.svg", "isPro": false, "fullname": "SII-TianchengHAN", "user": "GenSouKai", "type": "user"}, "name": "Tiancheng Han", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:30.106Z", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a7", "name": "Haozhe Wang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a8", "name": "Jianing Wang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73a9", "name": "Xiaocheng Zhang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73aa", "name": "Xin Yang", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ab", "name": "Dengchang Zhao", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ac", "name": "Jinrui Ding", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ad", "name": "Xiandi Ma", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73ae", "name": "Yuchen Xie", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73af", "name": "Peng Pei", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73b0", "name": "Xunliang Cai", "hidden": false}, {"_id": "6972d8d5fb12c92b735b73b1", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-01-22T11:36:43.000Z", "submittedOnDailyAt": "2026-01-23T07:54:00.525Z", "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience", "submittedOnDailyBy": {"_id": "6459c7c10aba070266e41bb1", "avatarUrl": "/avatars/2178cac69cf4123db5e85191160f3795.svg", "isPro": false, "fullname": "mqhuang", "user": "LutherXD", "type": "user"}, "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.", "upvotes": 62, "discussionId": "6972d8d5fb12c92b735b73b2", "ai_summary": "EvoCUA introduces an evolutionary approach to computer-use agents that combines autonomous task generation with policy optimization to achieve superior performance in complex, long-horizon tasks.", "ai_keywords": ["computer-use agents", "native computer-use agents", "data generation", "policy optimization", "evolutionary cycle", "verifiable synthesis engine", "executable validators", "sandbox rollouts", "iterative evolving learning", "capability boundaries", "error analysis", "self-correction", "OSWorld benchmark", "foundation models"], "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86EvoCUA\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u578b\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u6a21\u578b\uff0c\u7a81\u7834\u4e86\u9759\u6001\u6570\u636e\u7684\u9650\u5236\u3002</li>\n    <li>EvoCUA\u901a\u8fc7\u751f\u6210\u6570\u636e\u548c\u4f18\u5316\u7b56\u7565\u5f62\u6210\u81ea\u6211\u7ef4\u6301\u7684\u8fdb\u5316\u5faa\u73af\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u9a8c\u8bc1\u7684\u5408\u6210\u5f15\u64ce\uff0c\u80fd\u591f\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u7684\u4efb\u52a1\u548c\u53ef\u6267\u884c\u7684\u9a8c\u8bc1\u5668\u3002</li>\n    <li>EvoCUA\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6210\u529f\u7387\u8fbe\u523056.7%\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u6700\u4f73\u6a21\u578bOpenCUA-72B\u548c\u5176\u4ed6\u95ed\u6e90\u6a21\u578b\u3002</li>\n    <li>\u8fd9\u4e00\u65b9\u6cd5\u5c55\u793a\u4e86\u901a\u8fc7\u7ecf\u9a8c\u5b66\u4e60\u7684\u8fdb\u5316\u8303\u5f0f\u5728\u4e0d\u540c\u89c4\u6a21\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u666e\u904d\u9002\u7528\u6027\uff0c\u63d0\u4f9b\u4e86\u63a8\u52a8\u4ee3\u7406\u80fd\u529b\u53d1\u5c55\u7684\u53ef\u6269\u5c55\u8def\u5f84\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>EvoCUA is a new type of computer-use agent that improves how AI handles complex tasks over time.</li>\n    <li>It uses a unique system that creates new data and improves its own decision-making process instead of just copying existing data.</li>\n    <li>The model can generate diverse tasks and validate them automatically to help it learn better.</li>\n    <li>EvoCUA has been tested and achieved a success rate of 56.7%, outperforming previous models significantly.</li>\n    <li>This approach shows that learning from experience can enhance AI performance, regardless of the model size.</li>\n</ul>"}, "publishedAt": "2026-01-22T06:36:43.000Z", "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience", "summary": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15876.png", "numComments": 1, "submittedBy": {"_id": "6459c7c10aba070266e41bb1", "avatarUrl": "/avatars/2178cac69cf4123db5e85191160f3795.svg", "fullname": "mqhuang", "name": "LutherXD", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.12993", "authors": [{"_id": "69705709a8be625b19c2af1f", "user": {"_id": "6708cbdcf8a1d7b26732c038", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/CN1WHMPKfjQ8wmfwOe0ni.png", "isPro": false, "fullname": "Hao Luo", "user": "Lightet", "type": "user"}, "name": "Hao Luo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T10:15:59.988Z", "hidden": false}, {"_id": "69705709a8be625b19c2af20", "name": "Ye Wang", "hidden": false}, {"_id": "69705709a8be625b19c2af21", "user": {"_id": "640dd700fdeaae139081f598", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg", "isPro": false, "fullname": "Wanpeng Zhang", "user": "zawnpn", "type": "user"}, "name": "Wanpeng Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:19:56.371Z", "hidden": false}, {"_id": "69705709a8be625b19c2af22", "user": {"_id": "64eac1f496f42afd627d439c", "avatarUrl": "/avatars/aa46265122b8a1170f57475494d7922e.svg", "isPro": false, "fullname": "Sipeng Zheng", "user": "sipeng9527", "type": "user"}, "name": "Sipeng Zheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:38.507Z", "hidden": false}, {"_id": "69705709a8be625b19c2af23", "user": {"_id": "66c84a9eab23d3d7dfb2a368", "avatarUrl": "/avatars/b0a50133c6a95ed340dfb462e87820f4.svg", "isPro": false, "fullname": "ziheng xi", "user": "zhenqis123", "type": "user"}, "name": "Ziheng Xi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:45.981Z", "hidden": false}, {"_id": "69705709a8be625b19c2af24", "user": {"_id": "64bdd5cc76a6e2efccb22100", "avatarUrl": "/avatars/5a0edc24283616dafc76ce5ec97ab5a0.svg", "isPro": false, "fullname": "xuchaoyi", "user": "co1one", "type": "user"}, "name": "Chaoyi Xu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:56.014Z", "hidden": false}, {"_id": "69705709a8be625b19c2af25", "user": {"_id": "68872ff6c18b7e1e13115564", "avatarUrl": "/avatars/f908fc3cc89cd81493105359093f299d.svg", "isPro": false, "fullname": "Haiweng Xu", "user": "Seaman05", "type": "user"}, "name": "Haiweng Xu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:50:01.261Z", "hidden": false}, {"_id": "69705709a8be625b19c2af26", "user": {"_id": "644560657a7b94ddc2d445a3", "avatarUrl": "/avatars/09d6447da6ff1bd0b2b00c899c9f1b28.svg", "isPro": false, "fullname": "Haoqi Yuan", "user": "Yaya041", "type": "user"}, "name": "Haoqi Yuan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:50:06.048Z", "hidden": false}, {"_id": "69705709a8be625b19c2af27", "name": "Chi Zhang", "hidden": false}, {"_id": "69705709a8be625b19c2af28", "name": "Yiqing Wang", "hidden": false}, {"_id": "69705709a8be625b19c2af29", "name": "Yicheng Feng", "hidden": false}, {"_id": "69705709a8be625b19c2af2a", "user": {"_id": "67d905c0e27ba28109384f5c", "avatarUrl": "/avatars/26712594ac9d43c8d1a3e75e36b5df16.svg", "isPro": false, "fullname": "Zongqing Lu", "user": "chungtsing", "type": "user"}, "name": "Zongqing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:50:24.833Z", "hidden": false}], "publishedAt": "2026-01-19T12:20:38.000Z", "submittedOnDailyAt": "2026-01-21T02:12:40.880Z", "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization", "submittedOnDailyBy": {"_id": "640dd700fdeaae139081f598", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg", "isPro": false, "fullname": "Wanpeng Zhang", "user": "zawnpn", "type": "user"}, "summary": "We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal \"mother tongue\" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.", "upvotes": 59, "discussionId": "69705709a8be625b19c2af2b", "projectPage": "https://research.beingbeyond.com/being-h05", "githubRepo": "https://github.com/BeingBeyond/Being-H", "githubRepoAddedBy": "user", "ai_summary": "Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.", "ai_keywords": ["Vision-Language-Action", "cross-embodiment generalization", "human-centric learning", "multimodal data", "Unified Action Space", "Mixture-of-Transformers", "Mixture-of-Flow", "manifold-preserving gating", "universal async chunking"], "githubStars": 265, "organization": {"_id": "687a8ba5aedd77694bc94386", "name": "BeingBeyond", "fullname": "BeingBeyond", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/A6vd2S9BqXgtEWHaPy5qW.png"}, "summary_zh": "<ul>\n    <li>Being-H0.5\u662f\u4e00\u4e2a\u57fa\u7840\u7684\u89c6\u89c9-\u8bed\u8a00-\u884c\u52a8\uff08VLA\uff09\u6a21\u578b\uff0c\u65e8\u5728\u589e\u5f3a\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u4e4b\u95f4\u7684\u666e\u9002\u6027\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u4eba\u7c7b\u4e92\u52a8\u8f68\u8ff9\u89c6\u4e3a\u7269\u7406\u4e92\u52a8\u7684\u201c\u6bcd\u8bed\u201d\u3002</li>\n    <li>UniHand-2.0\u662f\u76ee\u524d\u6700\u5927\u7684\u5177\u8eab\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc735,000\u5c0f\u65f6\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u6db5\u76d630\u79cd\u4e0d\u540c\u7684\u673a\u5668\u4eba\u5f62\u6001\u3002</li>\n    <li>\u6a21\u578b\u91c7\u7528\u7edf\u4e00\u7684\u884c\u52a8\u7a7a\u95f4\uff0c\u5c06\u4e0d\u540c\u673a\u5668\u4eba\u7684\u63a7\u5236\u6620\u5c04\u5230\u8bed\u4e49\u5bf9\u9f50\u7684\u69fd\u4e2d\uff0c\u5e2e\u52a9\u4f4e\u8d44\u6e90\u673a\u5668\u4eba\u5b66\u4e60\u6280\u80fd\u3002</li>\n    <li>Being-H0.5\u5728\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u4e94\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8de8\u5f62\u6001\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Being-H0.5 is a new model that helps robots understand and perform actions better across different types of robotic systems.</li>\n    <li>It uses a human-centered approach, treating human interactions as a common language for robots to learn from.</li>\n    <li>UniHand-2.0 is introduced as a large dataset with over 35,000 hours of data from 30 different robots to help train this model.</li>\n    <li>The model includes a unique design that allows different robots to share skills and improve learning efficiency.</li>\n    <li>Being-H0.5 has shown impressive results in tests, outperforming other models and demonstrating strong abilities across various robotic platforms.</li>\n</ul>"}, "publishedAt": "2026-01-19T07:20:38.000Z", "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization", "summary": "We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal \"mother tongue\" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12993.png", "numComments": 1, "submittedBy": {"_id": "640dd700fdeaae139081f598", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg", "fullname": "Wanpeng Zhang", "name": "zawnpn", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11, "isUserFollowing": false}, "organization": {"_id": "687a8ba5aedd77694bc94386", "name": "BeingBeyond", "fullname": "BeingBeyond", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/A6vd2S9BqXgtEWHaPy5qW.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.15165", "authors": [{"_id": "6971933ac1c7409747bf9597", "name": "Zanlin Ni", "hidden": false}, {"_id": "6971933ac1c7409747bf9598", "user": {"_id": "6486dde1f74857df3f1a5828", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg", "isPro": false, "fullname": "Shenzhi Wang", "user": "shenzhi-wang", "type": "user"}, "name": "Shenzhi Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:54.254Z", "hidden": false}, {"_id": "6971933ac1c7409747bf9599", "name": "Yang Yue", "hidden": false}, {"_id": "6971933ac1c7409747bf959a", "name": "Tianyu Yu", "hidden": false}, {"_id": "6971933ac1c7409747bf959b", "name": "Weilin Zhao", "hidden": false}, {"_id": "6971933ac1c7409747bf959c", "name": "Yeguo Hua", "hidden": false}, {"_id": "6971933ac1c7409747bf959d", "name": "Tianyi Chen", "hidden": false}, {"_id": "6971933ac1c7409747bf959e", "name": "Jun Song", "hidden": false}, {"_id": "6971933ac1c7409747bf959f", "name": "Cheng Yu", "hidden": false}, {"_id": "6971933ac1c7409747bf95a0", "name": "Bo Zheng", "hidden": false}, {"_id": "6971933ac1c7409747bf95a1", "name": "Gao Huang", "hidden": false}], "publishedAt": "2026-01-21T16:41:58.000Z", "submittedOnDailyAt": "2026-01-23T00:11:51.141Z", "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models", "submittedOnDailyBy": {"_id": "63987ffb2ceb55aabe0852f3", "avatarUrl": "/avatars/343b796ff6b8906203904e8c620d7eb5.svg", "isPro": false, "fullname": "Zanlin Ni", "user": "nzl-thu", "type": "user"}, "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap", "upvotes": 55, "discussionId": "6971933ac1c7409747bf95a2", "projectPage": "https://nzl-thu.github.io/the-flexibility-trap", "githubRepo": "https://github.com/LeapLabTHU/JustGRPO", "githubRepoAddedBy": "user", "ai_summary": "Arbitrary order generation in diffusion large language models limits reasoning capability by causing premature solution space collapse, making standard policy optimization more effective.", "ai_keywords": ["diffusion large language models", "left-to-right constraint", "token generation", "reinforcement learning", "reasoning potential", "mathematical reasoning", "coding tasks", "combinatorial trajectories", "likelihoods", "Group Relative Policy Optimization", "GRPO", "parallel decoding"], "githubStars": 66, "organization": {"_id": "69719700e3846c07669d13ee", "name": "Tsinghua-LeapLab", "fullname": "Tsinghua-LeapLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63987ffb2ceb55aabe0852f3/hflTWNTGxeJx83xNkYrDB.png"}, "summary_zh": "<ul>\n    <li>\u6269\u6563\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u6253\u7834\u4e86\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u7684\u4e25\u683c\u751f\u6210\u987a\u5e8f\u9650\u5236\uff0c\u8ba9\u751f\u6210\u7684\u987a\u5e8f\u66f4\u52a0\u7075\u6d3b\u3002</li>\n    <li>\u8fd9\u79cd\u7075\u6d3b\u6027\u672c\u5e94\u63d0\u9ad8\u6a21\u578b\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5b9e\u9645\u60c5\u51b5\u5374\u76f8\u53cd\u3002</li>\n    <li>dLLMs\u5f80\u5f80\u5229\u7528\u8fd9\u79cd\u7075\u6d3b\u6027\u6765\u8df3\u8fc7\u4e00\u4e9b\u9ad8\u4e0d\u786e\u5b9a\u6027\u7684\u5173\u952etoken\uff0c\u4ece\u800c\u5bfc\u81f4\u89e3\u51b3\u7a7a\u95f4\u7684\u63d0\u524d\u6536\u7f29\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5JustGRPO\uff0c\u901a\u8fc7\u653e\u5f03\u4efb\u610f\u987a\u5e8f\u751f\u6210\uff0c\u91c7\u7528\u6807\u51c6\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u5f15\u5bfc\u63a8\u7406\u3002</li>\n    <li>JustGRPO\u5728GSM8K\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e8689.1%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u7559\u4e86dLLMs\u7684\u5e76\u884c\u89e3\u7801\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion Large Language Models (dLLMs) can generate tokens in any order, unlike traditional models that go left-to-right.</li>\n    <li>This flexibility seems to improve reasoning for tasks like math and coding, but it actually limits dLLMs' reasoning abilities.</li>\n    <li>dLLMs often avoid important tokens that help with problem exploration, leading to a reduced range of solutions.</li>\n    <li>Current reinforcement learning methods for dLLMs struggle with this flexibility and complexity.</li>\n    <li>Using a simpler approach called JustGRPO improves reasoning while still allowing parallel token generation, achieving high accuracy on tests.</li>\n</ul>"}, "publishedAt": "2026-01-21T11:41:58.000Z", "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models", "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15165.png", "numComments": 1, "submittedBy": {"_id": "63987ffb2ceb55aabe0852f3", "avatarUrl": "/avatars/343b796ff6b8906203904e8c620d7eb5.svg", "fullname": "Zanlin Ni", "name": "nzl-thu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "69719700e3846c07669d13ee", "name": "Tsinghua-LeapLab", "fullname": "Tsinghua-LeapLab", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63987ffb2ceb55aabe0852f3/hflTWNTGxeJx83xNkYrDB.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.14724", "authors": [{"_id": "6972ee7afb12c92b735b74b4", "user": {"_id": "637169557a5e5d8efdc3e58e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg", "isPro": false, "fullname": "Haowei Zhang", "user": "freesky", "type": "user"}, "name": "Haowei Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:23.639Z", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b5", "name": "Shudong Yang", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b6", "name": "Jinlan Fu", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b7", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6972ee7afb12c92b735b74b8", "name": "Xipeng Qiu", "hidden": false}], "publishedAt": "2026-01-21T07:26:15.000Z", "submittedOnDailyAt": "2026-01-23T01:43:37.582Z", "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding", "submittedOnDailyBy": {"_id": "637169557a5e5d8efdc3e58e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg", "isPro": false, "fullname": "Haowei Zhang", "user": "freesky", "type": "user"}, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10times faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.", "upvotes": 52, "discussionId": "6972ee7bfb12c92b735b74b9", "projectPage": "https://hermes-streaming.github.io/", "githubRepo": "https://github.com/haowei-freesky/HERMES", "githubRepoAddedBy": "user", "ai_summary": "HERMES is a training-free architecture that enables real-time video stream understanding by utilizing a hierarchical memory framework based on KV cache reuse, achieving faster response times and maintained accuracy even with reduced video token input.", "ai_keywords": ["Multimodal Large Language Models", "video understanding", "streaming video inputs", "real-time responses", "KV cache", "hierarchical memory framework", "mechanistic attention", "video tokens", "TTFT"], "githubStars": 24, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u6700\u8fd1\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u79bb\u7ebf\u89c6\u9891\u7406\u89e3\u65b9\u9762\u6709\u663e\u8457\u8fdb\u5c55\u3002</li>\n    <li>\u5c06\u8fd9\u4e9b\u80fd\u529b\u6269\u5c55\u5230\u6d41\u5a92\u4f53\u89c6\u9891\u8f93\u5165\u4ecd\u7136\u5f88\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u7406\u89e3\u6027\u80fd\u7a33\u5b9a\u3001\u5b9e\u65f6\u54cd\u5e94\u548c\u4f4eGPU\u5185\u5b58\u4f7f\u7528\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51faHERMES\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u8bad\u7ec3\u67b6\u6784\uff0c\u7528\u4e8e\u5b9e\u65f6\u51c6\u786e\u7406\u89e3\u89c6\u9891\u6d41\u3002</li>\n    <li>HERMES\u5229\u7528\u7d27\u51d1\u7684KV\u7f13\u5b58\uff0c\u5b9e\u73b0\u8d44\u6e90\u53d7\u9650\u4e0b\u7684\u9ad8\u6548\u6d41\u5a92\u4f53\u7406\u89e3\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8ba1\u7b97\uff0c\u786e\u4fdd\u5b9e\u65f6\u54cd\u5e94\u3002</li>\n    <li>HERMES\u5728\u51cf\u5c11\u89c6\u9891\u6570\u636e\u91cf\u7684\u540c\u65f6\uff0c\u4f9d\u7136\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6d41\u5a92\u4f53\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e86\u6700\u9ad811.4%\u7684\u51c6\u786e\u7387\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Recent improvements in Multimodal Large Language Models (MLLMs) have enhanced offline video understanding, but real-time streaming video remains difficult.</li>\n    <li>HERMES is a new architecture that allows for fast and accurate understanding of video streams without needing extra training.</li>\n    <li>It uses a special memory system to efficiently manage video information and responds quickly to user queries.</li>\n    <li>HERMES is 10 times faster in processing than previous models and maintains high accuracy even with fewer video tokens.</li>\n    <li>It shows improvements in accuracy on streaming datasets, with gains of up to 11.4% compared to earlier methods.</li>\n</ul>"}, "publishedAt": "2026-01-21T02:26:15.000Z", "title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10times faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14724.png", "numComments": 2, "submittedBy": {"_id": "637169557a5e5d8efdc3e58e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668515232215-637169557a5e5d8efdc3e58e.jpeg", "fullname": "Haowei Zhang", "name": "freesky", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.15197", "authors": [{"_id": "6971c608c1c7409747bf96a5", "user": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "isPro": false, "fullname": "Shijie Lian", "user": "LiamLian0727", "type": "user"}, "name": "Shijie Lian", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:44:25.547Z", "hidden": false}, {"_id": "6971c608c1c7409747bf96a6", "name": "Bin Yu", "hidden": false}, {"_id": "6971c608c1c7409747bf96a7", "name": "Xiaopeng Lin", "hidden": false}, {"_id": "6971c608c1c7409747bf96a8", "name": "Laurence T. Yang", "hidden": false}, {"_id": "6971c608c1c7409747bf96a9", "name": "Zhaolong Shen", "hidden": false}, {"_id": "6971c608c1c7409747bf96aa", "name": "Changti Wu", "hidden": false}, {"_id": "6971c608c1c7409747bf96ab", "name": "Yuzhuo Miao", "hidden": false}, {"_id": "6971c608c1c7409747bf96ac", "name": "Cong Huang", "hidden": false}, {"_id": "6971c608c1c7409747bf96ad", "name": "Kai Chen", "hidden": false}], "publishedAt": "2026-01-21T17:15:22.000Z", "submittedOnDailyAt": "2026-01-23T00:45:20.588Z", "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries", "submittedOnDailyBy": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "isPro": false, "fullname": "Shijie Lian", "user": "LiamLian0727", "type": "user"}, "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior \u03c0(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.", "upvotes": 50, "discussionId": "6971c609c1c7409747bf96ae", "projectPage": "https://github.com/ZGC-EmbodyAI/BayesianVLA", "githubRepo": "https://github.com/ZGC-EmbodyAI/BayesianVLA", "githubRepoAddedBy": "user", "ai_summary": "BayesianVLA addresses language-action grounding issues in robot manipulation by using Bayesian decomposition to prevent information collapse and improve out-of-distribution generalization.", "ai_keywords": ["Vision-Language-Action models", "Information Collapse", "Bayesian decomposition", "latent action queries", "conditional Pointwise Mutual Information", "vision-only policies", "out-of-distribution generalization", "SimplerEnv", "RoboCasa"], "githubStars": 11, "organization": {"_id": "68896d3a716ee5bfb1428441", "name": "ZGCA", "fullname": "Zhongguancun Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"}, "summary_zh": "<ul>\n    <li>VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e0a\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u65b0\u6307\u4ee4\u548c\u590d\u6742\u591a\u4efb\u52a1\u573a\u666f\u4e2d\u5e38\u5e38\u96be\u4ee5\u6cdb\u5316\u3002</li>\n    <li>\u5f53\u524d\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u504f\u5dee\uff0c\u5bfc\u81f4\u6307\u4ee4\u548c\u89c6\u89c9\u89c2\u5bdf\u4e4b\u95f4\u7684\u6761\u4ef6\u4e92\u4fe1\u606f\u6d88\u5931\uff0c\u79f0\u4e3a\u4fe1\u606f\u5d29\u6e83\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86BayesianVLA\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u5206\u89e3\u6765\u5f3a\u5236\u9075\u5faa\u6307\u4ee4\u3002</li>\n    <li>\u8be5\u6846\u67b6\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u6f5c\u5728\u52a8\u4f5c\u67e5\u8be2\uff0c\u5efa\u7acb\u53cc\u5206\u652f\u67b6\u6784\uff0c\u4f18\u5316\u653f\u7b56\u4ee5\u6700\u5927\u5316\u52a8\u4f5c\u4e0e\u6307\u4ee4\u4e4b\u95f4\u7684\u6761\u4ef6\u4e92\u4fe1\u606f\u3002</li>\n    <li>BayesianVLA\u5728\u4e0d\u540c\u73af\u5883\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728OOD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u8fbe\u523011.3%\u7684\u6539\u5584\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-Language-Action (VLA) models are useful for robot manipulation but often fail with new instructions or complex tasks.</li>\n    <li>Current training methods create biased datasets where language instructions can be easily guessed from visuals, leading to \"Information Collapse.\" This makes models rely only on visual cues and ignore language.</li>\n    <li>To solve this, the authors propose a new framework called BayesianVLA that encourages models to follow instructions better.</li>\n    <li>BayesianVLA uses a two-part system to analyze both visual data and language instructions, improving how actions are linked to commands.</li>\n    <li>Tests show BayesianVLA enhances performance significantly, with an 11.3% improvement in challenging scenarios, proving it helps models understand language in actions better.</li>\n</ul>"}, "publishedAt": "2026-01-21T12:15:22.000Z", "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries", "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior p(a mid v) and a language-conditioned posterior \u03c0(a mid v, ell). We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15197.png", "numComments": 2, "submittedBy": {"_id": "65ec01fd770aa0e25d9374dc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ec01fd770aa0e25d9374dc/yvLWwBEdAdHb-8EdUHg3n.jpeg", "fullname": "Shijie Lian", "name": "LiamLian0727", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "68896d3a716ee5bfb1428441", "name": "ZGCA", "fullname": "Zhongguancun Academy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6854c3ab09a3ba7d16243875/aZ3tp3lZk1yQoXDwSklye.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16206", "authors": [{"_id": "6972e04dfb12c92b735b73cf", "user": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "isPro": false, "fullname": "Daixuan Cheng", "user": "daixuancheng", "type": "user"}, "name": "Daixuan Cheng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:28.070Z", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d0", "name": "Shaohan Huang", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d1", "name": "Yuxian Gu", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d2", "name": "Huatong Song", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d3", "name": "Guoxin Chen", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d4", "name": "Li Dong", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d5", "name": "Wayne Xin Zhao", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d6", "name": "Ji-Rong Wen", "hidden": false}, {"_id": "6972e04dfb12c92b735b73d7", "name": "Furu Wei", "hidden": false}], "publishedAt": "2026-01-22T18:57:09.000Z", "submittedOnDailyAt": "2026-01-23T00:27:12.305Z", "title": "LLM-in-Sandbox Elicits General Agentic Intelligence", "submittedOnDailyBy": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "isPro": false, "fullname": "Daixuan Cheng", "user": "daixuancheng", "type": "user"}, "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.", "upvotes": 46, "discussionId": "6972e04dfb12c92b735b73d8", "projectPage": "https://llm-in-sandbox.github.io", "githubRepo": "https://github.com/llm-in-sandbox/llm-in-sandbox", "githubRepoAddedBy": "user", "ai_summary": "LLM-in-Sandbox enables large language models to perform general intelligence tasks across diverse domains by allowing them to explore a code sandbox environment, achieving robust generalization without additional training.", "ai_keywords": ["LLM-in-Sandbox", "code sandbox", "virtual computer", "reinforcement learning", "non-agentic data", "sandbox exploration", "general intelligence", "long-context understanding", "instruction following"], "githubStars": 38, "organization": {"_id": "68151d0f51add3813f3f7d1b", "name": "MicrosoftResearch", "fullname": "Microsoft Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"}, "summary_zh": "<ul>\n    <li>\u4ecb\u7ecd\u4e86LLM-in-Sandbox\uff0c\u5141\u8bb8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u6c99\u76d2\u4e2d\u63a2\u7d22\uff0c\u4ee5\u589e\u5f3a\u5176\u5728\u975e\u4ee3\u7801\u9886\u57df\u7684\u901a\u7528\u667a\u80fd\u3002</li>\n    <li>\u5f3a\u5927\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6ca1\u6709\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u5229\u7528\u4ee3\u7801\u6c99\u76d2\u5904\u7406\u975e\u4ee3\u7801\u4efb\u52a1\u3002</li>\n    <li>\u6a21\u578b\u53ef\u4ee5\u81ea\u53d1\u8bbf\u95ee\u5916\u90e8\u8d44\u6e90\u83b7\u53d6\u65b0\u77e5\u8bc6\uff0c\u4f7f\u7528\u6587\u4ef6\u7cfb\u7edf\u5904\u7406\u957f\u4e0a\u4e0b\u6587\uff0c\u5e76\u6267\u884c\u811a\u672c\u6ee1\u8db3\u683c\u5f0f\u8981\u6c42\u3002</li>\n    <li>\u901a\u8fc7LLM-in-Sandbox\u5f3a\u5316\u5b66\u4e60\uff0c\u53ef\u4ee5\u8fdb\u4e00\u6b65\u589e\u5f3a\u6a21\u578b\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u4f7f\u7528\u975e\u667a\u80fd\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002</li>\n    <li>\u5b9e\u9a8c\u663e\u793aLLM-in-Sandbox\u5728\u591a\u4e2a\u9886\u57df\uff08\u5982\u6570\u5b66\u3001\u7269\u7406\u3001\u5316\u5b66\u7b49\uff09\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u5316\u80fd\u529b\uff0c\u5e76\u5df2\u5f00\u6e90\u4e3aPython\u5305\u4ee5\u4fbf\u5b9e\u9645\u5e94\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>LLM-in-Sandbox allows language models (LLMs) to work in a virtual environment to improve their problem-solving skills in various subjects.</li>\n    <li>These LLMs can access external information and use file systems to manage large amounts of data without needing extra training.</li>\n    <li>Using a special training method called LLM-in-Sandbox-RL, the models can improve their skills in exploring the sandbox.</li>\n    <li>Experiments show that LLM-in-Sandbox can effectively handle tasks in areas like math, science, and following instructions.</li>\n    <li>The tool is efficient and has been made available as an open-source Python package for practical use.</li>\n</ul>"}, "publishedAt": "2026-01-22T13:57:09.000Z", "title": "LLM-in-Sandbox Elicits General Agentic Intelligence", "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16206.png", "numComments": 2, "submittedBy": {"_id": "649e6761f9134a06ed1e0cea", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/649e6761f9134a06ed1e0cea/XNeKceE8xSwI0xWwWUwwJ.jpeg", "fullname": "Daixuan Cheng", "name": "daixuancheng", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 16, "isUserFollowing": false}, "organization": {"_id": "68151d0f51add3813f3f7d1b", "name": "MicrosoftResearch", "fullname": "Microsoft Research", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6529a4f2f1205983224fa513/PeuVr7jSuJflmDBBGxoDX.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.16208", "authors": [{"_id": "6972ea8bfb12c92b735b74a8", "name": "Shengbang Tong", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74a9", "name": "Boyang Zheng", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74aa", "user": {"_id": "64249f76d476e4ad55665d59", "avatarUrl": "/avatars/a0fec7e423ffae944a874ca267b55c1f.svg", "isPro": false, "fullname": "Ziteng Wang", "user": "AustinWang0330", "type": "user"}, "name": "Ziteng Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:01.136Z", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ab", "name": "Bingda Tang", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ac", "name": "Nanye Ma", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ad", "user": {"_id": "626dc5105f7327906f0b2a4e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626dc5105f7327906f0b2a4e/QCSzuwYqsv8ozRnusVb-F.jpeg", "isPro": true, "fullname": "Ellis Brown", "user": "ellisbrown", "type": "user"}, "name": "Ellis Brown", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T20:15:10.637Z", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74ae", "name": "Jihan Yang", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74af", "name": "Rob Fergus", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74b0", "name": "Yann LeCun", "hidden": false}, {"_id": "6972ea8bfb12c92b735b74b1", "name": "Saining Xie", "hidden": false}], "publishedAt": "2026-01-22T18:58:16.000Z", "submittedOnDailyAt": "2026-01-23T00:57:17.761Z", "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "submittedOnDailyBy": {"_id": "6434226da4c9c55871a78052", "avatarUrl": "/avatars/3309832b3115bc6ad08ae1d10f43118b.svg", "isPro": false, "fullname": "BoYang Zheng", "user": "bytetriper", "type": "user"}, "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.", "upvotes": 40, "discussionId": "6972ea8cfb12c92b735b74b2", "projectPage": "https://rae-dit.github.io/scale-rae/", "githubRepo": "https://github.com/ZitengWangNYU/Scale-RAE", "githubRepoAddedBy": "user", "ai_summary": "Representation Autoencoders (RAEs) demonstrate superior performance over VAEs in large-scale text-to-image generation, showing improved stability, faster convergence, and better quality while enabling unified multimodal reasoning in shared representation spaces.", "ai_keywords": ["representation autoencoders", "diffusion modeling", "semantic latent spaces", "text-to-image generation", "frozen representation encoder", "SigLIP-2", "noise scheduling", "diffusion transformers", "pretraining", "finetuning", "catastrophic overfitting", "multimodal model", "shared representation space"], "githubStars": 58, "organization": {"_id": "662741612ada5b77e310d171", "name": "nyu-visionx", "fullname": "VISIONx @ NYU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"}, "summary_zh": "<ul>\n    <li>\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u8868\u793a\u81ea\u7f16\u7801\u5668\uff08RAE\uff09\u5728\u5927\u89c4\u6a21\u81ea\u7531\u5f62\u5f0f\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5e94\u7528\u3002</li>\n    <li>\u6211\u4eec\u53d1\u73b0\uff0c\u5c3d\u7ba1\u6269\u5927\u89c4\u6a21\u63d0\u9ad8\u4e86\u6574\u4f53\u4fdd\u771f\u5ea6\uff0c\u4f46\u7279\u5b9a\u9886\u57df\uff08\u5982\u6587\u672c\uff09\u7684\u6570\u636e\u7ec4\u6210\u4ecd\u7136\u81f3\u5173\u91cd\u8981\u3002</li>\n    <li>RAE\u6a21\u578b\u5728\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u4e0b\uff08\u4ece0.5B\u52309.8B\uff09\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684FLUX VAE\u6a21\u578b\u3002</li>\n    <li>\u5728\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u5fae\u8c03\u8fc7\u7a0b\u4e2d\uff0cRAE\u6a21\u578b\u6bd4VAE\u6a21\u578b\u5728\u591a\u4e2a\u5468\u671f\u540e\u8868\u73b0\u66f4\u7a33\u5b9a\uff0c\u4e14\u6027\u80fd\u66f4\u597d\u3002</li>\n    <li>RAE\u6a21\u578b\u7684\u591a\u6a21\u6001\u7279\u6027\u4f7f\u5176\u80fd\u591f\u5728\u5171\u4eab\u8868\u793a\u7a7a\u95f4\u4e2d\u76f4\u63a5\u5904\u7406\u751f\u6210\u7684\u6f5c\u53d8\u91cf\uff0c\u5f00\u8f9f\u4e86\u7edf\u4e00\u6a21\u578b\u7684\u65b0\u53ef\u80fd\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Representation Autoencoders (RAEs) are effective for generating images from text and work well in high-dimensional spaces.</li>\n    <li>Researchers tested RAEs using large datasets and found that while size helps, using the right data is crucial for specific tasks like text generation.</li>\n    <li>They discovered that simplifying the RAE design made it more efficient, as some complex features offered little improvement at larger scales.</li>\n    <li>RAEs outperformed traditional Variational Autoencoders (VAEs) in multiple tests, showing better stability and quality during training.</li>\n    <li>RAEs enable new possibilities for models that understand and generate visuals by sharing representation spaces.</li>\n</ul>"}, "publishedAt": "2026-01-22T13:58:16.000Z", "title": "Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders", "summary": "Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16208.png", "numComments": 1, "submittedBy": {"_id": "6434226da4c9c55871a78052", "avatarUrl": "/avatars/3309832b3115bc6ad08ae1d10f43118b.svg", "fullname": "BoYang Zheng", "name": "bytetriper", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "662741612ada5b77e310d171", "name": "nyu-visionx", "fullname": "VISIONx @ NYU", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/626dc5105f7327906f0b2a4e/Kn-QtZjE6TJE-syTndXIW.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.15892", "authors": [{"_id": "6972d788fb12c92b735b7397", "user": {"_id": "641aa5e391e3376a057bbd4c", "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg", "isPro": false, "fullname": "Chenghao Fan", "user": "Facico", "type": "user"}, "name": "Chenghao Fan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-23T09:38:32.109Z", "hidden": false}, {"_id": "6972d788fb12c92b735b7398", "name": "Wen Heng", "hidden": false}, {"_id": "6972d788fb12c92b735b7399", "name": "Bo Li", "hidden": false}, {"_id": "6972d788fb12c92b735b739a", "name": "Sichen Liu", "hidden": false}, {"_id": "6972d788fb12c92b735b739b", "name": "Yuxuan Song", "hidden": false}, {"_id": "6972d788fb12c92b735b739c", "name": "Jing Su", "hidden": false}, {"_id": "6972d788fb12c92b735b739d", "name": "Xiaoye Qu", "hidden": false}, {"_id": "6972d788fb12c92b735b739e", "name": "Kai Shen", "hidden": false}, {"_id": "6972d788fb12c92b735b739f", "name": "Wei Wei", "hidden": false}], "publishedAt": "2026-01-22T12:13:17.000Z", "submittedOnDailyAt": "2026-01-23T00:09:35.389Z", "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "submittedOnDailyBy": {"_id": "641aa5e391e3376a057bbd4c", "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg", "isPro": false, "fullname": "Chenghao Fan", "user": "Facico", "type": "user"}, "summary": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.", "upvotes": 40, "discussionId": "6972d788fb12c92b735b73a0", "projectPage": "https://bytedance-seed.github.io/Stable-DiffCoder/", "githubRepo": "https://github.com/ByteDance-Seed/Stable-DiffCoder", "githubRepoAddedBy": "user", "ai_summary": "Stable-DiffCoder demonstrates superior code modeling performance compared to autoregressive baselines through block diffusion continual pretraining and efficient training mechanisms.", "ai_keywords": ["diffusion-based language models", "autoregressive models", "block diffusion", "continual pretraining", "warmup", "clipped noise schedule", "supervised fine-tuning", "code modeling", "structured code modeling", "data augmentation"], "githubStars": 16, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u6269\u6563\u57fa\u7840\u8bed\u8a00\u6a21\u578b\uff08DLLMs\uff09\u76f8\u6bd4\u81ea\u56de\u5f52\u6a21\u578b\uff08AR\uff09\uff0c\u5728\u751f\u6210\u65b9\u5f0f\u548c\u6570\u636e\u91cd\u7528\u4e0a\u66f4\u6709\u4f18\u52bf\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86Stable-DiffCoder\uff0c\u8fd9\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u5757\u6269\u6563\u4ee3\u7801\u6a21\u578b\uff0c\u4f7f\u7528\u4e86Seed-Coder\u67b6\u6784\u548c\u8bad\u7ec3\u6d41\u7a0b\u3002</li>\n    <li>\u4e3a\u4e86\u63d0\u9ad8\u77e5\u8bc6\u5b66\u4e60\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5f15\u5165\u4e86\u5757\u6269\u6563\u6301\u7eed\u9884\u8bad\u7ec3\uff08CPT\uff09\u9636\u6bb5\u3002</li>\n    <li>\u5728\u76f8\u540c\u7684\u6570\u636e\u548c\u67b6\u6784\u4e0b\uff0cStable-DiffCoder\u5728\u591a\u4e2a\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5176\u81ea\u56de\u5f52\u6a21\u578b\u3002</li>\n    <li>\u6269\u6563\u57fa\u7840\u7684\u5efa\u6a21\u65b9\u6cd5\u63d0\u5347\u4e86\u4ee3\u7801\u7684\u7f16\u8f91\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u5e2e\u52a9\u4f4e\u8d44\u6e90\u7f16\u7a0b\u8bed\u8a00\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Diffusion-based language models (DLLMs) can generate code more efficiently than autoregressive (AR) models.</li>\n    <li>Stable-DiffCoder is a new type of DLLM that builds on existing architecture and training methods.</li>\n    <li>It includes a special training phase to enhance learning and stability.</li>\n    <li>Stable-DiffCoder performs better than AR models on various coding tests, even with less data.</li>\n    <li>It also aids in editing and reasoning about code and helps with less common programming languages.</li>\n</ul>"}, "publishedAt": "2026-01-22T07:13:17.000Z", "title": "Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model", "summary": "Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of \\~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15892.png", "numComments": 0, "submittedBy": {"_id": "641aa5e391e3376a057bbd4c", "avatarUrl": "/avatars/5818797f27444fde078b503774ee081c.svg", "fullname": "Chenghao Fan", "name": "Facico", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 13, "isUserFollowing": false}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.15282", "authors": [{"_id": "69719a70c1c7409747bf9601", "user": {"_id": "68fce03ed1d0efce7ca87075", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68fce03ed1d0efce7ca87075/GRKTeVIaLZD_M-KoJE8YF.png", "isPro": false, "fullname": "yfdeng", "user": "yfdeng10", "type": "user"}, "name": "Yufan Deng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:44:51.909Z", "hidden": false}, {"_id": "69719a70c1c7409747bf9602", "name": "Zilin Pan", "hidden": false}, {"_id": "69719a70c1c7409747bf9603", "name": "Hongyu Zhang", "hidden": false}, {"_id": "69719a70c1c7409747bf9604", "name": "Xiaojie Li", "hidden": false}, {"_id": "69719a70c1c7409747bf9605", "name": "Ruoqing Hu", "hidden": false}, {"_id": "69719a70c1c7409747bf9606", "user": {"_id": "6661917459720067b2a15bd6", "avatarUrl": "/avatars/f1afe7dd1c538d209016eb5740772d8b.svg", "isPro": false, "fullname": "dyflional10", "user": "dyflional10", "type": "user"}, "name": "Yufei Ding", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:44:59.616Z", "hidden": true}, {"_id": "69719a70c1c7409747bf9607", "name": "Yiming Zou", "hidden": false}, {"_id": "69719a70c1c7409747bf9608", "name": "Yan Zeng", "hidden": false}, {"_id": "69719a70c1c7409747bf9609", "name": "Daquan Zhou", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/l2EDevN63IoqsgFC9Xn13.mp4"], "publishedAt": "2026-01-21T18:59:18.000Z", "submittedOnDailyAt": "2026-01-22T01:05:21.353Z", "title": "Rethinking Video Generation Model for the Embodied World", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.", "upvotes": 36, "discussionId": "69719a70c1c7409747bf960a", "projectPage": "https://dagroup-pku.github.io/ReVidgen.github.io/", "githubRepo": "https://github.com/DAGroup-PKU/ReVidgen/", "githubRepoAddedBy": "user", "ai_summary": "A comprehensive robotics benchmark evaluates video generation models across multiple task domains and robot embodiments, revealing significant gaps in physical realism and introducing a large-scale dataset to address training data limitations.", "ai_keywords": ["video generation models", "embodied intelligence", "robotics benchmark", "robot-oriented video generation", "task domains", "physical plausibility", "action completeness", "Spearman correlation coefficient", "RoVid-X", "data pipeline", "robotic dataset", "video models", "embodied AI", "general intelligence"], "githubStars": 7, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u673a\u5668\u4eba\u667a\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\uff0c\u53ef\u4ee5\u751f\u6210\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u6570\u636e\u3002</li>\n    <li>\u76ee\u524d\u9ad8\u8d28\u91cf\u89c6\u9891\u7684\u5408\u6210\u4ecd\u7136\u9762\u4e34\u6311\u6218\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u57fa\u51c6\u9650\u5236\u4e86\u6bd4\u8f83\u548c\u8fdb\u6b65\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u673a\u5668\u4eba\u57fa\u51c6RBench\uff0c\u8bc4\u4f30\u4e94\u4e2a\u4efb\u52a1\u9886\u57df\u548c\u56db\u79cd\u4e0d\u540c\u7684\u673a\u5668\u4eba\u8868\u73b0\u3002</li>\n    <li>RBench\u901a\u8fc7\u53ef\u91cd\u590d\u7684\u5b50\u6307\u6807\u8bc4\u4f30\u4efb\u52a1\u6b63\u786e\u6027\u548c\u89c6\u89c9\u771f\u5b9e\u6027\uff0c\u663e\u793a\u51fa\u8bb8\u591a\u6a21\u578b\u5728\u751f\u6210\u771f\u5b9e\u673a\u5668\u4eba\u884c\u4e3a\u65b9\u9762\u7684\u4e0d\u8db3\u3002</li>\n    <li>\u6211\u4eec\u8fd8\u5f15\u5165\u4e86RoVid-X\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u6700\u5927\u7684\u5f00\u6e90\u673a\u5668\u4eba\u89c6\u9891\u751f\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b400\u4e07\u4e2a\u6807\u6ce8\u89c6\u9891\u7247\u6bb5\uff0c\u652f\u6301\u52a0\u901f\u89c6\u9891\u6a21\u578b\u7684\u8bc4\u4f30\u548c\u8bad\u7ec3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video generation models are improving robot intelligence by creating diverse data that mimic real-world interactions.</li>\n    <li>There is a need for high-quality video generation that reflects how robots behave in reality, but current efforts lack standard benchmarks for fair comparison.</li>\n    <li>The new RBench benchmark evaluates robot video generation across various tasks and measures quality through specific metrics like structural consistency and action completeness.</li>\n    <li>Evaluation of 25 models using RBench shows a significant lack of realistic robot behavior, but RBench correlates highly with human assessments, proving its effectiveness.</li>\n    <li>To improve video generation, a new dataset, RoVid-X, has been created with 4 million annotated video clips, providing valuable training data for developing better robotic models.</li>\n</ul>"}, "publishedAt": "2026-01-21T13:59:18.000Z", "title": "Rethinking Video Generation Model for the Embodied World", "summary": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6039478ab3ecf716b1a5fd4d/l2EDevN63IoqsgFC9Xn13.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.15282.png", "numComments": 0, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 211, "isUserFollowing": false}, "organization": {"_id": "67d1140985ea0644e2f14b99", "name": "ByteDance-Seed", "fullname": "ByteDance Seed", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/flkDUqd_YEuFsjeNET3r-.png"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.14171", "authors": [{"_id": "69710b60c1c7409747bf9431", "user": {"_id": "6448b2f53e7b3c11be684348", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg", "isPro": true, "fullname": "Qianli Ma", "user": "Mqleet", "type": "user"}, "name": "Qianli Ma", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:46:28.336Z", "hidden": false}, {"_id": "69710b60c1c7409747bf9432", "name": "Chang Guo", "hidden": false}, {"_id": "69710b60c1c7409747bf9433", "name": "Zhiheng Tian", "hidden": false}, {"_id": "69710b60c1c7409747bf9434", "name": "Siyu Wang", "hidden": false}, {"_id": "69710b60c1c7409747bf9435", "name": "Jipeng Xiao", "hidden": false}, {"_id": "69710b60c1c7409747bf9436", "name": "Yuanhao Yue", "hidden": false}, {"_id": "69710b60c1c7409747bf9437", "name": "Zhipeng Zhang", "hidden": false}], "publishedAt": "2026-01-20T17:23:51.000Z", "submittedOnDailyAt": "2026-01-22T01:11:23.304Z", "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance", "submittedOnDailyBy": {"_id": "6448b2f53e7b3c11be684348", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg", "isPro": true, "fullname": "Qianli Ma", "user": "Mqleet", "type": "user"}, "summary": "Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce RebuttalAgent, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, RebuttalAgent ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed RebuttalBench and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.", "upvotes": 35, "discussionId": "69710b60c1c7409747bf9438", "projectPage": "https://mqleet.github.io/Paper2Rebuttal_ProjectPage/", "githubRepo": "https://github.com/AutoLab-SAI-SJTU/Paper2Rebuttal", "githubRepoAddedBy": "user", "ai_summary": "RebuttalAgent is a multi-agent framework that reframes rebuttal generation as an evidence-centric planning task, improving coverage, faithfulness, and strategic coherence in academic peer review.", "ai_keywords": ["multi-agents framework", "evidence-centric planning", "rebuttal generation", "peer review", "strategic coherence", "faithful generation", "external search module"], "githubStars": 146, "organization": {"_id": "68ee0edd23dc954f7744ac27", "name": "AutoLab-SJTU", "fullname": "AutoLab"}, "summary_zh": "<ul>\n    <li>\u64b0\u5199\u6709\u6548\u7684\u53cd\u9a73\u9700\u8981\u7cbe\u51c6\u5bf9\u9f50\u5ba1\u7a3f\u610f\u56fe\u548c\u7a3f\u4ef6\u7ec6\u8282\uff0c\u4e0d\u4ec5\u4ec5\u662f\u8bed\u8a00\u6d41\u5229\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u53cd\u9a73\u751f\u6210\u89c6\u4e3a\u76f4\u63a5\u6587\u672c\u751f\u6210\u95ee\u9898\uff0c\u5bb9\u6613\u51fa\u73b0\u9519\u8bef\u3001\u5ffd\u7565\u6279\u8bc4\u548c\u7f3a\u4e4f\u9a8c\u8bc1\u4f9d\u636e\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86RebuttalAgent\uff0c\u8fd9\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u5c06\u53cd\u9a73\u751f\u6210\u91cd\u6784\u4e3a\u4ee5\u8bc1\u636e\u4e3a\u4e2d\u5fc3\u7684\u89c4\u5212\u4efb\u52a1\u3002</li>\n    <li>\u8be5\u7cfb\u7edf\u5c06\u590d\u6742\u53cd\u9988\u5206\u89e3\u4e3a\u57fa\u672c\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7ed3\u5408\u9ad8\u4fdd\u771f\u6587\u672c\u548c\u538b\u7f29\u6458\u8981\u6784\u5efa\u52a8\u6001\u4e0a\u4e0b\u6587\u3002</li>\n    <li>RebuttalAgent\u5728\u751f\u6210\u54cd\u5e94\u8ba1\u5212\u65f6\u786e\u4fdd\u6bcf\u4e2a\u8bba\u70b9\u90fd\u6709\u5185\u90e8\u6216\u5916\u90e8\u8bc1\u636e\u652f\u6301\uff0c\u5c55\u793a\u4e86\u5728\u8986\u76d6\u7387\u3001\u771f\u5b9e\u6027\u548c\u6218\u7565\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Writing rebuttals is challenging and requires matching the reviewer's intent with the details of the manuscript.</li>\n    <li>Current methods for generating rebuttals often create inaccurate responses and miss important critiques.</li>\n    <li>RebuttalAgent is a new system that treats rebuttal writing as a planning task focused on using evidence.</li>\n    <li>The system breaks down feedback into specific issues and creates a detailed response plan using both summaries and external research.</li>\n    <li>Tests show that RebuttalAgent performs better than existing methods in providing thorough, reliable, and coherent rebuttals.</li>\n</ul>"}, "publishedAt": "2026-01-20T12:23:51.000Z", "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance", "summary": "Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce RebuttalAgent, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, RebuttalAgent ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed RebuttalBench and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14171.png", "numComments": 1, "submittedBy": {"_id": "6448b2f53e7b3c11be684348", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6448b2f53e7b3c11be684348/QvlUQG3pWf8ZyEVBV6F7w.jpeg", "fullname": "Qianli Ma", "name": "Mqleet", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "68ee0edd23dc954f7744ac27", "name": "AutoLab-SJTU", "fullname": "AutoLab"}, "isAuthorParticipating": true}],
    "month": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u5728\u89c6\u9891\u95ee\u7b54\u4e2d\uff0c\u89c6\u9891\u901a\u5e38\u53ea\u63d0\u4f9b\u5c40\u90e8\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u800c\u7b54\u6848\u5206\u5e03\u5728\u7f51\u7edc\u4e0a\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0c\u79f0\u4e3aVideoDR\u3002</li>\n    <li>VideoDR\u4e13\u6ce8\u4e8e\u5f00\u653e\u57df\u89c6\u9891\u95ee\u7b54\uff0c\u9700\u8981\u63d0\u53d6\u8de8\u5e27\u7684\u89c6\u89c9\u7ebf\u7d22\u3001\u8fdb\u884c\u7f51\u7edc\u68c0\u7d22\u548c\u591a\u8df3\u63a8\u7406\u3002</li>\n    <li>\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u5904\u7406\u957f\u68c0\u7d22\u94fe\u65f6\u7684\u80fd\u529b\u5f71\u54cd\u5176\u8868\u73b0\u3002</li>\n    <li>VideoDR\u4e3a\u7814\u7a76\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u89c6\u9891\u4ee3\u7406\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u4e86\u672a\u6765\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u7684\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video question answering often requires combining clues from videos and information from the web.</li>\n    <li>We created VideoDR, a new benchmark for video-based question answering that involves visual clue extraction and web retrieval.</li>\n    <li>VideoDR includes high-quality video samples across six different topics, supported by careful human review.</li>\n    <li>We tested various language models and found that their performance depends on how well they manage video clues during long retrieval tasks.</li>\n    <li>VideoDR highlights important challenges for future video research agents, such as maintaining focus and consistency over time.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u7406\u89e3\u65b9\u9762\u8868\u73b0\u8584\u5f31\uff0c\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u6765\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\u3002</li>\n    <li>\u6211\u4eec\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684MLLMs\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5373\u4f7f\u662f3\u5c81\u7684\u5b69\u5b50\u4e5f\u80fd\u8f7b\u677e\u5b8c\u6210\u8fd9\u4e9b\u4efb\u52a1\u3002</li>\n    <li>\u4e3a\u6b64\uff0c\u6211\u4eec\u63a8\u51fa\u4e86BabyVision\uff0c\u4e00\u4e2a\u9488\u5bf9MLLMs\u6838\u5fc3\u89c6\u89c9\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5171\u5305\u542b388\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\u548c\u56db\u4e2a\u4e3b\u8981\u7c7b\u522b\u3002</li>\n    <li>\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0c\u9886\u5148\u7684MLLMs\u5f97\u5206\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c6\uff0c\u4f8b\u5982Gemini3-Pro-Preview\u7684\u5f97\u5206\u4e3a49.7\uff0c\u8fdc\u4f4e\u4e8e6\u5c81\u513f\u7ae5\u548c\u6210\u5e74\u4eba\u7684\u5e73\u5747\u5f97\u5206\u3002</li>\n    <li>BabyVision\u7684\u8fdb\u5c55\u4e3a\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u7684\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u540c\u65f6\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86BabyVision-Gen\u548c\u81ea\u52a8\u8bc4\u4f30\u5de5\u5177\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Humans develop basic visual skills before learning language, but current Multimodal LLMs (MLLMs) rely too much on language to understand visuals.</li>\n  <li>A new benchmark called BabyVision is introduced to test MLLMs on visual tasks without using language.</li>\n  <li>BabyVision includes 388 tasks across 22 subclasses in four main categories.</li>\n  <li>Leading MLLMs, like Gemini3-Pro-Preview, score much lower than both 6-year-old children and average adults on these visual tasks.</li>\n  <li>The study aims to improve MLLMs' visual perception and reasoning, with additional tools and data available for research.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u8bb8\u591a\u793e\u4ea4\u8bed\u4e49\u5b9e\u4f53\uff0c\u51c6\u786e\u5206\u5272\u8fd9\u4e9b\u5b9e\u4f53\u5bf9\u5404\u79cd\u5e94\u7528\u5f88\u91cd\u8981\u3002</li>\n    <li>\u76ee\u524d\u7684\u5206\u5272\u6a21\u578b\u5bf9\u7269\u7406\u5c5e\u6027\u5b9a\u4e49\u7684\u5b9e\u4f53\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\uff08\u5982\u5b66\u6821\u3001\u516c\u56ed\uff09\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6SocioSeg\uff0c\u5305\u62ec\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6846\u67b6SocioReasoner\uff0c\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u6807\u6ce8\u793e\u4ea4\u8bed\u4e49\u5b9e\u4f53\u7684\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u4e14\u5177\u6709\u826f\u597d\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban surfaces contain many important features that need to be identified from satellite images.</li>\n    <li>Current models can identify physical features but struggle with social categories like schools and parks.</li>\n    <li>We created a new dataset called SocioSeg, which includes satellite images and labels for social features.</li>\n    <li>We developed a framework called SocioReasoner that helps identify these social features using a human-like reasoning process.</li>\n    <li>Our method shows better results than existing models and can adapt to new situations without prior training.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B \u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u7d27\u51d1\u6548\u7387\u548c\u591a\u6a21\u6001\u667a\u80fd\u7684\u5e73\u8861\u3002</li>\n    <li>\u5b83\u901a\u8fc7\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u6d41\u7a0b\u5b9e\u73b0\uff0c\u878d\u5408\u4e86\u8bed\u8a00\u5bf9\u9f50\u7684\u611f\u77e5\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u3002</li>\n    <li>\u8be5\u6a21\u578b\u4f7f\u7528\u5e76\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\u6765\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\uff0c\u4ee5\u652f\u6301\u590d\u6742\u7684\u89c6\u89c9\u63a8\u7406\u3002</li>\n    <li>\u5c3d\u7ba1\u6a21\u578b\u89c4\u6a21\u4ec5\u4e3a10\u4ebf\u53c2\u6570\uff0c\u4f46\u5176\u6027\u80fd\u53ef\u4e0e10\u523020\u500d\u66f4\u5927\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u751a\u81f3\u8d85\u8d8a\u4e00\u4e9b\u9876\u7ea7\u4e13\u6709\u6a21\u578b\u3002</li>\n    <li>STEP3-VL-10B \u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u5f3a\u5927\u3001\u6709\u6548\u4e14\u53ef\u91cd\u590d\u7684\u57fa\u51c6\u6a21\u578b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a small, open-source model that combines efficiency with advanced multimodal intelligence.</li>\n    <li>It uses a unified pre-training method on a large dataset of 1.2 trillion multimodal tokens, merging language and visual understanding.</li>\n    <li>The model features a unique post-training process with over 1,000 iterations of reinforcement learning.</li>\n    <li>STEP3-VL-10B performs exceptionally well, achieving high scores on various benchmarks, even against much larger models.</li>\n    <li>The full model is available for public use, offering a strong and efficient baseline for future research and applications.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u6839\u636e\u89c6\u89c9\u7ebf\u7d22\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5ffd\u7565\u4e86\u4eba\u7c7b\u5e38\u7528\u7684\u7b56\u7565\u2014\u2014\u4f7f\u7528\u5730\u56fe\u3002</li>\n    <li>\u672c\u7814\u7a76\u4e3a\u6a21\u578b\u589e\u52a0\u4e86\u201c\u601d\u8003\u5730\u56fe\u201d\u7684\u80fd\u529b\uff0c\u5e76\u5c06\u5176\u8bbe\u5b9a\u4e3a\u4e00\u4e2a\u201c\u5730\u56fe\u4e2d\u7684\u667a\u80fd\u4f53\u5faa\u73af\u201d\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u51c6\u786e\u7387\u4ece8.0%\u63d0\u9ad8\u523022.1%\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>The image geolocalization task predicts where an image was taken on Earth using visual clues.</li>\n  <li>Current models often ignore the human strategy of using maps for this task.</li>\n  <li>This study introduces a new approach that incorporates a \"Thinking with Map\" ability within a loop system.</li>\n  <li>The method includes a two-step process: reinforcement learning to boost efficiency and a technique to explore multiple options before finalizing predictions.</li>\n  <li>Tests show this new method outperforms existing models, significantly improving accuracy in location prediction.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.12538", "authors": [{"_id": "6971913fc1c7409747bf9564", "name": "Tianxin Wei", "hidden": false}, {"_id": "6971913fc1c7409747bf9565", "user": {"_id": "6742eb40924e80c3c80ebe13", "avatarUrl": "/avatars/e6ccb1a89a1ea0bfca70779966f4f429.svg", "isPro": false, "fullname": "Ting-Wei Li", "user": "tingwl0122", "type": "user"}, "name": "Ting-Wei Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T17:12:21.531Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9566", "name": "Zhining Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf9567", "name": "Xuying Ning", "hidden": false}, {"_id": "6971913fc1c7409747bf9568", "name": "Ze Yang", "hidden": false}, {"_id": "6971913fc1c7409747bf9569", "name": "Jiaru Zou", "hidden": false}, {"_id": "6971913fc1c7409747bf956a", "name": "Zhichen Zeng", "hidden": false}, {"_id": "6971913fc1c7409747bf956b", "name": "Ruizhong Qiu", "hidden": false}, {"_id": "6971913fc1c7409747bf956c", "name": "Xiao Lin", "hidden": false}, {"_id": "6971913fc1c7409747bf956d", "name": "Dongqi Fu", "hidden": false}, {"_id": "6971913fc1c7409747bf956e", "name": "Zihao Li", "hidden": false}, {"_id": "6971913fc1c7409747bf956f", "user": {"_id": "653962e75c8e4863e1a2068f", "avatarUrl": "/avatars/d4f5f5da141f37d53ca1986ff17b325e.svg", "isPro": false, "fullname": "Mengting Ai", "user": "famous-blue-raincoat", "type": "user"}, "name": "Mengting Ai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:10.378Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9570", "user": {"_id": "677830bd3f2e3ec475576303", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dhwqUDkk66m4oSGSbcd7j.png", "isPro": false, "fullname": "Duo Zhou", "user": "Claudius7", "type": "user"}, "name": "Duo Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:12.476Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9571", "name": "Wenxuan Bao", "hidden": false}, {"_id": "6971913fc1c7409747bf9572", "user": {"_id": "646323556c27a7e33b23f198", "avatarUrl": "/avatars/17fe142f689ab4be3c2374d1d90393db.svg", "isPro": false, "fullname": "Yunzhe Li", "user": "yunzhel2", "type": "user"}, "name": "Yunzhe Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-22T08:45:14.383Z", "hidden": false}, {"_id": "6971913fc1c7409747bf9573", "name": "Gaotang Li", "hidden": false}, {"_id": "6971913fc1c7409747bf9574", "name": "Cheng Qian", "hidden": false}, {"_id": "6971913fc1c7409747bf9575", "name": "Yu Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf9576", "name": "Xiangru Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf9577", "name": "Yin Xiao", "hidden": false}, {"_id": "6971913fc1c7409747bf9578", "name": "Liri Fang", "hidden": false}, {"_id": "6971913fc1c7409747bf9579", "name": "Hui Liu", "hidden": false}, {"_id": "6971913fc1c7409747bf957a", "name": "Xianfeng Tang", "hidden": false}, {"_id": "6971913fc1c7409747bf957b", "name": "Yuji Zhang", "hidden": false}, {"_id": "6971913fc1c7409747bf957c", "name": "Chi Wang", "hidden": false}, {"_id": "6971913fc1c7409747bf957d", "name": "Jiaxuan You", "hidden": false}, {"_id": "6971913fc1c7409747bf957e", "name": "Heng Ji", "hidden": false}, {"_id": "6971913fc1c7409747bf957f", "name": "Hanghang Tong", "hidden": false}, {"_id": "6971913fc1c7409747bf9580", "name": "Jingrui He", "hidden": false}], "publishedAt": "2026-01-18T18:58:23.000Z", "submittedOnDailyAt": "2026-01-22T00:27:25.162Z", "title": "Agentic Reasoning for Large Language Models", "submittedOnDailyBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "isPro": false, "fullname": "Jiaru Zou", "user": "jiaruz2", "type": "user"}, "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "upvotes": 125, "discussionId": "69719140c1c7409747bf9581", "githubRepo": "https://github.com/weitianxin/Awesome-Agentic-Reasoning", "githubRepoAddedBy": "user", "ai_summary": "Agentic reasoning redefines large language models as autonomous agents capable of planning, acting, and learning through continuous interaction in dynamic environments across single-agent and multi-agent frameworks.", "ai_keywords": ["large language models", "agentic reasoning", "autonomous agents", "planning", "tool use", "search", "feedback", "memory", "adaptation", "collaborative settings", "coordination", "knowledge sharing", "reinforcement learning", "supervised fine-tuning", "in-context reasoning", "post-training reasoning", "real-world applications", "benchmarks", "thought and action", "world modeling", "scalable multi-agent training", "governance"], "githubStars": 105, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "summary_zh": "<ul>\n    <li>\u63a8\u7406\u662f\u63a8\u65ad\u3001\u89e3\u51b3\u95ee\u9898\u548c\u51b3\u7b56\u7684\u91cd\u8981\u8ba4\u77e5\u8fc7\u7a0b\u3002</li>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5c01\u95ed\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5f00\u653e\u548c\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u80fd\u591f\u901a\u8fc7\u6301\u7eed\u4e92\u52a8\u8fdb\u884c\u8ba1\u5212\u3001\u884c\u52a8\u548c\u5b66\u4e60\u3002</li>\n    <li>\u4ee3\u7406\u63a8\u7406\u5206\u4e3a\u4e09\u5c42\uff1a\u57fa\u7840\u4ee3\u7406\u63a8\u7406\u3001\u81ea\u6211\u8fdb\u5316\u4ee3\u7406\u63a8\u7406\u548c\u96c6\u4f53\u591a\u4ee3\u7406\u63a8\u7406\u3002</li>\n    <li>\u8be5\u8c03\u67e5\u8fd8\u56de\u987e\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4ee3\u7406\u63a8\u7406\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u6311\u6218\u548c\u65b9\u5411\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reasoning helps with inference, problem-solving, and decision-making, but large language models (LLMs) have difficulty in open and dynamic situations.</li>\n    <li>Agentic reasoning treats LLMs as independent agents that can plan, act, and learn by interacting continuously with their environment.</li>\n    <li>The survey organizes agentic reasoning into three layers: foundational capabilities, self-improvement through feedback, and teamwork among multiple agents.</li>\n    <li>The paper distinguishes between in-context reasoning (interacting during tests) and post-training reasoning (improving through learning methods).</li>\n    <li>It reviews various applications of agentic reasoning, identifies challenges, and suggests future directions for improving these models in real-world use.</li>\n</ul>"}, "publishedAt": "2026-01-18T13:58:23.000Z", "title": "Agentic Reasoning for Large Language Models", "summary": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12538.png", "numComments": 3, "submittedBy": {"_id": "65c288280aa2d53135734a42", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c288280aa2d53135734a42/5WHmau52EaRS01TOMI3Qg.jpeg", "fullname": "Jiaru Zou", "name": "jiaruz2", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "65448bef5b5d9185ba3202b9", "name": "UIUC-CS", "fullname": "University of Illinois at Urbana-Champaign", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/65448b21fcb96b8b48733729/ycqcXFayMTTD_KpE37067.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.08763", "authors": [{"_id": "6969b0a232f0333869ff946a", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:38.232Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946b", "user": {"_id": "6891c906f3c31445cc040ab1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891c906f3c31445cc040ab1/NBqxXOY7al4CD0XBj8ke2.jpeg", "isPro": false, "fullname": "Yucheng Wang", "user": "DevilEnfant", "type": "user"}, "name": "Yucheng Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:48.080Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946c", "name": "Yufei He", "hidden": false}, {"_id": "6969b0a232f0333869ff946d", "user": {"_id": "682deb444988bd82847e2b03", "avatarUrl": "/avatars/15da087e84386ea72c6fa2db63571420.svg", "isPro": false, "fullname": "Jia-Ying Wu", "user": "EricaWu", "type": "user"}, "name": "Jiaying Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:59.692Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946e", "name": "Yilun Zhao", "hidden": false}, {"_id": "6969b0a232f0333869ff946f", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0a232f0333869ff9470", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:06.327Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9471", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:12.181Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9472", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:17.979Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9473", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:23.007Z", "hidden": false}], "publishedAt": "2026-01-13T17:48:43.000Z", "submittedOnDailyAt": "2026-01-16T01:00:36.686Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "upvotes": 111, "discussionId": "6969b0a232f0333869ff9474", "ai_summary": "Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.", "ai_keywords": ["reinforcement learning", "large language models", "exploration collapse", "pass@k", "pass@1", "rollout-level objective", "high-level solution strategies", "clustering", "policy advantages", "AUC@K"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60 (RL) \u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u540e\u8bad\u7ec3\u4e2d\u53d8\u5f97\u975e\u5e38\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u3002</li>\n    <li>\u5f53\u524d\u65b9\u6cd5\u5e38\u5e38\u51fa\u73b0\u63a2\u7d22\u5d29\u6e83\uff0c\u5bfc\u81f4\u7b56\u7565\u8fc7\u65e9\u96c6\u4e2d\u5728\u5c11\u6570\u4e3b\u5bfc\u63a8\u7406\u6a21\u5f0f\u4e0a\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u2014\u2014\u72ec\u7279\u6027\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\uff0c\u5956\u52b1\u5c55\u73b0\u7a00\u6709\u9ad8\u5c42\u7b56\u7565\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u805a\u7c7b\u76f8\u540c\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u65b0\u9896\u4f46\u6b63\u786e\u7b56\u7565\u7684\u5956\u52b1\u3002</li>\n    <li>\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u533b\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u9ad8\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u591a\u6837\u6027\u548c\u6574\u4f53\u8868\u73b0\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) helps large language models (LLMs) with complex reasoning tasks but can lead to exploration collapse.</li>\n    <li>Exploration collapse happens when models focus too much on a few reasoning patterns, limiting diversity and overall performance.</li>\n    <li>The proposed solution, Uniqueness-Aware Reinforcement Learning, encourages diverse and rare solution strategies rather than just common ones.</li>\n    <li>This method uses a judge to group similar rollouts and rewards unique strategies more than redundant ones.</li>\n    <li>The approach improves performance on various reasoning tasks without losing effectiveness on simpler evaluations, leading to more diverse solutions.</li>\n</ul>"}, "publishedAt": "2026-01-13T12:48:43.000Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08763.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 109, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 104, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5df2\u6210\u4e3a\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4e00\u79cd\u5e38\u7528\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u9700\u8981\u5168\u9762\u7406\u89e3\u548c\u6df1\u5165\u63a8\u7406\u7684\u4efb\u52a1\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u8bbe\u8ba1\u4e3b\u8981\u4f5c\u4e3a\u88ab\u52a8\u5b58\u50a8\uff0c\u79ef\u7d2f\u5b64\u7acb\u7684\u4e8b\u5b9e\uff0c\u9650\u5236\u4e86\u63a8\u7406\u7684\u8fde\u8d2f\u6027\u548c\u5168\u7403\u7406\u89e3\u80fd\u529b\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86HGMem\uff0c\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u5c06\u8bb0\u5fc6\u6269\u5c55\u4e3a\u52a8\u6001\u548c\u5bcc\u6709\u8868\u73b0\u529b\u7684\u7ed3\u6784\uff0c\u4fc3\u8fdb\u590d\u6742\u63a8\u7406\u548c\u5168\u9762\u7406\u89e3\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u56fe\u8868\u793a\u8bb0\u5fc6\uff0c\u5f62\u6210\u66f4\u9ad8\u9636\u7684\u4e92\u52a8\uff0c\u8fde\u63a5\u4e0e\u7126\u70b9\u95ee\u9898\u76f8\u5173\u7684\u4e8b\u5b9e\u548c\u601d\u7ef4\u3002</li>\n    <li>\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30HGMem\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u591a\u6b65\u9aa4RAG\u7684\u6548\u679c\uff0c\u8d85\u8d8a\u4e86\u5f3a\u57fa\u51c6\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) helps improve large language models (LLMs) for tasks that need deep understanding and reasoning.</li>\n    <li>Current memory designs in RAG systems mainly store information passively, which limits their effectiveness in reasoning and understanding.</li>\n    <li>HGMem introduces a new memory system based on hypergraphs, allowing for dynamic connections between facts for better reasoning.</li>\n    <li>This new memory structure creates stronger relationships among facts, leading to improved global understanding and reasoning.</li>\n    <li>Tests show that HGMem significantly outperforms existing systems on various challenging tasks requiring multi-step reasoning.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u5f3a\u5927\uff0c\u7528\u6237\u5e0c\u671b\u5b83\u4eec\u4e0d\u4ec5\u80fd\u63d0\u4f9b\u51c6\u786e\u7684\u56de\u7b54\uff0c\u8fd8\u80fd\u7b26\u5408\u591a\u6837\u5316\u7684\u4eba\u7c7b\u504f\u597d\u3002</li>\n    <li>\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5f00\u59cb\u4f7f\u7528\u591a\u4e2a\u5956\u52b1\u6765\u6307\u5bfc\u6a21\u578b\u5b9e\u73b0\u671f\u671b\u7684\u884c\u4e3a\u3002</li>\n    <li>\u76f4\u63a5\u5e94\u7528\u73b0\u6709\u7684\u591a\u5956\u52b1\u4f18\u5316\u65b9\u6cd5\uff08GRPO\uff09\u4f1a\u5bfc\u81f4\u5956\u52b1\u7ec4\u5408\u5931\u53bb\u533a\u522b\uff0c\u5f71\u54cd\u8bad\u7ec3\u6548\u679c\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff08GDPO\uff09\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u5904\u7406\u591a\u4e2a\u5956\u52b1\uff0c\u4fdd\u6301\u5b83\u4eec\u7684\u76f8\u5bf9\u5dee\u5f02\uff0c\u4ece\u800c\u63d0\u9ad8\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u3002</li>\n    <li>\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\uff0cGDPO\u7684\u8868\u73b0\u4f18\u4e8eGRPO\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Users want language models to provide accurate answers and behave according to different human preferences.</li>\n    <li>Reinforcement learning (RL) uses multiple rewards to guide models toward these preferred behaviors.</li>\n    <li>Current methods like Group Relative Policy Optimization (GRPO) can lead to poor training results by normalizing rewards too much.</li>\n    <li>The new method, Group reward-Decoupled Normalization Policy Optimization (GDPO), improves training by keeping individual rewards distinct.</li>\n    <li>GDPO has been shown to perform better than GRPO in various tasks, improving accuracy and adherence to constraints.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.08521", "authors": [{"_id": "69674059c5e371f6b235d1d8", "user": {"_id": "68920f91bcf2b25e8e121cf6", "avatarUrl": "/avatars/4bc69f43828a346a3ee24b026e0edbb4.svg", "isPro": false, "fullname": "Fengkai Yang", "user": "ShortCatisLong", "type": "user"}, "name": "Fengkai Yang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-19T14:33:21.899Z", "hidden": false}, {"_id": "69674059c5e371f6b235d1d9", "user": {"_id": "6969715fb2636f5f23a9a8c5", "avatarUrl": "/avatars/5e75043891ee41bb980f71fb9e3a33ab.svg", "isPro": false, "fullname": "Zherui Chen", "user": "chenzherui007", "type": "user"}, "name": "Zherui Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:33:53.078Z", "hidden": false}, {"_id": "69674059c5e371f6b235d1da", "name": "Xiaohan Wang", "hidden": false}, {"_id": "69674059c5e371f6b235d1db", "name": "Xiaodong Lu", "hidden": false}, {"_id": "69674059c5e371f6b235d1dc", "user": {"_id": "666eb642a119281ee0bfa443", "avatarUrl": "/avatars/71317810b00978754ad439837b04faff.svg", "isPro": false, "fullname": "Jiajun Chai", "user": "PandaChai", "type": "user"}, "name": "Jiajun Chai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-19T14:37:17.404Z", "hidden": false}, {"_id": "69674059c5e371f6b235d1dd", "name": "Guojun Yin", "hidden": false}, {"_id": "69674059c5e371f6b235d1de", "name": "Wei Lin", "hidden": false}, {"_id": "69674059c5e371f6b235d1df", "name": "Shuai Ma", "hidden": false}, {"_id": "69674059c5e371f6b235d1e0", "name": "Fuzhen Zhuang", "hidden": false}, {"_id": "69674059c5e371f6b235d1e1", "name": "Deqing Wang", "hidden": false}, {"_id": "69674059c5e371f6b235d1e2", "name": "Yaodong Yang", "hidden": false}, {"_id": "69674059c5e371f6b235d1e3", "name": "Jianxin Li", "hidden": false}, {"_id": "69674059c5e371f6b235d1e4", "user": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "isPro": false, "fullname": "Yikun Ban", "user": "Yikunb", "type": "user"}, "name": "Yikun Ban", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:33:50.655Z", "hidden": false}], "publishedAt": "2026-01-13T13:03:15.000Z", "submittedOnDailyAt": "2026-01-19T00:20:58.837Z", "title": "Your Group-Relative Advantage Is Biased", "submittedOnDailyBy": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "isPro": false, "fullname": "Yikun Ban", "user": "Yikunb", "type": "user"}, "summary": "Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.\n  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.", "upvotes": 95, "discussionId": "6967405ac5e371f6b235d1e5", "ai_summary": "Group-based reinforcement learning from verifier rewards suffers from biased advantage estimation that underestimates hard prompts and overestimates easy prompts, which is addressed through a history-aware adaptive difficulty weighting method that improves performance on mathematical reasoning benchmarks.", "ai_keywords": ["Reinforcement Learning from Verifier Rewards", "group-based methods", "GRPO", "advantage estimation", "bias correction", "adaptive reweighting", "difficulty weighting", "mathematical reasoning", "benchmark evaluation"], "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u8005\u5956\u52b1\uff08RLVR\uff09\u662f\u4e00\u79cd\u7528\u4e8e\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u4efb\u52a1\u7684\u65b9\u6cd5\u3002</li>\n    <li>\u5f53\u524d\u7684\u7fa4\u4f53\u57fa\u7840\u65b9\u6cd5\uff08\u5982GRPO\uff09\u4f7f\u7528\u7fa4\u4f53\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\uff0c\u4f46\u5176\u7406\u8bba\u7279\u6027\u5c1a\u4e0d\u6e05\u695a\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u7fa4\u4f53\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\u5668\u5b58\u5728\u56fa\u6709\u504f\u5dee\uff0c\u5bfc\u81f4\u5bf9\u56f0\u96be\u63d0\u793a\u7684\u4f4e\u4f30\u548c\u5bf9\u7b80\u5355\u63d0\u793a\u7684\u9ad8\u4f30\u3002</li>\n    <li>\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5386\u53f2\u611f\u77e5\u81ea\u9002\u5e94\u96be\u5ea6\u52a0\u6743\uff08HA-DW\uff09\u65b9\u6848\uff0c\u7528\u4e8e\u8c03\u6574\u4f18\u52bf\u4f30\u8ba1\u3002</li>\n    <li>\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8868\u660e\uff0cHA-DW\u5728GRPO\u53ca\u5176\u53d8\u4f53\u4e2d\u80fd\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u7ea0\u6b63\u504f\u5dee\u4f30\u8ba1\u5bf9\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement Learning from Verifier Rewards (RLVR) is popular for training large language models on reasoning tasks.</li>\n    <li>Group-based methods like GRPO use group-relative advantage estimation but have unclear theoretical properties.</li>\n    <li>This study finds that group-relative advantage estimators are biased, underestimating advantages for hard tasks and overestimating them for easy tasks.</li>\n    <li>To fix this bias, the authors propose a new method called History-Aware Adaptive Difficulty Weighting (HA-DW) that adjusts advantage estimates based on task difficulty.</li>\n    <li>HA-DW was tested on five reasoning benchmarks and improved performance when used with GRPO and its variants, highlighting the importance of correcting advantage estimation in RLVR training.</li>\n</ul>"}, "publishedAt": "2026-01-13T08:03:15.000Z", "title": "Your Group-Relative Advantage Is Biased", "summary": "Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.\n  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08521.png", "numComments": 5, "submittedBy": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "fullname": "Yikun Ban", "name": "Yikunb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "isAuthorParticipating": true}]
};
window.papersLastUpdated = "Jan 26, 2026";
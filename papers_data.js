window.trendingPapers = {
    "today": [{"paper": {"id": "2601.12993", "authors": [{"_id": "69705709a8be625b19c2af1f", "user": {"_id": "6708cbdcf8a1d7b26732c038", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/CN1WHMPKfjQ8wmfwOe0ni.png", "isPro": false, "fullname": "Hao Luo", "user": "Lightet", "type": "user"}, "name": "Hao Luo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T10:15:59.988Z", "hidden": false}, {"_id": "69705709a8be625b19c2af20", "name": "Ye Wang", "hidden": false}, {"_id": "69705709a8be625b19c2af21", "user": {"_id": "640dd700fdeaae139081f598", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg", "isPro": false, "fullname": "Wanpeng Zhang", "user": "zawnpn", "type": "user"}, "name": "Wanpeng Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:19:56.371Z", "hidden": false}, {"_id": "69705709a8be625b19c2af22", "user": {"_id": "64eac1f496f42afd627d439c", "avatarUrl": "/avatars/aa46265122b8a1170f57475494d7922e.svg", "isPro": false, "fullname": "Sipeng Zheng", "user": "sipeng9527", "type": "user"}, "name": "Sipeng Zheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:38.507Z", "hidden": false}, {"_id": "69705709a8be625b19c2af23", "user": {"_id": "66c84a9eab23d3d7dfb2a368", "avatarUrl": "/avatars/b0a50133c6a95ed340dfb462e87820f4.svg", "isPro": false, "fullname": "ziheng xi", "user": "zhenqis123", "type": "user"}, "name": "Ziheng Xi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:45.981Z", "hidden": false}, {"_id": "69705709a8be625b19c2af24", "user": {"_id": "64bdd5cc76a6e2efccb22100", "avatarUrl": "/avatars/5a0edc24283616dafc76ce5ec97ab5a0.svg", "isPro": false, "fullname": "xuchaoyi", "user": "co1one", "type": "user"}, "name": "Chaoyi Xu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:56.014Z", "hidden": false}, {"_id": "69705709a8be625b19c2af25", "user": {"_id": "68872ff6c18b7e1e13115564", "avatarUrl": "/avatars/f908fc3cc89cd81493105359093f299d.svg", "isPro": false, "fullname": "Haiweng Xu", "user": "Seaman05", "type": "user"}, "name": "Haiweng Xu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:50:01.261Z", "hidden": false}, {"_id": "69705709a8be625b19c2af26", "user": {"_id": "644560657a7b94ddc2d445a3", "avatarUrl": "/avatars/09d6447da6ff1bd0b2b00c899c9f1b28.svg", "isPro": false, "fullname": "Haoqi Yuan", "user": "Yaya041", "type": "user"}, "name": "Haoqi Yuan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:50:06.048Z", "hidden": false}, {"_id": "69705709a8be625b19c2af27", "name": "Chi Zhang", "hidden": false}, {"_id": "69705709a8be625b19c2af28", "name": "Yiqing Wang", "hidden": false}, {"_id": "69705709a8be625b19c2af29", "name": "Yicheng Feng", "hidden": false}, {"_id": "69705709a8be625b19c2af2a", "user": {"_id": "67d905c0e27ba28109384f5c", "avatarUrl": "/avatars/26712594ac9d43c8d1a3e75e36b5df16.svg", "isPro": false, "fullname": "Zongqing Lu", "user": "chungtsing", "type": "user"}, "name": "Zongqing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:50:24.833Z", "hidden": false}], "publishedAt": "2026-01-19T12:20:38.000Z", "submittedOnDailyAt": "2026-01-21T02:12:40.880Z", "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization", "submittedOnDailyBy": {"_id": "640dd700fdeaae139081f598", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg", "isPro": false, "fullname": "Wanpeng Zhang", "user": "zawnpn", "type": "user"}, "summary": "We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal \"mother tongue\" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.", "upvotes": 59, "discussionId": "69705709a8be625b19c2af2b", "projectPage": "https://research.beingbeyond.com/being-h05", "githubRepo": "https://github.com/BeingBeyond/Being-H", "githubRepoAddedBy": "user", "ai_summary": "Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.", "ai_keywords": ["Vision-Language-Action", "cross-embodiment generalization", "human-centric learning", "multimodal data", "Unified Action Space", "Mixture-of-Transformers", "Mixture-of-Flow", "manifold-preserving gating", "universal async chunking"], "githubStars": 265, "organization": {"_id": "687a8ba5aedd77694bc94386", "name": "BeingBeyond", "fullname": "BeingBeyond", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/A6vd2S9BqXgtEWHaPy5qW.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86Being-H0.5\uff0c\u4e00\u4e2a\u65e8\u5728\u589e\u5f3a\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u4e4b\u95f4\u8de8\u4f53\u73b0\u80fd\u529b\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u4eba\u7c7b\u4e92\u52a8\u6570\u636e\u89c6\u4e3a\u7269\u7406\u4e92\u52a8\u7684\u201c\u6bcd\u8bed\u201d\u3002</li>\n    <li>\u63a8\u51fa\u4e86UniHand-2.0\uff0c\u8fd9\u662f\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u5177\u8eab\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc735,000\u5c0f\u65f6\u7684\u591a\u6a21\u6001\u6570\u636e\u3002</li>\n    <li>Being-H0.5\u5229\u7528\u6df7\u5408\u53d8\u6362\u5668\u67b6\u6784\u548c\u65b0\u9896\u7684\u6df7\u5408\u6d41\u6846\u67b6\uff0c\u5c06\u5171\u4eab\u8fd0\u52a8\u539f\u7406\u4e0e\u7279\u5b9a\u4f53\u73b0\u7684\u4e13\u5bb6\u5206\u79bb\u3002</li>\n    <li>\u5728\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBeing-H0.5\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u5728\u4e94\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8de8\u4f53\u73b0\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Being-H0.5 is a new model that helps robots understand and perform tasks across different types of robotic systems.</li>\n    <li>The model learns from human interactions, using them as a guide for how robots should act.</li>\n    <li>It includes UniHand-2.0, a large dataset with over 35,000 hours of data from 30 different robots, to help improve learning.</li>\n    <li>The design of the model allows it to adapt and learn skills from both humans and more advanced robots.</li>\n    <li>Tests show that Being-H0.5 performs very well on simulation tasks and can work effectively on various robotic platforms.</li>\n</ul>"}, "publishedAt": "2026-01-19T07:20:38.000Z", "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization", "summary": "We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal \"mother tongue\" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12993.png", "numComments": 1, "submittedBy": {"_id": "640dd700fdeaae139081f598", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg", "fullname": "Wanpeng Zhang", "name": "zawnpn", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11, "isUserFollowing": false}, "organization": {"_id": "687a8ba5aedd77694bc94386", "name": "BeingBeyond", "fullname": "BeingBeyond", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/A6vd2S9BqXgtEWHaPy5qW.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.11655", "authors": [{"_id": "6970436ba8be625b19c2ae97", "user": {"_id": "64d668bf54bb9eb7040c477e", "avatarUrl": "/avatars/b171b9c1cbb22e2f86e4280099c0bf93.svg", "isPro": false, "fullname": "Caihua Li", "user": "LoisNotLo", "type": "user"}, "name": "Caihua Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:20:05.617Z", "hidden": false}, {"_id": "6970436ba8be625b19c2ae98", "user": {"_id": "64c52b6de356b52a9868bce3", "avatarUrl": "/avatars/43b05cc691f273447e8bc65fe7515176.svg", "isPro": false, "fullname": "Guo", "user": "glh123456", "type": "user"}, "name": "Lianghong Guo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:20:12.738Z", "hidden": false}, {"_id": "6970436ba8be625b19c2ae99", "user": {"_id": "680ef06cce6b5c5af1f29aec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/aTLjskuvCwTYs5JwjUtEF.png", "isPro": false, "fullname": "DeepSoftwareAnalytics", "user": "Yanlin-Wang", "type": "user"}, "name": "Yanlin Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:20:10.460Z", "hidden": false}, {"_id": "6970436ba8be625b19c2ae9a", "user": {"_id": "653df20eaa1f487614da4db1", "avatarUrl": "/avatars/12b27ce2c59f53b7e464039deab36a5d.svg", "isPro": false, "fullname": "Daya Guo", "user": "guoday", "type": "user"}, "name": "Daya Guo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:20:15.421Z", "hidden": false}, {"_id": "6970436ba8be625b19c2ae9b", "user": {"_id": "6355473d525beaee688b7ba1", "avatarUrl": "/avatars/1fb0d57ed5f1a9b872a1ada8b2973ffb.svg", "isPro": false, "fullname": "Wei Tao", "user": "itaowe", "type": "user"}, "name": "Wei Tao", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:20:17.409Z", "hidden": false}, {"_id": "6970436ba8be625b19c2ae9c", "name": "Zhenyu Shan", "hidden": false}, {"_id": "6970436ba8be625b19c2ae9d", "name": "Mingwei Liu", "hidden": false}, {"_id": "6970436ba8be625b19c2ae9e", "name": "Jiachi Chen", "hidden": false}, {"_id": "6970436ba8be625b19c2ae9f", "name": "Haoyu Song", "hidden": false}, {"_id": "6970436ba8be625b19c2aea0", "name": "Duyu Tang", "hidden": false}, {"_id": "6970436ba8be625b19c2aea1", "name": "Hongyu Zhang", "hidden": false}, {"_id": "6970436ba8be625b19c2aea2", "name": "Zibin Zheng", "hidden": false}], "publishedAt": "2026-01-15T18:55:03.000Z", "submittedOnDailyAt": "2026-01-21T00:52:01.626Z", "title": "Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey", "submittedOnDailyBy": {"_id": "6355473d525beaee688b7ba1", "avatarUrl": "/avatars/1fb0d57ed5f1a9b872a1ada8b2973ffb.svg", "isPro": false, "fullname": "Wei Tao", "user": "itaowe", "type": "user"}, "summary": "Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution to serve as a dynamic resource in this field.", "upvotes": 49, "discussionId": "6970436ba8be625b19c2aea3", "projectPage": "https://deepsoftwareanalytics.github.io/Awesome-Issue-Resolution/", "githubRepo": "https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution", "githubRepoAddedBy": "user", "ai_summary": "Large language models face significant challenges in software issue resolution, prompting the development of autonomous coding agents through various training-free and training-based methodologies.", "ai_keywords": ["large language models", "software engineering", "autonomous coding agents", "training-free frameworks", "supervised fine-tuning", "reinforcement learning", "data quality", "agent behavior"], "githubStars": 40, "organization": {"_id": "680ef1aaccefecd5aee18d1d", "name": "Deep-Software-Analytics", "fullname": "DeepSoftwareAnalytics"}, "summary_zh": "<ul>\n    <li>\u95ee\u9898\u89e3\u51b3\u662f\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u4e00\u4e2a\u590d\u6742\u4efb\u52a1\uff0c\u5bf9\u4eba\u5de5\u667a\u80fd\u6765\u8bf4\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002</li>\n    <li>SWE-bench\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u8fd9\u4e2a\u4efb\u52a1\u5bf9\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u975e\u5e38\u56f0\u96be\uff0c\u63a8\u52a8\u4e86\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\u7684\u53d1\u5c55\u3002</li>\n    <li>\u672c\u6587\u7cfb\u7edf\u6027\u5730\u8c03\u67e5\u4e86\u8fd9\u4e00\u65b0\u5174\u9886\u57df\uff0c\u5206\u6790\u4e86\u6570\u636e\u6784\u5efa\u6d41\u7a0b\u548c\u5404\u79cd\u65b9\u6cd5\u8bba\u3002</li>\n    <li>\u8ba8\u8bba\u4e86\u6570\u636e\u8d28\u91cf\u3001\u4ee3\u7406\u884c\u4e3a\u7684\u5173\u952e\u5206\u6790\u4ee5\u53ca\u5b9e\u9645\u5e94\u7528\u3002</li>\n    <li>\u786e\u5b9a\u4e86\u4e3b\u8981\u6311\u6218\uff0c\u5e76\u6982\u8ff0\u4e86\u672a\u6765\u7814\u7a76\u7684\u6709\u5e0c\u671b\u65b9\u5411\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u6e90\u8d44\u6e90\u5e93\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Issue resolution in software engineering is a tough challenge for artificial intelligence.</li>\n    <li>Benchmarks like SWE-bench show that large language models struggle with this task.</li>\n    <li>This paper surveys the development of autonomous coding agents and their methodologies.</li>\n    <li>It covers data collection methods, analysis of techniques, and the quality of data and agent behavior.</li>\n    <li>An open-source repository is available for further resources in this area.</li>\n</ul>"}, "publishedAt": "2026-01-15T13:55:03.000Z", "title": "Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey", "summary": "Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution to serve as a dynamic resource in this field.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11655.png", "numComments": 2, "submittedBy": {"_id": "6355473d525beaee688b7ba1", "avatarUrl": "/avatars/1fb0d57ed5f1a9b872a1ada8b2973ffb.svg", "fullname": "Wei Tao", "name": "itaowe", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "680ef1aaccefecd5aee18d1d", "name": "Deep-Software-Analytics", "fullname": "DeepSoftwareAnalytics"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.14250", "authors": [{"_id": "69705b78a8be625b19c2af4c", "user": {"_id": "6697765937d24838267b41e7", "avatarUrl": "/avatars/682bb4cf0a0009812b42748dc26916f9.svg", "isPro": false, "fullname": "PangzeCheung", "user": "PangzeCheung", "type": "user"}, "name": "Pengze Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:19:32.663Z", "hidden": false}, {"_id": "69705b78a8be625b19c2af4d", "name": "Yanze Wu", "hidden": false}, {"_id": "69705b78a8be625b19c2af4e", "user": {"_id": "6805bdfb344d6d8a8fd5b07a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dGLeWvN2CXLmxVP_b9S4R.png", "isPro": false, "fullname": "Mengtian Li", "user": "LemonSky1995", "type": "user"}, "name": "Mengtian Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:52:30.670Z", "hidden": false}, {"_id": "69705b78a8be625b19c2af4f", "name": "Xu Bai", "hidden": false}, {"_id": "69705b78a8be625b19c2af50", "name": "Songtao Zhao", "hidden": false}, {"_id": "69705b78a8be625b19c2af51", "user": {"_id": "6339029a76421c0543167075", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6339029a76421c0543167075/3npT0NxTMV-MLWDThyBy8.png", "isPro": false, "fullname": "fulong ye", "user": "Alon77777", "type": "user"}, "name": "Fulong Ye", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T11:46:59.343Z", "hidden": false}, {"_id": "69705b78a8be625b19c2af52", "name": "Chong Mou", "hidden": false}, {"_id": "69705b78a8be625b19c2af53", "user": {"_id": "6752cd83ffaeeb979db974ae", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png", "isPro": false, "fullname": "Xinghui Li", "user": "Crayon-Shinchan", "type": "user"}, "name": "Xinghui Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:52:58.973Z", "hidden": false}, {"_id": "69705b78a8be625b19c2af54", "user": {"_id": "6304e2dabad6ce7fc0287d57", "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg", "isPro": false, "fullname": "Zhuowei_Chen", "user": "ZhuoweiChen", "type": "user"}, "name": "Zhuowei Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:52:20.177Z", "hidden": false}, {"_id": "69705b78a8be625b19c2af55", "name": "Qian He", "hidden": false}, {"_id": "69705b78a8be625b19c2af56", "user": {"_id": "671aa30b496f0bc5ae04da4b", "avatarUrl": "/avatars/902d7f9fd56f84953d67d9229bd9d6b7.svg", "isPro": false, "fullname": "Mingyuan Gao", "user": "GMY1999", "type": "user"}, "name": "Mingyuan Gao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:52:13.464Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6697765937d24838267b41e7/309vLCCbUoiHGJkN0u3LH.mp4"], "publishedAt": "2026-01-20T18:58:11.000Z", "submittedOnDailyAt": "2026-01-21T02:29:32.365Z", "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer", "submittedOnDailyBy": {"_id": "6697765937d24838267b41e7", "avatarUrl": "/avatars/682bb4cf0a0009812b42748dc26916f9.svg", "isPro": false, "fullname": "PangzeCheung", "user": "PangzeCheung", "type": "user"}, "summary": "Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.", "upvotes": 29, "discussionId": "69705b78a8be625b19c2af57", "projectPage": "https://pangzecheung.github.io/OmniTransfer/", "githubRepo": "https://github.com/PangzeCheung/OmniTransfer", "githubRepoAddedBy": "user", "ai_summary": "OmniTransfer presents a unified framework for spatio-temporal video transfer that enhances appearance consistency and temporal control through multi-view information and multimodal semantic guidance.", "ai_keywords": ["video customization", "spatio-temporal video transfer", "multi-view information", "temporal cues", "temporal alignment", "appearance consistency", "reference-decoupled causal learning", "task-adaptive multimodal alignment", "pose-guided methods"], "githubStars": 54, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u6bd4\u56fe\u50cf\u6216\u6587\u672c\u4f20\u9012\u66f4\u4e30\u5bcc\u7684\u4fe1\u606f\uff0c\u5305\u542b\u7a7a\u95f4\u548c\u65f6\u95f4\u7684\u52a8\u6001\u3002</li>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u5b9a\u5236\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u53c2\u8003\u56fe\u50cf\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u89c6\u9891\u4e2d\u7684\u4e30\u5bcc\u65f6\u7a7a\u4fe1\u606f\u3002</li>\n    <li>\u63d0\u51fa\u4e86OmniTransfer\u6846\u67b6\uff0c\u65e8\u5728\u6539\u5584\u89c6\u9891\u8f6c\u79fb\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u63a7\u5236\u80fd\u529b\u3002</li>\n    <li>OmniTransfer\u7ed3\u5408\u4e86\u4e09\u9879\u5173\u952e\u8bbe\u8ba1\uff0c\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cOmniTransfer\u5728\u5916\u89c2\u548c\u65f6\u95f4\u8f6c\u79fb\u65b9\u9762\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5f00\u521b\u4e86\u9ad8\u4fdd\u771f\u89c6\u9891\u751f\u6210\u7684\u65b0\u6a21\u5f0f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Videos provide more detailed information than images or text by showing how things change over time and space.</li>\n    <li>Many current video customization methods do not use the full potential of video information, limiting their flexibility.</li>\n    <li>OmniTransfer is a new framework that improves video transfer by using information from multiple frames and controlling timing more precisely.</li>\n    <li>It includes three main features to enhance performance: adapting to video tasks, separating reference and target information for efficiency, and using different types of guidance for various tasks.</li>\n    <li>OmniTransfer outperforms current methods in both appearance and timing while still achieving good motion transfer results.</li>\n</ul>"}, "publishedAt": "2026-01-20T13:58:11.000Z", "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer", "summary": "Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6697765937d24838267b41e7/309vLCCbUoiHGJkN0u3LH.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14250.png", "numComments": 4, "submittedBy": {"_id": "6697765937d24838267b41e7", "avatarUrl": "/avatars/682bb4cf0a0009812b42748dc26916f9.svg", "fullname": "PangzeCheung", "name": "PangzeCheung", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.14192", "authors": [{"_id": "69705a68a8be625b19c2af3a", "user": {"_id": "6745c589d2d740914ec2574f", "avatarUrl": "/avatars/7b2ff6848d42cd140a775df0c2bc9384.svg", "isPro": false, "fullname": "Xiaofang Yang", "user": "fffovo", "type": "user"}, "name": "Xiaofang Yang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:48:39.583Z", "hidden": false}, {"_id": "69705a68a8be625b19c2af3b", "name": "Lijun Li", "hidden": false}, {"_id": "69705a68a8be625b19c2af3c", "user": {"_id": "660d17d6c9be0dcd31a30b3d", "avatarUrl": "/avatars/3743fe9b695c488ebe33f0d8fd607a8a.svg", "isPro": false, "fullname": "Zhou Heng", "user": "henggg", "type": "user"}, "name": "Heng Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:19:49.600Z", "hidden": false}, {"_id": "69705a68a8be625b19c2af3d", "name": "Tong Zhu", "hidden": false}, {"_id": "69705a68a8be625b19c2af3e", "name": "Xiaoye Qu", "hidden": false}, {"_id": "69705a68a8be625b19c2af3f", "name": "Yuchen Fan", "hidden": false}, {"_id": "69705a68a8be625b19c2af40", "user": {"_id": "6952244bfbddb08cb2562f3b", "avatarUrl": "/avatars/70d67319af29604129378fee3f216757.svg", "isPro": false, "fullname": "qianshan wei", "user": "b1intern", "type": "user"}, "name": "Qianshan Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:04.253Z", "hidden": false}, {"_id": "69705a68a8be625b19c2af41", "name": "Rui Ye", "hidden": false}, {"_id": "69705a68a8be625b19c2af42", "name": "Li Kang", "hidden": false}, {"_id": "69705a68a8be625b19c2af43", "name": "Yiran Qin", "hidden": false}, {"_id": "69705a68a8be625b19c2af44", "name": "Zhiqiang Kou", "hidden": false}, {"_id": "69705a68a8be625b19c2af45", "name": "Daizong Liu", "hidden": false}, {"_id": "69705a68a8be625b19c2af46", "name": "Qi Li", "hidden": false}, {"_id": "69705a68a8be625b19c2af47", "name": "Ning Ding", "hidden": false}, {"_id": "69705a68a8be625b19c2af48", "user": {"_id": "65257545b017be1fc1915364", "avatarUrl": "/avatars/9bffd3fb567d2fa1e5c3546d77560b43.svg", "isPro": false, "fullname": "Siheng Chen", "user": "sihengchen", "type": "user"}, "name": "Siheng Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:26.261Z", "hidden": false}, {"_id": "69705a68a8be625b19c2af49", "name": "Jing Shao", "hidden": false}], "publishedAt": "2026-01-20T17:51:56.000Z", "submittedOnDailyAt": "2026-01-21T02:28:39.429Z", "title": "Toward Efficient Agents: Memory, Tool learning, and Planning", "submittedOnDailyBy": {"_id": "641d3efac3983aa9491677b9", "avatarUrl": "/avatars/53565486351c16a1ac8ea863963e2d9b.svg", "isPro": false, "fullname": "Lijun Li", "user": "adwardlee", "type": "user"}, "summary": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.", "upvotes": 29, "discussionId": "69705a69a8be625b19c2af4a", "projectPage": "https://efficient-agents.github.io/", "githubRepo": "https://github.com/yxf203/Awesome-Efficient-Agents", "githubRepoAddedBy": "user", "ai_summary": "Efficiency in agentic systems is examined across memory, tool learning, and planning components, analyzing trade-offs between effectiveness and computational costs through various optimization strategies and benchmarks.", "ai_keywords": ["large language models", "agentic systems", "memory", "tool learning", "planning", "latency", "tokens", "steps", "reinforcement learning", "controlled search mechanisms", "Pareto frontier", "efficiency metrics", "evaluation protocols"], "githubStars": 27, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>\u8fd1\u5e74\u6765\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u7814\u7a76\u5173\u6ce8\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u4e3a\u667a\u80fd\u7cfb\u7edf\u3002</li>\n    <li>\u5c3d\u7ba1\u667a\u80fd\u4f53\u7684\u6709\u6548\u6027\u4e0d\u65ad\u63d0\u9ad8\uff0c\u4f46\u6548\u7387\uff08\u5982\u5ef6\u8fdf\u3001\u8017\u65f6\u7b49\uff09\u5e38\u5e38\u88ab\u5ffd\u89c6\u3002</li>\n    <li>\u672c\u6587\u4ece\u8bb0\u5fc6\u3001\u5de5\u5177\u5b66\u4e60\u548c\u89c4\u5212\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u63a2\u8ba8\u667a\u80fd\u4f53\u7684\u6548\u7387\u3002</li>\n    <li>\u6211\u4eec\u56de\u987e\u4e86\u4e0d\u540c\u5b9e\u73b0\u65b9\u6cd5\u7684\u7814\u7a76\uff0c\u5f3a\u8c03\u5171\u4eab\u7684\u9ad8\u5c42\u539f\u5219\u4ee5\u63d0\u9ad8\u6548\u7387\u3002</li>\n    <li>\u6700\u540e\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\uff0c\u4ee5\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper focuses on improving the efficiency of large language model agents, which is important for real-world use.</li>\n    <li>It examines three main areas of agent efficiency: memory, tool learning, and planning, looking at costs like speed and resource usage.</li>\n    <li>The authors review various approaches that aim to enhance efficiency while sharing common principles, such as reducing context size and optimizing reinforcement learning rewards.</li>\n    <li>Efficiency is evaluated by comparing effectiveness within set costs and analyzing costs at similar effectiveness levels.</li>\n    <li>The paper identifies key challenges and future directions to enhance agent efficiency and provides insights for further research.</li>\n</ul>"}, "publishedAt": "2026-01-20T12:51:56.000Z", "title": "Toward Efficient Agents: Memory, Tool learning, and Planning", "summary": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14192.png", "numComments": 2, "submittedBy": {"_id": "641d3efac3983aa9491677b9", "avatarUrl": "/avatars/53565486351c16a1ac8ea863963e2d9b.svg", "fullname": "Lijun Li", "name": "adwardlee", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": false}, {"paper": {"id": "2601.13029", "authors": [{"_id": "69708ffea8be625b19c2b04c", "user": {"_id": "6575702b15b1ca184b0b2700", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575702b15b1ca184b0b2700/O9cEodqQmG-gyqMiO_edR.jpeg", "isPro": false, "fullname": "Zaibin Zhang", "user": "MrBean2024", "type": "user"}, "name": "Zaibin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T11:47:45.167Z", "hidden": false}, {"_id": "69708ffea8be625b19c2b04d", "name": "Yuhan Wu", "hidden": false}, {"_id": "69708ffea8be625b19c2b04e", "name": "Lianjie Jia", "hidden": false}, {"_id": "69708ffea8be625b19c2b04f", "name": "Yifan Wang", "hidden": false}, {"_id": "69708ffea8be625b19c2b050", "name": "Zhongbo Zhang", "hidden": false}, {"_id": "69708ffea8be625b19c2b051", "user": {"_id": "6965e7d00aa591efb07b220c", "avatarUrl": "/avatars/d0f65aafc3b652084213f02a4f93c453.svg", "isPro": false, "fullname": "Yijiang Li", "user": "luciasnowblack", "type": "user"}, "name": "Yijiang Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T11:59:41.952Z", "hidden": false}, {"_id": "69708ffea8be625b19c2b052", "name": "Binghao Ran", "hidden": false}, {"_id": "69708ffea8be625b19c2b053", "name": "Fuxi Zhang", "hidden": false}, {"_id": "69708ffea8be625b19c2b054", "user": {"_id": "68ad6a9106bcf0ebe9624dc5", "avatarUrl": "/avatars/309e383889f848c828d4b1eb4542b54a.svg", "isPro": false, "fullname": "SunZhuohan", "user": "sunz525", "type": "user"}, "name": "Zhuohan Sun", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T11:59:29.495Z", "hidden": false}, {"_id": "69708ffea8be625b19c2b055", "user": {"_id": "64e314ad24809d7fa0f20fbc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bHE0w_hjDFvU-Aul0_E7g.jpeg", "isPro": false, "fullname": "Zhenfei Yin", "user": "JeremyYin", "type": "user"}, "name": "Zhenfei Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T11:59:18.401Z", "hidden": false}, {"_id": "69708ffea8be625b19c2b056", "name": "Lijun Wang", "hidden": false}, {"_id": "69708ffea8be625b19c2b057", "name": "Huchuan Lu", "hidden": false}], "publishedAt": "2026-01-19T13:13:54.000Z", "submittedOnDailyAt": "2026-01-21T06:09:04.854Z", "title": "Think3D: Thinking with Space for Spatial Reasoning", "submittedOnDailyBy": {"_id": "6419309f22270b3ccf177c77", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6419309f22270b3ccf177c77/KQa1586iBBKqucUlfpuPp.jpeg", "isPro": false, "fullname": "William Li", "user": "williamium", "type": "user"}, "summary": "Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.", "upvotes": 29, "discussionId": "69708fffa8be625b19c2b058", "githubRepo": "https://github.com/zhangzaibin/spagent", "githubRepoAddedBy": "user", "ai_summary": "Think3D enhances vision-language models' 3D reasoning capabilities by enabling interactive spatial exploration through 3D reconstruction and camera-based operations, improving performance without additional training.", "ai_keywords": ["vision large models", "3D reconstruction models", "point clouds", "camera poses", "spatial reasoning", "3D chain-of-thought process", "reinforcement learning policy", "multimodal agents", "3D reasoning"], "githubStars": 32, "summary_zh": "<ul>\n    <li>\u7406\u89e3\u548c\u63a8\u7406\u7269\u7406\u4e16\u754c\u9700\u8981\u7a7a\u95f4\u667a\u6167\uff0c\u5373\u8d85\u8d8a2D\u611f\u77e5\u7684\u51e0\u4f55\u3001\u89c6\u89d2\u548c\u7a7a\u95f4\u5173\u7cfb\u7684\u80fd\u529b\u3002</li>\n    <li>\u867d\u7136\u73b0\u6709\u7684\u89c6\u89c9\u5927\u578b\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u771f\u6b63\u76843D\u63a8\u7406\u4e0a\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>Think3D\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u4f7f\u89c6\u89c9\u5927\u578b\u6a21\u578b\u80fd\u591f\u57283D\u7a7a\u95f4\u4e2d\u8fdb\u884c\u601d\u8003\uff0c\u901a\u8fc73D\u91cd\u5efa\u6a21\u578b\u6765\u64cd\u4f5c\u7a7a\u95f4\u3002</li>\n    <li>Think3D\u663e\u8457\u63d0\u9ad8\u4e86\u5148\u8fdb\u6a21\u578b\uff08\u5982GPT-4.1\u548cGemini 2.5 Pro\uff09\u5728\u7a7a\u95f4\u63a8\u7406\u4e0a\u7684\u8868\u73b0\uff0c\u5e73\u5747\u63d0\u5347\u4e867.8%\u52304.7%\u3002</li>\n    <li>\u5c0f\u578b\u6a21\u578b\u5728\u7a7a\u95f4\u63a2\u7d22\u4e2d\u53d7\u76ca\u4e8e\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5de5\u5177\u4f7f\u7528\u7684\u6536\u76ca\u4ece0.7%\u589e\u52a0\u52306.8%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Understanding the physical world involves spatial intelligence, which goes beyond just 2D images.</li>\n    <li>Think3D is a new framework that helps vision models think in 3D by using 3D reconstruction from images or videos.</li>\n    <li>This framework allows models to manipulate space and reason in 3D without needing extra training.</li>\n    <li>Think3D improves performance in spatial reasoning tasks for advanced models like GPT-4.1 and Gemini 2.5 Pro.</li>\n    <li>Smaller models also benefit from a reinforcement learning technique that helps them choose better viewpoints and actions.</li>\n</ul>"}, "publishedAt": "2026-01-19T08:13:54.000Z", "title": "Think3D: Thinking with Space for Spatial Reasoning", "summary": "Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13029.png", "numComments": 1, "submittedBy": {"_id": "6419309f22270b3ccf177c77", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6419309f22270b3ccf177c77/KQa1586iBBKqucUlfpuPp.jpeg", "fullname": "William Li", "name": "williamium", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.13836", "authors": [{"_id": "697067f2a8be625b19c2afa5", "name": "Qian Chen", "hidden": false}, {"_id": "697067f2a8be625b19c2afa6", "user": {"_id": "618497ea8aaadc9253c2dfa9", "avatarUrl": "/avatars/2eb3954a99f5aede6f31b8ae49b8c910.svg", "isPro": false, "fullname": "Fu Jinlan", "user": "Jinlan", "type": "user"}, "name": "Jinlan Fu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:39:51.571Z", "hidden": false}, {"_id": "697067f2a8be625b19c2afa7", "name": "Changsong Li", "hidden": false}, {"_id": "697067f2a8be625b19c2afa8", "name": "See-Kiong Ng", "hidden": false}, {"_id": "697067f2a8be625b19c2afa9", "user": {"_id": "61457b8deff2c9fdb4de4988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg", "isPro": false, "fullname": "Xipeng Qiu", "user": "xpqiu", "type": "user"}, "name": "Xipeng Qiu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:48:17.801Z", "hidden": false}], "publishedAt": "2026-01-20T10:47:20.000Z", "submittedOnDailyAt": "2026-01-21T04:50:03.124Z", "title": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs", "submittedOnDailyBy": {"_id": "618497ea8aaadc9253c2dfa9", "avatarUrl": "/avatars/2eb3954a99f5aede6f31b8ae49b8c910.svg", "isPro": false, "fullname": "Fu Jinlan", "user": "Jinlan", "type": "user"}, "summary": "Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).", "upvotes": 27, "discussionId": "697067f2a8be625b19c2afaa", "projectPage": "https://openmoss.github.io/FutureOmni", "githubRepo": "https://github.com/OpenMOSS/FutureOmni", "githubRepoAddedBy": "user", "ai_summary": "FutureOmni presents the first benchmark for evaluating multimodal models' ability to forecast future events from audio-visual data, revealing current limitations and proposing an improved training strategy for better performance.", "ai_keywords": ["Multimodal Large Language Models", "audio-visual cues", "future forecasting", "cross-modal causal reasoning", "temporal reasoning", "internal knowledge", "LLM-assisted pipeline", "instruction-tuning dataset", "Omni-Modal Future Forecasting training strategy"], "githubStars": 8, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86FutureOmni\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u8bc4\u4f30\u97f3\u89c6\u9891\u73af\u5883\u4e2d\u672a\u6765\u9884\u6d4b\u7684\u57fa\u51c6\u3002</li>\n    <li>\u8be5\u57fa\u51c6\u8981\u6c42\u6a21\u578b\u8fdb\u884c\u8de8\u6a21\u6001\u56e0\u679c\u548c\u65f6\u95f4\u63a8\u7406\uff0c\u5e76\u5229\u7528\u5185\u90e8\u77e5\u8bc6\u6765\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\u3002</li>\n    <li>\u5305\u542b919\u4e2a\u89c6\u9891\u548c1,034\u4e2a\u591a\u9009\u95ee\u7b54\u5bf9\uff0c\u8986\u76d68\u4e2a\u4e3b\u8981\u9886\u57df\u3002</li>\n    <li>\u8bc4\u4f30\u663e\u793a\u73b0\u6709\u7cfb\u7edf\u5728\u97f3\u89c6\u9891\u672a\u6765\u9884\u6d4b\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u8bb2\u8bdd\u8f83\u591a\u7684\u573a\u666f\u4e2d\u3002</li>\n    <li>\u63a8\u51fa\u4e86\u4e00\u4e2a7K\u6837\u672c\u7684\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u4ee5\u63d0\u9ad8\u672a\u6765\u9884\u6d4b\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study introduces FutureOmni, a new benchmark for testing how well models can predict future events using audio and visual information.</li>\n    <li>Current models struggle with future predictions, especially in situations involving a lot of speech, with the best performance being 64.8% accuracy.</li>\n    <li>FutureOmni includes 919 videos and 1,034 multiple-choice questions across 8 different topics to evaluate models.</li>\n    <li>The research also offers a new training approach called Omni-Modal Future Forecasting (OFF) to improve models' ability to predict future events.</li>\n    <li>All code and datasets related to FutureOmni are publicly available for others to access and use.</li>\n</ul>"}, "publishedAt": "2026-01-20T05:47:20.000Z", "title": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs", "summary": "Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13836.png", "numComments": 1, "submittedBy": {"_id": "618497ea8aaadc9253c2dfa9", "avatarUrl": "/avatars/2eb3954a99f5aede6f31b8ae49b8c910.svg", "fullname": "Fu Jinlan", "name": "Jinlan", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.11969", "authors": [{"_id": "69703c1fa8be625b19c2ae73", "user": {"_id": "64096ef79e9f790c905b846d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg", "isPro": false, "fullname": "Zecheng Tang", "user": "ZetangForward", "type": "user"}, "name": "Zecheng Tang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:20:19.792Z", "hidden": false}, {"_id": "69703c1fa8be625b19c2ae74", "user": {"_id": "65731fc31345577b7071d7df", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65731fc31345577b7071d7df/T8qnqIxnycvy1AP8CKAPb.png", "isPro": false, "fullname": "Baibei Ji", "user": "iiiiGray", "type": "user"}, "name": "Baibei Ji", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T11:47:12.150Z", "hidden": false}, {"_id": "69703c1fa8be625b19c2ae75", "name": "Ruoxi Sun", "hidden": false}, {"_id": "69703c1fa8be625b19c2ae76", "name": "Haitian Wang", "hidden": false}, {"_id": "69703c1fa8be625b19c2ae77", "name": "WangJie You", "hidden": false}, {"_id": "69703c1fa8be625b19c2ae78", "user": {"_id": "67760e6700d3237a069893fe", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WblRHnyjQsyqoay21Y4cN.png", "isPro": false, "fullname": "yijun zhang", "user": "zhangyijun166", "type": "user"}, "name": "Zhang Yijun", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T11:47:32.235Z", "hidden": false}, {"_id": "69703c1fa8be625b19c2ae79", "user": {"_id": "690d4c6a262e79c6454e57c8", "avatarUrl": "/avatars/bb36807f6fcecf5faadb3aacba4b6e27.svg", "isPro": false, "fullname": "Wenpeng Zhu", "user": "Christal326", "type": "user"}, "name": "Wenpeng Zhu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:51:06.425Z", "hidden": false}, {"_id": "69703c1fa8be625b19c2ae7a", "name": "Ji Qi", "hidden": false}, {"_id": "69703c1fa8be625b19c2ae7b", "user": {"_id": "6670e285b0c03c4e9d6e0985", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uCZHm4gKSHZ2b0hpHWgZv.jpeg", "isPro": false, "fullname": "Juntao Li", "user": "douvleplus", "type": "user"}, "name": "Juntao Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:51:12.383Z", "hidden": false}, {"_id": "69703c1fa8be625b19c2ae7c", "name": "Min Zhang", "hidden": false}], "publishedAt": "2026-01-17T09:04:53.000Z", "submittedOnDailyAt": "2026-01-21T00:11:34.112Z", "title": "MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models", "submittedOnDailyBy": {"_id": "64096ef79e9f790c905b846d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg", "isPro": false, "fullname": "Zecheng Tang", "user": "ZetangForward", "type": "user"}, "summary": "Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce MemoryRewardBench, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. MemoryRewardBench covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.", "upvotes": 24, "discussionId": "69703c1fa8be625b19c2ae7d", "projectPage": "https://github.com/LCM-Lab/MemRewardBench", "githubRepo": "https://github.com/LCM-Lab/MemRewardBench", "githubRepoAddedBy": "user", "ai_summary": "A benchmark called MemoryRewardBench is introduced to systematically evaluate reward models' ability to assess long-term memory management in large language models across various context lengths and memory patterns.", "ai_keywords": ["memory-centric mechanisms", "long-context comprehension", "long-form generation", "reward models", "MemoryRewardBench", "memory management", "large language models"], "githubStars": 4, "organization": {"_id": "61f8e653129c9ff1b911293d", "name": "SUDA", "fullname": "Soochow University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643701821817-61f8e5934a8e5a275b2b3e5a.png"}, "summary_zh": "<ul>\n    <li>\u7814\u7a76\u8bb0\u5fc6\u7ba1\u7406\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u6587\u672c\u7684\u91cd\u8981\u6027\u3002</li>\n    <li>\u63a8\u51fa\u4e86MemoryRewardBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u8bc4\u4f30\u8bb0\u5fc6\u8d28\u91cf\u7684\u65b0\u57fa\u51c6\u3002</li>\n    <li>\u57fa\u51c6\u6db5\u76d6\u4e86\u957f\u6587\u672c\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\uff0c\u8bbe\u7f6e\u4e8610\u79cd\u4e0d\u540c\u7684\u8bb0\u5fc6\u7ba1\u7406\u6a21\u5f0f\u3002</li>\n    <li>\u8bc4\u4f30\u4e8613\u79cd\u5148\u8fdb\u7684\u5956\u52b1\u6a21\u578b\uff0c\u53d1\u73b0\u5f00\u6e90\u6a21\u578b\u548c\u4e13\u6709\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u5728\u7f29\u5c0f\u3002</li>\n    <li>\u63ed\u793a\u4e86\u5f53\u524d\u5956\u52b1\u6a21\u578b\u5728\u8bc4\u4f30\u8bb0\u5fc6\u7ba1\u7406\u65b9\u9762\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The study focuses on improving how large language models manage memory when processing long texts.</li>\n    <li>It introduces MemoryRewardBench, a benchmark for assessing how well reward models evaluate memory quality.</li>\n    <li>MemoryRewardBench includes 10 different tasks with varying memory management patterns, using context lengths from 8,000 to 128,000 tokens.</li>\n    <li>Evaluations show that newer models perform better than older ones, reducing the performance gap between open-source and proprietary models.</li>\n    <li>The research also highlights both the strengths and weaknesses of current reward models in evaluating memory management.</li>\n</ul>"}, "publishedAt": "2026-01-17T04:04:53.000Z", "title": "MemoryRewardBench: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models", "summary": "Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce MemoryRewardBench, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. MemoryRewardBench covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11969.png", "numComments": 1, "submittedBy": {"_id": "64096ef79e9f790c905b846d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg", "fullname": "Zecheng Tang", "name": "ZetangForward", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 4, "isUserFollowing": false}, "organization": {"_id": "61f8e653129c9ff1b911293d", "name": "SUDA", "fullname": "Soochow University", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1643701821817-61f8e5934a8e5a275b2b3e5a.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.14004", "authors": [{"_id": "69709468a8be625b19c2b069", "user": {"_id": "63870c8388b39a64e1e8cdfa", "avatarUrl": "/avatars/1813b49eca6eb7396fa18cccc6e24342.svg", "isPro": false, "fullname": "zhanghengyuan", "user": "hengyuanya", "type": "user"}, "name": "Hengyuan Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:18:44.875Z", "hidden": false}, {"_id": "69709468a8be625b19c2b06a", "name": "Zhihao Zhang", "hidden": false}, {"_id": "69709468a8be625b19c2b06b", "user": {"_id": "65b92649ba9f3fa7ed4af967", "avatarUrl": "/avatars/d3e072451f3a81b46ffdd142f72547d8.svg", "isPro": false, "fullname": "Mingyang Wang", "user": "mingyang26", "type": "user"}, "name": "Mingyang Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T10:15:55.556Z", "hidden": false}, {"_id": "69709468a8be625b19c2b06c", "user": {"_id": "655b4f5cc11dee7f7e882a0c", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655b4f5cc11dee7f7e882a0c/4GaAC5Qt55eYlbNRJpVD5.png", "isPro": false, "fullname": "zunhaisu", "user": "zunhai", "type": "user"}, "name": "Zunhai Su", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:25:40.905Z", "hidden": false}, {"_id": "69709468a8be625b19c2b06d", "user": {"_id": "61f2bc2df4eb3a3875013bff", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1658510064443-61f2bc2df4eb3a3875013bff.jpeg", "isPro": false, "fullname": "WYW", "user": "WANGYIWEI", "type": "user"}, "name": "Yiwei Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T10:15:52.802Z", "hidden": false}, {"_id": "69709468a8be625b19c2b06e", "user": {"_id": "63bbdb991374e3ef912d0f88", "avatarUrl": "/avatars/1e889b1589fccb44b3ec4bc040021fc8.svg", "isPro": false, "fullname": "Qianli Wang", "user": "qiaw99", "type": "user"}, "name": "Qianli Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T10:15:57.822Z", "hidden": false}, {"_id": "69709468a8be625b19c2b06f", "user": {"_id": "662ce44c8b8705f30371fba8", "avatarUrl": "/avatars/b96a25a8c124e7caa9de06b7188bdc15.svg", "isPro": false, "fullname": "Shuzhou Yuan", "user": "shuzyuan", "type": "user"}, "name": "Shuzhou Yuan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:27:18.543Z", "hidden": false}, {"_id": "69709468a8be625b19c2b070", "name": "Ercong Nie", "hidden": false}, {"_id": "69709468a8be625b19c2b071", "user": {"_id": "64832945413ff0a011f41fe8", "avatarUrl": "/avatars/32e576adbaeeead357d903d716638b0e.svg", "isPro": true, "fullname": "Xufeng Duan", "user": "XufengDuan", "type": "user"}, "name": "Xufeng Duan", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T10:15:50.690Z", "hidden": false}, {"_id": "69709468a8be625b19c2b072", "name": "Qibo Xue", "hidden": false}, {"_id": "69709468a8be625b19c2b073", "name": "Zeping Yu", "hidden": false}, {"_id": "69709468a8be625b19c2b074", "name": "Chenming Shang", "hidden": false}, {"_id": "69709468a8be625b19c2b075", "name": "Xiao Liang", "hidden": false}, {"_id": "69709468a8be625b19c2b076", "name": "Jing Xiong", "hidden": false}, {"_id": "69709468a8be625b19c2b077", "name": "Hui Shen", "hidden": false}, {"_id": "69709468a8be625b19c2b078", "name": "Chaofan Tao", "hidden": false}, {"_id": "69709468a8be625b19c2b079", "name": "Zhengwu Liu", "hidden": false}, {"_id": "69709468a8be625b19c2b07a", "name": "Senjie Jin", "hidden": false}, {"_id": "69709468a8be625b19c2b07b", "name": "Zhiheng Xi", "hidden": false}, {"_id": "69709468a8be625b19c2b07c", "name": "Dongdong Zhang", "hidden": false}, {"_id": "69709468a8be625b19c2b07d", "name": "Sophia Ananiadou", "hidden": false}, {"_id": "69709468a8be625b19c2b07e", "name": "Tao Gui", "hidden": false}, {"_id": "69709468a8be625b19c2b07f", "name": "Ruobing Xie", "hidden": false}, {"_id": "69709468a8be625b19c2b080", "name": "Hayden Kwok-Hay So", "hidden": false}, {"_id": "69709468a8be625b19c2b081", "name": "Hinrich Sch\u00fctze", "hidden": false}, {"_id": "69709468a8be625b19c2b082", "name": "Xuanjing Huang", "hidden": false}, {"_id": "69709468a8be625b19c2b083", "name": "Qi Zhang", "hidden": false}, {"_id": "69709468a8be625b19c2b084", "name": "Ngai Wong", "hidden": false}], "publishedAt": "2026-01-20T14:23:23.000Z", "submittedOnDailyAt": "2026-01-21T13:50:15.791Z", "title": "Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models", "submittedOnDailyBy": {"_id": "63870c8388b39a64e1e8cdfa", "avatarUrl": "/avatars/1813b49eca6eb7396fa18cccc6e24342.svg", "isPro": false, "fullname": "zhanghengyuan", "user": "hengyuanya", "type": "user"}, "summary": "Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque decision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as an observational science, summarizing analytical insights while lacking a systematic framework for actionable intervention. To bridge this gap, we present a practical survey structured around the pipeline: \"Locate, Steer, and Improve.\" We formally categorize Localizing (diagnosis) and Steering (intervention) methods based on specific Interpretable Objects to establish a rigorous intervention protocol. Furthermore, we demonstrate how this framework enables tangible improvements in Alignment, Capability, and Efficiency, effectively operationalizing MI as an actionable methodology for model optimization. The curated paper list of this work is available at https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey.", "upvotes": 17, "discussionId": "69709468a8be625b19c2b085", "ai_summary": "Mechanistic interpretability is presented as an actionable framework for understanding and optimizing large language models through systematic localization, steering, and improvement methods.", "ai_keywords": ["Mechanistic Interpretability", "Large Language Models", "Localizing", "Steering", "Interpretable Objects", "Alignment", "Capability", "Efficiency"], "summary_zh": "<ul>\n    <li>\u673a\u5236\u53ef\u89e3\u91ca\u6027\uff08MI\uff09\u662f\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u51b3\u7b56\u8fc7\u7a0b\u7684\u91cd\u8981\u65b9\u6cd5\u3002</li>\n    <li>\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u89c2\u5bdfMI\uff0c\u6ca1\u6709\u63d0\u4f9b\u7cfb\u7edf\u7684\u5e72\u9884\u6846\u67b6\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u8c03\u67e5\u6846\u67b6\uff0c\u5206\u4e3a\u201c\u5b9a\u4f4d\u3001\u5f15\u5bfc\u548c\u6539\u5584\u201d\u3002</li>\n    <li>\u6211\u4eec\u5bf9\u8bca\u65ad\u548c\u5e72\u9884\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u4ee5\u5efa\u7acb\u4e25\u8c28\u7684\u5e72\u9884\u534f\u8bae\u3002</li>\n    <li>\u8be5\u6846\u67b6\u53ef\u4ee5\u5728\u5bf9\u9f50\u3001\u80fd\u529b\u548c\u6548\u7387\u65b9\u9762\u5b9e\u73b0\u53ef\u89c2\u7684\u6539\u8fdb\u3002</li>\n</ul>", "summary_simple": "<ul>\n  <li>Mechanistic Interpretability (MI) helps explain how Large Language Models (LLMs) make decisions.</li>\n  <li>Current reviews mainly summarize MI insights but lack a clear framework for taking action.</li>\n  <li>This survey introduces a structured approach called \"Locate, Steer, and Improve\" for MI.</li>\n  <li>It categorizes methods for diagnosing and intervening in LLMs to improve their performance.</li>\n  <li>The framework aims to enhance alignment, capability, and efficiency of LLMs for better model optimization.</li>\n</ul>"}, "publishedAt": "2026-01-20T09:23:23.000Z", "title": "Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models", "summary": "Mechanistic Interpretability (MI) has emerged as a vital approach to demystify the opaque decision-making of Large Language Models (LLMs). However, existing reviews primarily treat MI as an observational science, summarizing analytical insights while lacking a systematic framework for actionable intervention. To bridge this gap, we present a practical survey structured around the pipeline: \"Locate, Steer, and Improve.\" We formally categorize Localizing (diagnosis) and Steering (intervention) methods based on specific Interpretable Objects to establish a rigorous intervention protocol. Furthermore, we demonstrate how this framework enables tangible improvements in Alignment, Capability, and Efficiency, effectively operationalizing MI as an actionable methodology for model optimization. The curated paper list of this work is available at https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14004.png", "numComments": 1, "submittedBy": {"_id": "63870c8388b39a64e1e8cdfa", "avatarUrl": "/avatars/1813b49eca6eb7396fa18cccc6e24342.svg", "fullname": "zhanghengyuan", "name": "hengyuanya", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.11522", "authors": [{"_id": "696f604fbfeda8d7e160c9a9", "user": {"_id": "67a711a835423a46f4f44583", "avatarUrl": "/avatars/1a6a6475f73463f1f4ddbbfc1d59c4ed.svg", "isPro": false, "fullname": "ruihengzhang", "user": "ZrH42", "type": "user"}, "name": "Ruiheng Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-20T14:48:05.051Z", "hidden": false}, {"_id": "696f604fbfeda8d7e160c9aa", "user": {"_id": "67756c9c846a267749304255", "avatarUrl": "/avatars/01f09805b561887c55d1b9ad4e96b461.svg", "isPro": false, "fullname": "Jingfeng Yao", "user": "MapleF9", "type": "user"}, "name": "Jingfeng Yao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T12:00:32.366Z", "hidden": false}, {"_id": "696f604fbfeda8d7e160c9ab", "name": "Huangxuan Zhao", "hidden": false}, {"_id": "696f604fbfeda8d7e160c9ac", "name": "Hao Yan", "hidden": false}, {"_id": "696f604fbfeda8d7e160c9ad", "name": "Xiao He", "hidden": false}, {"_id": "696f604fbfeda8d7e160c9ae", "name": "Lei Chen", "hidden": false}, {"_id": "696f604fbfeda8d7e160c9af", "name": "Zhou Wei", "hidden": false}, {"_id": "696f604fbfeda8d7e160c9b0", "name": "Yong Luo", "hidden": false}, {"_id": "696f604fbfeda8d7e160c9b1", "name": "Zengmao Wang", "hidden": false}, {"_id": "696f604fbfeda8d7e160c9b2", "name": "Lefei Zhang", "hidden": false}, {"_id": "696f604fbfeda8d7e160c9b3", "name": "Dacheng Tao", "hidden": false}, {"_id": "696f604fbfeda8d7e160c9b4", "name": "Bo Du", "hidden": false}], "publishedAt": "2026-01-16T18:59:58.000Z", "submittedOnDailyAt": "2026-01-21T00:35:45.455Z", "title": "UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation", "submittedOnDailyBy": {"_id": "67a711a835423a46f4f44583", "avatarUrl": "/avatars/1a6a6475f73463f1f4ddbbfc1d59c4ed.svg", "isPro": false, "fullname": "ruihengzhang", "user": "ZrH42", "type": "user"}, "summary": "Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.", "upvotes": 15, "discussionId": "696f604fbfeda8d7e160c9b5", "githubRepo": "https://github.com/ZrH42/UniX", "githubRepoAddedBy": "user", "ai_summary": "UniX presents a unified medical foundation model that decouples visual understanding and generation tasks using distinct autoregressive and diffusion branches with cross-modal attention for enhanced performance.", "ai_keywords": ["medical foundation models", "autoregressive architectures", "diffusion models", "cross-modal self-attention", "data cleaning pipeline", "multi-stage training strategy", "visual understanding", "image generation"], "githubStars": 19, "organization": {"_id": "6350bdf559bfa9a85d42fea4", "name": "WuhanUniversity", "fullname": "Wuhan Univeristy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6350bd20aaee2ec378dfe506/Bu1Fwz4dAwjwzWv-vZ2FN.png"}, "summary_zh": "<ul>\n    <li>\u5c3d\u7ba1\u53d6\u5f97\u4e86\u4e00\u4e9b\u8fdb\u5c55\uff0c\u533b\u7597\u57fa\u7840\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5bfc\u81f4\u7406\u89e3\u548c\u751f\u6210\u6027\u80fd\u53d7\u635f\u3002</li>\n    <li>UniX\u6a21\u578b\u5c06\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u5206\u4e3a\u4e0d\u540c\u7684\u5206\u652f\uff0c\u63d0\u9ad8\u4e86\u5404\u81ea\u7684\u6027\u80fd\u3002</li>\n    <li>\u5f15\u5165\u8de8\u6a21\u6001\u81ea\u6ce8\u610f\u673a\u5236\uff0c\u52a8\u6001\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\u3002</li>\n    <li>\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUniX\u5728\u7406\u89e3\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u5584\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Medical foundation models have difficulty combining visual understanding and generation due to conflicting goals.</li>\n    <li>UniX is a new model that separates understanding and generation into different branches to improve performance.</li>\n    <li>A special attention mechanism helps guide the generation based on understanding features.</li>\n    <li>UniX shows significant improvements in performance: 46.1% better understanding and 24.2% improved generation quality.</li>\n    <li>The model uses fewer parameters than previous models while achieving similar results, offering a scalable solution for medical image tasks.</li>\n</ul>"}, "publishedAt": "2026-01-16T13:59:58.000Z", "title": "UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation", "summary": "Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11522.png", "numComments": 1, "submittedBy": {"_id": "67a711a835423a46f4f44583", "avatarUrl": "/avatars/1a6a6475f73463f1f4ddbbfc1d59c4ed.svg", "fullname": "ruihengzhang", "name": "ZrH42", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "isUserFollowing": false}, "organization": {"_id": "6350bdf559bfa9a85d42fea4", "name": "WuhanUniversity", "fullname": "Wuhan Univeristy", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6350bd20aaee2ec378dfe506/Bu1Fwz4dAwjwzWv-vZ2FN.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.12294", "authors": [{"_id": "697055b4a8be625b19c2af18", "user": {"_id": "6474e1afb68461d5cf7c41cc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png", "isPro": false, "fullname": "Dawei Li", "user": "wjldw", "type": "user"}, "name": "Dawei Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T12:00:07.519Z", "hidden": false}, {"_id": "697055b4a8be625b19c2af19", "user": {"_id": "644cb05d778ecbfb9783fd8b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QSjINHKRs1OLz8Y34j8Ak.png", "isPro": false, "fullname": "Yuguang Yao", "user": "yaoyugua", "type": "user"}, "name": "Yuguang Yao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T12:00:12.781Z", "hidden": false}, {"_id": "697055b4a8be625b19c2af1a", "name": "Zhen Tan", "hidden": false}, {"_id": "697055b4a8be625b19c2af1b", "name": "Huan Liu", "hidden": false}, {"_id": "697055b4a8be625b19c2af1c", "user": {"_id": "65dcb410bda21d181b38321b", "avatarUrl": "/avatars/a9caed79c4eb14352b4015377fcae1d7.svg", "isPro": false, "fullname": "Ruocheng Guo", "user": "rguo12", "type": "user"}, "name": "Ruocheng Guo", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T12:00:23.178Z", "hidden": false}], "publishedAt": "2026-01-18T07:48:36.000Z", "submittedOnDailyAt": "2026-01-21T01:59:37.515Z", "title": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents", "submittedOnDailyBy": {"_id": "6474e1afb68461d5cf7c41cc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png", "isPro": false, "fullname": "Dawei Li", "user": "wjldw", "type": "user"}, "summary": "Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.", "upvotes": 13, "discussionId": "697055b4a8be625b19c2af1d", "ai_summary": "ToolPRMBench is introduced as a large-scale benchmark for evaluating process reward models in tool-using agents, featuring step-level test cases and multi-LLM verification to ensure data quality.", "ai_keywords": ["process reward models", "tool-using agents", "reward-guided search", "agent trajectories", "step-level rewards", "large language models", "multi-LLM verification", "offline sampling", "online sampling"], "organization": {"_id": "64e917fc662874dbc9b6a828", "name": "intuit", "fullname": "Intuit", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/AGLb0CFLiqEd5BBKLvtPO.jpeg"}, "summary_zh": "<ul>\n    <li>\u63d0\u51fa\u4e86ToolPRMBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u5de5\u5177\u4f7f\u7528\u4ee3\u7406\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u7684\u57fa\u51c6\u3002</li>\n    <li>ToolPRMBench\u57fa\u4e8e\u591a\u4e2a\u4ee3\u8868\u6027\u7684\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\uff0c\u8f6c\u6362\u4ee3\u7406\u7684\u8f68\u8ff9\u4e3a\u9010\u6b65\u6d4b\u8bd5\u6848\u4f8b\u3002</li>\n    <li>\u6bcf\u4e2a\u6d4b\u8bd5\u6848\u4f8b\u5305\u542b\u4ea4\u4e92\u5386\u53f2\u3001\u6b63\u786e\u52a8\u4f5c\u3001\u4e00\u4e2a\u5408\u7406\u4f46\u4e0d\u6b63\u786e\u7684\u66ff\u4ee3\u52a8\u4f5c\u53ca\u76f8\u5173\u5de5\u5177\u5143\u6570\u636e\u3002</li>\n    <li>\u91c7\u7528\u79bb\u7ebf\u91c7\u6837\u548c\u5728\u7ebf\u91c7\u6837\u6765\u5206\u522b\u6355\u6349\u5355\u6b65\u9519\u8bef\u548c\u591a\u6b65\u5931\u8d25\u3002</li>\n    <li>\u5b9e\u9a8c\u663e\u793a\u4e86\u4e0d\u540cPRM\u7684\u6709\u6548\u6027\u548c\u4e13\u95e8\u5316PRM\u5728\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u6f5c\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reward-guided search methods help improve agents that use tools by guiding their exploration of actions.</li>\n    <li>Process reward models (PRMs) provide detailed rewards for each step, but there are no solid benchmarks to evaluate them in tool-using scenarios.</li>\n    <li>This paper introduces ToolPRMBench, a benchmark designed to assess PRMs for tool-using agents.</li>\n    <li>ToolPRMBench creates test cases from agent actions, including correct and incorrect actions, to evaluate performance.</li>\n    <li>Extensive tests show differences in how effective various PRMs are, especially highlighting the benefits of specialized PRMs for tool-using agents.</li>\n</ul>"}, "publishedAt": "2026-01-18T02:48:36.000Z", "title": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents", "summary": "Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12294.png", "numComments": 1, "submittedBy": {"_id": "6474e1afb68461d5cf7c41cc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png", "fullname": "Dawei Li", "name": "wjldw", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64e917fc662874dbc9b6a828", "name": "intuit", "fullname": "Intuit", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/AGLb0CFLiqEd5BBKLvtPO.jpeg"}, "isAuthorParticipating": true}],
    "week": [{"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u8bb8\u591a\u8bed\u4e49\u5b9e\u4f53\uff0c\u9700\u8981\u4ece\u536b\u661f\u56fe\u50cf\u4e2d\u8fdb\u884c\u5206\u5272\u3002</li>\n    <li>\u5f53\u524d\u7684\u5206\u5272\u6a21\u578b\u5bf9\u7269\u7406\u5c5e\u6027\u7684\u5b9e\u4f53\uff08\u5982\u5efa\u7b51\u7269\u548c\u6c34\u4f53\uff09\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\uff08\u5982\u5b66\u6821\u548c\u516c\u56ed\uff09\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6SocioSeg\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u6846\u67b6SocioReasoner\uff0c\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u6807\u6ce8\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5e76\u4e14\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban areas have many different types of features, and separating them in satellite images is important for various applications.</li>\n    <li>Current models can identify physical features like buildings and water but struggle with social features like schools and parks.</li>\n    <li>We introduce a new dataset called SocioSeg, which includes satellite images and detailed labels for social features.</li>\n    <li>Our new framework, SocioReasoner, helps recognize and label social features using a method similar to human reasoning.</li>\n    <li>Our experiments show that our approach outperforms existing models and can generalize well to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.12993", "authors": [{"_id": "69705709a8be625b19c2af1f", "user": {"_id": "6708cbdcf8a1d7b26732c038", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/CN1WHMPKfjQ8wmfwOe0ni.png", "isPro": false, "fullname": "Hao Luo", "user": "Lightet", "type": "user"}, "name": "Hao Luo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T10:15:59.988Z", "hidden": false}, {"_id": "69705709a8be625b19c2af20", "name": "Ye Wang", "hidden": false}, {"_id": "69705709a8be625b19c2af21", "user": {"_id": "640dd700fdeaae139081f598", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg", "isPro": false, "fullname": "Wanpeng Zhang", "user": "zawnpn", "type": "user"}, "name": "Wanpeng Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:19:56.371Z", "hidden": false}, {"_id": "69705709a8be625b19c2af22", "user": {"_id": "64eac1f496f42afd627d439c", "avatarUrl": "/avatars/aa46265122b8a1170f57475494d7922e.svg", "isPro": false, "fullname": "Sipeng Zheng", "user": "sipeng9527", "type": "user"}, "name": "Sipeng Zheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:38.507Z", "hidden": false}, {"_id": "69705709a8be625b19c2af23", "user": {"_id": "66c84a9eab23d3d7dfb2a368", "avatarUrl": "/avatars/b0a50133c6a95ed340dfb462e87820f4.svg", "isPro": false, "fullname": "ziheng xi", "user": "zhenqis123", "type": "user"}, "name": "Ziheng Xi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:45.981Z", "hidden": false}, {"_id": "69705709a8be625b19c2af24", "user": {"_id": "64bdd5cc76a6e2efccb22100", "avatarUrl": "/avatars/5a0edc24283616dafc76ce5ec97ab5a0.svg", "isPro": false, "fullname": "xuchaoyi", "user": "co1one", "type": "user"}, "name": "Chaoyi Xu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:56.014Z", "hidden": false}, {"_id": "69705709a8be625b19c2af25", "user": {"_id": "68872ff6c18b7e1e13115564", "avatarUrl": "/avatars/f908fc3cc89cd81493105359093f299d.svg", "isPro": false, "fullname": "Haiweng Xu", "user": "Seaman05", "type": "user"}, "name": "Haiweng Xu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:50:01.261Z", "hidden": false}, {"_id": "69705709a8be625b19c2af26", "user": {"_id": "644560657a7b94ddc2d445a3", "avatarUrl": "/avatars/09d6447da6ff1bd0b2b00c899c9f1b28.svg", "isPro": false, "fullname": "Haoqi Yuan", "user": "Yaya041", "type": "user"}, "name": "Haoqi Yuan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:50:06.048Z", "hidden": false}, {"_id": "69705709a8be625b19c2af27", "name": "Chi Zhang", "hidden": false}, {"_id": "69705709a8be625b19c2af28", "name": "Yiqing Wang", "hidden": false}, {"_id": "69705709a8be625b19c2af29", "name": "Yicheng Feng", "hidden": false}, {"_id": "69705709a8be625b19c2af2a", "user": {"_id": "67d905c0e27ba28109384f5c", "avatarUrl": "/avatars/26712594ac9d43c8d1a3e75e36b5df16.svg", "isPro": false, "fullname": "Zongqing Lu", "user": "chungtsing", "type": "user"}, "name": "Zongqing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:50:24.833Z", "hidden": false}], "publishedAt": "2026-01-19T12:20:38.000Z", "submittedOnDailyAt": "2026-01-21T02:12:40.880Z", "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization", "submittedOnDailyBy": {"_id": "640dd700fdeaae139081f598", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg", "isPro": false, "fullname": "Wanpeng Zhang", "user": "zawnpn", "type": "user"}, "summary": "We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal \"mother tongue\" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.", "upvotes": 59, "discussionId": "69705709a8be625b19c2af2b", "projectPage": "https://research.beingbeyond.com/being-h05", "githubRepo": "https://github.com/BeingBeyond/Being-H", "githubRepoAddedBy": "user", "ai_summary": "Being-H0.5 is a Vision-Language-Action model that enables robust cross-embodiment generalization through human-centric learning and a Mixture-of-Transformers architecture with specialized embodiment handling.", "ai_keywords": ["Vision-Language-Action", "cross-embodiment generalization", "human-centric learning", "multimodal data", "Unified Action Space", "Mixture-of-Transformers", "Mixture-of-Flow", "manifold-preserving gating", "universal async chunking"], "githubStars": 265, "organization": {"_id": "687a8ba5aedd77694bc94386", "name": "BeingBeyond", "fullname": "BeingBeyond", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/A6vd2S9BqXgtEWHaPy5qW.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u4ecb\u7ecd\u4e86Being-H0.5\uff0c\u4e00\u4e2a\u65e8\u5728\u589e\u5f3a\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u4e4b\u95f4\u8de8\u4f53\u73b0\u80fd\u529b\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u3002</li>\n    <li>\u8be5\u6a21\u578b\u91c7\u7528\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u4eba\u7c7b\u4e92\u52a8\u6570\u636e\u89c6\u4e3a\u7269\u7406\u4e92\u52a8\u7684\u201c\u6bcd\u8bed\u201d\u3002</li>\n    <li>\u63a8\u51fa\u4e86UniHand-2.0\uff0c\u8fd9\u662f\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u5177\u8eab\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc735,000\u5c0f\u65f6\u7684\u591a\u6a21\u6001\u6570\u636e\u3002</li>\n    <li>Being-H0.5\u5229\u7528\u6df7\u5408\u53d8\u6362\u5668\u67b6\u6784\u548c\u65b0\u9896\u7684\u6df7\u5408\u6d41\u6846\u67b6\uff0c\u5c06\u5171\u4eab\u8fd0\u52a8\u539f\u7406\u4e0e\u7279\u5b9a\u4f53\u73b0\u7684\u4e13\u5bb6\u5206\u79bb\u3002</li>\n    <li>\u5728\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBeing-H0.5\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u5728\u4e94\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8de8\u4f53\u73b0\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Being-H0.5 is a new model that helps robots understand and perform tasks across different types of robotic systems.</li>\n    <li>The model learns from human interactions, using them as a guide for how robots should act.</li>\n    <li>It includes UniHand-2.0, a large dataset with over 35,000 hours of data from 30 different robots, to help improve learning.</li>\n    <li>The design of the model allows it to adapt and learn skills from both humans and more advanced robots.</li>\n    <li>Tests show that Being-H0.5 performs very well on simulation tasks and can work effectively on various robotic platforms.</li>\n</ul>"}, "publishedAt": "2026-01-19T07:20:38.000Z", "title": "Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization", "summary": "We introduce Being-H0.5, a foundational Vision-Language-Action (VLA) model designed for robust cross-embodiment generalization across diverse robotic platforms. While existing VLAs often struggle with morphological heterogeneity and data scarcity, we propose a human-centric learning paradigm that treats human interaction traces as a universal \"mother tongue\" for physical interaction. To support this, we present UniHand-2.0, the largest embodied pre-training recipe to date, comprising over 35,000 hours of multimodal data across 30 distinct robotic embodiments. Our approach introduces a Unified Action Space that maps heterogeneous robot controls into semantically aligned slots, enabling low-resource robots to bootstrap skills from human data and high-resource platforms. Built upon this human-centric foundation, we design a unified sequential modeling and multi-task pre-training paradigm to bridge human demonstrations and robotic execution. Architecturally, Being-H0.5 utilizes a Mixture-of-Transformers design featuring a novel Mixture-of-Flow (MoF) framework to decouple shared motor primitives from specialized embodiment-specific experts. Finally, to make cross-embodiment policies stable in the real world, we introduce Manifold-Preserving Gating for robustness under sensory shift and Universal Async Chunking to universalize chunked control across embodiments with different latency and control profiles. We empirically demonstrate that Being-H0.5 achieves state-of-the-art results on simulated benchmarks, such as LIBERO (98.9%) and RoboCasa (53.9%), while also exhibiting strong cross-embodiment capabilities on five robotic platforms.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.12993.png", "numComments": 1, "submittedBy": {"_id": "640dd700fdeaae139081f598", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640dd700fdeaae139081f598/L986zu4-iOPFs9Y3_T5Ue.jpeg", "fullname": "Wanpeng Zhang", "name": "zawnpn", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 11, "isUserFollowing": false}, "organization": {"_id": "687a8ba5aedd77694bc94386", "name": "BeingBeyond", "fullname": "BeingBeyond", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/640dd700fdeaae139081f598/A6vd2S9BqXgtEWHaPy5qW.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.11077", "authors": [{"_id": "696da04e3f1837bfb89709c2", "user": {"_id": "66206a2136201a18e5329631", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66206a2136201a18e5329631/9aJiGOKshh1tZ171zDw_D.png", "isPro": false, "fullname": "yangjie", "user": "red-fox-yj", "type": "user"}, "name": "Jie Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-20T09:41:07.348Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709c3", "user": {"_id": "638ef0b0c67af472d31674a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638ef0b0c67af472d31674a6/zXQjC3DdY3jpVkATkpms6.png", "isPro": false, "fullname": "Honglin Guo", "user": "KYLN24", "type": "user"}, "name": "Honglin Guo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-20T09:41:05.334Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709c4", "name": "Li Ji", "hidden": false}, {"_id": "696da04e3f1837bfb89709c5", "user": {"_id": "683c6a19a4b3e38a3e23d50a", "avatarUrl": "/avatars/ae9f212acaa9d1a65b4a5d86c5f7a355.svg", "isPro": false, "fullname": "Jiazheng Zhou", "user": "HaZ-K", "type": "user"}, "name": "Jiazheng Zhou", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:42:49.533Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709c6", "name": "Rui Zheng", "hidden": false}, {"_id": "696da04e3f1837bfb89709c7", "name": "Zhikai Lei", "hidden": false}, {"_id": "696da04e3f1837bfb89709c8", "user": {"_id": "6334f2f1259c518276efa730", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6334f2f1259c518276efa730/z_SH_OBkDyj4RCN9mqsKS.jpeg", "isPro": false, "fullname": "Shuo Zhang", "user": "Meteonis", "type": "user"}, "name": "Shuo Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-20T14:48:19.478Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709c9", "user": {"_id": "653a6e5cae155b92bae77b74", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653a6e5cae155b92bae77b74/TA5FWKAUsB249ux4MzD_R.jpeg", "isPro": false, "fullname": "Zhiheng Xi", "user": "WooooDyy", "type": "user"}, "name": "Zhiheng Xi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:43:14.561Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709ca", "user": {"_id": "65435cad429b80b14922ab8d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/N8oWq4ZZn3dRxmXi18FrA.jpeg", "isPro": false, "fullname": "Shichun Liu", "user": "Liusc2020", "type": "user"}, "name": "Shichun Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:43:20.286Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709cb", "name": "Yuxin Wang", "hidden": false}, {"_id": "696da04e3f1837bfb89709cc", "name": "Bo Wang", "hidden": false}, {"_id": "696da04e3f1837bfb89709cd", "user": {"_id": "64b7495a75b23e68c538f4c0", "avatarUrl": "/avatars/ce06f3b89f9e09dcbe748b208eec1e9d.svg", "isPro": false, "fullname": "Yining Zheng", "user": "WillQvQ", "type": "user"}, "name": "Yining Zheng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:43:29.765Z", "hidden": false}, {"_id": "696da04e3f1837bfb89709ce", "name": "Tao Gui", "hidden": false}, {"_id": "696da04e3f1837bfb89709cf", "user": {"_id": "61457b8deff2c9fdb4de4988", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1632381702899-61457b8deff2c9fdb4de4988.jpeg", "isPro": false, "fullname": "Xipeng Qiu", "user": "xpqiu", "type": "user"}, "name": "Xipeng Qiu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-20T09:43:35.123Z", "hidden": false}], "publishedAt": "2026-01-16T08:23:52.000Z", "submittedOnDailyAt": "2026-01-20T00:57:45.521Z", "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development", "submittedOnDailyBy": {"_id": "66206a2136201a18e5329631", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66206a2136201a18e5329631/9aJiGOKshh1tZ171zDw_D.png", "isPro": false, "fullname": "yangjie", "user": "red-fox-yj", "type": "user"}, "summary": "The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving. However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in backend development which demands rigorous environment configuration and service deployment. To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic backend coding within a realistic, executable workflow. Using a scalable automated pipeline, we curated 224 practical tasks spanning 8 languages and 19 frameworks from open-source repositories. Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests. Our extensive evaluation reveals that even state-of-the-art models struggle to deliver reliable performance on these holistic tasks, highlighting a substantial disparity between current model capabilities and the demands of practical backend engineering. Our code is available at https://github.com/OpenMOSS/ABC-Bench.", "upvotes": 49, "discussionId": "696da04e3f1837bfb89709d0", "projectPage": "https://dawning-road.github.io/blog/abc-bench", "githubRepo": "https://github.com/OpenMOSS/ABC-Bench", "githubRepoAddedBy": "user", "ai_summary": "ABC-Bench evaluates LLM agents on realistic backend coding tasks requiring full development lifecycle management from repository exploration to containerized service deployment and API testing.", "ai_keywords": ["Large Language Models", "agentic backend coding", "executable workflow", "development lifecycle", "containerized services", "end-to-end API tests"], "githubStars": 8, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "summary_zh": "<ul>\n    <li>\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u7ecf\u53d1\u5c55\u6210\u4e3a\u81ea\u4e3b\u4ee3\u7406\uff0c\u53ef\u4ee5\u89e3\u51b3\u66f4\u590d\u6742\u7684\u7f16\u7801\u95ee\u9898\u3002</li>\n    <li>\u76ee\u524d\u7684\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u4ee3\u7801\u903b\u8f91\uff0c\u5ffd\u7565\u4e86\u771f\u5b9e\u5de5\u7a0b\u4e2d\u52a8\u6001\u7684\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u540e\u7aef\u5f00\u53d1\u4e2d\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86ABC-Bench\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u540e\u7aef\u7f16\u7801\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u771f\u5b9e\u7684\u53ef\u6267\u884c\u5de5\u4f5c\u6d41\u7a0b\u3002</li>\n    <li>ABC-Bench\u5305\u62ec224\u4e2a\u5b9e\u9645\u4efb\u52a1\uff0c\u4f7f\u7528\u4e868\u79cd\u8bed\u8a00\u548c19\u79cd\u6846\u67b6\uff0c\u8981\u6c42\u4ee3\u7406\u7ba1\u7406\u6574\u4e2a\u5f00\u53d1\u751f\u547d\u5468\u671f\u3002</li>\n    <li>\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u8fd9\u4e9b\u5168\u9762\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4ecd\u4e0d\u53ef\u9760\uff0c\u8868\u660e\u6a21\u578b\u80fd\u529b\u4e0e\u5b9e\u9645\u540e\u7aef\u5de5\u7a0b\u9700\u6c42\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u5dee\u8ddd\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) are evolving into autonomous agents for coding, moving beyond simple code generation.</li>\n    <li>Current benchmarks mainly test code logic in static settings, missing real-world challenges in backend development.</li>\n    <li>ABC-Bench is a new benchmark designed to assess backend coding in realistic workflows, covering the entire development lifecycle.</li>\n    <li>It includes 224 practical tasks across 8 programming languages and 19 frameworks from open-source projects.</li>\n    <li>Results show that even advanced models struggle with these comprehensive tasks, indicating a gap in their capabilities for real backend engineering needs.</li>\n</ul>"}, "publishedAt": "2026-01-16T03:23:52.000Z", "title": "ABC-Bench: Benchmarking Agentic Backend Coding in Real-World Development", "summary": "The evolution of Large Language Models (LLMs) into autonomous agents has expanded the scope of AI coding from localized code generation to complex, repository-level, and execution-driven problem solving. However, current benchmarks predominantly evaluate code logic in static contexts, neglecting the dynamic, full-process requirements of real-world engineering, particularly in backend development which demands rigorous environment configuration and service deployment. To address this gap, we introduce ABC-Bench, a benchmark explicitly designed to evaluate agentic backend coding within a realistic, executable workflow. Using a scalable automated pipeline, we curated 224 practical tasks spanning 8 languages and 19 frameworks from open-source repositories. Distinct from previous evaluations, ABC-Bench require the agents to manage the entire development lifecycle from repository exploration to instantiating containerized services and pass the external end-to-end API tests. Our extensive evaluation reveals that even state-of-the-art models struggle to deliver reliable performance on these holistic tasks, highlighting a substantial disparity between current model capabilities and the demands of practical backend engineering. Our code is available at https://github.com/OpenMOSS/ABC-Bench.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11077.png", "numComments": 3, "submittedBy": {"_id": "66206a2136201a18e5329631", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66206a2136201a18e5329631/9aJiGOKshh1tZ171zDw_D.png", "fullname": "yangjie", "name": "red-fox-yj", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "613b0dee83ec35d460684607", "name": "OpenMOSS-Team", "fullname": "OpenMOSS", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61457b8deff2c9fdb4de4988/N5b9663zQ4uq5_OTNlnmw.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.11655", "authors": [{"_id": "6970436ba8be625b19c2ae97", "user": {"_id": "64d668bf54bb9eb7040c477e", "avatarUrl": "/avatars/b171b9c1cbb22e2f86e4280099c0bf93.svg", "isPro": false, "fullname": "Caihua Li", "user": "LoisNotLo", "type": "user"}, "name": "Caihua Li", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:20:05.617Z", "hidden": false}, {"_id": "6970436ba8be625b19c2ae98", "user": {"_id": "64c52b6de356b52a9868bce3", "avatarUrl": "/avatars/43b05cc691f273447e8bc65fe7515176.svg", "isPro": false, "fullname": "Guo", "user": "glh123456", "type": "user"}, "name": "Lianghong Guo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:20:12.738Z", "hidden": false}, {"_id": "6970436ba8be625b19c2ae99", "user": {"_id": "680ef06cce6b5c5af1f29aec", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/aTLjskuvCwTYs5JwjUtEF.png", "isPro": false, "fullname": "DeepSoftwareAnalytics", "user": "Yanlin-Wang", "type": "user"}, "name": "Yanlin Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:20:10.460Z", "hidden": false}, {"_id": "6970436ba8be625b19c2ae9a", "user": {"_id": "653df20eaa1f487614da4db1", "avatarUrl": "/avatars/12b27ce2c59f53b7e464039deab36a5d.svg", "isPro": false, "fullname": "Daya Guo", "user": "guoday", "type": "user"}, "name": "Daya Guo", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:20:15.421Z", "hidden": false}, {"_id": "6970436ba8be625b19c2ae9b", "user": {"_id": "6355473d525beaee688b7ba1", "avatarUrl": "/avatars/1fb0d57ed5f1a9b872a1ada8b2973ffb.svg", "isPro": false, "fullname": "Wei Tao", "user": "itaowe", "type": "user"}, "name": "Wei Tao", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:20:17.409Z", "hidden": false}, {"_id": "6970436ba8be625b19c2ae9c", "name": "Zhenyu Shan", "hidden": false}, {"_id": "6970436ba8be625b19c2ae9d", "name": "Mingwei Liu", "hidden": false}, {"_id": "6970436ba8be625b19c2ae9e", "name": "Jiachi Chen", "hidden": false}, {"_id": "6970436ba8be625b19c2ae9f", "name": "Haoyu Song", "hidden": false}, {"_id": "6970436ba8be625b19c2aea0", "name": "Duyu Tang", "hidden": false}, {"_id": "6970436ba8be625b19c2aea1", "name": "Hongyu Zhang", "hidden": false}, {"_id": "6970436ba8be625b19c2aea2", "name": "Zibin Zheng", "hidden": false}], "publishedAt": "2026-01-15T18:55:03.000Z", "submittedOnDailyAt": "2026-01-21T00:52:01.626Z", "title": "Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey", "submittedOnDailyBy": {"_id": "6355473d525beaee688b7ba1", "avatarUrl": "/avatars/1fb0d57ed5f1a9b872a1ada8b2973ffb.svg", "isPro": false, "fullname": "Wei Tao", "user": "itaowe", "type": "user"}, "summary": "Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution to serve as a dynamic resource in this field.", "upvotes": 49, "discussionId": "6970436ba8be625b19c2aea3", "projectPage": "https://deepsoftwareanalytics.github.io/Awesome-Issue-Resolution/", "githubRepo": "https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution", "githubRepoAddedBy": "user", "ai_summary": "Large language models face significant challenges in software issue resolution, prompting the development of autonomous coding agents through various training-free and training-based methodologies.", "ai_keywords": ["large language models", "software engineering", "autonomous coding agents", "training-free frameworks", "supervised fine-tuning", "reinforcement learning", "data quality", "agent behavior"], "githubStars": 40, "organization": {"_id": "680ef1aaccefecd5aee18d1d", "name": "Deep-Software-Analytics", "fullname": "DeepSoftwareAnalytics"}, "summary_zh": "<ul>\n    <li>\u95ee\u9898\u89e3\u51b3\u662f\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u4e00\u4e2a\u590d\u6742\u4efb\u52a1\uff0c\u5bf9\u4eba\u5de5\u667a\u80fd\u6765\u8bf4\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002</li>\n    <li>SWE-bench\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u8fd9\u4e2a\u4efb\u52a1\u5bf9\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u975e\u5e38\u56f0\u96be\uff0c\u63a8\u52a8\u4e86\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\u7684\u53d1\u5c55\u3002</li>\n    <li>\u672c\u6587\u7cfb\u7edf\u6027\u5730\u8c03\u67e5\u4e86\u8fd9\u4e00\u65b0\u5174\u9886\u57df\uff0c\u5206\u6790\u4e86\u6570\u636e\u6784\u5efa\u6d41\u7a0b\u548c\u5404\u79cd\u65b9\u6cd5\u8bba\u3002</li>\n    <li>\u8ba8\u8bba\u4e86\u6570\u636e\u8d28\u91cf\u3001\u4ee3\u7406\u884c\u4e3a\u7684\u5173\u952e\u5206\u6790\u4ee5\u53ca\u5b9e\u9645\u5e94\u7528\u3002</li>\n    <li>\u786e\u5b9a\u4e86\u4e3b\u8981\u6311\u6218\uff0c\u5e76\u6982\u8ff0\u4e86\u672a\u6765\u7814\u7a76\u7684\u6709\u5e0c\u671b\u65b9\u5411\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u6e90\u8d44\u6e90\u5e93\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Issue resolution in software engineering is a tough challenge for artificial intelligence.</li>\n    <li>Benchmarks like SWE-bench show that large language models struggle with this task.</li>\n    <li>This paper surveys the development of autonomous coding agents and their methodologies.</li>\n    <li>It covers data collection methods, analysis of techniques, and the quality of data and agent behavior.</li>\n    <li>An open-source repository is available for further resources in this area.</li>\n</ul>"}, "publishedAt": "2026-01-15T13:55:03.000Z", "title": "Advances and Frontiers of LLM-based Issue Resolution in Software Engineering: A Comprehensive Survey", "summary": "Issue resolution, a complex Software Engineering (SWE) task integral to real-world development, has emerged as a compelling challenge for artificial intelligence. The establishment of benchmarks like SWE-bench revealed this task as profoundly difficult for large language models, thereby significantly accelerating the evolution of autonomous coding agents. This paper presents a systematic survey of this emerging domain. We begin by examining data construction pipelines, covering automated collection and synthesis approaches. We then provide a comprehensive analysis of methodologies, spanning training-free frameworks with their modular components to training-based techniques, including supervised fine-tuning and reinforcement learning. Subsequently, we discuss critical analyses of data quality and agent behavior, alongside practical applications. Finally, we identify key challenges and outline promising directions for future research. An open-source repository is maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution to serve as a dynamic resource in this field.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11655.png", "numComments": 2, "submittedBy": {"_id": "6355473d525beaee688b7ba1", "avatarUrl": "/avatars/1fb0d57ed5f1a9b872a1ada8b2973ffb.svg", "fullname": "Wei Tao", "name": "itaowe", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 8, "isUserFollowing": false}, "organization": {"_id": "680ef1aaccefecd5aee18d1d", "name": "Deep-Software-Analytics", "fullname": "DeepSoftwareAnalytics"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.11496", "authors": [{"_id": "696de89f3f1837bfb8970ab3", "user": {"_id": "64802fb6c57f629056c59966", "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg", "isPro": false, "fullname": "Eilam Shapira", "user": "EilamSha", "type": "user"}, "name": "Eilam Shapira", "status": "admin_assigned", "statusLastChangedAt": "2026-01-19T14:37:41.780Z", "hidden": false}, {"_id": "696de89f3f1837bfb8970ab4", "name": "Roi Reichart", "hidden": false}, {"_id": "696de89f3f1837bfb8970ab5", "name": "Moshe Tennenholtz", "hidden": false}], "publishedAt": "2026-01-16T18:18:03.000Z", "submittedOnDailyAt": "2026-01-19T06:58:50.740Z", "title": "The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents", "submittedOnDailyBy": {"_id": "64802fb6c57f629056c59966", "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg", "isPro": false, "fullname": "Eilam Shapira", "user": "EilamSha", "type": "user"}, "summary": "The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the \"Poisoned Apple\" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator's choice of market design in their favor. This strategic release improves the releaser's welfare at the expense of their opponent and the regulator's fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.", "upvotes": 39, "discussionId": "696de8a03f1837bfb8970ab6", "organization": {"_id": "6393322be2364bc1eea56e45", "name": "Technion", "fullname": "Technion Israel institute of technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"}, "summary_zh": "<ul>\n    <li>\u5c06\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u5f15\u5165\u7ecf\u6d4e\u5e02\u573a\u4f1a\u6539\u53d8\u6218\u7565\u4e92\u52a8\u7684\u683c\u5c40\u3002</li>\n    <li>\u7814\u7a76\u4e86\u5728\u8ba8\u4ef7\u8fd8\u4ef7\u3001\u8c08\u5224\u548c\u8bf4\u670d\u7b49\u6e38\u620f\u7406\u8bba\u60c5\u5883\u4e2d\uff0c\u6280\u672f\u9009\u62e9\u6269\u5c55\u7684\u7ecf\u6d4e\u5f71\u54cd\u3002</li>\n    <li>\u589e\u52a0AI\u4ee3\u7406\u7684\u9009\u62e9\u53ef\u4ee5\u663e\u8457\u6539\u53d8\u5747\u8861\u6536\u76ca\u548c\u76d1\u7ba1\u7ed3\u679c\uff0c\u53ef\u80fd\u523a\u6fc0\u76d1\u7ba1\u8005\u4e3b\u52a8\u5f00\u53d1\u65b0\u6280\u672f\u3002</li>\n    <li>\u53d1\u73b0\u201c\u6bd2\u82f9\u679c\u6548\u5e94\u201d\uff1a\u4ee3\u7406\u53ef\u80fd\u4f1a\u53d1\u5e03\u65b0\u6280\u672f\uff0c\u76ee\u7684\u662f\u64cd\u7eb5\u76d1\u7ba1\u8005\u7684\u5e02\u573a\u8bbe\u8ba1\u9009\u62e9\u3002</li>\n    <li>\u9759\u6001\u76d1\u7ba1\u6846\u67b6\u5bb9\u6613\u88ab\u6280\u672f\u6269\u5c55\u64cd\u7eb5\uff0c\u56e0\u6b64\u9700\u8981\u7075\u6d3b\u7684\u5e02\u573a\u8bbe\u8ba1\u4ee5\u9002\u5e94AI\u80fd\u529b\u7684\u53d1\u5c55\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Introducing AI agents into economic markets changes how strategies are formed and interactions occur.</li>\n    <li>Adding more AI technology options can significantly alter outcomes in bargaining, negotiation, and persuasion scenarios.</li>\n    <li>More choices for AI can lead regulators to create and implement new technologies proactively.</li>\n    <li>The \"Poisoned Apple\" effect occurs when an agent releases a technology that is not used but influences the market rules in their favor.</li>\n    <li>This study shows that fixed regulations can be easily manipulated, suggesting the need for flexible market designs that keep up with AI advancements.</li>\n</ul>"}, "publishedAt": "2026-01-16T13:18:03.000Z", "title": "The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents", "summary": "The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the \"Poisoned Apple\" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator's choice of market design in their favor. This strategic release improves the releaser's welfare at the expense of their opponent and the regulator's fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.11496.png", "numComments": 2, "submittedBy": {"_id": "64802fb6c57f629056c59966", "avatarUrl": "/avatars/d5ecabaceeba759969855acf512b6649.svg", "fullname": "Eilam Shapira", "name": "EilamSha", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "6393322be2364bc1eea56e45", "name": "Technion", "fullname": "Technion Israel institute of technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1670591001944-63926124526c29d5b5011374.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.10355", "authors": [{"_id": "6969a11632f0333869ff9390", "user": {"_id": "65647e2b50a80d26dbfdf49c", "avatarUrl": "/avatars/aff0de9f9e4ed322e05d7f832c3c060d.svg", "isPro": false, "fullname": "Xu Zhihao", "user": "naiweizi", "type": "user"}, "name": "Zhihao Xu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:30:47.984Z", "hidden": false}, {"_id": "6969a11632f0333869ff9391", "name": "Rumei Li", "hidden": false}, {"_id": "6969a11632f0333869ff9392", "name": "Jiahuan Li", "hidden": false}, {"_id": "6969a11632f0333869ff9393", "user": {"_id": "601faf6053442c822abcad19", "avatarUrl": "/avatars/47889d2beea63fbaf46c203d00a33494.svg", "isPro": false, "fullname": "Rongxiang Weng", "user": "wengrx", "type": "user"}, "name": "Rongxiang Weng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-19T14:38:27.977Z", "hidden": false}, {"_id": "6969a11632f0333869ff9394", "user": {"_id": "647097cbcfd57849518e656b", "avatarUrl": "/avatars/c66fe0add29c1bde9e3a98bf4a8793b9.svg", "isPro": false, "fullname": "Jingang Wang", "user": "bitwjg", "type": "user"}, "name": "Jingang Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-19T14:38:22.544Z", "hidden": false}, {"_id": "6969a11632f0333869ff9395", "name": "Xunliang Cai", "hidden": false}, {"_id": "6969a11632f0333869ff9396", "user": {"_id": "640e962f3830fd441c2e250c", "avatarUrl": "/avatars/f88fbe06925064180b1867787b6d9a4d.svg", "isPro": false, "fullname": "Wang", "user": "Xiting", "type": "user"}, "name": "Xiting Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-19T14:38:07.595Z", "hidden": false}], "publishedAt": "2026-01-15T12:58:46.000Z", "submittedOnDailyAt": "2026-01-19T00:30:06.659Z", "title": "Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text", "submittedOnDailyBy": {"_id": "65647e2b50a80d26dbfdf49c", "avatarUrl": "/avatars/aff0de9f9e4ed322e05d7f832c3c060d.svg", "isPro": false, "fullname": "Xu Zhihao", "user": "naiweizi", "type": "user"}, "summary": "Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on \u03c4 - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs.", "upvotes": 30, "discussionId": "6969a11632f0333869ff9397", "ai_summary": "A text-based data synthesis approach generates multi-turn tool-use trajectories for large language models, achieving improved performance and reduced computational costs through a specialized trajectory synthesizer.", "ai_keywords": ["large language models", "multi-turn interactions", "tool-use data", "text corpora", "data synthesis pipeline", "relevance filtering", "workflow extraction", "trajectory grounding", "complexity refinement", "supervised fine-tuning", "trajectory synthesizer", "BFCL V3 Multi-turn benchmark", "\u03c4-bench", "inference latency"], "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "summary_zh": "<ul>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6587\u672c\u57fa\u7840\u8303\u5f0f\uff0c\u5229\u7528\u6587\u672c\u8bed\u6599\u4e2d\u7684\u591a\u6b65\u9aa4\u95ee\u9898\u89e3\u51b3\u7ecf\u9a8c\u6765\u751f\u6210\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u6570\u636e\u3002</li>\n    <li>\u4ecb\u7ecd\u4e86GEM\u6570\u636e\u5408\u6210\u7ba1\u9053\uff0c\u901a\u8fc7\u56db\u4e2a\u9636\u6bb5\u751f\u6210\u548c\u63d0\u53d6\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u8f68\u8ff9\uff1a\u76f8\u5173\u6027\u8fc7\u6ee4\u3001\u5de5\u4f5c\u6d41\u548c\u5de5\u5177\u63d0\u53d6\u3001\u8f68\u8ff9\u57fa\u7840\u548c\u590d\u6742\u6027\u7cbe\u70bc\u3002</li>\n    <li>\u4e3a\u4e86\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u8f68\u8ff9\u5408\u6210\u5668\uff0c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u8f68\u8ff9\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cGEM-32B\u5728\u591a\u8f6e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u9ad8\u4e8616.5%\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u8d85\u8fc7\u4e86\u57fa\u4e8e\u7279\u5b9a\u9886\u57df\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u3002</li>\n    <li>\u6211\u4eec\u7684\u8f68\u8ff9\u5408\u6210\u5668\u5728\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u548c\u6210\u672c\u7684\u540c\u65f6\uff0c\u5339\u914d\u4e86\u5b8c\u6574\u7ba1\u9053\u7684\u8d28\u91cf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Large Language Models (LLMs) need to use tools well in conversations for better autonomous agents.</li>\n    <li>It's hard to find good examples of multi-turn tool use, so a new method is needed.</li>\n    <li>The proposed method, called GEM, uses existing text to create useful multi-step problem-solving data.</li>\n    <li>GEM works in four steps to generate these tool-use examples, and a special model helps make this process faster and cheaper.</li>\n    <li>Tests show that GEM-32B improves performance on benchmarks and can compete with models trained on specific industry data.</li>\n</ul>"}, "publishedAt": "2026-01-15T07:58:46.000Z", "title": "Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text", "summary": "Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on \u03c4 - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10355.png", "numComments": 2, "submittedBy": {"_id": "65647e2b50a80d26dbfdf49c", "avatarUrl": "/avatars/aff0de9f9e4ed322e05d7f832c3c060d.svg", "fullname": "Xu Zhihao", "name": "naiweizi", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "68b28d79a176a9beb30d2049", "name": "meituan-longcat", "fullname": "LongCat", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/68a2a29ab9d4c5698e02c747/CDCAx7X7rXDt7xjI-DoxG.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.10305", "authors": [{"_id": "6969a0b932f0333869ff9381", "user": {"_id": "67e289aea1e569cd0a41db1d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/w7b_6u7nZDH9Lp6NJKdVJ.png", "isPro": false, "fullname": "shen hengyu", "user": "dewecho", "type": "user"}, "name": "Hengyu Shen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:35:54.646Z", "hidden": false}, {"_id": "6969a0b932f0333869ff9382", "user": {"_id": "641030c77a15af878ae5bd8f", "avatarUrl": "/avatars/8a5037edf55c78ebc317c8b191343671.svg", "isPro": false, "fullname": "TianchengGu", "user": "TianchengGu", "type": "user"}, "name": "Tiancheng Gu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:36:00.059Z", "hidden": false}, {"_id": "6969a0b932f0333869ff9383", "name": "Bin Qin", "hidden": false}, {"_id": "6969a0b932f0333869ff9384", "name": "Lan Wu", "hidden": false}, {"_id": "6969a0b932f0333869ff9385", "name": "Yuling Wu", "hidden": false}, {"_id": "6969a0b932f0333869ff9386", "name": "Shuo Tan", "hidden": false}, {"_id": "6969a0b932f0333869ff9387", "user": {"_id": "63dfc05342591dda0b945e58", "avatarUrl": "/avatars/3fd796035c2243d6b03cc361bc06e64e.svg", "isPro": false, "fullname": "Zelong Sun", "user": "dfgdgh", "type": "user"}, "name": "Zelong Sun", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:36:29.711Z", "hidden": false}, {"_id": "6969a0b932f0333869ff9388", "name": "Jun Wang", "hidden": false}, {"_id": "6969a0b932f0333869ff9389", "name": "Nan Wu", "hidden": false}, {"_id": "6969a0b932f0333869ff938a", "name": "Xiang An", "hidden": false}, {"_id": "6969a0b932f0333869ff938b", "user": {"_id": "6760a8f5e4b55ba1b2b0a7b4", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NddUMmwmZFbS25v1q8KyS.png", "isPro": false, "fullname": "Weidong Cai", "user": "SeriousBro", "type": "user"}, "name": "Weidong Cai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:36:16.748Z", "hidden": false}, {"_id": "6969a0b932f0333869ff938c", "user": {"_id": "694d00c3ece16a65e2b84774", "avatarUrl": "/avatars/72bfeec4602ba4069faf0dba02c2be96.svg", "isPro": false, "fullname": "Ziyong Feng", "user": "fengziyong", "type": "user"}, "name": "Ziyong Feng", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:36:10.787Z", "hidden": false}, {"_id": "6969a0b932f0333869ff938d", "user": {"_id": "63e202f352b7578dba448ab5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg", "isPro": false, "fullname": "Kaicheng Yang", "user": "Kaichengalex", "type": "user"}, "name": "Kaicheng Yang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:30:51.089Z", "hidden": false}], "publishedAt": "2026-01-15T11:28:58.000Z", "submittedOnDailyAt": "2026-01-16T00:37:46.383Z", "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset", "submittedOnDailyBy": {"_id": "63e202f352b7578dba448ab5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg", "isPro": false, "fullname": "Kaicheng Yang", "user": "Kaichengalex", "type": "user"}, "summary": "Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.", "upvotes": 29, "discussionId": "6969a0b932f0333869ff938e", "projectPage": "https://deepglint.github.io/DanQing/", "githubRepo": "https://github.com/deepglint/DanQing", "githubRepoAddedBy": "user", "ai_summary": "A large-scale Chinese image-text dataset called DanQing is introduced to advance vision-language pretraining, demonstrating superior performance in various downstream tasks through continual pretraining of the SigLIP2 model.", "ai_keywords": ["Vision-Language Pre-training", "contrastive pretraining", "cross-modal retrieval", "image captioning", "SigLIP2", "continual pre-training"], "githubStars": 12, "summary_zh": "<ul>\n    <li>\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u4e2d\u6587\u8de8\u6a21\u6001\u6570\u636e\u96c6\uff0c\u540d\u4e3aDanQing\uff0c\u5305\u542b1\u4ebf\u5bf9\u56fe\u50cf-\u6587\u672c\u6570\u636e\u3002</li>\n    <li>DanQing\u7684\u6570\u636e\u4e3b\u8981\u6765\u81ea2024-2025\u5e74\u7684\u7f51\u7edc\u6570\u636e\uff0c\u80fd\u66f4\u597d\u5730\u6355\u6349\u8bed\u4e49\u8d8b\u52bf\u3002</li>\n    <li>\u4e0e\u73b0\u6709\u6570\u636e\u96c6\u76f8\u6bd4\uff0cDanQing\u7ecf\u8fc7\u66f4\u4e25\u683c\u7684\u7b5b\u9009\u8fc7\u7a0b\uff0c\u6570\u636e\u8d28\u91cf\u66f4\u9ad8\u3002</li>\n    <li>\u901a\u8fc7\u5bf9SigLIP2\u6a21\u578b\u7684\u6301\u7eed\u9884\u8bad\u7ec3\uff0cDanQing\u5728\u591a\u4e2a\u4e2d\u6587\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002</li>\n    <li>DanQing\u6570\u636e\u96c6\u5c06\u4ee5CC-BY 4.0\u8bb8\u53ef\u8bc1\u5f00\u6e90\uff0c\u4fc3\u8fdb\u4e2d\u6587\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u7684\u7814\u7a76\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Vision-Language Pre-training (VLP) models are effective for tasks like image captioning and cross-modal retrieval using large image-text pairs.</li>\n    <li>There's a lack of high-quality Chinese image-text data compared to English datasets, which has slowed progress in Chinese VLP models.</li>\n    <li>To fill this gap, the DanQing dataset was created, containing 100 million high-quality image-text pairs collected from the web.</li>\n    <li>DanQing is curated with a strict selection process and includes recent web data from 2024-2025, improving its relevance and quality.</li>\n    <li>Experiments show that DanQing significantly outperforms existing datasets in various Chinese tasks, and it will be made available for public use.</li>\n</ul>"}, "publishedAt": "2026-01-15T06:28:58.000Z", "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset", "summary": "Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10305.png", "numComments": 2, "submittedBy": {"_id": "63e202f352b7578dba448ab5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e202f352b7578dba448ab5/8itVBLcv14m7OVsoF8h1o.jpeg", "fullname": "Kaicheng Yang", "name": "Kaichengalex", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.14250", "authors": [{"_id": "69705b78a8be625b19c2af4c", "user": {"_id": "6697765937d24838267b41e7", "avatarUrl": "/avatars/682bb4cf0a0009812b42748dc26916f9.svg", "isPro": false, "fullname": "PangzeCheung", "user": "PangzeCheung", "type": "user"}, "name": "Pengze Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:19:32.663Z", "hidden": false}, {"_id": "69705b78a8be625b19c2af4d", "name": "Yanze Wu", "hidden": false}, {"_id": "69705b78a8be625b19c2af4e", "user": {"_id": "6805bdfb344d6d8a8fd5b07a", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dGLeWvN2CXLmxVP_b9S4R.png", "isPro": false, "fullname": "Mengtian Li", "user": "LemonSky1995", "type": "user"}, "name": "Mengtian Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:52:30.670Z", "hidden": false}, {"_id": "69705b78a8be625b19c2af4f", "name": "Xu Bai", "hidden": false}, {"_id": "69705b78a8be625b19c2af50", "name": "Songtao Zhao", "hidden": false}, {"_id": "69705b78a8be625b19c2af51", "user": {"_id": "6339029a76421c0543167075", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6339029a76421c0543167075/3npT0NxTMV-MLWDThyBy8.png", "isPro": false, "fullname": "fulong ye", "user": "Alon77777", "type": "user"}, "name": "Fulong Ye", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T11:46:59.343Z", "hidden": false}, {"_id": "69705b78a8be625b19c2af52", "name": "Chong Mou", "hidden": false}, {"_id": "69705b78a8be625b19c2af53", "user": {"_id": "6752cd83ffaeeb979db974ae", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png", "isPro": false, "fullname": "Xinghui Li", "user": "Crayon-Shinchan", "type": "user"}, "name": "Xinghui Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:52:58.973Z", "hidden": false}, {"_id": "69705b78a8be625b19c2af54", "user": {"_id": "6304e2dabad6ce7fc0287d57", "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg", "isPro": false, "fullname": "Zhuowei_Chen", "user": "ZhuoweiChen", "type": "user"}, "name": "Zhuowei Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:52:20.177Z", "hidden": false}, {"_id": "69705b78a8be625b19c2af55", "name": "Qian He", "hidden": false}, {"_id": "69705b78a8be625b19c2af56", "user": {"_id": "671aa30b496f0bc5ae04da4b", "avatarUrl": "/avatars/902d7f9fd56f84953d67d9229bd9d6b7.svg", "isPro": false, "fullname": "Mingyuan Gao", "user": "GMY1999", "type": "user"}, "name": "Mingyuan Gao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:52:13.464Z", "hidden": false}], "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6697765937d24838267b41e7/309vLCCbUoiHGJkN0u3LH.mp4"], "publishedAt": "2026-01-20T18:58:11.000Z", "submittedOnDailyAt": "2026-01-21T02:29:32.365Z", "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer", "submittedOnDailyBy": {"_id": "6697765937d24838267b41e7", "avatarUrl": "/avatars/682bb4cf0a0009812b42748dc26916f9.svg", "isPro": false, "fullname": "PangzeCheung", "user": "PangzeCheung", "type": "user"}, "summary": "Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.", "upvotes": 29, "discussionId": "69705b78a8be625b19c2af57", "projectPage": "https://pangzecheung.github.io/OmniTransfer/", "githubRepo": "https://github.com/PangzeCheung/OmniTransfer", "githubRepoAddedBy": "user", "ai_summary": "OmniTransfer presents a unified framework for spatio-temporal video transfer that enhances appearance consistency and temporal control through multi-view information and multimodal semantic guidance.", "ai_keywords": ["video customization", "spatio-temporal video transfer", "multi-view information", "temporal cues", "temporal alignment", "appearance consistency", "reference-decoupled causal learning", "task-adaptive multimodal alignment", "pose-guided methods"], "githubStars": 54, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u6bd4\u56fe\u50cf\u6216\u6587\u672c\u4f20\u9012\u66f4\u4e30\u5bcc\u7684\u4fe1\u606f\uff0c\u5305\u542b\u7a7a\u95f4\u548c\u65f6\u95f4\u7684\u52a8\u6001\u3002</li>\n    <li>\u73b0\u6709\u7684\u89c6\u9891\u5b9a\u5236\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u53c2\u8003\u56fe\u50cf\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u89c6\u9891\u4e2d\u7684\u4e30\u5bcc\u65f6\u7a7a\u4fe1\u606f\u3002</li>\n    <li>\u63d0\u51fa\u4e86OmniTransfer\u6846\u67b6\uff0c\u65e8\u5728\u6539\u5584\u89c6\u9891\u8f6c\u79fb\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u548c\u63a7\u5236\u80fd\u529b\u3002</li>\n    <li>OmniTransfer\u7ed3\u5408\u4e86\u4e09\u9879\u5173\u952e\u8bbe\u8ba1\uff0c\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cOmniTransfer\u5728\u5916\u89c2\u548c\u65f6\u95f4\u8f6c\u79fb\u65b9\u9762\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5f00\u521b\u4e86\u9ad8\u4fdd\u771f\u89c6\u9891\u751f\u6210\u7684\u65b0\u6a21\u5f0f\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Videos provide more detailed information than images or text by showing how things change over time and space.</li>\n    <li>Many current video customization methods do not use the full potential of video information, limiting their flexibility.</li>\n    <li>OmniTransfer is a new framework that improves video transfer by using information from multiple frames and controlling timing more precisely.</li>\n    <li>It includes three main features to enhance performance: adapting to video tasks, separating reference and target information for efficiency, and using different types of guidance for various tasks.</li>\n    <li>OmniTransfer outperforms current methods in both appearance and timing while still achieving good motion transfer results.</li>\n</ul>"}, "publishedAt": "2026-01-20T13:58:11.000Z", "title": "OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer", "summary": "Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.", "mediaUrls": ["https://cdn-uploads.huggingface.co/production/uploads/6697765937d24838267b41e7/309vLCCbUoiHGJkN0u3LH.mp4"], "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14250.png", "numComments": 4, "submittedBy": {"_id": "6697765937d24838267b41e7", "avatarUrl": "/avatars/682bb4cf0a0009812b42748dc26916f9.svg", "fullname": "PangzeCheung", "name": "PangzeCheung", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "organization": {"_id": "653b817d32c97d0655575872", "name": "ByteDance", "fullname": "ByteDance", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/6535c9e88bde2fae19b6fb25/0clr54wj5Ly-RkYU9OXPp.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.14192", "authors": [{"_id": "69705a68a8be625b19c2af3a", "user": {"_id": "6745c589d2d740914ec2574f", "avatarUrl": "/avatars/7b2ff6848d42cd140a775df0c2bc9384.svg", "isPro": false, "fullname": "Xiaofang Yang", "user": "fffovo", "type": "user"}, "name": "Xiaofang Yang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:48:39.583Z", "hidden": false}, {"_id": "69705a68a8be625b19c2af3b", "name": "Lijun Li", "hidden": false}, {"_id": "69705a68a8be625b19c2af3c", "user": {"_id": "660d17d6c9be0dcd31a30b3d", "avatarUrl": "/avatars/3743fe9b695c488ebe33f0d8fd607a8a.svg", "isPro": false, "fullname": "Zhou Heng", "user": "henggg", "type": "user"}, "name": "Heng Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-21T09:19:49.600Z", "hidden": false}, {"_id": "69705a68a8be625b19c2af3d", "name": "Tong Zhu", "hidden": false}, {"_id": "69705a68a8be625b19c2af3e", "name": "Xiaoye Qu", "hidden": false}, {"_id": "69705a68a8be625b19c2af3f", "name": "Yuchen Fan", "hidden": false}, {"_id": "69705a68a8be625b19c2af40", "user": {"_id": "6952244bfbddb08cb2562f3b", "avatarUrl": "/avatars/70d67319af29604129378fee3f216757.svg", "isPro": false, "fullname": "qianshan wei", "user": "b1intern", "type": "user"}, "name": "Qianshan Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:04.253Z", "hidden": false}, {"_id": "69705a68a8be625b19c2af41", "name": "Rui Ye", "hidden": false}, {"_id": "69705a68a8be625b19c2af42", "name": "Li Kang", "hidden": false}, {"_id": "69705a68a8be625b19c2af43", "name": "Yiran Qin", "hidden": false}, {"_id": "69705a68a8be625b19c2af44", "name": "Zhiqiang Kou", "hidden": false}, {"_id": "69705a68a8be625b19c2af45", "name": "Daizong Liu", "hidden": false}, {"_id": "69705a68a8be625b19c2af46", "name": "Qi Li", "hidden": false}, {"_id": "69705a68a8be625b19c2af47", "name": "Ning Ding", "hidden": false}, {"_id": "69705a68a8be625b19c2af48", "user": {"_id": "65257545b017be1fc1915364", "avatarUrl": "/avatars/9bffd3fb567d2fa1e5c3546d77560b43.svg", "isPro": false, "fullname": "Siheng Chen", "user": "sihengchen", "type": "user"}, "name": "Siheng Chen", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T10:49:26.261Z", "hidden": false}, {"_id": "69705a68a8be625b19c2af49", "name": "Jing Shao", "hidden": false}], "publishedAt": "2026-01-20T17:51:56.000Z", "submittedOnDailyAt": "2026-01-21T02:28:39.429Z", "title": "Toward Efficient Agents: Memory, Tool learning, and Planning", "submittedOnDailyBy": {"_id": "641d3efac3983aa9491677b9", "avatarUrl": "/avatars/53565486351c16a1ac8ea863963e2d9b.svg", "isPro": false, "fullname": "Lijun Li", "user": "adwardlee", "type": "user"}, "summary": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.", "upvotes": 29, "discussionId": "69705a69a8be625b19c2af4a", "projectPage": "https://efficient-agents.github.io/", "githubRepo": "https://github.com/yxf203/Awesome-Efficient-Agents", "githubRepoAddedBy": "user", "ai_summary": "Efficiency in agentic systems is examined across memory, tool learning, and planning components, analyzing trade-offs between effectiveness and computational costs through various optimization strategies and benchmarks.", "ai_keywords": ["large language models", "agentic systems", "memory", "tool learning", "planning", "latency", "tokens", "steps", "reinforcement learning", "controlled search mechanisms", "Pareto frontier", "efficiency metrics", "evaluation protocols"], "githubStars": 27, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "summary_zh": "<ul>\n    <li>\u8fd1\u5e74\u6765\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u7814\u7a76\u5173\u6ce8\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u4e3a\u667a\u80fd\u7cfb\u7edf\u3002</li>\n    <li>\u5c3d\u7ba1\u667a\u80fd\u4f53\u7684\u6709\u6548\u6027\u4e0d\u65ad\u63d0\u9ad8\uff0c\u4f46\u6548\u7387\uff08\u5982\u5ef6\u8fdf\u3001\u8017\u65f6\u7b49\uff09\u5e38\u5e38\u88ab\u5ffd\u89c6\u3002</li>\n    <li>\u672c\u6587\u4ece\u8bb0\u5fc6\u3001\u5de5\u5177\u5b66\u4e60\u548c\u89c4\u5212\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u63a2\u8ba8\u667a\u80fd\u4f53\u7684\u6548\u7387\u3002</li>\n    <li>\u6211\u4eec\u56de\u987e\u4e86\u4e0d\u540c\u5b9e\u73b0\u65b9\u6cd5\u7684\u7814\u7a76\uff0c\u5f3a\u8c03\u5171\u4eab\u7684\u9ad8\u5c42\u539f\u5219\u4ee5\u63d0\u9ad8\u6548\u7387\u3002</li>\n    <li>\u6700\u540e\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\uff0c\u4ee5\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The paper focuses on improving the efficiency of large language model agents, which is important for real-world use.</li>\n    <li>It examines three main areas of agent efficiency: memory, tool learning, and planning, looking at costs like speed and resource usage.</li>\n    <li>The authors review various approaches that aim to enhance efficiency while sharing common principles, such as reducing context size and optimizing reinforcement learning rewards.</li>\n    <li>Efficiency is evaluated by comparing effectiveness within set costs and analyzing costs at similar effectiveness levels.</li>\n    <li>The paper identifies key challenges and future directions to enhance agent efficiency and provides insights for further research.</li>\n</ul>"}, "publishedAt": "2026-01-20T12:51:56.000Z", "title": "Toward Efficient Agents: Memory, Tool learning, and Planning", "summary": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.14192.png", "numComments": 2, "submittedBy": {"_id": "641d3efac3983aa9491677b9", "avatarUrl": "/avatars/53565486351c16a1ac8ea863963e2d9b.svg", "fullname": "Lijun Li", "name": "adwardlee", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 1, "isUserFollowing": false}, "organization": {"_id": "6747ee5decec679eafb90450", "name": "ShanghaiAiLab", "fullname": "shanghai ailab "}, "isAuthorParticipating": false}, {"paper": {"id": "2601.13029", "authors": [{"_id": "69708ffea8be625b19c2b04c", "user": {"_id": "6575702b15b1ca184b0b2700", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575702b15b1ca184b0b2700/O9cEodqQmG-gyqMiO_edR.jpeg", "isPro": false, "fullname": "Zaibin Zhang", "user": "MrBean2024", "type": "user"}, "name": "Zaibin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T11:47:45.167Z", "hidden": false}, {"_id": "69708ffea8be625b19c2b04d", "name": "Yuhan Wu", "hidden": false}, {"_id": "69708ffea8be625b19c2b04e", "name": "Lianjie Jia", "hidden": false}, {"_id": "69708ffea8be625b19c2b04f", "name": "Yifan Wang", "hidden": false}, {"_id": "69708ffea8be625b19c2b050", "name": "Zhongbo Zhang", "hidden": false}, {"_id": "69708ffea8be625b19c2b051", "user": {"_id": "6965e7d00aa591efb07b220c", "avatarUrl": "/avatars/d0f65aafc3b652084213f02a4f93c453.svg", "isPro": false, "fullname": "Yijiang Li", "user": "luciasnowblack", "type": "user"}, "name": "Yijiang Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T11:59:41.952Z", "hidden": false}, {"_id": "69708ffea8be625b19c2b052", "name": "Binghao Ran", "hidden": false}, {"_id": "69708ffea8be625b19c2b053", "name": "Fuxi Zhang", "hidden": false}, {"_id": "69708ffea8be625b19c2b054", "user": {"_id": "68ad6a9106bcf0ebe9624dc5", "avatarUrl": "/avatars/309e383889f848c828d4b1eb4542b54a.svg", "isPro": false, "fullname": "SunZhuohan", "user": "sunz525", "type": "user"}, "name": "Zhuohan Sun", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T11:59:29.495Z", "hidden": false}, {"_id": "69708ffea8be625b19c2b055", "user": {"_id": "64e314ad24809d7fa0f20fbc", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/bHE0w_hjDFvU-Aul0_E7g.jpeg", "isPro": false, "fullname": "Zhenfei Yin", "user": "JeremyYin", "type": "user"}, "name": "Zhenfei Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-21T11:59:18.401Z", "hidden": false}, {"_id": "69708ffea8be625b19c2b056", "name": "Lijun Wang", "hidden": false}, {"_id": "69708ffea8be625b19c2b057", "name": "Huchuan Lu", "hidden": false}], "publishedAt": "2026-01-19T13:13:54.000Z", "submittedOnDailyAt": "2026-01-21T06:09:04.854Z", "title": "Think3D: Thinking with Space for Spatial Reasoning", "submittedOnDailyBy": {"_id": "6419309f22270b3ccf177c77", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6419309f22270b3ccf177c77/KQa1586iBBKqucUlfpuPp.jpeg", "isPro": false, "fullname": "William Li", "user": "williamium", "type": "user"}, "summary": "Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.", "upvotes": 29, "discussionId": "69708fffa8be625b19c2b058", "githubRepo": "https://github.com/zhangzaibin/spagent", "githubRepoAddedBy": "user", "ai_summary": "Think3D enhances vision-language models' 3D reasoning capabilities by enabling interactive spatial exploration through 3D reconstruction and camera-based operations, improving performance without additional training.", "ai_keywords": ["vision large models", "3D reconstruction models", "point clouds", "camera poses", "spatial reasoning", "3D chain-of-thought process", "reinforcement learning policy", "multimodal agents", "3D reasoning"], "githubStars": 32, "summary_zh": "<ul>\n    <li>\u7406\u89e3\u548c\u63a8\u7406\u7269\u7406\u4e16\u754c\u9700\u8981\u7a7a\u95f4\u667a\u6167\uff0c\u5373\u8d85\u8d8a2D\u611f\u77e5\u7684\u51e0\u4f55\u3001\u89c6\u89d2\u548c\u7a7a\u95f4\u5173\u7cfb\u7684\u80fd\u529b\u3002</li>\n    <li>\u867d\u7136\u73b0\u6709\u7684\u89c6\u89c9\u5927\u578b\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u771f\u6b63\u76843D\u63a8\u7406\u4e0a\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>Think3D\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u4f7f\u89c6\u89c9\u5927\u578b\u6a21\u578b\u80fd\u591f\u57283D\u7a7a\u95f4\u4e2d\u8fdb\u884c\u601d\u8003\uff0c\u901a\u8fc73D\u91cd\u5efa\u6a21\u578b\u6765\u64cd\u4f5c\u7a7a\u95f4\u3002</li>\n    <li>Think3D\u663e\u8457\u63d0\u9ad8\u4e86\u5148\u8fdb\u6a21\u578b\uff08\u5982GPT-4.1\u548cGemini 2.5 Pro\uff09\u5728\u7a7a\u95f4\u63a8\u7406\u4e0a\u7684\u8868\u73b0\uff0c\u5e73\u5747\u63d0\u5347\u4e867.8%\u52304.7%\u3002</li>\n    <li>\u5c0f\u578b\u6a21\u578b\u5728\u7a7a\u95f4\u63a2\u7d22\u4e2d\u53d7\u76ca\u4e8e\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5de5\u5177\u4f7f\u7528\u7684\u6536\u76ca\u4ece0.7%\u589e\u52a0\u52306.8%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Understanding the physical world involves spatial intelligence, which goes beyond just 2D images.</li>\n    <li>Think3D is a new framework that helps vision models think in 3D by using 3D reconstruction from images or videos.</li>\n    <li>This framework allows models to manipulate space and reason in 3D without needing extra training.</li>\n    <li>Think3D improves performance in spatial reasoning tasks for advanced models like GPT-4.1 and Gemini 2.5 Pro.</li>\n    <li>Smaller models also benefit from a reinforcement learning technique that helps them choose better viewpoints and actions.</li>\n</ul>"}, "publishedAt": "2026-01-19T08:13:54.000Z", "title": "Think3D: Thinking with Space for Spatial Reasoning", "summary": "Understanding and reasoning about the physical world requires spatial intelligence: the ability to interpret geometry, perspective, and spatial relations beyond 2D perception. While recent vision large models (VLMs) excel at visual understanding, they remain fundamentally 2D perceivers and struggle with genuine 3D reasoning. We introduce Think3D, a framework that enables VLM agents to think with 3D space. By leveraging 3D reconstruction models that recover point clouds and camera poses from images or videos, Think3D allows the agent to actively manipulate space through camera-based operations and ego/global-view switching, transforming spatial reasoning into an interactive 3D chain-of-thought process. Without additional training, Think3D significantly improves the spatial reasoning performance of advanced models such as GPT-4.1 and Gemini 2.5 Pro, yielding average gains of +7.8% on BLINK Multi-view and MindCube, and +4.7% on VSI-Bench. We further show that smaller models, which struggle with spatial exploration, benefit significantly from a reinforcement learning policy that enables the model to select informative viewpoints and operations. With RL, the benefit from tool usage increases from +0.7% to +6.8%. Our findings demonstrate that training-free, tool-augmented spatial exploration is a viable path toward more flexible and human-like 3D reasoning in multimodal agents, establishing a new dimension of multimodal intelligence. Code and weights are released at https://github.com/zhangzaibin/spagent.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.13029.png", "numComments": 1, "submittedBy": {"_id": "6419309f22270b3ccf177c77", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6419309f22270b3ccf177c77/KQa1586iBBKqucUlfpuPp.jpeg", "fullname": "William Li", "name": "williamium", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "isAuthorParticipating": false}],
    "month": [{"paper": {"id": "2601.06943", "authors": [{"_id": "6965babdfc8c4ecc02c7f8f5", "user": {"_id": "6965e8d162405ba787fc50b2", "avatarUrl": "/avatars/52858daa454e710712c8a29307e0fe30.svg", "isPro": false, "fullname": "Chengwen Liu", "user": "POTATO66", "type": "user"}, "name": "Chengwen Liu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:54.096Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f6", "user": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "name": "Xiaomin Yu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-13T15:46:34.064Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f7", "name": "Zhuoyue Chang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f8", "name": "Zhe Huang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8f9", "name": "Shuo Zhang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fa", "name": "Heng Lian", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fb", "name": "Kunyi Wang", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fc", "name": "Rui Xu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fd", "name": "Sen Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8fe", "user": {"_id": "65e459ef400c626ca0968db7", "avatarUrl": "/avatars/23177b73ba6e4a9db1165d0b7036a4b7.svg", "isPro": false, "fullname": "Hou", "user": "HJH2CMD", "type": "user"}, "name": "Jianheng Hou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:36.919Z", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f8ff", "name": "Hao Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f900", "name": "Chengwei Qin", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f901", "name": "Xiaobin Hu", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f902", "name": "Hong Peng", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f903", "name": "Ronghao Chen", "hidden": false}, {"_id": "6965babdfc8c4ecc02c7f904", "name": "Huacan Wang", "hidden": false}], "publishedAt": "2026-01-11T15:07:37.000Z", "submittedOnDailyAt": "2026-01-13T01:12:08.706Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "submittedOnDailyBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "isPro": false, "fullname": "Yu_xm", "user": "Yu2020", "type": "user"}, "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "upvotes": 172, "discussionId": "6965babdfc8c4ecc02c7f905", "githubRepo": "https://github.com/QuantaAlpha/VideoDR-Benchmark", "githubRepoAddedBy": "user", "ai_summary": "VideoDR benchmark enables video question answering by combining cross-frame visual extraction, web retrieval, and multi-hop reasoning in open-domain settings.", "ai_keywords": ["video question answering", "cross-frame visual anchor extraction", "interactive web retrieval", "multi-hop reasoning", "multimodal large language models", "Workflow paradigm", "Agentic paradigm", "goal drift", "long-horizon consistency"], "githubStars": 51, "summary_zh": "<ul>\n    <li>\u89c6\u9891\u95ee\u7b54\u9700\u8981\u4ece\u89c6\u9891\u548c\u7f51\u7edc\u4e2d\u63d0\u53d6\u4fe1\u606f\uff0c\u5e76\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u548c\u9a8c\u8bc1\u3002</li>\n    <li>\u6211\u4eec\u521b\u5efa\u4e86\u7b2c\u4e00\u4e2a\u89c6\u9891\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\uff0c\u79f0\u4e3aVideoDR\uff0c\u4e13\u6ce8\u4e8e\u89c6\u9891\u6761\u4ef6\u4e0b\u7684\u5f00\u653e\u9886\u57df\u95ee\u7b54\u3002</li>\n    <li>VideoDR \u5305\u542b\u8de8\u5e27\u89c6\u89c9\u951a\u63d0\u53d6\u3001\u4e92\u52a8\u7f51\u7edc\u68c0\u7d22\u548c\u591a\u6b65\u63a8\u7406\u7b49\u4efb\u52a1\u3002</li>\n    <li>\u6211\u4eec\u8bc4\u4f30\u4e86\u591a\u79cd\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u5728\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\u3002</li>\n    <li>VideoDR \u65e8\u5728\u5e2e\u52a9\u7814\u7a76\u89c6\u9891\u667a\u80fd\u4f53\u5728\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u6311\u6218\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Video question answering needs to extract clues from videos and find answers on the web, which requires complex reasoning.</li>\n    <li>We created VideoDR, a benchmark for testing video-based question answering that combines video and web information.</li>\n    <li>VideoDR includes high-quality video samples across six different topics, verified by human annotators.</li>\n    <li>We tested various language models and found that their performance depends on keeping track of video clues during multiple retrieval steps.</li>\n    <li>VideoDR highlights important challenges for future video research agents, such as maintaining focus and consistency over long tasks.</li>\n</ul>"}, "publishedAt": "2026-01-11T10:07:37.000Z", "title": "Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning", "summary": "In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06943.png", "numComments": 4, "submittedBy": {"_id": "64084fa192033c150738e4f2", "avatarUrl": "/avatars/dfff2216eb235c635e5abe6fda3084f0.svg", "fullname": "Yu_xm", "name": "Yu2020", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.06521", "authors": [{"_id": "6965c124fc8c4ecc02c7f930", "name": "Liang Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f931", "name": "Weichu Xie", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f932", "name": "Yiyan Liang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f933", "name": "Hongfeng He", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f934", "name": "Hans Zhao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f935", "name": "Zhibo Yang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f936", "name": "Zhiqi Huang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f937", "name": "Haoning Wu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f938", "name": "Haoyu Lu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f939", "name": "Y. charles", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93a", "name": "Yiping Bao", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93b", "name": "Yuantao Fan", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93c", "name": "Guopeng Li", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93d", "name": "Haiyang Shen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93e", "user": {"_id": "65e6970d135c27ea806526fe", "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg", "isPro": false, "fullname": "Xuanzhong Chen", "user": "chenxz", "type": "user"}, "name": "Xuanzhong Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:52.086Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f93f", "name": "Wendong Xu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f940", "user": {"_id": "637c99bbfe115289cfedfb44", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/p4uSY0TKufJfcHpvEb_ZQ.jpeg", "isPro": false, "fullname": "ssz", "user": "ssz1111", "type": "user"}, "name": "Shuzheng Si", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:32.968Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f941", "name": "Zefan Cai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f942", "name": "Wenhao Chai", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f943", "user": {"_id": "60efe7fa0d920bc7805cada5", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60efe7fa0d920bc7805cada5/2LBrJBjSCOP5ilZIpWLHl.png", "isPro": false, "fullname": "Ziqi Huang", "user": "Ziqi", "type": "user"}, "name": "Ziqi Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T08:23:50.242Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f944", "user": {"_id": "6505a02f9310ce8c400edc63", "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg", "isPro": false, "fullname": "Fangfu Liu", "user": "Liuff23", "type": "user"}, "name": "Fangfu Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-13T15:45:35.158Z", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f945", "name": "Tianyu Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f946", "name": "Baobao Chang", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f947", "name": "Xiaobo Hu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f948", "name": "Kaiyuan Chen", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f949", "name": "Yixin Ren", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94a", "name": "Yang Liu", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94b", "name": "Yuan Gong", "hidden": false}, {"_id": "6965c124fc8c4ecc02c7f94c", "name": "Kuan Li", "hidden": false}], "publishedAt": "2026-01-10T10:42:44.000Z", "submittedOnDailyAt": "2026-01-13T01:21:01.708Z", "title": "BabyVision: Visual Reasoning Beyond Language", "submittedOnDailyBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "isPro": true, "fullname": "taesiri", "user": "taesiri", "type": "user"}, "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "upvotes": 146, "discussionId": "6965c124fc8c4ecc02c7f94d", "projectPage": "https://unipat.ai/blog/BabyVision", "githubRepo": "https://github.com/UniPat-AI/BabyVision", "githubRepoAddedBy": "user", "ai_summary": "Current multimodal large language models exhibit significant gaps in fundamental visual understanding compared to human children, as demonstrated by the BabyVision benchmark.", "ai_keywords": ["Multimodal LLMs", "visual reasoning", "core visual skills", "BabyVision benchmark", "visual perception", "visual primitives"], "githubStars": 81, "summary_zh": "<ul>\n    <li>\u4eba\u7c7b\u5728\u83b7\u53d6\u8bed\u8a00\u4e4b\u524d\uff0c\u5c31\u5df2\u7ecf\u53d1\u5c55\u4e86\u57fa\u672c\u7684\u89c6\u89c9\u6280\u80fd\uff0c\u4f46\u73b0\u4ee3\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4ecd\u7136\u4f9d\u8d56\u8bed\u8a00\u77e5\u8bc6\u6765\u5f25\u8865\u5176\u8106\u5f31\u7684\u89c6\u89c9\u7406\u89e3\u3002</li>\n    <li>\u7814\u7a76\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684MLLMs\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u8fd9\u4e9b\u4efb\u52a1\u4eba\u7c7b\uff08\u751a\u81f33\u5c81\u513f\u7ae5\uff09\u53ef\u4ee5\u8f7b\u677e\u5b8c\u6210\u3002</li>\n    <li>\u4e3a\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u8fd9\u4e00\u5dee\u8ddd\uff0c\u63d0\u51fa\u4e86BabyVision\u57fa\u51c6\uff0c\u65e8\u5728\u8bc4\u4f30MLLMs\u7684\u6838\u5fc3\u89c6\u89c9\u80fd\u529b\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u8bed\u8a00\u77e5\u8bc6\u3002</li>\n    <li>BabyVision\u6db5\u76d6388\u4e2a\u4efb\u52a1\uff0c\u5206\u4e3a22\u4e2a\u5b50\u7c7b\u548c\u56db\u4e2a\u5173\u952e\u7c7b\u522b\uff0c\u7ed3\u679c\u663e\u793a\u9886\u5148\u7684MLLMs\u8868\u73b0\u660e\u663e\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c6\u3002</li>\n    <li>\u5c3d\u7ba1\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5f53\u524d\u7684MLLMs\u4ecd\u7f3a\u4e4f\u57fa\u672c\u7684\u89c6\u89c9\u80fd\u529b\uff0c\u8fdb\u5c55BabyVision\u662f\u671d\u7740\u4eba\u7c7b\u6c34\u5e73\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\u8fc8\u8fdb\u7684\u4e00\u6b65\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Humans develop visual skills before learning language, but modern Multimodal LLMs (MLLMs) rely too much on language to understand visuals.</li>\n    <li>We found that advanced MLLMs struggle with basic visual tasks that even young children can do easily.</li>\n    <li>To examine this issue, we created BabyVision, a test with 388 visual tasks that do not require language skills.</li>\n    <li>Results show that top MLLMs, like Gemini3-Pro-Preview, score much lower than children and adults on these visual tasks.</li>\n    <li>Our work on BabyVision aims to help MLLMs improve their visual understanding and reasoning abilities.</li>\n</ul>"}, "publishedAt": "2026-01-10T05:42:44.000Z", "title": "BabyVision: Visual Reasoning Beyond Language", "summary": "While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.06521.png", "numComments": 3, "submittedBy": {"_id": "6039478ab3ecf716b1a5fd4d", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg", "fullname": "taesiri", "name": "taesiri", "type": "user", "isPro": true, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 207, "isUserFollowing": false}, "isAuthorParticipating": false}, {"paper": {"id": "2601.10477", "authors": [{"_id": "69699e5e32f0333869ff9378", "name": "Yu Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff9379", "name": "Yi Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937a", "user": {"_id": "661de9defdbc9c247f159d15", "avatarUrl": "/avatars/38e21e78327cc908201122405c48f41b.svg", "isPro": false, "fullname": "Rui Dai", "user": "DerryD", "type": "user"}, "name": "Rui Dai", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T14:43:46.050Z", "hidden": false}, {"_id": "69699e5e32f0333869ff937b", "name": "Yujie Wang", "hidden": false}, {"_id": "69699e5e32f0333869ff937c", "name": "Kaikui Liu", "hidden": false}, {"_id": "69699e5e32f0333869ff937d", "name": "Xiangxiang Chu", "hidden": false}, {"_id": "69699e5e32f0333869ff937e", "user": {"_id": "63ec91dec8827dd0f0f3b489", "avatarUrl": "/avatars/3d0d9479a26673f859c226efaf1e4a43.svg", "isPro": false, "fullname": "shengli", "user": "yanshengli", "type": "user"}, "name": "Yansheng Li", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:19.008Z", "hidden": false}], "publishedAt": "2026-01-15T15:00:36.000Z", "submittedOnDailyAt": "2026-01-16T03:49:39.109Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "upvotes": 138, "discussionId": "69699e5f32f0333869ff937f", "githubRepo": "https://github.com/AMAP-ML/SocioReasoner", "githubRepoAddedBy": "user", "ai_summary": "Urban socio-semantic segmentation is achieved through a vision-language model framework that combines cross-modal recognition and multi-stage reasoning with reinforcement learning optimization.", "ai_keywords": ["vision-language model", "cross-modal recognition", "multi-stage reasoning", "reinforcement learning", "socio-semantic segmentation", "Urban Socio-Semantic Segmentation dataset", "SocioReasoner"], "githubStars": 125, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u57ce\u5e02\u8868\u9762\u5305\u542b\u8bb8\u591a\u8bed\u4e49\u5b9e\u4f53\uff0c\u9700\u8981\u4ece\u536b\u661f\u56fe\u50cf\u4e2d\u8fdb\u884c\u5206\u5272\u3002</li>\n    <li>\u5f53\u524d\u7684\u5206\u5272\u6a21\u578b\u5bf9\u7269\u7406\u5c5e\u6027\u7684\u5b9e\u4f53\uff08\u5982\u5efa\u7b51\u7269\u548c\u6c34\u4f53\uff09\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u793e\u4f1a\u5b9a\u4e49\u7684\u7c7b\u522b\uff08\u5982\u5b66\u6821\u548c\u516c\u56ed\uff09\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6SocioSeg\uff0c\u5305\u542b\u536b\u661f\u56fe\u50cf\u3001\u6570\u5b57\u5730\u56fe\u548c\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u50cf\u7d20\u7ea7\u6807\u7b7e\u3002</li>\n    <li>\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684\u89c6\u89c9-\u8bed\u8a00\u63a8\u7406\u6846\u67b6SocioReasoner\uff0c\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u548c\u6807\u6ce8\u793e\u4f1a\u8bed\u4e49\u5b9e\u4f53\u7684\u8fc7\u7a0b\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5e76\u4e14\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Urban areas have many different types of features, and separating them in satellite images is important for various applications.</li>\n    <li>Current models can identify physical features like buildings and water but struggle with social features like schools and parks.</li>\n    <li>We introduce a new dataset called SocioSeg, which includes satellite images and detailed labels for social features.</li>\n    <li>Our new framework, SocioReasoner, helps recognize and label social features using a method similar to human reasoning.</li>\n    <li>Our experiments show that our approach outperforms existing models and can generalize well to new situations.</li>\n</ul>"}, "publishedAt": "2026-01-15T10:00:36.000Z", "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "summary": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.10477.png", "numComments": 2, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": false}, {"paper": {"id": "2601.09668", "authors": [{"_id": "6968bc424dcc6d53da2701df", "name": "Ailin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e0", "name": "Chengyuan Yao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e1", "name": "Chunrui Han", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e2", "user": {"_id": "62ecbffd99112e99c5f7fded", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62ecbffd99112e99c5f7fded/U6iXAJbpm2vaC5qksEPiH.png", "isPro": false, "fullname": "Fanqi Wan", "user": "Wanfq", "type": "user"}, "name": "Fanqi Wan", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:02.442Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e3", "name": "Hangyu Guo", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e4", "user": {"_id": "68c0dd3b8998cbe8217171a5", "avatarUrl": "/avatars/554301bdaa61f190693482f28500f7ae.svg", "isPro": false, "fullname": "\u5415\u6d69\u7136", "user": "HaoRanLv", "type": "user"}, "name": "Haoran Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:19.559Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e5", "name": "Hongyu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e6", "name": "Jia Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e7", "name": "Jian Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e8", "name": "Jianjian Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da2701e9", "user": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "name": "Jingcheng Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:19.060Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ea", "user": {"_id": "658a810665df457a55ffcd04", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a810665df457a55ffcd04/6Pe0mNao4mlWLIjYEoWv5.jpeg", "isPro": false, "fullname": "Linkangheng", "user": "Kangheng", "type": "user"}, "name": "Kangheng Lin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:41.402Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701eb", "name": "Liang Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ec", "name": "Mitt Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ed", "name": "Song Yuan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ee", "name": "Wenwen Qu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ef", "name": "Xiangfeng Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f0", "user": {"_id": "6845364527e777c8bc42e444", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mBRiFQzPPXwg2aECVkSdz.png", "isPro": false, "fullname": "yanlin lai", "user": "lyn22333", "type": "user"}, "name": "Yanlin Lai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:26.009Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f1", "user": {"_id": "639c0eb734967bcf4565cf29", "avatarUrl": "/avatars/f4788bb89b788b40ead4e1f3314044f7.svg", "isPro": false, "fullname": "Yingxiu Zhao", "user": "Yingxiu", "type": "user"}, "name": "Yingxiu Zhao", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:54.082Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f2", "user": {"_id": "664ae39ab5e5f95dc6209365", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/664ae39ab5e5f95dc6209365/8Z9ERYhX6URXh4si6jWGm.jpeg", "isPro": false, "fullname": "Yinmin Zhang", "user": "YinminZhang", "type": "user"}, "name": "Yinmin Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:29:48.054Z", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f3", "name": "Yukang Shi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f4", "name": "Yuyang Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f5", "name": "Zejia Weng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f6", "name": "Ziyang Meng", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f7", "name": "Ang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f8", "name": "Aobo Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701f9", "name": "Bo Dong", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fa", "name": "Changyi Wan", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fb", "name": "David Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fc", "name": "Di Qi", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fd", "name": "Dingming Li", "hidden": false}, {"_id": "6968bc424dcc6d53da2701fe", "name": "En Yu", "hidden": false}, {"_id": "6968bc424dcc6d53da2701ff", "name": "Guopeng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270200", "name": "Haiquan Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da270201", "name": "Han Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270202", "name": "Hanshan Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270203", "name": "Haolong Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270204", "name": "Hebin Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da270205", "user": {"_id": "68106c88b924dd6c328889c2", "avatarUrl": "/avatars/8accf835b711bffa2ea307158950ab33.svg", "isPro": false, "fullname": "Hongbo Peng", "user": "M1chaelPeng", "type": "user"}, "name": "Hongbo Peng", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:32:21.188Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270206", "name": "Jiaran Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270207", "user": {"_id": "673e9988fc3c3c898a57949b", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/gsQlZCq1I2FrqqmMPgxoh.jpeg", "isPro": false, "fullname": "Jiashu Lv", "user": "Jserw", "type": "user"}, "name": "Jiashu Lv", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:23.399Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270208", "name": "Jiayi Fu", "hidden": false}, {"_id": "6968bc424dcc6d53da270209", "name": "Jie Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da27020a", "name": "Jie Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27020b", "name": "Jisheng Yin", "hidden": false}, {"_id": "6968bc424dcc6d53da27020c", "user": {"_id": "6502f241b1792803da7e8def", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6502f241b1792803da7e8def/mJ1XCVKivsMLi2Lo1kGKX.png", "isPro": false, "fullname": "JingJing Xie", "user": "ownerEli", "type": "user"}, "name": "Jingjing Xie", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:30:31.565Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27020d", "name": "Jingwei Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da27020e", "name": "Jun Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27020f", "name": "Junfeng Liu", "hidden": false}, {"_id": "6968bc424dcc6d53da270210", "name": "Kaijun Tan", "hidden": false}, {"_id": "6968bc424dcc6d53da270211", "name": "Kaiwen Yan", "hidden": false}, {"_id": "6968bc424dcc6d53da270212", "name": "Liangyu Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270213", "name": "Lina Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270214", "name": "Mingliang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270215", "name": "Qian Zhao", "hidden": false}, {"_id": "6968bc424dcc6d53da270216", "name": "Quan Sun", "hidden": false}, {"_id": "6968bc424dcc6d53da270217", "name": "Shaoliang Pang", "hidden": false}, {"_id": "6968bc424dcc6d53da270218", "name": "Shengjie Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270219", "name": "Shijie Shang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021a", "user": {"_id": "682703cde798014f05e8d224", "avatarUrl": "/avatars/167ba232ad427e995aa9629202c670d0.svg", "isPro": false, "fullname": "SiyuanZhang", "user": "SiyuanZhang", "type": "user"}, "name": "Siyuan Zhang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:04.562Z", "hidden": false}, {"_id": "6968bc424dcc6d53da27021b", "name": "Tianhao You", "hidden": false}, {"_id": "6968bc424dcc6d53da27021c", "name": "Wei Ji", "hidden": false}, {"_id": "6968bc424dcc6d53da27021d", "name": "Wuxun Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da27021e", "name": "Xiaobo Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da27021f", "name": "Xiaojie Hou", "hidden": false}, {"_id": "6968bc424dcc6d53da270220", "name": "Xiaoran Jiao", "hidden": false}, {"_id": "6968bc424dcc6d53da270221", "name": "Xiaoxiao Ren", "hidden": false}, {"_id": "6968bc424dcc6d53da270222", "name": "Xiangwen Kong", "hidden": false}, {"_id": "6968bc424dcc6d53da270223", "name": "Xin Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270224", "name": "Xin Wu", "hidden": false}, {"_id": "6968bc424dcc6d53da270225", "name": "Xing Chen", "hidden": false}, {"_id": "6968bc424dcc6d53da270226", "name": "Xinran Wang", "hidden": false}, {"_id": "6968bc424dcc6d53da270227", "name": "Xuelin Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270228", "user": {"_id": "64ae4d62179421d320b67c26", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae4d62179421d320b67c26/nz-tY6hX7mcDzhdtBmG8K.jpeg", "isPro": false, "fullname": "Yana Wei", "user": "llwswyn", "type": "user"}, "name": "Yana Wei", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:31:44.883Z", "hidden": false}, {"_id": "6968bc424dcc6d53da270229", "name": "Yang Li", "hidden": false}, {"_id": "6968bc424dcc6d53da27022a", "name": "Yanming Xu", "hidden": false}, {"_id": "6968bc424dcc6d53da27022b", "name": "Yeqing Shen", "hidden": false}, {"_id": "6968bc424dcc6d53da27022c", "name": "Yuang Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022d", "name": "Yue Peng", "hidden": false}, {"_id": "6968bc424dcc6d53da27022e", "name": "Yu Zhou", "hidden": false}, {"_id": "6968bc424dcc6d53da27022f", "name": "Yusheng Li", "hidden": false}, {"_id": "6968bc424dcc6d53da270230", "name": "Yuxiang Yang", "hidden": false}, {"_id": "6968bc424dcc6d53da270231", "name": "Yuyang Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da270232", "name": "Zhe Xie", "hidden": false}, {"_id": "6968bc424dcc6d53da270233", "name": "Zhewei Huang", "hidden": false}, {"_id": "6968bc424dcc6d53da270234", "name": "Zhenyi Lu", "hidden": false}, {"_id": "6968bc424dcc6d53da270235", "name": "Zhimin Fan", "hidden": false}, {"_id": "6968bc424dcc6d53da270236", "name": "Zihui Cheng", "hidden": false}, {"_id": "6968bc424dcc6d53da270237", "name": "Daxin Jiang", "hidden": false}, {"_id": "6968bc424dcc6d53da270238", "name": "Qi Han", "hidden": false}, {"_id": "6968bc424dcc6d53da270239", "name": "Xiangyu Zhang", "hidden": false}, {"_id": "6968bc424dcc6d53da27023a", "name": "Yibo Zhu", "hidden": false}, {"_id": "6968bc424dcc6d53da27023b", "name": "Zheng Ge", "hidden": false}], "publishedAt": "2026-01-14T17:58:24.000Z", "submittedOnDailyAt": "2026-01-16T01:39:25.029Z", "title": "STEP3-VL-10B Technical Report", "submittedOnDailyBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "isPro": false, "fullname": "Jingcheng Hu", "user": "reign12", "type": "user"}, "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "upvotes": 129, "discussionId": "6968bc434dcc6d53da27023c", "projectPage": "https://stepfun-ai.github.io/Step3-VL-10B", "githubRepo": "https://github.com/stepfun-ai/Step3-VL-10B", "githubRepoAddedBy": "auto", "ai_summary": "STEP3-VL-10B achieves superior multimodal performance through unified pre-training with a language-aligned Perception Encoder and Qwen3-8B decoder, combined with scaled post-training and Parallel Coordinated Reasoning for efficient large-scale visual reasoning.", "ai_keywords": ["multimodal tokens", "Perception Encoder", "Qwen3-8B decoder", "vision-language synergy", "reinforcement learning", "Parallel Coordinated Reasoning", "test-time compute", "visual hypotheses", "MMBench", "MMMU", "AIME2025", "MathVision"], "githubStars": 152, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "summary_zh": "<ul>\n    <li>STEP3-VL-10B\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u63d0\u9ad8\u591a\u6a21\u6001\u667a\u80fd\u7684\u6548\u7387\u3002</li>\n    <li>\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7ba1\u9053\u5b9e\u73b0\uff0c\u4f7f\u7528\u4e861.2\u4e07\u4ebf\u4e2a\u591a\u6a21\u6001\u6807\u8bb0\u3002</li>\n    <li>\u5b83\u5177\u6709\u5e76\u884c\u534f\u8c03\u63a8\u7406\uff08PaCoRe\uff09\u529f\u80fd\uff0c\u4ee5\u63d0\u9ad8\u6d4b\u8bd5\u65f6\u7684\u8ba1\u7b97\u6548\u7387\u3002</li>\n    <li>\u5c3d\u7ba1\u6a21\u578b\u4f53\u79ef\u5c0f\uff08\u4ec510\u4ebf\u53c2\u6570\uff09\uff0c\u4f46\u6027\u80fd\u4e0e\u66f4\u5927\u7684\u6a21\u578b\u76f8\u5f53\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u5b83\u4eec\u3002</li>\n    <li>\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u3001\u9ad8\u6548\u4e14\u53ef\u91cd\u73b0\u7684\u57fa\u7ebf\u4f9b\u793e\u533a\u4f7f\u7528\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>STEP3-VL-10B is a new open-source model that balances efficiency and advanced intelligence across different types of data.</li>\n    <li>It uses a unique training method with 1.2 trillion data points, combining language understanding and visual processing.</li>\n    <li>The model includes advanced reasoning capabilities that allow it to solve complex problems effectively.</li>\n    <li>Despite being smaller than many other models, STEP3-VL-10B performs as well as or better than much larger models.</li>\n    <li>The creators are sharing the model with the community to help others build on its capabilities.</li>\n</ul>"}, "publishedAt": "2026-01-14T12:58:24.000Z", "title": "STEP3-VL-10B Technical Report", "summary": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10times-20times larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.09668.png", "numComments": 4, "submittedBy": {"_id": "625026b7d2d191ac43320c5e", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/625026b7d2d191ac43320c5e/2ExzHlZ-Bk8SQMyBjeY6N.jpeg", "fullname": "Jingcheng Hu", "name": "reign12", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 20, "isUserFollowing": false}, "organization": {"_id": "66e43eae9d477f566f937935", "name": "stepfun-ai", "fullname": "StepFun", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/66935cee39002fc0569c2943/Qv8QPbkgoKE3wR4jTzHiy.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05432", "authors": [{"_id": "69646268138cc47cbd76527e", "user": {"_id": "666a83e9b2d8397c1e545785", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/666a83e9b2d8397c1e545785/7PxrVl38zWUbjAsZThHHb.jpeg", "isPro": false, "fullname": "Yuxiang Ji", "user": "Yux1ang", "type": "user"}, "name": "Yuxiang Ji", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:41.283Z", "hidden": false}, {"_id": "69646268138cc47cbd76527f", "name": "Yong Wang", "hidden": false}, {"_id": "69646268138cc47cbd765280", "name": "Ziyu Ma", "hidden": false}, {"_id": "69646268138cc47cbd765281", "name": "Yiming Hu", "hidden": false}, {"_id": "69646268138cc47cbd765282", "user": {"_id": "65003db8bef9b594656f8fa7", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65003db8bef9b594656f8fa7/L6cvPOAeBRnFnIQwWxYyf.png", "isPro": false, "fullname": "Hailang Huang", "user": "lerogo", "type": "user"}, "name": "Hailang Huang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-12T10:34:39.368Z", "hidden": false}, {"_id": "69646268138cc47cbd765283", "name": "Xuecai Hu", "hidden": false}, {"_id": "69646268138cc47cbd765284", "name": "Guanhua Chen", "hidden": false}, {"_id": "69646268138cc47cbd765285", "name": "Liaoni Wu", "hidden": false}, {"_id": "69646268138cc47cbd765286", "name": "Xiangxiang Chu", "hidden": false}], "publishedAt": "2026-01-08T23:47:30.000Z", "submittedOnDailyAt": "2026-01-12T01:15:15.959Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "submittedOnDailyBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "isPro": false, "fullname": "xiaochonglinghu", "user": "xiaochonglinghu", "type": "user"}, "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "upvotes": 129, "discussionId": "69646268138cc47cbd765287", "projectPage": "https://amap-ml.github.io/Thinking-with-Map/", "githubRepo": "https://github.com/AMAP-ML/Thinking-with-Map", "githubRepoAddedBy": "user", "ai_summary": "Large vision-language models are enhanced for image geolocalization by incorporating map-based reasoning and agent-in-the-map loop optimization, achieving superior accuracy compared to existing models.", "ai_keywords": ["vision-language model", "geolocalization", "chain-of-thought reasoning", "agentic capabilities", "agentic reinforcement learning", "parallel test-time scaling", "agent-in-the-map loop", "MAPBench", "Acc@500m"], "githubStars": 107, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "summary_zh": "<ul>\n    <li>\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u65e8\u5728\u6839\u636e\u89c6\u89c9\u7ebf\u7d22\u9884\u6d4b\u56fe\u50cf\u62cd\u6444\u5730\u70b9\u3002</li>\n    <li>\u73b0\u6709\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u5ffd\u7565\u4eba\u7c7b\u5e38\u7528\u7684\u5730\u56fe\u7b56\u7565\u3002</li>\n    <li>\u672c\u6587\u63d0\u51fa\u4e86\u201c\u601d\u7ef4\u4e0e\u5730\u56fe\u201d\u80fd\u529b\uff0c\u5e76\u91c7\u7528\u4e86\u57fa\u4e8e\u5730\u56fe\u7684\u5faa\u73af\u4ee3\u7406\u65b9\u6cd5\u3002</li>\n    <li>\u5f00\u53d1\u4e86\u4e24\u9636\u6bb5\u4f18\u5316\u65b9\u6848\uff0c\u5305\u62ec\u5f3a\u5316\u5b66\u4e60\u548c\u5e76\u884c\u6d4b\u8bd5\u65f6\u95f4\u7f29\u653e\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u5927\u591a\u6570\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0cAcc@500m \u4ece 8.0% \u63d0\u9ad8\u5230 22.1%\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>The task is to predict where a photo was taken on Earth using visual clues.</li>\n    <li>Current models use advanced reasoning but often ignore how humans use maps.</li>\n    <li>This research introduces a new method that incorporates map usage into the prediction process.</li>\n    <li>The method uses two stages: reinforcement learning to improve efficiency and a technique to explore multiple options before deciding.</li>\n    <li>Tests show this new approach outperforms existing models, significantly improving accuracy in predictions.</li>\n</ul>"}, "publishedAt": "2026-01-08T18:47:30.000Z", "title": "Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization", "summary": "The image geolocalization task aims to predict the location where an image was taken anywhere on Earth using visual clues. Existing large vision-language model (LVLM) approaches leverage world knowledge, chain-of-thought reasoning, and agentic capabilities, but overlook a common strategy used by humans -- using maps. In this work, we first equip the model Thinking with Map ability and formulate it as an agent-in-the-map loop. We develop a two-stage optimization scheme for it, including agentic reinforcement learning (RL) followed by parallel test-time scaling (TTS). The RL strengthens the agentic capability of model to improve sampling efficiency, and the parallel TTS enables the model to explore multiple candidate paths before making the final prediction, which is crucial for geolocalization. To evaluate our method on up-to-date and in-the-wild images, we further present MAPBench, a comprehensive geolocalization training and evaluation benchmark composed entirely of real-world images. Experimental results show that our method outperforms existing open- and closed-source models on most metrics, specifically improving Acc@500m from 8.0\\% to 22.1\\% compared to Gemini-3-Pro with Google Search/Map grounded mode.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05432.png", "numComments": 3, "submittedBy": {"_id": "66d255e3947594430c723ff6", "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg", "fullname": "xiaochonglinghu", "name": "xiaochonglinghu", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 6, "isUserFollowing": false}, "organization": {"_id": "64488b334988ee01f2a8d856", "name": "alibaba-inc", "fullname": "alibaba-inc", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/61ac8f8a00d01045fca0ad2f/MX4wxQVaFm1A1wqnrL2WU.jpeg"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.08763", "authors": [{"_id": "6969b0a232f0333869ff946a", "user": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "name": "Zhiyuan Hu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:38.232Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946b", "user": {"_id": "6891c906f3c31445cc040ab1", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6891c906f3c31445cc040ab1/NBqxXOY7al4CD0XBj8ke2.jpeg", "isPro": false, "fullname": "Yucheng Wang", "user": "DevilEnfant", "type": "user"}, "name": "Yucheng Wang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:48.080Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946c", "name": "Yufei He", "hidden": false}, {"_id": "6969b0a232f0333869ff946d", "user": {"_id": "682deb444988bd82847e2b03", "avatarUrl": "/avatars/15da087e84386ea72c6fa2db63571420.svg", "isPro": false, "fullname": "Jia-Ying Wu", "user": "EricaWu", "type": "user"}, "name": "Jiaying Wu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:32:59.692Z", "hidden": false}, {"_id": "6969b0a232f0333869ff946e", "name": "Yilun Zhao", "hidden": false}, {"_id": "6969b0a232f0333869ff946f", "name": "See-Kiong Ng", "hidden": false}, {"_id": "6969b0a232f0333869ff9470", "user": {"_id": "672793ffa5255a517fd02045", "avatarUrl": "/avatars/a2569be6f2e952b5b00e5d4b89a7cede.svg", "isPro": false, "fullname": "Cynthia Breazeal", "user": "cynthiabreazeal", "type": "user"}, "name": "Cynthia Breazeal", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:06.327Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9471", "user": {"_id": "655722e80438e0854fae7554", "avatarUrl": "/avatars/b93a74f7c7880f9fe0f3ffb47e2aef5e.svg", "isPro": false, "fullname": "Luu Anh Tuan", "user": "anhtuanluu36", "type": "user"}, "name": "Anh Tuan Luu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:12.181Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9472", "user": {"_id": "682352cdb1c5350f850dd952", "avatarUrl": "/avatars/5426efe0195ac8f914839e6585b1a112.svg", "isPro": false, "fullname": "Hae Won Park", "user": "robohaewon", "type": "user"}, "name": "Hae Won Park", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:17.979Z", "hidden": false}, {"_id": "6969b0a232f0333869ff9473", "user": {"_id": "651d8032c50012d33e914f2f", "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg", "isPro": false, "fullname": "Bryan Hooi", "user": "bhooi", "type": "user"}, "name": "Bryan Hooi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-16T15:33:23.007Z", "hidden": false}], "publishedAt": "2026-01-13T17:48:43.000Z", "submittedOnDailyAt": "2026-01-16T01:00:36.686Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "submittedOnDailyBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "isPro": false, "fullname": "Zhiyuan Hu", "user": "zhiyuanhucs", "type": "user"}, "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "upvotes": 111, "discussionId": "6969b0a232f0333869ff9474", "ai_summary": "Reinforcement learning for large language models is enhanced by a rollout-level objective that rewards rare high-level reasoning strategies, improving diverse solution discovery without sacrificing initial performance.", "ai_keywords": ["reinforcement learning", "large language models", "exploration collapse", "pass@k", "pass@1", "rollout-level objective", "high-level solution strategies", "clustering", "policy advantages", "AUC@K"], "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u540e\u671f\u8bad\u7ec3\u4e2d\u53d8\u5f97\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5e38\u5e38\u51fa\u73b0\u63a2\u7d22\u5d29\u6e83\u7684\u95ee\u9898\u3002</li>\n    <li>\u8fd9\u79cd\u95ee\u9898\u7684\u539f\u56e0\u5728\u4e8e\u8fc7\u4e8e\u5173\u6ce8\u5c40\u90e8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u800c\u4e0d\u662f\u89e3\u51b3\u65b9\u6848\u7684\u591a\u6837\u6027\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u79f0\u4e3a\u72ec\u7279\u6027\u610f\u8bc6\u5f3a\u5316\u5b66\u4e60\uff0c\u5956\u52b1\u5c55\u793a\u7a00\u6709\u9ad8\u7ea7\u7b56\u7565\u7684\u6b63\u786e\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u8be5\u65b9\u6cd5\u901a\u8fc7\u805a\u7c7b\u76f8\u540c\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6839\u636e\u9ad8\u7ea7\u7b56\u7565\u8fdb\u884c\u5206\u7c7b\uff0c\u5ffd\u7565\u8868\u9762\u5dee\u5f02\u3002</li>\n    <li>\u5728\u6570\u5b66\u3001\u7269\u7406\u548c\u533b\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u591a\u6837\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6807\u51c6\u7684\u51c6\u786e\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement learning (RL) helps improve large language models (LLMs) for complex reasoning tasks but often leads to a lack of exploration in strategies.</li>\n    <li>This issue arises because RL focuses too much on common solutions instead of encouraging diverse approaches.</li>\n    <li>The proposed solution, called Uniqueness-Aware Reinforcement Learning, rewards rare and effective strategies rather than just frequent ones.</li>\n    <li>It uses an LLM-based judge to group solutions by their overall strategy, which helps identify and reward unique approaches.</li>\n    <li>This method improves performance on mathematics, physics, and medical reasoning tasks, enhancing the variety of solutions while maintaining accuracy.</li>\n</ul>"}, "publishedAt": "2026-01-13T12:48:43.000Z", "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs", "summary": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@k across large sampling budgets and increases the area under the pass@k curve (AUC@K) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08763.png", "numComments": 3, "submittedBy": {"_id": "64351475901c5734bcb64248", "avatarUrl": "/avatars/12346d4301c1bfb00ce0ea128a93cc15.svg", "fullname": "Zhiyuan Hu", "name": "zhiyuanhucs", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 5, "isUserFollowing": false}, "organization": {"_id": "63728bde14d543d507ae970d", "name": "MIT", "fullname": "Massachusetts Institute of Technology", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/noauth/S90qoeEJeEYaYf-c7Zs8g.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2512.23959", "authors": [{"_id": "69575365832867f2535258c9", "user": {"_id": "674ac97729a3bb873fc995c6", "avatarUrl": "/avatars/cd5dc0bb367b552eeaefee4343adb89b.svg", "isPro": false, "fullname": "Zhou Chulun", "user": "Chow1997-CUHK", "type": "user"}, "name": "Chulun Zhou", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:44.435Z", "hidden": false}, {"_id": "69575365832867f2535258ca", "user": {"_id": "647738744aad13a4ea40ea25", "avatarUrl": "/avatars/1b12dc3698982c5328d5dc69438a5d18.svg", "isPro": false, "fullname": "chunkang zhang", "user": "eziosauditore", "type": "user"}, "name": "Chunkang Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-04T20:09:44.016Z", "hidden": false}, {"_id": "69575365832867f2535258cb", "name": "Guoxin Yu", "hidden": false}, {"_id": "69575365832867f2535258cc", "name": "Fandong Meng", "hidden": false}, {"_id": "69575365832867f2535258cd", "name": "Jie Zhou", "hidden": false}, {"_id": "69575365832867f2535258ce", "name": "Wai Lam", "hidden": false}, {"_id": "69575365832867f2535258cf", "user": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "name": "Mo Yu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-02T15:37:42.305Z", "hidden": false}], "publishedAt": "2025-12-30T03:13:10.000Z", "submittedOnDailyAt": "2026-01-02T11:19:59.871Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "submittedOnDailyBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "isPro": false, "fullname": "Mo", "user": "BishopGorov", "type": "user"}, "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "upvotes": 105, "discussionId": "69575365832867f2535258d0", "githubRepo": "https://github.com/Encyclomen/HGMem", "githubRepoAddedBy": "user", "githubStars": 97, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "summary_zh": "<ul>\n    <li>\u591a\u6b65\u9aa4\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5df2\u6210\u4e3a\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u7684\u6d41\u884c\u7b56\u7565\u3002</li>\n    <li>\u73b0\u6709\u7684\u8bb0\u5fc6\u6a21\u5757\u4e3b\u8981\u4f5c\u4e3a\u88ab\u52a8\u5b58\u50a8\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u4fe1\u606f\u4e4b\u95f4\u7684\u9ad8\u9636\u5173\u8054\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51faHGMem\uff0c\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u5c06\u8bb0\u5fc6\u4ece\u7b80\u5355\u5b58\u50a8\u6269\u5c55\u4e3a\u52a8\u6001\u7684\u3001\u5bcc\u6709\u8868\u73b0\u529b\u7684\u7ed3\u6784\u3002</li>\n    <li>HGMem\u901a\u8fc7\u8d85\u56fe\u8fde\u63a5\u4e8b\u5b9e\u548c\u601d\u7ef4\uff0c\u5f62\u6210\u66f4\u5f3a\u7684\u77e5\u8bc6\u7ed3\u6784\uff0c\u4fc3\u8fdb\u6df1\u5165\u63a8\u7406\u3002</li>\n    <li>\u5728\u591a\u4e2a\u5168\u7403\u7406\u89e3\u76f8\u5173\u7684\u6570\u636e\u96c6\u4e0a\uff0cHGMem\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6b65\u9aa4RAG\u7684\u8868\u73b0\uff0c\u8d85\u8d8a\u4e86\u8bb8\u591a\u5f3a\u57fa\u7ebf\u7cfb\u7edf\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Multi-step retrieval-augmented generation (RAG) improves large language models for complex reasoning tasks.</li>\n    <li>Current memory designs in RAG systems only store isolated facts, which limits their effectiveness in reasoning.</li>\n    <li>HGMem introduces a new memory mechanism using hypergraphs to create dynamic connections between facts.</li>\n    <li>This new structure allows for better understanding and reasoning by forming higher-order interactions in memory.</li>\n    <li>Tests show that HGMem significantly enhances RAG performance on challenging datasets compared to existing systems.</li>\n</ul>"}, "publishedAt": "2025-12-29T22:13:10.000Z", "title": "Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling", "summary": "Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2512.23959.png", "numComments": 3, "submittedBy": {"_id": "67af92045a86287292026808", "avatarUrl": "/avatars/8bad9272fe73ba04e077b5484837c8d3.svg", "fullname": "Mo", "name": "BishopGorov", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 9, "isUserFollowing": false}, "organization": {"_id": "66543b6e420092799d2f625c", "name": "tencent", "fullname": "Tencent", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/5dd96eb166059660ed1ee413/Lp3m-XLpjQGwBItlvn69q.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.05242", "authors": [{"_id": "69607a225b7998385e63952a", "user": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "name": "Shih-Yang Liu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:01.190Z", "hidden": false}, {"_id": "69607a225b7998385e63952b", "name": "Xin Dong", "hidden": false}, {"_id": "69607a225b7998385e63952c", "user": {"_id": "640928bd3461c51cf7378707", "avatarUrl": "/avatars/b29fcb7388b81f8686086352f6321d06.svg", "isPro": false, "fullname": "Ximing Lu", "user": "Ximing", "type": "user"}, "name": "Ximing Lu", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T08:49:57.401Z", "hidden": false}, {"_id": "69607a225b7998385e63952d", "name": "Shizhe Diao", "hidden": false}, {"_id": "69607a225b7998385e63952e", "user": {"_id": "63e8cccddd2c4effdd6283cf", "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg", "isPro": false, "fullname": "Peter Belcak", "user": "pbelcak", "type": "user"}, "name": "Peter Belcak", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:49:07.360Z", "hidden": false}, {"_id": "69607a225b7998385e63952f", "name": "Mingjie Liu", "hidden": false}, {"_id": "69607a225b7998385e639530", "user": {"_id": "64ae22dd1aee69ece065cdcd", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png", "isPro": false, "fullname": "Min-Hung Chen", "user": "cmhungsteve", "type": "user"}, "name": "Min-Hung Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-09T08:35:03.130Z", "hidden": false}, {"_id": "69607a225b7998385e639531", "user": {"_id": "65a8b7f69aec1645994e7a15", "avatarUrl": "/avatars/debc086f3fea029db22847bde80799a0.svg", "isPro": false, "fullname": "Hongxu Yin", "user": "yinhongxu", "type": "user"}, "name": "Hongxu Yin", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:57.052Z", "hidden": false}, {"_id": "69607a225b7998385e639532", "name": "Yu-Chiang Frank Wang", "hidden": false}, {"_id": "69607a225b7998385e639533", "name": "Kwang-Ting Cheng", "hidden": false}, {"_id": "69607a225b7998385e639534", "user": {"_id": "64d42729f63b01b7f676b176", "avatarUrl": "/avatars/52e54bdd6a1fb6c774a40cd70f3d7925.svg", "isPro": false, "fullname": "Yejin Choi", "user": "yejinchoinka", "type": "user"}, "name": "Yejin Choi", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:43.597Z", "hidden": false}, {"_id": "69607a225b7998385e639535", "name": "Jan Kautz", "hidden": false}, {"_id": "69607a225b7998385e639536", "user": {"_id": "646d0c1c534e52f8c30500a6", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646d0c1c534e52f8c30500a6/75VH8ClbRaP75BU2ONfXE.png", "isPro": true, "fullname": "Pavlo Molchanov", "user": "pmolchanov", "type": "user"}, "name": "Pavlo Molchanov", "status": "admin_assigned", "statusLastChangedAt": "2026-01-09T15:48:21.861Z", "hidden": false}], "publishedAt": "2026-01-08T18:59:24.000Z", "submittedOnDailyAt": "2026-01-09T01:16:50.715Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "submittedOnDailyBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "isPro": false, "fullname": "LIU Shih-yang", "user": "sliuau", "type": "user"}, "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "upvotes": 96, "discussionId": "69607a225b7998385e639537", "projectPage": "https://nvlabs.github.io/GDPO/", "githubRepo": "https://github.com/NVlabs/GDPO", "githubRepoAddedBy": "user", "ai_summary": "Multi-reward reinforcement learning suffers from reward normalization collapse in GRPO, which GDPO addresses by decoupling reward normalization for improved training stability and performance across reasoning tasks.", "ai_keywords": ["Reinforcement learning", "Group Relative Policy Optimization", "multi-reward setting", "policy optimization", "Group reward-Decoupled Normalization Policy Optimization", "reward normalization", "advantage values", "training stability", "multi-reward reinforcement learning"], "githubStars": 64, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "summary_zh": "<ul>\n    <li>\u8bed\u8a00\u6a21\u578b\u7684\u671f\u671b\u4e0d\u4ec5\u662f\u63d0\u4f9b\u51c6\u786e\u7684\u56de\u7b54\uff0c\u8fd8\u8981\u7b26\u5408\u591a\u6837\u7684\u7528\u6237\u504f\u597d\u3002</li>\n    <li>\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u7814\u7a76\u8005\u4eec\u5f00\u59cb\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u4f7f\u7528\u591a\u4e2a\u5956\u52b1\u6765\u5f15\u5bfc\u6a21\u578b\u884c\u4e3a\u3002</li>\n    <li>\u76f4\u63a5\u5e94\u7528\u73b0\u6709\u7684\u591a\u5956\u52b1\u4f18\u5316\u65b9\u6cd5\uff08GRPO\uff09\u4f1a\u5bfc\u81f4\u5956\u52b1\u4fe1\u53f7\u5931\u53bb\u533a\u5206\u5ea6\uff0c\u5f71\u54cd\u8bad\u7ec3\u6548\u679c\u3002</li>\n    <li>\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u65b9\u6cd5\uff08GDPO\uff09\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u4fdd\u6301\u5956\u52b1\u4e4b\u95f4\u7684\u76f8\u5bf9\u5dee\u5f02\u3002</li>\n    <li>\u5728\u5de5\u5177\u8c03\u7528\u3001\u6570\u5b66\u63a8\u7406\u548c\u7f16\u7801\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\uff0cGDPO\u5728\u51c6\u786e\u6027\u548c\u9075\u5faa\u7ea6\u675f\u65b9\u9762\u5747\u4f18\u4e8eGRPO\uff0c\u663e\u793a\u51fa\u5176\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Users expect language models to give accurate answers and behave in ways that match different human preferences.</li>\n    <li>Reinforcement learning (RL) now uses multiple rewards to guide models toward these preferred behaviors.</li>\n    <li>The current method, Group Relative Policy Optimization (GRPO), has issues when applied to different reward combinations, leading to poor training outcomes.</li>\n    <li>We propose a new method called Group reward-Decoupled Normalization Policy Optimization (GDPO) that improves the training process by keeping rewards distinct.</li>\n    <li>GDPO outperforms GRPO in several tasks related to tool use, math, and coding, showing better accuracy and adherence to constraints.</li>\n</ul>"}, "publishedAt": "2026-01-08T13:59:24.000Z", "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization", "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.05242.png", "numComments": 5, "submittedBy": {"_id": "62b58c68a1bae3c711c41321", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b58c68a1bae3c711c41321/FGQ1ifsPpmRi8P0ElxV5J.png", "fullname": "LIU Shih-yang", "name": "sliuau", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "60262b67268c201cdc8b7d43", "name": "nvidia", "fullname": "NVIDIA", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/1613114437487-60262a8e0703121c822a80b6.png"}, "isAuthorParticipating": true}, {"paper": {"id": "2601.08521", "authors": [{"_id": "69674059c5e371f6b235d1d8", "user": {"_id": "68920f91bcf2b25e8e121cf6", "avatarUrl": "/avatars/4bc69f43828a346a3ee24b026e0edbb4.svg", "isPro": false, "fullname": "Fengkai Yang", "user": "ShortCatisLong", "type": "user"}, "name": "Fengkai Yang", "status": "admin_assigned", "statusLastChangedAt": "2026-01-19T14:33:21.899Z", "hidden": false}, {"_id": "69674059c5e371f6b235d1d9", "user": {"_id": "6969715fb2636f5f23a9a8c5", "avatarUrl": "/avatars/5e75043891ee41bb980f71fb9e3a33ab.svg", "isPro": false, "fullname": "Zherui Chen", "user": "chenzherui007", "type": "user"}, "name": "Zherui Chen", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:33:53.078Z", "hidden": false}, {"_id": "69674059c5e371f6b235d1da", "name": "Xiaohan Wang", "hidden": false}, {"_id": "69674059c5e371f6b235d1db", "name": "Xiaodong Lu", "hidden": false}, {"_id": "69674059c5e371f6b235d1dc", "user": {"_id": "666eb642a119281ee0bfa443", "avatarUrl": "/avatars/71317810b00978754ad439837b04faff.svg", "isPro": false, "fullname": "Jiajun Chai", "user": "PandaChai", "type": "user"}, "name": "Jiajun Chai", "status": "admin_assigned", "statusLastChangedAt": "2026-01-19T14:37:17.404Z", "hidden": false}, {"_id": "69674059c5e371f6b235d1dd", "name": "Guojun Yin", "hidden": false}, {"_id": "69674059c5e371f6b235d1de", "name": "Wei Lin", "hidden": false}, {"_id": "69674059c5e371f6b235d1df", "name": "Shuai Ma", "hidden": false}, {"_id": "69674059c5e371f6b235d1e0", "name": "Fuzhen Zhuang", "hidden": false}, {"_id": "69674059c5e371f6b235d1e1", "name": "Deqing Wang", "hidden": false}, {"_id": "69674059c5e371f6b235d1e2", "name": "Yaodong Yang", "hidden": false}, {"_id": "69674059c5e371f6b235d1e3", "name": "Jianxin Li", "hidden": false}, {"_id": "69674059c5e371f6b235d1e4", "user": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "isPro": false, "fullname": "Yikun Ban", "user": "Yikunb", "type": "user"}, "name": "Yikun Ban", "status": "claimed_verified", "statusLastChangedAt": "2026-01-16T10:33:50.655Z", "hidden": false}], "publishedAt": "2026-01-13T13:03:15.000Z", "submittedOnDailyAt": "2026-01-19T00:20:58.837Z", "title": "Your Group-Relative Advantage Is Biased", "submittedOnDailyBy": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "isPro": false, "fullname": "Yikun Ban", "user": "Yikunb", "type": "user"}, "summary": "Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.\n  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.", "upvotes": 95, "discussionId": "6967405ac5e371f6b235d1e5", "ai_summary": "Group-based reinforcement learning from verifier rewards suffers from biased advantage estimation that underestimates hard prompts and overestimates easy prompts, which is addressed through a history-aware adaptive difficulty weighting method that improves performance on mathematical reasoning benchmarks.", "ai_keywords": ["Reinforcement Learning from Verifier Rewards", "group-based methods", "GRPO", "advantage estimation", "bias correction", "adaptive reweighting", "difficulty weighting", "mathematical reasoning", "benchmark evaluation"], "summary_zh": "<ul>\n    <li>\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7fa4\u4f53\u5956\u52b1\uff08RLVR\uff09\u65b9\u6cd5\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u88ab\u5e7f\u6cdb\u5e94\u7528\u3002</li>\n    <li>\u7fa4\u4f53\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\u5b58\u5728\u56fa\u6709\u504f\u5dee\uff0c\u5bfc\u81f4\u5728\u56f0\u96be\u63d0\u793a\u4e0b\u4f4e\u4f30\u4f18\u52bf\uff0c\u800c\u5728\u7b80\u5355\u63d0\u793a\u4e0b\u9ad8\u4f30\u4f18\u52bf\u3002</li>\n    <li>\u8fd9\u79cd\u504f\u5dee\u4f1a\u5bfc\u81f4\u63a2\u7d22\u548c\u5229\u7528\u7684\u4e0d\u5e73\u8861\u3002</li>\n    <li>\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u2014\u2014\u5386\u53f2\u611f\u77e5\u81ea\u9002\u5e94\u96be\u5ea6\u52a0\u6743\uff08HA-DW\uff09\uff0c\u53ef\u4ee5\u8c03\u6574\u4f18\u52bf\u4f30\u8ba1\u3002</li>\n    <li>\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHA-DW\u80fd\u663e\u8457\u63d0\u9ad8GRPO\u53ca\u5176\u53d8\u79cd\u7684\u6027\u80fd\uff0c\u7ea0\u6b63\u504f\u5dee\u4f30\u8ba1\u5bf9\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Reinforcement Learning from Verifier Rewards (RLVR) is a popular method for improving large language models on reasoning tasks.</li>\n    <li>Group-based methods like GRPO estimate advantages but have a bias, underestimating hard prompts and overestimating easy ones.</li>\n    <li>This bias leads to poor exploration and exploitation in training.</li>\n    <li>The new method, History-Aware Adaptive Difficulty Weighting (HA-DW), adjusts advantage estimates for better accuracy based on training progress.</li>\n    <li>HA-DW improves performance in tests and is important for effective RLVR training.</li>\n</ul>"}, "publishedAt": "2026-01-13T08:03:15.000Z", "title": "Your Group-Relative Advantage Is Biased", "summary": "Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.\n  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.08521.png", "numComments": 5, "submittedBy": {"_id": "68345345f4bbf856e2d708e2", "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68345345f4bbf856e2d708e2/L5H2HNCuWje3ti2tNbC5p.jpeg", "fullname": "Yikun Ban", "name": "Yikunb", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 2, "isUserFollowing": false}, "isAuthorParticipating": true}, {"paper": {"id": "2601.07348", "authors": [{"_id": "696855610ac10a06522f69cf", "user": {"_id": "662911a202f5ad9a5195932f", "avatarUrl": "/avatars/663d142e27abbdb319ed5fd2cbe3f1a4.svg", "isPro": false, "fullname": "Tu Hu", "user": "Blackteaxxx", "type": "user"}, "name": "Tu Hu", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:18.320Z", "hidden": false}, {"_id": "696855610ac10a06522f69d0", "name": "Ronghao Chen", "hidden": false}, {"_id": "696855610ac10a06522f69d1", "user": {"_id": "65562edfb7bad186e877c724", "avatarUrl": "/avatars/bb91f42b102e113208bbe3238916a015.svg", "isPro": false, "fullname": "zhangshuo", "user": "mcflurryshuoz", "type": "user"}, "name": "Shuo Zhang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:16.329Z", "hidden": false}, {"_id": "696855610ac10a06522f69d2", "name": "Jianghao Yin", "hidden": false}, {"_id": "696855610ac10a06522f69d3", "name": "Mou Xiao Feng", "hidden": false}, {"_id": "696855610ac10a06522f69d4", "name": "Jingping Liu", "hidden": false}, {"_id": "696855610ac10a06522f69d5", "name": "Shaolei Zhang", "hidden": false}, {"_id": "696855610ac10a06522f69d6", "name": "Wenqi Jiang", "hidden": false}, {"_id": "696855610ac10a06522f69d7", "name": "Yuqi Fang", "hidden": false}, {"_id": "696855610ac10a06522f69d8", "name": "Sen Hu", "hidden": false}, {"_id": "696855610ac10a06522f69d9", "name": "Yi Xu", "hidden": false}, {"_id": "696855610ac10a06522f69da", "user": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "name": "Huacan Wang", "status": "claimed_verified", "statusLastChangedAt": "2026-01-15T15:03:20.275Z", "hidden": false}], "publishedAt": "2026-01-12T09:23:13.000Z", "submittedOnDailyAt": "2026-01-15T00:23:14.421Z", "title": "Controlled Self-Evolution for Algorithmic Code Optimization", "submittedOnDailyBy": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "isPro": false, "fullname": "Huacan Wang", "user": "Huacan-Wang", "type": "user"}, "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.", "upvotes": 94, "discussionId": "696855610ac10a06522f69db", "githubRepo": "https://github.com/QuantaAlpha/EvoControl", "githubRepoAddedBy": "user", "ai_summary": "Controlled Self-Evolution method improves code generation through diversified initialization, feedback-guided genetic evolution, and hierarchical memory to enhance exploration efficiency and solution quality.", "ai_keywords": ["self-evolution methods", "generate-verify-refine cycles", "exploration efficiency", "initialization bias", "stochastic operations", "feedback guidance", "genetic evolution", "targeted mutation", "compositional crossover", "hierarchical evolution memory", "LLM backbones", "EffiBench-X"], "githubStars": 79, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "summary_zh": "<ul>\n    <li>\u81ea\u6211\u8fdb\u5316\u65b9\u6cd5\u901a\u8fc7\u201c\u751f\u6210-\u9a8c\u8bc1-\u6539\u8fdb\u201d\u5faa\u73af\u6765\u63d0\u5347\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002</li>\n    <li>\u73b0\u6709\u65b9\u6cd5\u63a2\u7d22\u6548\u7387\u4f4e\uff0c\u96be\u4ee5\u5728\u6709\u9650\u9884\u7b97\u5185\u627e\u5230\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002</li>\n    <li>\u63d0\u51fa\u4e86\u53d7\u63a7\u81ea\u6211\u8fdb\u5316\uff08CSE\uff09\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\u3002</li>\n    <li>CSE\u901a\u8fc7\u591a\u6837\u5316\u89c4\u5212\u521d\u59cb\u5316\u3001\u57fa\u56e0\u8fdb\u5316\u548c\u5c42\u7ea7\u8fdb\u5316\u8bb0\u5fc6\u6765\u63d0\u9ad8\u6548\u7387\u3002</li>\n    <li>\u5b9e\u9a8c\u8868\u660e\uff0cCSE\u5728\u4e0d\u540c\u7684LLM\u57fa\u7840\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u6301\u7eed\u6539\u8fdb\u3002</li>\n</ul>", "summary_simple": "<ul>\n    <li>Self-evolution methods help improve code generation but often struggle to find better solutions efficiently.</li>\n    <li>The main issues are poor starting points, random changes without guidance, and not using past experiences effectively.</li>\n    <li>To improve this, Controlled Self-Evolution (CSE) introduces three main features: diverse planning for better initial strategies, feedback-guided changes, and a memory system to learn from past experiences.</li>\n    <li>Tests show that CSE consistently performs better than other methods and improves quickly over time.</li>\n    <li>The code for this method is available online for others to use and test.</li>\n</ul>"}, "publishedAt": "2026-01-12T04:23:13.000Z", "title": "Controlled Self-Evolution for Algorithmic Code Optimization", "summary": "Self-evolution methods enhance code generation through iterative \"generate-verify-refine\" cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.", "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.07348.png", "numComments": 3, "submittedBy": {"_id": "6603d56ab4344a2b07cd6d21", "avatarUrl": "/avatars/1569bb60166532317c85e80da722ba1c.svg", "fullname": "Huacan Wang", "name": "Huacan-Wang", "type": "user", "isPro": false, "isHf": false, "isHfAdmin": false, "isMod": false, "followerCount": 3, "isUserFollowing": false}, "organization": {"_id": "68b33ab6a9ed99140481cf44", "name": "QuantaAlpha", "fullname": "QuantaAlpha", "avatar": "https://cdn-uploads.huggingface.co/production/uploads/63f7767fbd28622c9b9915e9/DRN8PvmnpKmn2MSLQ7qhF.jpeg"}, "isAuthorParticipating": false}]
};
window.papersLastUpdated = "Jan 22, 2026";